>>>> llms/llama/llama.py
# Copyright © 2023 Apple Inc.

import argparse
import glob
import json
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Optional, Tuple

import mlx.core as mx
import mlx.nn as nn
from mlx.utils import tree_unflatten
from sentencepiece import SentencePieceProcessor


@dataclass
class ModelArgs:
    dim: int
    n_layers: int
    head_dim: int
    hidden_dim: int
    n_heads: int
    n_kv_heads: int
    norm_eps: float
    vocab_size: int
    rope_theta: float
    rope_traditional: bool = True


class Attention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args

        self.n_heads: int = args.n_heads
        self.n_kv_heads: int = args.n_kv_heads

        self.repeats = self.n_heads // self.n_kv_heads

        self.scale = self.args.head_dim**-0.5

        self.wq = nn.Linear(args.dim, args.n_heads * args.head_dim, bias=False)
        self.wk = nn.Linear(args.dim, args.n_kv_heads * args.head_dim, bias=False)
        self.wv = nn.Linear(args.dim, args.n_kv_heads * args.head_dim, bias=False)
        self.wo = nn.Linear(args.n_heads * args.head_dim, args.dim, bias=False)
        self.rope = nn.RoPE(
            args.head_dim, traditional=args.rope_traditional, base=args.rope_theta
        )

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Tuple[mx.array, mx.array]] = None,
    ) -> Tuple[mx.array, Tuple[mx.array, mx.array]]:
        B, L, D = x.shape

        queries, keys, values = self.wq(x), self.wk(x), self.wv(x)

        # Prepare the queries, keys and values for the attention computation
        queries = queries.reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)
        keys = keys.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)
        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)

        def repeat(a):
            a = mx.concatenate([mx.expand_dims(a, 2)] * self.repeats, axis=2)
            return a.reshape([B, self.n_heads, L, -1])

        keys, values = map(repeat, (keys, values))

        if cache is not None:
            key_cache, value_cache = cache
            queries = self.rope(queries, offset=key_cache.shape[2])
            keys = self.rope(keys, offset=key_cache.shape[2])
            keys = mx.concatenate([key_cache, keys], axis=2)
            values = mx.concatenate([value_cache, values], axis=2)
        else:
            queries = self.rope(queries)
            keys = self.rope(keys)

        scores = (queries * self.scale) @ keys.transpose(0, 1, 3, 2)
        if mask is not None:
            scores += mask
        scores = mx.softmax(scores.astype(mx.float32), axis=-1).astype(scores.dtype)
        output = (scores @ values).transpose(0, 2, 1, 3).reshape(B, L, -1)
        return self.wo(output), (keys, values)


class FeedForward(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        self.w1 = nn.Linear(args.dim, args.hidden_dim, bias=False)
        self.w2 = nn.Linear(args.hidden_dim, args.dim, bias=False)
        self.w3 = nn.Linear(args.dim, args.hidden_dim, bias=False)

    def __call__(self, x) -> mx.array:
        return self.w2(nn.silu(self.w1(x)) * self.w3(x))


class TransformerBlock(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.n_heads = args.n_heads
        self.dim = args.dim
        self.attention = Attention(args)
        self.feed_forward = FeedForward(args=args)
        self.attention_norm = nn.RMSNorm(args.dim, eps=args.norm_eps)
        self.ffn_norm = nn.RMSNorm(args.dim, eps=args.norm_eps)
        self.args = args

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Tuple[mx.array, mx.array]] = None,
    ) -> mx.array:
        r, cache = self.attention(self.attention_norm(x), mask, cache)
        h = x + r
        r = self.feed_forward(self.ffn_norm(h))
        out = h + r
        return out, cache


class Llama(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.vocab_size = args.vocab_size
        self.tok_embeddings = nn.Embedding(args.vocab_size, args.dim)
        self.layers = [TransformerBlock(args=args) for _ in range(args.n_layers)]
        self.norm = nn.RMSNorm(args.dim, eps=args.norm_eps)
        self.output = nn.Linear(args.dim, args.vocab_size, bias=False)

    def __call__(self, x):
        mask = nn.MultiHeadAttention.create_additive_causal_mask(x.shape[1])
        mask = mask.astype(self.tok_embeddings.weight.dtype)

        x = self.tok_embeddings(x)
        for l in self.layers:
            x, _ = l(x, mask)
        x = self.norm(x)
        return self.output(x)

    def generate(self, x, temp=1.0):
        def sample(logits):
            if temp == 0:
                return mx.argmax(logits, axis=-1)
            else:
                return mx.random.categorical(logits * (1 / temp))

        cache = []

        # Make an additive causal mask. We will need that to process the prompt.
        mask = nn.MultiHeadAttention.create_additive_causal_mask(x.shape[1])
        mask = mask.astype(self.tok_embeddings.weight.dtype)

        # First we process the prompt x the same was as in __call__ but
        # save the caches in cache
        x = self.tok_embeddings(x)
        for l in self.layers:
            x, c = l(x, mask=mask)
            # We store the per layer cache in a simple python list
            cache.append(c)
        x = self.norm(x)
        # We only care about the last logits that generate the next token
        y = self.output(x[:, -1])
        y = sample(y)

        # y now has size [1]
        # Since MLX is lazily evaluated nothing is computed yet.
        # Calling y.item() would force the computation to happen at
        # this point but we can also choose not to do that and let the
        # user choose when to start the computation.
        yield y

        # Now we parsed the prompt and generated the first token we
        # need to feed it back into the model and loop to generate the
        # rest.
        while True:
            # Unsqueezing the last dimension to add a sequence length
            # dimension of 1
            x = y[:, None]

            x = self.tok_embeddings(x)
            for i in range(len(cache)):
                # We are overwriting the arrays in the cache list. When
                # the computation will happen, MLX will be discarding the
                # old cache the moment it is not needed anymore.
                x, cache[i] = self.layers[i](x, mask=None, cache=cache[i])
            x = self.norm(x)
            y = sample(self.output(x[:, -1]))

            yield y


def tic():
    return time.time()


def toc(msg, start):
    end = time.time()
    return f"[INFO] {msg}: {end - start:.3f} s"


def generate(args):
    input("Press enter to start generation")
    print("------")
    print(args.prompt)
    x = mx.array([[tokenizer.bos_id()] + tokenizer.encode(args.prompt)])
    skip = 0
    prompt_processing = None
    tokens = []
    start = tic()
    for token in model.generate(x, args.temp):
        tokens.append(token)

        if len(tokens) == 1:
            # Actually perform the computation to measure the prompt processing time
            mx.eval(token)
            prompt_processing = toc("Prompt processing", start)

        if len(tokens) >= args.max_tokens:
            break

        elif (len(tokens) % args.write_every) == 0:
            # It is perfectly ok to eval things we have already eval-ed.
            mx.eval(tokens)
            s = tokenizer.decode([t.item() for t in tokens])
            print(s[skip:], end="", flush=True)
            skip = len(s)

    mx.eval(tokens)
    full_gen = toc("Full generation", start)
    s = tokenizer.decode([t.item() for t in tokens])
    print(s[skip:], flush=True)
    print("------")
    print(prompt_processing)
    print(full_gen)


def few_shot_generate(args):
    def possible_end(s):
        word = "[Instruction]"
        for i in range(len(word) - 1, 0, -1):
            if s[-i:] == word[:i]:
                return 0
        if s[-len(word) :] == word:
            return 1
        return -1

    def generate(question):
        x = mx.array([[tokenizer.bos_id()] + tokenizer.encode(question)])
        skip = 0
        prompt_processing = None
        tokens = []
        start = tic()
        for token in model.generate(x, args.temp):
            tokens.append(token)

            if len(tokens) == 1:
                # Actually perform the computation to measure the prompt processing time
                mx.eval(token)
                prompt_processing = toc("Prompt processing", start)

            if len(tokens) >= args.max_tokens:
                break

            mx.eval(tokens)
            token_list = [t.item() for t in tokens]
            s = tokenizer.decode(token_list)

            end = possible_end(s)
            if end == 0:
                continue
            if end == 1:
                skip = len(s)
                break

            print(s[skip:], end="", flush=True)
            skip = len(s)
            if token_list[-1] == tokenizer.eos_id():
                break

        mx.eval(tokens)
        full_gen = toc("Full generation", start)
        s = tokenizer.decode([t.item() for t in tokens])
        print(s[skip:], end="", flush=True)

    print("[INFO] Loading few-shot examples from: {}".format(args.few_shot))
    prompt = open(args.few_shot).read().strip()
    while True:
        question = input("Ask a question: ")
        generate(prompt.replace("{}", question))
        print()


def sanitize_config(config, weights):
    config.pop("model_type", None)
    n_heads = config["n_heads"]
    if "n_kv_heads" not in config:
        config["n_kv_heads"] = n_heads
    if "head_dim" not in config:
        config["head_dim"] = config["dim"] // n_heads
    if "hidden_dim" not in config:
        config["hidden_dim"] = weights["layers.0.feed_forward.w1.weight"].shape[0]
    if config.get("vocab_size", -1) < 0:
        config["vocab_size"] = weights["output.weight"].shape[-1]
    if "rope_theta" not in config:
        config["rope_theta"] = 10000
    unused = ["multiple_of", "ffn_dim_multiplier"]
    for k in unused:
        config.pop(k, None)
    return config


def load_model(model_path):
    model_path = Path(model_path)

    unsharded_weights_path = Path(model_path / "weights.npz")
    if unsharded_weights_path.is_file():
        print("[INFO] Loading model from {}.".format(unsharded_weights_path))
        weights = mx.load(str(unsharded_weights_path))
    else:
        sharded_weights_glob = str(model_path / "weights.*.npz")
        weight_files = glob.glob(sharded_weights_glob)
        print("[INFO] Loading model from {}.".format(sharded_weights_glob))

        if len(weight_files) == 0:
            raise FileNotFoundError("No weights found in {}".format(model_path))

        weights = {}
        for wf in weight_files:
            weights.update(mx.load(wf).items())

    with open(model_path / "config.json", "r") as f:
        config = sanitize_config(json.loads(f.read()), weights)
        quantization = config.pop("quantization", None)
    model = Llama(ModelArgs(**config))
    if quantization is not None:
        nn.quantize(model, **quantization)
    model.update(tree_unflatten(list(weights.items())))
    tokenizer = SentencePieceProcessor(model_file=str(model_path / "tokenizer.model"))
    return model, tokenizer


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Llama inference script")
    parser.add_argument(
        "--model-path",
        help="Path to the model weights and tokenizer",
        default="mlx_model",
    )
    parser.add_argument(
        "--prompt",
        help="The message to be processed by the model. Ignored when --few-shot is provided.",
        default="In the beginning the Universe was created.",
    )
    parser.add_argument(
        "--few-shot",
        help="Read a few shot prompt from a file (as in `sample_prompt.txt`).",
    )
    parser.add_argument(
        "--max-tokens", "-m", type=int, default=100, help="How many tokens to generate"
    )
    parser.add_argument(
        "--write-every", type=int, default=1, help="After how many tokens to detokenize"
    )
    parser.add_argument(
        "--temp", type=float, default=0.0, help="The sampling temperature"
    )
    parser.add_argument("--seed", type=int, default=0, help="The PRNG seed")

    args = parser.parse_args()

    mx.random.seed(args.seed)

    model, tokenizer = load_model(args.model_path)
    if args.few_shot:
        few_shot_generate(args)
    else:
        generate(args)

>>>> llms/llama/README.md
# Llama

An example of generating text with Llama (1 or 2) using MLX.

Llama is a set of open source language models from Meta AI Research[^1][^2]
ranging from 7B to 70B parameters. This example also supports Meta's Llama Chat
and Code Llama models, as well as the 1.1B TinyLlama models from SUTD.[^3]

### Setup

Install the dependencies:

```
pip install -r requirements.txt
```

Next, download and convert the model. If you do not have access to the model
weights you will need to request access from Meta:

- [Request Llama v1](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform)
- [Request Llama v2](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)

> [!TIP] Alternatively, you can also download a few converted checkpoints from
> the [MLX Community](https://huggingface.co/mlx-community) organization on
> Hugging Face and skip the conversion step.

You can download the TinyLlama models directly from [Hugging
Face](https://huggingface.co/TinyLlama).

Convert the weights with:

```
python convert.py --torch-path <path_to_torch_model>
```

To generate a 4-bit quantized model use the `-q` flag:

```
python convert.py --torch-path <path_to_torch_model> -q
```

For TinyLlama use

```
python convert.py --torch-path <path_to_torch_model> --model-name tiny_llama
```

By default, the conversion script will make the directory `mlx_model` and save
the converted `weights.npz`, `tokenizer.model`, and `config.json` there.


### Run

Once you've converted the weights to MLX format, you can interact with the
LlamA model:

```
python llama.py --prompt "hello"
```

Run `python llama.py --help` for more details.

[^1]: For Llama v1 refer to the [arXiv paper](https://arxiv.org/abs/2302.13971) and [blog post](https://ai.meta.com/blog/large-language-model-llama-meta-ai/) for more details.
[^2]: For Llama v2 refer to the [blob post](https://ai.meta.com/llama/)
[^3]: For TinyLlama refer to the [gihub repository](https://github.com/jzhang38/TinyLlama?tab=readme-ov-file)

>>>> llms/llama/convert.py
# Copyright © 2023 Apple Inc.

import argparse
import collections
import copy
import glob
import json
import shutil
from pathlib import Path

import mlx.core as mx
import mlx.nn as nn
import torch
from llama import Llama, ModelArgs, sanitize_config
from mlx.utils import tree_flatten, tree_map, tree_unflatten


def torch_to_mx(a: torch.Tensor, *, dtype: str) -> mx.array:
    # bfloat16 is not numpy convertible. Upcast to float32 to avoid precision loss
    a = a.to(torch.float32) if dtype == "bfloat16" else a.to(getattr(torch, dtype))
    return mx.array(a.numpy(), getattr(mx, dtype))


def llama(model_path, *, dtype: str):
    SHARD_FIRST = ["wv", "wq", "wk", "w1", "w3", "output"]
    SHARD_SECOND = ["tok_embeddings", "wo", "w2"]
    SHARD_WEIGHTS = set(SHARD_FIRST + SHARD_SECOND)

    def shard_key(k):
        keys = k.split(".")
        if len(keys) < 2:
            return None
        return keys[-2]

    def unshard(k, v):
        wn = shard_key(k)
        if wn not in SHARD_WEIGHTS:
            return v
        elif wn in SHARD_FIRST:
            axis = 0
        elif wn in SHARD_SECOND:
            axis = 1
        else:
            raise ValueError("Invalid weight name")
        return mx.concatenate(v, axis=axis)

    torch_files = glob.glob(str(model_path / "consolidated.*.pth"))
    weights = collections.defaultdict(list)
    for wf in torch_files:
        state = torch.load(wf, map_location=torch.device("cpu"))
        for k, v in state.items():
            v = torch_to_mx(v, dtype=dtype)
            state[k] = None  # free memory
            if shard_key(k) in SHARD_WEIGHTS:
                weights[k].append(v)
            else:
                weights[k] = v

    for k, v in weights.items():
        weights[k] = unshard(k, v)
    with open(model_path / "params.json", "r") as f:
        params = json.loads(f.read())
    return weights, params


def tiny_llama(model_path, *, dtype: str):
    try:
        import transformers
    except ImportError:
        print("The transformers package must be installed for this model conversion:")
        print("pip install transformers")
        exit(1)

    model = transformers.AutoModelForCausalLM.from_pretrained(
        str(model_path)
    ).state_dict()
    config = transformers.AutoConfig.from_pretrained(model_path)

    # things to change
    # 1. there's no "model." in the weight names
    model = {k.replace("model.", ""): v for k, v in model.items()}

    # 2. mlp is called feed_forward
    model = {k.replace("mlp", "feed_forward"): v for k, v in model.items()}

    # 3. up_proj, down_proj, gate_proj
    model = {k.replace("down_proj", "w2"): v for k, v in model.items()}
    model = {k.replace("up_proj", "w3"): v for k, v in model.items()}
    model = {k.replace("gate_proj", "w1"): v for k, v in model.items()}

    # 4. layernorms
    model = {
        k.replace("input_layernorm", "attention_norm"): v for k, v in model.items()
    }
    model = {
        k.replace("post_attention_layernorm", "ffn_norm"): v for k, v in model.items()
    }

    # 5. lm head
    model = {k.replace("lm_head", "output"): v for k, v in model.items()}

    # 6. token emb
    model = {k.replace("embed_tokens", "tok_embeddings"): v for k, v in model.items()}

    # 7. attention
    model = {k.replace("self_attn", "attention"): v for k, v in model.items()}
    model = {k.replace("q_proj", "wq"): v for k, v in model.items()}
    model = {k.replace("k_proj", "wk"): v for k, v in model.items()}
    model = {k.replace("v_proj", "wv"): v for k, v in model.items()}
    model = {k.replace("o_proj", "wo"): v for k, v in model.items()}

    params = {}
    params["dim"] = config.hidden_size
    params["hidden_dim"] = config.intermediate_size
    params["n_heads"] = config.num_attention_heads
    if hasattr(config, "num_key_value_heads"):
        params["n_kv_heads"] = config.num_key_value_heads
    params["n_layers"] = config.num_hidden_layers
    params["vocab_size"] = config.vocab_size
    params["norm_eps"] = config.rms_norm_eps
    params["rope_traditional"] = False
    weights = {k: torch_to_mx(v, dtype=dtype) for k, v in model.items()}

    return weights, params


def quantize(weights, config, args):
    quantized_config = copy.deepcopy(config)

    # Load the model:
    config = sanitize_config(config, weights)
    model = Llama(ModelArgs(**config))
    weights = tree_map(mx.array, weights)
    model.update(tree_unflatten(list(weights.items())))

    # Quantize the model:
    nn.quantize(model, args.q_group_size, args.q_bits)

    # Update the config:
    quantized_config["quantization"] = {
        "group_size": args.q_group_size,
        "bits": args.q_bits,
    }
    quantized_weights = dict(tree_flatten(model.parameters()))

    return quantized_weights, quantized_config


def make_shards(weights: dict, max_file_size_gibibyte: int = 15):
    max_file_size_bytes = max_file_size_gibibyte << 30
    shards = []
    shard, shard_size = {}, 0
    for k, v in weights.items():
        if shard_size + v.nbytes > max_file_size_bytes:
            shards.append(shard)
            shard, shard_size = {}, 0
        shard[k] = v
        shard_size += v.nbytes
    shards.append(shard)
    return shards


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Convert Llama weights to MLX")
    parser.add_argument(
        "--torch-path",
        type=str,
        help="Path to the PyTorch model.",
    )
    parser.add_argument(
        "--mlx-path",
        type=str,
        default="mlx_model",
        help="Path to save the MLX model.",
    )
    parser.add_argument(
        "--model-name",
        help=(
            "Name of the model to convert. Use 'llama' for models in the "
            "Llama family distributed by Meta including Llama 1, Llama 2, "
            "Code Llama, and Llama chat."
        ),
        choices=["tiny_llama", "llama"],
        default="llama",
    )
    parser.add_argument(
        "-q",
        "--quantize",
        help="Generate a quantized model.",
        action="store_true",
    )
    parser.add_argument(
        "--q-group-size",
        help="Group size for quantization.",
        type=int,
        default=64,
    )
    parser.add_argument(
        "--q-bits",
        help="Bits per weight for quantization.",
        type=int,
        default=4,
    )
    parser.add_argument(
        "--dtype",
        help="dtype for loading the torch model and input for quantization or saving the converted model. "
        "The original weights are stored in bfloat16.",
        type=str,
        default="float16",
    )

    args = parser.parse_args()

    torch_path = Path(args.torch_path)
    mlx_path = Path(args.mlx_path)
    mlx_path.mkdir(parents=True, exist_ok=True)

    print("[INFO] Loading")
    weights, params = globals()[args.model_name](torch_path, dtype=args.dtype)
    params["model_type"] = "llama"
    if args.quantize:
        print("[INFO] Quantizing")
        weights, params = quantize(weights, params, args)

    print("[INFO] Saving")
    shutil.copyfile(
        str(torch_path / "tokenizer.model"),
        str(mlx_path / "tokenizer.model"),
    )
    shards = make_shards(weights)
    if len(shards) == 1:
        mx.savez(str(mlx_path / f"weights.npz"), **shards[0])
    else:
        for i, shard in enumerate(shards):
            mx.savez(str(mlx_path / f"weights.{i:02d}.npz"), **shard)
    with open(mlx_path / "config.json", "w") as fid:
        json.dump(params, fid, indent=4)

>>>> llms/mistral/README.md
# Mistral 

An example of generating text with Mistral using MLX.

Mistral 7B is one of the top large language models in its size class. It is
also fully open source with a permissive license[^1].

### Setup

Install the dependencies:

```
pip install -r requirements.txt
```

Next, download the model and tokenizer:

```
curl -O https://models.mistralcdn.com/mistral-7b-v0-1/mistral-7B-v0.1.tar
tar -xf mistral-7B-v0.1.tar
```

Then, convert the weights with:

```
python convert.py --torch-path <path_to_torch>
```

To generate a 4-bit quantized model, use ``-q``. For a full list of options:

```
python convert.py --help
```

By default, the conversion script will make the directory `mlx_model` and save
the converted `weights.npz`, `tokenizer.model`, and `config.json` there.

> [!TIP]
> Alternatively, you can also download a few converted checkpoints from the
> [MLX Community](https://huggingface.co/mlx-community) organization on Hugging
> Face and skip the conversion step.


### Run

Once you've converted the weights to MLX format, you can generate text with
the Mistral model:

```
python mistral.py --prompt "It is a truth universally acknowledged,"
```

Run `python mistral.py --help` for more details.

[^1]: Refer to the [blog post](https://mistral.ai/news/announcing-mistral-7b/)
and [github repository](https://github.com/mistralai/mistral-src) for more
details.

>>>> llms/mistral/test.py
# Copyright © 2023 Apple Inc.

import unittest

import mistral
import mlx.core as mx
from mlx.utils import tree_map


class TestMistral(unittest.TestCase):
    def test_model(self):
        vocab_size = 100
        L = 32
        args = mistral.ModelArgs(
            dim=128,
            n_layers=2,
            head_dim=32,
            hidden_dim=256,
            n_heads=4,
            n_kv_heads=4,
            norm_eps=1e-3,
            vocab_size=vocab_size,
        )

        model = mistral.Mistral(args)
        inputs = mx.random.randint(0, vocab_size, (L,))
        logits, cache = model(inputs[None])
        self.assertEqual(logits.shape, [1, L, vocab_size])
        self.assertEqual(logits.dtype, mx.float32)
        self.assertEqual(len(cache), args.n_layers)

        params = tree_map(lambda p: p.astype(mx.float16), model.parameters())
        model.update(params)
        logits, _ = model(inputs[None])
        self.assertEqual(logits.dtype, mx.float16)

    def test_generate(self):
        model, tokenizer = mistral.load_model("mistral-7B-v0.1")
        prompt = mx.array(tokenizer.encode("This is a test"))
        tokens = [t for t, _ in zip(mistral.generate(prompt, model), range(30))]
        mx.eval(tokens)
        tokens = [t.item() for t in tokens]
        expected = [
            302,
            272,
            11843,
            11837,
            1587,
            28723,
            851,
            349,
            865,
            264,
            1369,
            28723,
            13,
            13,
            3381,
            456,
            654,
            264,
            1353,
            11843,
            28725,
            368,
            682,
            347,
            2240,
            767,
            298,
            511,
            28723,
            13,
        ]
        self.assertEqual(tokens, expected)

    def benchmark(self):
        import time

        model, tokenizer = mistral.load_model("mistral-7B-v0.1")
        prompt = mx.random.randint(0, model.vocab_size, (128,))

        # warmup
        for _ in range(2):
            generator = mistral.generate(prompt, model)
            mx.eval(next(generator))

        tic = time.time()
        its = 5
        for _ in range(its):
            generator = mistral.generate(prompt, model)
            mx.eval(next(generator))
        toc = time.time()
        tps = its * prompt.size / (toc - tic)
        print(f"Prompt processing: {tps:.2f} tokens per second")

        # warmup
        for _ in range(2):
            tokens = [t for t, _ in zip(mistral.generate(prompt, model), range(101))]
            mx.eval(tokens)

        time_total = 0.0
        its = 2
        for _ in range(its):
            generator = mistral.generate(prompt, model)
            mx.eval(next(generator))
            tic = time.time()
            tokens = [t for t, _ in zip(generator, range(100))]
            mx.eval(tokens)
            time_total += time.time() - tic

        tps = len(tokens) * its / time_total
        print(f"Token generation: {tps:.3f} tokens per second")


if __name__ == "__main__":
    unittest.main()

>>>> llms/mistral/convert.py
# Copyright © 2023 Apple Inc.

import argparse
import copy
import json
import shutil
from pathlib import Path

import mlx.core as mx
import mlx.nn as nn
import numpy as np
import torch
from mistral import Mistral, ModelArgs
from mlx.utils import tree_flatten, tree_map, tree_unflatten


def quantize(weights, config, args):
    quantized_config = copy.deepcopy(config)

    # Load the model:
    config.pop("sliding_window", None)
    model = Mistral(ModelArgs(**config))
    weights = tree_map(mx.array, weights)
    model.update(tree_unflatten(list(weights.items())))

    # Quantize the model:
    nn.quantize(model, args.q_group_size, args.q_bits)

    # Update the config:
    quantized_config["quantization"] = {
        "group_size": args.q_group_size,
        "bits": args.q_bits,
    }
    quantized_weights = dict(tree_flatten(model.parameters()))

    return quantized_weights, quantized_config


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Convert Mistral weights to MLX.")
    parser.add_argument(
        "--torch-path",
        type=str,
        default="mistral-7B-v0.1",
        help="The path to the PyTorch model.",
    )
    parser.add_argument(
        "--mlx-path",
        type=str,
        default="mlx_model",
        help="The path to save the MLX model.",
    )
    parser.add_argument(
        "-q",
        "--quantize",
        help="Generate a quantized model.",
        action="store_true",
    )
    parser.add_argument(
        "--q-group-size",
        help="Group size for quantization.",
        type=int,
        default=64,
    )
    parser.add_argument(
        "--q-bits",
        help="Bits per weight for quantization.",
        type=int,
        default=4,
    )
    args = parser.parse_args()

    torch_path = Path(args.torch_path)
    state = torch.load(str(torch_path / "consolidated.00.pth"))
    mlx_path = Path(args.mlx_path)
    mlx_path.mkdir(parents=True, exist_ok=True)

    weights = {k: v.to(torch.float16).numpy() for k, v in state.items()}
    with open(torch_path / "params.json", "r") as f:
        config = json.loads(f.read())

    if args.quantize:
        print("[INFO] Quantizing")
        weights, config = quantize(weights, config, args)

    # Save weights
    np.savez(str(mlx_path / "weights.npz"), **weights)

    # Copy tokenizer
    shutil.copyfile(
        str(torch_path / "tokenizer.model"),
        str(mlx_path / "tokenizer.model"),
    )

    # Save config.json with model_type
    with open(mlx_path / "config.json", "w") as f:
        config["model_type"] = "mistral"
        json.dump(config, f, indent=4)

>>>> llms/mistral/mistral.py
# Copyright © 2023 Apple Inc.

import argparse
import json
import time
from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional, Tuple

import mlx.core as mx
import mlx.nn as nn
from mlx.utils import tree_unflatten
from sentencepiece import SentencePieceProcessor


@dataclass
class ModelArgs:
    dim: int
    n_layers: int
    head_dim: int
    hidden_dim: int
    n_heads: int
    n_kv_heads: int
    norm_eps: float
    vocab_size: int
    rope_theta: float = 10000


class Attention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args

        self.n_heads: int = args.n_heads
        self.n_kv_heads: int = args.n_kv_heads

        self.repeats = self.n_heads // self.n_kv_heads

        self.scale = self.args.head_dim**-0.5

        self.wq = nn.Linear(args.dim, args.n_heads * args.head_dim, bias=False)
        self.wk = nn.Linear(args.dim, args.n_kv_heads * args.head_dim, bias=False)
        self.wv = nn.Linear(args.dim, args.n_kv_heads * args.head_dim, bias=False)
        self.wo = nn.Linear(args.n_heads * args.head_dim, args.dim, bias=False)
        self.rope = nn.RoPE(args.head_dim, traditional=True, base=args.rope_theta)

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Tuple[mx.array, mx.array]] = None,
    ) -> mx.array:
        B, L, D = x.shape

        queries, keys, values = self.wq(x), self.wk(x), self.wv(x)

        # Prepare the queries, keys and values for the attention computation
        queries = queries.reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)
        keys = keys.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)
        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)

        if cache is not None:
            key_cache, value_cache = cache
            queries = self.rope(queries, offset=key_cache.shape[2])
            keys = self.rope(keys, offset=key_cache.shape[2])
            keys = mx.concatenate([key_cache, keys], axis=2)
            values = mx.concatenate([value_cache, values], axis=2)
        else:
            queries = self.rope(queries)
            keys = self.rope(keys)

        output = mx.fast.scaled_dot_product_attention(
            queries, keys, values, scale=self.scale, mask=mask
        )
        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)
        return self.wo(output), (keys, values)


class FeedForward(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        self.w1 = nn.Linear(args.dim, args.hidden_dim, bias=False)
        self.w2 = nn.Linear(args.hidden_dim, args.dim, bias=False)
        self.w3 = nn.Linear(args.dim, args.hidden_dim, bias=False)

    def __call__(self, x) -> mx.array:
        return self.w2(nn.silu(self.w1(x)) * self.w3(x))


class TransformerBlock(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.n_heads = args.n_heads
        self.dim = args.dim
        self.attention = Attention(args)
        self.feed_forward = FeedForward(args=args)
        self.attention_norm = nn.RMSNorm(args.dim, eps=args.norm_eps)
        self.ffn_norm = nn.RMSNorm(args.dim, eps=args.norm_eps)
        self.args = args

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Tuple[mx.array, mx.array]] = None,
    ) -> mx.array:
        r, cache = self.attention(self.attention_norm(x), mask, cache)
        h = x + r
        r = self.feed_forward(self.ffn_norm(h))
        out = h + r
        return out, cache


class Mistral(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.vocab_size = args.vocab_size
        self.n_layers = args.n_layers
        assert self.vocab_size > 0
        self.tok_embeddings = nn.Embedding(args.vocab_size, args.dim)
        self.layers = [TransformerBlock(args=args) for _ in range(args.n_layers)]
        self.norm = nn.RMSNorm(args.dim, eps=args.norm_eps)
        self.output = nn.Linear(args.dim, args.vocab_size, bias=False)

    def __call__(
        self,
        inputs: mx.array,
        cache=None,
    ):
        h = self.tok_embeddings(inputs)

        mask = None
        if h.shape[1] > 1:
            mask = nn.MultiHeadAttention.create_additive_causal_mask(h.shape[1])
            mask = mask.astype(h.dtype)

        if cache is None:
            cache = [None] * len(self.layers)

        for e, layer in enumerate(self.layers):
            h, cache[e] = layer(h, mask, cache[e])

        return self.output(self.norm(h)), cache


class Tokenizer:
    def __init__(self, model_path: str):
        assert Path(model_path).exists(), model_path
        self._model = SentencePieceProcessor(model_file=model_path)
        self._sep = "▁"
        assert self._model.vocab_size() == self._model.get_piece_size()

    @property
    def eos_id(self) -> int:
        return self._model.eos_id()

    @property
    def pad_id(self) -> int:
        return self._model.pad_id()

    def encode(self, s: str) -> List[int]:
        return [self._model.bos_id(), *self._model.encode(s)]

    def decode(self, t: List[int]) -> str:
        out = self._model.decode(t)
        if t and self._model.id_to_piece(t[0])[0] == self._sep:
            return " " + out
        return out


def load_model(folder: str):
    model_path = Path(folder)
    tokenizer = Tokenizer(str(model_path / "tokenizer.model"))
    with open(model_path / "config.json", "r") as f:
        config = json.loads(f.read())
        config.pop("sliding_window", None)
        config.pop("model_type", None)
        quantization = config.pop("quantization", None)
        model_args = ModelArgs(**config)
    weights = mx.load(str(model_path / "weights.npz"))
    weights = tree_unflatten(list(weights.items()))
    model = Mistral(model_args)
    if quantization is not None:
        nn.quantize(model, **quantization)
    model.update(weights)
    mx.eval(model.parameters())
    return model, tokenizer


def generate(prompt: mx.array, model: Mistral, temp: Optional[float] = 0.0):
    def sample(logits):
        if temp == 0:
            return mx.argmax(logits, axis=-1)
        else:
            return mx.random.categorical(logits * (1 / temp))

    logits, cache = model(prompt[None])
    y = sample(logits[:, -1, :])
    yield y

    while True:
        logits, cache = model(y[:, None], cache)
        y = sample(logits.squeeze(1))
        yield y


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Mistral inference script")
    parser.add_argument(
        "--model-path",
        type=str,
        default="mlx_model",
        help="The path to the model weights and tokenizer",
    )
    parser.add_argument(
        "--prompt",
        help="The message to be processed by the model",
        default="In the beginning the Universe was created.",
    )
    parser.add_argument(
        "--max-tokens",
        "-m",
        type=int,
        default=100,
        help="Maximum number of tokens to generate",
    )
    parser.add_argument(
        "--temp",
        help="The sampling temperature.",
        type=float,
        default=0.0,
    )
    parser.add_argument(
        "--tokens-per-eval",
        help="The batch size of tokens to generate.",
        type=int,
        default=10,
    )
    parser.add_argument("--seed", type=int, default=0, help="The PRNG seed")

    args = parser.parse_args()

    mx.random.seed(args.seed)
    print("[INFO] Loading model from disk.")
    model, tokenizer = load_model(args.model_path)

    print("[INFO] Starting generation...")
    tic = time.time()
    print(args.prompt, end="", flush=True)
    prompt = mx.array(tokenizer.encode(args.prompt))
    tokens = []
    for token, ntoks in zip(generate(prompt, model, args.temp), range(args.max_tokens)):
        tokens.append(token)
        if ntoks == 0:
            mx.eval(tokens)
            toc = time.time()
            prompt_tps = prompt.size / (toc - tic)
            tic = time.time()

        if (len(tokens) % args.tokens_per_eval) == 0:
            mx.eval(tokens)
            s = tokenizer.decode([t.item() for t in tokens])
            print(s, end="", flush=True)
            tokens = []

    mx.eval(tokens)
    s = tokenizer.decode([t.item() for t in tokens])
    print(s, flush=True)
    print("------")
    generation_tps = ntoks / (time.time() - tic)
    print(
        f"Tokens per second: prompt {prompt_tps:.3f}, "
        f"generation {generation_tps:.3f}"
    )

>>>> llms/CONTRIBUTING.md
# Contributing to MLX LM 

Below are some tips to port LLMs available on Hugging Face to MLX.

Before starting checkout the [general contribution
guidelines](https://github.com/ml-explore/mlx-examples/blob/main/CONTRIBUTING.md).

Next, from this directory, do an editable install:

```shell
pip install -e .
```

Then check if the model has weights in the
[safetensors](https://huggingface.co/docs/safetensors/index) format. If not
[follow instructions](https://huggingface.co/spaces/safetensors/convert) to
convert it.

After that, add the model file to the
[`mlx_lm/models`](https://github.com/ml-explore/mlx-examples/tree/main/llms/mlx_lm/models)
directory. You can see other examples there. We recommend starting from a model
that is similar to the model you are porting.

Make sure the name of the new model file is the same as the `model_type` in the
`config.json`, for example
[starcoder2](https://huggingface.co/bigcode/starcoder2-7b/blob/main/config.json#L17).

To determine the model layer names, we suggest either:

- Refer to the Transformers implementation if you are familiar with the
  codebase.
- Load the model weights and check the weight names which will tell you about
  the model structure.
- Look at the names of the weights by inspecting `model.safetensors.index.json`
  in the Hugging Face repo.

To add LoRA support edit
[`mlx_lm/tuner/utils.py`](https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/tuner/utils.py#L27-L60)

Finally, add a test for the new modle type to the [model
tests](https://github.com/ml-explore/mlx-examples/blob/main/llms/tests/test_models.py).

From the `llms/` directory, you can run the tests with:

```shell
python -m unittest discover tests/
```

>>>> llms/gguf_llm/utils.py
import sentencepiece as spm
import sentencepiece.sentencepiece_model_pb2 as model


def spm_tokenizer(metadata):
    tokens = metadata["tokenizer.ggml.tokens"]
    bos = metadata["tokenizer.ggml.bos_token_id"].item()
    eos = metadata["tokenizer.ggml.eos_token_id"].item()
    unk = metadata["tokenizer.ggml.unknown_token_id"].item()

    normalizer_spec = model.NormalizerSpec(
        name="identity",
        precompiled_charsmap=b"",
        add_dummy_prefix=True,
        remove_extra_whitespaces=False,
        normalization_rule_tsv=b"",
    )
    trainer_spec = model.TrainerSpec(
        model_type="BPE",
        vocab_size=len(tokens),
        input_format="text",
        split_by_unicode_script=True,
        split_by_whitespace=True,
        split_by_number=True,
        treat_whitespace_as_suffix=False,
        split_digits=True,
        allow_whitespace_only_pieces=True,
        vocabulary_output_piece_score=True,
        byte_fallback=True,
        unk_id=unk,
        bos_id=bos,
        eos_id=eos,
        pad_id=-1,
        unk_piece="<unk>",
        bos_piece="<s>",
        eos_piece="</s>",
        pad_piece="<pad>",
        pretokenization_delimiter="",
    )
    m = model.ModelProto(trainer_spec=trainer_spec, normalizer_spec=normalizer_spec)
    scores = metadata.get("tokenizer.ggml.scores", None)
    scores = scores.tolist() if scores is not None else None
    token_types = metadata.get("tokenizer.ggml.token_type", None)
    token_types = token_types.tolist() if token_types is not None else None

    for i, token in enumerate(tokens):
        score = scores[i] if scores else 0
        token_type = token_types[i] if token_types else 0
        m.pieces.append(
            model.ModelProto.SentencePiece(piece=token, score=score, type=token_type)
        )
    tokenizer = spm.SentencePieceProcessor(model_proto=m.SerializeToString())
    return tokenizer

>>>> llms/gguf_llm/README.md
# LLMs in MLX with GGUF

An example generating text using GGUF format models in MLX.[^1]

> [!NOTE]
> MLX is able to read most quantization formats from GGUF directly. However,
> only a few quantizations are supported directly: `Q4_0`, `Q4_1`, and `Q8_0`.
> Unsupported quantizations will be cast to `float16`.

## Setup

Install the dependencies:

```bash
pip install -r requirements.txt
```

### Run

Run with:

```bash
python generate.py 
  --repo <hugging_face_repo> 
  --gguf <file.gguf> 
  --prompt "Write a quicksort in Python"
```

For example, to generate text with Mistral 7B use:

```bash
python generate.py 
  --repo TheBloke/Mistral-7B-v0.1-GGUF 
  --gguf mistral-7b-v0.1.Q8_0.gguf 
  --prompt "Write a quicksort in Python"
```

Run `python generate.py --help` for more options.

Models that have been tested and work include:

- [TheBloke/Mistral-7B-v0.1-GGUF](https://huggingface.co/TheBloke/Mistral-7B-v0.1-GGUF),
  for quantized models use:
  - `mistral-7b-v0.1.Q8_0.gguf`
  - `mistral-7b-v0.1.Q4_0.gguf`

- [TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF](https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF),
  for quantized models use:
  - `tinyllama-1.1b-chat-v1.0.Q8_0.gguf`
  - `tinyllama-1.1b-chat-v1.0.Q4_0.gguf`

- [Jaward/phi-3-mini-4k-instruct.Q4_0.gguf](https://huggingface.co/Jaward/phi-3-mini-4k-instruct.Q4_0.gguf),
  for 4 bits quantized phi-3-mini-4k-instruct use:
  - `phi-3-mini-4k-instruct.Q4_0.gguf` 

[^1]: For more information on GGUF see [the documentation](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md).

>>>> llms/gguf_llm/generate.py
# Copyright © 2023 Apple Inc.

import argparse
import time

import mlx.core as mx
import models


def generate(
    model: models.Model,
    tokenizer: models.GGUFTokenizer,
    prompt: str,
    max_tokens: int,
    temp: float = 0.0,
):
    prompt = tokenizer.encode(prompt)

    tic = time.time()
    tokens = []
    skip = 0
    for token, n in zip(
        models.generate(prompt, model, args.temp),
        range(args.max_tokens),
    ):
        if token == tokenizer.eos_token_id:
            break

        if n == 0:
            prompt_time = time.time() - tic
            tic = time.time()

        tokens.append(token.item())
        s = tokenizer.decode(tokens)
        print(s[skip:], end="", flush=True)
        skip = len(s)
    print(tokenizer.decode(tokens)[skip:], flush=True)
    gen_time = time.time() - tic
    print("=" * 10)
    if len(tokens) == 0:
        print("No tokens generated for this prompt")
        return
    prompt_tps = prompt.size / prompt_time
    gen_tps = (len(tokens) - 1) / gen_time
    print(f"Prompt: {prompt_tps:.3f} tokens-per-sec")
    print(f"Generation: {gen_tps:.3f} tokens-per-sec")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Inference script")
    parser.add_argument(
        "--gguf",
        type=str,
        help="The GGUF file to load (and optionally download).",
    )
    parser.add_argument(
        "--repo",
        type=str,
        default=None,
        help="The Hugging Face repo if downloading from the Hub.",
    )

    parser.add_argument(
        "--prompt",
        help="The message to be processed by the model",
        default="In the beginning the Universe was created.",
    )
    parser.add_argument(
        "--max-tokens",
        "-m",
        type=int,
        default=100,
        help="Maximum number of tokens to generate",
    )
    parser.add_argument(
        "--temp",
        help="The sampling temperature.",
        type=float,
        default=0.0,
    )
    parser.add_argument("--seed", type=int, default=0, help="The PRNG seed")

    args = parser.parse_args()
    mx.random.seed(args.seed)
    model, tokenizer = models.load(args.gguf, args.repo)
    generate(model, tokenizer, args.prompt, args.max_tokens, args.temp)

>>>> llms/gguf_llm/models.py
# Copyright © 2023 Apple Inc.

from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union

import mlx.core as mx
import mlx.nn as nn
import utils
from huggingface_hub import snapshot_download


@dataclass
class ModelArgs:
    hidden_size: int
    num_hidden_layers: int
    intermediate_size: int
    num_attention_heads: int
    rms_norm_eps: float
    vocab_size: int
    context_length: int
    num_key_value_heads: int = None
    rope_theta: float = 10000
    rope_traditional: bool = False
    model_type: str = None
    rope_scaling: Optional[Dict[str, Union[float, str]]] = None

    def __post_init__(self):
        if self.num_key_value_heads is None:
            self.num_key_value_heads = self.num_attention_heads

        if self.rope_scaling:
            required_keys = {"factor", "type"}
            if not all(key in self.rope_scaling for key in required_keys):
                raise ValueError(f"rope_scaling must contain keys {required_keys}")

            if self.rope_scaling["type"] != "linear":
                raise ValueError("rope_scaling 'type' currently only supports 'linear'")

    @classmethod
    def from_dict(cls, params):
        return cls(
            **{
                k: v
                for k, v in params.items()
                if k in inspect.signature(cls).parameters
            }
        )


class Attention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        dim = args.hidden_size
        self.n_heads = n_heads = args.num_attention_heads
        self.n_kv_heads = n_kv_heads = args.num_key_value_heads

        self.repeats = n_heads // n_kv_heads

        head_dim = args.hidden_size // n_heads
        self.scale = head_dim**-0.5

        self.q_proj = nn.Linear(dim, n_heads * head_dim, bias=False)
        self.k_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=False)
        self.v_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=False)
        self.o_proj = nn.Linear(n_heads * head_dim, dim, bias=False)
        rope_scale = (
            1 / args.rope_scaling["factor"]
            if args.rope_scaling is not None and args.rope_scaling["type"] == "linear"
            else 1
        )
        self.rope = nn.RoPE(
            head_dim,
            traditional=args.rope_traditional,
            base=args.rope_theta,
            scale=rope_scale,
        )

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Tuple[mx.array, mx.array]] = None,
    ) -> mx.array:
        B, L, D = x.shape

        queries, keys, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)

        # Prepare the queries, keys and values for the attention computation
        queries = queries.reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)
        keys = keys.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)
        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)

        if cache is not None:
            key_cache, value_cache = cache
            queries = self.rope(queries, offset=key_cache.shape[2])
            keys = self.rope(keys, offset=key_cache.shape[2])
            keys = mx.concatenate([key_cache, keys], axis=2)
            values = mx.concatenate([value_cache, values], axis=2)
        else:
            queries = self.rope(queries)
            keys = self.rope(keys)

        output = mx.fast.scaled_dot_product_attention(
            queries, keys, values, scale=self.scale, mask=mask
        )
        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)
        return self.o_proj(output), (keys, values)


class MLP(nn.Module):
    def __init__(self, dim, hidden_dim):
        super().__init__()
        self.gate_proj = nn.Linear(dim, hidden_dim, bias=False)
        self.down_proj = nn.Linear(hidden_dim, dim, bias=False)
        self.up_proj = nn.Linear(dim, hidden_dim, bias=False)

    def __call__(self, x) -> mx.array:
        return self.down_proj(nn.silu(self.gate_proj(x)) * self.up_proj(x))


class TransformerBlock(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.num_attention_heads = args.num_attention_heads
        self.hidden_size = args.hidden_size
        self.self_attn = Attention(args)
        self.mlp = MLP(args.hidden_size, args.intermediate_size)
        self.input_layernorm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)
        self.post_attention_layernorm = nn.RMSNorm(
            args.hidden_size, eps=args.rms_norm_eps
        )
        self.args = args

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Tuple[mx.array, mx.array]] = None,
    ) -> mx.array:
        r, cache = self.self_attn(self.input_layernorm(x), mask, cache)
        h = x + r
        r = self.mlp(self.post_attention_layernorm(h))
        out = h + r
        return out, cache


class LlamaModel(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.vocab_size = args.vocab_size
        self.num_hidden_layers = args.num_hidden_layers
        assert self.vocab_size > 0
        self.embed_tokens = nn.Embedding(args.vocab_size, args.hidden_size)
        self.layers = [
            TransformerBlock(args=args) for _ in range(args.num_hidden_layers)
        ]
        self.norm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)
        # model info
        print(
            f"Model info\n"
            f"==========\n"
            f"Context length: {args.context_length}\n"
            f"Vocab size: {args.vocab_size}\n"
            f"Hidden size: {args.hidden_size}\n"
            f"Num layers: {args.num_hidden_layers}\n"
            f"Num attention heads: {args.num_attention_heads}\n"
        )

    def __call__(
        self,
        inputs: mx.array,
        cache=None,
    ):
        h = self.embed_tokens(inputs)

        mask = None
        if h.shape[1] > 1:
            mask = nn.MultiHeadAttention.create_additive_causal_mask(h.shape[1])
            mask = mask.astype(h.dtype)

        if cache is None:
            cache = [None] * len(self.layers)

        for e, layer in enumerate(self.layers):
            h, cache[e] = layer(h, mask, cache[e])

        return self.norm(h), cache


class Model(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.model = LlamaModel(args)
        self.lm_head = nn.Linear(args.hidden_size, args.vocab_size, bias=False)

    def __call__(
        self,
        inputs: mx.array,
        cache=None,
    ):
        out, cache = self.model(inputs, cache)
        return self.lm_head(out), cache


def get_config(metadata: dict):
    output = {
        "context_length": metadata["llama.context_length"],
        "hidden_size": metadata["llama.embedding_length"],
        "num_hidden_layers": metadata["llama.block_count"],
        "num_attention_heads": metadata["llama.attention.head_count"],
        "intermediate_size": metadata["llama.feed_forward_length"],
        "num_key_value_heads": metadata["llama.attention.head_count_kv"],
        "rms_norm_eps": metadata["llama.attention.layer_norm_rms_epsilon"],
        "vocab_size": len(metadata["tokenizer.ggml.tokens"]),
        "rope_theta": metadata["llama.rope.freq_base"],
        "rope_traditional": True,
    }
    output = {k: v.item() if isinstance(v, mx.array) else v for k, v in output.items()}
    return output


class GGUFTokenizer:
    def __init__(self, metadata):
        self._tokenizer = utils.spm_tokenizer(metadata)

    def encode(self, s: str) -> mx.array:
        return mx.array([self._tokenizer.bos_id()] + self._tokenizer.encode(s))

    @property
    def eos_token_id(self):
        return self._tokenizer.eos_id()

    def decode(self, toks: List[int]) -> str:
        return self._tokenizer.decode(toks)


def translate_weight_names(name):
    name = name.replace("blk.", "model.layers.")
    name = name.replace("ffn_gate", "mlp.gate_proj")
    name = name.replace("ffn_down", "mlp.down_proj")
    name = name.replace("ffn_up", "mlp.up_proj")
    name = name.replace("attn_q", "self_attn.q_proj")
    name = name.replace("attn_k", "self_attn.k_proj")
    name = name.replace("attn_v", "self_attn.v_proj")
    name = name.replace("attn_output", "self_attn.o_proj")
    name = name.replace("attn_norm", "input_layernorm")
    name = name.replace("ffn_norm", "post_attention_layernorm")
    name = name.replace("token_embd", "model.embed_tokens")
    name = name.replace("output_norm", "model.norm")
    name = name.replace("output", "lm_head")
    return name


def load(gguf_file: str, repo: str = None):
    # If the gguf_file exists, try to load model from it.
    # Otherwise try to download and cache from the HF repo
    if not Path(gguf_file).exists():
        if repo is None:
            raise ValueError(
                f"Could not find file {gguf_file}, and no Hugging Face"
                " repo provided for download."
            )
        model_path = snapshot_download(
            repo_id=repo,
            allow_patterns=[gguf_file],
        )
        if not (Path(model_path) / gguf_file).exists():
            raise ValueError(f"File {gguf_file} not in repo {repo}.")
        gguf_file = str(Path(model_path) / gguf_file)

    print(f"[INFO] Loading model from {gguf_file}")
    weights, metadata = mx.load(gguf_file, return_metadata=True)
    gguf_ft = metadata["general.file_type"]
    if gguf_ft == 0 or gguf_ft == 1:
        # ALL_F32 or MOSTLY_F16
        quantization = None
        pass
    elif gguf_ft == 2 or gguf_ft == 3:
        # MOSTLY_Q4_0 or MOSTLY_Q4_1
        quantization = {"group_size": 32, "bits": 4}
        # print bits value
        print(f"{quantization['bits']} bits quantized model")
    elif gguf_ft == 7:
        # MOSTLY_Q8_0 = 7
        quantization = {"group_size": 32, "bits": 8}
        print(f"{quantization['bits']} bits quantized model")
    else:
        quantization = None
        print("[WARNING] Using unsupported GGUF quantization. Casting to float16.")

    weights = {translate_weight_names(k): v for k, v in weights.items()}
    config = get_config(metadata)
    model = Model(ModelArgs(**config))
    if quantization is not None:
        class_predicate = (
            lambda p, m: isinstance(m, (nn.Linear, nn.Embedding))
            and f"{p}.scales" in weights
        )
        nn.quantize(
            model,
            **quantization,
            class_predicate=class_predicate,
        )

    tokenizer = GGUFTokenizer(metadata)
    model.load_weights(list(weights.items()))
    return model, tokenizer


def generate(prompt: mx.array, model: Model, temp: float = 0.0):
    def sample(logits):
        if temp == 0:
            return mx.argmax(logits, axis=-1)
        else:
            return mx.random.categorical(logits * (1 / temp))

    y = prompt
    cache = None
    while True:
        logits, cache = model(y[None], cache=cache)
        logits = logits[:, -1, :]
        y = sample(logits)
        yield y

>>>> llms/setup.py
# Copyright © 2024 Apple Inc.

import sys
from pathlib import Path

from setuptools import setup

package_dir = Path(__file__).parent / "mlx_lm"
with open(package_dir / "requirements.txt") as fid:
    requirements = [l.strip() for l in fid.readlines()]

sys.path.append(str(package_dir))
from _version import __version__

setup(
    name="mlx-lm",
    version=__version__,
    description="LLMs on Apple silicon with MLX and the Hugging Face Hub",
    long_description=open("README.md", encoding="utf-8").read(),
    long_description_content_type="text/markdown",
    readme="README.md",
    author_email="mlx@group.apple.com",
    author="MLX Contributors",
    url="https://github.com/ml-explore/mlx-examples",
    license="MIT",
    install_requires=requirements,
    packages=["mlx_lm", "mlx_lm.models", "mlx_lm.tuner"],
    python_requires=">=3.8",
    extras_require={
        "test": ["datasets"],
        "evaluate": ["lm-eval", "tqdm"],
    },
    entry_points={
        "console_scripts": [
            "mlx_lm.cache_prompt = mlx_lm.cache_prompt:main",
            "mlx_lm.chat = mlx_lm.chat:main",
            "mlx_lm.convert = mlx_lm.convert:main",
            "mlx_lm.evaluate = mlx_lm.evaluate:main",
            "mlx_lm.fuse = mlx_lm.fuse:main",
            "mlx_lm.generate = mlx_lm.generate:main",
            "mlx_lm.lora = mlx_lm.lora:main",
            "mlx_lm.merge = mlx_lm.merge:main",
            "mlx_lm.server = mlx_lm.server:main",
            "mlx_lm.manage = mlx_lm.manage:main",
        ]
    },
)

>>>> llms/README.md
## Generate Text with LLMs and MLX

The easiest way to get started is to install the `mlx-lm` package:

**With `pip`**:

```sh
pip install mlx-lm
```

**With `conda`**:

```sh
conda install -c conda-forge mlx-lm
```

The `mlx-lm` package also has:

- [LoRA, QLoRA, and full fine-tuning](https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/LORA.md)
- [Merging models](https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/MERGE.md)
- [HTTP model serving](https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/SERVER.md)

### Quick Start

To generate text with an LLM use:

```bash
mlx_lm.generate --prompt "Hi!"
```

To chat with an LLM use:

```bash
mlx_lm.chat
```

This will give you a chat REPL that you can use to interact with the LLM. The
chat context is preserved during the lifetime of the REPL.

Commands in `mlx-lm` typically take command line options which let you specify
the model, sampling parameters, and more. Use `-h` to see a list of available
options for a command, e.g.:

```bash
mlx_lm.generate -h
```

### Python API

You can use `mlx-lm` as a module:

```python
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

prompt = "Write a story about Einstein"

messages = [{"role": "user", "content": prompt}]
prompt = tokenizer.apply_chat_template(
    messages, add_generation_prompt=True
)

text = generate(model, tokenizer, prompt=prompt, verbose=True)
```

To see a description of all the arguments you can do:

```
>>> help(generate)
```

Check out the [generation
example](https://github.com/ml-explore/mlx-examples/tree/main/llms/mlx_lm/examples/generate_response.py)
to see how to use the API in more detail.

The `mlx-lm` package also comes with functionality to quantize and optionally
upload models to the Hugging Face Hub.

You can convert models using the Python API:

```python
from mlx_lm import convert

repo = "mistralai/Mistral-7B-Instruct-v0.3"
upload_repo = "mlx-community/My-Mistral-7B-Instruct-v0.3-4bit"

convert(repo, quantize=True, upload_repo=upload_repo)
```

This will generate a 4-bit quantized Mistral 7B and upload it to the repo
`mlx-community/My-Mistral-7B-Instruct-v0.3-4bit`. It will also save the
converted model in the path `mlx_model` by default.

To see a description of all the arguments you can do:

```
>>> help(convert)
```

#### Streaming

For streaming generation, use the `stream_generate` function. This yields
a generation response object.

For example,

```python
from mlx_lm import load, stream_generate

repo = "mlx-community/Mistral-7B-Instruct-v0.3-4bit"
model, tokenizer = load(repo)

prompt = "Write a story about Einstein"

messages = [{"role": "user", "content": prompt}]
prompt = tokenizer.apply_chat_template(
    messages, add_generation_prompt=True
)

for response in stream_generate(model, tokenizer, prompt, max_tokens=512):
    print(response.text, end="", flush=True)
print()
```

#### Sampling

The `generate` and `stream_generate` functions accept `sampler` and
`logits_processors` keyword arguments. A sampler is any callable which accepts
a possibly batched logits array and returns an array of sampled tokens.  The
`logits_processors` must be a list of callables which take the token history
and current logits as input and return the processed logits. The logits
processors are applied in order.

Some standard sampling functions and logits processors are provided in
`mlx_lm.sample_utils`.

### Command Line

You can also use `mlx-lm` from the command line with:

```
mlx_lm.generate --model mistralai/Mistral-7B-Instruct-v0.3 --prompt "hello"
```

This will download a Mistral 7B model from the Hugging Face Hub and generate
text using the given prompt.

For a full list of options run:

```
mlx_lm.generate --help
```

To quantize a model from the command line run:

```
mlx_lm.convert --hf-path mistralai/Mistral-7B-Instruct-v0.3 -q
```

For more options run:

```
mlx_lm.convert --help
```

You can upload new models to Hugging Face by specifying `--upload-repo` to
`convert`. For example, to upload a quantized Mistral-7B model to the
[MLX Hugging Face community](https://huggingface.co/mlx-community) you can do:

```
mlx_lm.convert 
    --hf-path mistralai/Mistral-7B-Instruct-v0.3 
    -q 
    --upload-repo mlx-community/my-4bit-mistral
```

Models can also be converted and quantized directly in the
[mlx-my-repo](https://huggingface.co/spaces/mlx-community/mlx-my-repo) Hugging
Face Space.

### Long Prompts and Generations 

`mlx-lm` has some tools to scale efficiently to long prompts and generations:

- A rotating fixed-size key-value cache.
- Prompt caching

To use the rotating key-value cache pass the argument `--max-kv-size n` where
`n` can be any integer. Smaller values like `512` will use very little RAM but
result in worse quality. Larger values like `4096` or higher will use more RAM
but have better quality.

Caching prompts can substantially speedup reusing the same long context with
different queries. To cache a prompt use `mlx_lm.cache_prompt`. For example:

```bash
cat prompt.txt | mlx_lm.cache_prompt 
  --model mistralai/Mistral-7B-Instruct-v0.3 
  --prompt - 
  --prompt-cache-file mistral_prompt.safetensors
``` 

Then use the cached prompt with `mlx_lm.generate`:

```
mlx_lm.generate 
    --prompt-cache-file mistral_prompt.safetensors 
    --prompt "\nSummarize the above text."
```

The cached prompt is treated as a prefix to the supplied prompt. Also notice
when using a cached prompt, the model to use is read from the cache and need
not be supplied explicitly.

Prompt caching can also be used in the Python API in order to to avoid
recomputing the prompt. This is useful in multi-turn dialogues or across
requests that use the same context. See the
[example](https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/examples/chat.py)
for more usage details.

### Supported Models

`mlx-lm` supports thousands of Hugging Face format LLMs. If the model you want to
run is not supported, file an
[issue](https://github.com/ml-explore/mlx-examples/issues/new) or better yet,
submit a pull request.

Here are a few examples of Hugging Face models that work with this example:

- [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)
- [meta-llama/Llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf)
- [deepseek-ai/deepseek-coder-6.7b-instruct](https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-instruct)
- [01-ai/Yi-6B-Chat](https://huggingface.co/01-ai/Yi-6B-Chat)
- [microsoft/phi-2](https://huggingface.co/microsoft/phi-2)
- [mistralai/Mixtral-8x7B-Instruct-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)
- [Qwen/Qwen-7B](https://huggingface.co/Qwen/Qwen-7B)
- [pfnet/plamo-13b](https://huggingface.co/pfnet/plamo-13b)
- [pfnet/plamo-13b-instruct](https://huggingface.co/pfnet/plamo-13b-instruct)
- [stabilityai/stablelm-2-zephyr-1_6b](https://huggingface.co/stabilityai/stablelm-2-zephyr-1_6b)
- [internlm/internlm2-7b](https://huggingface.co/internlm/internlm2-7b)
- [tiiuae/falcon-mamba-7b-instruct](https://huggingface.co/tiiuae/falcon-mamba-7b-instruct)

Most
[Mistral](https://huggingface.co/models?library=transformers,safetensors&other=mistral&sort=trending),
[Llama](https://huggingface.co/models?library=transformers,safetensors&other=llama&sort=trending),
[Phi-2](https://huggingface.co/models?library=transformers,safetensors&other=phi&sort=trending),
and
[Mixtral](https://huggingface.co/models?library=transformers,safetensors&other=mixtral&sort=trending)
style models should work out of the box.

For some models (such as `Qwen` and `plamo`) the tokenizer requires you to
enable the `trust_remote_code` option. You can do this by passing
`--trust-remote-code` in the command line. If you don't specify the flag
explicitly, you will be prompted to trust remote code in the terminal when
running the model. 

For `Qwen` models you must also specify the `eos_token`. You can do this by
passing `--eos-token "<|endoftext|>"` in the command
line. 

These options can also be set in the Python API. For example:

```python
model, tokenizer = load(
    "qwen/Qwen-7B",
    tokenizer_config={"eos_token": "<|endoftext|>", "trust_remote_code": True},
)
```

### Large Models

> [!NOTE]
    This requires macOS 15.0 or higher to work.

Models which are large relative to the total RAM available on the machine can
be slow. `mlx-lm` will attempt to make them faster by wiring the memory
occupied by the model and cache. This requires macOS 15 or higher to
work.

If you see the following warning message:

> [WARNING] Generating with a model that requires ...

then the model will likely be slow on the given machine. If the model fits in
RAM then it can often be sped up by increasing the system wired memory limit.
To increase the limit, set the following `sysctl`:

```bash
sudo sysctl iogpu.wired_limit_mb=N
```

The value `N` should be larger than the size of the model in megabytes but
smaller than the memory size of the machine.

>>>> llms/MANIFEST.in
include mlx_lm/requirements.txt
recursive-include mlx_lm/ *.py

>>>> llms/tests/test_models.py
# Copyright © 2024 Apple Inc.
import unittest

import mlx.core as mx
import mlx.nn as nn
from mlx.utils import tree_map
from mlx_lm.models import rope_utils
from mlx_lm.models.base import create_causal_mask
from mlx_lm.models.cache import KVCache, RotatingKVCache, make_prompt_cache


class TestModels(unittest.TestCase):

    def test_kv_cache(self):
        cache = KVCache()

        k = mx.ones((1, 4, 1, 32), mx.float16)
        v = mx.ones((1, 4, 1, 32), mx.float16)

        k_up, v_up = cache.update_and_fetch(k, v)
        self.assertTrue(mx.array_equal(k_up, k))
        self.assertTrue(mx.array_equal(v_up, v))
        self.assertEqual(cache.offset, 1)

        k = mx.ones((1, 4, cache.step, 32), mx.float16)
        v = mx.ones((1, 4, cache.step, 32), mx.float16)
        k_up, v_up = cache.update_and_fetch(k, v)

        expected = mx.ones((1, 4, cache.step + 1, 32), mx.float16)
        self.assertTrue(mx.array_equal(k_up, expected))
        self.assertTrue(mx.array_equal(v_up, expected))
        self.assertEqual(cache.offset, cache.step + 1)

    def test_rotating_kv_cache(self):
        b, h, d = 1, 2, 32
        cache = RotatingKVCache(max_size=8, step=4)

        k = mx.random.uniform(shape=(b, h, 2, d))
        v = mx.random.uniform(shape=(b, h, 2, d))

        k_up, v_up = cache.update_and_fetch(k, v)
        self.assertTrue(mx.array_equal(k_up, k))
        self.assertTrue(mx.array_equal(v_up, v))
        self.assertEqual(cache.offset, 2)

        k = mx.random.uniform(shape=(b, h, 5, d))
        v = mx.random.uniform(shape=(b, h, 5, d))
        k_up, v_up = cache.update_and_fetch(k, v)
        self.assertTrue(mx.array_equal(k_up[..., 2:, :], k))
        self.assertTrue(mx.array_equal(v_up[..., 2:, :], v))

        k = mx.random.uniform(shape=(b, h, 4, d))
        v = mx.random.uniform(shape=(b, h, 4, d))
        k_up, v_up = cache.update_and_fetch(k, v)
        self.assertTrue(mx.array_equal(k_up[..., -4:, :], k))
        self.assertTrue(mx.array_equal(v_up[..., -4:, :], v))

        idx = 0
        for _ in range(10):
            k = mx.random.uniform(shape=(b, h, 1, d))
            v = mx.random.uniform(shape=(b, h, 1, d))
            k_up, v_up = cache.update_and_fetch(k, v)
            self.assertTrue(mx.array_equal(k_up[..., idx : idx + 1, :], k))
            self.assertTrue(mx.array_equal(v_up[..., idx : idx + 1, :], v))
            idx += 1
            idx %= 8

        # Try with nonzero keep
        cache = RotatingKVCache(max_size=8, step=4, keep=2)

        # Check a large update
        k = mx.random.uniform(shape=(b, h, 20, d))
        v = mx.random.uniform(shape=(b, h, 20, d))
        k_up, v_up = cache.update_and_fetch(k, v)
        self.assertTrue(mx.array_equal(k_up, k))
        self.assertTrue(mx.array_equal(v_up, v))

        # A bunch of small updates
        self.assertEqual(cache.offset, 20)
        idx = 2
        for i in range(10):
            k = mx.random.uniform(shape=(b, h, 1, d))
            v = mx.random.uniform(shape=(b, h, 1, d))
            k_up, v_up = cache.update_and_fetch(k, v)
            self.assertTrue(mx.array_equal(k_up[..., idx : idx + 1, :], k))
            self.assertTrue(mx.array_equal(v_up[..., idx : idx + 1, :], v))
            self.assertEqual(cache.offset, 21 + i)
            idx += 1
            if idx >= 8:
                idx = 2

    def test_rotating_kv_cache_chat_mode(self):
        # Test that the rotating kv cache can handle
        # alternating prompt/prefill with generation
        d = 4
        h = 2
        cache = RotatingKVCache(max_size=18, step=4)

        x = mx.random.uniform(shape=(1, h, 8, d))
        k, v = cache.update_and_fetch(x, x)
        self.assertEqual(k.shape[2], 8)
        self.assertEqual(cache.offset, 8)

        x = mx.random.uniform(shape=(1, h, 1, d))
        k, v = cache.update_and_fetch(x, x)
        self.assertEqual(k.shape[2], 9)
        self.assertEqual(cache.offset, 9)
        self.assertTrue(mx.allclose(x, k[..., 8:9, :]))

        x = mx.random.uniform(shape=(1, h, 2, d))
        k, v = cache.update_and_fetch(x, x)
        self.assertEqual(k.shape[2], 11)
        self.assertEqual(cache.offset, 11)
        self.assertTrue(mx.allclose(x, k[..., 9:11, :]))

        x = mx.random.uniform(shape=(1, h, 3, d))
        k, v = cache.update_and_fetch(x, x)
        self.assertEqual(k.shape[2], 14)
        self.assertEqual(cache.offset, 14)
        self.assertTrue(mx.allclose(x, k[..., 11:14, :]))

        x = mx.random.uniform(shape=(1, h, 6, d))
        k, v = cache.update_and_fetch(x, x)
        self.assertEqual(cache.offset, 20)
        self.assertTrue(mx.allclose(x, k[..., -6:, :]))

        x = mx.random.uniform(shape=(1, h, 2, d))
        k, v = cache.update_and_fetch(x, x)
        self.assertEqual(cache.offset, 22)
        self.assertTrue(mx.allclose(x, k[..., -2:, :]))

    def test_causal_mask_lengths(self):
        mx.random.seed(8)
        B, N_q, T_q, N_kv, T_kv, D = (4, 8, 3, 2, 3, 2)
        lengths = mx.array([1, 2, 3, 1])
        q = mx.random.uniform(shape=(B, N_q, T_q, D))
        k = mx.random.uniform(shape=(B, N_kv, T_kv, D))
        v = k
        mask = create_causal_mask(T_q, 0, lengths=lengths)

        out1 = mx.fast.scaled_dot_product_attention(q, k, v, scale=1.0, mask=mask)
        q[1, :, 2:] = mx.ones_like(q[1, :, 2:])
        k[1, :, 2:] = mx.ones_like(k[1, :, 2:])
        v[1, :, 2:] = mx.ones_like(v[1, :, 2:])
        out2 = mx.fast.scaled_dot_product_attention(q, k, v, scale=1.0, mask=mask)
        self.assertTrue(mx.allclose(out1[1, :, :2], out2[1, :, :2]))

    def test_rope(self):
        rope = rope_utils.initialize_rope(32, base=100, traditional=False)
        self.assertTrue(isinstance(rope, nn.RoPE))

        rope = rope_utils.initialize_rope(
            32,
            base=100,
            traditional=False,
            scaling_config={"rope_type": "linear", "factor": 10.0},
        )
        self.assertTrue(isinstance(rope, nn.RoPE))

        rope = rope_utils.initialize_rope(
            32,
            base=100,
            traditional=False,
            scaling_config={"rope_type": "llama3", "factor": 2.0},
        )
        self.assertTrue(isinstance(rope, rope_utils.Llama3RoPE))

    def model_test_runner(self, model, model_type, vocab_size, num_layers):

        self.assertEqual(len(model.layers), num_layers)
        self.assertEqual(model.model_type, model_type)

        for t in [mx.float32, mx.float16]:
            model.update(tree_map(lambda p: p.astype(t), model.parameters()))

            inputs = mx.array([[0, 1]])
            outputs = model(inputs)
            self.assertEqual(outputs.shape, (1, 2, vocab_size))
            self.assertEqual(outputs.dtype, t)

            cache = make_prompt_cache(model)
            outputs = model(inputs, cache=cache)
            self.assertEqual(outputs.shape, (1, 2, vocab_size))
            self.assertEqual(outputs.dtype, t)

            if model_type not in ("mamba", "plamo2"):
                mask = create_causal_mask(inputs.shape[1], 0).astype(t)
                outputs = model(inputs, mask=mask)
                self.assertEqual(outputs.shape, (1, 2, vocab_size))
                self.assertEqual(outputs.dtype, t)

            outputs = model(mx.argmax(outputs[0, -1:, :], keepdims=True), cache=cache)
            self.assertEqual(outputs.shape, (1, 1, vocab_size))
            self.assertEqual(outputs.dtype, t)

    def test_llama(self):
        from mlx_lm.models import llama

        args = llama.ModelArgs(
            model_type="llama",
            hidden_size=1024,
            num_hidden_layers=4,
            intermediate_size=2048,
            num_attention_heads=4,
            rms_norm_eps=1e-5,
            vocab_size=10_000,
        )
        model = llama.Model(args)
        self.model_test_runner(
            model, args.model_type, args.vocab_size, args.num_hidden_layers
        )

    def test_phi2(self):
        from mlx_lm.models import phi

        args = phi.ModelArgs()
        model = phi.Model(args)
        self.model_test_runner(
            model, args.model_type, args.vocab_size, args.num_hidden_layers
        )

    def test_phixtral(self):
        from mlx_lm.models import phixtral

        args = phixtral.ModelArgs(
            "phixtral", num_vocab=1000, num_layers=4, model_dim=1024
        )
        model = phixtral.Model(args)
        self.model_test_runner(model, args.model_type, args.num_vocab, args.num_layers)

    def test_phi3(self):
        from mlx_lm.models import phi3

        args = phi3.ModelArgs(
            model_type="phi3",
            hidden_size=3072,
            num_hidden_layers=32,
            intermediate_size=8192,
            num_attention_heads=32,
            rms_norm_eps=1e-5,
            vocab_size=32064,
        )
        model = phi3.Model(args)
        self.model_test_runner(
            model, args.model_type, args.vocab_size, args.num_hidden_layers
        )

    def test_gemma(self):
        from mlx_lm.models import gemma

        args = gemma.ModelArgs(
            model_type="gemma",
            hidden_size=1024,
            num_hidden_layers=4,
            intermediate_size=2048,
            num_attention_heads=4,
            head_dim=128,
            rms_norm_eps=1e-5,
            vocab_size=10_000,
            num_key_value_heads=4,
        )
        model = gemma.Model(args)
        self.model_test_runner(
            model, args.model_type, args.vocab_size, args.num_hidden_layers
        )

    def test_mixtral(self):
        from mlx_lm.models import mixtral

        # Make a baby mixtral, because it will actually do the
        # eval
        args = mixtral.ModelArgs(
            model_type="mixtral",
            vocab_size=100,
            hidden_size=32,
            intermediate_size=128,
            num_hidden_layers=2,
            num_attention_heads=4,
            num_experts_per_tok=2,
            num_key_value_heads=2,
            num_local_experts=4,
        )
        model = mixtral.Model(args)
        self.model_test_runner(
            model, args.model_type, args.vocab_size, args.num_hidden_layers
        )

    @unittest.skip("requires ai2-olmo")
    def test_olmo(self):
        from mlx_lm.models import olmo

        args = olmo.ModelArgs(
            model_type="olmo",
            d_model=1024,
            n_layers=4,
            mlp_hidden_size=2048,
            n_heads=2,
            vocab_size=10_000,
            embedding_size=10_000,
        )
        model = olmo.Model(args)
        self.model_test_runner(
            model,
            args.model_type,
            args.vocab_size,
            args.n_layers,
        )

    def test_qwen2_moe(self):
        from mlx_lm.models import qwen2_moe

        args = qwen2_moe.ModelArgs(
            model_type="qwen2_moe",
            hidden_size=1024,
            num_hidden_layers=4,
            intermediate_size=2048,
            num_attention_heads=4,
            rms_norm_eps=1e-5,
            vocab_size=10_000,
            num_experts_per_tok=4,
            num_experts=16,
            moe_intermediate_size=1024,
            shared_expert_intermediate_size=2048,
        )
        model = qwen2_moe.Model(args)
        self.model_test_runner(
            model, args.model_type, args.vocab_size, args.num_hidden_layers
        )

    def test_qwen2(self):
        from mlx_lm.models import qwen2

        args = qwen2.ModelArgs(
            model_type="qwen2",
            hidden_size=1024,
            num_hidden_layers=4,
            intermediate_size=2048,
            num_attention_heads=4,
            rms_norm_eps=1e-5,
            vocab_size=10_000,
        )
        model = qwen2.Model(args)
        self.model_test_runner(
            model, args.model_type, args.vocab_size, args.num_hidden_layers
        )

    def test_qwen(self):
        from mlx_lm.models import qwen

        args = qwen.ModelArgs(
            model_type="qwen",
        )
        model = qwen.Model(args)
        self.model_test_runner(
            model, args.model_type, args.vocab_size, args.num_hidden_layers
        )

    def test_plamo(self):
        from mlx_lm.models import plamo

        args = plamo.ModelArgs(
            model_type="plamo",
            hidden_size=1024,
            num_hidden_layers=4,
            intermediate_size=2048,
            num_attention_heads=8,
            rms_norm_eps=1e-5,
            vocab_size=10_000,
        )
        model = plamo.Model(args)
        self.model_test_runner(
            model, args.model_type, args.vocab_size, args.num_hidden_layers
        )

    def test_plamo2(self):
        from mlx_lm.models import plamo2

        args = plamo2.ModelArgs(
            model_type="plamo2",
            hidden_size=1024,
            num_hidden_layers=4,
            intermediate_size=2048,
            num_attention_heads=8,
            rms_norm_eps=1e-5,
            vocab_size=10_000,
        )
        model = plamo2.Model(args)
        self.model_test_runner(
            model, args.model_type, args.vocab_size, args.num_hidden_layers
        )

    def test_stablelm(self):
        from mlx_lm.models import stablelm

        args = stablelm.ModelArgs(
            model_type="stablelm",
            vocab_size=10_000,
            hidden_size=1024,
            num_attention_heads=4,
            num_hidden_layers=4,
            num_key_value_heads=2,
            partial_rotary_factor=1.0,
            intermediate_size=2048,
            layer_norm_eps=1e-2,
            rope_theta=10_000,
            use_qkv_bias=False,
        )
        model = stablelm.Model(args)
        self.model_test_runner(
            model, args.model_type, args.vocab_size, args.num_hidden_layers
        )

        # StableLM 2
        args = stablelm.ModelArgs(
            model_type="stablelm",
            vocab_size=10000,
            hidden_size=512,
            num_attention_heads=8,
            num_hidden_layers=4,
            num_key_value_heads=2,
            partial_rotary_factor=0.25,
            intermediate_size=1024,
            layer_norm_eps=1e-5,
            rope_theta=10000,
            use_qkv_bias=True,
        )
        model = stablelm.Model(args)
        self.model_test_runner(
            model, args.model_type, args.vocab_size, args.num_hidden_layers
        )

    def test_starcoder2(self):
        from mlx_lm.models import starcoder2

        args = starcoder2.ModelArgs(
            model_type="starcoder2",
            hidden_size=1024,
            num_hidden_layers=4,
            intermediate_size=2048,
            num_attention_heads=4,
            num_key_value_heads=4,
        )
        model = starcoder2.Model(args)
        self.model_test_runner(
            model, args.model_type, args.vocab_size, args.num_hidden_layers
        )

    def test_cohere(self):
        from mlx_lm.models import cohere

        args = cohere.ModelArgs(
            model_type="cohere",
        )
        model = cohere.Model(args)
        self.model_test_runner(
            model, args.model_type, args.vocab_size, args.num_hidden_layers
        )

    def test_dbrx(self):
        from mlx_lm.models import dbrx

        args = dbrx.ModelArgs(
            model_type="dbrx",
            d_model=1024,
            ffn_config={"ffn_hidden_size": 2048, "moe_num_experts": 4, "moe_top_k": 2},
            attn_config={"kv_n_heads": 2, "clip_qkv": True, "rope_theta": 10000},
            n_layers=4,
            n_heads=4,
            vocab_size=10_000,
        )
        model = dbrx.Model(args)
        self.model_test_runner(model, args.model_type, args.vocab_size, args.n_layers)

    def test_minicpm(self):
        from mlx_lm.models import minicpm

        args = minicpm.ModelArgs(
            model_type="minicpm",
            hidden_size=1024,
            dim_model_base=1024,
            num_hidden_layers=4,
            intermediate_size=2048,
            num_attention_heads=4,
            rms_norm_eps=1e-4,
            vocab_size=10000,
            num_key_value_heads=2,
            scale_depth=1.0,
            scale_emb=1.0,
        )
        model = minicpm.Model(args)
        self.model_test_runner(
            model, args.model_type, args.vocab_size, args.num_hidden_layers
        )

    def test_mamba(self):
        from mlx_lm.models import mamba

        args = mamba.ModelArgs(
            model_type="mamba",
            vocab_size=10000,
            use_bias=False,
            use_conv_bias=True,
            conv_kernel=4,
            hidden_size=768,
            num_hidden_layers=24,
            state_size=16,
            intermediate_size=1536,
            time_step_rank=48,
        )
        model = mamba.Model(args)
        self.model_test_runner(
            model, args.model_type, args.vocab_size, args.num_hidden_layers
        )

    def test_gpt2(self):
        from mlx_lm.models import gpt2

        args = gpt2.ModelArgs(
            model_type="gpt2",
            n_ctx=1024,
            n_embd=768,
            n_head=12,
            n_layer=12,
            n_positions=1024,
            layer_norm_epsilon=1e-5,
            vocab_size=50256,
        )
        model = gpt2.Model(args)
        self.model_test_runner(model, args.model_type, args.vocab_size, args.n_layer)

    def test_gpt_neox(self):
        from mlx_lm.models import gpt_neox

        args = gpt_neox.ModelArgs(
            model_type="gpt_neox",
            max_position_embeddings=2048,
            hidden_size=6144,
            num_attention_heads=64,
            num_hidden_layers=44,
            layer_norm_eps=1e-5,
            vocab_size=50432,
            rotary_emb_base=10_000,
            rotary_pct=0.25,
        )
        model = gpt_neox.Model(args)
        self.model_test_runner(
            model, args.model_type, args.vocab_size, args.num_hidden_layers
        )

    def test_openelm(self):
        from mlx_lm.models import openelm

        args = openelm.ModelArgs(
            model_type="openelm",
            ffn_dim_divisor=256,
            ffn_multipliers=[
                0.5,
                0.73,
                0.97,
                1.2,
                1.43,
                1.67,
                1.9,
                2.13,
                2.37,
                2.6,
                2.83,
                3.07,
                3.3,
                3.53,
                3.77,
                4.0,
            ],
            head_dim=64,
            model_dim=1280,
            normalize_qk_projections=True,
            num_kv_heads=[3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5],
            num_query_heads=[
                12,
                12,
                12,
                12,
                12,
                16,
                16,
                16,
                16,
                16,
                16,
                16,
                20,
                20,
                20,
                20,
            ],
            num_transformer_layers=16,
            vocab_size=32000,
        )

        model = openelm.Model(args)
        self.model_test_runner(
            model,
            args.model_type,
            args.vocab_size,
            len(args.ffn_multipliers),
        )

    def test_internlm2(self):
        from mlx_lm.models import internlm2

        args = internlm2.ModelArgs(
            model_type="internlm2",
            hidden_size=1024,
            num_hidden_layers=4,
            intermediate_size=2048,
            num_attention_heads=4,
            rms_norm_eps=1e-5,
            vocab_size=10000,
        )
        model = internlm2.Model(args)
        self.model_test_runner(
            model, args.model_type, args.vocab_size, args.num_hidden_layers
        )

    def test_llama3_1(self):
        from mlx_lm.models import llama

        args = llama.ModelArgs(
            model_type="llama",
            hidden_size=1024,
            num_hidden_layers=4,
            intermediate_size=2048,
            num_attention_heads=4,
            rms_norm_eps=1e-5,
            vocab_size=10_000,
            max_position_embeddings=128,
            mlp_bias=False,
            num_key_value_heads=2,
            rope_scaling={
                "factor": 8.0,
                "low_freq_factor": 1.0,
                "high_freq_factor": 4.0,
                "original_max_position_embeddings": 8192,
                "rope_type": "llama3",
            },
        )
        model = llama.Model(args)
        self.model_test_runner(
            model, args.model_type, args.vocab_size, args.num_hidden_layers
        )

    def test_deepseek(self):
        from mlx_lm.models import deepseek

        args = deepseek.ModelArgs(
            model_type="deepseek",
            vocab_size=1024,
            hidden_size=128,
            intermediate_size=256,
            moe_intermediate_size=256,
            num_hidden_layers=4,
            num_attention_heads=8,
            num_key_value_heads=4,
        )
        model = deepseek.Model(args)
        self.model_test_runner(
            model, args.model_type, args.vocab_size, args.num_hidden_layers
        )

    def test_deepseek_v2(self):
        from mlx_lm.models import deepseek_v2

        args = deepseek_v2.ModelArgs(
            model_type="deepseek_v2",
            vocab_size=1024,
            hidden_size=128,
            intermediate_size=256,
            moe_intermediate_size=256,
            num_hidden_layers=4,
            num_attention_heads=4,
            num_key_value_heads=2,
            kv_lora_rank=4,
            q_lora_rank=4,
            qk_rope_head_dim=32,
            v_head_dim=16,
            qk_nope_head_dim=32,
            rope_scaling={
                "beta_fast": 32,
                "beta_slow": 1,
                "factor": 40,
                "mscale": 1.0,
                "mscale_all_dim": 1.0,
                "original_max_position_embeddings": 4096,
                "type": "yarn",
            },
        )
        model = deepseek_v2.Model(args)
        self.model_test_runner(
            model, args.model_type, args.vocab_size, args.num_hidden_layers
        )

    def test_deepseek_v3(self):
        from mlx_lm.models import deepseek_v3

        args = deepseek_v3.ModelArgs(
            model_type="deepseek_v3",
            vocab_size=1024,
            hidden_size=128,
            intermediate_size=256,
            moe_intermediate_size=256,
            num_hidden_layers=4,
            num_attention_heads=4,
            num_key_value_heads=2,
            n_routed_experts=4,
            n_group=2,
            topk_group=1,
            num_experts_per_tok=2,
            n_shared_experts=1,
            kv_lora_rank=4,
            q_lora_rank=4,
            qk_rope_head_dim=32,
            v_head_dim=16,
            qk_nope_head_dim=32,
            rope_scaling={
                "beta_fast": 32,
                "beta_slow": 1,
                "factor": 40,
                "mscale": 1.0,
                "mscale_all_dim": 1.0,
                "original_max_position_embeddings": 4096,
                "type": "yarn",
            },
        )
        model = deepseek_v3.Model(args)
        self.model_test_runner(
            model, args.model_type, args.vocab_size, args.num_hidden_layers
        )

    def test_gemma2(self):
        from mlx_lm.models import gemma2

        args = gemma2.ModelArgs(
            model_type="gemma2",
            hidden_size=128,
            num_hidden_layers=4,
            intermediate_size=256,
            num_attention_heads=2,
            head_dim=32,
            rms_norm_eps=1e-4,
            vocab_size=1024,
            num_key_value_heads=2,
        )
        model = gemma2.Model(args)
        self.model_test_runner(
            model, args.model_type, args.vocab_size, args.num_hidden_layers
        )

    def test_gpt_bigcode(self):
        from mlx_lm.models import gpt_bigcode

        args = gpt_bigcode.ModelArgs(
            model_type="gpt_bigcode",
            n_embd=128,
            n_layer=128,
            n_inner=256,
            n_head=4,
            n_positions=1000,
            layer_norm_epsilon=1e-5,
            vocab_size=1024,
        )
        model = gpt_bigcode.Model(args)
        self.model_test_runner(model, args.model_type, args.vocab_size, args.n_layer)

    def test_nemotron(self):
        from mlx_lm.models import nemotron

        args = nemotron.ModelArgs(
            model_type="nemotron",
            hidden_size=128,
            hidden_act="gelu",
            num_hidden_layers=4,
            intermediate_size=256,
            num_attention_heads=4,
            norm_eps=1e-5,
            vocab_size=1024,
            num_key_value_heads=2,
        )
        model = nemotron.Model(args)
        self.model_test_runner(
            model, args.model_type, args.vocab_size, args.num_hidden_layers
        )

    def test_phi3small(self):
        from mlx_lm.models import phi3small

        args = phi3small.ModelArgs(
            model_type="phi3small",
            hidden_size=128,
            dense_attention_every_n_layers=2,
            ff_intermediate_size=256,
            gegelu_limit=1.0,
            num_hidden_layers=4,
            num_attention_heads=4,
            num_key_value_heads=2,
            layer_norm_epsilon=1e-4,
            vocab_size=1000,
        )
        model = phi3small.Model(args)
        self.model_test_runner(
            model, args.model_type, args.vocab_size, args.num_hidden_layers
        )

    def test_phimoe(self):
        from mlx_lm.models import phimoe

        args = phimoe.ModelArgs(
            model_type="phimoe",
            vocab_size=320,
            hidden_size=128,
            intermediate_size=256,
            num_hidden_layers=4,
            num_attention_heads=4,
            num_key_value_heads=4,
            rope_scaling={
                "long_factor": [1.0] * 16,
                "long_mscale": 1.243163121016122,
                "original_max_position_embeddings": 4096,
                "short_factor": [1.0] * 16,
                "short_mscale": 1.243163121016122,
                "type": "longrope",
            },
        )
        model = phimoe.Model(args)
        self.model_test_runner(
            model, args.model_type, args.vocab_size, args.num_hidden_layers
        )

    def test_recurrent_gemma(self):
        from mlx_lm.models import recurrent_gemma

        args = recurrent_gemma.ModelArgs(
            model_type="recurrent_gemma",
            hidden_size=128,
            attention_bias=False,
            conv1d_width=3,
            intermediate_size=256,
            logits_soft_cap=1.0,
            num_attention_heads=4,
            num_hidden_layers=4,
            num_key_value_heads=2,
            rms_norm_eps=1e-4,
            rope_theta=1000,
            attention_window_size=1024,
            vocab_size=1000,
            block_types=["recurrent", "recurrent", "attention"],
        )
        model = recurrent_gemma.Model(args)
        self.model_test_runner(
            model, args.model_type, args.vocab_size, args.num_hidden_layers
        )

    def test_hunyuan(self):
        from mlx_lm.models import hunyuan

        args = hunyuan.ModelArgs(
            model_type="hunyuan",
            hidden_size=128,
            attention_bias=False,
            intermediate_size=256,
            num_attention_heads=4,
            num_hidden_layers=4,
            num_key_value_heads=2,
            rms_norm_eps=1e-4,
            rope_theta=1000,
            vocab_size=1000,
            moe_topk=2,
            num_experts=2,
            num_shared_expert=1,
            use_mixed_mlp_moe=True,
            use_qk_norm=True,
            rope_scaling={
                "alpha": 1000.0,
                "factor": 1.0,
                "type": "dynamic",
            },
            use_cla=True,
            cla_share_factor=2,
        )
        model = hunyuan.Model(args)
        self.model_test_runner(
            model, args.model_type, args.vocab_size, args.num_hidden_layers
        )

    def test_olmo2(self):
        from mlx_lm.models import olmo2

        args = olmo2.ModelArgs(
            model_type="olmo2",
            hidden_size=128,
            attention_bias=False,
            intermediate_size=256,
            num_attention_heads=4,
            num_hidden_layers=4,
            num_key_value_heads=2,
            rms_norm_eps=1e-4,
            rope_theta=1000,
            vocab_size=1000,
        )
        model = olmo2.Model(args)
        self.model_test_runner(
            model, args.model_type, args.vocab_size, args.num_hidden_layers
        )

    def test_exaone(self):
        from mlx_lm.models import exaone

        args = exaone.ModelArgs(
            model_type="exaone",
            hidden_size=128,
            num_layers=4,
            intermediate_size=256,
            num_attention_heads=8,
            num_key_value_heads=2,
            vocab_size=1000,
            layer_norm_epsilon=1e-4,
            rope_theta=10000,
        )
        model = exaone.Model(args)
        self.model_test_runner(model, args.model_type, args.vocab_size, args.num_layers)

    def test_cohere2(self):
        from mlx_lm.models import cohere2

        args = cohere2.ModelArgs(
            model_type="cohere2",
            hidden_size=4096,
            head_dim=128,
            num_hidden_layers=40,
            sliding_window=4096,
            sliding_window_pattern=4,
        )
        model = cohere2.Model(args)
        self.model_test_runner(
            model, args.model_type, args.vocab_size, args.num_hidden_layers
        )

    def test_internlm3(self):
        from mlx_lm.models import internlm3

        args = internlm3.ModelArgs(
            model_type="internlm3",
            hidden_size=1024,
            num_hidden_layers=4,
            intermediate_size=2048,
            num_attention_heads=4,
            rms_norm_eps=1e-5,
            vocab_size=10_000,
        )
        model = internlm3.Model(args)
        self.model_test_runner(
            model, args.model_type, args.vocab_size, args.num_hidden_layers
        )


if __name__ == "__main__":
    unittest.main()

>>>> llms/tests/test_finetune.py
# Copyright © 2024 Apple Inc.

import math
import sys
import unittest
from contextlib import contextmanager
from io import StringIO
from unittest.mock import MagicMock

import mlx.core as mx
import mlx.nn as nn
import mlx.optimizers as opt
from mlx.utils import tree_flatten
from mlx_lm import lora, tuner
from mlx_lm.tuner.dora import DoRAEmbedding, DoRALinear
from mlx_lm.tuner.lora import LoRAEmbedding, LoRALinear
from mlx_lm.tuner.trainer import evaluate
from mlx_lm.tuner.utils import build_schedule


@contextmanager
def swapped_with_identity(obj, func):
    old_func = getattr(obj, func)
    setattr(obj, func, lambda x, **kwargs: x)
    yield
    setattr(obj, func, old_func)


class TestLora(unittest.TestCase):
    def setUp(self):
        self.capturedOutput = StringIO()
        sys.stdout = self.capturedOutput

    def tearDown(self):
        sys.stdout = sys.__stdout__

    def test_llama(self):
        from mlx_lm.models import llama

        args = llama.ModelArgs(
            model_type="llama",
            hidden_size=1024,
            num_hidden_layers=4,
            intermediate_size=2048,
            num_attention_heads=4,
            rms_norm_eps=1e-5,
            vocab_size=10_000,
            tie_word_embeddings=False,
        )

        lora_layers = 4

        def check_config(params, expected_trainable_parameters=None):
            n_keys = 2
            if "keys" in params:
                n_keys = len(params["keys"])
            model = llama.Model(args)
            model.freeze()
            tuner.utils.linear_to_lora_layers(model, lora_layers, params)
            trainable_params = sum(
                v.size for _, v in tree_flatten(model.trainable_parameters())
            )

            expected_trainable_parameters = expected_trainable_parameters or (
                lora_layers * params["rank"] * args.hidden_size * 2 * n_keys
            )
            self.assertEqual(trainable_params, expected_trainable_parameters)

        params = {"rank": 8, "alpha": 16, "dropout": 0.0, "scale": 10.0}
        check_config(params)

        params["rank"] = 1
        check_config(params)

        params["keys"] = ["self_attn.k_proj"]
        check_config(params)

        params["keys"] = ["lm_head"]
        check_config(
            params,
            expected_trainable_parameters=(
                params["rank"] * (args.hidden_size + args.vocab_size)
            ),
        )

        params["keys"] = ["model.embed_tokens"]
        check_config(
            params,
            expected_trainable_parameters=(
                params["rank"] * (args.hidden_size + args.vocab_size)
            ),
        )

    def test_gpt_neox(self):
        from mlx_lm.models import gpt_neox

        args = gpt_neox.ModelArgs(
            model_type="gpt_neox",
            max_position_embeddings=2048,
            hidden_size=6144,
            num_attention_heads=64,
            num_hidden_layers=44,
            layer_norm_eps=1e-5,
            vocab_size=50432,
            rotary_emb_base=10_000,
            rotary_pct=0.25,
        )

        num_lora_layers = 4
        params = {"rank": 8, "alpha": 16, "dropout": 0.0, "scale": 10.0}

        model = gpt_neox.Model(args)
        model.freeze()
        tuner.utils.linear_to_lora_layers(model, num_lora_layers, params)

    def test_lora_embedding(self):
        num_embeddings = 256
        dims = 512
        tokens = mx.array([1, 2, 3])

        embedding = nn.QuantizedEmbedding(num_embeddings, dims)
        dequantized_weight = mx.dequantize(
            embedding.weight,
            embedding.scales,
            embedding.biases,
            embedding.group_size,
            embedding.bits,
        )
        lora_emb = LoRAEmbedding.from_base(embedding, r=8, dropout=0, scale=10)
        new_embedding = lora_emb.fuse(de_quantize=True)
        self.assertTrue(mx.array_equal(dequantized_weight, new_embedding.weight))
        self.assertTrue(mx.array_equal(embedding(tokens), lora_emb(tokens)))

        # as_linear
        attn_output = mx.random.uniform(shape=(dims,))
        embedding_lin_out = lora_emb.as_linear(attn_output)
        self.assertEqual(embedding_lin_out.shape, (num_embeddings,))
        self.assertTrue(
            mx.array_equal(embedding_lin_out, embedding.as_linear(attn_output))
        )

        # change the value of lora_b and the embeddings will no longer be equal
        lora_emb.lora_b = mx.random.uniform(shape=lora_emb.lora_b.shape)
        new_embedding = lora_emb.fuse(de_quantize=True)
        self.assertFalse(mx.array_equal(dequantized_weight, new_embedding.weight))
        self.assertFalse(mx.array_equal(embedding(tokens), lora_emb(tokens)))


class TestDora(unittest.TestCase):
    def test_dora_embedding(self):
        num_embeddings = 256
        dims = 512
        tokens = mx.array([1, 2, 3])

        embedding = nn.Embedding(num_embeddings, dims)

        dora_emb = DoRAEmbedding.from_base(embedding, r=8, dropout=0, scale=10)
        new_embedding = dora_emb.fuse()
        self.assertTrue(mx.array_equal(embedding.weight, new_embedding.weight))
        self.assertTrue(mx.array_equal(embedding(tokens), dora_emb(tokens)))

        # as_linear
        attn_output = mx.random.uniform(shape=(dims,))
        embedding_lin_out = dora_emb.as_linear(attn_output)
        self.assertEqual(embedding_lin_out.shape, (num_embeddings,))
        self.assertTrue(
            mx.array_equal(embedding_lin_out, embedding.as_linear(attn_output))
        )

        # change the value of lora_b and the embeddings will no longer be equal
        dora_emb.lora_b = mx.random.uniform(shape=dora_emb.lora_b.shape)
        new_embedding = dora_emb.fuse()
        self.assertFalse(mx.array_equal(embedding.weight, new_embedding.weight))
        self.assertFalse(mx.array_equal(embedding(tokens), dora_emb(tokens)))

    def test_llama(self):
        from mlx_lm.models import llama

        hidden_size = 1024
        intermediate_size = 2048
        args = llama.ModelArgs(
            model_type="llama",
            hidden_size=hidden_size,
            num_hidden_layers=4,
            intermediate_size=intermediate_size,
            num_attention_heads=4,
            rms_norm_eps=1e-5,
            vocab_size=10_000,
        )

        dora_layers = 4

        def check_config(params):
            n_keys = 2
            if "keys" in params:
                n_keys = len(params["keys"])
            model = llama.Model(args)
            model.freeze()
            tuner.utils.linear_to_lora_layers(model, dora_layers, params, use_dora=True)
            trainable_params = sum(
                v.size for _, v in tree_flatten(model.trainable_parameters())
            )
            self.assertEqual(
                trainable_params,
                dora_layers
                * (params["rank"] * hidden_size * 2 * n_keys + n_keys * hidden_size),
            )

        params = {"rank": 8, "alpha": 16, "dropout": 0.0, "scale": 10.0}
        check_config(params)

        params["rank"] = 1
        check_config(params)

        params["keys"] = ["self_attn.k_proj"]
        check_config(params)

    def test_dora_m_parameter(self):
        dora_lin = DoRALinear(input_dims=100, output_dims=100)
        self.assertTrue(
            mx.allclose(dora_lin.m, mx.linalg.norm(dora_lin.linear.weight, axis=1))
        )

        # Recomputes m when changing Linear
        inital_m = dora_lin.m
        lin = nn.Linear(10, 10)
        dora_lin.set_linear(lin)
        self.assertTrue(mx.allclose(dora_lin.m, mx.linalg.norm(lin.weight, axis=1)))

        # Works with quantized weights
        quantized_linear = nn.QuantizedLinear(512, 512)
        dora_lin.set_linear(quantized_linear)
        dequantized_weight = mx.dequantize(
            quantized_linear.weight,
            quantized_linear.scales,
            quantized_linear.biases,
            quantized_linear.group_size,
            quantized_linear.bits,
        )
        self.assertTrue(
            mx.allclose(dora_lin.m, mx.linalg.norm(dequantized_weight, axis=1))
        )

    def test_dora_from_linear(self):
        in_dims = 256
        out_dims = 256
        r = 4

        linear = nn.Linear(in_dims, out_dims)
        dora_lin = DoRALinear.from_base(linear, r)
        self.assertTrue(mx.allclose(dora_lin.m, mx.linalg.norm(linear.weight, axis=1)))
        self.assertEqual(dora_lin.lora_a.shape, (in_dims, r))
        self.assertEqual(dora_lin.lora_b.shape, (r, out_dims))
        self.assertEqual(dora_lin.m.shape, (out_dims,))

        quantized_linear = nn.QuantizedLinear(in_dims, out_dims)
        dequantized_weight = mx.dequantize(
            quantized_linear.weight,
            quantized_linear.scales,
            quantized_linear.biases,
            quantized_linear.group_size,
            quantized_linear.bits,
        )
        dora_quant_lin = DoRALinear.from_base(quantized_linear, r)
        self.assertTrue(
            mx.allclose(dora_quant_lin.m, mx.linalg.norm(dequantized_weight, axis=1))
        )
        self.assertEqual(dora_quant_lin.lora_a.shape, (in_dims, r))
        self.assertEqual(dora_quant_lin.lora_b.shape, (r, out_dims))
        self.assertEqual(dora_quant_lin.m.shape, (out_dims,))

    def test_dora_to_linear(self):
        in_dims = 256
        out_dims = 256
        r = 4

        linear = nn.Linear(in_dims, out_dims, bias=True)
        dora_lin = DoRALinear.from_base(linear, r)
        to_linear = dora_lin.fuse()
        self.assertTrue(mx.allclose(linear.weight, to_linear.weight))
        self.assertTrue(mx.allclose(linear.bias, to_linear.bias))

        def dequantize_weight(quantized_linear):
            return mx.dequantize(
                quantized_linear.weight,
                quantized_linear.scales,
                quantized_linear.biases,
                quantized_linear.group_size,
                quantized_linear.bits,
            )

        quantized_linear = nn.QuantizedLinear(in_dims, out_dims, bias=True)
        dora_quantized_linear = DoRALinear.from_base(quantized_linear, r)
        # Dequantize
        to_linear_from_quantized = dora_quantized_linear.fuse(de_quantize=True)
        self.assertTrue(
            mx.allclose(quantized_linear.bias, to_linear_from_quantized.bias)
        )
        self.assertTrue(
            mx.allclose(
                dequantize_weight(quantized_linear), to_linear_from_quantized.weight
            )
        )

    def test_dora_dtype(self):
        in_dims = 256
        out_dims = 256
        r = 4

        linear = nn.Linear(in_dims, out_dims, bias=True)
        linear.set_dtype(mx.float16)
        dora_lin = DoRALinear.from_base(linear, r)

        x = mx.random.uniform(shape=(2, 256)).astype(mx.float16)
        self.assertEqual(dora_lin(x).dtype, mx.float16)


class TestScheduleConfig(unittest.TestCase):
    def test_join(self):
        config = {"name": "cosine_decay", "warmup": 100, "arguments": [1e-5, 100]}
        cos_with_warmup = build_schedule(config)
        self.assertIsNotNone(cos_with_warmup)

        self.assertEqual(cos_with_warmup(0), 0.0)
        self.assertAlmostEqual(cos_with_warmup(101), 1e-5, delta=1e-1)
        optimizer = opt.Adam(learning_rate=cos_with_warmup)
        for _ in range(100):
            optimizer.update({}, {})
        self.assertAlmostEqual(optimizer.learning_rate.item(), 1e-5, delta=1e-1)
        for _ in range(100):
            optimizer.update({}, {})
        expected_lr = 1e-5 * 0.5 * (1.0 + math.cos(math.pi * 200 / 10))
        self.assertAlmostEqual(optimizer.learning_rate.item(), expected_lr, delta=1e-1)

    def test_single_schedule(self):

        config = {
            "name": "cosine_decay",
            "arguments": [0.1, 10],
        }
        lr_schedule = build_schedule(config)
        lr = lr_schedule(4)
        expected_lr = 0.1 * 0.5 * (1.0 + math.cos(math.pi * 4 / 10))
        self.assertAlmostEqual(lr, expected_lr, delta=1e-7)

    def test_non_zero_warmup(self):
        config = {
            "name": "cosine_decay",
            "warmup": 10,
            "warmup_init": 1e-6,
            "arguments": [1e-5, 20],
        }
        lr_schedule = build_schedule(config)
        lr = lr_schedule(0)
        self.assertAlmostEqual(lr, 1e-6, delta=1e-7)

    def test_malformed_config(self):
        config = {"warmup": 100}
        self.assertRaises(KeyError, build_schedule, config)

        config = {"cosine_decay": None}
        self.assertRaises(KeyError, build_schedule, config)

    def test_evaluate_calls(self):
        mock_model = MagicMock()
        mock_dataset = MagicMock()
        mock_tokenizer = MagicMock()
        mock_default_loss = MagicMock()
        mock_iterate_batches = MagicMock()

        mock_iterate_batches.return_value = [
            (MagicMock(), MagicMock()),
            (MagicMock(), MagicMock()),
            (MagicMock(), MagicMock()),
            (MagicMock(), MagicMock()),
            (MagicMock(), MagicMock()),
        ]

        mock_default_loss.side_effect = [
            (MagicMock(return_value=0.5), MagicMock(return_value=100)),
            (MagicMock(return_value=0.3), MagicMock(return_value=200)),
            (MagicMock(return_value=0.2), MagicMock(return_value=150)),
            (MagicMock(return_value=0.4), MagicMock(return_value=180)),
            (MagicMock(return_value=0.6), MagicMock(return_value=120)),
        ]
        with swapped_with_identity(mx.distributed, "all_sum"):
            evaluate(
                model=mock_model,
                dataset=mock_dataset,
                tokenizer=mock_tokenizer,
                batch_size=2,
                num_batches=2,
                max_seq_length=2048,
                loss=mock_default_loss,
                iterate_batches=mock_iterate_batches,
            )

        mock_iterate_batches.assert_called_once_with(
            dataset=mock_dataset,
            tokenizer=mock_tokenizer,
            batch_size=2,
            max_seq_length=2048,
        )
        self.assertEqual(mock_default_loss.call_count, 2)

    def test_evaluate_infinite_batches(self):
        mock_model = MagicMock()
        mock_dataset = MagicMock()
        mock_tokenizer = MagicMock()
        mock_default_loss = MagicMock()
        mock_iterate_batches = MagicMock()

        mock_iterate_batches.return_value = [
            (MagicMock(), MagicMock()),
            (MagicMock(), MagicMock()),
            (MagicMock(), MagicMock()),
        ]

        mock_default_loss.side_effect = [
            (MagicMock(return_value=0.5), MagicMock(return_value=100)),
            (MagicMock(return_value=0.3), MagicMock(return_value=200)),
            (MagicMock(return_value=0.2), MagicMock(return_value=150)),
        ]

        with swapped_with_identity(mx.distributed, "all_sum"):
            evaluate(
                model=mock_model,
                dataset=mock_dataset,
                tokenizer=mock_tokenizer,
                batch_size=2,
                num_batches=-1,
                max_seq_length=2048,
                loss=mock_default_loss,
                iterate_batches=mock_iterate_batches,
            )

        mock_iterate_batches.assert_called_once_with(
            dataset=mock_dataset,
            tokenizer=mock_tokenizer,
            batch_size=2,
            max_seq_length=2048,
        )
        self.assertEqual(mock_default_loss.call_count, 3)


if __name__ == "__main__":
    unittest.main()

>>>> llms/tests/test_server.py
# Copyright © 2024 Apple Inc.

import http
import json
import threading
import unittest

import requests
from mlx_lm.server import APIHandler
from mlx_lm.utils import load


class DummyModelProvider:
    def __init__(self):
        HF_MODEL_PATH = "mlx-community/Qwen1.5-0.5B-Chat-4bit"
        self.model, self.tokenizer = load(HF_MODEL_PATH)
        self.model_key = (HF_MODEL_PATH, None)

    def load(self, model, adapter=None):
        assert model in ["default_model", "chat_model"]
        return self.model, self.tokenizer


class TestServer(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        cls.model_provider = DummyModelProvider()
        cls.server_address = ("localhost", 0)
        cls.httpd = http.server.HTTPServer(
            cls.server_address,
            lambda *args, **kwargs: APIHandler(cls.model_provider, *args, **kwargs),
        )
        cls.port = cls.httpd.server_port
        cls.server_thread = threading.Thread(target=cls.httpd.serve_forever)
        cls.server_thread.daemon = True
        cls.server_thread.start()

    @classmethod
    def tearDownClass(cls):
        cls.httpd.shutdown()
        cls.httpd.server_close()
        cls.server_thread.join()

    def test_handle_completions(self):
        url = f"http://localhost:{self.port}/v1/completions"

        post_data = {
            "model": "default_model",
            "prompt": "Once upon a time",
            "max_tokens": 10,
            "temperature": 0.5,
            "top_p": 0.9,
            "repetition_penalty": 1.1,
            "repetition_context_size": 20,
            "stop": "stop sequence",
        }

        response = requests.post(url, json=post_data)

        response_body = response.text

        self.assertIn("id", response_body)
        self.assertIn("choices", response_body)

    def test_handle_chat_completions(self):
        url = f"http://localhost:{self.port}/v1/chat/completions"
        chat_post_data = {
            "model": "chat_model",
            "max_tokens": 10,
            "temperature": 0.7,
            "top_p": 0.85,
            "repetition_penalty": 1.2,
            "messages": [
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": "Hello!"},
            ],
        }
        response = requests.post(url, json=chat_post_data)
        response_body = response.text
        self.assertIn("id", response_body)
        self.assertIn("choices", response_body)

    def test_handle_chat_completions_with_content_fragments(self):
        url = f"http://localhost:{self.port}/v1/chat/completions"
        chat_post_data = {
            "model": "chat_model",
            "max_tokens": 10,
            "temperature": 0.7,
            "top_p": 0.85,
            "repetition_penalty": 1.2,
            "messages": [
                {
                    "role": "system",
                    "content": [
                        {"type": "text", "text": "You are a helpful assistant."}
                    ],
                },
                {"role": "user", "content": [{"type": "text", "text": "Hello!"}]},
            ],
        }
        response = requests.post(url, json=chat_post_data)
        response_body = response.text
        self.assertIn("id", response_body)
        self.assertIn("choices", response_body)

    def test_handle_models(self):
        url = f"http://localhost:{self.port}/v1/models"
        response = requests.get(url)
        self.assertEqual(response.status_code, 200)
        response_body = json.loads(response.text)
        self.assertEqual(response_body["object"], "list")
        self.assertIsInstance(response_body["data"], list)
        self.assertGreater(len(response_body["data"]), 0)
        model = response_body["data"][0]
        self.assertIn("id", model)
        self.assertEqual(model["object"], "model")
        self.assertIn("created", model)

    def test_sequence_overlap(self):
        from mlx_lm.server import sequence_overlap

        self.assertTrue(sequence_overlap([1], [1]))
        self.assertTrue(sequence_overlap([1, 2], [1, 2]))
        self.assertTrue(sequence_overlap([1, 3], [3, 4]))
        self.assertTrue(sequence_overlap([1, 2, 3], [2, 3]))

        self.assertFalse(sequence_overlap([1], [2]))
        self.assertFalse(sequence_overlap([1, 2], [3, 4]))
        self.assertFalse(sequence_overlap([1, 2, 3], [4, 1, 2, 3]))


if __name__ == "__main__":
    unittest.main()

>>>> llms/tests/test_prompt_cache.py
# Copyright © 2024 Apple Inc.

import copy
import os
import tempfile
import unittest

import mlx.core as mx
from mlx_lm.models.cache import (
    KVCache,
    MambaCache,
    QuantizedKVCache,
    RotatingKVCache,
    load_prompt_cache,
    make_prompt_cache,
    save_prompt_cache,
    trim_prompt_cache,
)
from mlx_lm.utils import generate_step, load

HF_MODEL_PATH = "mlx-community/Qwen1.5-0.5B-Chat-4bit"


class TestPromptCache(unittest.TestCase):

    @classmethod
    def setUpClass(cls):
        cls.test_dir_fid = tempfile.TemporaryDirectory()
        cls.test_dir = cls.test_dir_fid.name

    @classmethod
    def tearDownClass(cls):
        cls.test_dir_fid.cleanup()

    def test_save_load(self):
        cache = [KVCache() for _ in range(4)]
        for c in cache:
            x = mx.random.uniform(shape=(1, 8, 10, 4))
            c.update_and_fetch(x, x)
        cache_file = os.path.join(self.test_dir, "prompt_cache.safetensors")
        save_prompt_cache(cache_file, cache)
        loaded_cache = load_prompt_cache(cache_file)
        self.assertTrue(len(cache), len(loaded_cache))
        for c, lc in zip(cache, loaded_cache):
            self.assertEqual(c.offset, lc.offset)
            self.assertTrue(mx.array_equal(c.state[0], lc.state[0]))
            self.assertTrue(mx.array_equal(c.state[1], lc.state[1]))

        # Test with metadata
        cache_file = os.path.join(self.test_dir, "prompt_cache.safetensors")
        metadata = {"a": "b", "c": "d"}
        save_prompt_cache(cache_file, cache, metadata)
        _, loaded_metadata = load_prompt_cache(cache_file, return_metadata=True)
        self.assertEqual(metadata, loaded_metadata)

    def test_save_load_rotating_cache(self):
        cache_file = os.path.join(self.test_dir, "prompt_cache.safetensors")

        # Test with rotating cache
        cache = [RotatingKVCache(max_size=8, keep=2) for _ in range(4)]
        for c in cache:
            x = mx.random.uniform(shape=(1, 8, 10, 4))
            c.update_and_fetch(x, x)

        save_prompt_cache(cache_file, cache)
        loaded_cache = load_prompt_cache(cache_file)
        self.assertTrue(len(cache), len(loaded_cache))
        for c, lc in zip(cache, loaded_cache):
            self.assertEqual(c.offset, lc.offset)
            self.assertEqual(c.keep, lc.keep)
            self.assertEqual(c.max_size, lc.max_size)
            self.assertEqual(c.step, lc.step)
            self.assertTrue(mx.array_equal(c.state[0], lc.state[0]))
            self.assertTrue(mx.array_equal(c.state[1], lc.state[1]))

        # Do a couple single token updates to get a rotation
        for _ in range(2):
            for c in cache:
                x = mx.random.uniform(shape=(1, 8, 1, 4))
                c.update_and_fetch(x, x)

        save_prompt_cache(cache_file, cache)
        loaded_cache = load_prompt_cache(cache_file)

        for c, lc in zip(cache, loaded_cache):
            x = mx.random.uniform(shape=(1, 8, 1, 4))
            k, v = c.update_and_fetch(x, x)
            lk, lv = lc.update_and_fetch(x, x)
            self.assertEqual(c.offset, lc.offset)
            self.assertTrue(mx.array_equal(k, lk))
            self.assertTrue(mx.array_equal(v, lv))

    def test_save_load_mixed_cache(self):
        cache_file = os.path.join(self.test_dir, "prompt_cache.safetensors")

        cache = [MambaCache(), KVCache(), RotatingKVCache(8), MambaCache()]
        for c in cache:
            if isinstance(c, MambaCache):
                c[0] = mx.random.uniform(shape=(4, 4, 4))
                c[1] = mx.random.uniform(shape=(4, 4, 4))
            else:
                x = mx.random.uniform(shape=(4, 4, 7, 4))
                y = mx.random.uniform(shape=(4, 4, 7, 4))
                c.update_and_fetch(x, y)

        save_prompt_cache(cache_file, cache)
        loaded_cache = load_prompt_cache(cache_file)
        for c, lc in zip(cache, loaded_cache):
            if isinstance(c, MambaCache):
                self.assertTrue(mx.array_equal(c[0], lc[0]))
                self.assertTrue(mx.array_equal(c[1], lc[1]))
            else:
                x = mx.random.uniform(shape=(4, 4, 1, 4))
                y = mx.random.uniform(shape=(4, 4, 1, 4))
                k, v = c.update_and_fetch(x, y)
                lk, lv = lc.update_and_fetch(x, y)
                self.assertEqual(c.offset, lc.offset)
                self.assertTrue(mx.array_equal(k, lk))
                self.assertTrue(mx.array_equal(v, lv))

    def test_cache_with_generate(self):
        model, tokenizer = load(HF_MODEL_PATH)
        prompt = tokenizer.encode("this is a prompt", return_tensors="mlx")[0]
        results = list(generate_step(prompt, model, max_tokens=4))
        toks, all_logits = zip(*results)

        prompt_cache = make_prompt_cache(model)
        i = 0
        for tok, logits in generate_step(
            prompt, model, prompt_cache=prompt_cache, max_tokens=2
        ):
            self.assertEqual(tok, toks[i])
            self.assertTrue(mx.allclose(logits, all_logits[i]))
            i += 1

        for tok, logits in generate_step(
            mx.array([toks[i]]), model, prompt_cache=prompt_cache, max_tokens=1
        ):
            i += 1
            self.assertEqual(tok, toks[i])
            self.assertTrue(mx.allclose(logits, all_logits[i]))

    def test_trim_cache(self):
        cache = [KVCache() for _ in range(2)]
        for c in cache:
            x = mx.random.uniform(shape=(1, 8, 10, 4))
            c.update_and_fetch(x, x)

        # Trim
        num_trimmed = trim_prompt_cache(cache, 7)
        self.assertEqual(num_trimmed, 7)

        # Trim more tokens than remain
        num_trimmed = trim_prompt_cache(cache, 4)
        self.assertEqual(num_trimmed, 3)

        # Can't trim mamba cache
        cache = [MambaCache() for _ in range(2)]
        for c in cache:
            c.state = mx.zeros((5, 5))
        num_trimmed = trim_prompt_cache(cache, 7)
        self.assertEqual(num_trimmed, 0)

        # All cache's have to be trimmable
        cache = [MambaCache(), KVCache()]
        cache[0].state = mx.zeros((5, 5))
        x = mx.random.uniform(shape=(1, 8, 10, 4))
        cache[1].update_and_fetch(x, x)
        num_trimmed = trim_prompt_cache(cache, 1)
        self.assertEqual(num_trimmed, 0)

        cache = [RotatingKVCache(max_size=6) for _ in range(2)]
        for c in cache:
            x = mx.random.uniform(shape=(1, 8, 5, 4))
            c.update_and_fetch(x, x)

        num_trimmed = trim_prompt_cache(cache, 4)
        self.assertEqual(num_trimmed, 4)

        # Can't trim fixed-size KV cache after processing
        # more than max_kv_size tokens
        for c in cache:
            x = mx.random.uniform(shape=(1, 8, 10, 4))
            c.update_and_fetch(x, x)

        num_trimmed = trim_prompt_cache(cache, 4)
        self.assertEqual(num_trimmed, 0)

        cache = [QuantizedKVCache() for _ in range(2)]
        for c in cache:
            x = mx.random.uniform(shape=(1, 8, 10, 64))
            c.update_and_fetch(x, x)

        num_trimmed = trim_prompt_cache(cache, 7)
        self.assertEqual(num_trimmed, 7)

        # Trim more tokens than remain
        num_trimmed = trim_prompt_cache(cache, 4)
        self.assertEqual(num_trimmed, 3)

    def test_trim_cache_with_generate(self):
        model, tokenizer = load(HF_MODEL_PATH)
        prompt = tokenizer.encode("this is a prompt", return_tensors="mlx")[0]

        prompt_cache = make_prompt_cache(model)

        # Generate one token so we process the full prompt
        last_tok, _ = next(generate_step(prompt, model, prompt_cache=prompt_cache))
        last_tok = mx.array([last_tok])

        # Generate two more tokens
        results = zip(
            range(2), generate_step(last_tok, model, prompt_cache=prompt_cache)
        )
        toks, all_logits = zip(*(r[1] for r in results))

        # To get back to the cache just after processing the prompt,
        # trim by 3 tokens
        trim_prompt_cache(prompt_cache, 3)

        # Generate the same thing again
        results = zip(
            range(2), generate_step(last_tok, model, prompt_cache=prompt_cache)
        )
        second_toks, second_all_logits = zip(*(r[1] for r in results))
        self.assertEqual(toks, second_toks)
        self.assertTrue(
            all(mx.allclose(l, l2) for l, l2 in zip(all_logits, second_all_logits))
        )

    def test_cache_copying(self):
        cache = [KVCache()]

        x = mx.random.uniform(shape=(1, 8, 10, 4))
        cache[0].update_and_fetch(x, x)

        y = mx.random.uniform(shape=(1, 8, 1, 4))
        cache[0].update_and_fetch(y, y)

        old_cache = copy.deepcopy(cache)

        trim_prompt_cache(cache, 1)

        self.assertTrue(old_cache[0].offset, 11)
        self.assertTrue(cache[0].offset, 10)

        z = mx.random.uniform(shape=(1, 8, 1, 4))
        cache[0].update_and_fetch(z, z)

        self.assertTrue(mx.allclose(old_cache[0].keys[..., 10:11, :], y))
        self.assertTrue(mx.allclose(cache[0].keys[..., 10:11, :], z))

    def test_save_load_quantized_cache(self):
        cache = [QuantizedKVCache(bits=4, group_size=32) for _ in range(4)]
        for c in cache:
            x = mx.random.uniform(shape=(1, 8, 10, 32))
            c.update_and_fetch(x, x)
        cache_file = os.path.join(self.test_dir, "prompt_cache.safetensors")
        save_prompt_cache(cache_file, cache)
        loaded_cache = load_prompt_cache(cache_file)
        self.assertTrue(loaded_cache[0].bits == cache[0].bits)
        self.assertTrue(loaded_cache[0].group_size == cache[0].group_size)
        self.assertTrue(len(cache), len(loaded_cache))
        for c, lc in zip(cache, loaded_cache):
            self.assertEqual(c.offset, lc.offset)
            # Loop over quantized tuple
            for i in range(3):
                self.assertTrue(mx.array_equal(c.state[0][i], lc.state[0][i]))
                self.assertTrue(mx.array_equal(c.state[1][i], lc.state[1][i]))

        # Test with metadata
        cache_file = os.path.join(self.test_dir, "prompt_cache.safetensors")
        metadata = {"a": "b", "c": "d"}
        save_prompt_cache(cache_file, cache, metadata)
        _, loaded_metadata = load_prompt_cache(cache_file, return_metadata=True)
        self.assertEqual(metadata, loaded_metadata)

    def test_cache_to_quantized(self):
        model, tokenizer = load(HF_MODEL_PATH)
        prompt = tokenizer.encode("this is a prompt", return_tensors="mlx")[0]
        results = zip(range(4), generate_step(prompt, model))
        toks, all_logits = zip(*(r[1] for r in results))

        prompt_cache = make_prompt_cache(model)
        i = 0
        for _, (tok, logits) in zip(
            range(2), generate_step(prompt, model, prompt_cache=prompt_cache)
        ):
            self.assertEqual(tok, toks[i])
            self.assertTrue(mx.allclose(logits, all_logits[i]))
            i += 1

        prompt_cache = [c.to_quantized(bits=8, group_size=32) for c in prompt_cache]

        for _, (tok, logits) in zip(
            range(1),
            generate_step(mx.array([toks[i]]), model, prompt_cache=prompt_cache),
        ):
            i += 1
            self.assertEqual(tok, toks[i])
            self.assertTrue(mx.allclose(logits, all_logits[i], rtol=3e-2))


if __name__ == "__main__":
    unittest.main()

>>>> llms/tests/test_sample_utils.py
import unittest

import mlx.core as mx
from mlx_lm.sample_utils import min_p_sampling, top_k_sampling, top_p_sampling


class TestSampleUtils(unittest.TestCase):
    def test_top_p_sampling(self):
        probs = mx.array([0.9, 0.0, 0.0, 0.1])[None]
        logits = mx.log(probs)
        temperature = 1.0

        token = top_p_sampling(logits, 0.3, temperature).item()
        self.assertEqual(token, 0)

        token = top_p_sampling(logits, 0.95, temperature).item()
        self.assertTrue(token in (0, 3))

        probs = mx.array([0.0, 0.5, 0.4, 0.1])[None]
        logits = mx.log(probs)

        token = top_p_sampling(logits, 0.4, temperature).item()
        self.assertEqual(token, 1)

        token = top_p_sampling(logits, 0.6, temperature).item()
        self.assertTrue(token in (1, 2))

        token = top_p_sampling(logits, 0.95, temperature).item()
        self.assertTrue(token in (1, 2, 3))

        # Batch mode works
        probs = mx.array([[0.9, 0.0, 0.0, 0.1], [0.0, 0.8, 0.0, 0.1]])
        logits = mx.log(probs)
        tokens = top_p_sampling(logits, 0.5, temperature)
        self.assertEqual(tokens.tolist(), [0, 1])

    def test_min_p_sampling(self):
        probs = mx.array([0.9, 0.0, 0.0, 0.1])[None]
        logits = mx.log(probs)
        temperature = 1.0
        token = min_p_sampling(logits, 0.8)
        self.assertEqual(token, 0)

        probs = mx.array([0.9, 0.0, 0.0, 0.1])[None]
        logits = mx.log(probs)
        temperature = 1.0
        for _ in range(5):
            token = min_p_sampling(logits, 0.05)
            self.assertTrue(token in (0, 3))

        # Batch mode works
        probs = mx.array([[0.9, 0.0, 0.0, 0.1], [0.0, 0.8, 0.0, 0.1]])
        logits = mx.log(probs)
        tokens = min_p_sampling(logits, 0.7)
        self.assertEqual(tokens.tolist(), [0, 1])

    def test_top_k_sampling(self):
        probs = mx.array([0.9, 0.0, 0.0, 0.1])[None]
        logits = mx.log(probs)

        token = top_k_sampling(logits, 1).item()
        self.assertEqual(token, 0)

        probs = mx.array([0.5, 0.0, 0.0, 0.5])[None]
        tokens = set()
        for _ in range(100):
            token = top_k_sampling(logits, 2)
            tokens.add(token.item())
        self.assertEqual(tokens, {0, 3})

        # Batch mode works
        probs = mx.array([[0.9, 0.0, 0.0, 0.1], [0.0, 0.8, 0.0, 0.1]])
        logits = mx.log(probs)

        tokens = top_k_sampling(logits, 1)
        self.assertEqual(tokens.tolist(), [0, 1])


if __name__ == "__main__":
    unittest.main()

>>>> llms/tests/test_generate.py
# Copyright © 2024 Apple Inc.

import unittest
from typing import List

from mlx_lm.sample_utils import make_logits_processors
from mlx_lm.utils import (
    GenerationResponse,
    generate,
    load,
    make_sampler,
    stream_generate,
)


class TestGenerate(unittest.TestCase):

    @classmethod
    def setUpClass(cls):
        cls.HF_MODEL_PATH = "mlx-community/Qwen1.5-0.5B-Chat-4bit"
        cls.model, cls.tokenizer = load(cls.HF_MODEL_PATH)

    def test_generate(self):
        # Simple test that generation runs
        text = generate(
            self.model, self.tokenizer, "hello", max_tokens=5, verbose=False
        )

    def test_generate_with_logit_bias(self):
        logit_bias = {0: 2000.0, 1: -20.0}
        text = generate(
            self.model,
            self.tokenizer,
            "hello",
            max_tokens=5,
            logits_processors=make_logits_processors(logit_bias),
            verbose=False,
        )
        self.assertEqual(text, "!!!!!")

    def test_generate_with_processor(self):
        init_toks = self.tokenizer.encode("hello")

        all_toks = None

        def logits_processor(toks, logits):
            nonlocal all_toks
            all_toks = toks
            return logits

        generate(
            self.model,
            self.tokenizer,
            "hello",
            max_tokens=5,
            verbose=False,
            logits_processors=[logits_processor],
        )
        self.assertEqual(len(all_toks), len(init_toks) + 5)

    def test_stream_generate_speculative(self):
        # Use same model as draft model, this is not a speed test
        draft_model, _ = load(self.HF_MODEL_PATH)

        results: List[GenerationResponse] = []
        drafted: List[bool] = []

        # make a determinate sampler
        sampler = make_sampler(temp=0.0)

        for generation_result in stream_generate(
            model=self.model,
            tokenizer=self.tokenizer,
            prompt="hello",
            max_tokens=5,
            draft_model=draft_model,
            num_draft_tokens=2,
            sampler=sampler,
        ):
            drafted.append(generation_result.from_draft)
            results.append(generation_result)

        self.assertEqual(len(results), 5)
        # since num_draft_tokens is 2 and draft model is the same, the
        # first 2 generations should be drafts, the third should come
        # from the target model, and last two should be drafts
        self.assertEqual(drafted, [True, True, False, True, True])


if __name__ == "__main__":
    unittest.main()

>>>> llms/tests/test_utils_load_model.py
import unittest
from pathlib import Path

import mlx.nn as nn
from mlx_lm.models.qwen2 import Model as Qwen2Model
from mlx_lm.utils import get_model_path, load_model

HF_MODEL_PATH = "mlx-community/Qwen1.5-0.5B-Chat-4bit"


class TestLoadModelCustomGetClasses(unittest.TestCase):

    def test_load_model_with_custom_get_classes(self):
        class CustomQwenModel(nn.Module):
            def __init__(self, args):
                super().__init__()
                self.config = args
                self.custom_attribute = "This is a custom model"

            def load_weights(self, weights, **kwargs):
                self.qwenWeights = weights

        class CustomQwenConfig:
            @classmethod
            def from_dict(cls, config):
                instance = cls()
                for k, v in config.items():
                    setattr(instance, k, v)
                return instance

        def custom_get_classes(config):
            return CustomQwenModel, CustomQwenConfig

        model_path = get_model_path(HF_MODEL_PATH)
        model, _ = load_model(model_path, get_model_classes=custom_get_classes)

        self.assertIsInstance(model, CustomQwenModel)
        self.assertTrue(hasattr(model, "custom_attribute"))
        self.assertEqual(model.custom_attribute, "This is a custom model")
        self.assertTrue(hasattr(model, "qwenWeights"))

    def test_load_model_with_default_get_classes(self):
        model_path = get_model_path(HF_MODEL_PATH)
        model, _ = load_model(model_path)

        self.assertIsInstance(model, Qwen2Model)


if __name__ == "__main__":
    unittest.main()

>>>> llms/tests/test_tokenizers.py
# Copyright © 2024 Apple Inc.

import unittest
from pathlib import Path

from huggingface_hub import snapshot_download
from mlx_lm.tokenizer_utils import (
    BPEStreamingDetokenizer,
    NaiveStreamingDetokenizer,
    SPMStreamingDetokenizer,
    load_tokenizer,
)


class TestTokenizers(unittest.TestCase):

    def download_tokenizer(self, repo):
        path = Path(
            snapshot_download(
                repo_id=repo,
                allow_patterns=[
                    "tokenizer.json",
                    "tokenizer_config.json",
                    "special_tokens_map.json",
                    "tokenizer.model",
                ],
            )
        )
        return load_tokenizer(path)

    def check_tokenizer(self, tokenizer):
        def check(tokens):
            expected_text = tokenizer.decode(tokens)
            detokenizer = tokenizer.detokenizer
            detokenizer.reset()
            text = ""
            for e, t in enumerate(tokens):
                detokenizer.add_token(t)
                seg = detokenizer.last_segment
                text += seg
                self.assertEqual(detokenizer.tokens, tokens[: e + 1])
            detokenizer.finalize()
            text += detokenizer.last_segment
            self.assertEqual(text, expected_text)

        tokens = tokenizer.encode("こんにちは！私の名前はAI")
        check(tokens)

        tokens = tokenizer.encode("a ,b")
        check(tokens)

        tokens = tokenizer.encode('{"why_its_funny" :"a_joke_explainer" ,"rating":3.5}')
        check(tokens)

        tokens = tokenizer.encode("3 3")
        check(tokens)

        tokens = tokenizer.encode("import 'package:flutter/material.dart';")
        check(tokens)

        tokens = tokenizer.encode("hello\nworld")
        check(tokens)

    def test_tokenizers(self):
        tokenizer_repos = [
            ("mlx-community/Qwen1.5-0.5B-Chat-4bit", BPEStreamingDetokenizer),
            ("mlx-community/Mistral-7B-v0.2-4bit", SPMStreamingDetokenizer),
            ("mlx-community/Phi-3.5-mini-instruct-4bit", SPMStreamingDetokenizer),
            ("mlx-community/Mistral-7B-Instruct-v0.3", SPMStreamingDetokenizer),
            ("mlx-community/Llama-3.2-1B-Instruct-4bit", BPEStreamingDetokenizer),
            ("mlx-community/Falcon3-7B-Instruct-4bit", BPEStreamingDetokenizer),
        ]
        for tokenizer_repo, expected_detokenizer in tokenizer_repos:
            with self.subTest(tokenizer=tokenizer_repo):
                tokenizer = self.download_tokenizer(tokenizer_repo)
                tokenizer.decode([0, 1, 2])
                self.assertTrue(isinstance(tokenizer.detokenizer, expected_detokenizer))
                self.check_tokenizer(tokenizer)

        # Try one with a naive detokenizer
        tokenizer = self.download_tokenizer("mlx-community/Llama-3.2-1B-Instruct-4bit")
        tokenizer._detokenizer = NaiveStreamingDetokenizer(tokenizer)
        self.check_tokenizer(tokenizer)

    def test_special_tokens(self):
        tokenizer_repo = "mlx-community/DeepSeek-Coder-V2-Lite-Instruct-4bit-mlx"
        tokenizer = self.download_tokenizer(tokenizer_repo)

        detokenizer = tokenizer.detokenizer
        detokenizer.reset()
        detokenizer.add_token(tokenizer.eos_token_id)
        detokenizer.finalize()

        self.assertEqual(detokenizer.last_segment, tokenizer.eos_token)


if __name__ == "__main__":
    unittest.main()

>>>> llms/tests/test_gguf.py
import os
import tempfile
import unittest
from pathlib import Path
from unittest.mock import MagicMock, patch

import mlx.core as mx
from mlx_lm.gguf import convert_to_gguf


class TestConvertToGGUFWithoutMocks(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        cls.test_dir_fid = tempfile.TemporaryDirectory()
        cls.test_dir = cls.test_dir_fid.name
        cls.tokenizer_file_path = os.path.join(cls.test_dir, "tokenizer.json")
        with open(cls.tokenizer_file_path, "w") as f:
            f.write("{}")

    @classmethod
    def tearDownClass(cls):
        cls.test_dir_fid.cleanup()

    @patch("transformers.AutoTokenizer.from_pretrained")
    @patch("mlx.core.save_gguf")
    def test_convert_to_gguf(
        self,
        mock_save_gguf,
        mock_from_pretrained,
    ):
        mock_tokenizer = MagicMock()
        mock_tokenizer.vocab_size = 3
        mock_tokenizer.get_added_vocab.return_value = {}
        mock_tokenizer.get_vocab.return_value = {"<pad>": 0, "hello": 1, "world": 2}
        mock_tokenizer.all_special_tokens = ["<pad>"]
        mock_tokenizer.all_special_ids = [0]
        mock_from_pretrained.return_value = mock_tokenizer

        model_path = Path(self.test_dir)
        weights = {
            "self_attn.q_proj.weight": mx.random.uniform(shape=[768, 768]),
        }
        config = {
            "num_attention_heads": 1,
            "num_hidden_layers": 1,
            "hidden_size": 768,
            "intermediate_size": 3072,
            "_name_or_path": "test-llama",
        }
        output_file_path = "/fake/output/path/gguf_model.gguf"

        convert_to_gguf(model_path, weights, config, output_file_path)
        called_args, _ = mock_save_gguf.call_args
        self.assertEqual(called_args[0], output_file_path)


if __name__ == "__main__":
    unittest.main()

>>>> llms/tests/test_datsets.py
# Copyright © 2024 Apple Inc.

import json
import os
import tempfile
import types
import unittest

from mlx_lm.tuner import datasets
from transformers import AutoTokenizer

HF_MODEL_PATH = "mlx-community/Qwen1.5-0.5B-Chat-4bit"


class TestDatasets(unittest.TestCase):

    @classmethod
    def setUpClass(cls):
        cls.test_dir_fid = tempfile.TemporaryDirectory()
        cls.test_dir = cls.test_dir_fid.name
        if not os.path.isdir(cls.test_dir):
            os.mkdir(cls.test_dir_fid.name)

    @classmethod
    def tearDownClass(cls):
        cls.test_dir_fid.cleanup()

    def save_data(self, data):
        for ds in ["train", "valid"]:
            with open(os.path.join(self.test_dir, f"{ds}.jsonl"), "w") as fid:
                for l in data:
                    json.dump(l, fid)
                    fid.write("\n")

    def test_text(self):
        data = {"text": "This is an example for the model."}
        self.save_data(4 * [data])
        args = types.SimpleNamespace(train=True, test=False, data=self.test_dir)
        tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_PATH)
        train, valid, test = datasets.load_dataset(args, tokenizer)
        self.assertEqual(len(train), 4)
        self.assertEqual(len(valid), 4)
        self.assertEqual(len(test), 0)
        self.assertTrue(len(train[0]) > 0)
        self.assertTrue(len(valid[0]) > 0)
        self.assertTrue(isinstance(train, datasets.Dataset))

    def test_completions(self):
        data = {"prompt": "What is the capital of France?", "completion": "Paris."}
        self.save_data(4 * [data])
        args = types.SimpleNamespace(train=True, test=False, data=self.test_dir)
        tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_PATH)
        train, valid, test = datasets.load_dataset(args, tokenizer)
        self.assertEqual(len(train), 4)
        self.assertEqual(len(valid), 4)
        self.assertEqual(len(test), 0)
        self.assertTrue(len(train[0]) > 0)
        self.assertTrue(len(valid[0]) > 0)
        self.assertTrue(isinstance(train, datasets.CompletionsDataset))

    def test_chat(self):
        data = {
            "messages": [
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": "Hello."},
                {"role": "assistant", "content": "How can I assistant you today."},
            ]
        }
        self.save_data(4 * [data])
        args = types.SimpleNamespace(train=True, test=False, data=self.test_dir)
        tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_PATH)
        train, valid, test = datasets.load_dataset(args, tokenizer)
        self.assertEqual(len(train), 4)
        self.assertEqual(len(valid), 4)
        self.assertEqual(len(test), 0)
        self.assertTrue(len(train[0]) > 0)
        self.assertTrue(len(valid[0]) > 0)
        self.assertTrue(isinstance(train, datasets.ChatDataset))

    def test_hf(self):
        hf_args = {
            "name": "billsum",
            "prompt_feature": "text",
            "completion_feature": "summary",
            "train_split": "train[:2%]",
            "valid_split": "train[-2%:]",
        }
        args = types.SimpleNamespace(
            hf_dataset=hf_args,
            test=False,
            train=True,
        )
        tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_PATH)
        train, valid, test = datasets.load_dataset(args, tokenizer)
        self.assertTrue(len(train) > 0)
        self.assertTrue(len(train[0]) > 0)
        self.assertTrue(len(valid) > 0)
        self.assertTrue(len(valid[0]) > 0)
        self.assertEqual(len(test), 0)

        args = types.SimpleNamespace(
            hf_dataset=[hf_args, hf_args],
            test=False,
            train=True,
        )
        train_double, valid_double, test_double = datasets.load_dataset(args, tokenizer)
        self.assertEqual(2 * len(train), len(train_double))
        self.assertEqual(2 * len(valid), len(valid_double))
        self.assertEqual(2 * len(test), len(test_double))


if __name__ == "__main__":
    unittest.main()

>>>> llms/tests/test_utils.py
# Copyright © 2024 Apple Inc.

import os
import tempfile
import unittest

import mlx.core as mx
import mlx.nn as nn
from mlx.utils import tree_flatten
from mlx_lm import utils

HF_MODEL_PATH = "mlx-community/Qwen1.5-0.5B-Chat-4bit"


class TestUtils(unittest.TestCase):

    @classmethod
    def setUpClass(cls):
        cls.test_dir_fid = tempfile.TemporaryDirectory()
        cls.test_dir = cls.test_dir_fid.name
        if not os.path.isdir(cls.test_dir):
            os.mkdir(cls.test_dir_fid.name)

    @classmethod
    def tearDownClass(cls):
        cls.test_dir_fid.cleanup()

    def test_load(self):
        model, _ = utils.load(HF_MODEL_PATH)

        model_lazy, _ = utils.load(HF_MODEL_PATH, lazy=True)

        mx.eval(model_lazy.parameters())

        p1 = model.layers[0].mlp.up_proj.weight
        p2 = model_lazy.layers[0].mlp.up_proj.weight
        self.assertTrue(mx.allclose(p1, p2))

    def test_make_shards(self):
        from mlx_lm.models import llama

        args = llama.ModelArgs(
            model_type="llama",
            hidden_size=2048,
            num_hidden_layers=32,
            intermediate_size=4096,
            num_attention_heads=32,
            rms_norm_eps=1e-5,
            vocab_size=30_000,
        )
        model = llama.Model(args)
        weights = tree_flatten(model.parameters())
        gb = sum(p.nbytes for _, p in weights) // 2**30
        shards = utils.make_shards(dict(weights), 1)
        self.assertTrue(gb <= len(shards) <= gb + 1)

    def test_quantize(self):
        from mlx_lm.models import llama

        args = llama.ModelArgs(
            model_type="llama",
            hidden_size=1024,
            num_hidden_layers=4,
            intermediate_size=2048,
            num_attention_heads=4,
            rms_norm_eps=1e-5,
            vocab_size=10_000,
        )
        model = llama.Model(args)
        weights, config = utils.quantize_model(model, {}, 64, 4)
        self.assertTrue("model.layers.2.mlp.up_proj.scales" in weights)
        self.assertTrue("model.layers.2.mlp.up_proj.biases" in weights)
        self.assertEqual(config["quantization"]["group_size"], 64)
        self.assertEqual(config["quantization"]["bits"], 4)

    def test_convert(self):
        mlx_path = os.path.join(self.test_dir, "mlx_model")

        utils.convert(HF_MODEL_PATH, mlx_path=mlx_path, quantize=True)
        model, _ = utils.load(mlx_path)
        self.assertTrue(isinstance(model.layers[0].mlp.up_proj, nn.QuantizedLinear))
        self.assertTrue(isinstance(model.layers[-1].mlp.up_proj, nn.QuantizedLinear))

        # Check model weights have right type
        mlx_path = os.path.join(self.test_dir, "mlx_model_bf16")
        utils.convert(HF_MODEL_PATH, mlx_path=mlx_path, dtype="bfloat16")
        model, _ = utils.load(mlx_path)

        self.assertEqual(model.layers[0].mlp.up_proj.weight.dtype, mx.bfloat16)
        self.assertEqual(model.layers[-1].mlp.up_proj.weight.dtype, mx.bfloat16)


if __name__ == "__main__":
    unittest.main()

>>>> llms/tests/test_tuner_utils.py
# Copyright © 2024 Apple Inc.

import sys
import unittest
from io import StringIO
from unittest.mock import MagicMock

import mlx.nn as nn
from mlx_lm.tuner.lora import LoRALinear
from mlx_lm.tuner.utils import print_trainable_parameters


class TestTunerUtils(unittest.TestCase):
    def setUp(self):
        self.capturedOutput = StringIO()
        sys.stdout = self.capturedOutput

    def tearDown(self):
        sys.stdout = sys.__stdout__

    def test_quantized_print_trainable_parameters(self):
        model = MagicMock()
        quantized_linear = MagicMock(spec=nn.QuantizedLinear)
        quantized_linear.weight = MagicMock(size=1e6)
        quantized_linear.bits = 8
        lora_linear = MagicMock(spec=LoRALinear)
        lora_linear.weight = MagicMock(size=2e6)
        lora_linear.parameters.return_value = [lora_linear.weight]

        linear = MagicMock(spec=nn.Linear)
        linear.weight = MagicMock(size=3e6)
        linear.parameters.return_value = [linear.weight]

        model.leaf_modules.return_value = {
            "quantized_linear": quantized_linear,
            "lora_linear": lora_linear,
            "linear": linear,
        }

        model.trainable_parameters.return_value = {
            "layer1.weight": MagicMock(size=1e6),
            "layer3.weight": MagicMock(size=2e6),
        }
        expected_output_8bits = "Trainable parameters: 33.333% (3.000M/9.000M)\n"
        print_trainable_parameters(model)
        self.assertEqual(self.capturedOutput.getvalue(), expected_output_8bits)
        self.capturedOutput.truncate(0)
        self.capturedOutput.seek(0)

        quantized_linear.weight = MagicMock(size=1e6)
        quantized_linear.bits = 4
        expected_output_4bits = "Trainable parameters: 23.077% (3.000M/13.000M)\n"
        print_trainable_parameters(model)
        self.assertEqual(self.capturedOutput.getvalue(), expected_output_4bits)
        self.capturedOutput.truncate(0)
        self.capturedOutput.seek(0)

    def test_print_trainable_parameters(self):
        model = MagicMock()
        linear1 = MagicMock(spec=nn.Linear)
        linear1.weight = MagicMock(size=1e6)
        linear1.parameters.return_value = [linear1.weight]
        linear2 = MagicMock(spec=nn.Linear)
        linear2.weight = MagicMock(size=2e6)
        linear2.parameters.return_value = [linear2.weight]
        lora_linear = MagicMock(spec=LoRALinear)
        lora_linear.weight = MagicMock(size=3e6)
        lora_linear.parameters.return_value = [lora_linear.weight]
        model.leaf_modules.return_value = {
            "linear1": linear1,
            "linear2": linear2,
            "lora_linear": lora_linear,
        }

        model.trainable_parameters.return_value = {
            "layer1.weight": MagicMock(size=1e6),
            "layer3.weight": MagicMock(size=2e6),
        }
        expected_output = "Trainable parameters: 50.000% (3.000M/6.000M)\n"
        print_trainable_parameters(model)
        self.assertEqual(self.capturedOutput.getvalue(), expected_output)


if __name__ == "__main__":
    unittest.main()

>>>> llms/mixtral/mixtral.py
# Copyright © 2023 Apple Inc.

import argparse
import glob
import json
from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional, Tuple

import mlx.core as mx
import mlx.nn as nn
from mlx.utils import tree_unflatten
from sentencepiece import SentencePieceProcessor


@dataclass
class ModelArgs:
    dim: int
    n_layers: int
    head_dim: int
    hidden_dim: int
    n_heads: int
    n_kv_heads: int
    norm_eps: float
    vocab_size: int
    moe: dict = None


class Attention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args

        self.n_heads: int = args.n_heads
        self.n_kv_heads: int = args.n_kv_heads

        self.repeats = self.n_heads // self.n_kv_heads

        self.scale = self.args.head_dim**-0.5

        self.wq = nn.Linear(args.dim, args.n_heads * args.head_dim, bias=False)
        self.wk = nn.Linear(args.dim, args.n_kv_heads * args.head_dim, bias=False)
        self.wv = nn.Linear(args.dim, args.n_kv_heads * args.head_dim, bias=False)
        self.wo = nn.Linear(args.n_heads * args.head_dim, args.dim, bias=False)
        self.rope = nn.RoPE(args.head_dim, traditional=True, base=1000000)

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Tuple[mx.array, mx.array]] = None,
    ) -> mx.array:
        B, L, D = x.shape

        queries, keys, values = self.wq(x), self.wk(x), self.wv(x)

        # Prepare the queries, keys and values for the attention computation
        queries = queries.reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)
        keys = keys.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)
        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)

        if cache is not None:
            key_cache, value_cache = cache
            queries = self.rope(queries, offset=key_cache.shape[2])
            keys = self.rope(keys, offset=key_cache.shape[2])
            keys = mx.concatenate([key_cache, keys], axis=2)
            values = mx.concatenate([value_cache, values], axis=2)
        else:
            queries = self.rope(queries)
            keys = self.rope(keys)

        output = mx.fast.scaled_dot_product_attention(
            queries, keys, values, scale=self.scale, mask=mask
        )
        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)
        return self.wo(output), (keys, values)


class FeedForward(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        self.w1 = nn.Linear(args.dim, args.hidden_dim, bias=False)
        self.w2 = nn.Linear(args.hidden_dim, args.dim, bias=False)
        self.w3 = nn.Linear(args.dim, args.hidden_dim, bias=False)

    def __call__(self, x) -> mx.array:
        return self.w2(nn.silu(self.w1(x)) * self.w3(x))


class MOEFeedForward(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        self.num_experts = args.moe["num_experts"]
        self.num_experts_per_tok = args.moe["num_experts_per_tok"]
        self.experts = [FeedForward(args) for _ in range(self.num_experts)]
        self.gate = nn.Linear(args.dim, self.num_experts, bias=False)

    def __call__(self, x) -> mx.array:
        ne = self.num_experts_per_tok
        orig_shape = x.shape
        x = x.reshape(-1, x.shape[-1])

        gates = self.gate(x)
        inds = mx.argpartition(-gates, kth=ne - 1, axis=-1)[:, :ne]
        scores = mx.softmax(
            mx.take_along_axis(gates, inds, axis=-1).astype(mx.float32),
            axis=-1,
        ).astype(gates.dtype)

        y = []
        for xt, st, it in zip(x, scores, inds.tolist()):
            yt = mx.concatenate([self.experts[e](xt)[:, None] for e in it], axis=-1)
            yt = (yt * st).sum(axis=-1)
            y.append(yt[None, :])
        y = mx.concatenate(y)

        return y.reshape(orig_shape)


class MOETransformerBlock(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.n_heads = args.n_heads
        self.dim = args.dim
        self.attention = Attention(args)
        self.feed_forward = MOEFeedForward(args=args)
        self.attention_norm = nn.RMSNorm(args.dim, eps=args.norm_eps)
        self.ffn_norm = nn.RMSNorm(args.dim, eps=args.norm_eps)
        self.args = args

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Tuple[mx.array, mx.array]] = None,
    ) -> mx.array:
        r, cache = self.attention(self.attention_norm(x), mask, cache)
        h = x + r
        r = self.feed_forward(self.ffn_norm(h))
        out = h + r
        return out, cache


class Mixtral(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.vocab_size = args.vocab_size
        self.n_layers = args.n_layers
        assert self.vocab_size > 0
        self.tok_embeddings = nn.Embedding(args.vocab_size, args.dim)
        self.layers = [MOETransformerBlock(args=args) for _ in range(args.n_layers)]
        self.norm = nn.RMSNorm(args.dim, eps=args.norm_eps)
        self.output = nn.Linear(args.dim, args.vocab_size, bias=False)

    def __call__(
        self,
        inputs: mx.array,
        cache=None,
    ):
        h = self.tok_embeddings(inputs)

        mask = None
        T = h.shape[1]
        if T > 1:
            mask = nn.MultiHeadAttention.create_additive_causal_mask(T)
            mask = mask.astype(h.dtype)

        if cache is None:
            cache = [None] * len(self.layers)

        for e, layer in enumerate(self.layers):
            h, cache[e] = layer(h, mask, cache[e])

        return self.output(self.norm(h[:, T - 1 : T, :])), cache


class Tokenizer:
    def __init__(self, model_path: str):
        assert Path(model_path).exists(), model_path
        self._model = SentencePieceProcessor(model_file=model_path)
        self._sep = "▁"
        assert self._model.vocab_size() == self._model.get_piece_size()

    @property
    def eos_id(self) -> int:
        return self._model.eos_id()

    @property
    def pad_id(self) -> int:
        return self._model.pad_id()

    def encode(self, s: str) -> List[int]:
        return [self._model.bos_id(), *self._model.encode(s)]

    def decode(self, t: List[int]) -> str:
        out = self._model.decode(t)
        if t and self._model.id_to_piece(t[0])[0] == self._sep:
            return " " + out
        return out


def load_model(folder: str):
    model_path = Path(folder)
    tokenizer = Tokenizer(str(model_path / "tokenizer.model"))
    with open(model_path / "config.json", "r") as f:
        config = json.loads(f.read())
        config.pop("model_type", None)
        quantization = config.pop("quantization", None)
        model_args = ModelArgs(**config)
    weight_files = glob.glob(str(model_path / "weights.*.npz"))
    weights = {}
    for wf in weight_files:
        weights.update(mx.load(wf).items())
    weights = tree_unflatten(list(weights.items()))
    model = Mixtral(model_args)
    if quantization is not None:
        nn.quantize(model, **quantization)

    model.update(weights)
    return model, tokenizer


def generate(prompt: mx.array, model: Mixtral, temp: Optional[float] = 0.0):
    def sample(logits):
        if temp == 0:
            return mx.argmax(logits, axis=-1)
        else:
            return mx.random.categorical(logits * (1 / temp))

    logits, cache = model(prompt[None])
    y = sample(logits[:, -1, :])
    yield y

    while True:
        logits, cache = model(y[:, None], cache)
        y = sample(logits.squeeze(1))
        yield y


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Mixtral inference script")
    parser.add_argument(
        "--model-path",
        type=str,
        default="mlx_model",
        help="The path to the model weights, tokenizer, and config",
    )
    parser.add_argument(
        "--prompt",
        help="The message to be processed by the model",
        default="In the beginning the Universe was created.",
    )
    parser.add_argument(
        "--max-tokens",
        "-m",
        type=int,
        default=100,
        help="Maximum number of tokens to generate",
    )
    parser.add_argument(
        "--temp",
        help="The sampling temperature.",
        type=float,
        default=0.0,
    )
    parser.add_argument("--seed", type=int, default=0, help="The PRNG seed")

    args = parser.parse_args()

    mx.random.seed(args.seed)
    print("[INFO] Loading model from disk.")
    model, tokenizer = load_model(args.model_path)

    print("[INFO] Starting generation...")

    print(args.prompt, end="", flush=True)
    prompt = mx.array(tokenizer.encode(args.prompt))
    tokens = []
    for token, _ in zip(generate(prompt, model, args.temp), range(args.max_tokens)):
        tokens.append(token)

        if (len(tokens) % 10) == 0:
            mx.eval(tokens)
            eos_index = next(
                (i for i, t in enumerate(tokens) if t.item() == tokenizer.eos_id), None
            )
            if eos_index is not None:
                tokens = tokens[:eos_index]
            s = tokenizer.decode([t.item() for t in tokens])
            print(s, end="", flush=True)
            tokens = []
            if eos_index is not None:
                break

    mx.eval(tokens)
    s = tokenizer.decode([t.item() for t in tokens])
    print(s, flush=True)

>>>> llms/mixtral/README.md
## Mixtral 8x7B

Run the Mixtral[^mixtral] 8x7B mixture-of-experts (MoE) model in MLX on Apple silicon.

This example also supports the instruction fine-tuned Mixtral model.[^instruct]

Note, for 16-bit precision this model needs a machine with substantial RAM (~100GB) to run.

### Setup

Install [Git Large File
Storage](https://docs.github.com/en/repositories/working-with-files/managing-large-files/installing-git-large-file-storage).
For example with Homebrew:

```
brew install git-lfs
```

Download the models from Hugging Face:

For the base model use:

```
export MIXTRAL_MODEL=Mixtral-8x7B-v0.1
```

For the instruction fine-tuned model use:

```
export MIXTRAL_MODEL=Mixtral-8x7B-Instruct-v0.1
```

Then run:

```
GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/mistralai/${MIXTRAL_MODEL}/
cd $MIXTRAL_MODEL/ && 
  git lfs pull --include "consolidated.*.pt" && 
  git lfs pull --include "tokenizer.model"
```

Now from `mlx-exmaples/mixtral` convert and save the weights as NumPy arrays so
MLX can read them:

```
python convert.py --torch-path $MIXTRAL_MODEL/
```

To generate a 4-bit quantized model, use ``-q``. For a full list of options:

```
python convert.py --help
```

By default, the conversion script will make the directory `mlx_model` and save
the converted `weights.npz`, `tokenizer.model`, and `config.json` there.


### Generate

As easy as:

```
python mixtral.py --model-path mlx_model
```

For more options including how to prompt the model, run:

```
python mixtral.py --help
```

For the Instruction model, make sure to follow the prompt format:

```
[INST] Instruction prompt [/INST]
```

[^mixtral]: Refer to Mistral's [blog post](https://mistral.ai/news/mixtral-of-experts/) and the [Hugging Face blog post](https://huggingface.co/blog/mixtral) for more details.
[^instruc]: Refer to the [Hugging Face repo](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) for more
details

>>>> llms/mixtral/convert.py
# Copyright © 2023 Apple Inc.

import argparse
import copy
import glob
import json
import shutil
from pathlib import Path

import mlx.core as mx
import mlx.nn as nn
import numpy as np
import torch
from mixtral import Mixtral, ModelArgs
from mlx.utils import tree_flatten, tree_map, tree_unflatten


def convert(tf, config):
    def convert_single(k, v):
        v = v.to(torch.float16).numpy()
        if "block_sparse_moe" not in k:
            return [(k, v)]
        if "gate" in k:
            return [(k.replace("block_sparse_moe", "feed_forward"), v)]

        # From: layers.N.block_sparse_moe.w
        # To: layers.N.experts.M.w
        num_experts = config["moe"]["num_experts"]
        key_path = k.split(".")
        v = np.split(v, num_experts, axis=0)
        if key_path[-1] == "w2":
            v = [u.T for u in v]

        w_name = key_path.pop()
        key_path[-1] = "feed_forward.experts"
        return [
            (".".join(key_path + [str(e), w_name, "weight"]), u)
            for e, u in enumerate(v)
        ]

    state = torch.load(tf)
    weights = {}
    for k, v in state.items():
        weights.update(convert_single(k, v))
    return weights


def quantize(weights, config, args):
    quantized_config = copy.deepcopy(config)

    # Load the model and update with the subset of weights:
    config.pop("quantization", None)
    model = Mixtral(ModelArgs(**config))
    all_weights = dict(tree_flatten(model.parameters()))

    weights = tree_map(mx.array, weights)

    all_weights.update(weights)
    all_weights = tree_unflatten(list(all_weights.items()))
    model.update(all_weights)

    # Quantize the model:
    nn.quantize(
        model,
        args.q_group_size,
        args.q_bits,
    )

    # Extract the subset of quantized weights:
    all_weights = dict(tree_flatten(model.parameters()))
    quantized_weights = {}
    for k, v in all_weights.items():
        if k not in weights:
            continue
        quantized_weights[k] = v
        prefix = k.split(".")[:-1]
        for qw in ["scales", "biases"]:
            if (k := ".".join(prefix + [qw])) in all_weights:
                quantized_weights[k] = all_weights[k]

    # Update the config:
    quantized_config["quantization"] = {
        "group_size": args.q_group_size,
        "bits": args.q_bits,
    }
    return quantized_weights, quantized_config


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Convert Mixtral weights to MLX.")
    parser.add_argument(
        "--torch-path",
        type=str,
        default="Mixtral-8x7B-v0.1",
        help="The path to the PyTorch model.",
    )
    parser.add_argument(
        "--mlx-path",
        type=str,
        default="mlx_model",
        help="The path to save the MLX model.",
    )
    parser.add_argument(
        "-q",
        "--quantize",
        help="Generate a quantized model.",
        action="store_true",
    )
    parser.add_argument(
        "--q-group-size",
        help="Group size for quantization.",
        type=int,
        default=64,
    )
    parser.add_argument(
        "--q-bits",
        help="Bits per weight for quantization.",
        type=int,
        default=4,
    )
    args = parser.parse_args()
    torch_path = Path(args.torch_path)
    mlx_path = Path(args.mlx_path)
    mlx_path.mkdir(parents=True, exist_ok=True)

    with open("params.json") as fid:
        config = json.load(fid)

    # Copy tokenizer
    shutil.copyfile(
        str(torch_path / "tokenizer.model"),
        str(mlx_path / "tokenizer.model"),
    )

    # Convert and save model in shards
    torch_files = glob.glob(str(torch_path / "consolidated.*.pt"))
    torch_files = sorted(torch_files, key=lambda tf: int(tf.split(".")[-2]))
    for e, tf in enumerate(torch_files):
        print(f"[INFO] Converting file {e + 1}/{len(torch_files)}")
        weights = convert(tf, config)
        if args.quantize:
            print("[INFO] Quantizing")
            weights, config = quantize(weights, config, args)
        np.savez(str(mlx_path / f"weights.{e}.npz"), **weights)

    # Save updated config
    with open(mlx_path / "config.json", "w") as f:
        config["model_type"] = "mixtral"
        json.dump(config, f, indent=4)

>>>> llms/speculative_decoding/main.py
import argparse
import time

import mlx.core as mx
from decoder import SpeculativeDecoder
from mlx.utils import tree_unflatten
from model import Model
from transformers import T5Config


def load_model(model_name: str):
    config = T5Config.from_pretrained(model_name)
    model = Model(config)
    weights = mx.load(f"{model_name}.npz")
    weights = tree_unflatten(list(weights.items()))
    model.update(weights)
    mx.eval(model.parameters())
    return model


def main(args):
    mx.random.seed(args.seed)

    spec_decoder = SpeculativeDecoder(
        model=load_model(args.model_name),
        draft_model=load_model(args.draft_model_name),
        tokenizer=args.model_name,
        delta=args.delta,
        num_draft=args.num_draft,
    )

    tic = time.time()
    print(args.prompt)
    if args.regular_decode:
        spec_decoder.generate(args.prompt, max_tokens=args.max_tokens)
    else:
        stats = spec_decoder.speculative_decode(args.prompt, max_tokens=args.max_tokens)
        print("=" * 10)
        print(f"Accepted {stats['n_accepted']} / {stats['n_draft']}.")
        print(f"Decoding steps {stats['n_steps']}.")

    toc = time.time()
    print("=" * 10)
    print(f"Full generation time {toc - tic:.3f}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Convert Llama weights to MLX")
    parser.add_argument(
        "--num-draft",
        type=int,
        default=5,
        help="Number of draft tokens to use per decoding step.",
    )
    parser.add_argument(
        "--model-name",
        help="Name of the model.",
        default="t5-small",
    )
    parser.add_argument(
        "--draft-model-name",
        help="Name of the draft model.",
        default="t5-small",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=0,
        help="PRNG seed.",
    )
    parser.add_argument(
        "--max-tokens",
        "-m",
        type=int,
        default=100,
        help="Maximum number of tokens to generate.",
    )
    parser.add_argument(
        "--prompt",
        default="translate English to French: Let's go to the store and buy some groceries including eggs, avocadoes, and bread.",
        help="The prompt processed by the model.",
    )
    parser.add_argument(
        "--delta",
        type=float,
        default=0.1,
        help="Lenience for accepting the proposal tokens.",
    )
    parser.add_argument(
        "--regular-decode",
        action="store_true",
        help="Use regular decoding instead of speculative decoding.",
    )
    args = parser.parse_args()
    main(args)

>>>> llms/speculative_decoding/README.md
# Speculative Decoding

This example implements speculative decoding with the T5 model for text
generation.[^1][^2] Speculative decoding uses a smaller draft model to propose
several tokens, and a larger model to decide which tokens to accept. The
distribution of the generated text is identical to what the larger model would
produce on its own, but with far fewer forward passes of the large model since
it can evaluate the draft tokens in parallel.

### Setup

First, install the requirements:

```
cd speculative_decoding
pip install -r requirements.txt
```

Then convert the model and the draft model. We'll use T5-XXL (11B parameters)
for the main model. Convert it with:

```
python convert.py --model t5-11b
```

We'll use T5-small for the draft model. Convert it with:

```
python convert.py --model t5-small
```

### Run

You can run with the default arguments:

```
python main.py
```

To see a full list of options use:
```
python main.py --help
```

### Notes

Speculative decoding works well when most of the tokens from the draft model
are accepted by the larger model. That's more likely to happen if the models
are trained on similar data.

One way to increase the chance of accepting a draft token is with the parameter
`--delta`. This parameter can be in the range $[0, 1]$. If it is $1$ then all
the draft tokens will be accepted by the model. If it is $0$, then only draft
tokens that match the original acceptance criterion are kept.[^1] Values
closer to $1$ increase the chance that a draft token is accepted.

Conversely, the fewer draft tokens accepted by the main model, the more
expensive speculative decoding is. You can use `--num-draft` to tune the number
of draft tokens per model evaluation to reduce the number of discarded
draft tokens. Decreasing `--num-draft` will decrease the number of discarded
draft tokens at the expense of more large model evaluations.

[^1]: See the paper [Fast Inference from Transformers via Speculative
Decoding](https://arxiv.org/abs/2211.17192)
[^2]: For more information on T5 see the [original paper](https://arxiv.org/abs/1910.10683)
   or the [Hugging Face page](https://huggingface.co/docs/transformers/model_doc/t5).

>>>> llms/speculative_decoding/model.py
from typing import List, Optional, Tuple

import mlx.core as mx
import mlx.nn as nn
import numpy as np
from mlx.utils import tree_map
from transformers import T5Config


def _relative_position_bucket(
    relative_position, bidirectional=True, num_buckets=32, max_distance=128
):
    """
    Adapted from HF Tensorflow:
    https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/modeling_t5.py

    Translate relative position to a bucket number for relative attention. The relative position is defined as
    memory_position - query_position, i.e. the distance in tokens from the attending position to the attended-to
    position. If bidirectional=False, then positive relative positions are invalid. We use smaller buckets for
    small absolute relative_position and larger buckets for larger absolute relative_positions. All relative
    positions >=max_distance map to the same bucket. All relative positions <=-max_distance map to the same bucket.
    This should allow for more graceful generalization to longer sequences than the model has been trained on

    Args:
        relative_position: an int32 Tensor
        bidirectional: a boolean - whether the attention is bidirectional
        num_buckets: an integer
        max_distance: an integer

    Returns:
        a Tensor with the same shape as relative_position, containing int32 values in the range [0, num_buckets)
    """
    relative_buckets = 0
    if bidirectional:
        num_buckets //= 2
        relative_buckets += (relative_position > 0).astype(mx.int16) * num_buckets
        relative_position = mx.abs(relative_position)
    else:
        relative_position = -mx.minimum(
            relative_position, mx.zeros_like(relative_position)
        )
    # now relative_position is in the range [0, inf)

    # half of the buckets are for exact increments in positions
    max_exact = num_buckets // 2
    is_small = relative_position < max_exact

    # The other half of the buckets are for logarithmically bigger bins in positions up to max_distance
    scale = (num_buckets - max_exact) / np.log(max_distance / max_exact)
    relative_position_if_large = max_exact + (
        mx.log(relative_position.astype(mx.float32) / max_exact) * scale
    ).astype(mx.int16)
    relative_position_if_large = mx.minimum(relative_position_if_large, num_buckets - 1)
    relative_buckets += mx.where(
        is_small, relative_position, relative_position_if_large
    )
    return relative_buckets


class RelativePositionBias(nn.Module):
    def __init__(self, config: T5Config, bidirectional: bool):
        self.bidirectional = bidirectional
        self.num_buckets = config.relative_attention_num_buckets
        self.max_distance = config.relative_attention_max_distance
        self.n_heads = config.num_heads
        self.embeddings = nn.Embedding(
            config.relative_attention_num_buckets, config.num_heads
        )

    def __call__(self, query_length: int, key_length: int, offset: int = 0):
        """Compute binned relative position bias"""
        context_position = mx.arange(offset, query_length)[:, None]
        memory_position = mx.arange(key_length)[None, :]

        # shape (query_length, key_length)
        relative_position = memory_position - context_position
        relative_position_bucket = _relative_position_bucket(
            relative_position,
            bidirectional=self.bidirectional,
            num_buckets=self.num_buckets,
            max_distance=self.max_distance,
        )

        # shape (query_length, key_length, num_heads)
        values = self.embeddings(relative_position_bucket)

        # shape (num_heads, query_length, key_length)
        return values.transpose(2, 0, 1)


class MultiHeadAttention(nn.Module):
    def __init__(self, config: T5Config):
        super().__init__()
        inner_dim = config.d_kv * config.num_heads
        self.num_heads = config.num_heads
        self.query_proj = nn.Linear(config.d_model, inner_dim, bias=False)
        self.key_proj = nn.Linear(config.d_model, inner_dim, bias=False)
        self.value_proj = nn.Linear(config.d_model, inner_dim, bias=False)
        self.out_proj = nn.Linear(inner_dim, config.d_model, bias=False)

    def __call__(
        self,
        queries: mx.array,
        keys: mx.array,
        values: mx.array,
        mask: Optional[mx.array],
        cache: Optional[Tuple[mx.array, mx.array]] = None,
    ) -> Tuple[mx.array, Tuple[mx.array, mx.array]]:
        queries = self.query_proj(queries)
        keys = self.key_proj(keys)
        values = self.value_proj(values)

        num_heads = self.num_heads
        B, L, _ = queries.shape
        _, S, _ = keys.shape
        queries = queries.reshape(B, L, num_heads, -1).transpose(0, 2, 1, 3)
        keys = keys.reshape(B, S, num_heads, -1).transpose(0, 2, 1, 3)
        values = values.reshape(B, S, num_heads, -1).transpose(0, 2, 1, 3)

        if cache is not None:
            key_cache, value_cache = cache
            keys = mx.concatenate([key_cache, keys], axis=2)
            values = mx.concatenate([value_cache, values], axis=2)

        # Dimensions are [batch x num heads x sequence x hidden dim]
        scores = queries @ keys.transpose(0, 1, 3, 2)
        if mask is not None:
            scores = scores + mask.astype(scores.dtype)

        scores = mx.softmax(scores.astype(mx.float32), axis=-1).astype(scores.dtype)
        values_hat = (scores @ values).transpose(0, 2, 1, 3).reshape(B, L, -1)
        return self.out_proj(values_hat), (keys, values)


class DenseActivation(nn.Module):
    def __init__(self, config: T5Config):
        super().__init__()
        mlp_dims = config.d_ff or config.d_model * 4
        self.gated = config.feed_forward_proj.startswith("gated")
        if self.gated:
            self.wi_0 = nn.Linear(config.d_model, mlp_dims, bias=False)
            self.wi_1 = nn.Linear(config.d_model, mlp_dims, bias=False)
        else:
            self.wi = nn.Linear(config.d_model, mlp_dims, bias=False)
        self.wo = nn.Linear(mlp_dims, config.d_model, bias=False)
        activation = config.feed_forward_proj.removeprefix("gated-")
        if activation == "relu":
            self.act = nn.relu
        elif activation == "gelu":
            self.act = nn.gelu
        elif activation == "silu":
            self.act = nn.silu
        else:
            raise ValueError(f"Unknown activation: {activation}")

    def __call__(self, x):
        if self.gated:
            hidden_act = self.act(self.wi_0(x))
            hidden_linear = self.wi_1(x)
            x = hidden_act * hidden_linear
        else:
            x = self.act(self.wi(x))
        return self.wo(x)


class TransformerEncoderLayer(nn.Module):
    def __init__(self, config: T5Config):
        super().__init__()
        self.attention = MultiHeadAttention(config)
        self.ln1 = nn.RMSNorm(config.d_model, eps=config.layer_norm_epsilon)
        self.ln2 = nn.RMSNorm(config.d_model, eps=config.layer_norm_epsilon)
        self.dense = DenseActivation(config)

    def __call__(self, x, mask):
        y = self.ln1(x)
        y, _ = self.attention(y, y, y, mask=mask)
        x = x + y

        y = self.ln2(x)
        y = self.dense(y)
        return x + y


class TransformerEncoder(nn.Module):
    def __init__(self, config: T5Config):
        super().__init__()
        self.layers = [
            TransformerEncoderLayer(config) for i in range(config.num_layers)
        ]
        self.ln = nn.RMSNorm(config.d_model, eps=config.layer_norm_epsilon)
        self.relative_attention_bias = RelativePositionBias(config, bidirectional=True)

    def __call__(self, x: mx.array):
        pos_bias = self.relative_attention_bias(x.shape[1], x.shape[1])
        for layer in self.layers:
            x = layer(x, mask=pos_bias)
        return self.ln(x)


class TransformerDecoderLayer(nn.Module):
    def __init__(self, config: T5Config):
        super().__init__()
        self.self_attention = MultiHeadAttention(config)
        self.cross_attention = MultiHeadAttention(config)
        self.ln1 = nn.RMSNorm(config.d_model, eps=config.layer_norm_epsilon)
        self.ln2 = nn.RMSNorm(config.d_model, eps=config.layer_norm_epsilon)
        self.ln3 = nn.RMSNorm(config.d_model, eps=config.layer_norm_epsilon)
        self.dense = DenseActivation(config)

    def __call__(
        self,
        x: mx.array,
        memory: mx.array,
        mask: mx.array,
        memory_mask: mx.array,
        cache: Optional[List[Tuple[mx.array, mx.array]]] = None,
    ):
        y = self.ln1(x)
        y, cache = self.self_attention(y, y, y, mask, cache)
        x = x + y

        y = self.ln2(x)
        y, _ = self.cross_attention(y, memory, memory, memory_mask)
        x = x + y

        y = self.ln3(x)
        y = self.dense(y)
        x = x + y

        return x, cache


def create_additive_causal_mask(N: int, offset: int = 0):
    rinds = mx.arange(offset + N)
    linds = mx.arange(offset, offset + N) if offset else rinds
    mask = linds[:, None] < rinds[None]
    return mask * -1e9


class TransformerDecoder(nn.Module):
    def __init__(self, config: T5Config):
        super().__init__()
        n_layers = getattr(config, "num_decoder_layers", config.num_layers)
        self.layers = [TransformerDecoderLayer(config) for i in range(n_layers)]
        self.ln = nn.RMSNorm(config.d_model, eps=config.layer_norm_epsilon)
        self.relative_attention_bias = RelativePositionBias(config, bidirectional=False)

    def __call__(self, x, memory, cache=None):
        if cache[0] is not None:
            offset = cache[0][0].shape[2]
        else:
            offset = 0

        T = x.shape[1]
        if T > 1:
            mask = create_additive_causal_mask(T, offset)
        else:
            mask = None

        pos_bias = self.relative_attention_bias(T + offset, T + offset, offset=offset)
        if mask is not None:
            mask += pos_bias
        else:
            mask = pos_bias

        for e, layer in enumerate(self.layers):
            x, cache[e] = layer(x, memory, mask, None, cache=cache[e])
        x = self.ln(x)

        return x, cache


class OutputHead(nn.Module):
    def __init__(self, config: T5Config):
        self.linear = nn.Linear(config.d_model, config.vocab_size, bias=False)

    def __call__(self, inputs):
        return self.linear(inputs)


class Model(nn.Module):
    def __init__(self, config: T5Config):
        self.wte = nn.Embedding(config.vocab_size, config.d_model)
        self.encoder = TransformerEncoder(config)
        self.decoder = TransformerDecoder(config)
        self.tie_word_embeddings = config.tie_word_embeddings
        if not self.tie_word_embeddings:
            self.lm_head = OutputHead(config)
        self.model_dim = config.d_model
        self.reset_cache()

    def encode(self, inputs: mx.array):
        return self.encoder(self.wte(inputs))

    def truncate_cache(self, num_to_truncate):
        if num_to_truncate <= 0:
            return
        cache_length = self.cache[0][0].shape[2]
        if num_to_truncate < cache_length:
            self.cache = tree_map(lambda x: x[:, :, :-num_to_truncate, :], self.cache)
        else:
            self.reset_cache()

    def reset_cache(self):
        self.cache = [None] * len(self.decoder.layers)

    def decode(
        self,
        inputs: mx.array,
        memory: mx.array,
    ):
        inputs = self.wte(inputs)
        y, self.cache = self.decoder(inputs, memory=memory, cache=self.cache)
        if not self.tie_word_embeddings:
            y *= self.model_dim**-0.5
            y = self.lm_head(y)
        else:
            y = y @ self.wte.weight.T
        return y

    def __call__(
        self,
        inputs: mx.array,
        decoder_inputs: mx.array,
    ):
        return self.decode(decoder_inputs, self.encode(inputs))[0]

>>>> llms/speculative_decoding/convert.py
import numpy as np
from transformers import T5ForConditionalGeneration

SHARED_REPLACEMENT_PATTERNS = [
    (".block.", ".layers."),
    (".k.", ".key_proj."),
    (".o.", ".out_proj."),
    (".q.", ".query_proj."),
    (".v.", ".value_proj."),
    ("shared.", "wte."),
    ("lm_head.", "lm_head.linear."),
    (".layer.0.layer_norm.", ".ln1."),
    (".layer.1.layer_norm.", ".ln2."),
    (".layer.2.layer_norm.", ".ln3."),
    (".final_layer_norm.", ".ln."),
    (
        "layers.0.layer.0.SelfAttention.relative_attention_bias.",
        "relative_attention_bias.embeddings.",
    ),
]

ENCODER_REPLACEMENT_PATTERNS = [
    (".layer.0.SelfAttention.", ".attention."),
    (".layer.1.DenseReluDense.", ".dense."),
]

DECODER_REPLACEMENT_PATTERNS = [
    (".layer.0.SelfAttention.", ".self_attention."),
    (".layer.1.EncDecAttention.", ".cross_attention."),
    (".layer.2.DenseReluDense.", ".dense."),
]


def replace_key(key: str) -> str:
    for old, new in SHARED_REPLACEMENT_PATTERNS:
        key = key.replace(old, new)
    if key.startswith("encoder."):
        for old, new in ENCODER_REPLACEMENT_PATTERNS:
            key = key.replace(old, new)
    elif key.startswith("decoder."):
        for old, new in DECODER_REPLACEMENT_PATTERNS:
            key = key.replace(old, new)
    return key


def convert(model_name, dtype):
    dtype = getattr(np, dtype)
    model = T5ForConditionalGeneration.from_pretrained(model_name, torch_dtype="auto")
    weights = {
        replace_key(k): v.numpy().astype(dtype) for k, v in model.state_dict().items()
    }
    file_name = model_name.replace("/", "-")
    print(f"Saving weights to {file_name}.npz")
    np.savez(f"{file_name}.npz", **weights)


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Convert T5 weights to MLX")
    parser.add_argument(
        "--model",
        type=str,
        help="Name of the T5 model.",
        default="t5-small",
    )
    parser.add_argument(
        "--dtype",
        help="The model data type.",
        type=str,
        choices=["float16", "float32"],
        default="float32",
    )
    args = parser.parse_args()
    convert(args.model, args.dtype)

>>>> llms/speculative_decoding/decoder.py
from typing import List

import mlx.core as mx
import transformers
from model import Model


class Tokenizer:
    def __init__(self, model_name: str):
        self._tokenizer = transformers.AutoTokenizer.from_pretrained(
            model_name,
            legacy=False,
            model_max_length=512,
        )
        self._decoder_start_id = 0

    @property
    def eos_id(self) -> int:
        return self._tokenizer.eos_token_id

    @property
    def decoder_start_id(self) -> int:
        return self._decoder_start_id

    def encode(self, s: str) -> mx.array:
        return mx.array(
            self._tokenizer(
                s,
                return_tensors="np",
                return_attention_mask=False,
            )[
                "input_ids"
            ].squeeze(0)
        )

    def decode(self, t: List[int]) -> str:
        return self._tokenizer.decode(t)


class SpeculativeDecoder:
    def __init__(
        self,
        model: Model,
        draft_model: Model,
        tokenizer: str,
        num_draft: int = 5,
        delta: float = 0.0,
    ):
        self.tokenizer = Tokenizer(tokenizer)
        self.model = model
        self.draft_model = draft_model
        self.num_draft = num_draft
        self.delta = delta

    def _generate(
        self,
        x: mx.array,
        memory: mx.array,
        draft: bool = False,
    ):
        model = self.draft_model if draft else self.model
        while True:
            logits = model.decode(x[None], memory)[0, -1]
            x = mx.argmax(logits, keepdims=True)
            lognorm = mx.logsumexp(logits.astype(mx.float32))
            logprob = logits[x] - lognorm
            yield x, logprob

    def generate(
        self,
        prompt,
        max_tokens: int = 100,
    ):
        memory = self.model.encode(self.tokenizer.encode(prompt)[None])
        x = mx.array([self.tokenizer.decoder_start_id])
        skip = 0
        outputs = []
        for (token, _), n in zip(self._generate(x, memory), range(max_tokens)):
            if token == self.tokenizer.eos_id:
                break
            outputs.append(token.item())
            if (n + 1) % 10 == 0:
                str_output = self.tokenizer.decode(outputs)
                print(str_output[skip:], end="", flush=True)
                skip = len(str_output)

        print(self.tokenizer.decode(outputs)[skip:], end="", flush=True)
        print()
        self.model.reset_cache()

    def _get_num_accept(self, draft_tokens, draft_probs, model_logits):
        # accept_toks = mx.argmax(model_logits, axis=-1) == draft_tokens
        model_probs = mx.take_along_axis(
            model_logits,
            draft_tokens[:, None],
            axis=-1,
        ).squeeze(-1)
        model_probs -= mx.logsumexp(model_logits.astype(mx.float32), axis=-1)
        unis = mx.random.uniform(shape=(draft_tokens.size,))
        log_unis = mx.log(mx.maximum(unis - self.delta, 0.0))
        accept_toks = log_unis <= ((model_probs - draft_probs))
        num_to_accept = (accept_toks.tolist() + [False]).index(False)
        return num_to_accept

    def speculative_decode(
        self,
        prompt,
        max_tokens: int = 100,
    ):
        def sample(logits):
            return mx.argmax(logits, axis=-1)

        prompt = mx.array(self.tokenizer.encode(prompt), mx.uint32)[None]
        memory = self.model.encode(prompt)
        draft_memory = self.draft_model.encode(prompt)

        tokens = mx.array([self.tokenizer.decoder_start_id])

        n_steps = 0
        ntoks = 0
        n_accepted = 0
        n_draft = 0

        outputs = []
        skip = 0
        draft_inputs = tokens
        inputs = tokens
        while True:
            # For each decoding step: generate n tokens from a draft model
            draft_tokens = []
            draft_probs = []
            for _, (t, p) in zip(
                range(ntoks, min(ntoks + self.num_draft, max_tokens)),
                self._generate(draft_inputs, draft_memory, draft=True),
            ):
                draft_tokens.append(t)
                draft_probs.append(p)
                if t.item() == self.tokenizer.eos_id:
                    break

            # Verify the draft tokens with the last verified token:
            draft_tokens = mx.concatenate(draft_tokens)
            draft_probs = mx.concatenate(draft_probs)
            verify_tokens = mx.concatenate([inputs, draft_tokens])
            logits = self.model.decode(
                verify_tokens[None, :],
                memory,
            ).squeeze(0)

            # Only keep samples that match the draft:
            num_to_accept = self._get_num_accept(
                draft_tokens,
                draft_probs,
                logits[:-1],
            )
            new_tokens = draft_tokens[:num_to_accept]
            # Get the next token from the main model as well
            new_tokens = mx.concatenate(
                [new_tokens, mx.argmax(logits[num_to_accept], keepdims=True)]
            )

            n_accepted += num_to_accept
            n_draft += draft_tokens.size

            # Rewind the cache for unaccepted tokens:
            if (n := draft_tokens.size) > num_to_accept:
                self.draft_model.truncate_cache(n - new_tokens.size)
                self.model.truncate_cache(n - new_tokens.size + 1)

            n_steps += 1

            for t in new_tokens.tolist():
                if t == self.tokenizer.eos_id or ntoks >= max_tokens:
                    break
                outputs.append(t)
                ntoks += 1

            str_output = self.tokenizer.decode(outputs)
            print(str_output[skip:], end="", flush=True)
            skip = len(str_output)

            if ntoks >= max_tokens or new_tokens[-1] == self.tokenizer.eos_id:
                break
            draft_inputs = new_tokens[max(new_tokens.size - 2, 0) :]
            inputs = draft_inputs[-1:]

        print(self.tokenizer.decode(outputs)[skip:], end="", flush=True)
        print()

        self.model.reset_cache()
        self.draft_model.reset_cache()
        return {"n_accepted": n_accepted, "n_draft": n_draft, "n_steps": n_steps}

>>>> llms/mlx_lm/fuse.py
import argparse
import glob
import shutil
from pathlib import Path

from mlx.utils import tree_flatten, tree_unflatten

from .gguf import convert_to_gguf
from .tuner.dora import DoRAEmbedding, DoRALinear
from .tuner.lora import LoRAEmbedding, LoRALinear, LoRASwitchLinear
from .tuner.utils import dequantize, load_adapters
from .utils import (
    fetch_from_hub,
    get_model_path,
    save_config,
    save_weights,
    upload_to_hub,
)


def parse_arguments() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Fuse fine-tuned adapters into the base model."
    )
    parser.add_argument(
        "--model",
        default="mlx_model",
        help="The path to the local model directory or Hugging Face repo.",
    )
    parser.add_argument(
        "--save-path",
        default="fused_model",
        help="The path to save the fused model.",
    )
    parser.add_argument(
        "--adapter-path",
        type=str,
        default="adapters",
        help="Path to the trained adapter weights and config.",
    )
    parser.add_argument(
        "--hf-path",
        type=str,
        default=None,
        help="Path to the original Hugging Face model. Required for upload if --model is a local directory.",
    )
    parser.add_argument(
        "--upload-repo",
        help="The Hugging Face repo to upload the model to.",
        type=str,
        default=None,
    )
    parser.add_argument(
        "--de-quantize",
        help="Generate a de-quantized model.",
        action="store_true",
    )
    parser.add_argument(
        "--export-gguf",
        help="Export model weights in GGUF format.",
        action="store_true",
    )
    parser.add_argument(
        "--gguf-path",
        help="Path to save the exported GGUF format model weights. Default is ggml-model-f16.gguf.",
        default="ggml-model-f16.gguf",
        type=str,
    )
    return parser.parse_args()


def main() -> None:
    print("Loading pretrained model")
    args = parse_arguments()

    model_path = get_model_path(args.model)
    model, config, tokenizer = fetch_from_hub(model_path)

    model.freeze()
    model = load_adapters(model, args.adapter_path)

    fused_linears = [
        (n, m.fuse()) for n, m in model.named_modules() if hasattr(m, "fuse")
    ]

    if fused_linears:
        model.update_modules(tree_unflatten(fused_linears))

    if args.de_quantize:
        print("De-quantizing model")
        model = dequantize(model)

    weights = dict(tree_flatten(model.parameters()))

    save_path = Path(args.save_path)

    save_weights(save_path, weights)

    py_files = glob.glob(str(model_path / "*.py"))
    for file in py_files:
        shutil.copy(file, save_path)

    tokenizer.save_pretrained(save_path)

    if args.de_quantize:
        config.pop("quantization", None)

    save_config(config, config_path=save_path / "config.json")

    if args.export_gguf:
        model_type = config["model_type"]
        if model_type not in ["llama", "mixtral", "mistral"]:
            raise ValueError(
                f"Model type {model_type} not supported for GGUF conversion."
            )
        convert_to_gguf(model_path, weights, config, str(save_path / args.gguf_path))

    if args.upload_repo is not None:
        hf_path = args.hf_path or (
            args.model if not Path(args.model).exists() else None
        )
        if hf_path is None:
            raise ValueError(
                "Must provide original Hugging Face repo to upload local model."
            )
        upload_to_hub(args.save_path, args.upload_repo, hf_path)


if __name__ == "__main__":
    main()

>>>> llms/mlx_lm/lora.py
# Copyright © 2024 Apple Inc.

import argparse
import math
import os
import re
import types
from pathlib import Path

import mlx.nn as nn
import mlx.optimizers as optim
import numpy as np
import yaml

from .tokenizer_utils import TokenizerWrapper
from .tuner.datasets import load_dataset
from .tuner.trainer import TrainingArgs, TrainingCallback, evaluate, train
from .tuner.utils import (
    build_schedule,
    linear_to_lora_layers,
    load_adapters,
    print_trainable_parameters,
)
from .utils import load, save_config

yaml_loader = yaml.SafeLoader
yaml_loader.add_implicit_resolver(
    "tag:yaml.org,2002:float",
    re.compile(
        """^(?:
     [-+]?(?:[0-9][0-9_]*)\\.[0-9_]*(?:[eE][-+]?[0-9]+)?
    |[-+]?(?:[0-9][0-9_]*)(?:[eE][-+]?[0-9]+)
    |\\.[0-9_]+(?:[eE][-+][0-9]+)?
    |[-+]?[0-9][0-9_]*(?::[0-5]?[0-9])+\\.[0-9_]*
    |[-+]?\\.(?:inf|Inf|INF)
    |\\.(?:nan|NaN|NAN))$""",
        re.X,
    ),
    list("-+0123456789."),
)

CONFIG_DEFAULTS = {
    "model": "mlx_model",
    "train": False,
    "fine_tune_type": "lora",
    "optimizer": "adam",
    "optimizer_config": {
        "adam": {},
        "adamw": {},
    },
    "data": "data/",
    "seed": 0,
    "num_layers": 16,
    "batch_size": 4,
    "iters": 1000,
    "val_batches": 25,
    "learning_rate": 1e-5,
    "steps_per_report": 10,
    "steps_per_eval": 200,
    "resume_adapter_file": None,
    "adapter_path": "adapters",
    "save_every": 100,
    "test": False,
    "test_batches": 500,
    "max_seq_length": 2048,
    "config": None,
    "grad_checkpoint": False,
    "lr_schedule": None,
    "lora_parameters": {"rank": 8, "alpha": 16, "dropout": 0.0, "scale": 10.0},
    "mask_prompt": False,
}


def build_parser():
    parser = argparse.ArgumentParser(description="LoRA or QLoRA finetuning.")
    parser.add_argument(
        "--model",
        type=str,
        help="The path to the local model directory or Hugging Face repo.",
    )

    # Training args
    parser.add_argument(
        "--train",
        action="store_true",
        help="Do training",
        default=None,
    )
    parser.add_argument(
        "--data",
        type=str,
        help=(
            "Directory with {train, valid, test}.jsonl files or the name "
            "of a Hugging Face dataset (e.g., 'mlx-community/wikisql')"
        ),
    )
    parser.add_argument(
        "--fine-tune-type",
        type=str,
        choices=["lora", "dora", "full"],
        help="Type of fine-tuning to perform: lora, dora, or full.",
    )
    parser.add_argument(
        "--optimizer",
        type=str,
        choices=["adam", "adamw"],
        default=None,
        help="Optimizer to use for training: adam or adamw",
    )
    parser.add_argument(
        "--mask-prompt",
        action="store_true",
        help="Mask the prompt in the loss when training",
        default=None,
    )
    parser.add_argument(
        "--num-layers",
        type=int,
        help="Number of layers to fine-tune. Default is 16, use -1 for all.",
    )
    parser.add_argument("--batch-size", type=int, help="Minibatch size.")
    parser.add_argument("--iters", type=int, help="Iterations to train for.")
    parser.add_argument(
        "--val-batches",
        type=int,
        help="Number of validation batches, -1 uses the entire validation set.",
    )
    parser.add_argument("--learning-rate", type=float, help="Adam learning rate.")
    parser.add_argument(
        "--steps-per-report",
        type=int,
        help="Number of training steps between loss reporting.",
    )
    parser.add_argument(
        "--steps-per-eval",
        type=int,
        help="Number of training steps between validations.",
    )
    parser.add_argument(
        "--resume-adapter-file",
        type=str,
        help="Load path to resume training from the given fine-tuned weights.",
    )
    parser.add_argument(
        "--adapter-path",
        type=str,
        help="Save/load path for the fine-tuned weights.",
    )
    parser.add_argument(
        "--save-every",
        type=int,
        help="Save the model every N iterations.",
    )
    parser.add_argument(
        "--test",
        action="store_true",
        help="Evaluate on the test set after training",
        default=None,
    )
    parser.add_argument(
        "--test-batches",
        type=int,
        help="Number of test set batches, -1 uses the entire test set.",
    )
    parser.add_argument(
        "--max-seq-length",
        type=int,
        help="Maximum sequence length.",
    )
    parser.add_argument(
        "-c",
        "--config",
        type=str,
        help="A YAML configuration file with the training options",
    )
    parser.add_argument(
        "--grad-checkpoint",
        action="store_true",
        help="Use gradient checkpointing to reduce memory use.",
        default=None,
    )
    parser.add_argument("--seed", type=int, help="The PRNG seed")
    return parser


def train_model(
    args,
    model: nn.Module,
    tokenizer: TokenizerWrapper,
    train_set,
    valid_set,
    training_callback: TrainingCallback = None,
):
    model.freeze()
    if args.num_layers > len(model.layers):
        raise ValueError(
            f"Requested to train {args.num_layers} layers "
            f"but the model only has {len(model.layers)} layers."
        )

    if args.fine_tune_type == "full":
        for l in model.layers[-max(args.num_layers, 0) :]:
            l.unfreeze()
    elif args.fine_tune_type in ["lora", "dora"]:
        # Convert linear layers to lora/dora layers and unfreeze in the process
        linear_to_lora_layers(
            model,
            args.num_layers,
            args.lora_parameters,
            use_dora=(args.fine_tune_type == "dora"),
        )
    else:
        raise ValueError(f"Received unknown fine-tune-type {args.fine_tune_type}")

    # Resume from weights if provided
    if args.resume_adapter_file is not None:
        print(f"Loading fine-tuned weights from {args.resume_adapter_file}")
        model.load_weights(args.resume_adapter_file, strict=False)

    print_trainable_parameters(model)

    adapter_path = Path(args.adapter_path)
    adapter_path.mkdir(parents=True, exist_ok=True)

    adapter_file = adapter_path / "adapters.safetensors"
    save_config(vars(args), adapter_path / "adapter_config.json")

    # init training args
    training_args = TrainingArgs(
        batch_size=args.batch_size,
        iters=args.iters,
        val_batches=args.val_batches,
        steps_per_report=args.steps_per_report,
        steps_per_eval=args.steps_per_eval,
        steps_per_save=args.save_every,
        adapter_file=adapter_file,
        max_seq_length=args.max_seq_length,
        grad_checkpoint=args.grad_checkpoint,
    )

    model.train()

    # Initialize the selected optimizer
    lr = build_schedule(args.lr_schedule) if args.lr_schedule else args.learning_rate

    optimizer_name = args.optimizer.lower()
    optimizer_config = args.optimizer_config.get(optimizer_name, {})

    if optimizer_name == "adam":
        opt_class = optim.Adam
    elif optimizer_name == "adamw":
        opt_class = optim.AdamW
    else:
        raise ValueError(f"Unsupported optimizer: {optimizer_name}")

    opt = opt_class(learning_rate=lr, **optimizer_config)

    # Train model
    train(
        model=model,
        tokenizer=tokenizer,
        args=training_args,
        optimizer=opt,
        train_dataset=train_set,
        val_dataset=valid_set,
        training_callback=training_callback,
    )


def evaluate_model(args, model: nn.Module, tokenizer: TokenizerWrapper, test_set):
    model.eval()

    test_loss = evaluate(
        model=model,
        dataset=test_set,
        tokenizer=tokenizer,
        batch_size=args.batch_size,
        num_batches=args.test_batches,
        max_seq_length=args.max_seq_length,
    )

    test_ppl = math.exp(test_loss)

    print(f"Test loss {test_loss:.3f}, Test ppl {test_ppl:.3f}.")


def run(args, training_callback: TrainingCallback = None):
    np.random.seed(args.seed)

    print("Loading pretrained model")
    model, tokenizer = load(args.model)

    print("Loading datasets")
    train_set, valid_set, test_set = load_dataset(args, tokenizer)

    if args.test and not args.train:
        # Allow testing without LoRA layers by providing empty path
        if args.adapter_path != "":
            load_adapters(model, args.adapter_path)

    elif args.train:
        print("Training")
        train_model(args, model, tokenizer, train_set, valid_set, training_callback)
    else:
        raise ValueError("Must provide at least one of --train or --test")

    if args.test:
        print("Testing")
        evaluate_model(args, model, tokenizer, test_set)


def main():
    os.environ["TOKENIZERS_PARALLELISM"] = "true"
    parser = build_parser()
    args = parser.parse_args()
    config = args.config
    args = vars(args)
    if config:
        print("Loading configuration file", config)
        with open(config, "r") as file:
            config = yaml.load(file, yaml_loader)
        # Prefer parameters from command-line arguments
        for k, v in config.items():
            if args.get(k, None) is None:
                args[k] = v

    # Update defaults for unspecified parameters
    for k, v in CONFIG_DEFAULTS.items():
        if args.get(k, None) is None:
            args[k] = v
    run(types.SimpleNamespace(**args))


if __name__ == "__main__":
    main()

>>>> llms/mlx_lm/manage.py
import argparse
from typing import List, Union

from huggingface_hub import scan_cache_dir


def tabulate(rows: List[List[Union[str, int]]], headers: List[str]) -> str:
    """
    Inspired by:
    - stackoverflow.com/a/8356620/593036
    - stackoverflow.com/questions/9535954/printing-lists-as-tabular-data
    """
    col_widths = [max(len(str(x)) for x in col) for col in zip(*rows, headers)]
    row_format = ("{{:{}}} " * len(headers)).format(*col_widths)
    lines = []
    lines.append(row_format.format(*headers))
    lines.append(row_format.format(*["-" * w for w in col_widths]))
    for row in rows:
        lines.append(row_format.format(*row))
    return "\n".join(lines)


def ask_for_confirmation(message: str) -> bool:
    """Ask user for confirmation with Y/N prompt.
    Returns True for Y/yes, False for N/no/empty."""
    y = ("y", "yes", "1")
    n = ("n", "no", "0", "")
    full_message = f"{message} (y/n) "
    while True:
        answer = input(full_message).lower()
        if answer in y:
            return True
        if answer in n:
            return False
        print(f"Invalid input. Must be one of: yes/no/y/n or empty for no")


def main():
    parser = argparse.ArgumentParser(description="MLX Model Cache.")
    parser.add_argument(
        "--scan",
        action="store_true",
        help="Scan Hugging Face cache for mlx models.",
    )
    parser.add_argument(
        "--delete",
        action="store_true",
        help="Delete models matching the given pattern.",
    )
    parser.add_argument(
        "--pattern",
        type=str,
        help="Model repos contain the pattern.",
        default="mlx",
    )

    args = parser.parse_args()

    if args.scan:
        print(f'Scanning Hugging Face cache for models with pattern "{args.pattern}".')
        hf_cache_info = scan_cache_dir()
        print(
            tabulate(
                rows=[
                    [
                        repo.repo_id,
                        repo.repo_type,
                        "{:>12}".format(repo.size_on_disk_str),
                        repo.nb_files,
                        repo.last_accessed_str,
                        repo.last_modified_str,
                        str(repo.repo_path),
                    ]
                    for repo in sorted(
                        hf_cache_info.repos, key=lambda repo: repo.repo_path
                    )
                    if args.pattern in repo.repo_id
                ],
                headers=[
                    "REPO ID",
                    "REPO TYPE",
                    "SIZE ON DISK",
                    "NB FILES",
                    "LAST_ACCESSED",
                    "LAST_MODIFIED",
                    "LOCAL PATH",
                ],
            )
        )

    if args.delete:
        print(f'Deleting models matching pattern "{args.pattern}"')
        hf_cache_info = scan_cache_dir()

        repos = [
            repo
            for repo in sorted(hf_cache_info.repos, key=lambda repo: repo.repo_path)
            if args.pattern in repo.repo_id
        ]
        if repos:
            print("\nFound the following models:")
            print(
                tabulate(
                    rows=[
                        [
                            repo.repo_id,
                            repo.size_on_disk_str,  # Added size information
                            str(repo.repo_path),
                        ]
                        for repo in repos
                    ],
                    headers=[
                        "REPO ID",
                        "SIZE",  # Added size header
                        "LOCAL PATH",
                    ],
                )
            )

            confirmed = ask_for_confirmation(
                "\nAre you sure you want to delete these models?"
            )
            if confirmed:
                for model_info in repos:
                    print(f"\nDeleting {model_info.repo_id}...")
                    for revision in sorted(
                        model_info.revisions, key=lambda revision: revision.commit_hash
                    ):
                        strategy = hf_cache_info.delete_revisions(revision.commit_hash)
                        strategy.execute()
                print("\nModel(s) deleted successfully.")
            else:
                print("\nDeletion cancelled - no changes made.")
        else:
            print(f'No models found matching pattern "{args.pattern}"')


if __name__ == "__main__":
    main()

>>>> llms/mlx_lm/evaluate.py
# Copyright © 2024 Apple Inc.

"""
Adapted from a PyTorch implementation by David Grangier
"""

import argparse
import json
import logging
import os
from importlib.metadata import version
from pathlib import Path
from typing import Optional, Union

import lm_eval
import mlx.core as mx
import mlx.nn as nn
import numpy as np
from lm_eval.api.model import LM
from lm_eval.api.registry import register_model
from tqdm import tqdm

from .models.cache import make_prompt_cache
from .utils import load, stream_generate

PAD = 0


def _len_longest_common_prefix(a, b):
    l = 0
    for item_a, item_b in zip(a, b):
        if item_a != item_b:
            break
        l += 1
    return l


def _rstrip_until(s, untils):
    """Limit a string <s> to the first occurrence of any substring in untils."""
    l = len(s)
    f = [s.find(u) for u in untils]
    f = [l if x < 0 else x for x in f]
    return s[: min(f)]


def _pad_inputs(
    inputs,
    maxlen,
    genlen=0,
    pad_left=False,
    pad_multiple=32,
    truncate=False,
):
    # pad the prompts to the left with at least genlen tokens.
    actual_maxlen = max(len(p) for p in inputs) + genlen
    if actual_maxlen > maxlen:
        if not truncate:
            raise ValueError("Inputs are too long.")
        else:  # drop begining
            actual_maxlen = maxlen
            inputs = [p[max(0, len(p) - maxlen) :] for p in inputs]
    if pad_multiple > 0:
        maxlen = (actual_maxlen + pad_multiple - 1) // pad_multiple
        maxlen *= pad_multiple
    assert PAD == 0
    lr = np.array((1, 0) if pad_left else (0, 1))
    return np.stack(
        [np.pad(np.array(x, np.int32), lr * (maxlen - len(x))) for x in inputs],
        axis=0,
    )


@register_model("mlxlm")
class MLXLM(LM):
    def __init__(
        self,
        path_or_hf_repo: str,
        batch_size: int = 16,
        max_tokens: Optional[int] = None,
        use_chat_template: Optional[bool] = None,
    ) -> None:
        super().__init__()
        self._batch_size = batch_size
        self._model, self.tokenizer = load(path_or_hf_repo)
        self._max_tokens = max_tokens or self.tokenizer.model_max_length
        self.use_chat_template = use_chat_template or (
            self.tokenizer.chat_template is not None
        )

    def _score_fn(self, inputs, tokenize=True, step_size=32):
        if tokenize:
            inputs = self._tokenize(inputs)
        inputs = _pad_inputs(inputs, self._max_tokens, truncate=False)
        inputs = mx.array(inputs)
        inputs, targets = inputs[..., :-1], inputs[..., 1:]

        cache = make_prompt_cache(self._model)

        mask = targets != PAD

        scores, is_greedy = [], []
        for i in range(0, inputs.shape[1], step_size):
            logits = self._model(inputs[:, i : i + step_size], cache=cache)

            log_probs = nn.log_softmax(logits.astype(mx.float32))
            score = mx.take_along_axis(
                log_probs, targets[:, i : i + step_size, mx.newaxis], axis=-1
            )[..., 0]
            ig = mask[:, i : i + step_size] * (
                targets[:, i : i + step_size] == mx.argmax(logits, axis=-1)
            )

            mx.eval(score, ig)
            mx.metal.clear_cache()

            is_greedy.append(ig)
            scores.append(score)

        scores = mx.concatenate(scores, axis=1)
        is_greedy = mx.concatenate(is_greedy, axis=1)

        return scores, mask.sum(axis=-1), is_greedy

    def _loglikelihood(self, texts, score_spans=None, tokenize=True):
        # sort by length to get batches with little padding.
        sorted_indices = sorted(range(len(texts)), key=lambda i: -len(texts[i]))
        sorted_inputs = [texts[sorted_indices[i]] for i in range(len(texts))]
        sorted_spans = None
        if score_spans is not None:
            sorted_spans = [score_spans[sorted_indices[i]] for i in range(len(texts))]

        results = []
        for i in tqdm(range(0, len(sorted_inputs), self._batch_size)):
            batch = sorted_inputs[i : i + self._batch_size]
            scores, length, is_greedy = self._score_fn(batch, tokenize=tokenize)
            for j in range(len(batch)):
                if sorted_spans is None:  # full sequence score
                    mask = mx.arange(scores[j].shape[-1]) < length
                    score = (scores[j].astype(mx.float32) * mask).sum(axis=-1)
                    ig = (is_greedy[j].astype(mx.int32) * mask).sum(axis=-1)
                else:  # subsequence score
                    start, end = sorted_spans[i + j]
                    score = scores[j][start:end].astype(mx.float32).sum()
                    ig = is_greedy[j][start:end].astype(mx.int32).sum()
                    length = end - start

                results.append((score.item(), ig.item(), length))

        # reorder the outputs
        inv_sort = np.argsort(sorted_indices)
        results = [results[inv_sort[i]] for i in range(len(results))]

        return results

    def _tokenize(self, texts):
        return [
            tuple(
                self.tokenizer.encode(t, add_special_tokens=not self.use_chat_template)
            )
            for t in texts
        ]

    def loglikelihood(self, requests) -> list[tuple[float, bool]]:
        """Compute log-likelihood of generating a continuation from a context.
        Downstream tasks should attempt to use loglikelihood instead of other
        LM calls whenever possible.
        :param requests: list[Instance]
            A list of Instance objects, with property `args` which returns a tuple (context, continuation).
            `context: str`
                Context string. Implementations of LM must be able to handle an
                empty context string.
            `continuation: str`
                The continuation over which log likelihood will be calculated. If
                there is a word boundary, the space should be in the continuation.
                For example, context="hello" continuation=" world" is correct.
        :return: list[tuple[float, bool]]
            A list of pairs (logprob, isgreedy)
            `logprob: float`
                The log probability of `continuation`.
            `isgreedy`:
                Whether `continuation` would be generated by greedy sampling from `context`.
        """
        logging.info("Estimating loglikelihood for %d pairs." % len(requests))

        # tokenize prefix and prefix + completion for all requests.
        tokenized = self._tokenize(
            [t for r in requests for t in [r.args[0], r.args[0] + r.args[1]]]
        )

        # max length (prefix + completion) and longest common prefix per question.
        length_stats = {}
        for prefix, completed in zip(tokenized[0::2], tokenized[1::2]):
            max_completed_l, min_prefix_l = length_stats.get(prefix, (0, 1e8))
            length_stats[prefix] = (
                max(max_completed_l, len(completed)),
                min(min_prefix_l, _len_longest_common_prefix(prefix, completed)),
            )

        # truncate requests for completed sequences longer than model context.
        shortened = []
        completion_spans = []
        long_completions = 0
        for prefix, completed in zip(tokenized[0::2], tokenized[1::2]):
            max_completed_l, prefix_l = length_stats[prefix]
            # compute truncation length
            truncation = max(0, max_completed_l - self._max_tokens - 1)
            prefix_l = prefix_l - truncation
            if prefix_l <= 0:
                # completion too long, prefix is eliminated for some requests.
                long_completions += 1
                truncation = max(0, len(completed) - self._max_tokens - 1)
                prefix_l = 1
            # truncate the completed sequence
            completed = completed[truncation:]
            shortened.append(completed)
            # scores do not include initial bos, substract 1 to span bounds
            completion_spans.append((prefix_l - 1, len(completed) - 1))

        if long_completions > 0:
            logging.info(
                f"Prefix eliminated for {long_completions} requests with "
                + "completion longer than context."
            )

        # model scoring, returns num_requests x (logp, is_greedy, length).
        results = self._loglikelihood(
            shortened,
            score_spans=completion_spans,
            tokenize=False,
        )
        return [(r[0], r[1] == r[2]) for r in results]

    tokenizer_name = lm_eval.models.huggingface.HFLM.tokenizer_name
    apply_chat_template = lm_eval.models.huggingface.HFLM.apply_chat_template

    def loglikelihood_rolling(self, requests) -> list[float]:
        """Compute full log-likelihood of a string, with no truncation, for perplexity computation
        - We will use the full max context length of the model.
        - For inputs that exceed the max context length, we divide the tokenized string into chunks of up to
        the max context length.
        - IMPORTANT: Each document's loglikelihood/perplexity is computed *separately*, unlike other implementations
          which may simply concatenate multiple documents together.
        - IMPORTANT: We maximize the amount of context for each prediction. Specifically, for inputs that we break into
          multiple chunks, the last input will still a full-sized context.
          Example:
            Input tokens: [ 0 1 2 3 4 5 6 7 8 9 ]
            Prefix: EOT
            Max context length: 4
            Resulting input/prediction pairs:
                INPUT:  EOT   0   1   2
                PRED:     0   1   2   3
                INPUT:    3   4   5   6
                PRED:     4   5   6   7
                INPUT:    5   6   7   8
                PRED:             8   9
          Observe that:
            1. Each token is predicted exactly once
            2. For the last pair, we provide the full context, but only score the last two tokens
        :param requests: list[Instance]
            A list of Instance objects with property `args` which returns a tuple (context,).
            string: str
                String for which we are computing overall loglikelihood
        :return: list[tuple[float]]
            A list of tuples (logprob,)
            logprob: float
                The log probability of `context` conditioned on the EOT token.
        """
        logging.info(
            "Estimating loglikelihood rolling for %d sequences." % len(requests)
        )
        inputs = [req.args[0] for req in requests]
        return [t[0] for t in self._loglikelihood(inputs)]

    def generate_until(self, requests) -> list[str]:
        """Generate greedily until a stopping sequence
        :param requests: list[Instance]
            A list of Instance objects with property `args` which returns a tuple (context, until).
            context: str
                Context string
            until: [str]
                The string sequences to generate until. These string sequences
                may each span across multiple tokens, or may be part of one token.
        :return: list[str]
            A list of strings continuation
            continuation: str
                The generated continuation.
        """
        logging.info("Generating continuation for %d sequences." % len(requests))
        contexts, options = zip(*[req.args for req in requests])
        # contrary to the doc the second element of the tuple contains
        # {'do_sample': False, 'until': ['\n\n'], 'temperature': 0}
        completions = []

        for context, opt in tqdm(zip(contexts, options), total=len(contexts)):
            until = opt["until"]
            context = self.tokenizer.encode(
                context, add_special_tokens=not self.use_chat_template
            )
            max_tokens = min(
                opt.get("max_gen_tokens", self._max_tokens),
                self.tokenizer.model_max_length - len(context),
            )
            text = ""
            for response in stream_generate(
                self._model, self.tokenizer, prompt=context, max_tokens=max_tokens
            ):
                text += response.text
                if any(u in text for u in until):
                    text = _rstrip_until(text, until)
                    completions.append(text)
                    break
            else:
                completions.append(text)
        return completions


def main():
    parser = argparse.ArgumentParser(
        "Evaluate an MLX model using lm-evaluation-harness."
    )
    parser.add_argument("--model", help="Model to evaluate", required=True)
    parser.add_argument("--tasks", nargs="+", required=True)
    parser.add_argument(
        "--output-dir", default=".", help="Output directory for result files."
    )
    parser.add_argument("--batch-size", type=int, default=16, help="Batch size")
    parser.add_argument("--num-shots", type=int, default=0, help="Number of shots")
    parser.add_argument(
        "--max-tokens",
        type=int,
        help="Maximum nunber of tokens to generate. Defaults to the model's max context length.",
    )
    parser.add_argument(
        "--limit",
        default=100,
        help="Limit the number of examples per task.",
        type=int,
    )
    parser.add_argument("--seed", type=int, default=123, help="Random seed.")
    parser.add_argument(
        "--fewshot-as-multiturn",
        action="store_true",
        help="Whether to provide the fewshot examples as a multiturn "
        "conversation or a single user turn.",
        default=False,
    )
    parser.add_argument(
        "--apply-chat-template",
        action=argparse.BooleanOptionalAction,
        help="Specifies whether to apply a chat template to the prompt. If "
        "the model has a chat template, this defaults to `True`, "
        "otherwise `False`.",
        default=None,
    )
    args = parser.parse_args()

    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # Silence tokenizer warnings
    os.environ["TOKENIZERS_PARALLELISM"] = "false"

    mx.random.seed(args.seed)

    lm = MLXLM(
        args.model,
        batch_size=args.batch_size,
        max_tokens=args.max_tokens,
        use_chat_template=args.apply_chat_template,
    )
    results = lm_eval.simple_evaluate(
        model=lm,
        tasks=args.tasks,
        fewshot_as_multiturn=args.fewshot_as_multiturn,
        apply_chat_template=lm.use_chat_template,
        num_fewshot=args.num_shots,
        limit=args.limit,
        random_seed=args.seed,
        numpy_random_seed=args.seed,
        torch_random_seed=args.seed,
        fewshot_random_seed=args.seed,
    )

    model_name = args.model.replace("/", "_")
    task_names = "_".join(args.tasks)
    ver = version("lm_eval")
    filename = f"eval_{model_name}_{task_names}_{args.num_shots:02d}_v_{ver}.json"
    output_path = output_dir / filename
    output_path.write_text(json.dumps(results["results"], indent=4))
    print("Results:")
    for result in results["results"].values():
        print(json.dumps(result, indent=4))

>>>> llms/mlx_lm/py.typed


>>>> llms/mlx_lm/SERVER.md
# HTTP Model Server

You use `mlx-lm` to make an HTTP API for generating text with any supported
model. The HTTP API is intended to be similar to the [OpenAI chat
API](https://platform.openai.com/docs/api-reference).

> [!NOTE]  
> The MLX LM server is not recommended for production as it only implements
> basic security checks.

Start the server with: 

```shell
mlx_lm.server --model <path_to_model_or_hf_repo>
```

For example:

```shell
mlx_lm.server --model mlx-community/Mistral-7B-Instruct-v0.3-4bit
```

This will start a text generation server on port `8080` of the `localhost`
using Mistral 7B instruct. The model will be downloaded from the provided
Hugging Face repo if it is not already in the local cache.

To see a full list of options run:

```shell
mlx_lm.server --help
```

You can make a request to the model by running:

```shell
curl localhost:8080/v1/chat/completions 
  -H "Content-Type: application/json" 
  -d '{
     "messages": [{"role": "user", "content": "Say this is a test!"}],
     "temperature": 0.7
   }'
```

### Request Fields

- `messages`: An array of message objects representing the conversation
  history. Each message object should have a role (e.g. user, assistant) and
  content (the message text).

- `role_mapping`: (Optional) A dictionary to customize the role prefixes in
  the generated prompt. If not provided, the default mappings are used.

- `stop`: (Optional) An array of strings or a single string. These are
  sequences of tokens on which the generation should stop.

- `max_tokens`: (Optional) An integer specifying the maximum number of tokens
  to generate. Defaults to `100`.

- `stream`: (Optional) A boolean indicating if the response should be
  streamed. If true, responses are sent as they are generated. Defaults to
  false.

- `temperature`: (Optional) A float specifying the sampling temperature.
  Defaults to `1.0`.

- `top_p`: (Optional) A float specifying the nucleus sampling parameter.
  Defaults to `1.0`.

- `repetition_penalty`: (Optional) Applies a penalty to repeated tokens.
  Defaults to `1.0`.

- `repetition_context_size`: (Optional) The size of the context window for
  applying repetition penalty. Defaults to `20`.

- `logit_bias`: (Optional) A dictionary mapping token IDs to their bias
  values. Defaults to `None`.

- `logprobs`: (Optional) An integer specifying the number of top tokens and
  corresponding log probabilities to return for each output in the generated
  sequence. If set, this can be any value between 1 and 10, inclusive.

- `model`: (Optional) A string path to a local model or Hugging Face repo id.
  If the path is local is must be relative to the directory the server was
  started in.

- `adapters`: (Optional) A string path to low-rank adapters. The path must be
  relative to the directory the server was started in.

### Response Fields

- `id`: A unique identifier for the chat.

- `system_fingerprint`: A unique identifier for the system.

- `object`: Any of "chat.completion", "chat.completion.chunk" (for
  streaming), or "text.completion".

- `model`: The model repo or path (e.g. `"mlx-community/Llama-3.2-3B-Instruct-4bit"`).

- `created`: A time-stamp for when the request was processed.

- `choices`: A list of outputs. Each output is a dictionary containing the fields:
    - `index`: The index in the list.
    - `logprobs`: A dictionary containing the fields:
        - `token_logprobs`: A list of the log probabilities for the generated
          tokens.
        - `tokens`: A list of the generated token ids.
        - `top_logprobs`: A list of lists. Each list contains the `logprobs`
          top tokens (if requested) with their corresponding probabilities.
    - `finish_reason`: The reason the completion ended. This can be either of
      `"stop"` or `"length"`.
    - `message`: The text response from the model.

- `usage`: A dictionary containing the fields:
    - `prompt_tokens`: The number of prompt tokens processed.
    - `completion_tokens`: The number of tokens generated.
    - `total_tokens`: The total number of tokens, i.e. the sum of the above two fields.

### List Models

Use the `v1/models` endpoint to list available models:

```shell
curl localhost:8080/v1/models -H "Content-Type: application/json"
```

This will return a list of locally available models where each model in the
list contains the following fields:

- `id`: The Hugging Face repo id.
- `created`: A time-stamp representing the model creation time.

>>>> llms/mlx_lm/MERGE.md
# Model Merging

You can use `mlx-lm` to merge models and upload them to the Hugging
Face hub or save them locally for LoRA fine tuning.

The main command is `mlx_lm.merge`:

```shell
mlx_lm.merge --config config.yaml 
```

The merged model will be saved by default in `mlx_merged_model`. To see a
full list of options run:

```shell
mlx_lm.merge --help
```

Here is an example `config.yaml`:

```yaml
models:
  - OpenPipe/mistral-ft-optimized-1218
  - mlabonne/NeuralHermes-2.5-Mistral-7B
method: slerp
parameters:
  t:
    - filter: self_attn
      value: [0, 0.5, 0.3, 0.7, 1]
    - filter: mlp
      value: [1, 0.5, 0.7, 0.3, 0]
    - value: 0.5
```

The `models` field is a list of Hugging Face repo ids. The first model in the
list is treated as the base model into which the remaining models are merged.

The `method` field is the merging method. Right now `slerp` is the only
supported method.

The `parameters` are the corresponding parameters for the given `method`.
Each parameter is a list with `filter` determining which layer the parameter
applies to and `value` determining the actual value used. The last item in
the list without a `filter` field is the default.

If `value` is a list, it specifies the start and end values for the
corresponding segment of blocks. In the example above, the models have 32
blocks. For blocks 1-8, the layers with `self_attn` in the name will use the
values `np.linspace(0, 0.5, 8)`, the same layers in the next 8 blocks (9-16)
will use `np.linspace(0.5, 0.3, 8)`, and so on.

>>>> llms/mlx_lm/examples/lora_config.yaml
# The path to the local model directory or Hugging Face repo.
model: "mlx_model"

# Whether or not to train (boolean)
train: true

# The fine-tuning method: "lora", "dora", or "full".
fine_tune_type: lora

# The Optimizer with its possible inputs
optimizer: adamw
# optimizer_config:
#   adamw:
#     betas: [0.9, 0.98]
#     eps: 1e-6
#     weight_decay: 0.05
#     bias_correction: true

# Directory with {train, valid, test}.jsonl files
data: "/path/to/training/data"

# The PRNG seed
seed: 0

# Number of layers to fine-tune
num_layers: 16

# Minibatch size.
batch_size: 4

# Iterations to train for.
iters: 1000

# Number of validation batches, -1 uses the entire validation set.
val_batches: 25

# Adam learning rate.
learning_rate: 1e-5

# Number of training steps between loss reporting.
steps_per_report: 10

# Number of training steps between validations.
steps_per_eval: 200

# Load path to resume training with the given adapter weights.
resume_adapter_file: null

# Save/load path for the trained adapter weights.
adapter_path: "adapters"

# Save the model every N iterations.
save_every: 100

# Evaluate on the test set after training
test: false

# Number of test set batches, -1 uses the entire test set.
test_batches: 100

# Maximum sequence length.
max_seq_length: 2048

# Use gradient checkpointing to reduce memory use.
grad_checkpoint: false

# LoRA parameters can only be specified in a config file
lora_parameters:
  # The layer keys to apply LoRA to.
  # These will be applied for the last lora_layers
  keys: ["self_attn.q_proj", "self_attn.v_proj"]
  rank: 8
  scale: 20.0
  dropout: 0.0

# Schedule can only be specified in a config file, uncomment to use.
#lr_schedule:
#  name: cosine_decay
#  warmup: 100 # 0 for no warmup
#  warmup_init: 1e-7 # 0 if not specified
#  arguments: [1e-5, 1000, 1e-7] # passed to scheduler

#hf_dataset:
#  name: "billsum"
#  train_split: "train[:1000]"
#  valid_split: "train[-100:]"
#  prompt_feature: "text"
#  completion_feature: "summary"


>>>> llms/mlx_lm/examples/chat.py
# Copyright © 2024 Apple Inc.

"""
An example of a multi-turn chat with prompt caching.
"""

from mlx_lm import generate, load
from mlx_lm.models.cache import load_prompt_cache, make_prompt_cache, save_prompt_cache

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Make the initial prompt cache for the model
prompt_cache = make_prompt_cache(model)

# User turn
prompt = "Hi my name is <Name>."
messages = [{"role": "user", "content": prompt}]
prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True)

# Assistant response
response = generate(
    model,
    tokenizer,
    prompt=prompt,
    verbose=True,
    prompt_cache=prompt_cache,
)

# User turn
prompt = "What's my name?"
messages = [{"role": "user", "content": prompt}]
prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True)

# Assistant response
response = generate(
    model,
    tokenizer,
    prompt=prompt,
    verbose=True,
    prompt_cache=prompt_cache,
)

# Save the prompt cache to disk to reuse it at a later time
save_prompt_cache("mistral_prompt.safetensors", prompt_cache)

# Load the prompt cache from disk
prompt_cache = load_prompt_cache("mistral_prompt.safetensors")

>>>> llms/mlx_lm/examples/tool_use.py
# Copyright © 2025 Apple Inc.

import json

from mlx_lm import generate, load
from mlx_lm.models.cache import make_prompt_cache

# Specify the checkpoint
checkpoint = "mlx-community/Qwen2.5-32B-Instruct-4bit"

# Load the corresponding model and tokenizer
model, tokenizer = load(path_or_hf_repo=checkpoint)


# An example tool, make sure to include a docstring and type hints
def multiply(a: float, b: float):
    """
    A function that multiplies two numbers

    Args:
        a: The first number to multiply
        b: The second number to multiply
    """
    return a * b


tools = {"multiply": multiply}

# Specify the prompt and conversation history
prompt = "Multiply 12234585 and 48838483920."
messages = [{"role": "user", "content": prompt}]

prompt = tokenizer.apply_chat_template(
    messages, add_generation_prompt=True, tools=list(tools.values())
)

prompt_cache = make_prompt_cache(model)

# Generate the initial tool call:
response = generate(
    model=model,
    tokenizer=tokenizer,
    prompt=prompt,
    max_tokens=2048,
    verbose=True,
    prompt_cache=prompt_cache,
)

# Parse the tool call:
# (Note, the tool call format is model specific)
tool_open = "<tool_call>"
tool_close = "</tool_call>"
start_tool = response.find(tool_open) + len(tool_open)
end_tool = response.find(tool_close)
tool_call = json.loads(response[start_tool:end_tool].strip())
tool_result = tools[tool_call["name"]](**tool_call["arguments"])

# Put the tool result in the prompt
messages = [{"role": "tool", "name": tool_call["name"], "content": tool_result}]
prompt = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
)

# Generate the final response:
response = generate(
    model=model,
    tokenizer=tokenizer,
    prompt=prompt,
    max_tokens=2048,
    verbose=True,
    prompt_cache=prompt_cache,
)

>>>> llms/mlx_lm/examples/merge_config.yaml
models: 
  - OpenPipe/mistral-ft-optimized-1218
  - mlabonne/NeuralHermes-2.5-Mistral-7B
method: slerp
parameters:
  t:
    - filter: self_attn
      value: [0, 0.5, 0.3, 0.7, 1]
    - filter: mlp
      value: [1, 0.5, 0.7, 0.3, 0]
    - value: 0.5

>>>> llms/mlx_lm/examples/generate_response.py
# Copyright © 2024 Apple Inc.

from mlx_lm import generate, load

# Specify the checkpoint
checkpoint = "mistralai/Mistral-7B-Instruct-v0.3"

# Load the corresponding model and tokenizer
model, tokenizer = load(path_or_hf_repo=checkpoint)

# Specify the prompt and conversation history
prompt = "Why is the sky blue?"
conversation = [{"role": "user", "content": prompt}]

# Transform the prompt into the chat template
prompt = tokenizer.apply_chat_template(
    conversation=conversation, add_generation_prompt=True
)

# Specify the maximum number of tokens
max_tokens = 1_000

# Specify if tokens and timing information will be printed
verbose = True

# Generate a response with the specified settings
response = generate(
    model=model,
    tokenizer=tokenizer,
    prompt=prompt,
    max_tokens=max_tokens,
    verbose=verbose,
)

>>>> llms/mlx_lm/examples/pipeline_generate.py
# Copyright © 2024 Apple Inc.

"""
Run with:

```
mlx.launch 
 --hostfile /path/to/hosts.txt 
 --backend mpi 
 /path/to/pipeline_generate.py 
 --prompt "hello world"
```

Make sure you can run MLX over MPI on two hosts. For more information see the
documentation:

https://ml-explore.github.io/mlx/build/html/usage/distributed.html).
"""

import argparse
import json
from pathlib import Path

import mlx.core as mx
from huggingface_hub import snapshot_download
from mlx.utils import tree_flatten
from mlx_lm import load, stream_generate
from mlx_lm.utils import load_model, load_tokenizer


def download(repo: str, allow_patterns: list[str]) -> Path:
    return Path(
        snapshot_download(
            repo,
            allow_patterns=allow_patterns,
        )
    )


def shard_and_load(repo):
    # Get model path with everything but weight safetensors
    model_path = download(
        args.model,
        allow_patterns=["*.json", "*.py", "tokenizer.model", "*.tiktoken", "*.txt"],
    )

    # Lazy load and shard model to figure out
    # which weights we need
    model, _ = load_model(model_path, lazy=True, strict=False)

    group = mx.distributed.init(backend="mpi")
    rank = group.rank()
    model.model.pipeline(group)

    # Figure out which files we need for the local shard
    with open(model_path / "model.safetensors.index.json", "r") as fid:
        weight_index = json.load(fid)["weight_map"]

    local_files = set()
    for k, _ in tree_flatten(model.parameters()):
        local_files.add(weight_index[k])

    # Download weights for local shard
    download(args.model, allow_patterns=local_files)

    # Load and shard the model, and load the weights
    tokenizer = load_tokenizer(model_path)
    model, _ = load_model(model_path, lazy=True, strict=False)
    model.model.pipeline(group)
    mx.eval(model.parameters())

    # Synchronize processes before generation to avoid timeout if downloading
    # model for the first time.
    mx.eval(mx.distributed.all_sum(mx.array(1.0), stream=mx.cpu))
    return model, tokenizer


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="LLM pipelined inference example")
    parser.add_argument(
        "--model",
        default="mlx-community/DeepSeek-R1-3bit",
        help="HF repo or path to local model.",
    )
    parser.add_argument(
        "--prompt",
        "-p",
        default="Write a quicksort in C++.",
        help="Message to be processed by the model ('-' reads from stdin)",
    )
    parser.add_argument(
        "--max-tokens",
        "-m",
        type=int,
        default=256,
        help="Maximum number of tokens to generate",
    )
    args = parser.parse_args()

    group = mx.distributed.init(backend="mpi")
    rank = group.rank()

    def rprint(*args, **kwargs):
        if rank == 0:
            print(*args, **kwargs)

    model, tokenizer = shard_and_load(args.model)

    messages = [{"role": "user", "content": args.prompt}]
    prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True)

    for response in stream_generate(
        model, tokenizer, prompt, max_tokens=args.max_tokens
    ):
        rprint(response.text, end="", flush=True)

    rprint()
    rprint("=" * 10)
    rprint(
        f"Prompt: {response.prompt_tokens} tokens, "
        f"{response.prompt_tps:.3f} tokens-per-sec"
    )
    rprint(
        f"Generation: {response.generation_tokens} tokens, "
        f"{response.generation_tps:.3f} tokens-per-sec"
    )
    rprint(f"Peak memory: {response.peak_memory:.3f} GB")

>>>> llms/mlx_lm/sample_utils.py
# Copyright © 2023-2024 Apple Inc.

import math
from functools import partial
from typing import Callable, Dict, Optional

import mlx.core as mx


def make_sampler(
    temp: float = 0.0,
    top_p: float = 0.0,
    min_p: float = 0.0,
    min_tokens_to_keep: int = 1,
    top_k: int = -1,
) -> Callable[mx.array, mx.array]:
    """
    Make a sampler function for use with ``generate_step``.

    Args:
        temp (float): The temperature for sampling, if 0 the argmax is used.
          Default: ``0``.
        top_p (float, optional): Nulceus sampling, higher means model considers
          more less likely words.
        min_p (float, optional): The minimum value (scaled by the top token's
          probability) that a token probability must have to be considered.
        min_tokens_to_keep (int, optional): Minimum number of tokens that cannot
          be filtered by min_p sampling.
        top_k (int, optional): The top k tokens ranked by probability to constrain
          the sampling to.

    Returns:
        Callable[mx.array, mx.array]:
            A sampler which takes log-probabilities and returns tokens.
    """
    if temp == 0:
        return lambda x: mx.argmax(x, axis=-1)
    elif top_p > 0 and top_p < 1.0:
        return lambda x: top_p_sampling(x, top_p, temp)
    elif min_p != 0.0:
        return lambda x: min_p_sampling(x, min_p, min_tokens_to_keep, temp)
    elif top_k > 0:
        return lambda x: top_k_sampling(x, top_k, temp)
    else:
        return lambda x: categorical_sampling(x, temp)


def make_logits_processors(
    logit_bias: Optional[Dict[int, float]] = None,
    repetition_penalty: Optional[float] = None,
    repetition_context_size: Optional[int] = 20,
):
    """
    Make logits processors for use with ``generate_step``.

    Args:
        repetition_penalty (float, optional): The penalty factor for repeating
          tokens.
        repetition_context_size (int, optional): The number of tokens to
          consider for repetition penalty. Default: ``20``.
        logit_bias (dictionary, optional): Additive logit bias.

    Returns:
        List[Callable[[mx.array, mx.array], mx.array]]:
            A list of logits processors. Each processor in the list is a
            callable which takes an array of tokens and an array of logits
            and returns the updated logits.
    """
    logits_processors = []
    if logit_bias:
        indices = mx.array(list(logit_bias.keys()))
        values = mx.array(list(logit_bias.values()))

        def logit_bias_processor(_, logits):
            logits[:, indices] += values
            return logits

        logits_processors.append(logit_bias_processor)

    if repetition_penalty and repetition_penalty != 0.0:
        logits_processors.append(
            make_repetition_penalty(repetition_penalty, repetition_context_size)
        )
    return logits_processors


@partial(mx.compile, inputs=mx.random.state, outputs=mx.random.state)
def top_k_sampling(
    logprobs: mx.array,
    top_k: int,
    temperature=1.0,
) -> mx.array:
    """
    Sample from only the top K tokens ranked by probability.

    Args:
        logprobs: A vector of log probabilities.
        top_k (int): Top k tokens to sample from.
    """
    vocab_size = logprobs.shape[-1]
    if not isinstance(top_k, int) or not (0 < top_k < vocab_size):
        raise ValueError(
            f"`top_k` has to be an integer in the (0, {vocab_size}] interval,"
            f" but is {top_k}."
        )
    logprobs = logprobs * (1 / temperature)
    mask_idx = mx.argpartition(-logprobs, kth=top_k - 1, axis=-1)[..., top_k:]
    masked_logprobs = mx.put_along_axis(
        logprobs, mask_idx, mx.array(-float("inf"), logprobs.dtype), axis=-1
    )
    return mx.random.categorical(masked_logprobs, axis=-1)


@partial(mx.compile, inputs=mx.random.state, outputs=mx.random.state)
def min_p_sampling(
    logprobs: mx.array,
    min_p: float,
    min_tokens_to_keep: int = 1,
    temperature=1.0,
) -> mx.array:
    """
    Apply min-p sampling to the logprobs.

    Min-p keeps all tokens that are above a minimum probability, scaled by the
    probability of the most likely token. As a result, the filter is more
    aggressive given a very high-probability token.

    Args:
        logprobs: A vector of log probabilities.
        min_p (float): Minimum token probability. Typical values are in the
            0.01-0.2 range, comparably selective as setting `top_p` in the
            0.99-0.8 range.
        min_tokens_to_keep (int, optional): Minimum number of tokens that cannot
            be filtered. Default: ``1``.

    """
    if not (0 <= min_p <= 1.0):
        raise ValueError(
            f"`min_p` has to be a float in the [0, 1] interval, but is {min_p}"
        )
    if not isinstance(min_tokens_to_keep, int) or (min_tokens_to_keep < 1):
        raise ValueError(
            f"`min_tokens_to_keep` has to be a positive integer, but is {min_tokens_to_keep}"
        )
    # reference implementation: https://github.com/huggingface/transformers/blob/main/src/transformers/generation/logits_process.py#L531-L605

    logprobs = logprobs * (1 / temperature)

    # Indices sorted in decreasing order
    sorted_indices = mx.argsort(-logprobs, axis=-1)
    sorted_logprobs = mx.take_along_axis(logprobs, sorted_indices, axis=-1)

    # Top probability
    top_logprobs = sorted_logprobs[:, 0:1]

    # Calculate the min_p threshold
    scaled_min_p = top_logprobs + math.log(min_p)

    # Mask tokens that have a probability less than the scaled min_p
    tokens_to_remove = sorted_logprobs < scaled_min_p
    tokens_to_remove[..., :min_tokens_to_keep] = False

    # Create pool of tokens with probability less than scaled min_p
    selected_logprobs = mx.where(tokens_to_remove, -float("inf"), sorted_logprobs)

    # Return sampled tokens
    sorted_tokens = mx.random.categorical(selected_logprobs, axis=-1)[:, None]
    return mx.take_along_axis(sorted_indices, sorted_tokens, axis=-1).squeeze(1)


@partial(mx.compile, inputs=mx.random.state, outputs=mx.random.state)
def top_p_sampling(logits: mx.array, top_p: float, temperature: float) -> mx.array:
    """
    Apply top-p (nucleus) sampling to logits.

    Args:
        logits: The logits from the model's output.
        top_p: The cumulative probability threshold for top-p filtering.
        temperature: Temperature parameter for softmax distribution reshaping.
    Returns:
        token selected based on the top-p criterion.
    """
    # referenced implementation from https://github.com/huggingface/transformers/blob/main/src/transformers/generation/logits_process.py#L449-L460
    probs = mx.softmax(logits * (1 / temperature), axis=-1)

    # sort probs in ascending order
    sorted_indices = mx.argsort(probs, axis=-1)
    sorted_probs = mx.take_along_axis(probs, sorted_indices, axis=-1)

    cumulative_probs = mx.cumsum(sorted_probs, axis=-1)

    # select tokens with cumulative probs below threshold
    top_probs = mx.where(
        cumulative_probs > 1 - top_p,
        sorted_probs,
        0,
    )

    sorted_tokens = mx.random.categorical(mx.log(top_probs), axis=-1)[:, None]
    return mx.take_along_axis(sorted_indices, sorted_tokens, axis=-1).squeeze(1)


@partial(mx.compile, inputs=mx.random.state, outputs=mx.random.state)
def categorical_sampling(logits, temp):
    return mx.random.categorical(logits * (1 / temp))


def make_repetition_penalty(penalty: float, context_size: int = 20):
    """
    Make repetition penalty processor.

    Paper: https://arxiv.org/abs/1909.05858

    Args:
        penalty (float): The repetition penalty factor to be applied.
        context_size (int): The number of previous tokens to use.
            Default: ``20``.

    Returns:
        Callable[[mx.array, List[int]], mx.array]:
            The repetition penalty processor.
    """
    if penalty < 0 or not isinstance(penalty, (int, float)):
        raise ValueError(f"penalty must be a non-negative float, got {penalty}")

    def repetition_penalty_processor(tokens, logits):
        if len(tokens) > 0:
            tokens = tokens[-context_size:]
            selected_logits = logits[:, tokens]
            selected_logits = mx.where(
                selected_logits < 0,
                selected_logits * penalty,
                selected_logits / penalty,
            )
            logits[:, tokens] = selected_logits
        return logits

    return repetition_penalty_processor

>>>> llms/mlx_lm/utils.py
# Copyright © 2023-2024 Apple Inc.

import contextlib
import copy
import functools
import glob
import importlib
import json
import logging
import os
import shutil
import time
from dataclasses import dataclass
from pathlib import Path
from textwrap import dedent
from typing import (
    Any,
    Callable,
    Dict,
    Generator,
    List,
    NamedTuple,
    Optional,
    Tuple,
    Type,
    Union,
)

import mlx.core as mx
import mlx.nn as nn

if os.getenv("MLXLM_USE_MODELSCOPE", "False").lower() == "true":
    try:
        from modelscope import snapshot_download
    except ImportError:
        raise ImportError(
            "Please run `pip install modelscope` to activate the ModelScope."
        )
else:
    from huggingface_hub import snapshot_download

from mlx.utils import tree_flatten, tree_reduce
from transformers import PreTrainedTokenizer

# Local imports
from .models import cache
from .sample_utils import make_logits_processors, make_sampler
from .tokenizer_utils import TokenizerWrapper, load_tokenizer
from .tuner.utils import dequantize as dequantize_model
from .tuner.utils import load_adapters, nparams

# Constants
MODEL_REMAPPING = {
    "mistral": "llama",  # mistral is compatible with llama
    "phi-msft": "phixtral",
    "falcon_mamba": "mamba",
}

MAX_FILE_SIZE_GB = 5

# A stream on the default device just for generation
generation_stream = mx.new_stream(mx.default_device())


class ModelNotFoundError(Exception):
    def __init__(self, message):
        self.message = message
        super().__init__(self.message)


@dataclass
class GenerationResponse:
    """
    The output of :func:`stream_generate`.

    Args:
        text (str): The next segment of decoded text. This can be an empty string.
        token (int): The next token.
        from_draft (bool): Whether the token was generated by the draft model.
        logprobs (mx.array): A vector of log probabilities.
        prompt_tokens (int): The number of tokens in the prompt.
        prompt_tps (float): The prompt processing tokens-per-second.
        generation_tokens (int): The number of generated tokens.
        generation_tps (float): The tokens-per-second for generation.
        peak_memory (float): The peak memory used so far in GB.
        finish_reason (str): The reason the response is being sent: "length", "stop" or `None`
    """

    text: str
    token: int
    logprobs: mx.array
    from_draft: bool
    prompt_tokens: int
    prompt_tps: float
    generation_tokens: int
    generation_tps: float
    peak_memory: float
    finish_reason: Optional[str] = None


@contextlib.contextmanager
def wired_limit(model: nn.Module, streams: Optional[List[mx.Stream]] = None):
    """
    A context manager to temporarily change the wired limit.

    Note, the wired limit should not be changed during an async eval.  If an
    async eval could be running pass in the streams to synchronize with prior
    to exiting the context manager.
    """
    model_bytes = tree_reduce(
        lambda acc, x: acc + x.nbytes if isinstance(x, mx.array) else acc, model, 0
    )
    max_rec_size = mx.metal.device_info()["max_recommended_working_set_size"]
    if model_bytes > 0.9 * max_rec_size:
        model_mb = model_bytes // 2**20
        max_rec_mb = max_rec_size // 2**20
        print(
            f"[WARNING] Generating with a model that requires {model_mb} MB "
            f"which is close to the maximum recommended size of {max_rec_mb} "
            "MB. This can be slow. See the documentation for possible work-arounds: "
            "https://github.com/ml-explore/mlx-examples/tree/main/llms#large-models"
        )
    old_limit = mx.metal.set_wired_limit(max_rec_size)
    try:
        yield None
    finally:
        if streams is not None:
            for s in streams:
                mx.synchronize(s)
        else:
            mx.synchronize()
        mx.metal.set_wired_limit(old_limit)


def _get_classes(config: dict):
    """
    Retrieve the model and model args classes based on the configuration.

    Args:
        config (dict): The model configuration.

    Returns:
        A tuple containing the Model class and the ModelArgs class.
    """
    model_type = config["model_type"]
    model_type = MODEL_REMAPPING.get(model_type, model_type)
    try:
        arch = importlib.import_module(f"mlx_lm.models.{model_type}")
    except ImportError:
        msg = f"Model type {model_type} not supported."
        logging.error(msg)
        raise ValueError(msg)

    return arch.Model, arch.ModelArgs


def compute_bits_per_weight(model):
    model_bytes = tree_reduce(
        lambda acc, x: acc + x.nbytes if isinstance(x, mx.array) else acc, model, 0
    )
    leaf_modules = tree_flatten(
        model.leaf_modules(), is_leaf=lambda m: isinstance(m, nn.Module)
    )
    model_params = sum(nparams(m) for _, m in leaf_modules)
    return model_bytes * 8 / model_params


def get_model_path(path_or_hf_repo: str, revision: Optional[str] = None) -> Path:
    """
    Ensures the model is available locally. If the path does not exist locally,
    it is downloaded from the Hugging Face Hub.

    Args:
        path_or_hf_repo (str): The local path or Hugging Face repository ID of the model.
        revision (str, optional): A revision id which can be a branch name, a tag, or a commit hash.

    Returns:
        Path: The path to the model.
    """
    model_path = Path(path_or_hf_repo)

    if not model_path.exists():
        try:
            model_path = Path(
                snapshot_download(
                    path_or_hf_repo,
                    revision=revision,
                    allow_patterns=[
                        "*.json",
                        "*.safetensors",
                        "*.py",
                        "tokenizer.model",
                        "*.tiktoken",
                        "tiktoken.model",
                        "*.txt",
                        "*.jsonl",
                    ],
                )
            )
        except:
            raise ModelNotFoundError(
                f"Model not found for path or HF repo: {path_or_hf_repo}.\n"
                "Please make sure you specified the local path or Hugging Face"
                " repo id correctly.\nIf you are trying to access a private or"
                " gated Hugging Face repo, make sure you are authenticated:\n"
                "https://huggingface.co/docs/huggingface_hub/en/guides/cli#huggingface-cli-login"
            ) from None
    return model_path


def maybe_quantize_kv_cache(prompt_cache, quantized_kv_start, kv_group_size, kv_bits):
    if (
        kv_bits is not None
        and not isinstance(prompt_cache[0], cache.QuantizedKVCache)
        and prompt_cache[0].offset > quantized_kv_start
    ):
        for i in range(len(prompt_cache)):
            if isinstance(prompt_cache[i], cache.KVCache):
                prompt_cache[i] = prompt_cache[i].to_quantized(
                    group_size=kv_group_size, bits=kv_bits
                )


def generate_step(
    prompt: mx.array,
    model: nn.Module,
    *,
    max_tokens: int = 256,
    sampler: Optional[Callable[mx.array, mx.array]] = None,
    logits_processors: Optional[List[Callable[[mx.array, mx.array], mx.array]]] = None,
    max_kv_size: Optional[int] = None,
    prompt_cache: Optional[Any] = None,
    prefill_step_size: int = 512,
    kv_bits: Optional[int] = None,
    kv_group_size: int = 64,
    quantized_kv_start: int = 0,
    prompt_progress_callback: Optional[Callable[int, int]] = None,
) -> Generator[Tuple[mx.array, mx.array], None, None]:
    """
    A generator producing token ids based on the given prompt from the model.

    Args:
        prompt (mx.array): The input prompt.
        model (nn.Module): The model to use for generation.
        max_tokens (int): The maximum number of tokens. Use``-1`` for an infinite
          generator. Default: ``256``.
        sampler (Callable[mx.array, mx.array], optional): A sampler for sampling a
          token from a vector of log probabilities. Default: ``None``.
        logits_processors (List[Callable[[mx.array, mx.array], mx.array]], optional):
          A list of functions that take tokens and logits and return the processed
          logits. Default: ``None``.
        max_kv_size (int, optional): Maximum size of the key-value cache. Old
          entries (except the first 4 tokens) will be overwritten.
        prompt_cache (List[Any], optional): A pre-computed prompt cache. Note, if
          provided, the cache will be updated in place.
        prefill_step_size (int): Step size for processing the prompt.
        kv_bits (int, optional): Number of bits to use for KV cache quantization.
          None implies no cache quantization. Default: ``None``.
        kv_group_size (int): Group size for KV cache quantization. Default: ``64``.
        quantized_kv_start (int): Step to begin using a quantized KV cache.
           when ``kv_bits`` is non-None. Default: ``0``.
        prompt_prorgress_callback (Callable[int, int]): A call-back which takes the
           prompt tokens processed so far and the total number of prompt tokens.

    Yields:
        Tuple[mx.array, mx.array]: One token and a vector of log probabilities.
    """

    y = prompt
    tokens = None

    # Create the KV cache for generation
    if prompt_cache is None:
        prompt_cache = cache.make_prompt_cache(
            model,
            max_kv_size=max_kv_size,
        )
    elif len(prompt_cache) != len(model.layers):
        raise ValueError("Wrong number of layers in the prompt cache.")

    prompt_progress_callback = prompt_progress_callback or (lambda *_: None)

    quantize_cache_fn = functools.partial(
        maybe_quantize_kv_cache,
        quantized_kv_start=quantized_kv_start,
        kv_group_size=kv_group_size,
        kv_bits=kv_bits,
    )

    sampler = sampler or (lambda x: mx.argmax(x, axis=-1))

    def _step(y):
        with mx.stream(generation_stream):
            logits = model(y[None], cache=prompt_cache)
            logits = logits[:, -1, :]

            if logits_processors:
                nonlocal tokens
                tokens = mx.concat([tokens, y]) if tokens is not None else y

                for processor in logits_processors:
                    logits = processor(tokens, logits)

            quantize_cache_fn(prompt_cache)

            logprobs = logits - mx.logsumexp(logits, keepdims=True)
            y = sampler(logprobs)
            return y, logprobs.squeeze(0)

    with mx.stream(generation_stream):
        total_prompt_tokens = y.size
        prompt_processed_tokens = 0
        while y.size > prefill_step_size:
            model(y[:prefill_step_size][None], cache=prompt_cache)
            quantize_cache_fn(prompt_cache)
            mx.eval([c.state for c in prompt_cache])
            prompt_progress_callback(prompt_processed_tokens, total_prompt_tokens)
            prompt_processed_tokens += prefill_step_size
            y = y[prefill_step_size:]
            mx.metal.clear_cache()

        y, logprobs = _step(y)

    mx.async_eval(y, logprobs)
    n = 0
    while True:
        if n != max_tokens:
            next_y, next_logprobs = _step(y)
            mx.async_eval(next_y, next_logprobs)
        if n == 0:
            mx.eval(y)
            prompt_progress_callback(total_prompt_tokens, total_prompt_tokens)
        if n == max_tokens:
            break
        yield y.item(), logprobs
        if n % 256 == 0:
            mx.metal.clear_cache()
        y, logprobs = next_y, next_logprobs
        n += 1


def speculative_generate_step(
    prompt: mx.array,
    model: nn.Module,
    draft_model: nn.Module,
    *,
    num_draft_tokens=2,
    max_tokens: int = 256,
    sampler: Optional[Callable[mx.array, mx.array]] = None,
    logits_processors: Optional[List[Callable[[mx.array, mx.array], mx.array]]] = None,
    prompt_cache: Optional[Any] = None,
    prefill_step_size: int = 512,
    kv_bits: Optional[int] = None,
    kv_group_size: int = 64,
    quantized_kv_start: int = 0,
) -> Generator[Tuple[mx.array, mx.array, bool], None, None]:
    """
    A generator producing token ids based on the given prompt from the model.

    Args:
        prompt (mx.array): The input prompt.
        model (nn.Module): The model to use for generation.
        draft_model (nn.Module): The draft model for speculative decoding.
        num_draft_tokens (int, optional): The number of draft tokens for
          speculative decoding. Default: ``2``.
        max_tokens (int): The maximum number of tokens. Use``-1`` for an infinite
          generator. Default: ``256``.
        sampler (Callable[mx.array, mx.array], optional): A sampler for sampling a
          token from a vector of log probabilities. Default: ``None``.
        logits_processors (List[Callable[[mx.array, mx.array], mx.array]], optional):
          A list of functions that take tokens and logits and return the processed
          logits. Default: ``None``.
        prompt_cache (List[Any], optional): A pre-computed prompt cache. Note, if
          provided, the cache will be updated in place. The cache must be trimmable.
        prefill_step_size (int): Step size for processing the prompt.
        kv_bits (int, optional): Number of bits to use for KV cache quantization.
          None implies no cache quantization. Default: ``None``.
        kv_group_size (int): Group size for KV cache quantization. Default: ``64``.
        quantized_kv_start (int): Step to begin using a quantized KV cache.
           when ``kv_bits`` is non-None. Default: ``0``.

    Yields:
        Tuple[mx.array, mx.array, bool]: One token, a vector of log probabilities,
          and a bool indicating if the token was generated by the draft model
    """

    y = prompt.astype(mx.uint32)
    prev_tokens = None

    # Create the KV cache for generation
    if prompt_cache is None:
        model_cache = cache.make_prompt_cache(model)
        draft_cache = cache.make_prompt_cache(draft_model)
    elif len(prompt_cache) != (len(model.layers) + len(draft_model.layers)):
        raise ValueError("Wrong number of layers in the prompt cache.")
    else:
        model_cache = prompt_cache[: len(model.layers)]
        draft_cache = prompt_cache[len(model.layers) :]

    sampler = sampler or (lambda x: mx.argmax(x, axis=-1))

    quantize_cache_fn = functools.partial(
        maybe_quantize_kv_cache,
        quantized_kv_start=quantized_kv_start,
        kv_group_size=kv_group_size,
        kv_bits=kv_bits,
    )

    def _process_and_sample(tokens, logits):
        if logits_processors:
            for processor in logits_processors:
                logits = processor(tokens, logits)

        logprobs = logits - mx.logsumexp(logits, axis=-1, keepdims=True)
        y = sampler(logprobs)
        return y, logprobs

    def _step(model, cache, y, n_predict=1):
        with mx.stream(generation_stream):
            logits = model(y[None], cache=cache)
            logits = logits[:, -n_predict:, :]

            quantize_cache_fn(cache)
            if logits_processors:
                nonlocal prev_tokens
                out_y, out_logprobs = [], []
                if n_predict > 1:
                    y = y[: -(n_predict - 1)]
                for i in range(n_predict):
                    prev_tokens = (
                        mx.concat([prev_tokens, y]) if prev_tokens is not None else y
                    )
                    y, logprobs = _process_and_sample(prev_tokens, logits[:, i, :])
                    out_y.append(y)
                    out_logprobs.append(logprobs)
                return mx.concatenate(out_y, axis=0), mx.concatenate(
                    out_logprobs, axis=0
                )
            else:
                return _process_and_sample(None, logits.squeeze(0))

    def _prefill(model, cache, y):
        while y.size > prefill_step_size:
            model(y[:prefill_step_size][None], cache=cache)
            quantize_cache_fn(cache)
            mx.eval([c.state for c in cache])
            y = y[prefill_step_size:]
            mx.metal.clear_cache()
        return y

    def _rewind_cache(num_draft, num_accept):
        cache.trim_prompt_cache(model_cache, num_draft - num_accept)
        cache.trim_prompt_cache(draft_cache, max(num_draft - num_accept - 1, 0))

    def _draft_generate(y, num_draft):
        if num_draft == 0:
            return mx.array([], mx.uint32)
        ys = []
        for _ in range(num_draft):
            y, _ = _step(draft_model, draft_cache, y)
            mx.async_eval(y)
            ys.append(y)
        return mx.concatenate(ys)

    with mx.stream(generation_stream):
        draft_y = _prefill(draft_model, draft_cache, y)
        y = _prefill(model, model_cache, y)

    ntoks = 0
    # Set these so the finally block doesn't raise
    num_draft = 0
    n = 0
    try:
        while True:
            num_draft = min(max_tokens - ntoks, num_draft_tokens)
            draft_tokens = _draft_generate(draft_y, num_draft)
            if prev_tokens is not None:
                prev_tokens = prev_tokens[: prev_tokens.size - y.size - num_draft + 1]
            y = mx.concatenate([y, draft_tokens])
            tokens, logprobs = _step(model, model_cache, y, num_draft + 1)
            mx.eval(tokens, draft_tokens)
            draft_tokens = draft_tokens.tolist()
            tokens = tokens.tolist()
            n = 0
            while n < num_draft:
                tn, dtn, lpn = tokens[n], draft_tokens[n], logprobs[n]
                if tn != dtn:
                    break
                n += 1
                ntoks += 1
                yield tn, lpn, True
                if ntoks == max_tokens:
                    break
            if ntoks < max_tokens:
                ntoks += 1
                yield tokens[n], logprobs[n], False

            if ntoks == max_tokens:
                break

            y = mx.array([tokens[n]], mx.uint32)
            draft_y = y

            # If we accepted all the draft tokens, include the last
            # draft token in the next draft step since it hasn't been
            # processed yet by the draft model
            if n == num_draft:
                draft_y = mx.concatenate(
                    [mx.array(draft_tokens[-1:], mx.uint32), draft_y]
                )

            if prev_tokens is not None:
                prev_tokens = prev_tokens[: -max(num_draft - n, 1)]
            _rewind_cache(num_draft, n)
    finally:
        _rewind_cache(num_draft, n)


def stream_generate(
    model: nn.Module,
    tokenizer: Union[PreTrainedTokenizer, TokenizerWrapper],
    prompt: Union[str, mx.array, List[int]],
    draft_model: Optional[nn.Module] = None,
    **kwargs,
) -> Generator[GenerationResponse, None, None]:
    """
    A generator producing text based on the given prompt from the model.

    Args:
        model (nn.Module): The model to use for generation.
        tokenizer (PreTrainedTokenizer): The tokenizer.
        prompt (Union[str, mx.array, List[int]]): The input prompt string or
          integer tokens.
        draft_model (Optional[nn.Module]): An optional draft model. If provided
          then speculative decoding is used. The draft model must use the same
          tokenizer as the main model. Default: ``None``.
        kwargs: The remaining options get passed to :func:`generate_step`.
          See :func:`generate_step` for more details.

    Yields:
        GenerationResponse: An instance containing the generated text segment and
            associated metadata. See :class:`GenerationResponse` for details.
    """
    if not isinstance(tokenizer, TokenizerWrapper):
        tokenizer = TokenizerWrapper(tokenizer)

    if not isinstance(prompt, mx.array):
        if isinstance(prompt, str):
            # Try to infer if special tokens are needed
            add_special_tokens = tokenizer.bos_token is None or not prompt.startswith(
                tokenizer.bos_token
            )
            prompt = tokenizer.encode(prompt, add_special_tokens=add_special_tokens)
        prompt = mx.array(prompt)

    detokenizer = tokenizer.detokenizer

    if draft_model is None:
        kwargs.pop("num_draft_tokens", None)
        token_generator = generate_step(prompt, model, **kwargs)
        # from_draft always false for non-speculative generation
        token_generator = (
            (token, logprobs, False) for token, logprobs in token_generator
        )
    else:
        kwargs.pop("max_kv_size", None)
        token_generator = speculative_generate_step(
            prompt, model, draft_model, **kwargs
        )
    with wired_limit(model, [generation_stream]):
        detokenizer.reset()
        tic = time.perf_counter()
        for n, (token, logprobs, from_draft) in enumerate(token_generator):
            if n == 0:
                prompt_time = time.perf_counter() - tic
                prompt_tps = prompt.size / prompt_time
                tic = time.perf_counter()
            if token in tokenizer.eos_token_ids:
                break

            detokenizer.add_token(token)

            yield GenerationResponse(
                text=detokenizer.last_segment,
                token=token,
                logprobs=logprobs,
                from_draft=from_draft,
                prompt_tokens=prompt.size,
                prompt_tps=prompt_tps,
                generation_tokens=n + 1,
                generation_tps=(n + 1) / (time.perf_counter() - tic),
                peak_memory=mx.metal.get_peak_memory() / 1e9,
                finish_reason=None,
            )

        detokenizer.finalize()
        yield GenerationResponse(
            text=detokenizer.last_segment,
            token=token,
            logprobs=logprobs,
            from_draft=from_draft,
            prompt_tokens=prompt.size,
            prompt_tps=prompt_tps,
            generation_tokens=n + 1,
            generation_tps=(n + 1) / (time.perf_counter() - tic),
            peak_memory=mx.metal.get_peak_memory() / 1e9,
            finish_reason="stop" if token in tokenizer.eos_token_ids else "length",
        )


def generate(
    model: nn.Module,
    tokenizer: Union[PreTrainedTokenizer, TokenizerWrapper],
    prompt: Union[str, List[int]],
    verbose: bool = False,
    formatter: Optional[Callable] = None,
    **kwargs,
) -> str:
    """
    Generate a complete response from the model.

    Args:
       model (nn.Module): The language model.
       tokenizer (PreTrainedTokenizer): The tokenizer.
       prompt (Union[str, List[int]]): The input prompt string or integer tokens.
       verbose (bool): If ``True``, print tokens and timing information.
           Default: ``False``.
       kwargs: The remaining options get passed to :func:`stream_generate`.
          See :func:`stream_generate` for more details.
    """
    if formatter is not None:
        print(
            "[Warning] Text formatting is deprecated and no longer used. "
            "The argument will be removed in a future version."
        )
    if verbose:
        print("=" * 10)

    text = ""
    for response in stream_generate(model, tokenizer, prompt, **kwargs):
        if verbose:
            print(response.text, end="", flush=True)
        text += response.text

    if verbose:
        print()
        print("=" * 10)
        if len(text) == 0:
            print("No text generated for this prompt")
            return
        print(
            f"Prompt: {response.prompt_tokens} tokens, "
            f"{response.prompt_tps:.3f} tokens-per-sec"
        )
        print(
            f"Generation: {response.generation_tokens} tokens, "
            f"{response.generation_tps:.3f} tokens-per-sec"
        )
        print(f"Peak memory: {response.peak_memory:.3f} GB")
    return text


def load_config(model_path: Path) -> dict:
    try:
        with open(model_path / "config.json", "r") as f:
            config = json.load(f)
    except FileNotFoundError:
        logging.error(f"Config file not found in {model_path}")
        raise
    return config


def load_model(
    model_path: Path,
    lazy: bool = False,
    strict: bool = True,
    model_config: dict = {},
    get_model_classes: Callable[[dict], Tuple[Type[nn.Module], Type]] = _get_classes,
) -> nn.Module:
    """
    Load and initialize the model from a given path.

    Args:
        model_path (Path): The path to load the model from.
        lazy (bool): If False eval the model parameters to make sure they are
            loaded in memory before returning, otherwise they will be loaded
            when needed. Default: ``False``
        strict (bool): Whether or not to raise an exception if weights don't
            match. Default: ``True``
        model_config (dict, optional): Optional configuration parameters for the
            model. Defaults to an empty dictionary.
        get_model_classes (Callable[[dict], Tuple[Type[nn.Module], Type]], optional):
            A function that returns the model class and model args class given a config.
            Defaults to the ``_get_classes`` function.

    Returns:
        nn.Module: The loaded and initialized model.

    Raises:
        FileNotFoundError: If the weight files (.safetensors) are not found.
        ValueError: If the model class or args class are not found or cannot be instantiated.
    """
    config = load_config(model_path)
    config.update(model_config)

    weight_files = glob.glob(str(model_path / "model*.safetensors"))

    if not weight_files:
        # Try weight for back-compat
        weight_files = glob.glob(str(model_path / "weight*.safetensors"))

    if not weight_files and strict:
        logging.error(f"No safetensors found in {model_path}")
        raise FileNotFoundError(f"No safetensors found in {model_path}")

    weights = {}
    for wf in weight_files:
        weights.update(mx.load(wf))

    model_class, model_args_class = get_model_classes(config=config)

    model_args = model_args_class.from_dict(config)
    model = model_class(model_args)

    if hasattr(model, "sanitize"):
        weights = model.sanitize(weights)

    if (quantization := config.get("quantization", None)) is not None:

        def class_predicate(p, m):
            # Handle custom per layer quantizations
            if p in config["quantization"]:
                return config["quantization"][p]
            if not hasattr(m, "to_quantized"):
                return False
            # Handle legacy models which may not have everything quantized
            return f"{p}.scales" in weights

        nn.quantize(
            model,
            group_size=quantization["group_size"],
            bits=quantization["bits"],
            class_predicate=class_predicate,
        )

    model.load_weights(list(weights.items()), strict=strict)

    if not lazy:
        mx.eval(model.parameters())

    model.eval()
    return model, config


def load(
    path_or_hf_repo: str,
    tokenizer_config={},
    model_config={},
    adapter_path: Optional[str] = None,
    lazy: bool = False,
) -> Tuple[nn.Module, TokenizerWrapper]:
    """
    Load the model and tokenizer from a given path or a huggingface repository.

    Args:
        path_or_hf_repo (Path): The path or the huggingface repository to load the model from.
        tokenizer_config (dict, optional): Configuration parameters specifically for the tokenizer.
            Defaults to an empty dictionary.
        model_config(dict, optional): Configuration parameters specifically for the model.
            Defaults to an empty dictionary.
        adapter_path (str, optional): Path to the LoRA adapters. If provided, applies LoRA layers
            to the model. Default: ``None``.
        lazy (bool): If ``False`` eval the model parameters to make sure they are
            loaded in memory before returning, otherwise they will be loaded
            when needed. Default: ``False``
    Returns:
        Tuple[nn.Module, TokenizerWrapper]: A tuple containing the loaded model and tokenizer.

    Raises:
        FileNotFoundError: If config file or safetensors are not found.
        ValueError: If model class or args class are not found.
    """
    model_path = get_model_path(path_or_hf_repo)

    model, config = load_model(model_path, lazy)
    if adapter_path is not None:
        model = load_adapters(model, adapter_path)
        model.eval()
    tokenizer = load_tokenizer(
        model_path, tokenizer_config, eos_token_ids=config.get("eos_token_id", None)
    )

    return model, tokenizer


def fetch_from_hub(
    model_path: Path, lazy: bool = False
) -> Tuple[nn.Module, dict, PreTrainedTokenizer]:
    model, config = load_model(model_path, lazy)
    tokenizer = load_tokenizer(
        model_path, eos_token_ids=config.get("eos_token_id", None)
    )
    return model, config, tokenizer


def make_shards(weights: dict, max_file_size_gb: int = MAX_FILE_SIZE_GB) -> list:
    """
    Splits the weights into smaller shards.

    Args:
        weights (dict): Model weights.
        max_file_size_gb (int): Maximum size of each shard in gigabytes.

    Returns:
        list: List of weight shards.
    """
    max_file_size_bytes = max_file_size_gb << 30
    shards = []
    shard, shard_size = {}, 0
    for k, v in weights.items():
        if shard_size + v.nbytes > max_file_size_bytes:
            shards.append(shard)
            shard, shard_size = {}, 0
        shard[k] = v
        shard_size += v.nbytes
    shards.append(shard)
    return shards


def upload_to_hub(path: str, upload_repo: str, hf_path: str):
    """
    Uploads the model to Hugging Face hub.

    Args:
        path (str): Local path to the model.
        upload_repo (str): Name of the HF repo to upload to.
        hf_path (str): Path to the original Hugging Face model.
    """
    import os

    from huggingface_hub import HfApi, ModelCard, logging

    from . import __version__

    card = ModelCard.load(hf_path)
    card.data.tags = ["mlx"] if card.data.tags is None else card.data.tags + ["mlx"]
    card.data.base_model = hf_path
    card.text = dedent(
        f"""
        # {upload_repo}

        The Model [{upload_repo}](https://huggingface.co/{upload_repo}) was
        converted to MLX format from [{hf_path}](https://huggingface.co/{hf_path})
        using mlx-lm version **{__version__}**.

        ## Use with mlx

        ```bash
        pip install mlx-lm
        ```

        ```python
        from mlx_lm import load, generate

        model, tokenizer = load("{upload_repo}")

        prompt = "hello"

        if tokenizer.chat_template is not None:
            messages = [{{"role": "user", "content": prompt}}]
            prompt = tokenizer.apply_chat_template(
                messages, add_generation_prompt=True
            )

        response = generate(model, tokenizer, prompt=prompt, verbose=True)
        ```
        """
    )
    card.save(os.path.join(path, "README.md"))

    logging.set_verbosity_info()

    api = HfApi()
    api.create_repo(repo_id=upload_repo, exist_ok=True)
    api.upload_large_folder(
        folder_path=path,
        repo_id=upload_repo,
        repo_type="model",
    )
    print(f"Upload successful, go to https://huggingface.co/{upload_repo} for details.")


def save_weights(
    save_path: Union[str, Path],
    weights: Dict[str, Any],
    *,
    donate_weights: bool = False,
) -> None:
    """Save model weights into specified directory."""
    if isinstance(save_path, str):
        save_path = Path(save_path)
    save_path.mkdir(parents=True, exist_ok=True)

    shards = make_shards(weights)
    shards_count = len(shards)
    shard_file_format = (
        "model-{:05d}-of-{:05d}.safetensors"
        if shards_count > 1
        else "model.safetensors"
    )

    total_size = sum(v.nbytes for v in weights.values())
    index_data = {"metadata": {"total_size": total_size}, "weight_map": {}}

    # Write the weights and make sure no references are kept other than the
    # necessary ones
    if donate_weights:
        weights.clear()
        del weights

    for i in range(len(shards)):
        shard = shards[i]
        shards[i] = None
        shard_name = shard_file_format.format(i + 1, shards_count)
        shard_path = save_path / shard_name

        mx.save_safetensors(str(shard_path), shard, metadata={"format": "mlx"})

        for weight_name in shard.keys():
            index_data["weight_map"][weight_name] = shard_name
        del shard

    index_data["weight_map"] = {
        k: index_data["weight_map"][k] for k in sorted(index_data["weight_map"])
    }

    with open(save_path / "model.safetensors.index.json", "w") as f:
        json.dump(
            index_data,
            f,
            indent=4,
        )


def quantize_model(
    model: nn.Module,
    config: dict,
    q_group_size: int,
    q_bits: int,
    quant_predicate: Optional[
        Callable[[str, nn.Module, dict], Union[bool, dict]]
    ] = None,
) -> Tuple:
    """
    Applies quantization to the model weights.

    Args:
        model (nn.Module): The model to be quantized.
        config (dict): Model configuration.
        q_group_size (int): Group size for quantization.
        q_bits (int): Bits per weight for quantization.
        quant_predicate (Callable): A callable that decides how
            to quantize each layer based on the path.
            Accepts the layer `path`, the `module` and the model `config`.
            Returns either a bool to signify quantize/no quantize or
            a dict of quantization parameters to pass to `to_quantized`.

    Returns:
        Tuple: Tuple containing quantized weights and config.
    """
    quantized_config = copy.deepcopy(config)
    quantized_config["quantization"] = {"group_size": q_group_size, "bits": q_bits}

    # Add any custom quantization parameters to the config as we go
    def _class_predicate(p, m):
        bool_or_params = quant_predicate(p, m, config)
        quantized_config["quantization"][p] = bool_or_params
        return bool_or_params

    nn.quantize(
        model,
        q_group_size,
        q_bits,
        class_predicate=_class_predicate if quant_predicate else None,
    )
    # support hf model tree #957
    quantized_config["quantization_config"] = quantized_config["quantization"]
    quantized_weights = dict(tree_flatten(model.parameters()))

    bpw = compute_bits_per_weight(model)
    print(f"[INFO] Quantized model with {bpw:.3f} bits per weight.")

    return quantized_weights, quantized_config


def save_config(
    config: dict,
    config_path: Union[str, Path],
) -> None:
    """Save the model configuration to the ``config_path``.

    The final configuration will be sorted before saving for better readability.

    Args:
        config (dict): The model configuration.
        config_path (Union[str, Path]): Model configuration file path.
    """
    # Clean unused keys
    config.pop("_name_or_path", None)

    # sort the config for better readability
    config = dict(sorted(config.items()))

    # write the updated config to the config_path (if provided)
    with open(config_path, "w") as fid:
        json.dump(config, fid, indent=4)


def mixed_quant_predicate_builder(
    low_bits: int = 4, high_bits: int = 4, group_size: int = 64
) -> Callable[[str, nn.Module, dict], Union[bool, dict]]:
    def mixed_quant_predicate(
        path: str,
        module: nn.Module,
        config: dict,
    ) -> Union[bool, dict]:
        """Implements mixed quantization predicates with similar choices to, for example, llama.cpp's Q4_K_M.
        Ref: https://github.com/ggerganov/llama.cpp/blob/917786f43d0f29b7c77a0c56767c0fa4df68b1c5/src/llama.cpp#L5265
        By Alex Barron: https://gist.github.com/barronalex/84addb8078be21969f1690c1454855f3
        """

        if not hasattr(module, "to_quantized"):
            return False

        index = int(path.split(".")[2]) if len(path.split(".")) > 2 else 0

        num_layers = config["num_hidden_layers"]
        use_more_bits = (
            index < num_layers // 8
            or index >= 7 * num_layers // 8
            or (index - num_layers // 8) % 3 == 2
        )
        if "v_proj" in path and use_more_bits:
            return {"group_size": group_size, "bits": high_bits}
        if "down_proj" in path and use_more_bits:
            return {"group_size": group_size, "bits": high_bits}
        if "lm_head" in path:
            return {"group_size": group_size, "bits": high_bits}

        return {"group_size": group_size, "bits": low_bits}

    return mixed_quant_predicate


mixed_3_6 = mixed_quant_predicate_builder(low_bits=3)
mixed_2_6 = mixed_quant_predicate_builder(low_bits=2)


def convert(
    hf_path: str,
    mlx_path: str = "mlx_model",
    quantize: bool = False,
    q_group_size: int = 64,
    q_bits: int = 4,
    dtype: str = "float16",
    upload_repo: str = None,
    revision: Optional[str] = None,
    dequantize: bool = False,
    quant_predicate: Optional[
        Callable[[str, nn.Module, dict], Union[bool, dict]]
    ] = None,
):
    # Check the save path is empty
    if isinstance(mlx_path, str):
        mlx_path = Path(mlx_path)

    if mlx_path.exists():
        raise ValueError(
            f"Cannot save to the path {mlx_path} as it already exists."
            " Please delete the file/directory or specify a new path to save to."
        )

    print("[INFO] Loading")
    model_path = get_model_path(hf_path, revision=revision)
    model, config, tokenizer = fetch_from_hub(model_path, lazy=True)

    weights = dict(tree_flatten(model.parameters()))
    dtype = getattr(mx, dtype)
    weights = {k: v.astype(dtype) for k, v in weights.items()}

    if quantize and dequantize:
        raise ValueError("Choose either quantize or dequantize, not both.")

    if quantize:
        print("[INFO] Quantizing")
        model.load_weights(list(weights.items()))
        weights, config = quantize_model(
            model, config, q_group_size, q_bits, quant_predicate=quant_predicate
        )

    if dequantize:
        print("[INFO] Dequantizing")
        model = dequantize_model(model)
        weights = dict(tree_flatten(model.parameters()))

    del model
    save_weights(mlx_path, weights, donate_weights=True)

    py_files = glob.glob(str(model_path / "*.py"))
    for file in py_files:
        shutil.copy(file, mlx_path)

    tokenizer.save_pretrained(mlx_path)

    save_config(config, config_path=mlx_path / "config.json")

    if upload_repo is not None:
        upload_to_hub(mlx_path, upload_repo, hf_path)

>>>> llms/mlx_lm/UPLOAD.md
### Packaging for PyPI

Install `build` and `twine`:

```
pip install --user --upgrade build
pip install --user --upgrade twine
```

Generate the source distribution and wheel:

```
python -m build
```

> [!warning]
> Use a test server first

#### Test Upload

Upload to test server:

```
python -m twine upload --repository testpypi dist/*
```

Install from test server and check that it works:

```
python -m pip install --index-url https://test.pypi.org/simple/ --no-deps mlx-lm
```

#### Upload

```
python -m twine upload dist/*
```

>>>> llms/mlx_lm/chat.py
# Copyright © 2023-2024 Apple Inc.

import argparse
import json

import mlx.core as mx

from .models.cache import make_prompt_cache
from .sample_utils import make_sampler
from .utils import load, stream_generate

DEFAULT_TEMP = 0.0
DEFAULT_TOP_P = 1.0
DEFAULT_SEED = None
DEFAULT_MAX_TOKENS = 256
DEFAULT_MODEL = "mlx-community/Llama-3.2-3B-Instruct-4bit"


def setup_arg_parser():
    """Set up and return the argument parser."""
    parser = argparse.ArgumentParser(description="Chat with an LLM")
    parser.add_argument(
        "--model",
        type=str,
        help="The path to the local model directory or Hugging Face repo.",
        default=DEFAULT_MODEL,
    )
    parser.add_argument(
        "--adapter-path",
        type=str,
        help="Optional path for the trained adapter weights and config.",
    )
    parser.add_argument(
        "--temp", type=float, default=DEFAULT_TEMP, help="Sampling temperature"
    )
    parser.add_argument(
        "--top-p", type=float, default=DEFAULT_TOP_P, help="Sampling top-p"
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=DEFAULT_SEED,
        help="PRNG seed",
    )
    parser.add_argument(
        "--max-kv-size",
        type=int,
        help="Set the maximum key-value cache size",
        default=None,
    )
    parser.add_argument(
        "--max-tokens",
        "-m",
        type=int,
        default=DEFAULT_MAX_TOKENS,
        help="Maximum number of tokens to generate",
    )
    return parser


def main():
    parser = setup_arg_parser()
    args = parser.parse_args()

    if args.seed is not None:
        mx.random.seed(args.seed)

    model, tokenizer = load(
        args.model,
        adapter_path=args.adapter_path,
        tokenizer_config={"trust_remote_code": True},
    )

    def print_help():
        print("The command list:")
        print("- 'q' to exit")
        print("- 'r' to reset the chat")
        print("- 'h' to display these commands")

    print(f"[INFO] Starting chat session with {args.model}.")
    print_help()
    prompt_cache = make_prompt_cache(model, args.max_kv_size)
    while True:
        query = input(">> ")
        if query == "q":
            break
        if query == "r":
            prompt_cache = make_prompt_cache(model, args.max_kv_size)
            continue
        if query == "h":
            print_help()
            continue
        messages = [{"role": "user", "content": query}]
        prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True)
        for response in stream_generate(
            model,
            tokenizer,
            prompt,
            max_tokens=args.max_tokens,
            sampler=make_sampler(args.temp, args.top_p),
            prompt_cache=prompt_cache,
        ):
            print(response.text, flush=True, end="")
        print()


if __name__ == "__main__":
    main()

>>>> llms/mlx_lm/README.md
## Generate Text with MLX and :hugs: Hugging Face

This an example of large language model text generation that can pull models from
the Hugging Face Hub.

For more information on this example, see the [README](../README.md) in the
parent directory.

This package also supports fine tuning with LoRA or QLoRA. For more information
see the [LoRA documentation](LORA.md).

>>>> llms/mlx_lm/models/phi.py
# Copyright © 2023-2024 Apple Inc.

import math
from dataclasses import dataclass
from typing import Tuple

import mlx.core as mx
import mlx.nn as nn

from .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention


@dataclass
class ModelArgs(BaseModelArgs):
    model_type: str = "phi"
    max_position_embeddings: int = 2048
    vocab_size: int = 51200
    hidden_size: int = 2560
    num_attention_heads: int = 32
    num_hidden_layers: int = 32
    num_key_value_heads: int = 32
    partial_rotary_factor: float = 0.4
    intermediate_size: int = 10240
    layer_norm_eps: float = 1e-5
    rope_theta: float = 10000.0

    def __post_init__(self):
        if self.num_key_value_heads is None:
            self.num_key_value_heads = self.num_attention_heads


class PhiAttention(nn.Module):
    def __init__(self, config: ModelArgs):
        super().__init__()

        self.hidden_size = config.hidden_size
        self.num_heads = config.num_attention_heads
        self.head_dim = self.hidden_size // self.num_heads
        self.num_key_value_heads = config.num_key_value_heads
        self.repeats = self.num_heads // self.num_key_value_heads
        self.rope_theta = config.rope_theta
        self.partial_rotary_factor = config.partial_rotary_factor

        if (self.head_dim * self.num_heads) != self.hidden_size:
            raise ValueError(
                f"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}"
                f" and `num_heads`: {self.num_heads})."
            )

        self.q_proj = nn.Linear(
            self.hidden_size, self.num_heads * self.head_dim, bias=True
        )
        self.k_proj = nn.Linear(
            self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True
        )
        self.v_proj = nn.Linear(
            self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True
        )
        self.dense = nn.Linear(
            self.num_heads * self.head_dim, self.hidden_size, bias=True
        )

        self.rope = nn.RoPE(
            int(self.partial_rotary_factor * self.head_dim),
            traditional=False,
            base=self.rope_theta,
        )

    def __call__(self, x, mask=None, cache=None):
        queries, keys, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)

        # Extract some shapes
        B, L, D = queries.shape
        n_heads, n_kv_heads = self.num_heads, self.num_key_value_heads

        # Prepare the queries, keys and values for the attention computation
        queries = queries.reshape(
            B,
            L,
            n_heads,
            -1,
        ).moveaxis(1, 2)
        keys = keys.reshape(B, L, n_kv_heads, -1).moveaxis(1, 2)
        values = values.reshape(B, L, n_kv_heads, -1).moveaxis(1, 2)

        # Add RoPE to the queries and keys and combine them with the cache
        if cache is not None:
            queries = self.rope(queries, offset=cache.offset)
            keys = self.rope(keys, offset=cache.offset)
            keys, values = cache.update_and_fetch(keys, values)
        else:
            queries = self.rope(queries)
            keys = self.rope(keys)

        scale = math.sqrt(1 / queries.shape[-1])
        output = scaled_dot_product_attention(
            queries.astype(mx.float32),
            keys,
            values,
            cache=cache,
            scale=scale,
            mask=mask,
        ).astype(values.dtype)

        output = output.moveaxis(2, 1).reshape(B, L, -1)

        return self.dense(output)


class PhiMLP(nn.Module):
    def __init__(self, config: ModelArgs):
        super().__init__()
        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)
        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)
        self.act = nn.GELU(approx="precise")

    def __call__(self, x) -> mx.array:
        return self.fc2(self.act(self.fc1(x)))


class PhiDecoderLayer(nn.Module):
    def __init__(self, config: ModelArgs):
        super().__init__()
        self.self_attn = PhiAttention(config=config)
        self.input_layernorm = nn.LayerNorm(
            config.hidden_size, eps=config.layer_norm_eps
        )
        self.mlp = PhiMLP(config)

    def __call__(self, x, mask, cache):
        h = self.input_layernorm(x)
        attn_h = self.self_attn(h, mask, cache)
        ff_h = self.mlp(h)
        return attn_h + ff_h + x


class PhiModel(nn.Module):
    def __init__(self, config: ModelArgs):
        super().__init__()
        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)
        self.layers = [PhiDecoderLayer(config) for i in range(config.num_hidden_layers)]
        self.final_layernorm = nn.LayerNorm(
            config.hidden_size, eps=config.layer_norm_eps
        )

    def __call__(self, x, mask, cache):
        x = self.embed_tokens(x)

        if mask is None:
            mask = create_attention_mask(x, cache)

        if cache is None:
            cache = [None] * len(self.layers)

        for layer, c in zip(self.layers, cache):
            x = layer(x, mask, c)
        return self.final_layernorm(x)


class Model(nn.Module):
    def __init__(self, config: ModelArgs):
        super().__init__()
        self.model_type = config.model_type
        self.model = PhiModel(config)
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=True)
        self.args = config

    def __call__(
        self,
        x: mx.array,
        mask: mx.array = None,
        cache=None,
    ) -> mx.array:
        y = self.model(x, mask, cache)
        return self.lm_head(y)

    @property
    def layers(self):
        return self.model.layers

>>>> llms/mlx_lm/models/qwen2_moe.py
# Copyright © 2023-2024 Apple Inc.

import math
from dataclasses import dataclass
from typing import Any, Dict, Optional, Union

import mlx.core as mx
import mlx.nn as nn

from .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention
from .switch_layers import SwitchGLU


@dataclass
class ModelArgs(BaseModelArgs):
    model_type: str
    hidden_size: int
    num_hidden_layers: int
    intermediate_size: int
    num_attention_heads: int
    num_experts_per_tok: int
    num_experts: int
    moe_intermediate_size: int
    shared_expert_intermediate_size: int
    rms_norm_eps: float
    vocab_size: int
    num_key_value_heads: Optional[int] = None
    rope_theta: float = 1000000
    rope_traditional: bool = False
    rope_scaling: Optional[Dict[str, Union[float, str]]] = None
    tie_word_embeddings: bool = False

    def __post_init__(self):
        if self.num_key_value_heads is None:
            self.num_key_value_heads = self.num_attention_heads

        if self.rope_scaling:
            required_keys = {"factor", "type"}
            if not all(key in self.rope_scaling for key in required_keys):
                raise ValueError(f"rope_scaling must contain keys {required_keys}")

            if self.rope_scaling["type"] != "linear":
                raise ValueError("rope_scaling 'type' currently only supports 'linear'")


class Attention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        dim = args.hidden_size
        self.n_heads = n_heads = args.num_attention_heads
        assert args.num_key_value_heads is not None
        self.n_kv_heads = n_kv_heads = args.num_key_value_heads

        head_dim = args.hidden_size // n_heads
        self.scale = head_dim**-0.5

        self.q_proj = nn.Linear(dim, n_heads * head_dim, bias=True)
        self.k_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=True)
        self.v_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=True)
        self.o_proj = nn.Linear(n_heads * head_dim, dim, bias=False)

        self.rope = nn.RoPE(
            head_dim,
            traditional=args.rope_traditional,
            base=args.rope_theta,
        )

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        B, L, D = x.shape

        queries, keys, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)

        # Prepare the queries, keys and values for the attention computation
        queries = queries.reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)
        keys = keys.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)
        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)

        if cache is not None:
            queries = self.rope(queries, offset=cache.offset)
            keys = self.rope(keys, offset=cache.offset)
            keys, values = cache.update_and_fetch(keys, values)
        else:
            queries = self.rope(queries)
            keys = self.rope(keys)

        output = scaled_dot_product_attention(
            queries, keys, values, cache=cache, scale=self.scale, mask=mask
        )
        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)
        return self.o_proj(output)


class MLP(nn.Module):
    def __init__(self, dim, hidden_dim):
        super().__init__()
        self.gate_proj = nn.Linear(dim, hidden_dim, bias=False)
        self.down_proj = nn.Linear(hidden_dim, dim, bias=False)
        self.up_proj = nn.Linear(dim, hidden_dim, bias=False)

    def __call__(self, x) -> mx.array:
        return self.down_proj(nn.silu(self.gate_proj(x)) * self.up_proj(x))


class Qwen2MoeSparseMoeBlock(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        dim = args.hidden_size
        intermediate_size = args.moe_intermediate_size
        shared_expert_intermediate_size = args.shared_expert_intermediate_size

        self.num_experts = num_experts = args.num_experts
        self.top_k = args.num_experts_per_tok

        self.gate = nn.Linear(dim, num_experts, bias=False)
        self.switch_mlp = SwitchGLU(dim, intermediate_size, num_experts)

        self.shared_expert = MLP(dim, shared_expert_intermediate_size)
        self.shared_expert_gate = nn.Linear(dim, 1, bias=False)

    def __call__(
        self,
        x: mx.array,
    ):
        gates = self.gate(x)
        gates = mx.softmax(gates, axis=-1, precise=True)

        k = self.top_k
        inds = mx.stop_gradient(mx.argpartition(-gates, kth=k - 1, axis=-1)[..., :k])
        scores = mx.take_along_axis(gates, inds, axis=-1)

        y = self.switch_mlp(x, inds)
        y = (y * scores[..., None]).sum(axis=-2)

        shared_expert_output = self.shared_expert(x)
        shared_expert_output = (
            mx.sigmoid(self.shared_expert_gate(x)) * shared_expert_output
        )

        return y + shared_expert_output


class Qwen2MoeDecoderLayer(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.hidden_size = args.hidden_size
        self.self_attn = Attention(args)
        self.mlp = Qwen2MoeSparseMoeBlock(args)

        self.input_layernorm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)
        self.post_attention_layernorm = nn.RMSNorm(
            args.hidden_size, eps=args.rms_norm_eps
        )
        self.args = args

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        r = self.self_attn(self.input_layernorm(x), mask, cache)
        h = x + r
        r = self.mlp(self.post_attention_layernorm(h))
        out = h + r
        return out


class Qwen2MoeModel(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.vocab_size = args.vocab_size
        self.num_hidden_layers = args.num_hidden_layers
        assert self.vocab_size > 0
        self.embed_tokens = nn.Embedding(args.vocab_size, args.hidden_size)
        self.layers = [
            Qwen2MoeDecoderLayer(args=args) for _ in range(args.num_hidden_layers)
        ]
        self.norm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        h = self.embed_tokens(inputs)

        if mask is None:
            mask = create_attention_mask(h, cache)

        if cache is None:
            cache = [None] * len(self.layers)

        for layer, c in zip(self.layers, cache):
            h = layer(h, mask, c)

        return self.norm(h)


class Model(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.model_type = args.model_type
        self.model = Qwen2MoeModel(args)
        self.lm_head = nn.Linear(args.hidden_size, args.vocab_size, bias=False)

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        out = self.model(inputs, mask, cache)
        return self.lm_head(out)

    def sanitize(self, weights):
        if "model.layers.0.mlp.experts.0.up_proj.weight" not in weights:
            return weights
        for l in range(self.args.num_hidden_layers):
            prefix = f"model.layers.{l}"
            for n in ["up_proj", "down_proj", "gate_proj"]:
                for k in ["weight", "scales", "biases"]:
                    if f"{prefix}.mlp.experts.0.{n}.{k}" in weights:
                        to_join = [
                            weights.pop(f"{prefix}.mlp.experts.{e}.{n}.{k}")
                            for e in range(self.args.num_experts)
                        ]
                        weights[f"{prefix}.mlp.switch_mlp.{n}.{k}"] = mx.stack(to_join)
        return weights

    @property
    def layers(self):
        return self.model.layers

>>>> llms/mlx_lm/models/switch_layers.py
# Copyright © 2023-2024 Apple Inc.

import math

import mlx.core as mx
import mlx.nn as nn


class QuantizedSwitchLinear(nn.Module):
    def __init__(
        self,
        input_dims: int,
        output_dims: int,
        num_experts: int,
        bias: bool = True,
        group_size: int = 64,
        bits: int = 4,
    ):
        super().__init__()

        scale = math.sqrt(1 / input_dims)
        self.weight, self.scales, self.biases = mx.quantize(
            mx.random.uniform(
                low=-scale,
                high=scale,
                shape=(num_experts, output_dims, input_dims),
            ),
            group_size=group_size,
            bits=bits,
        )

        if bias:
            self.bias = mx.zeros((num_experts, output_dims))

        self.group_size = group_size
        self.bits = bits

        # Freeze this model's parameters
        self.freeze()

    def unfreeze(self, *args, **kwargs):
        """Wrap unfreeze so that we unfreeze any layers we might contain but
        our parameters will remain frozen."""
        super().unfreeze(*args, **kwargs)
        self.freeze(recurse=False)

    @property
    def input_dims(self):
        return self.scales.shape[2] * self.group_size

    @property
    def output_dims(self):
        return self.weight.shape[1]

    @property
    def num_experts(self):
        return self.weight.shape[0]

    def __call__(self, x, indices):
        x = mx.gather_qmm(
            x,
            self["weight"],
            self["scales"],
            self["biases"],
            rhs_indices=indices,
            transpose=True,
            group_size=self.group_size,
            bits=self.bits,
        )
        if "bias" in self:
            x = x + mx.expand_dims(self["bias"][indices], -2)
        return x


class SwitchLinear(nn.Module):
    def __init__(
        self, input_dims: int, output_dims: int, num_experts: int, bias: bool = True
    ):
        super().__init__()
        scale = math.sqrt(1 / input_dims)
        self.weight = mx.random.uniform(
            low=-scale,
            high=scale,
            shape=(num_experts, output_dims, input_dims),
        )

        if bias:
            self.bias = mx.zeros((num_experts, output_dims))

    @property
    def input_dims(self):
        return self.weight.shape[2]

    @property
    def output_dims(self):
        return self.weight.shape[1]

    @property
    def num_experts(self):
        return self.weight.shape[0]

    def __call__(self, x, indices):
        x = mx.gather_mm(x, self["weight"].swapaxes(-1, -2), rhs_indices=indices)
        if "bias" in self:
            x = x + mx.expand_dims(self["bias"][indices], -2)
        return x

    def to_quantized(self, group_size: int = 64, bits: int = 4):
        num_experts, output_dims, input_dims = self.weight.shape
        ql = QuantizedSwitchLinear(
            input_dims, output_dims, num_experts, False, group_size, bits
        )
        ql.weight, ql.scales, ql.biases = mx.quantize(self.weight, group_size, bits)
        if "bias" in self:
            ql.bias = self.bias
        return ql


class SwitchGLU(nn.Module):
    def __init__(
        self,
        input_dims: int,
        hidden_dims: int,
        num_experts: int,
        activation=nn.silu,
        bias: bool = False,
    ):
        super().__init__()

        self.gate_proj = SwitchLinear(input_dims, hidden_dims, num_experts, bias=bias)
        self.up_proj = SwitchLinear(input_dims, hidden_dims, num_experts, bias=bias)
        self.down_proj = SwitchLinear(hidden_dims, input_dims, num_experts, bias=bias)
        self.activation = activation

    def __call__(self, x, indices) -> mx.array:
        x = mx.expand_dims(x, (-2, -3))

        x_up = self.up_proj(x, indices)
        x_gate = self.gate_proj(x, indices)
        x = self.down_proj(self.activation(x_gate) * x_up, indices)

        return x.squeeze(-2)


class SwitchMLP(nn.Module):
    def __init__(
        self,
        input_dims: int,
        hidden_dims: int,
        num_experts: int,
        activation=nn.gelu_approx,
        bias: bool = False,
    ):
        super().__init__()

        self.fc1 = SwitchLinear(input_dims, hidden_dims, num_experts, bias=bias)
        self.fc2 = SwitchLinear(hidden_dims, input_dims, num_experts, bias=bias)
        self.activation = activation

    def __call__(self, x, indices) -> mx.array:
        x = mx.expand_dims(x, (-2, -3))

        x = self.fc1(x, indices)
        x = self.activation(x)
        x = self.fc2(x, indices)

        return x.squeeze(-2)

>>>> llms/mlx_lm/models/mixtral.py
# Copyright © 2023-2024 Apple Inc.

import math
from dataclasses import dataclass
from typing import Any, Dict, Optional, Tuple, Union

import mlx.core as mx
import mlx.nn as nn

from .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention
from .switch_layers import SwitchGLU


@dataclass
class ModelArgs(BaseModelArgs):
    model_type: str
    vocab_size: int = 32000
    hidden_size: int = 4096
    intermediate_size: int = 14336
    num_hidden_layers: int = 32
    num_attention_heads: int = 32
    num_experts_per_tok: int = 2
    num_key_value_heads: int = 8
    num_local_experts: int = 8
    rms_norm_eps: float = 1e-5
    rope_theta: float = 1e6
    rope_traditional: bool = False
    rope_scaling: Optional[Dict[str, Union[float, str]]] = None

    def __post_init__(self):
        if self.num_key_value_heads is None:
            self.num_key_value_heads = self.num_attention_heads


class MixtralAttention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.hidden_size = args.hidden_size
        self.num_heads = args.num_attention_heads
        self.head_dim = self.hidden_size // self.num_heads
        self.num_key_value_heads = args.num_key_value_heads
        self.rope_theta = args.rope_theta

        self.scale = self.head_dim**-0.5

        self.q_proj = nn.Linear(
            self.hidden_size, self.num_heads * self.head_dim, bias=False
        )
        self.k_proj = nn.Linear(
            self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False
        )
        self.v_proj = nn.Linear(
            self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False
        )
        self.o_proj = nn.Linear(
            self.num_heads * self.head_dim, self.hidden_size, bias=False
        )

        self.rope = nn.RoPE(
            self.head_dim,
            traditional=args.rope_traditional,
            base=args.rope_theta,
        )

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        B, L, D = x.shape

        queries, keys, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)

        # Prepare the queries, keys and values for the attention computation
        queries = queries.reshape(B, L, self.num_heads, -1).transpose(0, 2, 1, 3)
        keys = keys.reshape(B, L, self.num_key_value_heads, -1).transpose(0, 2, 1, 3)
        values = values.reshape(B, L, self.num_key_value_heads, -1).transpose(
            0, 2, 1, 3
        )

        if cache is not None:
            queries = self.rope(queries, offset=cache.offset)
            keys = self.rope(keys, offset=cache.offset)
            keys, values = cache.update_and_fetch(keys, values)
        else:
            queries = self.rope(queries)
            keys = self.rope(keys)

        output = scaled_dot_product_attention(
            queries, keys, values, cache=cache, scale=self.scale, mask=mask
        )
        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)
        return self.o_proj(output)


class MixtralSparseMoeBlock(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.hidden_dim = args.hidden_size
        self.ffn_dim = args.intermediate_size
        self.num_experts = args.num_local_experts
        self.num_experts_per_tok = args.num_experts_per_tok

        # gating
        self.gate = nn.Linear(self.hidden_dim, self.num_experts, bias=False)

        self.switch_mlp = SwitchGLU(self.hidden_dim, self.ffn_dim, self.num_experts)

    def __call__(self, x: mx.array) -> mx.array:
        gates = self.gate(x)

        k = self.num_experts_per_tok
        inds = mx.stop_gradient(mx.argpartition(-gates, kth=k - 1, axis=-1)[..., :k])
        scores = mx.take_along_axis(gates, inds, axis=-1)
        scores = mx.softmax(scores, axis=-1, precise=True)

        y = self.switch_mlp(x, inds)
        y = (y * scores[..., None]).sum(axis=-2)

        return y


class MixtralDecoderLayer(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.hidden_size = args.hidden_size

        self.self_attn = MixtralAttention(args)

        self.block_sparse_moe = MixtralSparseMoeBlock(args)
        self.input_layernorm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)
        self.post_attention_layernorm = nn.RMSNorm(
            args.hidden_size, eps=args.rms_norm_eps
        )

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        r = self.self_attn(self.input_layernorm(x), mask, cache)
        h = x + r
        r = self.block_sparse_moe(self.post_attention_layernorm(h))
        out = h + r
        return out


class MixtralModel(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.vocab_size = args.vocab_size
        self.num_hidden_layers = args.num_hidden_layers

        self.embed_tokens = nn.Embedding(args.vocab_size, args.hidden_size)
        self.layers = [
            MixtralDecoderLayer(args=args) for _ in range(args.num_hidden_layers)
        ]
        self.norm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        h = self.embed_tokens(inputs)

        if mask is None:
            mask = create_attention_mask(h, cache)

        if cache is None:
            cache = [None] * len(self.layers)

        for layer, c in zip(self.layers, cache):
            h = layer(h, mask, c)

        return self.norm(h)


class Model(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.model_type = args.model_type
        self.model = MixtralModel(args)
        self.lm_head = nn.Linear(args.hidden_size, args.vocab_size, bias=False)
        self.args = args

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        out = self.model(inputs, mask, cache)
        return self.lm_head(out)

    def sanitize(self, weights):
        if "model.layers.0.block_sparse_moe.experts.0.w1.weight" not in weights:
            return weights
        for l in range(self.args.num_hidden_layers):
            prefix = f"model.layers.{l}"
            for n, m in [("w1", "gate_proj"), ("w2", "down_proj"), ("w3", "up_proj")]:
                for k in ["weight", "scales", "biases"]:
                    if f"{prefix}.block_sparse_moe.experts.0.{n}.{k}" in weights:
                        to_join = [
                            weights.pop(
                                f"{prefix}.block_sparse_moe.experts.{e}.{n}.{k}"
                            )
                            for e in range(self.args.num_local_experts)
                        ]
                        weights[f"{prefix}.block_sparse_moe.switch_mlp.{m}.{k}"] = (
                            mx.stack(to_join)
                        )
        return weights

    @property
    def layers(self):
        return self.model.layers

>>>> llms/mlx_lm/models/base.py
# Copyright © 2023-2024 Apple Inc.

import inspect
from dataclasses import dataclass
from typing import Any, Optional

import mlx.core as mx
from mlx.utils import tree_map

from .cache import QuantizedKVCache


@dataclass
class BaseModelArgs:
    @classmethod
    def from_dict(cls, params):
        return cls(
            **{
                k: v
                for k, v in params.items()
                if k in inspect.signature(cls).parameters
            }
        )


def create_causal_mask(
    N: int,
    offset: int = 0,
    window_size: Optional[int] = None,
    lengths: Optional[mx.array] = None,
):
    rinds = mx.arange(offset + N)
    linds = mx.arange(offset, offset + N) if offset else rinds
    linds = linds[:, None]
    rinds = rinds[None]
    mask = linds >= rinds
    if window_size is not None:
        mask = mask & (linds <= rinds + window_size)
    if lengths is not None:
        lengths = lengths[:, None, None, None]
        mask = mask & (rinds < lengths)
    return mask


def create_attention_mask(h: mx.array, cache: Optional[Any] = None):
    T = h.shape[1]
    if T > 1:
        window_size = None
        offset = 0
        if cache is not None and cache[0] is not None:
            c = cache[0]
            if hasattr(c, "max_size"):
                offset = min(c.max_size, c.offset)
                window_size = c.max_size
            else:
                offset = c.offset
        mask = create_causal_mask(T, offset, window_size=window_size)
    else:
        mask = None
    return mask


def quantized_scaled_dot_product_attention(
    queries: mx.array,
    q_keys: tuple[mx.array, mx.array, mx.array],
    q_values: tuple[mx.array, mx.array, mx.array],
    scale: float,
    mask: Optional[mx.array],
    group_size: int = 64,
    bits: int = 8,
) -> mx.array:
    B, n_q_heads, L, D = queries.shape
    n_kv_heads = q_keys[0].shape[-3]
    n_repeats = n_q_heads // n_kv_heads

    queries *= scale

    if n_repeats > 1:
        queries = mx.reshape(queries, (B, n_kv_heads, n_repeats, L, D))
        q_keys = tree_map(lambda x: mx.expand_dims(x, axis=-3), q_keys)
        q_values = tree_map(lambda x: mx.expand_dims(x, axis=-3), q_values)

    scores = mx.quantized_matmul(
        queries, *q_keys, transpose=True, group_size=group_size, bits=bits
    )
    if mask is not None:
        scores += mask
    scores = mx.softmax(scores, axis=-1, precise=True)
    out = mx.quantized_matmul(
        scores, *q_values, transpose=False, group_size=group_size, bits=bits
    )

    if n_repeats > 1:
        out = mx.reshape(out, (B, n_q_heads, L, D))

    return out


def scaled_dot_product_attention(
    queries,
    keys,
    values,
    cache,
    scale: float,
    mask: Optional[mx.array],
) -> mx.array:
    if isinstance(cache, QuantizedKVCache):
        return quantized_scaled_dot_product_attention(
            queries,
            keys,
            values,
            scale=scale,
            mask=mask,
            group_size=cache.group_size,
            bits=cache.bits,
        )
    else:
        return mx.fast.scaled_dot_product_attention(
            queries, keys, values, scale=scale, mask=mask
        )

>>>> llms/mlx_lm/models/olmo.py
# Copyright © 2023-2024 Apple Inc.

import sys
from dataclasses import dataclass
from typing import Any, Optional, Tuple

import mlx.core as mx
import mlx.nn as nn

from .base import BaseModelArgs, create_attention_mask

try:
    import hf_olmo
except ImportError:
    print("To run olmo install ai2-olmo: pip install ai2-olmo")
    sys.exit(1)


@dataclass
class ModelArgs(BaseModelArgs):
    model_type: str
    d_model: int
    n_layers: int
    mlp_hidden_size: int
    n_heads: int
    vocab_size: int
    embedding_size: int
    rope_theta: float = 10000
    rope_traditional: bool = False
    mlp_ratio: int = 4
    weight_tying: bool = False

    def __post_init__(self):
        self.mlp_hidden_size = (
            self.mlp_hidden_size
            if self.mlp_hidden_size is not None
            else self.mlp_ratio * self.d_model
        )


class TransformerBlock(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.n_heads = args.n_heads
        dim = args.d_model

        self.ff_proj = nn.Linear(dim, args.mlp_hidden_size, bias=False)
        self.ff_out = nn.Linear(args.mlp_hidden_size // 2, dim, bias=False)

        self.att_norm = nn.LayerNorm(dim, affine=False)
        self.ff_norm = nn.LayerNorm(dim, affine=False)

        head_dim = dim // self.n_heads
        self.scale = head_dim**-0.5

        self.att_proj = nn.Linear(dim, 3 * dim, bias=False)
        self.attn_out = nn.Linear(dim, dim, bias=False)

        self.rope = nn.RoPE(
            head_dim,
            traditional=args.rope_traditional,
            base=args.rope_theta,
        )

        self.args = args

    def attend(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        B, L, D = x.shape

        queries, keys, values = mx.split(self.att_proj(x), 3, axis=-1)

        # Prepare the queries, keys and values for the attention computation
        queries = queries.reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)
        keys = keys.reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)
        values = values.reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)

        if cache is not None:
            queries = self.rope(queries, offset=cache.offset)
            keys = self.rope(keys, offset=cache.offset)
            keys, values = cache.update_and_fetch(keys, values)
        else:
            queries = self.rope(queries)
            keys = self.rope(keys)

        scores = (queries * self.scale) @ keys.transpose(0, 1, 3, 2)
        if mask is not None:
            scores += mask
        scores = mx.softmax(scores.astype(mx.float32), axis=-1).astype(scores.dtype)
        output = (scores @ values).transpose(0, 2, 1, 3).reshape(B, L, -1)
        return self.attn_out(output)

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        r = self.attend(self.att_norm(x), mask, cache)
        h = x + r

        x1, x2 = mx.split(self.ff_proj(self.ff_norm(h)), 2, axis=-1)

        out = h + self.ff_out(nn.silu(x2) * x1)
        return out


class Transformer(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.n_layers = args.n_layers
        self.weight_tying = args.weight_tying

        self.wte = nn.Embedding(args.embedding_size, args.d_model)
        self.blocks = [TransformerBlock(args=args) for _ in range(args.n_layers)]
        if not self.weight_tying:
            self.ff_out = nn.Linear(args.d_model, args.embedding_size, bias=False)
        self.norm = nn.LayerNorm(args.d_model, affine=False)

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        h = self.wte(inputs)

        if mask is None:
            mask = create_attention_mask(h, cache)

        if cache is None:
            cache = [None] * len(self.blocks)

        for block, c in zip(self.blocks, cache):
            h = block(h, mask, c)

        h = self.norm(h)

        if self.weight_tying:
            return self.wte.as_linear(h), cache

        return self.ff_out(h)


class OlmoModel(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.transformer = Transformer(args)

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        return self.transformer(inputs, mask, cache)


class Model(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.model_type = args.model_type
        self.model = OlmoModel(args)
        self.args = args

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        return self.model(inputs, mask, cache)

    @property
    def layers(self):
        return self.model.transformer.blocks

>>>> llms/mlx_lm/models/deepseek_v2.py
# Copyright © 2023-2024 Apple Inc.

import math
from dataclasses import dataclass
from typing import Any, Dict, Optional, Tuple

import mlx.core as mx
import mlx.nn as nn

from .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention
from .switch_layers import SwitchGLU


@dataclass
class ModelArgs(BaseModelArgs):
    model_type: str = "deepseek_v2"
    vocab_size: int = 102400
    hidden_size: int = 4096
    intermediate_size: int = 11008
    moe_intermediate_size: int = 1407
    num_hidden_layers: int = 30
    num_attention_heads: int = 32
    num_key_value_heads: int = 32
    n_shared_experts: Optional[int] = None
    n_routed_experts: Optional[int] = None
    routed_scaling_factor: float = 1.0
    kv_lora_rank: int = 512
    q_lora_rank: int = 1536
    qk_rope_head_dim: int = 64
    v_head_dim: int = 128
    qk_nope_head_dim: int = 128
    topk_method: str = "gready"
    n_group: Optional[int] = None
    topk_group: Optional[int] = None
    num_experts_per_tok: Optional[int] = None
    moe_layer_freq: int = 1
    first_k_dense_replace: int = 0
    max_position_embeddings: int = 2048
    rms_norm_eps: float = 1e-6
    rope_theta: float = 10000.0
    rope_scaling: Dict = None
    attention_bias: bool = False


def yarn_find_correction_dim(
    num_rotations, dim, base=10000, max_position_embeddings=2048
):
    return (dim * math.log(max_position_embeddings / (num_rotations * 2 * math.pi))) / (
        2 * math.log(base)
    )


def yarn_find_correction_range(
    low_rot, high_rot, dim, base=10000, max_position_embeddings=2048
):
    low = math.floor(
        yarn_find_correction_dim(low_rot, dim, base, max_position_embeddings)
    )
    high = math.ceil(
        yarn_find_correction_dim(high_rot, dim, base, max_position_embeddings)
    )
    return max(low, 0), min(high, dim - 1)


def yarn_get_mscale(scale=1, mscale=1):
    if scale <= 1:
        return 1.0
    return 0.1 * mscale * math.log(scale) + 1.0


def yarn_linear_ramp_mask(min_val, max_val, dim):
    if min_val == max_val:
        max_val += 0.001  # Prevent singularity

    linear_func = (mx.arange(dim, dtype=mx.float32) - min_val) / (max_val - min_val)
    return mx.clip(linear_func, 0, 1)


class DeepseekV2YarnRotaryEmbedding(nn.Module):
    def __init__(
        self,
        dim,
        max_position_embeddings=2048,
        base=10000,
        scaling_factor=1.0,
        original_max_position_embeddings=4096,
        beta_fast=32,
        beta_slow=1,
        mscale=1,
        mscale_all_dim=0,
    ):
        super().__init__()
        self.mscale = yarn_get_mscale(scaling_factor, mscale) / yarn_get_mscale(
            scaling_factor, mscale_all_dim
        )
        freq_extra = base ** (mx.arange(0, dim, 2, dtype=mx.float32) / dim)
        freq_inter = scaling_factor * base ** (
            mx.arange(0, dim, 2, dtype=mx.float32) / dim
        )
        low, high = yarn_find_correction_range(
            beta_fast,
            beta_slow,
            dim,
            base,
            original_max_position_embeddings,
        )
        freq_mask = 1.0 - yarn_linear_ramp_mask(low, high, dim // 2)
        self._freqs = (freq_inter * freq_extra) / (
            freq_inter * freq_mask + freq_extra * (1 - freq_mask)
        )

    def __call__(self, x, offset=0):
        if self.mscale != 1.0:
            x = self.mscale * x
        return mx.fast.rope(
            x,
            x.shape[-1],
            traditional=True,
            base=None,
            scale=1.0,
            offset=offset,
            freqs=self._freqs,
        )


class DeepseekV2Attention(nn.Module):
    def __init__(self, config: ModelArgs):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.num_heads = config.num_attention_heads
        self.max_position_embeddings = config.max_position_embeddings
        self.rope_theta = config.rope_theta
        self.q_lora_rank = config.q_lora_rank
        self.qk_rope_head_dim = config.qk_rope_head_dim
        self.kv_lora_rank = config.kv_lora_rank
        self.v_head_dim = config.v_head_dim
        self.qk_nope_head_dim = config.qk_nope_head_dim
        self.q_head_dim = config.qk_nope_head_dim + config.qk_rope_head_dim

        self.scale = self.q_head_dim**-0.5

        if self.q_lora_rank is None:
            self.q_proj = nn.Linear(
                self.hidden_size, self.num_heads * self.q_head_dim, bias=False
            )
        else:
            self.q_a_proj = nn.Linear(
                self.hidden_size, self.q_lora_rank, bias=config.attention_bias
            )
            self.q_a_layernorm = nn.RMSNorm(self.q_lora_rank)
            self.q_b_proj = nn.Linear(
                self.q_lora_rank, self.num_heads * self.q_head_dim, bias=False
            )

        self.kv_a_proj_with_mqa = nn.Linear(
            self.hidden_size,
            self.kv_lora_rank + self.qk_rope_head_dim,
            bias=config.attention_bias,
        )
        self.kv_a_layernorm = nn.RMSNorm(self.kv_lora_rank)
        self.kv_b_proj = nn.Linear(
            self.kv_lora_rank,
            self.num_heads
            * (self.q_head_dim - self.qk_rope_head_dim + self.v_head_dim),
            bias=False,
        )

        self.o_proj = nn.Linear(
            self.num_heads * self.v_head_dim,
            self.hidden_size,
            bias=config.attention_bias,
        )

        mscale_all_dim = self.config.rope_scaling.get("mscale_all_dim", 0)
        scaling_factor = self.config.rope_scaling["factor"]
        if mscale_all_dim:
            mscale = yarn_get_mscale(scaling_factor, mscale_all_dim)
            self.scale = self.scale * mscale * mscale

        rope_kwargs = {
            key: self.config.rope_scaling[key]
            for key in [
                "original_max_position_embeddings",
                "beta_fast",
                "beta_slow",
                "mscale",
                "mscale_all_dim",
            ]
            if key in self.config.rope_scaling
        }
        self.rope = DeepseekV2YarnRotaryEmbedding(
            dim=self.qk_rope_head_dim,
            max_position_embeddings=self.max_position_embeddings,
            scaling_factor=scaling_factor,
            base=self.rope_theta,
            **rope_kwargs,
        )

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        B, L, D = x.shape

        if self.q_lora_rank is None:
            q = self.q_proj(x)
        else:
            q = self.q_b_proj(self.q_a_layernorm(self.q_a_proj(x)))

        q = q.reshape(B, L, self.num_heads, self.q_head_dim).transpose(0, 2, 1, 3)
        q_nope, q_pe = mx.split(q, [self.qk_nope_head_dim], axis=-1)
        compressed_kv = self.kv_a_proj_with_mqa(x)
        compressed_kv, k_pe = mx.split(compressed_kv, [self.kv_lora_rank], axis=-1)
        k_pe = k_pe.reshape(B, L, 1, self.qk_rope_head_dim).transpose(0, 2, 1, 3)
        kv = self.kv_b_proj(self.kv_a_layernorm(compressed_kv))
        kv = kv.reshape(B, L, self.num_heads, -1).transpose(0, 2, 1, 3)

        k_nope, values = mx.split(kv, [self.qk_nope_head_dim], axis=-1)

        if cache is not None:
            q_pe = self.rope(q_pe, cache.offset)
            k_pe = self.rope(k_pe, cache.offset)
            k_pe = mx.repeat(k_pe, self.num_heads, axis=1)
            keys, values = cache.update_and_fetch(
                mx.concatenate([k_nope, k_pe], axis=-1), values
            )
        else:
            q_pe = self.rope(q_pe)
            k_pe = self.rope(k_pe)
            k_pe = mx.repeat(k_pe, self.num_heads, axis=1)
            keys = mx.concatenate([k_nope, k_pe], axis=-1)

        queries = mx.concatenate([q_nope, q_pe], axis=-1)

        output = scaled_dot_product_attention(
            queries, keys, values, cache=cache, scale=self.scale, mask=mask
        )
        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)
        return self.o_proj(output)


class DeepseekV2MLP(nn.Module):
    def __init__(
        self, config: ModelArgs, hidden_size: int = None, intermediate_size: int = None
    ):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size if hidden_size is None else hidden_size
        self.intermediate_size = (
            config.intermediate_size if intermediate_size is None else intermediate_size
        )

        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)

    def __call__(self, x):
        down_proj = self.down_proj(nn.silu(self.gate_proj(x)) * self.up_proj(x))
        return down_proj


class MoEGate(nn.Module):
    def __init__(self, config: ModelArgs):
        super().__init__()
        self.config = config
        self.top_k = config.num_experts_per_tok
        self.n_routed_experts = config.n_routed_experts
        self.routed_scaling_factor = config.routed_scaling_factor
        self.topk_method = config.topk_method
        self.n_group = config.n_group
        self.topk_group = config.topk_group
        self.weight = mx.zeros((self.n_routed_experts, config.hidden_size))

    def __call__(self, x):
        gates = x @ self.weight.T

        scores = mx.softmax(gates, axis=-1, precise=True)

        if self.topk_method == "group_limited_greedy":
            bsz, seq_len = x.shape[:2]
            scores = scores.reshape(bsz, seq_len, self.n_group, -1)
            group_scores = scores.max(axis=-1, keepdims=True)
            k = self.n_group - self.topk_group
            group_idx = mx.argpartition(group_scores, kth=k - 1, axis=-2)[..., :k, :]
            scores = mx.put_along_axis(
                scores, group_idx, mx.array(0.0, scores.dtype), axis=-2
            )
            scores = scores.reshape(bsz, seq_len, -1)

        k = self.top_k
        inds = mx.argpartition(-scores, kth=k - 1, axis=-1)[..., :k]
        scores = mx.take_along_axis(scores, inds, axis=-1)
        scores = scores * self.routed_scaling_factor

        return inds, scores


class DeepseekV2MoE(nn.Module):
    def __init__(self, config: ModelArgs):
        super().__init__()
        self.config = config
        self.num_experts_per_tok = config.num_experts_per_tok
        self.switch_mlp = SwitchGLU(
            config.hidden_size, config.moe_intermediate_size, config.n_routed_experts
        )

        self.gate = MoEGate(config)
        if config.n_shared_experts is not None:
            intermediate_size = config.moe_intermediate_size * config.n_shared_experts
            self.shared_experts = DeepseekV2MLP(
                config=config, intermediate_size=intermediate_size
            )

    def __call__(self, x):
        inds, scores = self.gate(x)
        y = self.switch_mlp(x, inds)
        y = (y * scores[..., None]).sum(axis=-2)
        if self.config.n_shared_experts is not None:
            y = y + self.shared_experts(x)

        return y


class DeepseekV2DecoderLayer(nn.Module):
    def __init__(self, config: ModelArgs, layer_idx: int):
        super().__init__()
        self.self_attn = DeepseekV2Attention(config)
        self.mlp = (
            DeepseekV2MoE(config)
            if (
                config.n_routed_experts is not None
                and layer_idx >= config.first_k_dense_replace
                and layer_idx % config.moe_layer_freq == 0
            )
            else DeepseekV2MLP(config)
        )
        self.input_layernorm = nn.RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = nn.RMSNorm(
            config.hidden_size, eps=config.rms_norm_eps
        )

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        r = self.self_attn(self.input_layernorm(x), mask, cache)
        h = x + r
        r = self.mlp(self.post_attention_layernorm(h))
        out = h + r
        return out


class DeepseekV2Model(nn.Module):
    def __init__(self, config: ModelArgs):
        super().__init__()
        self.vocab_size = config.vocab_size
        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)
        self.layers = [
            DeepseekV2DecoderLayer(config, idx)
            for idx in range(config.num_hidden_layers)
        ]
        self.start_idx = 0
        self.end_idx = len(self.layers)
        self.num_layers = self.end_idx

        self.norm = nn.RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

        self.pipeline_rank = 0
        self.pipeline_size = 1

    def pipeline(self, group):
        # Split layers in reverse so rank=0 gets the last layers and
        # rank=pipeline_size-1 gets the first
        self.pipeline_rank = group.rank()
        self.pipeline_size = group.size()
        layers_per_rank = len(self.layers) // self.pipeline_size
        extra = len(self.layers) - layers_per_rank * self.pipeline_size
        if self.pipeline_rank < extra:
            layers_per_rank += 1

        self.start_idx = (self.pipeline_size - self.pipeline_rank - 1) * layers_per_rank
        self.end_idx = self.start_idx + layers_per_rank
        self.num_layers = layers_per_rank
        self.layers = self.layers[: self.end_idx]
        self.layers[: self.start_idx] = [None] * self.start_idx
        self.num_layers = len(self.layers) - self.start_idx

    def __call__(
        self,
        x: mx.array,
        cache: Optional[Any] = None,
        mask: Optional[mx.array] = None,
    ) -> mx.array:
        h = self.embed_tokens(x)

        pipeline_rank = self.pipeline_rank
        pipeline_size = self.pipeline_size
        # Hack to avoid time-outs during prompt-processing
        dist_stream = mx.cpu if h.shape[1] > 1 else mx.gpu
        if mask is None:
            mask = create_attention_mask(h, cache)

        if cache is None:
            cache = [None] * self.num_layers

        # Receive from the previous process in the pipeline
        if pipeline_rank < pipeline_size - 1:
            h = mx.distributed.recv_like(h, (pipeline_rank + 1), stream=dist_stream)

        for i in range(self.num_layers):
            h = self.layers[self.start_idx + i](h, mask, cache[i])

        # Send to the next process in the pipeline
        if pipeline_rank != 0:
            h = mx.distributed.send(
                h, (pipeline_rank - 1) % pipeline_size, stream=dist_stream
            )

        # Broadcast h while keeping it in the graph
        h = mx.distributed.all_gather(h, stream=dist_stream)[: h.shape[0]]

        return self.norm(h)


class Model(nn.Module):
    def __init__(self, config: ModelArgs):
        super().__init__()
        self.args = config
        self.model_type = config.model_type
        self.model = DeepseekV2Model(config)
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)

    def __call__(
        self,
        inputs: mx.array,
        cache: Optional[Any] = None,
        mask: Optional[mx.array] = None,
    ):
        out = self.model(inputs, cache, mask)
        return self.lm_head(out)

    def sanitize(self, weights):
        for l in range(self.args.num_hidden_layers):
            prefix = f"model.layers.{l}"
            for n, m in [("w1", "gate_proj"), ("w2", "down_proj"), ("w3", "up_proj")]:
                for k in ["weight", "scales", "biases"]:
                    if f"{prefix}.mlp.experts.0.{m}.{k}" in weights:
                        to_join = [
                            weights.pop(f"{prefix}.mlp.experts.{e}.{m}.{k}")
                            for e in range(self.args.n_routed_experts)
                        ]
                        weights[f"{prefix}.mlp.switch_mlp.{m}.{k}"] = mx.stack(to_join)
        return weights

    @property
    def layers(self):
        return self.model.layers[self.model.start_idx : self.model.end_idx]

>>>> llms/mlx_lm/models/qwen2.py
# Copyright © 2023-2024 Apple Inc.

from dataclasses import dataclass
from typing import Any, Dict, Optional, Union

import mlx.core as mx
import mlx.nn as nn

from .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention


@dataclass
class ModelArgs(BaseModelArgs):
    model_type: str
    hidden_size: int
    num_hidden_layers: int
    intermediate_size: int
    num_attention_heads: int
    rms_norm_eps: float
    vocab_size: int
    num_key_value_heads: Optional[int] = None
    rope_theta: float = 1000000
    rope_traditional: bool = False
    rope_scaling: Optional[Dict[str, Union[float, str]]] = None
    tie_word_embeddings: bool = True

    def __post_init__(self):
        if self.num_key_value_heads is None:
            self.num_key_value_heads = self.num_attention_heads

        if self.rope_scaling:
            required_keys = {"factor", "type"}
            if not all(key in self.rope_scaling for key in required_keys):
                raise ValueError(f"rope_scaling must contain keys {required_keys}")

            if self.rope_scaling["type"] != "linear":
                raise ValueError("rope_scaling 'type' currently only supports 'linear'")


class Attention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        dim = args.hidden_size
        self.n_heads = n_heads = args.num_attention_heads
        assert args.num_key_value_heads is not None
        self.n_kv_heads = n_kv_heads = args.num_key_value_heads

        head_dim = args.hidden_size // n_heads
        self.scale = head_dim**-0.5

        self.q_proj = nn.Linear(dim, n_heads * head_dim, bias=True)
        self.k_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=True)
        self.v_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=True)
        self.o_proj = nn.Linear(n_heads * head_dim, dim, bias=False)

        rope_scale = (
            1 / args.rope_scaling["factor"]
            if args.rope_scaling is not None and args.rope_scaling["type"] == "linear"
            else 1
        )
        self.rope = nn.RoPE(
            head_dim,
            traditional=args.rope_traditional,
            base=args.rope_theta,
            scale=rope_scale,
        )

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        B, L, D = x.shape

        queries, keys, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)

        # Prepare the queries, keys and values for the attention computation
        queries = queries.reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)
        keys = keys.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)
        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)

        if cache is not None:
            queries = self.rope(queries, offset=cache.offset)
            keys = self.rope(keys, offset=cache.offset)
            keys, values = cache.update_and_fetch(keys, values)
        else:
            queries = self.rope(queries)
            keys = self.rope(keys)

        output = scaled_dot_product_attention(
            queries, keys, values, cache=cache, scale=self.scale, mask=mask
        )
        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)
        return self.o_proj(output)


class MLP(nn.Module):
    def __init__(self, dim, hidden_dim):
        super().__init__()
        self.gate_proj = nn.Linear(dim, hidden_dim, bias=False)
        self.down_proj = nn.Linear(hidden_dim, dim, bias=False)
        self.up_proj = nn.Linear(dim, hidden_dim, bias=False)

    def __call__(self, x) -> mx.array:
        return self.down_proj(nn.silu(self.gate_proj(x)) * self.up_proj(x))


class TransformerBlock(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.num_attention_heads = args.num_attention_heads
        self.hidden_size = args.hidden_size
        self.self_attn = Attention(args)
        self.mlp = MLP(args.hidden_size, args.intermediate_size)
        self.input_layernorm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)
        self.post_attention_layernorm = nn.RMSNorm(
            args.hidden_size, eps=args.rms_norm_eps
        )
        self.args = args

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        r = self.self_attn(self.input_layernorm(x), mask, cache)
        h = x + r
        r = self.mlp(self.post_attention_layernorm(h))
        out = h + r
        return out


class Qwen2Model(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.vocab_size = args.vocab_size
        self.num_hidden_layers = args.num_hidden_layers
        assert self.vocab_size > 0
        self.embed_tokens = nn.Embedding(args.vocab_size, args.hidden_size)
        self.layers = [
            TransformerBlock(args=args) for _ in range(args.num_hidden_layers)
        ]
        self.norm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        h = self.embed_tokens(inputs)

        if mask is None:
            mask = create_attention_mask(h, cache)

        if cache is None:
            cache = [None] * len(self.layers)

        for layer, c in zip(self.layers, cache):
            h = layer(h, mask, c)

        return self.norm(h)


class Model(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.model_type = args.model_type
        self.model = Qwen2Model(args)
        if not args.tie_word_embeddings:
            self.lm_head = nn.Linear(args.hidden_size, args.vocab_size, bias=False)

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        out = self.model(inputs, mask, cache)
        if self.args.tie_word_embeddings:
            out = self.model.embed_tokens.as_linear(out)
        else:
            out = self.lm_head(out)
        return out

    def sanitize(self, weights):
        if self.args.tie_word_embeddings:
            weights.pop("lm_head.weight", None)
        # Remove unused precomputed rotary freqs
        return {
            k: v for k, v in weights.items() if "self_attn.rotary_emb.inv_freq" not in k
        }

    @property
    def layers(self):
        return self.model.layers

>>>> llms/mlx_lm/models/cohere.py
# Copyright © 2023-2024 Apple Inc.

from dataclasses import dataclass
from typing import Any, Optional, Tuple

import mlx.core as mx
import mlx.nn as nn

from .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention


@dataclass
class ModelArgs(BaseModelArgs):
    model_type: str
    hidden_size: int = 8192
    num_hidden_layers: int = 40
    intermediate_size: int = 22528
    num_attention_heads: int = 64
    num_key_value_heads: int = 64
    rope_theta: float = 8000000.0
    vocab_size: int = 256000
    layer_norm_eps: float = 1e-05
    logit_scale: float = 0.0625
    attention_bias: bool = False
    layer_norm_bias: bool = False
    use_qk_norm: bool = False


class LayerNorm2D(nn.Module):

    def __init__(self, d1, d2, eps):
        super().__init__()
        self.weight = mx.zeros((d1, d2))
        self.eps = eps

    def __call__(self, x):
        return self.weight * mx.fast.layer_norm(x, None, None, self.eps)


class Attention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args

        dim = args.hidden_size
        self.n_heads = n_heads = args.num_attention_heads
        self.n_kv_heads = n_kv_heads = args.num_key_value_heads

        head_dim = args.hidden_size // args.num_attention_heads
        self.scale = head_dim**-0.5

        attetion_bias = args.attention_bias

        self.q_proj = nn.Linear(dim, n_heads * head_dim, bias=attetion_bias)
        self.k_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=attetion_bias)
        self.v_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=attetion_bias)
        self.o_proj = nn.Linear(n_heads * head_dim, dim, bias=attetion_bias)

        self.use_qk_norm = args.use_qk_norm
        if self.use_qk_norm:
            self.q_norm = LayerNorm2D(self.n_heads, head_dim, eps=args.layer_norm_eps)
            self.k_norm = LayerNorm2D(
                self.n_kv_heads, head_dim, eps=args.layer_norm_eps
            )

        self.rope = nn.RoPE(head_dim, traditional=True, base=args.rope_theta)

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        B, L, D = x.shape

        queries, keys, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)

        queries = queries.reshape(B, L, self.n_heads, -1)
        keys = keys.reshape(B, L, self.n_kv_heads, -1)
        if self.use_qk_norm:
            queries = self.q_norm(queries)
            keys = self.k_norm(keys)

        queries = queries.transpose(0, 2, 1, 3)
        keys = keys.transpose(0, 2, 1, 3)
        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)

        if cache is not None:
            queries = self.rope(queries, offset=cache.offset)
            keys = self.rope(keys, offset=cache.offset)
            keys, values = cache.update_and_fetch(keys, values)
        else:
            queries = self.rope(queries)
            keys = self.rope(keys)

        output = scaled_dot_product_attention(
            queries, keys, values, cache=cache, scale=self.scale, mask=mask
        )

        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)
        return self.o_proj(output)


class MLP(nn.Module):
    def __init__(self, dim, hidden_dim):
        super().__init__()
        self.gate_proj = nn.Linear(dim, hidden_dim, bias=False)
        self.up_proj = nn.Linear(dim, hidden_dim, bias=False)
        self.down_proj = nn.Linear(hidden_dim, dim, bias=False)

    def __call__(self, x):
        return self.down_proj(nn.silu(self.gate_proj(x)) * self.up_proj(x))


class TransformerBlock(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.hidden_size = args.hidden_size
        self.n_heads = args.num_attention_heads

        self.self_attn = Attention(args)
        self.mlp = MLP(args.hidden_size, args.intermediate_size)
        self.input_layernorm = nn.LayerNorm(
            args.hidden_size, eps=args.layer_norm_eps, bias=args.layer_norm_bias
        )
        self.args = args

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        h = self.input_layernorm(x)
        attn_h = self.self_attn(h, mask, cache)
        ff_h = self.mlp(h)
        return attn_h + ff_h + x


class CohereModel(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.vocab_size = args.vocab_size
        self.num_hidden_layers = args.num_hidden_layers
        assert self.vocab_size > 0
        self.embed_tokens = nn.Embedding(args.vocab_size, args.hidden_size)
        self.layers = [
            TransformerBlock(args=args) for _ in range(args.num_hidden_layers)
        ]
        self.norm = nn.LayerNorm(
            args.hidden_size, eps=args.layer_norm_eps, bias=args.layer_norm_bias
        )

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        h = self.embed_tokens(inputs)

        if mask is None:
            mask = create_attention_mask(h, cache)

        if cache is None:
            cache = [None] * len(self.layers)

        for layer, c in zip(self.layers, cache):
            h = layer(h, mask, c)

        return self.norm(h)


class Model(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.model_type = args.model_type
        self.model = CohereModel(args)
        self.args = args

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        out = self.model(inputs, mask, cache)
        out = self.model.embed_tokens.as_linear(out)
        out = out * self.model.args.logit_scale
        return out

    @property
    def layers(self):
        return self.model.layers

>>>> llms/mlx_lm/models/granite.py
# Copyright © 2023-2024 Apple Inc.

from dataclasses import dataclass
from typing import Any, Dict, Optional, Union

import mlx.core as mx
import mlx.nn as nn

from .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention
from .rope_utils import initialize_rope


@dataclass
class ModelArgs(BaseModelArgs):
    model_type: str
    hidden_size: int
    num_hidden_layers: int
    intermediate_size: int
    num_attention_heads: int
    rms_norm_eps: float
    vocab_size: int
    logits_scaling: float
    attention_multiplier: float
    embedding_multiplier: float
    residual_multiplier: float
    max_position_embeddings: int
    num_key_value_heads: int
    attention_bias: bool
    mlp_bias: bool
    rope_theta: float
    rope_scaling: Optional[Dict[str, Union[float, str]]] = None
    tie_word_embeddings: bool = True


class Attention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        dim = args.hidden_size
        self.n_heads = n_heads = args.num_attention_heads
        self.n_kv_heads = n_kv_heads = args.num_key_value_heads

        self.head_dim = head_dim = args.hidden_size // n_heads

        self.scale = args.attention_multiplier
        attention_bias = args.attention_bias
        self.q_proj = nn.Linear(dim, n_heads * head_dim, bias=attention_bias)
        self.k_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=attention_bias)
        self.v_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=attention_bias)
        self.o_proj = nn.Linear(n_heads * head_dim, dim, bias=attention_bias)

        self.rope = initialize_rope(
            self.head_dim,
            args.rope_theta,
            False,
            args.rope_scaling,
            args.max_position_embeddings,
        )

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        B, L, D = x.shape

        queries, keys, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)

        # Prepare the queries, keys and values for the attention computation
        queries = queries.reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)
        keys = keys.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)
        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)

        if cache is not None:
            queries = self.rope(queries, offset=cache.offset)
            keys = self.rope(keys, offset=cache.offset)
            keys, values = cache.update_and_fetch(keys, values)
        else:
            queries = self.rope(queries)
            keys = self.rope(keys)

        output = scaled_dot_product_attention(
            queries, keys, values, cache=cache, scale=self.scale, mask=mask
        )

        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)
        return self.o_proj(output)


class MLP(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        dim = args.hidden_size
        hidden_dim = args.intermediate_size
        if hasattr(args, "mlp_bias"):
            mlp_bias = args.mlp_bias
        else:
            mlp_bias = False

        self.gate_proj = nn.Linear(dim, hidden_dim, bias=mlp_bias)
        self.down_proj = nn.Linear(hidden_dim, dim, bias=mlp_bias)
        self.up_proj = nn.Linear(dim, hidden_dim, bias=mlp_bias)

    def __call__(self, x) -> mx.array:
        return self.down_proj(nn.silu(self.gate_proj(x)) * self.up_proj(x))


class TransformerBlock(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.num_attention_heads = args.num_attention_heads
        self.hidden_size = args.hidden_size
        self.self_attn = Attention(args)
        self.mlp = MLP(args)
        self.input_layernorm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)
        self.post_attention_layernorm = nn.RMSNorm(
            args.hidden_size, eps=args.rms_norm_eps
        )
        self.residual_multiplier = args.residual_multiplier

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        r = self.self_attn(self.input_layernorm(x), mask, cache)
        h = x + r * self.residual_multiplier
        r = self.mlp(self.post_attention_layernorm(h))
        out = h + r * self.residual_multiplier
        return out


class GraniteModel(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.vocab_size = args.vocab_size
        self.num_hidden_layers = args.num_hidden_layers
        assert self.vocab_size > 0
        self.embed_tokens = nn.Embedding(args.vocab_size, args.hidden_size)
        self.layers = [
            TransformerBlock(args=args) for _ in range(args.num_hidden_layers)
        ]
        self.norm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)
        self.embedding_multiplier = args.embedding_multiplier

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        h = self.embed_tokens(inputs) * self.embedding_multiplier

        if mask is None:
            mask = create_attention_mask(h, cache)

        if cache is None:
            cache = [None] * len(self.layers)

        for layer, c in zip(self.layers, cache):
            h = layer(h, mask, cache=c)

        return self.norm(h)


class Model(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.model_type = args.model_type
        self.model = GraniteModel(args)
        if not args.tie_word_embeddings:
            self.lm_head = nn.Linear(args.hidden_size, args.vocab_size, bias=False)
        self.logits_scaling = args.logits_scaling

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        out = self.model(inputs, mask, cache)
        if self.args.tie_word_embeddings:
            out = self.model.embed_tokens.as_linear(out)
        else:
            out = self.lm_head(out)
        return out / self.logits_scaling

    @property
    def layers(self):
        return self.model.layers

>>>> llms/mlx_lm/models/gpt_neox.py
# Copyright © 2023-2024 Apple Inc.

from dataclasses import dataclass
from typing import Any, Dict, Optional, Tuple, Union

import mlx.core as mx
import mlx.nn as nn
import numpy as np

from .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention

# Based on the transformers implementation at:
# https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt_neox/modeling_gpt_neox.py


@dataclass
class ModelArgs(BaseModelArgs):
    model_type: str
    max_position_embeddings: int
    hidden_size: int
    num_attention_heads: int
    num_hidden_layers: int
    layer_norm_eps: float
    vocab_size: int
    rotary_emb_base: int
    rotary_pct: float
    num_key_value_heads: int = None

    def __post_init__(self):
        if self.num_key_value_heads is None:
            self.num_key_value_heads = self.num_attention_heads


class Attention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        assert (
            args.hidden_size % args.num_attention_heads == 0
        ), "hidden_size must be divisible by num_attention_heads"

        self.hidden_size = args.hidden_size
        self.num_attention_heads = args.num_attention_heads
        self.head_dim = self.hidden_size // self.num_attention_heads

        self.rope = nn.RoPE(
            dims=int(self.head_dim * args.rotary_pct),
            traditional=False,
            base=args.rotary_emb_base,
        )

        self.scale = self.head_dim**-0.5

        self.query_key_value = nn.Linear(
            self.hidden_size, 3 * self.hidden_size, bias=True
        )
        self.dense = nn.Linear(self.hidden_size, self.hidden_size, bias=True)

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        B, L, D = x.shape

        qkv = self.query_key_value(x)

        new_qkv_shape = qkv.shape[:-1] + (self.num_attention_heads, 3 * self.head_dim)
        qkv = qkv.reshape(*new_qkv_shape)

        queries, keys, values = [x.transpose(0, 2, 1, 3) for x in qkv.split(3, -1)]

        if cache is not None:
            queries = self.rope(queries, offset=cache.offset)
            keys = self.rope(keys, offset=cache.offset)
            keys, values = cache.update_and_fetch(keys, values)
        else:
            queries = self.rope(queries)
            keys = self.rope(keys)

        output = scaled_dot_product_attention(
            queries, keys, values, cache=cache, scale=self.scale, mask=mask
        )

        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)
        return self.dense(output)


class MLP(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        self.hidden_size = args.hidden_size
        self.dense_h_to_4h = nn.Linear(self.hidden_size, 4 * self.hidden_size)
        self.dense_4h_to_h = nn.Linear(4 * self.hidden_size, self.hidden_size)

    def __call__(self, x) -> mx.array:
        # gelu_approx corresponds to FastGELUActivation in transformers.
        return self.dense_4h_to_h(nn.gelu_approx(self.dense_h_to_4h(x)))


class TransformerBlock(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        self.hidden_size = args.hidden_size
        self.layer_norm_eps = args.layer_norm_eps
        self.attention = Attention(args)
        self.mlp = MLP(args)
        self.input_layernorm = nn.LayerNorm(
            self.hidden_size,
            eps=self.layer_norm_eps,
        )
        self.post_attention_layernorm = nn.LayerNorm(
            self.hidden_size, eps=self.layer_norm_eps
        )

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        residual = x
        # NeoX runs attention and feedforward network in parallel.
        attn = self.attention(self.input_layernorm(x), mask, cache)
        ffn = self.mlp(self.post_attention_layernorm(x))
        out = attn + ffn + residual
        return out


class GPTNeoXModel(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.hidden_size = args.hidden_size
        self.vocab_size = args.vocab_size
        self.num_hidden_layers = args.num_hidden_layers
        self.layer_norm_eps = args.layer_norm_eps
        assert self.vocab_size > 0
        self.embed_in = nn.Embedding(self.vocab_size, self.hidden_size)
        self.embed_out = nn.Linear(args.hidden_size, args.vocab_size, bias=False)
        self.h = [TransformerBlock(args=args) for _ in range(self.num_hidden_layers)]
        self.final_layer_norm = nn.LayerNorm(self.hidden_size, eps=self.layer_norm_eps)

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        _, L = inputs.shape

        hidden_states = self.embed_in(inputs)

        if mask is None:
            mask = create_attention_mask(hidden_states, cache)

        if cache is None:
            cache = [None] * len(self.h)

        for layer, c in zip(self.h, cache):
            hidden_states = layer(hidden_states, mask, cache=c)

        out = self.final_layer_norm(hidden_states)
        out = self.embed_out(out)

        return out


class Model(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.model_type = args.model_type
        self.model = GPTNeoXModel(args)

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        out = self.model(inputs, mask, cache)
        return out

    def sanitize(self, weights):
        new_weights = {}

        for w_key, w_value in weights.items():
            # Created through register_buffer in Pytorch, not needed here.
            ignore_suffixes = [
                ".attention.bias",
                ".attention.masked_bias",
                ".attention.rotary_emb.inv_freq",
            ]

            skip_weight = False
            for ignored_suffix in ignore_suffixes:
                if w_key.endswith(ignored_suffix):
                    skip_weight = True
                    break

            if skip_weight:
                continue

            if not w_key.startswith("model."):
                w_key = f"model.{w_key}"

            w_key = w_key.replace(".gpt_neox.layers.", ".h.")
            w_key = w_key.replace(".gpt_neox.", ".")

            new_weights[w_key] = w_value

        return new_weights

    @property
    def layers(self):
        return self.model.h

>>>> llms/mlx_lm/models/exaone.py
# Copyright © 2024 Apple Inc.

from dataclasses import dataclass
from typing import Any, Dict, Optional, Union

import mlx.core as mx
import mlx.nn as nn

from .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention
from .rope_utils import initialize_rope


@dataclass
class ModelArgs(BaseModelArgs):
    model_type: str
    hidden_size: int
    num_layers: int
    intermediate_size: int
    num_attention_heads: int
    vocab_size: int
    rope_theta: float
    layer_norm_epsilon: float
    num_key_value_heads: int
    head_dim: Optional[int] = None
    max_position_embeddings: Optional[int] = None
    rope_traditional: bool = False
    rope_scaling: Optional[Dict[str, Union[float, str]]] = None
    tie_word_embeddings: bool = True
    attention_bias: bool = False
    mlp_bias: bool = False


class AttentionModule(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        dim = args.hidden_size
        self.n_heads = n_heads = args.num_attention_heads
        self.n_kv_heads = n_kv_heads = args.num_key_value_heads
        self.head_dim = head_dim = args.head_dim or (dim // n_heads)
        self.scale = head_dim**-0.5

        self.q_proj = nn.Linear(dim, n_heads * head_dim, bias=args.attention_bias)
        self.k_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=args.attention_bias)
        self.v_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=args.attention_bias)
        self.out_proj = nn.Linear(n_heads * head_dim, dim, bias=args.attention_bias)

        self.rope = initialize_rope(
            self.head_dim,
            args.rope_theta,
            args.rope_traditional,
            args.rope_scaling,
            args.max_position_embeddings,
        )

    def __call__(
        self, x: mx.array, mask: Optional[mx.array] = None, cache: Optional[Any] = None
    ) -> mx.array:
        B, L, D = x.shape
        q = self.q_proj(x).reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)
        k = self.k_proj(x).reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)
        v = self.v_proj(x).reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)

        if cache is not None:
            q = self.rope(q, offset=cache.offset)
            k = self.rope(k, offset=cache.offset)
            k, v = cache.update_and_fetch(k, v)
        else:
            q = self.rope(q)
            k = self.rope(k)

        out = scaled_dot_product_attention(
            q, k, v, cache=cache, scale=self.scale, mask=mask
        )
        out = out.transpose(0, 2, 1, 3).reshape(B, L, D)
        return self.out_proj(out)


class Attention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.attention = AttentionModule(args)


class MLP(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        dim = args.hidden_size
        hidden_dim = args.intermediate_size
        self.c_fc_0 = nn.Linear(dim, hidden_dim, bias=args.mlp_bias)
        self.c_fc_1 = nn.Linear(dim, hidden_dim, bias=args.mlp_bias)
        self.c_proj = nn.Linear(hidden_dim, dim, bias=args.mlp_bias)

    def __call__(self, x: mx.array) -> mx.array:
        return self.c_proj(nn.silu(self.c_fc_0(x)) * self.c_fc_1(x))


class TransformerBlock(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.ln_1 = nn.RMSNorm(args.hidden_size, eps=args.layer_norm_epsilon)
        self.attn = Attention(args)
        self.ln_2 = nn.RMSNorm(args.hidden_size, eps=args.layer_norm_epsilon)
        self.mlp = MLP(args)

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        h = x + self.attn.attention(self.ln_1(x), mask, cache)
        out = h + self.mlp(self.ln_2(h))
        return out


class ExaoneModel(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.wte = nn.Embedding(args.vocab_size, args.hidden_size)
        self.h = [TransformerBlock(args) for _ in range(args.num_layers)]
        self.ln_f = nn.RMSNorm(args.hidden_size, eps=args.layer_norm_epsilon)

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        h = self.wte(inputs)
        if mask is None:
            mask = create_attention_mask(h, cache)

        if cache is None:
            cache = [None] * len(self.h)

        for layer, c in zip(self.h, cache):
            h = layer(h, mask, cache=c)

        return self.ln_f(h)


class Model(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.model_type = args.model_type
        self.transformer = ExaoneModel(args)
        if not args.tie_word_embeddings:
            self.lm_head = nn.Linear(args.hidden_size, args.vocab_size, bias=False)

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        out = self.transformer(inputs, mask, cache)
        if self.args.tie_word_embeddings:
            out = self.transformer.wte.as_linear(out)
        else:
            out = self.lm_head(out)
        return out

    @property
    def layers(self):
        return self.transformer.h

>>>> llms/mlx_lm/models/deepseek_v3.py
# Copyright © 2024 Apple Inc.

import math
from dataclasses import dataclass
from functools import partial
from typing import Any, Dict, Optional, Tuple

import mlx.core as mx
import mlx.nn as nn

from .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention
from .switch_layers import SwitchGLU


@dataclass
class ModelArgs(BaseModelArgs):
    model_type: str = "deepseek_v3"
    vocab_size: int = 102400
    hidden_size: int = 4096
    intermediate_size: int = 11008
    moe_intermediate_size: int = 1407
    num_hidden_layers: int = 30
    num_attention_heads: int = 32
    num_key_value_heads: int = 32
    n_shared_experts: Optional[int] = None
    n_routed_experts: Optional[int] = None
    routed_scaling_factor: float = 1.0
    kv_lora_rank: int = 512
    q_lora_rank: int = 1536
    qk_rope_head_dim: int = 64
    v_head_dim: int = 128
    qk_nope_head_dim: int = 128
    topk_method: str = "noaux_tc"
    scoring_func: str = "sigmoid"
    norm_topk_prob: bool = True
    n_group: Optional[int] = None
    topk_group: Optional[int] = None
    num_experts_per_tok: Optional[int] = None
    moe_layer_freq: int = 1
    first_k_dense_replace: int = 0
    max_position_embeddings: int = 2048
    rms_norm_eps: float = 1e-6
    rope_theta: float = 10000.0
    rope_scaling: Dict = None
    attention_bias: bool = False


def yarn_find_correction_dim(
    num_rotations, dim, base=10000, max_position_embeddings=2048
):
    return (dim * math.log(max_position_embeddings / (num_rotations * 2 * math.pi))) / (
        2 * math.log(base)
    )


def yarn_find_correction_range(
    low_rot, high_rot, dim, base=10000, max_position_embeddings=2048
):
    low = math.floor(
        yarn_find_correction_dim(low_rot, dim, base, max_position_embeddings)
    )
    high = math.ceil(
        yarn_find_correction_dim(high_rot, dim, base, max_position_embeddings)
    )
    return max(low, 0), min(high, dim - 1)


def yarn_get_mscale(scale=1, mscale=1):
    if scale <= 1:
        return 1.0
    return 0.1 * mscale * math.log(scale) + 1.0


def yarn_linear_ramp_mask(min_val, max_val, dim):
    if min_val == max_val:
        max_val += 0.001  # Prevent singularity

    linear_func = (mx.arange(dim, dtype=mx.float32) - min_val) / (max_val - min_val)
    return mx.clip(linear_func, 0, 1)


class DeepseekV3YarnRotaryEmbedding(nn.Module):
    def __init__(
        self,
        dim,
        max_position_embeddings=2048,
        base=10000,
        scaling_factor=1.0,
        original_max_position_embeddings=4096,
        beta_fast=32,
        beta_slow=1,
        mscale=1,
        mscale_all_dim=0,
    ):
        super().__init__()
        self.mscale = yarn_get_mscale(scaling_factor, mscale) / yarn_get_mscale(
            scaling_factor, mscale_all_dim
        )
        freq_extra = base ** (mx.arange(0, dim, 2, dtype=mx.float32) / dim)
        freq_inter = scaling_factor * base ** (
            mx.arange(0, dim, 2, dtype=mx.float32) / dim
        )
        low, high = yarn_find_correction_range(
            beta_fast,
            beta_slow,
            dim,
            base,
            original_max_position_embeddings,
        )
        freq_mask = 1.0 - yarn_linear_ramp_mask(low, high, dim // 2)
        self._freqs = (freq_inter * freq_extra) / (
            freq_inter * freq_mask + freq_extra * (1 - freq_mask)
        )

    def __call__(self, x, offset=0):
        if self.mscale != 1.0:
            x = self.mscale * x
        return mx.fast.rope(
            x,
            x.shape[-1],
            traditional=True,
            base=None,
            scale=1.0,
            offset=offset,
            freqs=self._freqs,
        )


# A clipped silu to prevent fp16 from overflowing
@partial(mx.compile, shapeless=True)
def clipped_silu(x):
    return mx.clip(x * mx.sigmoid(x), a_min=-100, a_max=100)


class DeepseekV3Attention(nn.Module):
    def __init__(self, config: ModelArgs):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.num_heads = config.num_attention_heads
        self.max_position_embeddings = config.max_position_embeddings
        self.rope_theta = config.rope_theta
        self.q_lora_rank = config.q_lora_rank
        self.qk_rope_head_dim = config.qk_rope_head_dim
        self.kv_lora_rank = config.kv_lora_rank
        self.v_head_dim = config.v_head_dim
        self.qk_nope_head_dim = config.qk_nope_head_dim
        self.q_head_dim = config.qk_nope_head_dim + config.qk_rope_head_dim

        self.scale = self.q_head_dim**-0.5

        if self.q_lora_rank is None:
            self.q_proj = nn.Linear(
                self.hidden_size, self.num_heads * self.q_head_dim, bias=False
            )
        else:
            self.q_a_proj = nn.Linear(
                self.hidden_size, self.q_lora_rank, bias=config.attention_bias
            )
            self.q_a_layernorm = nn.RMSNorm(self.q_lora_rank)
            self.q_b_proj = nn.Linear(
                self.q_lora_rank, self.num_heads * self.q_head_dim, bias=False
            )

        self.kv_a_proj_with_mqa = nn.Linear(
            self.hidden_size,
            self.kv_lora_rank + self.qk_rope_head_dim,
            bias=config.attention_bias,
        )
        self.kv_a_layernorm = nn.RMSNorm(self.kv_lora_rank)
        self.kv_b_proj = nn.Linear(
            self.kv_lora_rank,
            self.num_heads
            * (self.q_head_dim - self.qk_rope_head_dim + self.v_head_dim),
            bias=False,
        )

        self.o_proj = nn.Linear(
            self.num_heads * self.v_head_dim,
            self.hidden_size,
            bias=config.attention_bias,
        )

        if self.config.rope_scaling is not None:
            mscale_all_dim = self.config.rope_scaling.get("mscale_all_dim", 0)
            scaling_factor = self.config.rope_scaling["factor"]
            if mscale_all_dim:
                mscale = yarn_get_mscale(scaling_factor, mscale_all_dim)
                self.scale = self.scale * mscale * mscale

            rope_kwargs = {
                key: self.config.rope_scaling[key]
                for key in [
                    "original_max_position_embeddings",
                    "beta_fast",
                    "beta_slow",
                    "mscale",
                    "mscale_all_dim",
                ]
                if key in self.config.rope_scaling
            }
            self.rope = DeepseekV3YarnRotaryEmbedding(
                dim=self.qk_rope_head_dim,
                max_position_embeddings=self.max_position_embeddings,
                scaling_factor=scaling_factor,
                base=self.rope_theta,
                **rope_kwargs,
            )
        else:
            self.rope = nn.RoPE(
                dims=self.qk_rope_head_dim,
                base=self.rope_theta,
                traditional=True,
            )

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        B, L, D = x.shape

        if self.q_lora_rank is None:
            q = self.q_proj(x)
        else:
            q = self.q_b_proj(self.q_a_layernorm(self.q_a_proj(x)))

        q = q.reshape(B, L, self.num_heads, self.q_head_dim).transpose(0, 2, 1, 3)
        q_nope, q_pe = mx.split(q, [self.qk_nope_head_dim], axis=-1)
        compressed_kv = self.kv_a_proj_with_mqa(x)
        compressed_kv, k_pe = mx.split(compressed_kv, [self.kv_lora_rank], axis=-1)
        k_pe = k_pe.reshape(B, L, 1, self.qk_rope_head_dim).transpose(0, 2, 1, 3)
        kv = self.kv_b_proj(self.kv_a_layernorm(compressed_kv))
        kv = kv.reshape(B, L, self.num_heads, -1).transpose(0, 2, 1, 3)

        k_nope, values = mx.split(kv, [self.qk_nope_head_dim], axis=-1)

        if cache is not None:
            q_pe = self.rope(q_pe, cache.offset)
            k_pe = self.rope(k_pe, cache.offset)
            k_pe = mx.repeat(k_pe, self.num_heads, axis=1)
            keys, values = cache.update_and_fetch(
                mx.concatenate([k_nope, k_pe], axis=-1), values
            )
        else:
            q_pe = self.rope(q_pe)
            k_pe = self.rope(k_pe)
            k_pe = mx.repeat(k_pe, self.num_heads, axis=1)
            keys = mx.concatenate([k_nope, k_pe], axis=-1)

        queries = mx.concatenate([q_nope, q_pe], axis=-1)

        output = scaled_dot_product_attention(
            queries, keys, values, cache=cache, scale=self.scale, mask=mask
        )
        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)
        return self.o_proj(output)


class DeepseekV3MLP(nn.Module):
    def __init__(
        self, config: ModelArgs, hidden_size: int = None, intermediate_size: int = None
    ):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size if hidden_size is None else hidden_size
        self.intermediate_size = (
            config.intermediate_size if intermediate_size is None else intermediate_size
        )

        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)

    def __call__(self, x):
        down_proj = self.down_proj(nn.silu(self.gate_proj(x)) * self.up_proj(x))
        return down_proj


@mx.compile
def group_expert_select(
    gates,
    e_score_correction_bias,
    top_k,
    n_group,
    topk_group,
    routed_scaling_factor,
    norm_topk_prob,
):

    k = top_k
    scores = mx.sigmoid(gates.astype(mx.float32))
    scores = scores + e_score_correction_bias
    scores = mx.unflatten(scores, axis=-1, shape=(n_group, -1))
    group_scores = mx.topk(scores, 2, axis=-1).sum(axis=-1, keepdims=True)
    k = n_group - topk_group
    group_idx = mx.argpartition(group_scores, kth=k - 1, axis=-2)[..., :k, :]
    scores = mx.put_along_axis(scores, group_idx, mx.array(0.0), axis=-2)
    scores = mx.flatten(scores, -2, -1)

    k = top_k
    inds = mx.argpartition(-scores, kth=k - 1, axis=-1)[..., :k]
    scores = mx.take_along_axis(scores, inds, axis=-1)
    if top_k > 1 and norm_topk_prob:
        denominator = scores.sum(axis=-1, keepdims=True) + 1e-20
        scores = scores / denominator
    scores = scores * routed_scaling_factor

    return inds, scores


class MoEGate(nn.Module):
    def __init__(self, config: ModelArgs):
        super().__init__()
        self.config = config
        self.top_k = config.num_experts_per_tok
        self.norm_topk_prob = config.norm_topk_prob
        self.n_routed_experts = config.n_routed_experts
        self.routed_scaling_factor = config.routed_scaling_factor
        self.n_group = config.n_group
        self.topk_group = config.topk_group
        self.weight = mx.zeros((self.n_routed_experts, config.hidden_size))
        self.e_score_correction_bias = mx.zeros((self.n_routed_experts,))
        assert config.topk_method == "noaux_tc", "Unsupported topk method."

    def __call__(self, x):
        return group_expert_select(
            x @ self.weight.T,
            self.e_score_correction_bias,
            self.top_k,
            self.n_group,
            self.topk_group,
            self.routed_scaling_factor,
            self.norm_topk_prob,
        )


class DeepseekV3MoE(nn.Module):
    def __init__(self, config: ModelArgs):
        super().__init__()
        self.config = config
        self.num_experts_per_tok = config.num_experts_per_tok
        self.switch_mlp = SwitchGLU(
            config.hidden_size,
            config.moe_intermediate_size,
            config.n_routed_experts,
            activation=clipped_silu,
        )

        self.gate = MoEGate(config)
        if config.n_shared_experts is not None:
            intermediate_size = config.moe_intermediate_size * config.n_shared_experts
            self.shared_experts = DeepseekV3MLP(
                config=config, intermediate_size=intermediate_size
            )

    def __call__(self, x):
        inds, scores = self.gate(x)
        y = self.switch_mlp(x, inds)
        y = (y * scores[..., None]).sum(axis=-2).astype(y.dtype)
        if self.config.n_shared_experts is not None:
            y = y + self.shared_experts(x)

        return y


class DeepseekV3DecoderLayer(nn.Module):
    def __init__(self, config: ModelArgs, layer_idx: int):
        super().__init__()
        self.self_attn = DeepseekV3Attention(config)
        self.mlp = (
            DeepseekV3MoE(config)
            if (
                config.n_routed_experts is not None
                and layer_idx >= config.first_k_dense_replace
                and layer_idx % config.moe_layer_freq == 0
            )
            else DeepseekV3MLP(config)
        )
        self.input_layernorm = nn.RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = nn.RMSNorm(
            config.hidden_size, eps=config.rms_norm_eps
        )

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        r = self.self_attn(self.input_layernorm(x), mask, cache)
        h = x + r
        r = self.mlp(self.post_attention_layernorm(h))
        return h + r


class DeepseekV3Model(nn.Module):
    def __init__(self, config: ModelArgs):
        super().__init__()
        self.vocab_size = config.vocab_size
        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)
        self.layers = [
            DeepseekV3DecoderLayer(config, idx)
            for idx in range(config.num_hidden_layers)
        ]
        self.start_idx = 0
        self.end_idx = len(self.layers)
        self.num_layers = self.end_idx

        self.norm = nn.RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.pipeline_rank = 0
        self.pipeline_size = 1

    def pipeline(self, group):
        # Split layers in reverse so rank=0 gets the last layers and
        # rank=pipeline_size-1 gets the first
        self.pipeline_rank = group.rank()
        self.pipeline_size = group.size()
        layers_per_rank = len(self.layers) // self.pipeline_size
        extra = len(self.layers) - layers_per_rank * self.pipeline_size
        if self.pipeline_rank < extra:
            layers_per_rank += 1
        self.start_idx = (self.pipeline_size - self.pipeline_rank - 1) * layers_per_rank
        self.end_idx = self.start_idx + layers_per_rank
        self.layers = self.layers[: self.end_idx]
        self.layers[: self.start_idx] = [None] * self.start_idx
        self.num_layers = len(self.layers) - self.start_idx

    def __call__(
        self,
        x: mx.array,
        cache: Optional[Any] = None,
        mask: Optional[mx.array] = None,
    ) -> mx.array:
        h = self.embed_tokens(x)

        pipeline_rank = self.pipeline_rank
        pipeline_size = self.pipeline_size
        # Hack to avoid time-outs during prompt-processing
        dist_stream = mx.cpu if h.shape[1] > 1 else mx.gpu
        if mask is None:
            mask = create_attention_mask(h, cache)

        if cache is None:
            cache = [None] * self.num_layers

        # Receive from the previous process in the pipeline

        if pipeline_rank < pipeline_size - 1:
            h = mx.distributed.recv_like(h, (pipeline_rank + 1), stream=dist_stream)

        for i in range(self.num_layers):
            h = self.layers[self.start_idx + i](h, mask, cache[i])

        # Send to the next process in the pipeline
        if pipeline_rank != 0:
            h = mx.distributed.send(
                h, (pipeline_rank - 1) % pipeline_size, stream=dist_stream
            )

        # Broadcast h while keeping it in the graph
        h = mx.distributed.all_gather(h, stream=dist_stream)[: h.shape[0]]

        return self.norm(h)


class Model(nn.Module):
    def __init__(self, config: ModelArgs):
        super().__init__()
        self.args = config
        self.model_type = config.model_type
        self.model = DeepseekV3Model(config)
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)

    def __call__(
        self,
        inputs: mx.array,
        cache: Optional[Any] = None,
        mask: Optional[mx.array] = None,
    ):
        out = self.model(inputs, cache, mask)
        return self.lm_head(out)

    def sanitize(self, weights):
        for l in range(self.args.num_hidden_layers):
            prefix = f"model.layers.{l}"
            for n, m in [("w1", "gate_proj"), ("w2", "down_proj"), ("w3", "up_proj")]:
                for k in ["weight", "scales", "biases"]:
                    if f"{prefix}.mlp.experts.0.{m}.{k}" in weights:
                        to_join = [
                            weights.pop(f"{prefix}.mlp.experts.{e}.{m}.{k}")
                            for e in range(self.args.n_routed_experts)
                        ]
                        weights[f"{prefix}.mlp.switch_mlp.{m}.{k}"] = mx.stack(to_join)

        # Remove multi-token prediction layer and any unused precomputed rotary freqs
        return {
            k: v
            for k, v in weights.items()
            if not k.startswith("model.layers.61") and "rotary_emb.inv_freq" not in k
        }

    @property
    def layers(self):
        return self.model.layers[self.model.start_idx : self.model.end_idx]

>>>> llms/mlx_lm/models/gpt_bigcode.py
# Copyright © 2023-2024 Apple Inc.

from dataclasses import dataclass
from typing import Any, Dict, Optional, Tuple, Union

import mlx.core as mx
import mlx.nn as nn
import numpy as np

from .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention


@dataclass
class ModelArgs(BaseModelArgs):
    model_type: str
    n_embd: int
    n_layer: int
    n_inner: int
    n_head: int
    n_positions: int
    layer_norm_epsilon: float
    vocab_size: int
    num_key_value_heads: int = None
    multi_query: bool = True
    attention_bias: bool = True
    mlp_bias: bool = True
    tie_word_embeddings: bool = True

    def __post_init__(self):
        if self.num_key_value_heads is None:
            self.num_key_value_heads = 1 if self.multi_query else self.n_head


class Attention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        self.dim = dim = args.n_embd
        self.n_heads = n_heads = args.n_head
        self.n_kv_heads = n_kv_heads = 1 if args.multi_query else args.n_head

        self.head_dim = head_dim = dim // n_heads

        self.kv_dim = n_kv_heads * head_dim

        self.scale = head_dim**-0.5

        if hasattr(args, "attention_bias"):
            attention_bias = args.attention_bias
        else:
            attention_bias = False

        self.c_attn = nn.Linear(dim, dim + 2 * self.kv_dim, bias=attention_bias)
        self.c_proj = nn.Linear(dim, dim, bias=attention_bias)

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        B, L, D = x.shape

        qkv = self.c_attn(x)
        queries, keys, values = mx.split(
            qkv, [self.dim, self.dim + self.kv_dim], axis=-1
        )

        # Prepare the queries, keys and values for the attention computation
        queries = queries.reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)
        keys = keys.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)
        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)

        if cache is not None:
            keys, values = cache.update_and_fetch(keys, values)

        output = scaled_dot_product_attention(
            queries, keys, values, cache=cache, scale=self.scale, mask=mask
        )
        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)
        return self.c_proj(output)


class MLP(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        dim = args.n_embd
        hidden_dim = args.n_inner
        if hasattr(args, "mlp_bias"):
            mlp_bias = args.mlp_bias
        else:
            mlp_bias = False

        self.c_fc = nn.Linear(dim, hidden_dim, bias=mlp_bias)
        self.c_proj = nn.Linear(hidden_dim, dim, bias=mlp_bias)

    def __call__(self, x) -> mx.array:
        return self.c_proj(nn.gelu(self.c_fc(x)))


class TransformerBlock(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.n_head = args.n_head
        self.n_embd = args.n_embd
        self.attn = Attention(args)
        self.mlp = MLP(args)
        self.ln_1 = nn.LayerNorm(args.n_embd, eps=args.layer_norm_epsilon)
        self.ln_2 = nn.LayerNorm(args.n_embd, eps=args.layer_norm_epsilon)
        self.args = args

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        r = self.attn(self.ln_1(x), mask, cache)
        h = x + r
        r = self.mlp(self.ln_2(h))
        out = h + r
        return out


class GPTBigCodeModel(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.vocab_size = args.vocab_size
        assert self.vocab_size > 0
        self.wte = nn.Embedding(args.vocab_size, args.n_embd)
        self.wpe = nn.Embedding(args.n_positions, args.n_embd)
        self.h = [TransformerBlock(args=args) for _ in range(args.n_layer)]
        self.ln_f = nn.LayerNorm(args.n_embd, eps=args.layer_norm_epsilon)

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        B, L = inputs.shape

        hidden_states = self.wte(inputs)

        mask = None
        if mask is not None and hidden_states.shape[1] > 1:
            mask = create_attention_mask(hidden_states, cache)

        if cache is None:
            cache = [None] * len(self.h)
            position_ids = mx.array(np.arange(L))
        else:
            position_ids = mx.array(np.arange(cache[0].offset, cache[0].offset + L))

        hidden_states += self.wpe(position_ids)

        for layer, c in zip(self.h, cache):
            hidden_states = layer(hidden_states, mask, cache=c)

        return self.ln_f(hidden_states)


class Model(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.model_type = args.model_type
        self.transformer = GPTBigCodeModel(args)
        if not args.tie_word_embeddings:
            self.lm_head = nn.Linear(args.n_embd, args.vocab_size, bias=False)

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        out = self.transformer(inputs, mask, cache)
        if self.args.tie_word_embeddings:
            out = self.transformer.wte.as_linear(out)
        else:
            out = self.lm_head(out)
        return out

    @property
    def layers(self):
        return self.transformer.h

>>>> llms/mlx_lm/models/gemma.py
# Copyright © 2023-2024 Apple Inc.

from dataclasses import dataclass
from typing import Any, Optional, Tuple

import mlx.core as mx
import mlx.nn as nn

from .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention


@dataclass
class ModelArgs(BaseModelArgs):
    model_type: str
    hidden_size: int
    num_hidden_layers: int
    intermediate_size: int
    num_attention_heads: int
    head_dim: int
    rms_norm_eps: float
    vocab_size: int
    num_key_value_heads: int
    rope_theta: float = 10000
    rope_traditional: bool = False


class RMSNorm(nn.Module):
    def __init__(self, dims: int, eps: float = 1e-5):
        super().__init__()
        self.weight = mx.ones((dims,))
        self.eps = eps

    def __call__(self, x):
        return mx.fast.rms_norm(x, 1.0 + self.weight, self.eps)


class Attention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        dim = args.hidden_size
        self.n_heads = n_heads = args.num_attention_heads
        self.n_kv_heads = n_kv_heads = args.num_key_value_heads
        self.head_dim = head_dim = args.head_dim

        self.scale = head_dim**-0.5

        self.q_proj = nn.Linear(dim, n_heads * head_dim, bias=False)
        self.k_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=False)
        self.v_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=False)
        self.o_proj = nn.Linear(n_heads * head_dim, dim, bias=False)

        self.rope = nn.RoPE(
            head_dim,
            traditional=args.rope_traditional,
            base=args.rope_theta,
        )

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        B, L, D = x.shape

        queries, keys, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)

        # Prepare the queries, keys and values for the attention computation
        queries = queries.reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)
        keys = keys.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)
        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)

        if cache is not None:
            queries = self.rope(queries, offset=cache.offset)
            keys = self.rope(keys, offset=cache.offset)
            keys, values = cache.update_and_fetch(keys, values)
        else:
            queries = self.rope(queries)
            keys = self.rope(keys)

        output = scaled_dot_product_attention(
            queries, keys, values, cache=cache, scale=self.scale, mask=mask
        )

        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)
        return self.o_proj(output)


class MLP(nn.Module):
    def __init__(self, dim, hidden_dim):
        super().__init__()
        self.gate_proj = nn.Linear(dim, hidden_dim, bias=False)
        self.down_proj = nn.Linear(hidden_dim, dim, bias=False)
        self.up_proj = nn.Linear(dim, hidden_dim, bias=False)

    def __call__(self, x) -> mx.array:
        return self.down_proj(nn.gelu(self.gate_proj(x)) * self.up_proj(x))


class TransformerBlock(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.num_attention_heads = args.num_attention_heads
        self.hidden_size = args.hidden_size
        self.self_attn = Attention(args)
        self.mlp = MLP(args.hidden_size, args.intermediate_size)
        self.input_layernorm = RMSNorm(args.hidden_size, eps=args.rms_norm_eps)
        self.post_attention_layernorm = RMSNorm(args.hidden_size, eps=args.rms_norm_eps)
        self.args = args

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        r = self.self_attn(self.input_layernorm(x), mask, cache)
        h = x + r
        r = self.mlp(self.post_attention_layernorm(h))
        out = h + r
        return out


class GemmaModel(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.vocab_size = args.vocab_size
        self.num_hidden_layers = args.num_hidden_layers
        assert self.vocab_size > 0
        self.embed_tokens = nn.Embedding(args.vocab_size, args.hidden_size)
        self.layers = [
            TransformerBlock(args=args) for _ in range(args.num_hidden_layers)
        ]
        self.norm = RMSNorm(args.hidden_size, eps=args.rms_norm_eps)

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        h = self.embed_tokens(inputs)
        h = h * (self.args.hidden_size**0.5)

        if mask is None:
            mask = create_attention_mask(h, cache)

        if cache is None:
            cache = [None] * len(self.layers)

        for layer, c in zip(self.layers, cache):
            h = layer(h, mask, c)

        return self.norm(h)


class Model(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.model_type = args.model_type
        self.model = GemmaModel(args)
        self.args = args

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        out = self.model(inputs, mask, cache)
        out = self.model.embed_tokens.as_linear(out)
        return out

    @property
    def layers(self):
        return self.model.layers

>>>> llms/mlx_lm/models/phimoe.py
# Copyright © 2024 Apple Inc.
import math
from dataclasses import dataclass
from typing import Dict, List, Optional, Union

import mlx.core as mx
import mlx.nn as nn

from .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention
from .su_rope import SuScaledRotaryEmbedding
from .switch_layers import SwitchGLU


@dataclass
class ModelArgs(BaseModelArgs):
    model_type: str = "phimoe"
    vocab_size: int = 32064
    hidden_size: int = 4096
    intermediate_size: int = 6400
    num_hidden_layers: int = 32
    num_attention_heads: int = 32
    num_key_value_heads: int = 8
    max_position_embeddings: int = 131072
    original_max_position_embeddings: int = 4096
    rms_norm_eps: float = 1e-6
    rope_scaling: Dict[str, Union[float, List[float]]] = None
    num_local_experts: int = 16
    num_experts_per_tok: int = 2
    rope_theta: float = 10000.0


class Attention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        dim = args.hidden_size
        self.n_heads = n_heads = args.num_attention_heads
        self.n_kv_heads = n_kv_heads = args.num_key_value_heads

        head_dim = args.hidden_size // n_heads
        self.scale = head_dim**-0.5

        self.q_proj = nn.Linear(dim, n_heads * head_dim, bias=True)
        self.k_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=True)
        self.v_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=True)
        self.o_proj = nn.Linear(n_heads * head_dim, dim, bias=True)

        self.rope = SuScaledRotaryEmbedding(
            head_dim,
            base=args.rope_theta,
            max_position_embeddings=args.max_position_embeddings,
            original_max_position_embeddings=args.original_max_position_embeddings,
            short_factor=args.rope_scaling["short_factor"],
            long_factor=args.rope_scaling["long_factor"],
            short_mscale=args.rope_scaling["short_mscale"],
            long_mscale=args.rope_scaling["long_mscale"],
        )

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache=None,
    ) -> mx.array:
        B, L, D = x.shape

        queries, keys, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)

        # Prepare the queries, keys and values for the attention computation
        queries = queries.reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)
        keys = keys.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)
        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)

        if cache is not None:
            queries = self.rope(queries, offset=cache.offset)
            keys = self.rope(keys, offset=cache.offset)
            keys, values = cache.update_and_fetch(keys, values)
        else:
            queries = self.rope(queries)
            keys = self.rope(keys)

        output = scaled_dot_product_attention(
            queries, keys, values, cache=cache, scale=self.scale, mask=mask
        )
        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)
        return self.o_proj(output)


class PhiMoESparseMoeBlock(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.hidden_dim = args.hidden_size
        self.ffn_dim = args.intermediate_size
        self.num_experts = args.num_local_experts
        self.top_k = args.num_experts_per_tok

        self.gate = nn.Linear(self.hidden_dim, self.num_experts, bias=False)
        self.switch_mlp = SwitchGLU(self.hidden_dim, self.ffn_dim, self.num_experts)

    def __call__(self, x: mx.array) -> mx.array:
        gates = self.gate(x)

        k = self.top_k
        inds = mx.stop_gradient(mx.argpartition(-gates, kth=k - 1, axis=-1)[..., :k])
        scores = mx.take_along_axis(gates, inds, axis=-1)
        scores = mx.softmax(scores, axis=-1, precise=True)

        y = self.switch_mlp(x, inds)
        y = (y * scores[..., None]).sum(axis=-2)

        return y


class PhiMoEDecoderLayer(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.hidden_size = args.hidden_size

        self.self_attn = Attention(args)
        self.block_sparse_moe = PhiMoESparseMoeBlock(args)
        self.input_layernorm = nn.LayerNorm(args.hidden_size, eps=args.rms_norm_eps)
        self.post_attention_layernorm = nn.LayerNorm(
            args.hidden_size, eps=args.rms_norm_eps
        )

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache=None,
    ) -> mx.array:
        residual = x
        hidden_states = self.input_layernorm(x)
        hidden_states = self.self_attn(hidden_states, mask=mask, cache=cache)
        hidden_states = residual + hidden_states

        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = self.block_sparse_moe(hidden_states)
        hidden_states = residual + hidden_states

        return hidden_states


class PhiMoEModel(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.vocab_size = args.vocab_size

        self.embed_tokens = nn.Embedding(args.vocab_size, args.hidden_size)
        self.layers = [PhiMoEDecoderLayer(args) for _ in range(args.num_hidden_layers)]
        self.norm = nn.LayerNorm(args.hidden_size, eps=args.rms_norm_eps)

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ) -> mx.array:
        h = self.embed_tokens(inputs)

        if mask is None:
            mask = create_attention_mask(h, cache)

        if cache is None:
            cache = [None] * len(self.layers)

        for layer, c in zip(self.layers, cache):
            h = layer(h, mask, c)

        return self.norm(h)


class Model(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.model_type = args.model_type
        self.args = args
        self.model = PhiMoEModel(args)
        self.lm_head = nn.Linear(args.hidden_size, args.vocab_size, bias=True)

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        out = self.model(inputs, mask, cache)
        return self.lm_head(out)

    def sanitize(self, weights):
        if "model.layers.0.block_sparse_moe.experts.0.w1.weight" not in weights:
            return weights
        for l in range(self.args.num_hidden_layers):
            prefix = f"model.layers.{l}"
            for n, m in [("w1", "gate_proj"), ("w2", "down_proj"), ("w3", "up_proj")]:
                for k in ["weight", "scales", "biases"]:
                    if f"{prefix}.block_sparse_moe.experts.0.{n}.{k}" in weights:
                        to_join = [
                            weights.pop(
                                f"{prefix}.block_sparse_moe.experts.{e}.{n}.{k}"
                            )
                            for e in range(self.args.num_local_experts)
                        ]
                        weights[f"{prefix}.block_sparse_moe.switch_mlp.{m}.{k}"] = (
                            mx.stack(to_join)
                        )

        return weights

    @property
    def layers(self):
        return self.model.layers

>>>> llms/mlx_lm/models/plamo.py
# Copyright © 2023-2024 Apple Inc.

from dataclasses import dataclass
from typing import Any, Optional

import mlx.core as mx
import mlx.nn as nn
import numpy as np

from .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention


@dataclass
class ModelArgs(BaseModelArgs):
    model_type: str
    hidden_size: int
    num_hidden_layers: int
    intermediate_size: int
    num_attention_heads: int
    rms_norm_eps: float
    vocab_size: int
    n_shared_head: int = 8
    rope_theta: float = 10000
    rope_traditional: bool = False


class Attention(nn.Module):
    def __init__(self, config: ModelArgs) -> None:
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        head_dim = self.hidden_size // config.num_attention_heads

        self.q_num_heads = config.num_attention_heads
        self.qk_dim = self.v_dim = head_dim
        self.k_num_heads = self.v_num_heads = int(
            np.ceil(self.q_num_heads / config.n_shared_head)
        )

        self.scale = head_dim**-0.5

        self.q_proj = nn.Linear(
            self.hidden_size, self.q_num_heads * self.qk_dim, bias=False
        )
        self.k_proj = nn.Linear(
            self.hidden_size, self.k_num_heads * self.qk_dim, bias=False
        )
        self.v_proj = nn.Linear(
            self.hidden_size, self.v_num_heads * self.v_dim, bias=False
        )
        self.o_proj = nn.Linear(
            self.q_num_heads * self.v_dim, self.hidden_size, bias=False
        )
        self.rotary_emb = nn.RoPE(
            head_dim,
            traditional=config.rope_traditional,
            base=config.rope_theta,
            scale=1.0,
        )

    def __call__(
        self,
        hidden_states: mx.array,
        attention_mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        bsz, q_len, _ = hidden_states.shape

        queries = self.q_proj(hidden_states)
        keys = self.k_proj(hidden_states)
        values = self.v_proj(hidden_states)

        # Prepare the queries, keys and values for the attention computation
        queries = queries.reshape(bsz, q_len, self.q_num_heads, self.qk_dim).transpose(
            0, 2, 1, 3
        )
        keys = keys.reshape(bsz, q_len, self.k_num_heads, self.qk_dim).transpose(
            0, 2, 1, 3
        )
        values = values.reshape(bsz, q_len, self.v_num_heads, self.v_dim).transpose(
            0, 2, 1, 3
        )

        if cache is not None:
            queries = self.rotary_emb(queries, offset=cache.offset)
            keys = self.rotary_emb(keys, offset=cache.offset)
            keys, values = cache.update_and_fetch(keys, values)
        else:
            queries = self.rotary_emb(queries)
            keys = self.rotary_emb(keys)

        keys = mx.tile(keys, [1, self.config.n_shared_head, 1, 1])
        values = mx.tile(values, [1, self.config.n_shared_head, 1, 1])

        output = scaled_dot_product_attention(
            queries,
            keys,
            values,
            cache=cache,
            scale=self.scale,
            mask=attention_mask,
        )
        output = output.transpose(0, 2, 1, 3).reshape(bsz, q_len, -1)
        return self.o_proj(output)


class MLP(nn.Module):
    def __init__(self, config: ModelArgs) -> None:
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size
        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)

    def __call__(self, x: mx.array) -> mx.array:
        return self.down_proj(nn.silu(self.gate_proj(x)) * self.up_proj(x))  # type: ignore


class PlamoDecoderLayer(nn.Module):
    def __init__(self, config: ModelArgs) -> None:
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.self_attn = Attention(config)
        self.mlp = MLP(config)
        self.norm = nn.RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

    def __call__(
        self,
        hidden_states: mx.array,
        attention_mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ):
        # from LlamaDecoder
        residual = hidden_states

        hidden_states = self.norm(hidden_states)

        # Self Attention
        hidden_states_sa = self.self_attn(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            cache=cache,
        )

        # Fully Connected
        hidden_states_mlp = self.mlp(hidden_states)

        hidden_states = residual + hidden_states_sa + hidden_states_mlp
        return hidden_states


class PlamoDecoder(nn.Module):
    def __init__(self, config: ModelArgs) -> None:
        super().__init__()
        self.layers = [
            PlamoDecoderLayer(config) for _ in range(config.num_hidden_layers)
        ]


class PlamoModel(nn.Module):
    def __init__(self, config: ModelArgs):
        super().__init__()
        self.config = config
        self.vocab_size = config.vocab_size

        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)
        self.layers = PlamoDecoder(config)  # type: ignore
        self.norm = nn.RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

    def __call__(
        self,
        inputs: mx.array,
        cache: Optional[Any] = None,
        mask: Optional[mx.array] = None,
    ) -> mx.array:
        h = self.embed_tokens(inputs)

        if mask is None:
            mask = create_attention_mask(h, cache)

        if cache is None:
            cache = [None for _ in range(len(self.layers.layers))]

        for layer, c in zip(self.layers.layers, cache):
            h = layer(h, mask, cache=c)

        return self.norm(h)


class Model(nn.Module):
    def __init__(self, args: ModelArgs) -> None:
        super().__init__()
        self.model_type = args.model_type
        self.model = PlamoModel(args)
        self.lm_head: nn.Module = nn.Linear(
            args.hidden_size, args.vocab_size, bias=False
        )
        self.args = args

    def __call__(
        self,
        inputs: mx.array,
        cache: Optional[Any] = None,
        mask: Optional[mx.array] = None,
    ) -> mx.array:
        out = self.model(inputs, cache, mask)
        return self.lm_head(out)

    @property
    def layers(self):
        return self.model.layers.layers

>>>> llms/mlx_lm/models/deepseek.py
from dataclasses import dataclass
from typing import Any, Dict, Optional

import mlx.core as mx
import mlx.nn as nn

from .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention
from .switch_layers import SwitchGLU


@dataclass
class ModelArgs(BaseModelArgs):
    model_type: str = "deepseek"
    vocab_size: int = 102400
    hidden_size: int = 4096
    intermediate_size: int = 11008
    moe_intermediate_size: int = 1407
    num_hidden_layers: int = 30
    num_attention_heads: int = 32
    num_key_value_heads: int = 32
    n_shared_experts: Optional[int] = None
    n_routed_experts: Optional[int] = None
    num_experts_per_tok: Optional[int] = None
    moe_layer_freq: int = 1
    first_k_dense_replace: int = 0
    max_position_embeddings: int = 2048
    rms_norm_eps: float = 1e-6
    rope_theta: float = 10000.0
    rope_scaling: Optional[Dict] = None
    attention_bias: bool = False


class DeepseekAttention(nn.Module):
    def __init__(self, config: ModelArgs):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.num_attention_heads = config.num_attention_heads
        self.num_kv_heads = config.num_key_value_heads
        self.head_dim = config.hidden_size // config.num_attention_heads
        self.scale = self.head_dim**-0.5

        attention_bias = getattr(config, "attention_bias", False)

        self.q_proj = nn.Linear(
            self.hidden_size,
            config.num_attention_heads * self.head_dim,
            bias=attention_bias,
        )
        self.k_proj = nn.Linear(
            self.hidden_size,
            config.num_key_value_heads * self.head_dim,
            bias=attention_bias,
        )
        self.v_proj = nn.Linear(
            self.hidden_size,
            config.num_key_value_heads * self.head_dim,
            bias=attention_bias,
        )
        self.o_proj = nn.Linear(
            self.hidden_size,
            config.num_attention_heads * self.head_dim,
            bias=attention_bias,
        )

        rope_scale = 1.0
        if config.rope_scaling and config.rope_scaling["type"] == "linear":
            assert isinstance(config.rope_scaling["factor"], float)
            rope_scale = 1 / config.rope_scaling["factor"]
        self.rope = nn.RoPE(
            self.head_dim,
            base=config.rope_theta,
            scale=rope_scale,
        )

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        B, L, _ = x.shape

        queries, keys, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)

        queries = queries.reshape(B, L, self.num_attention_heads, -1).transpose(
            0, 2, 1, 3
        )
        keys = keys.reshape(B, L, self.num_kv_heads, -1).transpose(0, 2, 1, 3)
        values = values.reshape(B, L, self.num_kv_heads, -1).transpose(0, 2, 1, 3)

        if cache is not None:
            queries = self.rope(queries, offset=cache.offset)
            keys = self.rope(keys, offset=cache.offset)
            keys, values = cache.update_and_fetch(keys, values)
        else:
            queries = self.rope(queries)
            keys = self.rope(keys)

        output = scaled_dot_product_attention(
            queries, keys, values, cache=cache, scale=self.scale, mask=mask
        )
        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)
        return self.o_proj(output)


class DeepseekMLP(nn.Module):
    def __init__(
        self,
        config: ModelArgs,
        hidden_size: Optional[int] = None,
        intermediate_size: Optional[int] = None,
    ):
        super().__init__()
        self.config = config
        self.hidden_size = hidden_size or config.hidden_size
        self.intermediate_size = intermediate_size or config.intermediate_size
        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
        self.act_fn = nn.silu

    def __call__(self, x: mx.array) -> mx.array:
        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))


class MoEGate(nn.Module):
    def __init__(self, config: ModelArgs):
        super().__init__()
        self.config = config
        self.top_k = config.num_experts_per_tok
        self.n_routed_experts = config.n_routed_experts
        self.weight = mx.zeros((self.n_routed_experts, config.hidden_size))

    def __call__(self, x):
        gates = x @ self.weight.T
        scores = mx.softmax(gates, axis=-1, precise=True)
        k = self.top_k
        inds = mx.stop_gradient(mx.argpartition(-scores, kth=k - 1, axis=-1)[..., :k])
        scores = mx.take_along_axis(scores, inds, axis=-1)
        return inds, scores


class DeepseekMoE(nn.Module):
    def __init__(self, config: ModelArgs):
        super().__init__()
        self.config = config
        self.switch_mlp = SwitchGLU(
            config.hidden_size, config.moe_intermediate_size, config.n_routed_experts
        )

        self.gate = MoEGate(config)
        if config.n_shared_experts is not None:
            intermediate_size = config.moe_intermediate_size * config.n_shared_experts
            self.shared_experts = DeepseekMLP(
                config=config, intermediate_size=intermediate_size
            )

    def __call__(self, x):
        inds, scores = self.gate(x)
        y = self.switch_mlp(x, inds)
        y = (y * scores[..., None]).sum(axis=-2)
        if self.config.n_shared_experts is not None:
            y = y + self.shared_experts(x)

        return y


class DeepseekDecoderLayer(nn.Module):
    def __init__(self, config: ModelArgs, layer_idx: int):
        super().__init__()
        self.self_attn = DeepseekAttention(config)
        self.mlp = (
            DeepseekMoE(config)
            if (
                config.n_routed_experts is not None
                and layer_idx >= config.first_k_dense_replace
                and layer_idx % config.moe_layer_freq == 0
            )
            else DeepseekMLP(config)
        )
        self.input_layernorm = nn.RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = nn.RMSNorm(
            config.hidden_size, eps=config.rms_norm_eps
        )

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        r = self.self_attn(self.input_layernorm(x), mask, cache)
        h = x + r
        r = self.mlp(self.post_attention_layernorm(h))
        out = h + r
        return out


class DeepseekModel(nn.Module):
    def __init__(self, config: ModelArgs):
        super().__init__()
        self.config = config
        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)
        self.layers = [
            DeepseekDecoderLayer(config, idx) for idx in range(config.num_hidden_layers)
        ]
        self.norm = nn.RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

    def __call__(
        self,
        x: mx.array,
        cache: Optional[Any] = None,
        mask: Optional[mx.array] = None,
    ) -> mx.array:
        h = self.embed_tokens(x)
        if mask is None:
            mask = create_attention_mask(h, cache)

        if cache is None:
            cache = [None] * len(self.layers)

        for layer, c in zip(self.layers, cache):
            h = layer(h, mask, c)

        return self.norm(h)


class Model(nn.Module):
    def __init__(self, config: ModelArgs):
        super().__init__()
        self.args = config
        self.model_type = config.model_type
        self.model = DeepseekModel(config)
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)

    def __call__(
        self,
        inputs: mx.array,
        cache: Optional[Any] = None,
        mask: Optional[mx.array] = None,
    ):
        out = self.model(inputs, cache, mask)
        return self.lm_head(out)

    def sanitize(self, weights):
        for l in range(self.args.num_hidden_layers):
            prefix = f"model.layers.{l}"
            for m in ["gate_proj", "down_proj", "up_proj"]:
                for k in ["weight", "scales", "biases"]:
                    if f"{prefix}.mlp.experts.0.{m}.{k}" in weights:
                        to_join = [
                            weights.pop(f"{prefix}.mlp.experts.{e}.{m}.{k}")
                            for e in range(self.args.n_routed_experts)
                        ]
                        weights[f"{prefix}.mlp.switch_mlp.{m}.{k}"] = mx.stack(to_join)
        return weights

    @property
    def layers(self):
        return self.model.layers

>>>> llms/mlx_lm/models/qwen.py
# Copyright © 2023-2024 Apple Inc.

from dataclasses import dataclass

import mlx.core as mx
import mlx.nn as nn

from .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention


@dataclass
class ModelArgs(BaseModelArgs):
    model_type: str
    hidden_size: int = 2048
    num_attention_heads: int = 16
    num_hidden_layers: int = 24
    kv_channels: int = 128
    max_position_embeddings: int = 8192
    layer_norm_epsilon: float = 1e-6
    intermediate_size: int = 11008
    no_bias: bool = True
    vocab_size: int = 151936
    num_key_value_heads = None

    def __post_init__(self):
        if self.num_key_value_heads is None:
            self.num_key_value_heads = self.num_attention_heads


class Attention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        hidden_size = args.hidden_size
        self.num_attention_heads = args.num_attention_heads

        hidden_size_per_attention_head = hidden_size // self.num_attention_heads

        self.rotary_emb = nn.RoPE(hidden_size_per_attention_head, traditional=False)

        proj_size = args.kv_channels * self.num_attention_heads

        self.c_attn = nn.Linear(hidden_size, proj_size * 3, bias=True)
        self.c_proj = nn.Linear(hidden_size, proj_size, bias=not args.no_bias)

        self.scale = hidden_size_per_attention_head**-0.5

    def __call__(self, x, mask=None, cache=None):
        qkv = self.c_attn(x)

        q, k, v = mx.split(qkv, 3, axis=-1)

        B, L, _ = q.shape

        queries = q.reshape(B, L, self.num_attention_heads, -1).transpose(0, 2, 1, 3)
        keys = k.reshape(B, L, self.num_attention_heads, -1).transpose(0, 2, 1, 3)
        values = v.reshape(B, L, self.num_attention_heads, -1).transpose(0, 2, 1, 3)

        if cache is not None:
            queries = self.rotary_emb(queries, offset=cache.offset)
            keys = self.rotary_emb(keys, offset=cache.offset)
            keys, values = cache.update_and_fetch(keys, values)
        else:
            queries = self.rotary_emb(queries)
            keys = self.rotary_emb(keys)

        output = scaled_dot_product_attention(
            queries, keys, values, cache=cache, scale=self.scale, mask=mask
        )
        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)

        return self.c_proj(output)


class MLP(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        self.w1 = nn.Linear(
            args.hidden_size, args.intermediate_size // 2, bias=not args.no_bias
        )
        self.w2 = nn.Linear(
            args.hidden_size, args.intermediate_size // 2, bias=not args.no_bias
        )
        self.c_proj = nn.Linear(
            args.intermediate_size // 2, args.hidden_size, bias=not args.no_bias
        )

    def __call__(self, x):
        a1 = self.w1(x)
        a2 = self.w2(x)
        return self.c_proj(a1 * nn.silu(a2))


class TransformerBlock(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        self.ln_1 = nn.RMSNorm(args.hidden_size, eps=args.layer_norm_epsilon)
        self.attn = Attention(args)
        self.ln_2 = nn.RMSNorm(args.hidden_size, eps=args.layer_norm_epsilon)
        self.mlp = MLP(args)

    def __call__(self, x, mask=None, cache=None):
        residual = x
        x = self.ln_1(x)
        x = self.attn(x, mask=mask, cache=cache)
        residual = x + residual
        x = self.ln_2(residual)
        x = self.mlp(x)
        x = x + residual

        return x


class QwenModel(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.wte = nn.Embedding(args.vocab_size, args.hidden_size)
        self.h = [TransformerBlock(args) for _ in range(args.num_hidden_layers)]
        self.ln_f = nn.RMSNorm(args.hidden_size, eps=args.layer_norm_epsilon)

    def __call__(self, inputs, mask=None, cache=None):
        x = self.wte(inputs)

        if mask is None:
            mask = create_attention_mask(x, cache)

        if cache is None:
            cache = [None] * len(self.h)

        for layer, c in zip(self.h, cache):
            x = layer(x, mask, c)

        return self.ln_f(x)


class Model(nn.Module):
    def __init__(self, config: ModelArgs):
        super().__init__()
        self.model_type = config.model_type
        self.transformer = QwenModel(config)
        self.lm_head = nn.Linear(
            config.hidden_size, config.vocab_size, bias=not config.no_bias
        )
        self.args = config

    def __call__(
        self,
        x: mx.array,
        mask: mx.array = None,
        cache=None,
    ) -> mx.array:
        y = self.transformer(x, mask, cache)
        return self.lm_head(y)

    @property
    def layers(self):
        return self.transformer.h

>>>> llms/mlx_lm/models/cohere2.py
# Copyright © 2023-2024 Apple Inc.

from dataclasses import dataclass
from typing import Optional, Tuple

import mlx.core as mx
import mlx.nn as nn

from .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention
from .cache import KVCache, RotatingKVCache


@dataclass
class ModelArgs(BaseModelArgs):
    model_type: str
    hidden_size: int = 4096
    head_dim: int = 128
    num_hidden_layers: int = 32
    intermediate_size: int = 14336
    num_attention_heads: int = 32
    num_key_value_heads: int = 8
    rope_theta: float = 50000.0
    vocab_size: int = 256000
    layer_norm_eps: float = 1e-05
    logit_scale: float = 0.0625
    attention_bias: bool = False
    layer_norm_bias: bool = False
    sliding_window: int = 4096
    sliding_window_pattern: int = 4


class Attention(nn.Module):
    def __init__(self, args: ModelArgs, layer_idx: int):
        super().__init__()
        self.args = args
        self.layer_idx = layer_idx

        dim = args.hidden_size
        self.n_heads = n_heads = args.num_attention_heads
        self.n_kv_heads = n_kv_heads = args.num_key_value_heads
        self.head_dim = head_dim = args.head_dim
        if (head_dim * n_heads) != dim:
            raise ValueError(
                f"hidden_size must be divisible by num_heads (got `hidden_size`: {dim}"
                f" and `num_heads`: {n_heads})."
            )
        self.scale = head_dim**-0.5

        attetion_bias = args.attention_bias

        self.q_proj = nn.Linear(dim, n_heads * head_dim, bias=attetion_bias)
        self.k_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=attetion_bias)
        self.v_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=attetion_bias)
        self.o_proj = nn.Linear(n_heads * head_dim, dim, bias=attetion_bias)

        self.rope = nn.RoPE(head_dim, traditional=True, base=args.rope_theta)

        self.use_sliding_window = (layer_idx + 1) % args.sliding_window_pattern != 0

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Tuple[mx.array, mx.array]] = None,
    ) -> mx.array:
        B, L, D = x.shape

        queries, keys, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)

        queries = queries.reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)
        keys = keys.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)
        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)

        # Apply RoPE only if sliding window is enabled
        if self.use_sliding_window:
            if cache is None:
                queries = self.rope(queries)
                keys = self.rope(keys)
            else:
                queries = self.rope(queries, offset=cache.offset)
                keys = self.rope(keys, offset=cache.offset)

        if cache is not None:
            keys, values = cache.update_and_fetch(keys, values)

        if self.use_sliding_window and mask is not None:
            key_len = keys.shape[-2]
            if mask.shape[-1] != key_len:
                mask = mask[..., -key_len:]

        output = scaled_dot_product_attention(
            queries, keys, values, cache=cache, scale=self.scale, mask=mask
        )

        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)
        return self.o_proj(output)


class MLP(nn.Module):
    def __init__(self, dim, hidden_dim):
        super().__init__()
        self.gate_proj = nn.Linear(dim, hidden_dim, bias=False)
        self.up_proj = nn.Linear(dim, hidden_dim, bias=False)
        self.down_proj = nn.Linear(hidden_dim, dim, bias=False)

    def __call__(self, x):
        return self.down_proj(nn.silu(self.gate_proj(x)) * self.up_proj(x))


class TransformerBlock(nn.Module):
    def __init__(self, args: ModelArgs, layer_idx: int):
        super().__init__()
        self.hidden_size = args.hidden_size
        self.n_heads = args.num_attention_heads

        self.self_attn = Attention(args, layer_idx)
        self.mlp = MLP(args.hidden_size, args.intermediate_size)
        self.input_layernorm = nn.LayerNorm(
            args.hidden_size, eps=args.layer_norm_eps, bias=args.layer_norm_bias
        )
        self.args = args

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Tuple[mx.array, mx.array]] = None,
    ) -> mx.array:
        h = self.input_layernorm(x)
        attn_h = self.self_attn(h, mask, cache)
        ff_h = self.mlp(h)
        return attn_h + ff_h + x


class CohereModel(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.vocab_size = args.vocab_size
        self.num_hidden_layers = args.num_hidden_layers
        assert self.vocab_size > 0
        self.embed_tokens = nn.Embedding(args.vocab_size, args.hidden_size)
        self.layers = [
            TransformerBlock(args=args, layer_idx=i)
            for i in range(args.num_hidden_layers)
        ]
        self.norm = nn.LayerNorm(
            args.hidden_size, eps=args.layer_norm_eps, bias=args.layer_norm_bias
        )

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        h = self.embed_tokens(inputs)

        if cache is None:
            cache = [None] * len(self.layers)

        if mask is None:
            j = self.args.sliding_window_pattern
            mask = create_attention_mask(h, cache[j - 1 : j])

        for layer, c in zip(self.layers, cache):
            h = layer(h, mask, c)

        return self.norm(h)


class Model(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.model_type = args.model_type
        self.model = CohereModel(args)
        self.args = args

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        out = self.model(inputs, mask, cache)
        out = self.model.embed_tokens.as_linear(out)
        out = out * self.model.args.logit_scale
        return out

    def make_cache(self):
        caches = []
        for i in range(self.args.num_hidden_layers):
            if (
                i % self.args.sliding_window_pattern
                == self.args.sliding_window_pattern - 1
            ):
                caches.append(KVCache())
            else:
                caches.append(
                    RotatingKVCache(max_size=self.args.sliding_window, keep=0)
                )
        return caches

    @property
    def layers(self):
        return self.model.layers

>>>> llms/mlx_lm/models/gemma2.py
# Copyright © 2023-2024 Apple Inc.

from dataclasses import dataclass
from typing import Any, Optional, Tuple

import mlx.core as mx
import mlx.nn as nn

from .base import BaseModelArgs, create_attention_mask


@dataclass
class ModelArgs(BaseModelArgs):
    model_type: str
    hidden_size: int
    num_hidden_layers: int
    intermediate_size: int
    num_attention_heads: int
    head_dim: int
    rms_norm_eps: float
    vocab_size: int
    num_key_value_heads: int
    rope_theta: float = 10000
    rope_traditional: bool = False
    attn_logit_softcapping: float = 50.0
    final_logit_softcapping: float = 30.0
    query_pre_attn_scalar: float = 144.0


class RMSNorm(nn.Module):
    def __init__(self, dims: int, eps: float = 1e-5):
        super().__init__()
        self.weight = mx.ones((dims,))
        self.eps = eps

    def __call__(self, x):
        return mx.fast.rms_norm(x, 1.0 + self.weight, self.eps)


class Attention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        dim = args.hidden_size
        self.n_heads = n_heads = args.num_attention_heads
        self.n_kv_heads = n_kv_heads = args.num_key_value_heads
        self.repeats = n_heads // n_kv_heads
        self.head_dim = head_dim = args.head_dim

        self.scale = 1.0 / (args.query_pre_attn_scalar**0.5)

        self.q_proj = nn.Linear(dim, n_heads * head_dim, bias=False)
        self.k_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=False)
        self.v_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=False)
        self.o_proj = nn.Linear(n_heads * head_dim, dim, bias=False)
        self.attn_logit_softcapping = args.attn_logit_softcapping
        self.rope = nn.RoPE(
            head_dim,
            traditional=args.rope_traditional,
            base=args.rope_theta,
        )

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        B, L, D = x.shape
        queries, keys, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)
        queries = queries.reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)
        keys = keys.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)
        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)

        if cache is not None:
            queries = self.rope(queries, offset=cache.offset)
            keys = self.rope(keys, offset=cache.offset)
            keys, values = cache.update_and_fetch(keys, values)
        else:
            queries = self.rope(queries)
            keys = self.rope(keys)

        queries = queries * self.scale

        if self.repeats > 1:
            queries = queries.reshape(
                B, self.n_kv_heads, self.repeats, L, self.head_dim
            )
            keys = mx.expand_dims(keys, 2)
            values = mx.expand_dims(values, 2)

        scores = queries @ keys.swapaxes(-1, -2)
        scores = mx.tanh(scores / self.attn_logit_softcapping)
        scores *= self.attn_logit_softcapping

        if mask is not None:
            scores = scores + mask
        scores = mx.softmax(scores, precise=True, axis=-1)
        output = scores @ values
        if self.repeats > 1:
            output = output.reshape(B, self.n_heads, L, self.head_dim)
        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)
        return self.o_proj(output)


class MLP(nn.Module):
    def __init__(self, dim, hidden_dim):
        super().__init__()
        self.gate_proj = nn.Linear(dim, hidden_dim, bias=False)
        self.down_proj = nn.Linear(hidden_dim, dim, bias=False)
        self.up_proj = nn.Linear(dim, hidden_dim, bias=False)

    def __call__(self, x) -> mx.array:
        return self.down_proj(nn.gelu_approx(self.gate_proj(x)) * self.up_proj(x))


class TransformerBlock(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.num_attention_heads = args.num_attention_heads
        self.hidden_size = args.hidden_size
        self.self_attn = Attention(args)
        self.mlp = MLP(args.hidden_size, args.intermediate_size)
        self.input_layernorm = RMSNorm(args.hidden_size, eps=args.rms_norm_eps)
        self.pre_feedforward_layernorm = RMSNorm(
            args.hidden_size, eps=args.rms_norm_eps
        )
        self.post_feedforward_layernorm = RMSNorm(
            args.hidden_size, eps=args.rms_norm_eps
        )
        self.post_attention_layernorm = RMSNorm(args.hidden_size, eps=args.rms_norm_eps)
        self.args = args

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        r = self.self_attn(self.input_layernorm(x), mask, cache)
        h = x + self.post_attention_layernorm(r)
        r = self.mlp(self.pre_feedforward_layernorm(h))
        out = h + self.post_feedforward_layernorm(r)
        return out


class GemmaModel(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.vocab_size = args.vocab_size
        self.num_hidden_layers = args.num_hidden_layers
        assert self.vocab_size > 0
        self.embed_tokens = nn.Embedding(args.vocab_size, args.hidden_size)
        self.layers = [
            TransformerBlock(args=args) for _ in range(args.num_hidden_layers)
        ]
        self.norm = RMSNorm(args.hidden_size, eps=args.rms_norm_eps)

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        h = self.embed_tokens(inputs)
        h = h * (self.args.hidden_size**0.5)

        if mask is None:
            mask = create_attention_mask(h, cache)

        if cache is None:
            cache = [None] * len(self.layers)

        for layer, c in zip(self.layers, cache):
            h = layer(h, mask, c)

        return self.norm(h)


class Model(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.model_type = args.model_type
        self.final_logit_softcapping = args.final_logit_softcapping
        self.model = GemmaModel(args)
        self.args = args

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        out = self.model(inputs, mask, cache)
        out = self.model.embed_tokens.as_linear(out)
        out = mx.tanh(out / self.final_logit_softcapping)
        out = out * self.final_logit_softcapping
        return out

    @property
    def layers(self):
        return self.model.layers

>>>> llms/mlx_lm/models/llama.py
# Copyright © 2023-2024 Apple Inc.

from dataclasses import dataclass
from typing import Any, Dict, Optional, Union

import mlx.core as mx
import mlx.nn as nn

from .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention
from .rope_utils import initialize_rope


@dataclass
class ModelArgs(BaseModelArgs):
    model_type: str
    hidden_size: int
    num_hidden_layers: int
    intermediate_size: int
    num_attention_heads: int
    rms_norm_eps: float
    vocab_size: int
    head_dim: Optional[int] = None
    max_position_embeddings: Optional[int] = None
    num_key_value_heads: Optional[int] = None
    attention_bias: bool = False
    mlp_bias: bool = False
    rope_theta: float = 10000
    rope_traditional: bool = False
    rope_scaling: Optional[Dict[str, Union[float, str]]] = None
    tie_word_embeddings: bool = True

    def __post_init__(self):
        if self.num_key_value_heads is None:
            self.num_key_value_heads = self.num_attention_heads


class Attention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        dim = args.hidden_size
        self.n_heads = n_heads = args.num_attention_heads
        self.n_kv_heads = n_kv_heads = args.num_key_value_heads

        self.head_dim = head_dim = args.head_dim or args.hidden_size // n_heads

        self.scale = head_dim**-0.5
        if hasattr(args, "attention_bias"):
            attention_bias = args.attention_bias
        else:
            attention_bias = False

        self.q_proj = nn.Linear(dim, n_heads * head_dim, bias=attention_bias)
        self.k_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=attention_bias)
        self.v_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=attention_bias)
        self.o_proj = nn.Linear(n_heads * head_dim, dim, bias=attention_bias)

        self.rope = initialize_rope(
            self.head_dim,
            args.rope_theta,
            args.rope_traditional,
            args.rope_scaling,
            args.max_position_embeddings,
        )

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        B, L, D = x.shape

        queries, keys, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)

        # Prepare the queries, keys and values for the attention computation
        queries = queries.reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)
        keys = keys.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)
        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)

        if cache is not None:
            queries = self.rope(queries, offset=cache.offset)
            keys = self.rope(keys, offset=cache.offset)
            keys, values = cache.update_and_fetch(keys, values)
        else:
            queries = self.rope(queries)
            keys = self.rope(keys)

        output = scaled_dot_product_attention(
            queries, keys, values, cache=cache, scale=self.scale, mask=mask
        )

        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)
        return self.o_proj(output)


class MLP(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        dim = args.hidden_size
        hidden_dim = args.intermediate_size
        if hasattr(args, "mlp_bias"):
            mlp_bias = args.mlp_bias
        else:
            mlp_bias = False

        self.gate_proj = nn.Linear(dim, hidden_dim, bias=mlp_bias)
        self.down_proj = nn.Linear(hidden_dim, dim, bias=mlp_bias)
        self.up_proj = nn.Linear(dim, hidden_dim, bias=mlp_bias)

    def __call__(self, x) -> mx.array:
        return self.down_proj(nn.silu(self.gate_proj(x)) * self.up_proj(x))


class TransformerBlock(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.num_attention_heads = args.num_attention_heads
        self.hidden_size = args.hidden_size
        self.self_attn = Attention(args)
        self.mlp = MLP(args)
        self.input_layernorm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)
        self.post_attention_layernorm = nn.RMSNorm(
            args.hidden_size, eps=args.rms_norm_eps
        )
        self.args = args

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        r = self.self_attn(self.input_layernorm(x), mask, cache)
        h = x + r
        r = self.mlp(self.post_attention_layernorm(h))
        out = h + r
        return out


class LlamaModel(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.vocab_size = args.vocab_size
        self.num_hidden_layers = args.num_hidden_layers
        assert self.vocab_size > 0
        self.embed_tokens = nn.Embedding(args.vocab_size, args.hidden_size)
        self.layers = [
            TransformerBlock(args=args) for _ in range(args.num_hidden_layers)
        ]
        self.norm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        h = self.embed_tokens(inputs)

        if mask is None:
            mask = create_attention_mask(h, cache)

        if cache is None:
            cache = [None] * len(self.layers)

        for layer, c in zip(self.layers, cache):
            h = layer(h, mask, cache=c)

        return self.norm(h)


class Model(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.model_type = args.model_type
        self.model = LlamaModel(args)
        if not args.tie_word_embeddings:
            self.lm_head = nn.Linear(args.hidden_size, args.vocab_size, bias=False)

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        out = self.model(inputs, mask, cache)
        if self.args.tie_word_embeddings:
            out = self.model.embed_tokens.as_linear(out)
        else:
            out = self.lm_head(out)
        return out

    def sanitize(self, weights):
        # Remove unused precomputed rotary freqs
        weights = {
            k: v for k, v in weights.items() if "self_attn.rotary_emb.inv_freq" not in k
        }
        if self.args.tie_word_embeddings:
            weights.pop("lm_head.weight", None)
        return weights

    @property
    def layers(self):
        return self.model.layers

>>>> llms/mlx_lm/models/phi3small.py
# Copyright © 2023-2024 Apple Inc.

import math
from dataclasses import dataclass
from functools import partial
from typing import Any, Optional

import mlx.core as mx
import mlx.nn as nn

from .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention


@dataclass
class ModelArgs(BaseModelArgs):
    model_type: str
    hidden_size: int
    dense_attention_every_n_layers: int
    ff_intermediate_size: int
    gegelu_limit: float
    num_hidden_layers: int
    num_attention_heads: int
    layer_norm_epsilon: float
    vocab_size: int
    num_key_value_heads: int
    mup_attn_multiplier: float = 1.0
    mup_use_scaling: bool = True
    mup_embedding_multiplier: float = 10.0
    mup_width_multiplier: float = 8.0
    rope_embedding_base: float = 1000000
    rope_position_scale: float = 1.0
    blocksparse_block_size: int = 64
    blocksparse_num_local_blocks: int = 16
    blocksparse_vert_stride: int = 8


@partial(mx.compile, shapeless=True)
def gegelu_impl(a_gelu, a_linear, limit):
    a_gelu = mx.where(
        mx.isinf(a_gelu),
        a_gelu,
        mx.clip(a_gelu, a_min=None, a_max=limit),
    )
    a_linear = mx.where(
        mx.isinf(a_linear),
        a_linear,
        mx.clip(a_linear, a_min=-limit, a_max=limit),
    )
    out_gelu = a_gelu * mx.sigmoid(1.702 * a_gelu)
    return out_gelu * (a_linear + 1.0)


def gegelu(x, limit):
    a_gelu, a_linear = x[..., ::2], x[..., 1::2]
    return gegelu_impl(a_gelu, a_linear, limit)


class Attention(nn.Module):
    def __init__(self, args: ModelArgs, layer_idx):
        super().__init__()

        dim = args.hidden_size
        self.n_heads = n_heads = args.num_attention_heads
        self.n_kv_heads = n_kv_heads = args.num_key_value_heads
        self.n_q_per_kv = n_heads // n_kv_heads

        self.head_dim = head_dim = args.hidden_size // n_heads

        self.query_key_value = nn.Linear(
            dim, (self.n_heads + 2 * self.n_kv_heads) * head_dim
        )
        self.dense = nn.Linear(dim, dim)

        if args.mup_use_scaling:
            norm_factor = head_dim / args.mup_attn_multiplier
        else:
            norm_factor = math.sqrt(head_dim)
        self.scale = 1.0 / norm_factor

        self.rope = nn.RoPE(
            head_dim,
            traditional=False,
            base=args.rope_embedding_base,
            scale=args.rope_position_scale,
        )

        if layer_idx % args.dense_attention_every_n_layers == 0:
            self.block_sparse = True
            self.blocksparse_block_size = args.blocksparse_block_size
            if self.blocksparse_block_size not in (32, 64):
                raise ValueError(
                    f"Unsupported block size {self.blocksparse_block_size}"
                )
            self.blocksparse_num_local_blocks = args.blocksparse_num_local_blocks
            self.blocksparse_vert_stride = args.blocksparse_vert_stride
        else:
            self.block_sparse = False

    def _block_sparse_mask(self, q_len, kv_len):
        vert_stride = self.blocksparse_vert_stride
        local_blocks = self.blocksparse_num_local_blocks
        block_size = self.blocksparse_block_size
        n_heads = self.n_heads

        kv_blocks = (kv_len + block_size - 1) // block_size
        q_blocks = (q_len + block_size - 1) // block_size
        q_pos = mx.arange(kv_blocks - q_blocks, kv_blocks)[None, :, None]
        k_pos = mx.arange(kv_blocks)[None, None]

        mask_vert_strided = (
            mx.arange(kv_blocks)[None, :] + mx.arange(1, n_heads + 1)[:, None]
        ) % vert_stride
        mask_vert_strided = (mask_vert_strided == 0)[:, None, :]

        block_mask = (q_pos >= k_pos) & (
            (q_pos - k_pos < local_blocks) | mask_vert_strided
        )
        block_mask = block_mask.reshape(
            self.n_kv_heads, self.n_q_per_kv, *block_mask.shape[-2:]
        )
        dense_mask = mx.repeat(
            mx.repeat(block_mask, block_size, axis=-1), block_size, axis=-2
        )
        return block_mask, dense_mask[..., -q_len:, :kv_len]

    def _block_sparse_attention(self, queries, keys, values, scale, mask):
        queries = scale * queries
        B = queries.shape[0]
        L = queries.shape[2]
        queries = mx.reshape(queries, (B, self.n_kv_heads, self.n_q_per_kv, L, -1))
        keys = mx.expand_dims(keys, 2)
        values = mx.expand_dims(values, 2)

        # TODO get rid of dense mask if we have a fill value
        block_mask, dense_mask = self._block_sparse_mask(L, keys.shape[-2])
        scores = queries @ mx.swapaxes(keys, -1, -2)
        # TODO, uncomment when faster
        # scores = mx.block_masked_mm(
        #   queries,
        #   mx.swapaxes(keys, -1, -2),
        #   mask_out=block_mask,
        #   block_size=self.blocksparse_block_size,
        # )

        if mask is not None:
            scores = scores + mask
        scores = scores + mx.where(
            dense_mask, mx.array(0, scores.dtype), mx.array(-float("inf"), scores.dtype)
        )
        scores = mx.softmax(scores, axis=-1, precise=True)

        output = scores @ values
        # TODO, uncomment when faster
        # output = mx.block_masked_mm(
        #    scores, values, mask_lhs=block_mask, block_size=self.blocksparse_block_size
        # )
        return mx.reshape(output, (B, self.n_heads, L, -1))

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        B, L, D = x.shape

        qkv = self.query_key_value(x)
        qkv = qkv.reshape(B, L, -1, self.n_q_per_kv + 2, self.head_dim)
        queries = qkv[..., :-2, :].flatten(-3, -2)
        keys = qkv[..., -2, :]
        values = qkv[..., -1, :]

        # Prepare the queries, keys and values for the attention computation
        queries = queries.transpose(0, 2, 1, 3)
        keys = keys.transpose(0, 2, 1, 3)
        values = values.transpose(0, 2, 1, 3)

        if cache is not None:
            queries = self.rope(queries, offset=cache.offset)
            keys = self.rope(keys, offset=cache.offset)
            keys, values = cache.update_and_fetch(keys, values)
        else:
            queries = self.rope(queries)
            keys = self.rope(keys)

        if self.block_sparse:
            output = self._block_sparse_attention(
                queries, keys, values, scale=self.scale, mask=mask
            )
        else:
            output = scaled_dot_product_attention(
                queries, keys, values, cache=cache, scale=self.scale, mask=mask
            )
        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)
        return self.dense(output)


class MLP(nn.Module):
    def __init__(self, args):
        super().__init__()
        dim = args.hidden_size
        hidden_dim = args.ff_intermediate_size
        self.gegelu_limit = args.gegelu_limit
        self.up_proj = nn.Linear(dim, 2 * hidden_dim)
        self.down_proj = nn.Linear(hidden_dim, dim)

    def __call__(self, x) -> mx.array:
        x = self.up_proj(x)
        return self.down_proj(gegelu(x, self.gegelu_limit))


class TransformerBlock(nn.Module):
    def __init__(self, args: ModelArgs, layer_idx):
        super().__init__()
        self.num_attention_heads = args.num_attention_heads
        self.hidden_size = args.hidden_size
        self.self_attn = Attention(args, layer_idx)
        self.mlp = MLP(args)
        self.input_layernorm = nn.LayerNorm(
            args.hidden_size, eps=args.layer_norm_epsilon
        )
        self.post_attention_layernorm = nn.LayerNorm(
            args.hidden_size,
            eps=args.layer_norm_epsilon,
        )
        self.args = args

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        r = self.self_attn(self.input_layernorm(x), mask, cache)
        h = x + r
        r = self.mlp(self.post_attention_layernorm(h))
        out = h + r
        return out


class Phi3Model(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.vocab_size = args.vocab_size
        self.num_hidden_layers = args.num_hidden_layers
        assert self.vocab_size > 0
        self.mup_embedding_multiplier = args.mup_embedding_multiplier
        self.embed_tokens = nn.Embedding(args.vocab_size, args.hidden_size)
        self.layers = [
            TransformerBlock(args=args, layer_idx=l)
            for l in range(args.num_hidden_layers)
        ]
        self.final_layernorm = nn.LayerNorm(
            args.hidden_size, eps=args.layer_norm_epsilon
        )

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        h = self.embed_tokens(inputs)
        if self.mup_embedding_multiplier:
            h = self.mup_embedding_multiplier * h

        if mask is None:
            mask = create_attention_mask(h, cache)

        if cache is None:
            cache = [None] * len(self.layers)

        for layer, c in zip(self.layers, cache):
            h = layer(h, mask, c)

        return self.final_layernorm(h)


class Model(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.model_type = args.model_type
        self.model = Phi3Model(args)
        self.args = args
        self.mup_width_multiplier = args.mup_width_multiplier
        self._dummy_tokenizer_ids = mx.array(
            [100256, 100258, 100259, 100260, 100264, 100265]
            + list(range(100267, 100352))
        )

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        out = self.model(inputs, mask, cache)
        out = self.model.embed_tokens.as_linear(out)
        if self.mup_width_multiplier:
            out = out / self.mup_width_multiplier
        out[self._dummy_tokenizer_ids] = -float("inf")
        return out

    @property
    def layers(self):
        return self.model.layers

    def sanitize(self, weights):
        # Remove unused precomputed rotary freqs
        return {
            k: v for k, v in weights.items() if "self_attn.rotary_emb.inv_freq" not in k
        }

>>>> llms/mlx_lm/models/nemotron.py
# Copyright © 2024 Apple Inc.

from dataclasses import dataclass
from functools import partial
from typing import Any, Dict, Optional, Union

import mlx.core as mx
import mlx.nn as nn

from .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention


@dataclass
class ModelArgs(BaseModelArgs):
    model_type: str
    hidden_size: int
    hidden_act: str
    num_hidden_layers: int
    intermediate_size: int
    num_attention_heads: int
    norm_eps: float
    vocab_size: int
    num_key_value_heads: int
    head_dim: Optional[int] = None
    max_position_embeddings: Optional[int] = None
    attention_bias: bool = False
    mlp_bias: bool = False
    partial_rotary_factor: float = 0.5
    rope_theta: float = 10000.0
    rope_traditional: bool = False
    rope_scaling: Optional[Dict[str, Union[float, str]]] = None
    tie_word_embeddings: bool = False

    def __post_init__(self):
        if self.rope_scaling:
            if not "factor" in self.rope_scaling:
                raise ValueError(f"rope_scaling must contain 'factor'")
            rope_type = self.rope_scaling.get("type") or self.rope_scaling.get(
                "rope_type"
            )
            if rope_type is None:
                raise ValueError(
                    f"rope_scaling must contain either 'type' or 'rope_type'"
                )
            if rope_type not in ["linear"]:
                raise ValueError("rope_scaling 'type' currently only supports 'linear'")


@partial(mx.compile, shapeless=True)
def relu_squared(x):
    return nn.relu(x).square()


class NemotronLayerNorm1P(nn.LayerNorm):
    def __call__(self, x):
        weight = self.weight + 1 if "weight" in self else None
        bias = self.bias if "bias" in self else None
        return mx.fast.layer_norm(x, weight, bias, self.eps)


class Attention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        dim = args.hidden_size
        self.n_heads = n_heads = args.num_attention_heads
        self.n_kv_heads = n_kv_heads = args.num_key_value_heads

        self.head_dim = head_dim = args.head_dim or args.hidden_size // n_heads
        self.partial_rotary_factor = args.partial_rotary_factor

        self.scale = head_dim**-0.5
        if hasattr(args, "attention_bias"):
            attention_bias = args.attention_bias
        else:
            attention_bias = False

        self.q_proj = nn.Linear(dim, n_heads * head_dim, bias=attention_bias)
        self.k_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=attention_bias)
        self.v_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=attention_bias)
        self.o_proj = nn.Linear(n_heads * head_dim, dim, bias=attention_bias)

        rope_scale = 1.0
        if args.rope_scaling and args.rope_scaling["type"] == "linear":
            assert isinstance(args.rope_scaling["factor"], float)
            rope_scale = 1 / args.rope_scaling["factor"]
        self.rope = nn.RoPE(
            int(self.partial_rotary_factor * self.head_dim),
            base=args.rope_theta,
            scale=rope_scale,
        )

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        B, L, _ = x.shape

        queries, keys, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)

        # Prepare the queries, keys and values for the attention computation
        queries = queries.reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)
        keys = keys.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)
        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)

        if cache is not None:
            queries = self.rope(queries, offset=cache.offset)
            keys = self.rope(keys, offset=cache.offset)
            keys, values = cache.update_and_fetch(keys, values)
        else:
            queries = self.rope(queries)
            keys = self.rope(keys)

        output = scaled_dot_product_attention(
            queries, keys, values, cache=cache, scale=self.scale, mask=mask
        )
        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)
        return self.o_proj(output)


class MLP(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        dim = args.hidden_size
        hidden_dim = args.intermediate_size
        mlp_bias = args.mlp_bias

        self.down_proj = nn.Linear(hidden_dim, dim, bias=mlp_bias)
        self.up_proj = nn.Linear(dim, hidden_dim, bias=mlp_bias)

    def __call__(self, x) -> mx.array:
        return self.down_proj(relu_squared(self.up_proj(x)))


class TransformerBlock(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.num_attention_heads = args.num_attention_heads
        self.hidden_size = args.hidden_size
        self.self_attn = Attention(args)
        self.mlp = MLP(args)
        self.input_layernorm = NemotronLayerNorm1P(args.hidden_size, eps=args.norm_eps)
        self.post_attention_layernorm = NemotronLayerNorm1P(
            args.hidden_size, eps=args.norm_eps
        )

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        r = self.self_attn(self.input_layernorm(x), mask, cache)
        h = x + r
        r = self.mlp(self.post_attention_layernorm(h))
        out = h + r
        return out


class NemotronModel(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.vocab_size = args.vocab_size
        self.num_hidden_layers = args.num_hidden_layers
        assert self.vocab_size > 0
        self.embed_tokens = nn.Embedding(args.vocab_size, args.hidden_size)
        self.layers = [
            TransformerBlock(args=args) for _ in range(args.num_hidden_layers)
        ]
        self.norm = NemotronLayerNorm1P(args.hidden_size, eps=args.norm_eps)

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        h = self.embed_tokens(inputs)

        if mask is None:
            mask = create_attention_mask(h, cache)

        if cache is None:
            cache = [None] * len(self.layers)

        for layer, c in zip(self.layers, cache):
            h = layer(h, mask, cache=c)

        return self.norm(h)


class Model(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.model_type = args.model_type
        self.model = NemotronModel(args)
        if not args.tie_word_embeddings:
            self.lm_head = nn.Linear(args.hidden_size, args.vocab_size, bias=False)

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        out = self.model(inputs, mask, cache)
        if self.args.tie_word_embeddings:
            out = self.model.embed_tokens.as_linear(out)
        else:
            out = self.lm_head(out)
        return out

    @property
    def layers(self):
        return self.model.layers

>>>> llms/mlx_lm/models/minicpm.py
# Copyright © 2023-2025 Apple Inc.

from dataclasses import dataclass
from typing import Any, Dict, Optional, Tuple, Union

import mlx.core as mx
import mlx.nn as nn
import numpy as np

from .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention


@dataclass
class ModelArgs(BaseModelArgs):
    model_type: str
    hidden_size: int
    dim_model_base: int
    num_hidden_layers: int
    intermediate_size: int
    num_attention_heads: int
    rms_norm_eps: float
    vocab_size: int
    num_key_value_heads: int
    scale_depth: float
    scale_emb: float
    rope_theta: float = 1000000.0
    rope_traditional: bool = False
    rope_scaling: Optional[Dict[str, Union[str, float]]] = None
    tie_word_embeddings: bool = False


class MLP(nn.Module):
    def __init__(self, args):
        super().__init__()
        self.gate_proj = nn.Linear(args.hidden_size, args.intermediate_size, bias=False)
        self.up_proj = nn.Linear(args.hidden_size, args.intermediate_size, bias=False)
        self.down_proj = nn.Linear(args.intermediate_size, args.hidden_size, bias=False)

    def __call__(self, x):
        return self.down_proj(nn.silu(self.gate_proj(x)) * self.up_proj(x))


class Attention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args

        self.hidden_size = args.hidden_size
        self.num_heads = n_heads = args.num_attention_heads
        self.rope_theta = args.rope_theta

        self.head_dim = head_dim = args.hidden_size // n_heads
        self.scale = head_dim**-0.5

        self.num_key_value_heads = args.num_key_value_heads
        self.num_key_value_groups = self.num_heads // self.num_key_value_heads

        self.q_proj = nn.Linear(
            self.hidden_size, self.num_heads * self.head_dim, bias=False
        )
        self.k_proj = nn.Linear(
            self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False
        )
        self.v_proj = nn.Linear(
            self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False
        )
        self.o_proj = nn.Linear(
            self.num_heads * self.head_dim, self.hidden_size, bias=False
        )

        rope_scale = (
            1 / args.rope_scaling["factor"]
            if args.rope_scaling is not None and args.rope_scaling["type"] == "linear"
            else 1
        )

        self.rope = nn.RoPE(
            dims=self.head_dim,
            traditional=args.rope_traditional,
            base=self.rope_theta,
            scale=rope_scale,
        )

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ):
        B, L, _ = x.shape

        queries, keys, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)

        queries = queries.reshape(B, L, self.num_heads, -1).transpose(0, 2, 1, 3)
        keys = keys.reshape(B, L, self.num_key_value_heads, -1).transpose(0, 2, 1, 3)
        values = values.reshape(B, L, self.num_key_value_heads, -1).transpose(
            0, 2, 1, 3
        )

        if cache is not None:
            queries = self.rope(queries, offset=cache.offset)
            keys = self.rope(keys, offset=cache.offset)
            keys, values = cache.update_and_fetch(keys, values)
        else:
            queries = self.rope(queries)
            keys = self.rope(keys)

        attn_output = scaled_dot_product_attention(
            queries, keys, values, cache=cache, scale=self.scale, mask=mask
        )

        attn_output = attn_output.transpose(0, 2, 1, 3).reshape(B, L, -1)

        return self.o_proj(attn_output)


class DecoderLayer(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.hidden_size = args.hidden_size
        self.num_hidden_layers = args.num_hidden_layers

        self.self_attn = Attention(args)
        self.mlp = MLP(args)
        self.input_layernorm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)
        self.post_attention_layernorm = nn.RMSNorm(
            args.hidden_size, eps=args.rms_norm_eps
        )

        self.scale_depth = args.scale_depth
        self.num_hidden_layers = args.num_hidden_layers

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        r = self.self_attn(self.input_layernorm(x), mask, cache)
        h = x + r * (self.scale_depth / np.sqrt(self.num_hidden_layers))
        r = self.mlp(self.post_attention_layernorm(h))
        out = h + r * (self.scale_depth / np.sqrt(self.num_hidden_layers))
        return out


class MiniCPMModel(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.vocab_size = args.vocab_size
        assert self.vocab_size > 0

        self.embed_tokens = nn.Embedding(args.vocab_size, args.hidden_size)
        self.layers = [DecoderLayer(args) for _ in range(args.num_hidden_layers)]
        self.norm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        h = self.embed_tokens(inputs) * self.args.scale_emb

        if mask is None:
            mask = create_attention_mask(h, cache)

        if cache is None:
            cache = [None] * len(self.layers)

        for layer, c in zip(self.layers, cache):
            h = layer(h, mask, c)

        return self.norm(h)


class Model(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.model_type = args.model_type
        self.model = MiniCPMModel(args)

        if not self.args.tie_word_embeddings:
            self.lm_head = nn.Linear(args.hidden_size, args.vocab_size, bias=False)

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        out = self.model(inputs, mask, cache)

        if not self.args.tie_word_embeddings:
            out = self.lm_head(out / (self.args.hidden_size / self.args.dim_model_base))
        else:
            out = out @ self.model.embed_tokens.weight.T

        return out

    def sanitize(self, weights):
        if "lm_head.weight" not in weights:
            weights["lm_head.weight"] = weights["model.embed_tokens.weight"]
        return weights

    @property
    def layers(self):
        return self.model.layers

>>>> llms/mlx_lm/models/stablelm.py
# Copyright © 2023-2024 Apple Inc.

import math
from dataclasses import dataclass

import mlx.core as mx
import mlx.nn as nn

from .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention


@dataclass
class ModelArgs(BaseModelArgs):
    model_type: str
    vocab_size: int
    hidden_size: int
    num_attention_heads: int
    num_hidden_layers: int
    num_key_value_heads: int
    intermediate_size: int
    rope_theta: float
    use_qkv_bias: bool
    partial_rotary_factor: float
    layer_norm_eps: float
    use_parallel_residual: bool = False
    qk_layernorm: bool = False


class LayerNormPerHead(nn.Module):

    def __init__(self, head_dim, num_heads, eps):
        super().__init__()
        self.norms = [
            nn.LayerNorm(head_dim, eps=eps, bias=False) for _ in range(num_heads)
        ]
        self.eps = eps

    def __call__(self, x):
        w = mx.stack([n.weight for n in self.norms])
        return w * mx.fast.layer_norm(x, None, None, self.eps)


class Attention(nn.Module):
    def __init__(self, config: ModelArgs):
        super().__init__()

        self.hidden_size = config.hidden_size
        self.num_heads = config.num_attention_heads
        self.head_dim = self.hidden_size // self.num_heads
        self.num_key_value_heads = config.num_key_value_heads
        self.rope_theta = config.rope_theta
        self.partial_rotary_factor = config.partial_rotary_factor

        if (self.head_dim * self.num_heads) != self.hidden_size:
            raise ValueError(
                f"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}"
                f" and `num_heads`: {self.num_heads})."
            )

        self.q_proj = nn.Linear(
            self.hidden_size, self.num_heads * self.head_dim, bias=config.use_qkv_bias
        )
        self.k_proj = nn.Linear(
            self.hidden_size,
            self.num_key_value_heads * self.head_dim,
            bias=config.use_qkv_bias,
        )
        self.v_proj = nn.Linear(
            self.hidden_size,
            self.num_key_value_heads * self.head_dim,
            bias=config.use_qkv_bias,
        )
        self.o_proj = nn.Linear(
            self.num_heads * self.head_dim, self.hidden_size, bias=False
        )

        self.rope = nn.RoPE(
            int(self.partial_rotary_factor * self.head_dim),
            traditional=False,
            base=self.rope_theta,
        )

        self.qk_layernorm = config.qk_layernorm
        if self.qk_layernorm:
            self.q_layernorm = LayerNormPerHead(
                self.head_dim, self.num_heads, eps=config.layer_norm_eps
            )
            self.k_layernorm = LayerNormPerHead(
                self.head_dim, self.num_key_value_heads, eps=config.layer_norm_eps
            )

    def __call__(self, x, mask=None, cache=None):
        queries, keys, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)

        # Extract some shapes
        B, L, D = queries.shape

        queries = queries.reshape(B, L, self.num_heads, -1)
        keys = keys.reshape(B, L, self.num_key_value_heads, -1)
        if self.qk_layernorm:
            queries = self.q_layernorm(queries)
            keys = self.k_layernorm(keys)
        queries = queries.transpose(0, 2, 1, 3)
        keys = keys.transpose(0, 2, 1, 3)
        values = values.reshape(B, L, self.num_key_value_heads, -1).transpose(
            0, 2, 1, 3
        )

        # Add RoPE to the queries and keys and combine them with the cache
        if cache is not None:
            queries = self.rope(queries, offset=cache.offset)
            keys = self.rope(keys, offset=cache.offset)
            keys, values = cache.update_and_fetch(keys, values)
        else:
            queries = self.rope(queries)
            keys = self.rope(keys)

        queries = queries.astype(mx.float32)
        keys = keys.astype(mx.float32)

        # Finally perform the attention computation
        scale = math.sqrt(1 / queries.shape[-1])
        output = scaled_dot_product_attention(
            queries, keys, values, cache=cache, scale=scale, mask=mask
        ).astype(values.dtype)
        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)
        return self.o_proj(output)


class MLP(nn.Module):
    def __init__(self, dim, hidden_dim):
        super().__init__()
        self.gate_proj = nn.Linear(dim, hidden_dim, bias=False)
        self.down_proj = nn.Linear(hidden_dim, dim, bias=False)
        self.up_proj = nn.Linear(dim, hidden_dim, bias=False)

    def __call__(self, x) -> mx.array:
        return self.down_proj(nn.silu(self.gate_proj(x)) * self.up_proj(x))


class DecoderLayer(nn.Module):
    def __init__(self, config: ModelArgs):
        super().__init__()
        self.self_attn = Attention(config=config)
        self.mlp = MLP(config.hidden_size, config.intermediate_size)
        self.input_layernorm = nn.LayerNorm(
            config.hidden_size,
            eps=config.layer_norm_eps,
        )
        self.use_parallel_residual = config.use_parallel_residual
        if not self.use_parallel_residual:
            self.post_attention_layernorm = nn.LayerNorm(
                config.hidden_size,
                eps=config.layer_norm_eps,
            )

    def __call__(self, x, mask, cache):
        h = self.input_layernorm(x)
        r = self.self_attn(h, mask, cache)

        if self.use_parallel_residual:
            out = x + r + self.mlp(h)
        else:
            h = x + r
            r = self.mlp(self.post_attention_layernorm(h))
            out = h + r
        return out


class StableLM(nn.Module):
    def __init__(self, config: ModelArgs):
        super().__init__()
        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)
        self.layers = [DecoderLayer(config) for i in range(config.num_hidden_layers)]
        self.norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)

    def __call__(self, x, mask, cache):
        x = self.embed_tokens(x)
        if cache is None:
            cache = [None] * len(self.layers)

        for layer, c in zip(self.layers, cache):
            x = layer(x, mask, cache=c)

        return self.norm(x)


class Model(nn.Module):
    def __init__(self, config: ModelArgs):
        super().__init__()
        self.model_type = config.model_type
        self.model = StableLM(config)
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
        self.args = config

    def __call__(
        self,
        x: mx.array,
        mask: mx.array = None,
        cache=None,
    ) -> mx.array:

        if mask is None:
            mask = create_attention_mask(x, cache)

        y = self.model(x, mask, cache)
        return self.lm_head(y)

    @property
    def layers(self):
        return self.model.layers

>>>> llms/mlx_lm/models/phixtral.py
# Copyright © 2023-2024 Apple Inc.

import inspect
import math
from dataclasses import dataclass
from typing import Tuple

import mlx.core as mx
import mlx.nn as nn

from .base import create_attention_mask, scaled_dot_product_attention
from .switch_layers import SwitchMLP


@dataclass
class ModelArgs:
    model_type: str
    num_vocab: int = 51200
    model_dim: int = 2560
    num_heads: int = 32
    num_layers: int = 32
    rotary_dim: int = 32
    num_experts_per_tok: int = 2
    num_local_experts: int = 4

    @classmethod
    def from_dict(cls, params):
        return cls(
            **{
                k: v
                for k, v in params.items()
                if k in inspect.signature(cls).parameters
            }
        )


class RoPEAttention(nn.Module):
    def __init__(self, dims: int, num_heads: int, rotary_dim: int):
        super().__init__()

        self.num_heads = num_heads

        self.rope = nn.RoPE(rotary_dim, traditional=False)
        self.Wqkv = nn.Linear(dims, 3 * dims)
        self.out_proj = nn.Linear(dims, dims)

    def __call__(self, x, mask=None, cache=None):
        qkv = self.Wqkv(x)
        queries, keys, values = mx.split(qkv, 3, axis=-1)

        # Extract some shapes
        num_heads = self.num_heads
        B, L, D = queries.shape

        # Prepare the queries, keys and values for the attention computation
        queries = queries.reshape(B, L, num_heads, -1).transpose(0, 2, 1, 3)
        keys = keys.reshape(B, L, num_heads, -1).transpose(0, 2, 1, 3)
        values = values.reshape(B, L, num_heads, -1).transpose(0, 2, 1, 3)

        # Add RoPE to the queries and keys and combine them with the cache
        if cache is not None:
            queries = self.rope(queries, offset=cache.offset)
            keys = self.rope(keys, offset=cache.offset)
            keys, values = cache.update_and_fetch(keys, values)
        else:
            queries = self.rope(queries)
            keys = self.rope(keys)

        queries = queries.astype(mx.float32)

        # Finally perform the attention computation
        scale = math.sqrt(1 / queries.shape[-1])

        output = scaled_dot_product_attention(
            queries.astype(mx.float32),
            keys,
            values,
            cache=cache,
            scale=scale,
            mask=mask,
        ).astype(values.dtype)
        output = output.moveaxis(2, 1).reshape(B, L, -1)

        return self.out_proj(output)


class MOE(nn.Module):
    def __init__(self, args: ModelArgs, dim: int, hidden_dim: int):
        super().__init__()
        self.dim = dim
        self.hidden_dim = hidden_dim
        self.num_experts = args.num_local_experts
        self.num_experts_per_tok = args.num_experts_per_tok
        self.switch_mlp = SwitchMLP(
            self.dim, self.hidden_dim, self.num_experts, bias=True
        )
        self.gate = nn.Linear(args.model_dim, self.num_experts, bias=False)

    def __call__(self, x: mx.array) -> mx.array:
        gates = self.gate(x)

        k = self.num_experts_per_tok
        inds = mx.stop_gradient(mx.argpartition(-gates, kth=k - 1, axis=-1))[..., :k]
        scores = mx.take_along_axis(gates, inds, axis=-1)
        scores = mx.softmax(scores, axis=-1, precise=True)

        y = self.switch_mlp(x, inds)
        y = (y * scores[..., None]).sum(axis=-2)

        return y


class ParallelBlock(nn.Module):
    def __init__(self, config: ModelArgs):
        super().__init__()
        dims = config.model_dim
        mlp_dims = dims * 4
        self.mixer = RoPEAttention(dims, config.num_heads, config.rotary_dim)
        self.ln = nn.LayerNorm(dims)
        self.moe = MOE(config, dims, mlp_dims)

    def __call__(self, x, mask, cache):
        h = self.ln(x)
        attn_h = self.mixer(h, mask, cache)
        ff_h = self.moe(h)
        return attn_h + ff_h + x


class TransformerDecoder(nn.Module):
    def __init__(self, config: ModelArgs):
        super().__init__()
        self.embd = Embd(config)
        self.h = [ParallelBlock(config) for i in range(config.num_layers)]

    def __call__(self, x, mask, cache):
        x = self.embd(x)
        if cache is None:
            cache = [None] * len(self.h)

        for layer, c in zip(self.h, cache):
            x = layer(x, mask, c)
        return x


class Embd(nn.Module):
    def __init__(self, config: ModelArgs):
        super().__init__()
        self.wte = nn.Embedding(config.num_vocab, config.model_dim)

    def __call__(self, x):
        return self.wte(x)


class OutputHead(nn.Module):
    def __init__(self, config: ModelArgs) -> None:
        super().__init__()
        self.ln = nn.LayerNorm(config.model_dim)
        self.linear = nn.Linear(config.model_dim, config.num_vocab)

    def __call__(self, inputs):
        return self.linear(self.ln(inputs))


class Model(nn.Module):
    def __init__(self, config: ModelArgs):
        super().__init__()
        self.model_type = config.model_type
        self.transformer = TransformerDecoder(config)
        self.lm_head = OutputHead(config)
        self.args = config

    def __call__(
        self,
        x: mx.array,
        mask: mx.array = None,
        cache=None,
    ) -> mx.array:

        if mask is None:
            mask = create_attention_mask(x, cache)

        y = self.transformer(x, mask, cache)
        return self.lm_head(y)

    def sanitize(self, weights):
        if "transformer.h.0.moe.mlp.0.fc1.weight" not in weights:
            return weights
        for l in range(self.args.num_layers):
            prefix = f"transformer.h.{l}"
            for n in ["fc1", "fc2"]:
                for k in ["weight", "scales", "biases", "bias"]:
                    if f"{prefix}.moe.mlp.0.{n}.{k}" in weights:
                        to_join = [
                            weights.pop(f"{prefix}.moe.mlp.{e}.{n}.{k}")
                            for e in range(self.args.num_local_experts)
                        ]
                        weights[f"{prefix}.moe.switch_mlp.{n}.{k}"] = mx.stack(to_join)
        return weights

    @property
    def layers(self):
        return self.transformer.h

>>>> llms/mlx_lm/models/starcoder2.py
# Copyright © 2023-2024 Apple Inc.

from dataclasses import dataclass
from typing import Any, Optional

import mlx.core as mx
import mlx.nn as nn

from .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention


@dataclass
class ModelArgs(BaseModelArgs):
    model_type: str
    hidden_size: int
    num_hidden_layers: int
    intermediate_size: int
    num_attention_heads: int
    num_key_value_heads: int
    norm_epsilon: float = 1e-5
    vocab_size: int = 49152
    rope_theta: float = 100000
    tie_word_embeddings: bool = True


class Attention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args

        dim = args.hidden_size
        self.n_heads = n_heads = args.num_attention_heads
        self.n_kv_heads = n_kv_heads = args.num_key_value_heads

        head_dim = args.hidden_size // args.num_attention_heads
        self.scale = head_dim**-0.5

        self.q_proj = nn.Linear(dim, n_heads * head_dim, bias=True)
        self.k_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=True)
        self.v_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=True)
        self.o_proj = nn.Linear(n_heads * head_dim, dim, bias=True)
        self.rope = nn.RoPE(head_dim, traditional=False, base=args.rope_theta)

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        B, L, D = x.shape

        queries, keys, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)

        # Prepare the queries, keys and values for the attention computation
        queries = queries.reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)
        keys = keys.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)
        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)

        if cache is not None:
            queries = self.rope(queries, offset=cache.offset)
            keys = self.rope(keys, offset=cache.offset)
            keys, values = cache.update_and_fetch(keys, values)
        else:
            queries = self.rope(queries)
            keys = self.rope(keys)

        output = scaled_dot_product_attention(
            queries, keys, values, cache=cache, scale=self.scale, mask=mask
        )

        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)
        return self.o_proj(output)


class MLP(nn.Module):
    def __init__(self, dim, hidden_dim):
        super().__init__()
        self.c_fc = nn.Linear(dim, hidden_dim, bias=True)
        self.c_proj = nn.Linear(hidden_dim, dim, bias=True)

    def __call__(self, x):
        return self.c_proj(nn.gelu(self.c_fc(x)))


class TransformerBlock(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.hidden_size = args.hidden_size
        self.n_heads = args.num_attention_heads

        self.self_attn = Attention(args)
        self.mlp = MLP(args.hidden_size, args.intermediate_size)
        self.input_layernorm = nn.LayerNorm(args.hidden_size, eps=args.norm_epsilon)
        self.post_attention_layernorm = nn.LayerNorm(
            args.hidden_size, eps=args.norm_epsilon
        )
        self.args = args

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        r = self.self_attn(self.input_layernorm(x), mask, cache)
        h = x + r
        r = self.mlp(self.post_attention_layernorm(h))
        out = h + r
        return out


class Starcoder2Model(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.vocab_size = args.vocab_size
        self.num_hidden_layers = args.num_hidden_layers
        assert self.vocab_size > 0
        self.embed_tokens = nn.Embedding(args.vocab_size, args.hidden_size)
        self.layers = [
            TransformerBlock(args=args) for _ in range(args.num_hidden_layers)
        ]
        self.norm = nn.LayerNorm(args.hidden_size, eps=args.norm_epsilon)

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        h = self.embed_tokens(inputs)

        if mask is None:
            mask = create_attention_mask(h, cache)

        if cache is None:
            cache = [None] * len(self.layers)

        for layer, c in zip(self.layers, cache):
            h = layer(h, mask, c)

        return self.norm(h)


class Model(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.model_type = args.model_type
        self.model = Starcoder2Model(args)
        if not args.tie_word_embeddings:
            self.lm_head = nn.Linear(args.hidden_size, args.vocab_size, bias=False)

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        out = self.model(inputs, mask, cache)
        if self.args.tie_word_embeddings:
            out = self.model.embed_tokens.as_linear(out)
        else:
            out = self.lm_head(out)
        return out

    @property
    def layers(self):
        return self.model.layers

>>>> llms/mlx_lm/models/cache.py
# Copyright © 2023-2024 Apple Inc.

from typing import Any, Dict, List, Optional

import mlx.core as mx
import mlx.nn as nn
from mlx.utils import tree_flatten, tree_map, tree_unflatten


def make_prompt_cache(
    model: nn.Module,
    max_kv_size: Optional[int] = None,
) -> List[Any]:
    """
    Construct the model's cache for use when cgeneration.

    This function will defer the cache construction to the model if it has a
    ``make_cache`` method, otherwise it will make a default KV cache.

    Args:
        model (nn.Module): The language model.
        max_kv_size (Optional[int]): If provided and the model does not have a
            ``make_cache`` method, a ``RotatingKVCache`` is used with a maximum
            size of ``max_kv_size``
    """
    if hasattr(model, "make_cache"):
        return model.make_cache()

    num_layers = len(model.layers)
    if max_kv_size is not None:
        return [
            RotatingKVCache(max_size=max_kv_size, keep=4) for _ in range(num_layers)
        ]
    else:
        return [KVCache() for _ in range(num_layers)]


def save_prompt_cache(file_name: str, cache: List[Any], metadata: Dict[str, str] = {}):
    """
    Save a pre-computed prompt cache to a file.

    Args:
        file_name (str): The ``.safetensors`` file name.
        cache (List[Any]): The model state.
        metadata (Dict[str, str]): Optional metadata to save along with model
            state.
    """
    cache_data = [c.state for c in cache]
    cache_info = [c.meta_state for c in cache]
    cache_data = dict(tree_flatten(cache_data))
    cache_classes = [type(c).__name__ for c in cache]
    cache_metadata = [cache_info, metadata, cache_classes]
    cache_metadata = dict(tree_flatten(cache_metadata))
    mx.save_safetensors(file_name, cache_data, cache_metadata)


def load_prompt_cache(file_name, return_metadata=False):
    """
    Load a prompt cache from a file.

    Args:
        file_name (str): The ``.safetensors`` file name.
        return_metadata (bool): Whether or not to return metadata.
            Default: ``False``.

    Returns:
        List[Any] or Tuple[List[Any], Dict[str, str]]: The prompt cache and
            the metadata if requested.
    """
    arrays, cache_metadata = mx.load(file_name, return_metadata=True)
    arrays = tree_unflatten(list(arrays.items()))
    cache_metadata = tree_unflatten(list(cache_metadata.items()))
    info, metadata, classes = cache_metadata
    cache = [globals()[c]() for c in classes]
    for c, state, meta_state in zip(cache, arrays, info):
        c.state = state
        c.meta_state = meta_state
    if return_metadata:
        return cache, metadata
    return cache


def can_trim_prompt_cache(cache: List[Any]) -> bool:
    """
    Check if model's cache can be trimmed.
    """
    return all(c.is_trimmable() for c in cache)


def trim_prompt_cache(cache: List[Any], num_tokens: int) -> List[Any]:
    """
    Trim the model's cache by the given number of tokens.

    This function will trim the cache if possible (in-place) and return the
    number of tokens that were trimmed.

    Args:
        cache (List[Any]): The model's cache.
        num_tokens (int): The number of tokens to trim.

    Returns:
        (int): The number of tokens that were trimmed.
    """
    if not can_trim_prompt_cache(cache) or len(cache) == 0:
        return 0
    return [c.trim(num_tokens) for c in cache][0]


class _BaseCache:
    @property
    def state(self):
        return []

    @state.setter
    def state(self, v):
        if v is not None and v:
            raise ValueError("This cache has no state but a state was set.")

    @property
    def meta_state(self):
        return ""

    @meta_state.setter
    def meta_state(self, v):
        if v is not None and v:
            raise ValueError("This cache has no meta_state but a meta_state was set.")

    def is_trimmable(self):
        return False


class QuantizedKVCache(_BaseCache):
    def __init__(self, group_size: int = 64, bits: int = 8):
        self.keys = None
        self.values = None
        self.offset = 0
        self.step = 256
        self.group_size = group_size
        self.bits = bits

    def update_and_fetch(self, keys, values):
        B, n_kv_heads, num_steps, k_head_dim = keys.shape
        v_head_dim = values.shape[-1]
        prev = self.offset

        if self.keys is None or (prev + num_steps) > self.keys[0].shape[-2]:
            el_per_int = 8 * mx.uint32.size // self.bits
            new_steps = (self.step + num_steps - 1) // self.step * self.step
            shape = (B, n_kv_heads, new_steps)

            def init_quant(dim):
                return (
                    mx.zeros((*shape, dim // el_per_int), dtype=mx.uint32),
                    mx.zeros((*shape, dim // self.group_size), dtype=keys.dtype),
                    mx.zeros((*shape, dim // self.group_size), dtype=keys.dtype),
                )

            def expand_quant(x):
                new_x = mx.zeros((*shape, x.shape[-1]), dtype=x.dtype)
                return mx.concatenate([x, new_x], axis=-2)

            if self.keys is not None:
                if prev % self.step != 0:
                    self.keys, self.values = tree_map(
                        lambda x: x[..., :prev, :], (self.keys, self.values)
                    )

                self.keys, self.values = tree_map(
                    expand_quant, (self.keys, self.values)
                )
            else:
                self.keys, self.values = init_quant(k_head_dim), init_quant(v_head_dim)

        self.offset += num_steps

        keys = mx.quantize(keys, group_size=self.group_size, bits=self.bits)
        values = mx.quantize(values, group_size=self.group_size, bits=self.bits)
        for i in range(len(self.keys)):
            self.keys[i][..., prev : self.offset, :] = keys[i]
            self.values[i][..., prev : self.offset, :] = values[i]

        return tree_map(lambda x: x[..., : self.offset, :], (self.keys, self.values))

    @property
    def state(self):
        if self.offset == self.keys[0].shape[2]:
            return self.keys, self.values
        else:
            return tree_map(
                lambda x: x[..., : self.offset, :], (self.keys, self.values)
            )

    @state.setter
    def state(self, v):
        self.keys, self.values = v

    @property
    def meta_state(self):
        return tuple(map(str, (self.step, self.offset, self.group_size, self.bits)))

    @meta_state.setter
    def meta_state(self, v):
        self.step, self.offset, self.group_size, self.bits = map(int, v)

    def is_trimmable(self):
        return True

    def trim(self, n):
        n = min(self.offset, n)
        self.offset -= n
        return n


class KVCache(_BaseCache):
    def __init__(self):
        self.keys = None
        self.values = None
        self.offset = 0
        self.step = 256

    def update_and_fetch(self, keys, values):
        prev = self.offset
        if self.keys is None or (prev + keys.shape[2]) > self.keys.shape[2]:
            B, n_kv_heads, _, k_head_dim = keys.shape
            v_head_dim = values.shape[3]
            n_steps = (self.step + keys.shape[2] - 1) // self.step
            k_shape = (B, n_kv_heads, n_steps * self.step, k_head_dim)
            v_shape = (B, n_kv_heads, n_steps * self.step, v_head_dim)
            new_k = mx.zeros(k_shape, keys.dtype)
            new_v = mx.zeros(v_shape, values.dtype)
            if self.keys is not None:
                if prev % self.step != 0:
                    self.keys = self.keys[..., :prev, :]
                    self.values = self.values[..., :prev, :]
                self.keys = mx.concatenate([self.keys, new_k], axis=2)
                self.values = mx.concatenate([self.values, new_v], axis=2)
            else:
                self.keys, self.values = new_k, new_v

        self.offset += keys.shape[2]
        self.keys[..., prev : self.offset, :] = keys
        self.values[..., prev : self.offset, :] = values
        return self.keys[..., : self.offset, :], self.values[..., : self.offset, :]

    @property
    def state(self):
        if self.offset == self.keys.shape[2]:
            return self.keys, self.values
        else:
            return (
                self.keys[..., : self.offset, :],
                self.values[..., : self.offset, :],
            )

    @state.setter
    def state(self, v):
        self.keys, self.values = v
        self.offset = self.keys.shape[2]

    def is_trimmable(self):
        return True

    def trim(self, n):
        n = min(self.offset, n)
        self.offset -= n
        return n

    def to_quantized(self, group_size: int = 64, bits: int = 4) -> QuantizedKVCache:
        quant_cache = QuantizedKVCache(group_size=group_size, bits=bits)
        quant_cache.offset = self.offset
        if self.keys is not None:
            quant_cache.keys = mx.quantize(self.keys, group_size=group_size, bits=bits)
            quant_cache.values = mx.quantize(
                self.values, group_size=group_size, bits=bits
            )
        return quant_cache


class RotatingKVCache(_BaseCache):

    def __init__(self, max_size=None, keep=0, step=256):
        self.keep = keep
        self.keys = None
        self.values = None
        self.offset = 0
        self.max_size = max_size
        self.step = step
        self._idx = 0

    def _trim(self, trim_size, v, append=None):
        to_cat = []
        if trim_size > 0:
            to_cat = [v[..., : self.keep, :], v[..., trim_size + self.keep :, :]]
        else:
            to_cat = [v]
        if append is not None:
            to_cat.append(append)
        return mx.concatenate(to_cat, axis=2)

    def _temporal_order(self, v):
        """
        Rearrange the cache into temporal order, slicing off the end if unused.
        """
        if self._idx == v.shape[2]:
            return v
        elif self._idx < self.offset:
            return mx.concatenate(
                [
                    v[..., : self.keep, :],
                    v[..., self._idx :, :],
                    v[..., self.keep : self._idx, :],
                ],
                axis=2,
            )
        else:
            return v[..., : self._idx, :]

    def _update_concat(self, keys, values):
        if self.keys is None:
            self.keys = keys
            self.values = values
        else:
            # Put the keys/values in temporal order to
            # preserve context
            self.keys = self._temporal_order(self.keys)
            self.values = self._temporal_order(self.values)

            # The largest size is self.max_size + S to ensure
            # every token gets at least self.max_size context
            trim_size = self._idx - self.max_size
            self.keys = self._trim(trim_size, self.keys, keys)
            self.values = self._trim(trim_size, self.values, values)
        self.offset += keys.shape[2]
        self._idx = self.keys.shape[2]
        return self.keys, self.values

    def _update_in_place(self, keys, values):
        # May not have hit the max size yet, so potentially
        # keep growing the cache
        B, n_kv_heads, S, k_head_dim = keys.shape
        prev = self.offset
        if self.keys is None or (
            prev >= self.keys.shape[2] and self.keys.shape[2] < self.max_size
        ):
            v_head_dim = values.shape[3]
            new_size = min(self.step, self.max_size - prev)
            k_shape = (B, n_kv_heads, new_size, k_head_dim)
            v_shape = (B, n_kv_heads, new_size, v_head_dim)
            new_k = mx.zeros(k_shape, keys.dtype)
            new_v = mx.zeros(v_shape, values.dtype)
            if self.keys is not None:
                self.keys = mx.concatenate([self.keys, new_k], axis=2)
                self.values = mx.concatenate([self.values, new_v], axis=2)
            else:
                self.keys, self.values = new_k, new_v
            self._idx = prev

        # Trim if needed
        trim_size = self.keys.shape[2] - self.max_size
        if trim_size > 0:
            self.keys = self._trim(trim_size, self.keys)
            self.values = self._trim(trim_size, self.values)
            self._idx = self.max_size

        # Rotate
        if self._idx == self.max_size:
            self._idx = self.keep

        # Assign
        self.keys[..., self._idx : self._idx + S, :] = keys
        self.values[..., self._idx : self._idx + S, :] = values
        self.offset += S
        self._idx += S

        # If the buffer is not full, slice off the end
        if self.offset < self.max_size:
            return self.keys[..., : self.offset, :], self.values[..., : self.offset, :]
        return self.keys, self.values

    def update_and_fetch(self, keys, values):
        if keys.shape[2] == 1:
            return self._update_in_place(keys, values)
        return self._update_concat(keys, values)

    @property
    def state(self):
        if self.offset < self.keys.shape[2]:
            return self.keys[..., : self.offset, :], self.values[..., : self.offset, :]
        else:
            return self.keys, self.values

    @state.setter
    def state(self, v):
        self.keys, self.values = v

    @property
    def meta_state(self):
        return tuple(
            map(str, (self.keep, self.max_size, self.step, self.offset, self._idx))
        )

    @meta_state.setter
    def meta_state(self, v):
        self.keep, self.max_size, self.step, self.offset, self._idx = map(
            int,
            v,
        )

    def is_trimmable(self):
        return self.offset < self.max_size

    def trim(self, n):
        n = min(self.offset, n)
        self.offset -= n
        self._idx -= n
        return n

    def to_quantized(self, group_size: int = 64, bits: int = 4) -> QuantizedKVCache:
        raise NotImplementedError("RotatingKVCache Quantization NYI")


class MambaCache(_BaseCache):
    def __init__(self):
        self.cache = [None, None]

    def __setitem__(self, idx, value):
        self.cache[idx] = value

    def __getitem__(self, idx):
        return self.cache[idx]

    @property
    def state(self):
        return self.cache

    @state.setter
    def state(self, v):
        self.cache = v

>>>> llms/mlx_lm/models/internlm3.py
# Copyright © 2023-2024 Apple Inc.

from dataclasses import dataclass
from typing import Any, Dict, Optional, Tuple, Union

import mlx.core as mx
import mlx.nn as nn

from .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention


@dataclass
class ModelArgs(BaseModelArgs):
    model_type: str
    hidden_size: int
    num_hidden_layers: int
    intermediate_size: int
    num_attention_heads: int
    rms_norm_eps: float
    vocab_size: int
    bias: bool = False
    qkv_bias: bool = False
    max_position_embeddings: int = 32768
    num_key_value_heads: int = None
    rope_theta: float = 10000
    rope_traditional: bool = False
    rope_scaling: Optional[Dict[str, Union[float, str]]] = None
    tie_word_embeddings: bool = False

    def __post_init__(self):
        if self.num_key_value_heads is None:
            self.num_key_value_heads = self.num_attention_heads

        if self.rope_scaling:
            required_keys = {"factor", "rope_type"}
            if not all(key in self.rope_scaling for key in required_keys):
                raise ValueError(f"rope_scaling must contain keys {required_keys}")

            if self.rope_scaling["rope_type"] not in ["linear", "dynamic"]:
                raise ValueError(
                    "rope_scaling 'rope_type' currently only supports 'linear' or 'dynamic"
                )


class DynamicNTKScalingRoPE(nn.Module):
    """Implements the rotary positional encoding with Dynamic NTK scaling."""

    def __init__(
        self,
        dims: int,
        max_position_embeddings: int = 2048,
        traditional: bool = False,
        base: float = 10000,
        scale: float = 1.0,
    ):
        super().__init__()
        self.max_position_embeddings = max_position_embeddings
        self.original_base = base
        self.dims = dims
        self.traditional = traditional
        self.scale = scale

    def extra_repr(self):
        return f"{self.dims}, traditional={self.traditional}, max_position_embeddings={self.max_position_embeddings}, scaling_factor={self.scaling_factor}"

    def __call__(self, x, offset: int = 0):
        seq_len = x.shape[1] + offset
        if seq_len > self.max_position_embeddings:
            base = self.original_base * (
                (self.scale * seq_len / self.max_position_embeddings) - (self.scale - 1)
            ) ** (self.dims / (self.dims - 2))
        else:
            base = self.original_base

        return mx.fast.rope(
            x,
            self.dims,
            traditional=self.traditional,
            base=base,
            scale=self.scale,
            offset=offset,
        )


class Attention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        dim = args.hidden_size
        qkv_bias = args.qkv_bias
        self.n_heads = n_heads = args.num_attention_heads
        self.n_kv_heads = n_kv_heads = args.num_key_value_heads
        self.n_kv_groups = n_heads // args.num_key_value_heads

        self.head_dim = head_dim = args.hidden_size // n_heads
        self.scale = head_dim**-0.5

        self.q_proj = nn.Linear(dim, n_heads * head_dim, bias=qkv_bias)
        self.k_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=qkv_bias)
        self.v_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=qkv_bias)
        self.o_proj = nn.Linear(n_heads * head_dim, dim, bias=qkv_bias)

        rope_scale = (
            1 / args.rope_scaling["factor"]
            if args.rope_scaling is not None
            and args.rope_scaling["rope_type"] == "linear"
            else 2.0
        )

        self.rope = DynamicNTKScalingRoPE(
            head_dim,
            max_position_embeddings=args.max_position_embeddings,
            traditional=args.rope_traditional,
            base=args.rope_theta,
            scale=rope_scale,
        )

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        B, L, D = x.shape

        queries, keys, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)

        # Prepare the queries, keys and values for the attention computation
        queries = queries.reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)
        keys = keys.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)
        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)

        if cache is not None:
            queries = self.rope(queries, offset=cache.offset)
            keys = self.rope(keys, offset=cache.offset)
            keys, values = cache.update_and_fetch(keys, values)
        else:
            queries = self.rope(queries)
            keys = self.rope(keys)

        output = scaled_dot_product_attention(
            queries, keys, values, cache=cache, scale=self.scale, mask=mask
        )

        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)
        return self.o_proj(output)


class MLP(nn.Module):
    def __init__(self, dim, hidden_dim, bias):
        super().__init__()
        self.gate_proj = nn.Linear(dim, hidden_dim, bias=bias)
        self.down_proj = nn.Linear(hidden_dim, dim, bias=bias)
        self.up_proj = nn.Linear(dim, hidden_dim, bias=bias)

    def __call__(self, x) -> mx.array:
        return self.down_proj(nn.silu(self.gate_proj(x)) * self.up_proj(x))


class TransformerBlock(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.self_attn = Attention(args)
        self.mlp = MLP(args.hidden_size, args.intermediate_size, args.bias)
        self.input_layernorm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)
        self.post_attention_layernorm = nn.RMSNorm(
            args.hidden_size, eps=args.rms_norm_eps
        )

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        r = self.self_attn(self.input_layernorm(x), mask, cache)
        h = x + r
        r = self.mlp(self.post_attention_layernorm(h))
        out = h + r
        return out


class InternLM2Model(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        assert args.vocab_size > 0
        self.embed_tokens = nn.Embedding(args.vocab_size, args.hidden_size)
        self.layers = [
            TransformerBlock(args=args) for _ in range(args.num_hidden_layers)
        ]
        self.norm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        h = self.embed_tokens(inputs)

        if mask is None:
            mask = create_attention_mask(h, cache)

        if cache is None:
            cache = [None] * len(self.layers)

        for layer, c in zip(self.layers, cache):
            h = layer(h, mask, cache=c)

        return self.norm(h)


class Model(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.model_type = args.model_type
        self.model = InternLM2Model(args)
        if not args.tie_word_embeddings:
            self.lm_head = nn.Linear(args.hidden_size, args.vocab_size, bias=False)

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        out = self.model(inputs, mask, cache)
        if self.args.tie_word_embeddings:
            out = self.model.embed_tokens.as_linear(out)
        else:
            out = self.lm_head(out)
        return out

    def sanitize(self, weights):
        # Remove unused precomputed rotary freqs
        return {k: v for k, v in weights.items() if "attention.rope.inv_freq" not in k}

    @property
    def layers(self):
        return self.model.layers

>>>> llms/mlx_lm/models/helium.py
# Copyright © 2025 Apple Inc.

from dataclasses import dataclass
from typing import Any, Optional, Tuple

import mlx.core as mx
import mlx.nn as nn

from .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention


@dataclass
class ModelArgs(BaseModelArgs):
    hidden_size: int
    num_hidden_layers: int
    intermediate_size: int
    num_attention_heads: int
    num_key_value_heads: int
    rms_norm_eps: float
    vocab_size: int
    attention_bias: bool
    head_dim: int
    max_position_embeddings: int
    mlp_bias: bool
    model_type: str
    rope_theta: float
    tie_word_embeddings: bool


class HeliumAttention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        dim = args.hidden_size
        self.n_heads = n_heads = args.num_attention_heads
        assert args.num_key_value_heads is not None
        self.n_kv_heads = n_kv_heads = args.num_key_value_heads

        head_dim = args.hidden_size // n_heads
        self.scale = head_dim**-0.5

        self.q_proj = nn.Linear(dim, n_heads * head_dim, bias=args.attention_bias)
        self.k_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=args.attention_bias)
        self.v_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=args.attention_bias)
        self.o_proj = nn.Linear(n_heads * head_dim, dim, bias=False)
        self.rope = nn.RoPE(head_dim, traditional=True, base=args.rope_theta)

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        B, L, D = x.shape

        queries, keys, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)

        # Prepare the queries, keys and values for the attention computation
        queries = queries.reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)
        keys = keys.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)
        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)

        if cache is not None:
            queries = self.rope(queries, offset=cache.offset)
            keys = self.rope(keys, offset=cache.offset)
            keys, values = cache.update_and_fetch(keys, values)
        else:
            queries = self.rope(queries)
            keys = self.rope(keys)

        output = scaled_dot_product_attention(
            queries, keys, values, cache=cache, scale=self.scale, mask=mask
        )
        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)
        return self.o_proj(output)


class HeliumMLP(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.hidden_size = args.hidden_size
        self.intermediate_size = args.intermediate_size

        self.gate_proj = nn.Linear(
            self.hidden_size, self.intermediate_size, bias=args.mlp_bias
        )
        self.up_proj = nn.Linear(
            self.hidden_size, self.intermediate_size, bias=args.mlp_bias
        )
        self.down_proj = nn.Linear(
            self.intermediate_size, self.hidden_size, bias=args.mlp_bias
        )

    def __call__(self, x: mx.array) -> mx.array:
        return self.down_proj(nn.silu(self.gate_proj(x)) * self.up_proj(x))


class HeliumDecoderLayer(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.hidden_size = args.hidden_size

        self.self_attn = HeliumAttention(args)
        self.mlp = HeliumMLP(args)
        self.input_layernorm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)
        self.post_attention_layernorm = nn.RMSNorm(
            args.hidden_size, eps=args.rms_norm_eps
        )

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        r = self.self_attn(self.input_layernorm(x), mask, cache)
        h = x + r
        r = self.mlp(self.post_attention_layernorm(h))
        out = h + r
        return out


class HeliumModel(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.num_hidden_layers = args.num_hidden_layers
        self.vocab_size = args.vocab_size

        assert self.vocab_size > 0
        self.embed_tokens = nn.Embedding(args.vocab_size, args.hidden_size)

        self.layers = [HeliumDecoderLayer(args) for _ in range(args.num_hidden_layers)]

        self.norm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ) -> mx.array:
        h = self.embed_tokens(inputs)

        if mask is None:
            mask = create_attention_mask(h, cache)

        if cache is None:
            cache = [None] * len(self.layers)

        for layer, c in zip(self.layers, cache):
            h = layer(h, mask, c)

        return self.norm(h)


class Model(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.model_type = args.model_type

        self.model = HeliumModel(args)

        self.vocab_size = args.vocab_size
        self.lm_head = nn.Linear(args.hidden_size, args.vocab_size, bias=False)

        if not args.tie_word_embeddings:
            self.lm_head = nn.Linear(args.hidden_size, args.vocab_size, bias=False)

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ) -> mx.array:
        out = self.model(inputs, mask, cache)
        if self.args.tie_word_embeddings:
            out = self.model.embed_tokens.as_linear(out)
        else:
            out = self.lm_head(out)
        return out

    @property
    def layers(self):
        return self.model.layers

>>>> llms/mlx_lm/models/su_rope.py
# Copyright © 2023-2024 Apple Inc.

import math
from typing import List, Union

import mlx.core as mx
import mlx.nn as nn


class SuScaledRotaryEmbedding(nn.Module):
    def __init__(
        self,
        dims: int,
        base: float = 10000.0,
        max_position_embeddings: int = 131072,
        original_max_position_embeddings: int = 4096,
        short_factor: Union[List[float], float] = 1.0,
        long_factor: Union[List[float], float] = 1.0,
        short_mscale: float = None,
        long_mscale: float = None,
    ):
        """
        Phi3Su Scaled Rotary Embedding layer for Phi-3 models.

        Args:
            dims (int): The feature dimensions to be rotated.
            base (int, optional): Base for the exponential scaling.
            max_position_embeddings (int, optional): The maximum sequence
              length that this model was trained with. This is used to determine
              the size of the original RoPE embeddings when using long scaling.
              Default: ``131072``.
            original_max_position_embeddings (int, optional): The maximum
              sequence length that this model was trained with. This is used to
              determine the size of the original RoPE embeddings when using long
              scaling. Default: ``4096``.
            short_factor (float or list[float], optional): List of scaling
              factors for sequences of length lesser than
              ``original_max_position_embeddings``. Default: ``1.0``.
            long_factor (float or list[float], optional): List of scaling
              factors for sequences of length greater than
              ``original_max_position_embeddings``.  Default: ``1.0``.
            short_mscale (float, optional): Scale the input prior to embedding.
            long_mscale (float, optional): Scale the input prior to embedding.
        """
        super().__init__()
        freqs = base ** (mx.arange(0, dims, 2, dtype=mx.float32) / dims)
        self._freqs = mx.array(long_factor, dtype=mx.float32) * freqs
        self.original_max_position_embeddings = original_max_position_embeddings
        self.scale = long_mscale or math.sqrt(
            1
            + math.log(max_position_embeddings / original_max_position_embeddings)
            / math.log(original_max_position_embeddings)
        )
        self.dim = dims

    def __call__(self, x, offset: int = 0):
        x[..., : self.dim] = self.scale * x[..., : self.dim]
        return mx.fast.rope(
            x,
            self.dim,
            traditional=False,
            base=None,
            scale=1.0,
            offset=offset,
            freqs=self._freqs,
        )

>>>> llms/mlx_lm/models/internlm2.py
# Copyright © 2023-2024 Apple Inc.

from dataclasses import dataclass
from typing import Any, Dict, Optional, Tuple, Union

import mlx.core as mx
import mlx.nn as nn

from .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention


@dataclass
class ModelArgs(BaseModelArgs):
    model_type: str
    hidden_size: int
    num_hidden_layers: int
    intermediate_size: int
    num_attention_heads: int
    rms_norm_eps: float
    vocab_size: int
    bias: bool = True
    max_position_embeddings: int = 32768
    num_key_value_heads: int = None
    rope_theta: float = 10000
    rope_traditional: bool = False
    rope_scaling: Optional[Dict[str, Union[float, str]]] = None
    tie_word_embeddings: bool = False

    def __post_init__(self):
        if self.num_key_value_heads is None:
            self.num_key_value_heads = self.num_attention_heads

        if self.rope_scaling:
            required_keys = {"factor", "type"}
            if not all(key in self.rope_scaling for key in required_keys):
                raise ValueError(f"rope_scaling must contain keys {required_keys}")

            if self.rope_scaling["type"] not in ["linear", "dynamic"]:
                raise ValueError(
                    "rope_scaling 'type' currently only supports 'linear' or 'dynamic"
                )


class DynamicNTKScalingRoPE(nn.Module):
    """Implements the rotary positional encoding with Dynamic NTK scaling."""

    def __init__(
        self,
        dims: int,
        max_position_embeddings: int = 2048,
        traditional: bool = False,
        base: float = 10000,
        scale: float = 1.0,
    ):
        super().__init__()
        self.max_position_embeddings = max_position_embeddings
        self.original_base = base
        self.dims = dims
        self.traditional = traditional
        self.scale = scale

    def extra_repr(self):
        return f"{self.dims}, traditional={self.traditional}, max_position_embeddings={self.max_position_embeddings}, scaling_factor={self.scaling_factor}"

    def __call__(self, x, offset: int = 0):
        seq_len = x.shape[1] + offset
        if seq_len > self.max_position_embeddings:
            base = self.original_base * (
                (self.scale * seq_len / self.max_position_embeddings) - (self.scale - 1)
            ) ** (self.dims / (self.dims - 2))
        else:
            base = self.original_base

        return mx.fast.rope(
            x,
            self.dims,
            traditional=self.traditional,
            base=base,
            scale=self.scale,
            offset=offset,
        )


class Attention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        dim = args.hidden_size
        self.n_heads = n_heads = args.num_attention_heads
        self.n_kv_heads = n_kv_heads = args.num_key_value_heads
        self.n_kv_groups = n_heads // args.num_key_value_heads

        self.head_dim = head_dim = args.hidden_size // n_heads
        self.scale = head_dim**-0.5

        self.wqkv = nn.Linear(
            dim, (n_heads + 2 * n_kv_heads) * head_dim, bias=args.bias
        )
        self.wo = nn.Linear(n_heads * head_dim, dim, bias=args.bias)

        rope_scale = (
            1 / args.rope_scaling["factor"]
            if args.rope_scaling is not None and args.rope_scaling["type"] == "linear"
            else 2.0
        )

        self.rope = DynamicNTKScalingRoPE(
            head_dim,
            max_position_embeddings=args.max_position_embeddings,
            traditional=args.rope_traditional,
            base=args.rope_theta,
            scale=rope_scale,
        )

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        B, L, D = x.shape

        qkv_states = self.wqkv(x)
        qkv_states = qkv_states.reshape(B, L, -1, 2 + self.n_kv_groups, self.head_dim)

        queries = qkv_states[..., : self.n_kv_groups, :]
        queries = queries.reshape(B, L, -1, self.head_dim)
        keys = qkv_states[..., -2, :]
        values = qkv_states[..., -1, :]

        # Prepare the queries, keys and values for the attention computation
        queries = queries.reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)
        keys = keys.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)
        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)

        if cache is not None:
            queries = self.rope(queries, offset=cache.offset)
            keys = self.rope(keys, offset=cache.offset)
            keys, values = cache.update_and_fetch(keys, values)
        else:
            queries = self.rope(queries)
            keys = self.rope(keys)

        output = scaled_dot_product_attention(
            queries, keys, values, cache=cache, scale=self.scale, mask=mask
        )
        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)
        return self.wo(output)


class MLP(nn.Module):
    def __init__(self, dim, hidden_dim):
        super().__init__()
        self.w1 = nn.Linear(dim, hidden_dim, bias=False)
        self.w2 = nn.Linear(hidden_dim, dim, bias=False)
        self.w3 = nn.Linear(dim, hidden_dim, bias=False)

    def __call__(self, x) -> mx.array:
        return self.w2(nn.silu(self.w1(x)) * self.w3(x))


class TransformerBlock(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.attention = Attention(args)
        self.feed_forward = MLP(args.hidden_size, args.intermediate_size)
        self.attention_norm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)
        self.ffn_norm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        r = self.attention(self.attention_norm(x), mask, cache)
        h = x + r
        r = self.feed_forward(self.ffn_norm(h))
        out = h + r
        return out


class InternLM2Model(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        assert args.vocab_size > 0
        self.tok_embeddings = nn.Embedding(args.vocab_size, args.hidden_size)
        self.layers = [
            TransformerBlock(args=args) for _ in range(args.num_hidden_layers)
        ]
        self.norm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        h = self.tok_embeddings(inputs)

        if mask is None:
            mask = create_attention_mask(h, cache)

        if cache is None:
            cache = [None] * len(self.layers)

        for layer, c in zip(self.layers, cache):
            h = layer(h, mask, cache=c)

        return self.norm(h)


class Model(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.model_type = args.model_type
        self.model = InternLM2Model(args)
        if not args.tie_word_embeddings:
            self.output = nn.Linear(args.hidden_size, args.vocab_size, bias=False)

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        out = self.model(inputs, mask, cache)
        if self.args.tie_word_embeddings:
            out = self.model.tok_embeddings.as_linear(out)
        else:
            out = self.output(out)
        return out

    def sanitize(self, weights):
        # Remove unused precomputed rotary freqs
        return {k: v for k, v in weights.items() if "attention.rope.inv_freq" not in k}

    @property
    def layers(self):
        return self.model.layers

>>>> llms/mlx_lm/models/mamba.py
# Copyright © 2024-2025 Apple Inc.

import math
from dataclasses import dataclass

import mlx.core as mx
import mlx.nn as nn

from .base import BaseModelArgs
from .cache import MambaCache


@dataclass
class ModelArgs(BaseModelArgs):
    model_type: str
    vocab_size: int
    hidden_size: int
    intermediate_size: int
    state_size: int
    num_hidden_layers: int
    conv_kernel: int
    use_bias: bool
    use_conv_bias: bool
    time_step_rank: int
    tie_word_embeddings: bool = True
    use_bcdt_rms: bool = False
    mixer_rms_eps: float = 1e-6

    def __post_init__(self):
        if not hasattr(self, "hidden_size") and hasattr(self, "d_model"):
            self.hidden_size = self.d_model
        if not hasattr(self, "intermediate_size") and hasattr(self, "d_inner"):
            self.intermediate_size = self.d_inner
        if not hasattr(self, "state_size") and hasattr(self, "d_state"):
            self.state_size = self.d_state
        if not hasattr(self, "num_hidden_layers") and hasattr(self, "n_layer"):
            self.num_hidden_layers = self.n_layer
        if not hasattr(self, "num_hidden_layers") and hasattr(self, "n_layers"):
            self.num_hidden_layers = self.n_layers
        if not hasattr(self, "conv_kernel") and hasattr(self, "d_conv"):
            self.conv_kernel = self.d_conv
        if not hasattr(self, "use_bias") and hasattr(self, "bias"):
            self.use_bias = self.bias
        if not hasattr(self, "use_conv_bias") and hasattr(self, "conv_bias"):
            self.use_conv_bias = self.conv_bias

        if self.time_step_rank == "auto":
            self.time_step_rank = math.ceil(self.hidden_size / 16)
        if self.model_type == "falcon_mamba":
            self.use_bcdt_rms = True


class DepthWiseConv1d(nn.Module):
    def __init__(self, channels, kernel_size, bias=True, padding=0):
        super().__init__()
        self.channels = channels
        self.kernel_size = kernel_size
        self.padding = padding
        self.weight = mx.random.normal((self.channels, kernel_size, 1))
        self.bias = mx.zeros((channels,)) if bias else None

    def __call__(self, x, cache=None):
        B, L, C = x.shape
        groups, K, _ = self.weight.shape

        if cache is not None:
            x = mx.concatenate([cache, x], axis=1)
        else:
            x = mx.pad(x, [(0, 0), (K - 1, 0), (0, 0)])

        y = mx.conv_general(x, self.weight, groups=groups)

        if self.bias is not None:
            y = y + self.bias

        return y, x[:, -K + 1 :, :]


class MambaBlock(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args

        self.hidden_size = args.hidden_size
        self.ssm_state_size = args.state_size
        self.conv_kernel_size = args.conv_kernel
        self.intermediate_size = args.intermediate_size
        self.time_step_rank = int(args.time_step_rank)
        self.use_conv_bias = args.use_conv_bias
        self.use_bcdt_rms = args.use_bcdt_rms
        if self.use_bcdt_rms:
            self.mixer_norm = lambda x: mx.fast.rms_norm(
                x, mx.ones(x.shape[-1], x.dtype), eps=args.mixer_rms_eps
            )

        self.in_proj = nn.Linear(
            self.hidden_size, self.intermediate_size * 2, bias=args.use_bias
        )

        self.conv1d = DepthWiseConv1d(
            channels=self.intermediate_size,
            kernel_size=self.conv_kernel_size,
            bias=self.use_conv_bias,
            padding=self.conv_kernel_size - 1,
        )

        self.x_proj = nn.Linear(
            self.intermediate_size,
            self.time_step_rank + 2 * self.ssm_state_size,
            bias=False,
        )
        self.dt_proj = nn.Linear(self.time_step_rank, self.intermediate_size, bias=True)

        A = mx.repeat(
            mx.arange(1.0, self.ssm_state_size + 1.0).reshape([1, self.ssm_state_size]),
            repeats=self.intermediate_size,
            axis=0,
        )
        self.A_log = mx.log(A)
        self.D = mx.ones([self.intermediate_size])

        self.out_proj = nn.Linear(
            self.intermediate_size, self.hidden_size, bias=args.use_bias
        )

    def ssm_step(self, x, A, state=None):
        D = self.D
        deltaBC = self.x_proj(x)
        delta, B, C = map(
            self.mixer_norm if self.use_bcdt_rms else lambda x: x,
            mx.split(
                deltaBC,
                [self.time_step_rank, self.time_step_rank + self.ssm_state_size],
                axis=-1,
            ),
        )
        if self.use_bcdt_rms:
            delta, B, C = map(self.mixer_norm, (delta, B, C))
        delta = nn.softplus(self.dt_proj(delta))
        new_state = mx.expand_dims(delta * x, -1) * mx.expand_dims(B, 1)
        if state is not None:
            new_state += state * mx.exp(mx.expand_dims(delta, -1) * A)
        y = (new_state @ mx.expand_dims(C, -1)).squeeze(2)
        y = y + D * x
        return y, new_state

    def _process_sequence(self, x, conv_cache, state_cache):
        B, T, D = x.shape
        xz = self.in_proj(x)
        x, z = xz.split(indices_or_sections=2, axis=-1)

        conv_out, new_conv_cache = self.conv1d(x, conv_cache)
        x = nn.silu(conv_out)

        A = -mx.exp(self.A_log)

        outputs = []
        current_state = state_cache
        y = []
        for t in range(T):
            y_t, current_state = self.ssm_step(x[:, t], A, current_state)
            y.append(y_t)
        y = mx.stack(y, axis=1)
        z = self.out_proj(nn.silu(z) * y)
        return z, (new_conv_cache, current_state)

    def __call__(self, x, cache):
        if cache is None:
            conv_cache, state_cache = None, None
        else:
            conv_cache, state_cache = cache[0], cache[1]

        output, (new_conv_cache, new_state_cache) = self._process_sequence(
            x, conv_cache, state_cache
        )

        if isinstance(cache, MambaCache):
            cache[0] = new_conv_cache
            cache[1] = new_state_cache

        return output


class ResidualBlock(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.mixer = MambaBlock(args)
        self.norm = nn.RMSNorm(args.hidden_size)

    def __call__(self, x: mx.array, cache):
        return self.mixer(self.norm(x), cache) + x


class Mamba(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.embeddings = nn.Embedding(args.vocab_size, args.hidden_size)
        self.layers = [ResidualBlock(args) for _ in range(args.num_hidden_layers)]
        self.norm_f = nn.RMSNorm(args.hidden_size)

    def __call__(self, x: mx.array, cache):
        x = self.embeddings(x)
        if cache is None:
            cache = [None] * len(self.layers)
        for layer, c in zip(self.layers, cache):
            x = layer(x, c)
        return self.norm_f(x)


class Model(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.model_type = args.model_type
        self.backbone = Mamba(args)
        if not args.tie_word_embeddings:
            self.lm_head = nn.Linear(args.hidden_size, args.vocab_size, bias=False)

    def __call__(self, inputs: mx.array, cache=None):
        B, T = inputs.shape

        x = self.backbone(inputs, cache)

        if self.args.tie_word_embeddings:
            logits = self.backbone.embeddings.as_linear(x)
        else:
            logits = self.lm_head(x)

        return logits

    def sanitize(self, weights):
        for k, v in weights.items():
            if "conv1d.weight" in k and v.shape[-1] != 1:
                weights[k] = v.moveaxis(2, 1)
        return weights

    def make_cache(self):
        return [MambaCache() for _ in range(len(self.layers))]

    @property
    def layers(self):
        return self.backbone.layers

>>>> llms/mlx_lm/models/olmoe.py
# Copyright © 2023-2024 Apple Inc.

from dataclasses import dataclass
from typing import Any, Dict, Optional, Union

import mlx.core as mx
import mlx.nn as nn

from .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention
from .rope_utils import initialize_rope
from .switch_layers import SwitchGLU


@dataclass
class ModelArgs(BaseModelArgs):
    model_type: str
    hidden_size: int
    num_hidden_layers: int
    intermediate_size: int
    num_attention_heads: int
    rms_norm_eps: float
    vocab_size: int
    num_experts: int
    num_experts_per_tok: int
    norm_topk_prob: bool = False
    head_dim: Optional[int] = None
    max_position_embeddings: Optional[int] = None
    num_key_value_heads: Optional[int] = None
    attention_bias: bool = False
    mlp_bias: bool = False
    rope_theta: float = 10000
    rope_traditional: bool = False
    rope_scaling: Optional[Dict[str, Union[float, str]]] = None
    tie_word_embeddings: bool = True

    def __post_init__(self):
        if self.num_key_value_heads is None:
            self.num_key_value_heads = self.num_attention_heads


class Attention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        dim = args.hidden_size
        self.n_heads = n_heads = args.num_attention_heads
        self.n_kv_heads = n_kv_heads = args.num_key_value_heads

        self.head_dim = head_dim = args.head_dim or args.hidden_size // n_heads

        self.scale = head_dim**-0.5

        self.q_proj = nn.Linear(dim, n_heads * head_dim, bias=args.attention_bias)
        self.k_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=args.attention_bias)
        self.v_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=args.attention_bias)
        self.o_proj = nn.Linear(n_heads * head_dim, dim, bias=args.attention_bias)

        self.rope = initialize_rope(
            self.head_dim,
            args.rope_theta,
            args.rope_traditional,
            args.rope_scaling,
            args.max_position_embeddings,
        )

        self.q_norm = nn.RMSNorm(n_heads * head_dim, args.rms_norm_eps)
        self.k_norm = nn.RMSNorm(n_kv_heads * head_dim, args.rms_norm_eps)

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        B, L, D = x.shape
        queries, keys, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)
        queries = self.q_norm(queries)
        keys = self.k_norm(keys)
        queries = queries.reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)
        keys = keys.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)
        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)
        if cache is not None:
            queries = self.rope(queries, offset=cache.offset)
            keys = self.rope(keys, offset=cache.offset)
            keys, values = cache.update_and_fetch(keys, values)
        else:
            queries = self.rope(queries)
            keys = self.rope(keys)
        output = scaled_dot_product_attention(
            queries, keys, values, cache=cache, scale=self.scale, mask=mask
        )
        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)
        return self.o_proj(output)


class OlmoeSparseMoeBlock(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.num_experts = args.num_experts
        self.top_k = args.num_experts_per_tok
        self.norm_topk_prob = args.norm_topk_prob

        self.gate = nn.Linear(args.hidden_size, self.num_experts, bias=False)
        self.switch_mlp = SwitchGLU(
            args.hidden_size,
            args.intermediate_size,
            self.num_experts,
            bias=args.mlp_bias,
        )

    def __call__(self, x: mx.array) -> mx.array:
        B, L, D = x.shape
        x_flat = x.reshape(-1, D)
        router_logits = self.gate(x_flat)
        routing_weights = mx.softmax(router_logits, axis=1, precise=True)
        k = self.top_k
        indices = mx.stop_gradient(
            mx.argpartition(-routing_weights, kth=k - 1, axis=-1)[..., :k]
        )
        scores = mx.take_along_axis(routing_weights, indices, axis=-1)
        if self.norm_topk_prob:
            scores = scores / scores.sum(axis=-1, keepdims=True)
        y = self.switch_mlp(x_flat, indices)
        y = (y * scores[..., None]).sum(axis=-2)
        return y.reshape(B, L, D)


class TransformerBlock(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.self_attn = Attention(args)
        self.mlp = OlmoeSparseMoeBlock(args)
        self.input_layernorm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)
        self.post_attention_layernorm = nn.RMSNorm(
            args.hidden_size, eps=args.rms_norm_eps
        )

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        x = x + self.self_attn(self.input_layernorm(x), mask, cache)
        x = x + self.mlp(self.post_attention_layernorm(x))
        return x


class OlmoeModel(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.vocab_size = args.vocab_size
        self.num_hidden_layers = args.num_hidden_layers
        assert self.vocab_size > 0
        self.embed_tokens = nn.Embedding(args.vocab_size, args.hidden_size)
        self.layers = [
            TransformerBlock(args=args) for _ in range(args.num_hidden_layers)
        ]
        self.norm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)

    def __call__(
        self,
        inputs: mx.array,
        cache=None,
        mask=None,
    ):
        h = self.embed_tokens(inputs)
        if mask is None:
            mask = create_attention_mask(h, cache)
        if cache is None:
            cache = [None] * len(self.layers)
        for layer, c in zip(self.layers, cache):
            h = layer(h, mask, cache=c)
        return self.norm(h)


class Model(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.model_type = args.model_type
        self.model = OlmoeModel(args)
        if not args.tie_word_embeddings:
            self.lm_head = nn.Linear(args.hidden_size, args.vocab_size, bias=False)

    def __call__(
        self,
        inputs: mx.array,
        cache=None,
        mask=None,
    ):
        out = self.model(inputs, cache, mask)
        if self.args.tie_word_embeddings:
            out = self.model.embed_tokens.as_linear(out)
        else:
            out = self.lm_head(out)
        return out

    def sanitize(self, weights):
        if "model.layers.0.mlp.experts.0.up_proj.weight" not in weights:
            return weights
        for l in range(self.args.num_hidden_layers):
            prefix = f"model.layers.{l}"
            for n in ["up_proj", "down_proj", "gate_proj"]:
                for k in ["weight", "scales", "biases"]:
                    if f"{prefix}.mlp.experts.0.{n}.{k}" in weights:
                        to_join = [
                            weights.pop(f"{prefix}.mlp.experts.{e}.{n}.{k}")
                            for e in range(self.args.num_experts)
                        ]
                        weights[f"{prefix}.mlp.switch_mlp.{n}.{k}"] = mx.stack(to_join)
        return weights

    @property
    def layers(self):
        return self.model.layers

>>>> llms/mlx_lm/models/gpt2.py
# Copyright © 2023-2024 Apple Inc.

from dataclasses import dataclass
from typing import Any, Dict, Optional, Tuple, Union

import mlx.core as mx
import mlx.nn as nn
import numpy as np

from .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention


@dataclass
class ModelArgs(BaseModelArgs):
    model_type: str
    n_ctx: int
    n_embd: int
    n_head: int
    n_layer: int
    n_positions: int
    layer_norm_epsilon: float
    vocab_size: int
    num_key_value_heads: int = None

    def __post_init__(self):
        if self.num_key_value_heads is None:
            self.num_key_value_heads = self.n_head


class Attention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        assert args.n_embd % args.n_head == 0, "n_embd must be divisible by n_head"

        self.n_embd = args.n_embd
        self.n_head = args.n_head
        self.head_dim = self.n_embd // self.n_head

        self.scale = self.head_dim**-0.5

        self.c_attn = nn.Linear(self.n_embd, 3 * self.n_embd, bias=True)
        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=True)

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        B, L, D = x.shape

        qkv = self.c_attn(x)
        queries, keys, values = mx.split(qkv, 3, axis=-1)

        # Prepare the queries, keys and values for the attention computation
        queries = queries.reshape(B, L, self.n_head, -1).transpose(0, 2, 1, 3)
        keys = keys.reshape(B, L, self.n_head, -1).transpose(0, 2, 1, 3)
        values = values.reshape(B, L, self.n_head, -1).transpose(0, 2, 1, 3)

        if cache is not None:
            keys, values = cache.update_and_fetch(keys, values)

        output = scaled_dot_product_attention(
            queries, keys, values, cache=cache, scale=self.scale, mask=mask
        )

        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)
        return self.c_proj(output)


class MLP(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        self.n_embd = args.n_embd
        self.c_fc = nn.Linear(self.n_embd, 4 * self.n_embd)
        self.c_proj = nn.Linear(4 * self.n_embd, self.n_embd)

    def __call__(self, x) -> mx.array:
        return self.c_proj(nn.gelu_approx(self.c_fc(x)))


class TransformerBlock(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        self.n_head = args.n_head
        self.n_embd = args.n_embd
        self.layer_norm_epsilon = args.layer_norm_epsilon
        self.attn = Attention(args)
        self.mlp = MLP(args)
        self.ln_1 = nn.LayerNorm(
            self.n_embd,
            eps=self.layer_norm_epsilon,
        )
        self.ln_2 = nn.LayerNorm(self.n_embd, eps=self.layer_norm_epsilon)

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        r = self.attn(self.ln_1(x), mask, cache)
        h = x + r
        r = self.mlp(self.ln_2(h))
        out = h + r
        return out


class GPT2Model(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.n_embd = args.n_embd
        self.n_positions = args.n_positions
        self.vocab_size = args.vocab_size
        self.n_layer = args.n_layer
        self.layer_norm_epsilon = args.layer_norm_epsilon
        assert self.vocab_size > 0
        self.wte = nn.Embedding(self.vocab_size, self.n_embd)
        self.wpe = nn.Embedding(self.n_positions, self.n_embd)
        self.h = [TransformerBlock(args=args) for _ in range(self.n_layer)]
        self.ln_f = nn.LayerNorm(self.n_embd, eps=self.layer_norm_epsilon)

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        _, L = inputs.shape

        hidden_states = self.wte(inputs)

        mask = None
        if hidden_states.shape[1] > 1:

            position_ids = mx.array(np.arange(L))
            hidden_states += self.wpe(position_ids)

            if mask is None:
                mask = create_attention_mask(hidden_states, cache)

        if cache is None:
            cache = [None] * len(self.h)

        for layer, c in zip(self.h, cache):
            hidden_states = layer(hidden_states, mask, cache=c)

        return self.ln_f(hidden_states)


class Model(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.model_type = args.model_type
        self.model = GPT2Model(args)

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        out = self.model(inputs, mask, cache)
        out = self.model.wte.as_linear(out)
        return out

    def sanitize(self, weights):
        new_weights = {}
        for i in range(self.args.n_layer):
            if f"h.{i}.attn.bias" in weights:
                del weights[f"h.{i}.attn.bias"]
            if f"h.{i}.attn.c_attn.weight" in weights:
                weights[f"h.{i}.attn.c_attn.weight"] = weights[
                    f"h.{i}.attn.c_attn.weight"
                ].transpose(1, 0)
            if f"h.{i}.attn.c_proj.weight" in weights:
                weights[f"h.{i}.attn.c_proj.weight"] = weights[
                    f"h.{i}.attn.c_proj.weight"
                ].transpose(1, 0)
            if f"h.{i}.mlp.c_fc.weight" in weights:
                weights[f"h.{i}.mlp.c_fc.weight"] = weights[
                    f"h.{i}.mlp.c_fc.weight"
                ].transpose(1, 0)
            if f"h.{i}.mlp.c_proj.weight" in weights:
                weights[f"h.{i}.mlp.c_proj.weight"] = weights[
                    f"h.{i}.mlp.c_proj.weight"
                ].transpose(1, 0)
        for weight in weights:
            if not weight.startswith("model."):
                new_weights[f"model.{weight}"] = weights[weight]
            else:
                new_weights[weight] = weights[weight]
        return new_weights

    @property
    def layers(self):
        return self.model.h

>>>> llms/mlx_lm/models/olmo2.py
# Copyright © 2023-2024 Apple Inc.

from dataclasses import dataclass
from typing import Any, Dict, Optional, Union

import mlx.core as mx
import mlx.nn as nn

from .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention
from .rope_utils import initialize_rope


@dataclass
class ModelArgs(BaseModelArgs):
    model_type: str
    hidden_size: int
    num_hidden_layers: int
    intermediate_size: int
    num_attention_heads: int
    rms_norm_eps: float
    vocab_size: int
    head_dim: Optional[int] = None
    max_position_embeddings: Optional[int] = None
    num_key_value_heads: Optional[int] = None
    attention_bias: bool = False
    mlp_bias: bool = False
    rope_theta: float = 10000
    rope_traditional: bool = False
    rope_scaling: Optional[Dict[str, Union[float, str]]] = None
    tie_word_embeddings: bool = True

    def __post_init__(self):
        if self.num_key_value_heads is None:
            self.num_key_value_heads = self.num_attention_heads


class Attention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        dim = args.hidden_size
        self.n_heads = n_heads = args.num_attention_heads
        self.n_kv_heads = n_kv_heads = args.num_key_value_heads

        self.head_dim = head_dim = args.head_dim or args.hidden_size // n_heads

        self.scale = head_dim**-0.5
        if hasattr(args, "attention_bias"):
            attention_bias = args.attention_bias
        else:
            attention_bias = False

        self.q_proj = nn.Linear(dim, n_heads * head_dim, bias=attention_bias)
        self.k_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=attention_bias)
        self.v_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=attention_bias)
        self.o_proj = nn.Linear(n_heads * head_dim, dim, bias=attention_bias)

        self.rope = initialize_rope(
            self.head_dim,
            args.rope_theta,
            args.rope_traditional,
            args.rope_scaling,
            args.max_position_embeddings,
        )

        self.q_norm = nn.RMSNorm(n_heads * head_dim, args.rms_norm_eps)
        self.k_norm = nn.RMSNorm(n_kv_heads * head_dim, args.rms_norm_eps)

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        B, L, D = x.shape

        queries, keys, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)
        queries = self.q_norm(queries)
        keys = self.k_norm(keys)

        # Prepare the queries, keys and values for the attention computation
        queries = queries.reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)
        keys = keys.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)
        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)

        if cache is not None:
            queries = self.rope(queries, offset=cache.offset)
            keys = self.rope(keys, offset=cache.offset)
            keys, values = cache.update_and_fetch(keys, values)
        else:
            queries = self.rope(queries)
            keys = self.rope(keys)

        output = scaled_dot_product_attention(
            queries, keys, values, cache=cache, scale=self.scale, mask=mask
        )

        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)
        return self.o_proj(output)


class MLP(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        dim = args.hidden_size
        hidden_dim = args.intermediate_size
        if hasattr(args, "mlp_bias"):
            mlp_bias = args.mlp_bias
        else:
            mlp_bias = False

        self.gate_proj = nn.Linear(dim, hidden_dim, bias=mlp_bias)
        self.down_proj = nn.Linear(hidden_dim, dim, bias=mlp_bias)
        self.up_proj = nn.Linear(dim, hidden_dim, bias=mlp_bias)

    def __call__(self, x) -> mx.array:
        return self.down_proj(nn.silu(self.gate_proj(x)) * self.up_proj(x))


class TransformerBlock(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.num_attention_heads = args.num_attention_heads
        self.hidden_size = args.hidden_size
        self.self_attn = Attention(args)
        self.mlp = MLP(args)
        self.post_attention_layernorm = nn.RMSNorm(
            args.hidden_size, eps=args.rms_norm_eps
        )
        self.post_feedforward_layernorm = nn.RMSNorm(
            args.hidden_size, eps=args.rms_norm_eps
        )
        self.args = args

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        r = self.post_attention_layernorm(self.self_attn(x, mask, cache))
        h = x + r
        r = self.post_feedforward_layernorm(self.mlp(h))
        out = h + r
        return out


class LlamaModel(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.vocab_size = args.vocab_size
        self.num_hidden_layers = args.num_hidden_layers
        assert self.vocab_size > 0
        self.embed_tokens = nn.Embedding(args.vocab_size, args.hidden_size)
        self.layers = [
            TransformerBlock(args=args) for _ in range(args.num_hidden_layers)
        ]
        self.norm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)

    def __call__(
        self,
        inputs: mx.array,
        cache=None,
        mask=None,
    ):
        h = self.embed_tokens(inputs)

        if mask is None:
            mask = create_attention_mask(h, cache)

        if cache is None:
            cache = [None] * len(self.layers)

        for layer, c in zip(self.layers, cache):
            h = layer(h, mask, cache=c)

        return self.norm(h)


class Model(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.model_type = args.model_type
        self.model = LlamaModel(args)
        if not args.tie_word_embeddings:
            self.lm_head = nn.Linear(args.hidden_size, args.vocab_size, bias=False)

    def __call__(
        self,
        inputs: mx.array,
        cache=None,
        mask=None,
    ):
        out = self.model(inputs, cache, mask)
        if self.args.tie_word_embeddings:
            out = self.model.embed_tokens.as_linear(out)
        else:
            out = self.lm_head(out)
        return out

    def sanitize(self, weights):
        # Remove unused precomputed rotary freqs
        return {
            k: v for k, v in weights.items() if "self_attn.rotary_emb.inv_freq" not in k
        }

    @property
    def layers(self):
        return self.model.layers

>>>> llms/mlx_lm/models/plamo2.py
# Copyright © 2025 Apple Inc.

import math
from dataclasses import dataclass
from typing import Any, Optional

import mlx.core as mx
import mlx.nn as nn
from mlx_lm.models.base import BaseModelArgs, create_attention_mask

from .cache import KVCache, MambaCache


@dataclass
class ModelArgs(BaseModelArgs):
    model_type: str = "plamo2"
    hidden_size: int = 4096
    num_hidden_layers: int = 32
    rms_norm_eps: float = 1e-6
    tie_word_embeddings: bool = True
    num_attention_heads: int = 32
    num_key_value_heads: int = 4
    hidden_size_per_head: int = 128
    max_position_embeddings: int = 2048
    attention_window_size: int = 2048
    full_attention_idx: Optional[list[int]] = None
    mamba_d_state: int = 64
    mamba_d_conv: int = 4
    mamba_num_heads: int = 64
    mamba_step: int = 2
    mamba_chunk_size: int = 256
    mamba_enabled: bool = True
    intermediate_size: int = 13312
    vocab_size: int = 32000


class RMSNorm(nn.Module):
    def __init__(
        self,
        hidden_size: int,
        eps: float = 1e-6,
        offset: float = 1.0,
    ) -> None:
        super().__init__()
        self.weight = mx.zeros(hidden_size)
        self.variance_epsilon = eps
        self.offset = offset

    def __call__(self, hidden_states: mx.array) -> mx.array:
        return mx.fast.rms_norm(
            hidden_states, self.weight + self.offset, self.variance_epsilon
        )


def _rms_norm(hidden_states: mx.array, eps: float) -> mx.array:
    input_dtype = hidden_states.dtype
    hidden_states = hidden_states.astype(mx.float32)
    variance = mx.power(hidden_states, 2).mean(-1, keepdims=True)
    hidden_states = hidden_states * mx.rsqrt(variance + eps)
    hidden_states = hidden_states.astype(input_dtype)

    return hidden_states


def get_initial_dt_bias(num_heads: int) -> mx.array:
    dt_min = 0.001
    dt_max = 0.1
    dt = mx.exp(
        mx.random.uniform(shape=(num_heads,)) * (math.log(dt_max) - math.log(dt_min))
        + math.log(dt_min)
    )
    dt = mx.clip(dt, a_min=1e-4, a_max=None)
    inv_dt = dt + mx.log(-mx.expm1(-dt))
    return inv_dt


def get_initial_A(num_heads: int) -> mx.array:
    A = mx.arange(1, num_heads + 1, dtype=mx.float32)
    return mx.log(A)


# From: https://github.com/state-spaces/mamba/blob/0cce0fa645f100f00620ddf2333c2b7712abfdec/mamba_ssm/ops/triton/selective_state_update.py#L219
def selective_state_update_ref(
    state, x, dt, A, B, C, D=None, z=None, dt_bias=None, dt_softplus=False
) -> tuple[mx.array, mx.array]:
    """
    Argument:
        state: (batch, dim, dstate) or (batch, nheads, dim, dstate)
        x: (batch, dim) or (batch, nheads, dim)
        dt: (batch, dim) or (batch, nheads, dim)
        A: (dim, dstate) or (nheads, dim, dstate)
        B: (batch, dstate) or (batch, ngroups, dstate)
        C: (batch, dstate) or (batch, ngroups, dstate)
        D: (dim,) or (nheads, dim)
        z: (batch, dim) or (batch, nheads, dim)
        dt_bias: (dim,) or (nheads, dim)
    Return:
        out: (batch, dim) or (batch, nheads, dim)
    """
    has_heads = state.ndim > 3
    if state.ndim == 3:
        state = mx.expand_dims(state, 1)
    if x.ndim == 2:
        x = mx.expand_dims(x, 1)
    if dt.ndim == 2:
        dt = mx.expand_dims(dt, 1)
    if A.ndim == 2:
        A = mx.expand_dims(A, 0)
    if B.ndim == 2:
        B = mx.expand_dims(B, 1)
    if C.ndim == 2:
        C = mx.expand_dims(C, 1)
    if D is not None and D.ndim == 1:
        D = mx.expand_dims(D, 0)
    if z is not None and z.ndim == 2:
        z = mx.expand_dims(z, 1)
    if dt_bias is not None and dt_bias.ndim == 1:
        dt_bias = mx.expand_dims(dt_bias, 0)
    batch, nheads, dim, dstate = state.shape
    assert x.shape == (batch, nheads, dim)
    assert dt.shape == x.shape
    assert A.shape == (nheads, dim, dstate)
    ngroups = B.shape[1]
    assert nheads % ngroups == 0, "nheads must be divisible by ngroups"
    assert B.shape == (batch, ngroups, dstate)
    assert C.shape == B.shape
    if D is not None:
        assert D.shape == (nheads, dim)
    if z is not None:
        assert z.shape == x.shape
    if dt_bias is not None:
        assert dt_bias.shape == (nheads, dim)
        dt = dt + dt_bias
    dt = nn.softplus(dt) if dt_softplus else dt
    dA = mx.exp(mx.expand_dims(dt, axis=-1) * A)  # (batch, nheads, dim, dstate)
    B = mx.reshape(
        mx.repeat(mx.expand_dims(B, axis=2), nheads // ngroups, 2),
        (batch, nheads, dstate),
    )  # (batch, nheads, dstate)
    C = mx.reshape(
        mx.repeat(mx.expand_dims(C, axis=2), nheads // ngroups, 2),
        (batch, nheads, dstate),
    )  # (batch, nheads, dstate)
    dB = mx.expand_dims(dt, axis=-1) * mx.expand_dims(
        B, axis=-2
    )  # (batch, nheads, dim, dstate)
    state = state * dA + dB * mx.expand_dims(x, axis=-1)  # (batch, dim, dstate)
    out = mx.einsum("bhdn,bhn->bhd", state.astype(C.dtype), C)
    if D is not None:
        out += (x * D).astype(out.dtype)
    out = (out if z is None else out * nn.silu(z)).astype(x.dtype)
    if not has_heads:
        out = out.squeeze(1)
    return out, state


def ssd_update_state(
    ssm_state: mx.array,
    x: mx.array,
    dt: mx.array,
    A: mx.array,
    B: mx.array,
    C: mx.array,
    D: mx.array,
    z: mx.array,
    dt_bias: mx.array,
    dt_softplus: bool,
) -> tuple[mx.array, mx.array]:
    assert ssm_state.dtype == mx.float32
    dtype = x.dtype

    hidden_size_per_head = x.shape[-1]
    d_state = B.shape[-1]
    A = mx.broadcast_to(
        A[:, None, None], (A.shape[0], hidden_size_per_head, d_state)
    ).astype(mx.float32)
    dt = mx.broadcast_to(
        dt[..., None], (dt.shape[0], dt.shape[1], hidden_size_per_head)
    )
    dt_bias = mx.broadcast_to(
        dt_bias[:, None], (dt_bias.shape[0], hidden_size_per_head)
    )
    D = mx.broadcast_to(D[:, None], (D.shape[0], hidden_size_per_head))
    out, ssm_state = selective_state_update_ref(
        ssm_state,
        x.astype(dtype),
        dt.astype(dtype),
        A.astype(mx.float32),
        B.astype(dtype),
        C.astype(dtype),
        D.astype(mx.float32),
        z.astype(dtype),
        dt_bias.astype(mx.float32),
        dt_softplus=dt_softplus,
    )
    return out[:, None], ssm_state


def ssd_chunk_scan_combined(
    x: mx.array,
    dt: mx.array,
    A: mx.array,
    B: mx.array,
    C: mx.array,
    D: mx.array,
    z: mx.array,
    dt_bias: mx.array,
    dt_softplus: bool,
    ssm_state: mx.array,
) -> tuple[mx.array, mx.array]:
    assert ssm_state.dtype == mx.float32
    length = x.shape[1]
    ys = []
    for i in range(length):
        y, ssm_state = ssd_update_state(
            ssm_state,
            x[:, i],
            dt[:, i],
            A,
            B[:, i],
            C[:, i],
            D if D.ndim == 1 else D[:, i],
            z=z[:, i],
            dt_bias=dt_bias,
            dt_softplus=dt_softplus,
        )
        ys.append(y)
    return mx.concatenate(ys, axis=1), ssm_state


def causal_conv1d_update(conv_state, x, weight) -> tuple[mx.array, mx.array]:
    _, seqlen, dim = x.shape
    state_len = conv_state.shape[-2]
    x = mx.concatenate([conv_state, x], axis=-2)
    conv_state = x[:, -state_len:]
    out = mx.conv1d(
        x,
        weight,
        padding=0,
        groups=dim,
    )[:, -seqlen:]
    return nn.silu(out), conv_state


class Mamba(nn.Module):
    def __init__(self, config: ModelArgs) -> None:
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.d_state = config.mamba_d_state
        self.d_conv = config.mamba_d_conv
        self.chunk_size = config.mamba_chunk_size
        self.num_heads = config.mamba_num_heads
        self.hidden_size_per_head = config.hidden_size_per_head

        self.intermediate_size = self.num_heads * self.hidden_size_per_head

        self.in_proj = nn.Linear(
            self.hidden_size, 2 * self.intermediate_size, bias=False
        )
        self.conv1d = nn.Conv1d(
            in_channels=self.intermediate_size,
            out_channels=self.intermediate_size,
            bias=False,
            kernel_size=self.d_conv,
            groups=self.intermediate_size,
            padding=0,
        )
        self.dt_dim = max(64, self.hidden_size // 16)
        self.bcdt_proj = nn.Linear(
            self.intermediate_size,
            self.dt_dim + 2 * self.d_state,
            bias=False,
        )
        self.dt_proj = nn.Linear(self.dt_dim, self.num_heads, bias=False)

        self.dt_bias = get_initial_dt_bias(self.num_heads)
        self.A_log = get_initial_A(self.num_heads)
        self.D = mx.ones(self.num_heads, dtype=mx.float32)

        self.dt_norm_weight = mx.ones(self.dt_dim)
        self.B_norm_weight = mx.ones(self.d_state)
        self.C_norm_weight = mx.ones(self.d_state)

        self.out_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)

    def __call__(
        self,
        hidden_states: mx.array,
        mask: Optional[mx.array] = None,
        cache=None,
    ):
        bsize, length, _ = hidden_states.shape

        if cache is not None and cache[0] is not None:
            conv_state = cache[0]
            ssm_state = cache[1]
        else:
            conv_state = mx.zeros(
                (bsize, self.d_conv - 1, self.intermediate_size),
                dtype=hidden_states.dtype,
            )
            ssm_state = mx.zeros(
                (bsize, self.num_heads, self.hidden_size_per_head, self.d_state),
                dtype=mx.float32,
            )

        zx = self.in_proj(hidden_states)
        zx = zx.reshape(bsize, length, self.num_heads, -1)
        # z: (bsize, length, num_heads, hidden_size_per_head)
        # x: (bsize, length, num_heads, hidden_size_per_head)
        z, x = mx.split(
            zx,
            [
                self.hidden_size_per_head,
            ],
            axis=-1,
        )

        x = x.reshape(bsize, -1, self.num_heads * self.hidden_size_per_head)
        x, conv_state = causal_conv1d_update(conv_state, x, self.conv1d.weight)
        BCdt = self.bcdt_proj(x)
        x = x.reshape(bsize, length, self.num_heads, -1)
        B, C, dt = mx.split(BCdt, [self.d_state, self.d_state * 2], axis=-1)

        A = -mx.exp(self.A_log.astype(mx.float32))  # (num_heads,)
        dt = mx.fast.rms_norm(dt, self.dt_norm_weight, self.config.rms_norm_eps)
        B = mx.fast.rms_norm(B, self.B_norm_weight, self.config.rms_norm_eps)
        C = mx.fast.rms_norm(C, self.C_norm_weight, self.config.rms_norm_eps)

        # (bsize, length, num_heads, 1)
        dt = self.dt_proj(dt)[..., None]

        out, ssm_state = ssd_chunk_scan_combined(
            x,
            dt.reshape(bsize, length, -1),
            A,
            B,
            C,
            D=self.D,
            z=z,
            dt_bias=self.dt_bias,
            dt_softplus=True,
            ssm_state=ssm_state,
        )

        if cache is not None:
            cache[0] = conv_state
            cache[1] = ssm_state
        y = self.out_proj(out.reshape(bsize, length, -1))

        return y


class Attention(nn.Module):
    def __init__(self, config: ModelArgs) -> None:
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        head_dim = config.hidden_size_per_head
        self.max_position_embeddings = config.max_position_embeddings
        self.scale = head_dim**-0.5

        self.q_num_heads = config.num_attention_heads
        self.qk_dim = self.v_dim = head_dim
        self.k_num_heads = self.v_num_heads = config.num_key_value_heads
        assert self.q_num_heads % self.k_num_heads == 0
        self.n_group = self.q_num_heads // self.k_num_heads

        self.q_proj_dim = self.q_num_heads * self.qk_dim
        self.k_proj_dim = self.k_num_heads * self.qk_dim
        self.v_proj_dim = self.k_num_heads * self.v_dim
        self.qkv_proj = nn.Linear(
            self.hidden_size,
            self.q_proj_dim + self.k_proj_dim + self.v_proj_dim,
            bias=False,
        )
        self.o_proj = nn.Linear(
            self.q_num_heads * self.v_dim, self.hidden_size, bias=False
        )

        self.q_weight = mx.ones((self.q_num_heads, self.qk_dim))
        self.k_weight = mx.ones((self.k_num_heads, self.qk_dim))

        self.rope = nn.RoPE(self.qk_dim)

    def __call__(
        self,
        hidden_states: mx.array,
        mask: Optional[mx.array] = None,
        cache=None,
    ):
        B, T, _ = hidden_states.shape

        qkv = self.qkv_proj(hidden_states)
        q, k, v = mx.split(
            qkv, [self.q_proj_dim, self.q_proj_dim + self.k_proj_dim], axis=-1
        )
        q = q.reshape(B, T, self.q_num_heads, self.qk_dim).transpose(0, 2, 1, 3)
        k = k.reshape(B, T, self.k_num_heads, self.qk_dim).transpose(0, 2, 1, 3)
        v = v.reshape(B, T, self.v_num_heads, self.v_dim).transpose(0, 2, 1, 3)

        q = _rms_norm(q, 1e-6) * self.q_weight[:, None]
        k = _rms_norm(k, 1e-6) * self.k_weight[:, None]

        if cache is not None:
            q = self.rope(q, offset=cache.offset)
            k = self.rope(k, offset=cache.offset)
            k, v = cache.update_and_fetch(k, v)
        else:
            q = self.rope(q)
            k = self.rope(k)

        output = mx.fast.scaled_dot_product_attention(
            q,
            k,
            v,
            scale=self.scale,
            mask=mask,
        )
        output = output.transpose(0, 2, 1, 3).reshape(
            B, T, self.q_num_heads * self.v_dim
        )
        return self.o_proj(output)


class MLP(nn.Module):
    def __init__(self, config: ModelArgs) -> None:
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size
        self.gate_up_proj = nn.Linear(
            self.hidden_size, self.intermediate_size * 2, bias=False
        )
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)

    def __call__(self, x: mx.array) -> mx.array:
        h = self.gate_up_proj(x)
        hs = mx.split(h, 2, axis=-1)
        return self.down_proj(nn.silu(hs[0]) * hs[1])


class PlamoDecoderLayer(nn.Module):
    def __init__(self, config: ModelArgs, is_mamba: bool) -> None:
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.is_mamba = is_mamba
        self.mixer: nn.Module
        if is_mamba:
            self.mixer = Mamba(config)
        else:
            self.mixer = Attention(config)
        self.mlp = MLP(config)
        self.pre_mixer_norm = RMSNorm(
            config.hidden_size, eps=config.rms_norm_eps, offset=1.0
        )
        self.post_mixer_norm = RMSNorm(
            config.hidden_size, eps=config.rms_norm_eps, offset=1.0 / 5
        )
        self.pre_mlp_norm = RMSNorm(
            config.hidden_size, eps=config.rms_norm_eps, offset=1.0
        )
        self.post_mlp_norm = RMSNorm(
            config.hidden_size, eps=config.rms_norm_eps, offset=1.0 / (5**1.5)
        )

    def __call__(
        self,
        hidden_states: mx.array,
        mask: Optional[mx.array] = None,
        cache=None,
    ):
        residual = hidden_states
        hidden_states = self.pre_mixer_norm(hidden_states)

        hidden_states_sa = self.mixer(
            hidden_states=hidden_states,
            mask=mask,
            cache=cache,
        )

        hidden_states_sa = self.post_mixer_norm(hidden_states_sa)
        hidden_states = residual + hidden_states_sa

        residual = hidden_states
        hidden_states = self.pre_mlp_norm(hidden_states)

        # Fully Connected
        hidden_states_mlp = self.mlp(hidden_states)

        # Residual
        hidden_states_mlp = self.post_mlp_norm(hidden_states_mlp)
        return residual + hidden_states_mlp


def is_mamba(config: ModelArgs, i: int) -> bool:
    if not config.mamba_enabled:
        return False
    assert config.mamba_step > 1
    assert i < config.num_hidden_layers

    if config.num_hidden_layers <= (config.mamba_step // 2):
        # use attention in last layer
        return i != config.num_hidden_layers - 1
    return (i % config.mamba_step) != (config.mamba_step // 2)


class PlamoDecoder(nn.Module):
    def __init__(self, config: ModelArgs) -> None:
        super().__init__()

        self.layers = [
            PlamoDecoderLayer(config, is_mamba=is_mamba(config, i))
            for i in range(config.num_hidden_layers)
        ]

    def __call__(self, x: mx.array, mask: mx.array, cache):
        for i, decoder_layer in enumerate(self.layers):
            x = decoder_layer(
                x,
                mask=mask,
                cache=cache[i],
            )
        return x


class PlamoModel(nn.Module):
    def __init__(self, config: ModelArgs):
        super().__init__()

        self.config = config
        self.vocab_size = config.vocab_size

        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)
        self.layers = PlamoDecoder(config)  # type: ignore
        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

    def __call__(
        self,
        inputs: mx.array,
        mask: Optional[mx.array] = None,
        cache=None,
    ):
        batch_size, seq_length = inputs.shape

        h = self.embed_tokens(inputs)

        if mask is None:
            mask = create_attention_mask(h, [cache[1]] if cache is not None else None)

        if cache is None:
            cache = [None] * len(self.layers.layers)

        # decoder layers
        out = self.layers(
            h,
            mask,
            cache,
        )

        return self.norm(out)


class Model(nn.Module):
    def __init__(self, config: ModelArgs) -> None:
        super().__init__()
        self.config = config
        self.model_type = config.model_type
        self.model = PlamoModel(config)

        self.vocab_size = config.vocab_size

        if not config.tie_word_embeddings:
            self.lm_head: nn.Module = nn.Linear(
                config.hidden_size, self.vocab_size, bias=False
            )

    def sanitize(self, weights: dict[Any, Any]) -> dict[Any, Any]:
        for k, v in weights.items():
            if "conv1d.weight" in k and v.shape[-1] != 1:
                weights[k] = v.moveaxis(2, 1)
        return weights

    def make_cache(self):
        # TODO use RotatingKVCache is not full_attn
        # full_attn = self.layer_idx in self.config.full_attention_idx
        return [MambaCache() if l.is_mamba else KVCache() for l in self.layers]

    def __call__(
        self, inputs: mx.array, mask: Optional[mx.array] = None, cache=None
    ) -> mx.array:
        outputs = self.model(
            inputs=inputs,
            mask=None,
            cache=cache,
        )
        if self.config.tie_word_embeddings:
            logits = self.model.embed_tokens.as_linear(outputs)
        else:
            logits = self.lm_head(outputs)

        return logits

    @property
    def layers(self):
        return self.model.layers.layers

>>>> llms/mlx_lm/models/dbrx.py
# Copyright © 2023-2024 Apple Inc.

from dataclasses import dataclass
from typing import Any, Optional, Tuple

import mlx.core as mx
import mlx.nn as nn
import numpy as np

from .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention


@dataclass
class ModelArgs(BaseModelArgs):
    model_type: str
    vocab_size: int
    d_model: int
    ffn_config: dict
    attn_config: dict
    n_layers: int
    n_heads: int


class Attention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.num_heads = args.n_heads
        self.d_model = args.d_model
        self.head_dim = args.d_model // args.n_heads
        self.num_key_value_heads = args.attn_config["kv_n_heads"]
        self.clip_qkv = args.attn_config["clip_qkv"]
        self.rope_theta = args.attn_config["rope_theta"]

        self.scale = self.head_dim**-0.5

        self.Wqkv = nn.Linear(
            args.d_model,
            (self.num_key_value_heads * 2 + self.num_heads) * self.head_dim,
            bias=False,
        )
        self.out_proj = nn.Linear(args.d_model, args.d_model, bias=False)
        self.rope = nn.RoPE(
            self.head_dim,
            traditional=False,
            base=self.rope_theta,
        )

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:

        qkv = self.Wqkv(x)
        qkv = mx.clip(qkv, a_min=-self.clip_qkv, a_max=self.clip_qkv)
        splits = [self.d_model, self.d_model + self.head_dim * self.num_key_value_heads]
        queries, keys, values = mx.split(qkv, splits, axis=-1)

        B, L, D = x.shape

        # Prepare the queries, keys and values for the attention computation
        queries = queries.reshape(B, L, self.num_heads, -1).transpose(0, 2, 1, 3)
        keys = keys.reshape(B, L, self.num_key_value_heads, -1).transpose(0, 2, 1, 3)
        values = values.reshape(B, L, self.num_key_value_heads, -1).transpose(
            0, 2, 1, 3
        )

        if cache is not None:
            queries = self.rope(queries, offset=cache.offset)
            keys = self.rope(keys, offset=cache.offset)
            keys, values = cache.update_and_fetch(keys, values)
        else:
            queries = self.rope(queries)
            keys = self.rope(keys)

        output = scaled_dot_product_attention(
            queries, keys, values, cache=cache, scale=self.scale, mask=mask
        )
        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)
        return self.out_proj(output)


class NormAttnNorm(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.norm_1 = nn.LayerNorm(args.d_model, bias=False)
        self.norm_2 = nn.LayerNorm(args.d_model, bias=False)
        self.attn = Attention(args)

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        h = self.attn(self.norm_1(x), mask=mask, cache=cache)
        x = h + x
        return x, self.norm_2(x)


class MLP(nn.Module):
    def __init__(self, d_model: int, ffn_dim: int):
        super().__init__()
        self.v1 = nn.Linear(d_model, ffn_dim, bias=False)
        self.w1 = nn.Linear(d_model, ffn_dim, bias=False)
        self.w2 = nn.Linear(ffn_dim, d_model, bias=False)
        self.act_fn = nn.silu

    def __call__(self, x: mx.array) -> mx.array:
        current_hidden_states = self.act_fn(self.w1(x)) * self.v1(x)
        current_hidden_states = self.w2(current_hidden_states)
        return current_hidden_states


class Router(nn.Module):
    def __init__(self, d_model: int, num_experts: int):
        super().__init__()
        self.layer = nn.Linear(d_model, num_experts, bias=False)

    def __call__(self, x: mx.array):
        return self.layer(x)


class SparseMoeBlock(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.d_model = args.d_model
        self.ffn_dim = args.ffn_config["ffn_hidden_size"]
        self.num_experts = args.ffn_config["moe_num_experts"]
        self.num_experts_per_tok = args.ffn_config["moe_top_k"]

        self.router = Router(self.d_model, self.num_experts)
        self.experts = [
            MLP(self.d_model, self.ffn_dim) for _ in range(self.num_experts)
        ]

    def __call__(self, x: mx.array) -> mx.array:
        ne = self.num_experts_per_tok
        orig_shape = x.shape
        x = x.reshape(-1, x.shape[-1])

        gates = self.router(x)
        gates = mx.softmax(gates.astype(mx.float32), axis=-1)

        inds = mx.stop_gradient(mx.argpartition(-gates, kth=ne - 1, axis=-1)[:, :ne])
        scores = mx.take_along_axis(gates, inds, axis=-1)
        scores = scores / mx.linalg.norm(scores, ord=1, axis=-1, keepdims=True)
        scores = scores.astype(x.dtype)

        if self.training:
            inds = np.array(inds)
            y = mx.zeros((x.shape[0], ne, x.shape[-1]), x.dtype)
            for e, expert in enumerate(self.experts):
                idx1, idx2 = map(mx.array, np.where(inds == e))
                if idx1.size == 0:
                    continue
                y[idx1, idx2] = expert(x[idx1])

            y = (y * scores[:, :, None]).sum(axis=1)
        else:
            y = []
            for xt, st, it in zip(x, scores, inds.tolist()):
                yt = mx.stack([self.experts[e](xt) for e in it], axis=-1)
                yt = (yt * st).sum(axis=-1)
                y.append(yt)
            y = mx.stack(y, axis=0)

        return y.reshape(orig_shape)


class DecoderLayer(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.ffn = SparseMoeBlock(args)
        self.norm_attn_norm = NormAttnNorm(args)

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        r, h = self.norm_attn_norm(x, mask, cache)
        out = self.ffn(h) + r
        return out


class DBRX(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.vocab_size = args.vocab_size
        self.wte = nn.Embedding(args.vocab_size, args.d_model)
        self.blocks = [DecoderLayer(args=args) for _ in range(args.n_layers)]
        self.norm_f = nn.LayerNorm(args.d_model, bias=False)

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        h = self.wte(inputs)

        if mask is None:
            mask = create_attention_mask(h, cache)

        if cache is None:
            cache = [None] * len(self.blocks)

        for layer, c in zip(self.blocks, cache):
            h = layer(h, mask, c)

        return self.norm_f(h)


class Model(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.model_type = args.model_type
        self.transformer = DBRX(args)
        self.lm_head = nn.Linear(args.d_model, args.vocab_size, bias=False)
        self.args = args

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        out = self.transformer(inputs, mask, cache)
        return self.lm_head(out)

    @property
    def layers(self):
        return self.transformer.blocks

    def sanitize(self, weights):
        # Split experts into sub matrices
        num_experts = self.args.ffn_config["moe_num_experts"]
        dim = self.args.ffn_config["ffn_hidden_size"]

        pattern = "experts.mlp"
        new_weights = {k: v for k, v in weights.items() if pattern not in k}
        for k, v in weights.items():
            if pattern in k:
                experts = [
                    (k.replace(".mlp", f".{e}") + ".weight", sv)
                    for e, sv in enumerate(mx.split(v, num_experts, axis=0))
                ]
                if k.endswith("w2"):
                    experts = [(s, sv.T) for s, sv in experts]
                new_weights.update(experts)
        return new_weights

>>>> llms/mlx_lm/models/recurrent_gemma.py
# Copyright © 2023-2024 Apple Inc.

import math
from dataclasses import dataclass
from typing import List, Literal, Optional

import mlx.core as mx
import mlx.nn as nn

from .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention
from .cache import MambaCache, RotatingKVCache


@dataclass
class ModelArgs(BaseModelArgs):
    model_type: str
    attention_bias: bool
    conv1d_width: int
    hidden_size: int
    intermediate_size: int
    logits_soft_cap: float
    num_attention_heads: int
    num_hidden_layers: int
    num_key_value_heads: int
    rms_norm_eps: float
    rope_theta: float
    attention_window_size: int
    vocab_size: int
    embeddings_scale_by_sqrt_dim: bool = True
    block_types: Optional[List[str]] = None
    _block_types: Optional[List[str]] = None

    def __post_init__(self):
        # For some reason these have different names in 2B and 9B
        if self.block_types is None:
            self.block_types = self._block_types


class RMSNorm(nn.Module):
    def __init__(self, dims: int, eps: float = 1e-5):
        super().__init__()
        self.weight = mx.ones((dims,))
        self.eps = eps

    def __call__(self, x):
        return mx.fast.rms_norm(x, 1.0 + self.weight, self.eps)


def rnn_scan(x, a, h0):
    assert x.ndim == 3
    assert a.shape == x.shape[-a.ndim :]
    assert a.dtype == x.dtype

    if x.shape[1] == 1:
        # Using scan in sampling mode.
        if h0 is None:
            return x, x[:, 0]

        else:
            y = a * h0[:, None] + x
            return y, y[:, -1]

    else:
        # Using scan in linear mode.
        if h0 is not None:
            h_t = h0
        else:
            B, _, D = x.shape
            h_t = mx.zeros((B, D), dtype=x.dtype)

        y = mx.zeros_like(x)
        for t in range(x.shape[1]):
            h_t = a[:, t] * h_t + x[:, t]
            y[:, t] = h_t

    return y, h_t


class Conv1d(nn.Module):
    def __init__(
        self,
        channels: int,
        kernel_size: int,
    ):
        super().__init__()
        self.weight = mx.zeros((channels, kernel_size, 1))
        self.bias = mx.zeros((channels,))

    def __call__(self, x, cache=None):
        B, L, C = x.shape
        groups, K, _ = self.weight.shape

        if cache is not None:
            x = mx.concatenate([cache, x], axis=1)
        else:
            x = mx.pad(x, [(0, 0), (K - 1, 0), (0, 0)])

        y = mx.conv_general(x, self.weight, groups=groups)
        y = y + self.bias

        return y, x[:, -K + 1 :, :]


class RGLRU(nn.Module):
    """A Real-Gated Linear Recurrent Unit (RG-LRU) layer."""

    def __init__(
        self,
        width: int,
        num_heads: int,
    ):
        super().__init__()
        self.width = width
        self.num_heads = num_heads
        self.head_dim = self.width // self.num_heads

        self.recurrent_param = mx.zeros((self.width,))

        self.input_gate_weight = mx.zeros(
            (self.num_heads, self.head_dim, self.head_dim),
        )
        self.input_gate_bias = mx.zeros((self.num_heads, self.head_dim))

        self.recurrent_gate_weight = mx.zeros(
            (self.num_heads, self.head_dim, self.head_dim),
        )
        self.recurrent_gate_bias = mx.zeros((self.num_heads, self.head_dim))

    def __call__(
        self,
        x: mx.array,
        cache=None,
    ):
        B, L, _ = x.shape

        def apply_block_linear(h, w, b):
            h = h.reshape((B, L, self.num_heads, self.head_dim))
            h = (h.swapaxes(1, 2) @ w).swapaxes(1, 2) + b
            return mx.sigmoid(h.flatten(2, 3))

        # Gates for x and a.
        gate_x = apply_block_linear(x, self.input_gate_weight, self.input_gate_bias)
        gate_a = apply_block_linear(
            x, self.recurrent_gate_weight, self.recurrent_gate_bias
        )

        # Compute the parameter `A` of the recurrence.
        log_a = -8.0 * gate_a * nn.softplus(self.recurrent_param)
        a = mx.exp(log_a)
        a_square = mx.exp(2 * log_a)

        # Gate the input.
        gated_x = x * gate_x

        # Apply gamma normalization to the input.
        multiplier = mx.sqrt(1 - a_square)
        if cache is None:
            multiplier[:, 0, :] = 1.0
        normalized_x = gated_x * multiplier.astype(x.dtype)

        y, last_h = rnn_scan(
            x=normalized_x,
            a=a,
            h0=cache,
        )

        return y, last_h


class RecurrentBlock(nn.Module):

    def __init__(
        self,
        width: int,
        num_heads: int,
        lru_width: int = None,
        conv1d_temporal_width: int = 4,
    ):
        super().__init__()
        self.width = width
        self.num_heads = num_heads
        self.lru_width = lru_width or width
        self.conv1d_temporal_width = conv1d_temporal_width

        self.linear_y = nn.Linear(width, self.lru_width)
        self.linear_x = nn.Linear(width, self.lru_width)
        self.linear_out = nn.Linear(self.lru_width, width)
        self.conv_1d = Conv1d(
            channels=self.lru_width,
            kernel_size=self.conv1d_temporal_width,
        )
        self.rg_lru = RGLRU(
            width=self.lru_width,
            num_heads=self.num_heads,
        )

    def __call__(
        self,
        x: mx.array,
        cache=None,
        mask=None,
    ):
        # y branch.
        y = self.linear_y(x)
        y = nn.gelu_approx(y)

        # x branch.
        x = self.linear_x(x)
        if cache is None:
            cache = [None, None]
        x, cache[0] = self.conv_1d(x=x, cache=cache[0])
        x, cache[1] = self.rg_lru(x=x, cache=cache[1])

        x = x * y
        x = self.linear_out(x)

        return x


class LocalAttentionBlock(nn.Module):

    def __init__(
        self,
        width: int,
        num_heads: int,
        window_size: int,
    ):
        super().__init__()
        self.width = width
        self.num_heads = num_heads
        self.window_size = window_size
        self.scale = (width // num_heads) ** (-0.5)

        self.head_dim = self.width // self.num_heads
        self.q_proj = nn.Linear(self.width, self.width, bias=False)
        self.k_proj = nn.Linear(self.width, self.head_dim, bias=False)
        self.v_proj = nn.Linear(self.width, self.head_dim, bias=False)
        self.o_proj = nn.Linear(self.width, self.width, bias=True)
        self.rope = nn.RoPE(
            self.head_dim // 2,
            traditional=False,
        )

    def __call__(
        self,
        x: mx.array,
        cache=None,
        mask=None,
    ):
        B, L, D = x.shape

        queries, keys, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)

        queries = queries.reshape(B, L, self.num_heads, -1).transpose(0, 2, 1, 3)
        keys = keys.reshape(B, L, 1, -1).transpose(0, 2, 1, 3)
        values = values.reshape(B, L, 1, -1).transpose(0, 2, 1, 3)

        if cache is not None:
            queries = self.rope(queries, offset=cache.offset)
            keys = self.rope(keys, offset=cache.offset)
            keys, values = cache.update_and_fetch(keys, values)
        else:
            queries = self.rope(queries)
            keys = self.rope(keys)

        output = scaled_dot_product_attention(
            queries, keys, values, cache=cache, scale=self.scale, mask=mask
        )
        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)
        return self.o_proj(output)


class MLPBlock(nn.Module):

    def __init__(self, width: int, expanded_width: int):
        super().__init__()
        self.up_proj = nn.Linear(width, expanded_width // 2)
        self.gate_proj = nn.Linear(width, expanded_width // 2)
        self.down_proj = nn.Linear(expanded_width // 2, width)

    def __call__(self, x: mx.array):
        gate = self.gate_proj(x)
        x = self.up_proj(x)
        return self.down_proj(nn.gelu_approx(gate) * x)


class ResidualBlock(nn.Module):

    def __init__(
        self,
        width: int,
        mlp_expanded_width: int,
        num_heads: int,
        attention_window_size: int,
        temporal_block_type: str,
        lru_width: Optional[int] = None,
        conv1d_temporal_width: int = 4,
    ):
        """Initializes the residual block.

        Args:
          width: The width of the block.
          mlp_expanded_width: The width of the expansion inside the MLP block.
          num_heads: The number of heads for the Attention or the RG-LRU.
          attention_window_size: The window size for the local attention block.
          temporal_block_type: Either "recurrent" or "attention", specifying the
            type of recurrent block to use.
          lru_width: The width of the RG-LRU if different from `width`.
          conv1d_temporal_width: The width of the temporal convolution.
        """
        super().__init__()
        self.width = width
        self.mlp_expanded_width = mlp_expanded_width
        self.num_heads = num_heads
        self.attention_window_size = attention_window_size
        self.temporal_block_type = temporal_block_type
        self.lru_width = lru_width
        self.conv1d_temporal_width = conv1d_temporal_width

        self.temporal_pre_norm = RMSNorm(width)
        if self.temporal_block_type == "recurrent":
            self.temporal_block = RecurrentBlock(
                width=self.width,
                num_heads=self.num_heads,
                lru_width=self.lru_width,
                conv1d_temporal_width=self.conv1d_temporal_width,
            )

        else:
            self.temporal_block = LocalAttentionBlock(
                width=self.width,
                num_heads=self.num_heads,
                window_size=self.attention_window_size,
            )

        self.channel_pre_norm = RMSNorm(width)
        self.mlp_block = MLPBlock(
            width=self.width,
            expanded_width=self.mlp_expanded_width,
        )

    def __call__(
        self,
        x: mx.array,
        cache=None,
        mask=None,
    ):
        raw_x = x

        inputs_normalized = self.temporal_pre_norm(raw_x)

        x = self.temporal_block(inputs_normalized, cache=cache, mask=mask)
        residual = x + raw_x

        x = self.channel_pre_norm(residual)
        x = self.mlp_block(x)

        x = x + residual

        return x


class Griffin(nn.Module):
    def __init__(self, config):
        super().__init__()

        self.config = config
        self.embed_tokens = nn.Embedding(
            config.vocab_size,
            config.hidden_size,
        )

        self.scale_by_sqrt_dim = config.embeddings_scale_by_sqrt_dim
        block_types = config.block_types

        self.layers = [
            ResidualBlock(
                width=config.hidden_size,
                mlp_expanded_width=config.intermediate_size,
                num_heads=config.num_attention_heads,
                attention_window_size=config.attention_window_size,
                temporal_block_type=block_types[i % len(block_types)],
                lru_width=None,
            )
            for i in range(config.num_hidden_layers)
        ]
        self.final_norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

    def __call__(
        self,
        tokens,
        mask: mx.array = None,
        cache=None,
    ):
        x = self.embed_tokens(tokens)
        if self.scale_by_sqrt_dim:
            x = x * math.sqrt(x.shape[-1])

        if cache is None:
            cache = [None] * len(self.layers)

        for i, block in enumerate(self.layers):
            if block.temporal_block_type != "recurrent":
                mask_cache = [cache[i]]

        if mask is None:
            mask = create_attention_mask(x, mask_cache)

        for i, block in enumerate(self.layers):
            x = block(x, mask=mask, cache=cache[i])

        return self.final_norm(x)


class Model(nn.Module):

    def __init__(self, config):
        self.args = config
        self.model = Griffin(config)
        self.model_type = config.model_type
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)

    def __call__(self, tokens: mx.array, mask: mx.array = None, cache=None) -> mx.array:
        """
        Args:
          tokens: Sequence of input tokens.
        """
        logits = self.model(tokens, mask=mask, cache=cache)
        if "lm_head" in self:
            logits = self.lm_head(logits)
        else:
            logits = self.model.embed_tokens.as_linear(logits)

        c = self.args.logits_soft_cap
        if c:
            logits = mx.tanh(logits / c) * c
        return logits

    @property
    def layers(self):
        return self.model.layers

    def sanitize(self, weights):
        for k, v in weights.items():
            if "conv_1d.weight" in k and v.shape[-1] != 1:
                weights[k] = v.moveaxis(2, 1)
        if "lm_head.weight" not in weights:
            self.pop("lm_head")
        return weights

    def make_cache(self):
        cache = []
        for layer in self.layers:
            if layer.temporal_block_type == "recurrent":
                cache.append(MambaCache())
            else:
                cache.append(RotatingKVCache(max_size=self.args.attention_window_size))
        return cache

>>>> llms/mlx_lm/models/openelm.py
# Copyright © 2023-2024 Apple Inc.

from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple, Union

import mlx.core as mx
import mlx.nn as nn

from .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention


@dataclass
class ModelArgs(BaseModelArgs):
    model_type: str
    head_dim: int
    num_transformer_layers: int
    model_dim: int
    vocab_size: int
    ffn_dim_divisor: int
    num_query_heads: List
    num_kv_heads: List
    ffn_multipliers: List
    ffn_with_glu: bool = True
    normalize_qk_projections: bool = True
    share_input_output_layers: bool = True
    rms_norm_eps: float = 1e-6
    rope_freq_constant: float = 10000


def make_divisible(
    v: Union[float, int],
    divisor: Optional[int] = 8,
    min_value: Optional[Union[float, int]] = None,
) -> Union[float, int]:
    """
    This function is taken from the original tf repo.
    It ensures that all layers have a channel number that is divisible by the divisor
    It can be seen at:
    https://github.com/tensorflow/models/blob/2cfc99eff5e5eb729c6793d2f3d03aa1c9be2b15/research/slim/nets/mobilenet/mobilenet.py#L62
    Args:
        v: input value
        divisor: default to 8
        min_value: minimum divisor value
    Returns:
        new_v: new divisible value
    """
    if min_value is None:
        min_value = divisor
    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)
    # Make sure that round down does not go down by more than 10%.
    if new_v < 0.9 * v:
        new_v += divisor
    return new_v


class Attention(nn.Module):
    def __init__(self, args: ModelArgs, layer_id: int):
        super().__init__()
        self.head_dim = head_dim = args.head_dim
        self.layer_id = layer_id
        self.model_dim = model_dim = args.model_dim

        self.n_heads = n_heads = args.num_query_heads[layer_id]
        self.n_kv_heads = n_kv_heads = args.num_kv_heads[layer_id]
        self.scale = head_dim**-0.5

        op_size = (n_heads + (n_kv_heads * 2)) * head_dim
        self.qkv_proj = nn.Linear(model_dim, op_size, bias=False)
        self.out_proj = nn.Linear(n_heads * head_dim, model_dim, bias=False)

        self.normalize_qk_projections = args.normalize_qk_projections

        if self.normalize_qk_projections:
            self.q_norm = nn.RMSNorm(head_dim, eps=args.rms_norm_eps)
            self.k_norm = nn.RMSNorm(head_dim, eps=args.rms_norm_eps)

        self.rope = nn.RoPE(head_dim, traditional=False, base=args.rope_freq_constant)

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        B, L, D = x.shape

        qkv = self.qkv_proj(x)

        qkv = qkv.reshape(
            B, L, self.n_heads + (self.n_kv_heads * 2), self.head_dim
        ).transpose(0, 2, 1, 3)

        queries, keys, values = mx.split(
            qkv, [self.n_heads, self.n_heads + self.n_kv_heads], axis=1
        )

        # Prepare the queries, keys and values for the attention computation
        if self.normalize_qk_projections:
            queries = self.q_norm(queries)
            keys = self.k_norm(keys)

        if cache is not None:
            queries = self.rope(queries, offset=cache.offset)
            keys = self.rope(keys, offset=cache.offset)
            keys, values = cache.update_and_fetch(keys, values)
        else:
            queries = self.rope(queries)
            keys = self.rope(keys)

        output = scaled_dot_product_attention(
            queries, keys, values, cache=cache, scale=self.scale, mask=mask
        )

        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)

        return self.out_proj(output)


class MLP(nn.Module):
    def __init__(self, args: ModelArgs, layer_id: int):
        super().__init__()
        self.args = args
        dim = args.model_dim
        ffn_multiplier = args.ffn_multipliers[layer_id]

        intermediate_dim = int(
            make_divisible(
                ffn_multiplier * args.model_dim,
                divisor=args.ffn_dim_divisor,
            )
        )

        self.proj_1 = nn.Linear(dim, 2 * intermediate_dim, bias=False)
        self.proj_2 = nn.Linear(intermediate_dim, dim, bias=False)

    def __call__(self, x) -> mx.array:
        x = self.proj_1(x)
        gate, x = mx.split(x, 2, axis=-1)
        return self.proj_2(nn.silu(gate) * x)


class TransformerBlock(nn.Module):
    def __init__(self, args: ModelArgs, layer_id: int):
        super().__init__()
        dim = args.model_dim
        self.attn = Attention(args, layer_id=layer_id)
        self.ffn = MLP(args, layer_id=layer_id)
        self.ffn_norm = nn.RMSNorm(dim, eps=args.rms_norm_eps)
        self.attn_norm = nn.RMSNorm(dim, eps=args.rms_norm_eps)

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        r = self.attn(self.attn_norm(x), mask, cache)
        h = x + r
        r = self.ffn(self.ffn_norm(h))
        out = h + r
        return out


class OpenELMModel(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.vocab_size = args.vocab_size
        self.num_transformer_layers = args.num_transformer_layers
        assert self.vocab_size > 0
        self.token_embeddings = nn.Embedding(args.vocab_size, args.model_dim)
        self.layers = [
            TransformerBlock(args, layer_id=layer_id)
            for layer_id in range(self.num_transformer_layers)
        ]
        self.norm = nn.RMSNorm(args.model_dim, eps=args.rms_norm_eps)

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        h = self.token_embeddings(inputs)

        if mask is None:
            mask = create_attention_mask(h, cache)

        if cache is None:
            cache = [None] * len(self.layers)

        for layer, c in zip(self.layers, cache):
            h = layer(h, mask, cache=c)

        return self.norm(h)


class Model(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.model_type = args.model_type
        self.transformer = OpenELMModel(args)
        if not args.share_input_output_layers:
            self.lm_head = nn.Linear(args.model_dim, args.vocab_size, bias=False)

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        out = self.transformer(inputs, mask, cache)
        if self.args.share_input_output_layers:
            out = self.transformer.token_embeddings.as_linear(out)
        else:
            out = self.lm_head(out)

        return out

    @property
    def layers(self):
        return self.transformer.layers

>>>> llms/mlx_lm/models/phi3.py
# Copyright © 2023-2024 Apple Inc.

from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple, Union

import mlx.core as mx
import mlx.nn as nn

from .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention
from .su_rope import SuScaledRotaryEmbedding


@dataclass
class ModelArgs(BaseModelArgs):
    model_type: str
    hidden_size: int
    num_hidden_layers: int
    intermediate_size: int
    num_attention_heads: int
    rms_norm_eps: float
    vocab_size: int
    num_key_value_heads: Optional[int] = None
    rope_theta: float = 10000
    rope_traditional: bool = False
    rope_scaling: Optional[Dict[str, Union[float, List[float]]]] = None
    partial_rotary_factor: float = 1.0
    max_position_embeddings: int = 131072
    original_max_position_embeddings: int = 4096
    tie_word_embeddings: bool = False

    def __post_init__(self):
        if self.num_key_value_heads is None:
            self.num_key_value_heads = self.num_attention_heads

        if self.rope_scaling:
            required_keys = {"long_factor", "type"}
            if not all(key in self.rope_scaling for key in required_keys):
                raise ValueError(f"rope_scaling must contain keys {required_keys}")

            if self.rope_scaling["type"] not in ["longrope", "su", "linear"]:
                print(
                    "[WARNING] rope_scaling 'type' currently only supports 'linear', 'su', and 'longrope'; setting rope scaling to false."
                )
                self.rope_scaling = None


class Attention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        dim = args.hidden_size
        self.n_heads = n_heads = args.num_attention_heads
        assert args.num_key_value_heads is not None
        self.n_kv_heads = n_kv_heads = args.num_key_value_heads
        self.num_hidden_layers = args.num_hidden_layers

        self.head_dim = head_dim = args.hidden_size // n_heads
        self.scale = head_dim**-0.5

        op_size = n_heads * head_dim + 2 * (n_kv_heads * head_dim)
        self.qkv_proj = nn.Linear(dim, op_size, bias=False)
        self.o_proj = nn.Linear(n_heads * head_dim, dim, bias=False)

        rope_dim = int(head_dim * args.partial_rotary_factor)
        if args.rope_scaling and args.rope_scaling["type"] in ["longrope", "su"]:
            self.rope = SuScaledRotaryEmbedding(
                rope_dim,
                base=args.rope_theta,
                max_position_embeddings=args.max_position_embeddings,
                original_max_position_embeddings=args.original_max_position_embeddings,
                short_factor=args.rope_scaling["short_factor"],
                long_factor=args.rope_scaling["long_factor"],
            )
        else:
            rope_scale = 1.0
            if args.rope_scaling and args.rope_scaling["type"] == "linear":
                assert isinstance(args.rope_scaling["factor"], float)
                rope_scale = 1 / args.rope_scaling["factor"]
            self.rope = nn.RoPE(
                rope_dim,
                traditional=args.rope_traditional,
                base=args.rope_theta,
                scale=rope_scale,
            )

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        B, L, D = x.shape

        qkv = self.qkv_proj(x)
        query_pos = self.n_heads * self.head_dim
        queries, keys, values = mx.split(
            qkv, [query_pos, query_pos + self.n_kv_heads * self.head_dim], axis=-1
        )

        # Prepare the queries, keys and values for the attention computation
        queries = queries.reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)
        keys = keys.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)
        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)

        if cache is not None:
            queries = self.rope(queries, offset=cache.offset)
            keys = self.rope(keys, offset=cache.offset)
            keys, values = cache.update_and_fetch(keys, values)
        else:
            queries = self.rope(queries)
            keys = self.rope(keys)

        output = scaled_dot_product_attention(
            queries, keys, values, cache=cache, scale=self.scale, mask=mask
        )
        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)
        return self.o_proj(output)


class MLP(nn.Module):
    def __init__(self, dim, hidden_dim):
        super().__init__()
        self.gate_up_proj = nn.Linear(dim, 2 * hidden_dim, bias=False)
        self.down_proj = nn.Linear(hidden_dim, dim, bias=False)

    def __call__(self, x) -> mx.array:
        x = self.gate_up_proj(x)
        gate, x = mx.split(x, 2, axis=-1)
        return self.down_proj(nn.silu(gate) * x)


class TransformerBlock(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.num_attention_heads = args.num_attention_heads
        self.hidden_size = args.hidden_size
        self.self_attn = Attention(args)
        self.mlp = MLP(args.hidden_size, args.intermediate_size)
        self.input_layernorm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)
        self.post_attention_layernorm = nn.RMSNorm(
            args.hidden_size, eps=args.rms_norm_eps
        )
        self.args = args

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
    ) -> mx.array:
        r = self.self_attn(self.input_layernorm(x), mask, cache)
        h = x + r
        r = self.mlp(self.post_attention_layernorm(h))
        out = h + r
        return out


class Phi3Model(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.vocab_size = args.vocab_size
        self.num_hidden_layers = args.num_hidden_layers
        assert self.vocab_size > 0
        self.embed_tokens = nn.Embedding(args.vocab_size, args.hidden_size)
        self.layers = [
            TransformerBlock(args=args) for _ in range(args.num_hidden_layers)
        ]
        self.norm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        h = self.embed_tokens(inputs)

        if mask is None:
            mask = create_attention_mask(h, cache)

        if cache is None:
            cache = [None] * len(self.layers)

        for layer, c in zip(self.layers, cache):
            h = layer(h, mask, c)

        return self.norm(h)


class Model(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.model_type = args.model_type
        self.model = Phi3Model(args)
        if not args.tie_word_embeddings:
            self.lm_head = nn.Linear(args.hidden_size, args.vocab_size, bias=False)
        self.args = args

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        out = self.model(inputs, mask, cache)
        if self.args.tie_word_embeddings:
            out = self.model.embed_tokens.as_linear(out)
        else:
            out = self.lm_head(out)
        return out

    @property
    def layers(self):
        return self.model.layers

>>>> llms/mlx_lm/models/hunyuan.py
# Copyright © 2023-2024 Apple Inc.

import math
from dataclasses import dataclass
from typing import Any, Dict, Optional, Tuple, Union

import mlx.core as mx
import mlx.nn as nn

from .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention
from .switch_layers import SwitchGLU


@dataclass
class ModelArgs(BaseModelArgs):
    model_type: str
    vocab_size: int
    hidden_size: int
    num_hidden_layers: int
    intermediate_size: int
    num_attention_heads: int
    num_key_value_heads: int
    attention_bias: bool
    moe_topk: int
    num_experts: int
    num_shared_expert: int
    use_mixed_mlp_moe: bool
    use_qk_norm: bool
    rms_norm_eps: float
    rope_theta: float
    use_cla: bool
    cla_share_factor: 2
    rope_scaling: Optional[Dict[str, Union[float, str]]] = None
    tie_word_embeddings: bool = False

    def __post_init__(self):

        if self.rope_scaling:
            required_keys = {"factor", "type"}
            if not all(key in self.rope_scaling for key in required_keys):
                raise ValueError(f"rope_scaling must contain keys {required_keys}")


class DynamicNTKAlphaRoPE(nn.Module):
    def __init__(
        self,
        dims: int,
        base: float = 10000,
        scaling_alpha: float = 1.0,
    ):
        super().__init__()
        self.dims = dims
        base = base * scaling_alpha ** (dims / (dims - 2))
        self._freqs = base ** (mx.arange(0, self.dims, 2) / self.dims)

    def __call__(self, x, offset: int = 0):
        return mx.fast.rope(
            x,
            self.dims,
            traditional=False,
            base=None,
            scale=1.0,
            offset=offset,
            freqs=self._freqs,
        )


class Attention(nn.Module):
    def __init__(self, kv_proj: bool, args: ModelArgs):
        super().__init__()

        dim = args.hidden_size
        self.n_heads = n_heads = args.num_attention_heads
        assert args.num_key_value_heads is not None
        self.n_kv_heads = n_kv_heads = args.num_key_value_heads

        head_dim = args.hidden_size // n_heads
        self.scale = head_dim**-0.5
        self.q_proj = nn.Linear(dim, n_heads * head_dim, bias=args.attention_bias)
        if kv_proj:
            self.k_proj = nn.Linear(
                dim, n_kv_heads * head_dim, bias=args.attention_bias
            )
            self.v_proj = nn.Linear(
                dim, n_kv_heads * head_dim, bias=args.attention_bias
            )
        self.o_proj = nn.Linear(n_heads * head_dim, dim, bias=args.attention_bias)
        self.use_qk_norm = args.use_qk_norm
        if self.use_qk_norm:
            self.query_layernorm = nn.RMSNorm(head_dim, args.rms_norm_eps)
            self.key_layernorm = nn.RMSNorm(head_dim, args.rms_norm_eps)

        self.rope = DynamicNTKAlphaRoPE(
            head_dim,
            base=args.rope_theta,
            scaling_alpha=args.rope_scaling["alpha"],
        )

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
        kv_states=None,
    ) -> mx.array:
        B, L, D = x.shape

        queries = self.q_proj(x)
        if kv_states is None:
            keys, values = self.k_proj(x), self.v_proj(x)
            kv_states = keys, values
        else:
            keys, values = kv_states

        # Prepare the queries, keys and values for the attention computation
        queries = queries.reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)
        keys = keys.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)
        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)

        offset = cache.offset if cache else 0
        queries = self.rope(queries, offset=offset)
        keys = self.rope(keys, offset=offset)
        if self.use_qk_norm:
            queries = self.query_layernorm(queries)
            keys = self.key_layernorm(keys)

        if cache is not None:
            keys, values = cache.update_and_fetch(keys, values)

        output = scaled_dot_product_attention(
            queries, keys, values, cache=cache, scale=self.scale, mask=mask
        )
        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)
        return self.o_proj(output), kv_states


class MLP(nn.Module):
    def __init__(self, dim, hidden_dim):
        super().__init__()
        self.gate_proj = nn.Linear(dim, hidden_dim, bias=False)
        self.down_proj = nn.Linear(hidden_dim, dim, bias=False)
        self.up_proj = nn.Linear(dim, hidden_dim, bias=False)

    def __call__(self, x) -> mx.array:
        return self.down_proj(nn.silu(self.gate_proj(x)) * self.up_proj(x))


class Gate(nn.Module):
    def __init__(self, dim, num_experts):
        super().__init__()
        self.wg = nn.Linear(dim, num_experts, bias=False)

    def __call__(self, x) -> mx.array:
        return self.wg(x)


class MoeBlock(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        dim = args.hidden_size
        intermediate_size = args.intermediate_size
        self.use_shared_mlp = args.use_mixed_mlp_moe

        if args.use_mixed_mlp_moe:
            self.shared_mlp = MLP(dim, intermediate_size * args.num_shared_expert)

        self.num_experts = num_experts = args.num_experts
        self.top_k = args.moe_topk

        self.gate = Gate(dim, num_experts)
        self.switch_mlp = SwitchGLU(dim, intermediate_size, num_experts)

    def __call__(
        self,
        x: mx.array,
    ):
        gates = self.gate(x)
        gates = mx.softmax(gates, axis=-1, precise=True)

        k = self.top_k
        inds = mx.stop_gradient(mx.argpartition(-gates, kth=k - 1, axis=-1)[..., :k])
        scores = mx.take_along_axis(gates, inds, axis=-1)

        y = self.switch_mlp(x, inds)
        y = (y * scores[..., None]).sum(axis=-2)

        if self.use_shared_mlp:
            shared_expert_output = self.shared_mlp(x)
            y = y + shared_expert_output

        return y


class DecoderLayer(nn.Module):
    def __init__(self, args: ModelArgs, kv_proj: bool):
        super().__init__()
        self.hidden_size = args.hidden_size
        self.self_attn = Attention(kv_proj, args)
        if args.num_experts == 1:
            self.mlp = MLP(args.hidden_size, args.intermediate_size)
        else:
            self.mlp = MoeBlock(args)

        self.input_layernorm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)
        self.post_attention_layernorm = nn.RMSNorm(
            args.hidden_size, eps=args.rms_norm_eps
        )
        self.args = args

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
        shared_kv_states: Optional[Tuple[mx.array, mx.array]] = None,
    ):
        r, shared_kv_states = self.self_attn(
            self.input_layernorm(x), mask, cache, shared_kv_states
        )
        h = x + r
        r = self.mlp(self.post_attention_layernorm(h))
        out = h + r
        return out, shared_kv_states


class HunYuanModel(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.vocab_size = args.vocab_size
        self.num_hidden_layers = args.num_hidden_layers
        assert self.vocab_size > 0
        self.embed_tokens = nn.Embedding(args.vocab_size, args.hidden_size)
        self.layers = [
            DecoderLayer(
                args=args,
                kv_proj=(not args.use_cla) or (i % args.cla_share_factor) == 0,
            )
            for i in range(args.num_hidden_layers)
        ]
        self.norm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        h = self.embed_tokens(inputs)

        if mask is None:
            mask = create_attention_mask(h, cache)

        if cache is None:
            cache = [None] * len(self.layers)

        for i, (layer, c) in enumerate(zip(self.layers, cache)):
            if (not self.args.use_cla) or i % self.args.cla_share_factor == 0:
                shared_kv_states = None
            h, shared_kv_states = layer(h, mask, c, shared_kv_states)

        return self.norm(h)


class Model(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.model_type = args.model_type
        self.model = HunYuanModel(args)

    def __call__(
        self,
        inputs: mx.array,
        mask: mx.array = None,
        cache=None,
    ):
        out = self.model(inputs, mask, cache)
        return self.model.embed_tokens.as_linear(out)

    def sanitize(self, weights):

        if "model.layers.0.mlp.gate_and_up_proj.weight" in weights:
            new_weights = {}
            D = self.args.hidden_size
            n_kv_heads = self.args.num_key_value_heads
            n_kv_groups = self.args.num_attention_heads // n_kv_heads
            head_dim = D // self.args.num_attention_heads
            for k, v in weights.items():
                if "qkv_proj" in k:
                    v = v.reshape(n_kv_heads, n_kv_groups + 2, head_dim, -1)
                    splits = v.split([n_kv_groups, n_kv_groups + 1], axis=1)
                    for k_up, v_new in zip(["q_proj", "k_proj", "v_proj"], splits):
                        k_new = k.replace("qkv_proj", k_up)
                        new_weights[k_new] = mx.flatten(v_new, 0, 2)
                elif "gate_and_up_proj" in k:
                    splits = v.split(2, axis=0)
                    for k_up, v_new in zip(["up_proj", "gate_proj"], splits):
                        k_new = k.replace("gate_and_up_proj", k_up)
                        new_weights[k_new] = v_new
                else:
                    new_weights[k] = v
            weights = new_weights

        if "model.layers.0.mlp.experts.0.up_proj.weight" not in weights:
            return weights
        for l in range(self.args.num_hidden_layers):
            prefix = f"model.layers.{l}"
            for n in ["up_proj", "down_proj", "gate_proj"]:
                for k in ["weight", "scales", "biases"]:
                    if f"{prefix}.mlp.experts.0.{n}.{k}" in weights:
                        to_join = [
                            weights.pop(f"{prefix}.mlp.experts.{e}.{n}.{k}")
                            for e in range(self.args.num_experts)
                        ]
                        weights[f"{prefix}.mlp.switch_mlp.{n}.{k}"] = mx.stack(to_join)
        return weights

    @property
    def layers(self):
        return self.model.layers

>>>> llms/mlx_lm/models/rope_utils.py
# Copyright © 2023-2024 Apple Inc.

from typing import Optional

import mlx.core as mx
import mlx.nn as nn


class Llama3RoPE(nn.Module):

    def __init__(
        self,
        dims: int,
        max_position_embeddings: int = 2048,
        traditional: bool = False,
        base: float = 10000,
        scaling_config: dict = None,
    ):
        super().__init__()
        self.dims = dims
        self.max_position_embeddings = max_position_embeddings
        self.traditional = traditional

        factor = scaling_config["factor"]
        low_freq_factor = scaling_config.get("low_freq_factor", 1.0)
        high_freq_factor = scaling_config.get("high_freq_factor", 4.0)
        old_context_len = scaling_config.get(
            "original_max_position_embeddings",
            8192,
        )

        low_freq_wavelen = old_context_len / low_freq_factor
        high_freq_wavelen = old_context_len / high_freq_factor

        freqs = base ** (mx.arange(0, dims, 2) / dims)
        wavelens = 2 * mx.pi * freqs

        freqs = mx.where(wavelens > low_freq_wavelen, freqs * factor, freqs)
        is_medium_freq = (wavelens > high_freq_wavelen) & (wavelens < low_freq_wavelen)
        smooth_factors = (old_context_len / wavelens - low_freq_factor) / (
            high_freq_factor - low_freq_factor
        )
        smooth_freqs = freqs / ((1 - smooth_factors) / factor + smooth_factors)
        self._freqs = mx.where(is_medium_freq, smooth_freqs, freqs)

    def extra_repr(self):
        return (
            f"{self.dims}, traditional={self.traditional}, "
            f"max_position_embeddings={self.max_position_embeddings}"
        )

    def __call__(self, x, offset: int = 0):
        return mx.fast.rope(
            x,
            self.dims,
            traditional=self.traditional,
            base=None,
            scale=1.0,
            offset=offset,
            freqs=self._freqs,
        )


def initialize_rope(
    dims,
    base,
    traditional,
    scaling_config: Optional[dict] = None,
    max_position_embeddings: Optional[int] = None,
):
    if scaling_config is not None:
        rope_type = scaling_config.get("type") or scaling_config.get(
            "rope_type", "default"
        )
    else:
        rope_type = "default"

    if rope_type in ["default", "linear"]:
        scale = 1 / scaling_config["factor"] if rope_type == "linear" else 1.0
        return nn.RoPE(dims, traditional=traditional, base=base, scale=scale)

    elif rope_type == "llama3":
        return Llama3RoPE(
            dims=dims,
            max_position_embeddings=max_position_embeddings,
            traditional=traditional,
            base=base,
            scaling_config=scaling_config,
        )
    else:
        raise ValueError(f"Unsupported RoPE type {rope_type}")

>>>> llms/mlx_lm/MANAGE.md
# Managing Models

You can use `mlx-lm` to manage models downloaded locally in your machine. They
are stored in the Hugging Face cache.

Scan models: 

```shell
mlx_lm.manage --scan
```

Specify a `--pattern` to get info on a single or specific set of models:

```shell
mlx_lm.manage --scan --pattern mlx-community/Mistral-7B-Instruct-v0.2-4bit
```

To delete a model (or multiple models):

```shell
mlx_lm.manage --delete --pattern mlx-community/Mistral-7B-Instruct-v0.2-4bit
```

>>>> llms/mlx_lm/LORA.md
# Fine-Tuning with LoRA or QLoRA

You can use use the `mlx-lm` package to fine-tune an LLM with low rank
adaptation (LoRA) for a target task.[^lora] The example also supports quantized
LoRA (QLoRA).[^qlora] LoRA fine-tuning works with the following model families:

- Mistral
- Llama
- Phi2
- Mixtral
- Qwen2
- Gemma
- OLMo
- MiniCPM
- InternLM2

## Contents

- [Run](#Run)
  - [Fine-tune](#Fine-tune)
  - [Evaluate](#Evaluate)
  - [Generate](#Generate)
- [Fuse](#Fuse)
- [Data](#Data)
- [Memory Issues](#Memory-Issues)

## Run

The main command is `mlx_lm.lora`. To see a full list of command-line options run:

```shell
mlx_lm.lora --help
```

Note, in the following the `--model` argument can be any compatible Hugging
Face repo or a local path to a converted model.

You can also specify a YAML config with `-c`/`--config`. For more on the format see the
[example YAML](examples/lora_config.yaml). For example:

```shell
mlx_lm.lora --config /path/to/config.yaml
```

If command-line flags are also used, they will override the corresponding
values in the config.

### Fine-tune

To fine-tune a model use:

```shell
mlx_lm.lora 
    --model <path_to_model> 
    --train 
    --data <path_to_data> 
    --iters 600
```

To fine-tune the full model weights, add the `--fine-tune-type full` flag.
Currently supported fine-tuning types are `lora` (default), `dora`, and `full`.

The `--data` argument must specify a path to a `train.jsonl`, `valid.jsonl`
when using `--train` and a path to a `test.jsonl` when using `--test`. For more
details on the data format see the section on [Data](#Data).

For example, to fine-tune a Mistral 7B you can use `--model
mistralai/Mistral-7B-v0.1`.

If `--model` points to a quantized model, then the training will use QLoRA,
otherwise it will use regular LoRA.

By default, the adapter config and learned weights are saved in `adapters/`.
You can specify the output location with `--adapter-path`.

You can resume fine-tuning with an existing adapter with
`--resume-adapter-file <path_to_adapters.safetensors>`.

#### Prompt Masking

The default training computes a loss for every token in the sample. You can
ignore the prompt and compute loss for just the completion by passing
`--mask-prompt`. Note this is only supported for `chat` and `completion`
datasets. For `chat` datasets the final message in the message list is
considered the completion. See the [dataset section](#Data) for more details. 

### Evaluate

To compute test set perplexity use:

```shell
mlx_lm.lora 
    --model <path_to_model> 
    --adapter-path <path_to_adapters> 
    --data <path_to_data> 
    --test
```

### Generate

For generation use `mlx_lm.generate`:

```shell
mlx_lm.generate 
    --model <path_to_model> 
    --adapter-path <path_to_adapters> 
    --prompt "<your_model_prompt>"
```

## Fuse

You can generate a model fused with the low-rank adapters using the
`mlx_lm.fuse` command. This command also allows you to optionally:

- Upload the fused model to the Hugging Face Hub.
- Export the fused model to GGUF. Note GGUF support is limited to Mistral,
  Mixtral, and Llama style models in fp16 precision.

To see supported options run:

```shell
mlx_lm.fuse --help
```

To generate the fused model run:

```shell
mlx_lm.fuse --model <path_to_model>
```

This will by default load the adapters from `adapters/`, and save the fused
model in the path `fused_model/`. All of these are configurable.

To upload a fused model, supply the `--upload-repo` and `--hf-path` arguments
to `mlx_lm.fuse`. The latter is the repo name of the original model, which is
useful for the sake of attribution and model versioning.

For example, to fuse and upload a model derived from Mistral-7B-v0.1, run:

```shell
mlx_lm.fuse 
    --model mistralai/Mistral-7B-v0.1 
    --upload-repo mlx-community/my-lora-mistral-7b 
    --hf-path mistralai/Mistral-7B-v0.1
```

To export a fused model to GGUF, run:

```shell
mlx_lm.fuse 
    --model mistralai/Mistral-7B-v0.1 
    --export-gguf
```

This will save the GGUF model in `fused_model/ggml-model-f16.gguf`. You
can specify the file name with `--gguf-path`.

## Data

The LoRA command expects you to provide a dataset with `--data`. The MLX
Examples GitHub repo has an [example of the WikiSQL
data](https://github.com/ml-explore/mlx-examples/tree/main/lora/data) in the
correct format.

Datasets can be specified in `*.jsonl` files locally or loaded from Hugging
Face. 

### Local Datasets

For fine-tuning (`--train`), the data loader expects a `train.jsonl` and a
`valid.jsonl` to be in the data directory. For evaluation (`--test`), the data
loader expects a `test.jsonl` in the data directory. 

Currently, `*.jsonl` files support `chat`, `tools`, `completions`, and `text`
data formats. Here are examples of these formats:

`chat`:

```jsonl
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "Hello."}, {"role": "assistant", "content": "How can I assistant you today."}]}
```

`tools`:

```jsonl
{"messages":[{"role":"user","content":"What is the weather in San Francisco?"},{"role":"assistant","tool_calls":[{"id":"call_id","type":"function","function":{"name":"get_current_weather","arguments":"{\"location\": \"San Francisco, USA\", \"format\": \"celsius\"}"}}]}],"tools":[{"type":"function","function":{"name":"get_current_weather","description":"Get the current weather","parameters":{"type":"object","properties":{"location":{"type":"string","description":"The city and country, eg. San Francisco, USA"},"format":{"type":"string","enum":["celsius","fahrenheit"]}},"required":["location","format"]}}}]}
```

<details>
<summary>View the expanded single data tool format</summary>

```jsonl
{
    "messages": [
        { "role": "user", "content": "What is the weather in San Francisco?" },
        {
            "role": "assistant",
            "tool_calls": [
                {
                    "id": "call_id",
                    "type": "function",
                    "function": {
                        "name": "get_current_weather",
                        "arguments": "{\"location\": \"San Francisco, USA\", \"format\": \"celsius\"}"
                    }
                }
            ]
        }
    ],
    "tools": [
        {
            "type": "function",
            "function": {
                "name": "get_current_weather",
                "description": "Get the current weather",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "location": {
                            "type": "string",
                            "description": "The city and country, eg. San Francisco, USA"
                        },
                        "format": { "type": "string", "enum": ["celsius", "fahrenheit"] }
                    },
                    "required": ["location", "format"]
                }
            }
        }
    ]
}
```


The format for the `arguments` field in a function varies for different models.
Common formats include JSON strings and dictionaries. The example provided
follows the format used by
[OpenAI](https://platform.openai.com/docs/guides/fine-tuning/fine-tuning-examples)
and [Mistral
AI](https://github.com/mistralai/mistral-finetune?tab=readme-ov-file#instruct).
A dictionary format is used in Hugging Face's [chat
templates](https://huggingface.co/docs/transformers/main/en/chat_templating#a-complete-tool-use-example).
Refer to the documentation for the model you are fine-tuning for more details.

</details>

`completions`:

```jsonl
{"prompt": "What is the capital of France?", "completion": "Paris."}
```

For the `completions` data format, a different key can be used for the prompt
and completion by specifying the following in the YAML config:

```yaml
prompt_feature: "input"
completion_feature: "output"
```

Here, `"input"` is the expected key instead of the default `"prompt"`, and
`"output"` is the expected key instead of `"completion"`. 

`text`:

```jsonl
{"text": "This is an example for the model."}
```

Note, the format is automatically determined by the dataset. Note also, keys
in each line not expected by the loader will be ignored.

> [!NOTE]
> Each example in the datasets must be on a single line. Do not put more than
> one example per line and do not split an example across multiple lines.

### Hugging Face Datasets

To use Hugging Face datasets, first install the `datasets` package:

```
pip install datasets
```

If the Hugging Face dataset is already in a supported format, you can specify
it on the command line. For example, pass `--data mlx-community/wikisql` to
train on the pre-formatted WikiwSQL data.

Otherwise, provide a mapping of keys in the dataset to the features MLX LM
expects. Use a YAML config to specify the Hugging Face dataset arguments. For
example:

```yaml
hf_dataset:
  name: "billsum"
  prompt_feature: "text"
  completion_feature: "summary"
```

- Use `prompt_feature` and `completion_feature` to specify keys for a
  `completions` dataset. Use `text_feature` to specify the key for a `text`
  dataset. Use `chat_feature` to specify the key for a chat dataset.

- To specify the train, valid, or test splits, set the corresponding
  `{train,valid,test}_split` argument. 

You can specify a list of Hugging Face datasets with a list of records each
with the same structure as above. For example:

```yaml
hf_dataset:
  - name: "Open-Orca/OpenOrca"
    train_split: "train[:90%]"
    valid_split: "train[-10%:]"
    prompt_feature: "question"
    completion_feature: "response"
  - name: "trl-lib/ultrafeedback_binarized"
    train_split: "train[:90%]"
    valid_split: "train[-10%:]"
    chat_feature: "chosen"
```

- Arguments specified in `config` will be passed as keyword arguments to
  [`datasets.load_dataset`](https://huggingface.co/docs/datasets/v2.20.0/en/package_reference/loading_methods#datasets.load_dataset).

In general, for the `chat`, `tools` and `completions` formats, Hugging Face
[chat
templates](https://huggingface.co/docs/transformers/main/en/chat_templating)
are used. This applies the model's chat template by default. If the model does
not have a chat template, then Hugging Face will use a default. For example,
the final text in the `chat` example above with Hugging Face's default template
becomes:

```text
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
Hello.<|im_end|>
<|im_start|>assistant
How can I assistant you today.<|im_end|>
```

If you are unsure of the format to use, the `chat` or `completions` are good to
start with. For custom requirements on the format of the dataset, use the
`text` format to assemble the content yourself.

## Memory Issues

Fine-tuning a large model with LoRA requires a machine with a decent amount
of memory. Here are some tips to reduce memory use should you need to do so:

1. Try quantization (QLoRA). You can use QLoRA by generating a quantized model
   with `convert.py` and the `-q` flag. See the [Setup](#setup) section for
   more details.

2. Try using a smaller batch size with `--batch-size`. The default is `4` so
   setting this to `2` or `1` will reduce memory consumption. This may slow
   things down a little, but will also reduce the memory use.

3. Reduce the number of layers to fine-tune with `--num-layers`. The default
   is `16`, so you can try `8` or `4`. This reduces the amount of memory
   needed for back propagation. It may also reduce the quality of the
   fine-tuned model if you are fine-tuning with a lot of data.

4. Longer examples require more memory. If it makes sense for your data, one thing
   you can do is break your examples into smaller
   sequences when making the `{train, valid, test}.jsonl` files.

5. Gradient checkpointing lets you trade-off memory use (less) for computation
   (more) by recomputing instead of storing intermediate values needed by the
   backward pass. You can use gradient checkpointing by passing the
   `--grad-checkpoint` flag. Gradient checkpointing will be more helpful for
   larger batch sizes or sequence lengths with smaller or quantized models.

For example, for a machine with 32 GB the following should run reasonably fast:

```
mlx_lm.lora 
    --model mistralai/Mistral-7B-v0.1 
    --train 
    --batch-size 1 
    --num-layers 4 
    --data wikisql
```

The above command on an M1 Max with 32 GB runs at about 250
tokens-per-second, using the MLX Example
[`wikisql`](https://github.com/ml-explore/mlx-examples/tree/main/lora/data)
data set.

[^lora]: Refer to the [arXiv paper](https://arxiv.org/abs/2106.09685) for more details on LoRA.

[^qlora]: Refer to the paper [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)

>>>> llms/mlx_lm/convert.py
# Copyright © 2023-2024 Apple Inc.

import argparse

from . import utils
from .utils import convert

QUANT_RECIPES = [
    "mixed_2_6",
    "mixed_3_6",
]


def quant_args(arg):
    if arg not in QUANT_RECIPES:
        raise argparse.ArgumentTypeError(
            f"Invalid q-recipe {arg!r}. Choose from: {QUANT_RECIPES}"
        )
    else:
        return getattr(utils, arg)


def configure_parser() -> argparse.ArgumentParser:
    """
    Configures and returns the argument parser for the script.

    Returns:
        argparse.ArgumentParser: Configured argument parser.
    """
    parser = argparse.ArgumentParser(
        description="Convert Hugging Face model to MLX format"
    )

    parser.add_argument("--hf-path", type=str, help="Path to the Hugging Face model.")
    parser.add_argument(
        "--mlx-path", type=str, default="mlx_model", help="Path to save the MLX model."
    )
    parser.add_argument(
        "-q", "--quantize", help="Generate a quantized model.", action="store_true"
    )
    parser.add_argument(
        "--q-group-size", help="Group size for quantization.", type=int, default=64
    )
    parser.add_argument(
        "--q-bits", help="Bits per weight for quantization.", type=int, default=4
    )
    parser.add_argument(
        "--quant-predicate",
        help=f"Mixed-bit quantization recipe. Choices: {QUANT_RECIPES}",
        type=quant_args,
        required=False,
    )
    parser.add_argument(
        "--dtype",
        help="Type to save the non-quantized parameters.",
        type=str,
        choices=["float16", "bfloat16", "float32"],
        default="float16",
    )
    parser.add_argument(
        "--upload-repo",
        help="The Hugging Face repo to upload the model to.",
        type=str,
        default=None,
    )
    parser.add_argument(
        "-d",
        "--dequantize",
        help="Dequantize a quantized model.",
        action="store_true",
        default=False,
    )
    return parser


def main():
    parser = configure_parser()
    args = parser.parse_args()
    convert(**vars(args))


if __name__ == "__main__":
    main()

>>>> llms/mlx_lm/tokenizer_utils.py
import json
from functools import partial
from typing import List

from transformers import AutoTokenizer


class StreamingDetokenizer:
    """The streaming detokenizer interface so that we can detokenize one token at a time.

    Example usage is as follows:

        detokenizer = ...

        # Reset the tokenizer state
        detokenizer.reset()

        for token in generate(...):
            detokenizer.add_token(token.item())

            # Contains the whole text so far. Some tokens may not be included
            # since it contains whole words usually.
            detokenizer.text

            # Contains the printable segment (usually a word) since the last
            # time it was accessed
            detokenizer.last_segment

            # Contains all the tokens added so far
            detokenizer.tokens

        # Make sure that we detokenize any remaining tokens
        detokenizer.finalize()

        # Now detokenizer.text should match tokenizer.decode(detokenizer.tokens)
    """

    __slots__ = ("text", "tokens", "offset")

    def reset(self):
        raise NotImplementedError()

    def add_token(self, token):
        raise NotImplementedError()

    def finalize(self):
        raise NotImplementedError()

    @property
    def last_segment(self):
        """Return the last segment of readable text since last time this property was accessed."""
        text = self.text
        segment = text[self.offset :]
        self.offset = len(text)
        return segment


class NaiveStreamingDetokenizer(StreamingDetokenizer):
    """NaiveStreamingDetokenizer relies on the underlying tokenizer
    implementation and should work with every tokenizer.

    Its complexity is O(T^2) where T is the longest line since it will
    repeatedly detokenize the same tokens until a new line is generated.
    """

    def __init__(self, tokenizer):
        self._tokenizer = tokenizer
        self._tokenizer.decode([0])
        self.reset()

    def reset(self):
        self.offset = 0
        self.tokens = []
        self._text = ""
        self._current_tokens = []
        self._current_text = ""

    def add_token(self, token):
        self._current_tokens.append(token)
        self.tokens.append(token)

    def finalize(self):
        self._text += self._tokenizer.decode(self._current_tokens)
        self._current_tokens = []
        self._current_text = ""

    @property
    def text(self):
        if self._current_tokens:
            self._current_text = self._tokenizer.decode(self._current_tokens)
            if (
                self._tokenizer.clean_up_tokenization_spaces
                and self._current_text[-1] == " "
            ):
                self._current_text = self._current_text[:-1]
        if self._current_text and self._current_text[-1] == "\n":
            self._text += self._current_text
            self._current_tokens.clear()
            self._current_text = ""
        return self._text + self._current_text


class SPMStreamingDetokenizer(StreamingDetokenizer):
    """A streaming detokenizer for SPM models.

    It adds tokens to the text if the next token starts with the special SPM
    underscore which results in linear complexity.
    """

    def __init__(self, tokenizer, trim_space=True):
        self.trim_space = trim_space
        self._sep = "\u2581".encode()

        # Extract the tokens in a list from id to text
        self.tokenmap = [""] * (max(tokenizer.vocab.values()) + 1)
        for value, tokenid in tokenizer.vocab.items():
            if value.startswith("<0x"):
                # Replace bytes with their value
                self.tokenmap[tokenid] = bytes([int(value[3:5], 16)])
            else:
                self.tokenmap[tokenid] = value.encode()

        self.reset()

    def reset(self):
        self.offset = 0
        self._unflushed = b""
        self.text = ""
        self.tokens = []

    def _try_flush(self, force=False):
        text = self._unflushed.replace(self._sep, b" ").decode("utf-8", "replace")
        if not force and text.endswith("\ufffd"):
            return
        if not self.text and self.trim_space and text and text[0] == " ":
            text = text[1:]
        self.text += text
        self._unflushed = b""

    def add_token(self, token):
        self.tokens.append(token)
        v = self.tokenmap[token]
        self._unflushed += v
        self._try_flush()

    def finalize(self):
        self._try_flush(force=True)
        self._unflushed = b""


class BPEStreamingDetokenizer(StreamingDetokenizer):
    """A streaming detokenizer for OpenAI style BPE models.

    It adds tokens to the text if the next token starts with a space similar to
    the SPM detokenizer.
    """

    _byte_decoder = None
    _space_matches = (".", "?", "!", ",", "n't", "'m", "'s", "'ve", "'re")

    def __init__(self, tokenizer):
        self.clean_spaces = tokenizer.clean_up_tokenization_spaces

        # Extract the tokens in a list from id to text
        self.tokenmap = [None] * len(tokenizer.vocab)
        for value, tokenid in tokenizer.vocab.items():
            self.tokenmap[tokenid] = value

        self.reset()

        # Make the BPE byte decoder from
        # https://github.com/openai/gpt-2/blob/master/src/encoder.py
        self.make_byte_decoder()

    def reset(self):
        self.offset = 0
        self._unflushed = ""
        self.text = ""
        self.tokens = []

    def _decode_bytes(self, seq):
        barr = bytearray()
        for c in seq:
            res = self._byte_decoder.get(c, False)
            if res:
                barr.append(res)
            else:
                barr.extend(bytes(c, "utf-8"))
        return barr.decode("utf-8", "replace")

    def _maybe_trim_space(self, current_text):
        if len(current_text) == 0:
            return current_text
        elif current_text[0] != " ":
            return current_text
        elif not self.text:
            return current_text[1:]
        elif self.clean_spaces and current_text[1:].startswith(self._space_matches):
            return current_text[1:]
        return current_text

    def add_token(self, token):
        self.tokens.append(token)
        v = self.tokenmap[token]
        self._unflushed += v
        text = self._decode_bytes(self._unflushed)

        # For multi-byte utf-8 wait until they are complete
        # For single spaces wait until the next token to clean it if needed
        if not text.endswith("\ufffd") and not (
            len(v) == 1 and self._byte_decoder[v[0]] == 32
        ):
            self.text += self._maybe_trim_space(text)
            self._unflushed = ""

    def finalize(self):
        current_text = bytearray(self._byte_decoder[c] for c in self._unflushed).decode(
            "utf-8",
            "replace",
        )
        self.text += self._maybe_trim_space(current_text)
        self._unflushed = ""

    @classmethod
    def make_byte_decoder(cls):
        """See https://github.com/openai/gpt-2/blob/master/src/encoder.py for the rationale."""
        if cls._byte_decoder is not None:
            return

        char_to_bytes = {}
        limits = [
            0,
            ord("!"),
            ord("~") + 1,
            ord("¡"),
            ord("¬") + 1,
            ord("®"),
            ord("ÿ") + 1,
        ]
        n = 0
        for i, (start, stop) in enumerate(zip(limits, limits[1:])):
            if i % 2 == 0:
                for b in range(start, stop):
                    char_to_bytes[chr(2**8 + n)] = b
                    n += 1
            else:
                for b in range(start, stop):
                    char_to_bytes[chr(b)] = b
        cls._byte_decoder = char_to_bytes


class TokenizerWrapper:
    """A wrapper that combines an HF tokenizer and a detokenizer.

    Accessing any attribute other than the ``detokenizer`` is forwarded to the
    huggingface tokenizer.
    """

    def __init__(
        self, tokenizer, detokenizer_class=NaiveStreamingDetokenizer, eos_token_ids=None
    ):
        self._tokenizer = tokenizer
        self._detokenizer = detokenizer_class(tokenizer)
        self._eos_token_ids = (
            set(eos_token_ids)
            if eos_token_ids is not None
            else {tokenizer.eos_token_id}
        )

    def add_eos_token(self, token: str):
        token_id = None
        try:
            token_id = int(token)
        except ValueError:
            token_id = self._tokenizer.convert_tokens_to_ids(token)

        if token_id is None:
            raise ValueError(f"'{token}' is not a token for this tokenizer")

        self._eos_token_ids.add(token_id)

    def __getattr__(self, attr):
        if attr == "detokenizer":
            return self._detokenizer
        elif attr == "eos_token_ids":
            return self._eos_token_ids
        elif attr.startswith("_"):
            return self.__getattribute__(attr)
        else:
            return getattr(self._tokenizer, attr)

    def __setattr__(self, attr, value):
        if attr in {"detokenizer", "eos_token_ids"}:
            if attr == "detokenizer":
                raise AttributeError("Cannot set the detokenizer.")
            elif attr == "eos_token_ids":
                self._eos_token_ids = set(value) if value is not None else set()
        elif attr.startswith("_"):
            super().__setattr__(attr, value)
        else:
            setattr(self._tokenizer, attr, value)


def _match(a, b):
    if type(a) != type(b):
        return False
    if isinstance(a, dict):
        return len(a) == len(b) and all(k in b and _match(a[k], b[k]) for k in a)
    if isinstance(a, list):
        return len(a) == len(b) and all(_match(ai, bi) for ai, bi in zip(a, b))

    return a == b


def _is_spm_decoder(decoder):
    _target_description = {
        "type": "Sequence",
        "decoders": [
            {"type": "Replace", "pattern": {"String": "▁"}, "content": " "},
            {"type": "ByteFallback"},
            {"type": "Fuse"},
            {"type": "Strip", "content": " ", "start": 1, "stop": 0},
        ],
    }
    return _match(_target_description, decoder)


def _is_spm_decoder_no_space(decoder):
    _target_description = {
        "type": "Sequence",
        "decoders": [
            {"type": "Replace", "pattern": {"String": "▁"}, "content": " "},
            {"type": "ByteFallback"},
            {"type": "Fuse"},
        ],
    }
    return _match(_target_description, decoder)


def _is_bpe_decoder(decoder):
    return isinstance(decoder, dict) and decoder.get("type", None) == "ByteLevel"


def load_tokenizer(model_path, tokenizer_config_extra={}, eos_token_ids=None):
    """Load a huggingface tokenizer and try to infer the type of streaming
    detokenizer to use.

    Note, to use a fast streaming tokenizer, pass a local file path rather than
    a Hugging Face repo ID.
    """
    detokenizer_class = NaiveStreamingDetokenizer

    tokenizer_file = model_path / "tokenizer.json"
    if tokenizer_file.exists():
        with open(tokenizer_file, "r") as fid:
            tokenizer_content = json.load(fid)
        if "decoder" in tokenizer_content:
            if _is_spm_decoder(tokenizer_content["decoder"]):
                detokenizer_class = SPMStreamingDetokenizer
            elif _is_spm_decoder_no_space(tokenizer_content["decoder"]):
                detokenizer_class = partial(SPMStreamingDetokenizer, trim_space=False)
            elif _is_bpe_decoder(tokenizer_content["decoder"]):
                detokenizer_class = BPEStreamingDetokenizer

    if isinstance(eos_token_ids, int):
        eos_token_ids = [eos_token_ids]
    return TokenizerWrapper(
        AutoTokenizer.from_pretrained(model_path, **tokenizer_config_extra),
        detokenizer_class,
        eos_token_ids=eos_token_ids,
    )


def no_bos_or_eos(sequence: List, bos: int, eos: int) -> List:
    removed_bos = sequence if sequence[0] != bos else sequence[1:]
    return removed_bos[:-1] if removed_bos[-1] == eos else removed_bos

>>>> llms/mlx_lm/cache_prompt.py
# Copyright © 2024 Apple Inc.

import argparse
import json
import sys
import time

import mlx.core as mx

from .models.cache import make_prompt_cache, save_prompt_cache
from .utils import generate_step, load

DEFAULT_QUANTIZED_KV_START = 5000


def setup_arg_parser():
    """Set up and return the argument parser."""
    parser = argparse.ArgumentParser(
        description="Cache the state of a prompt to be reused with mlx_lm.generate"
    )
    parser.add_argument(
        "--model",
        type=str,
        default="mlx_model",
        help="The path to the local model directory or Hugging Face repo.",
    )
    parser.add_argument(
        "--adapter-path",
        type=str,
        help="Optional path for the trained adapter weights and config.",
    )
    parser.add_argument(
        "--trust-remote-code",
        action="store_true",
        help="Enable trusting remote code for tokenizer",
    )
    parser.add_argument(
        "--eos-token",
        type=str,
        default=None,
        help="End of sequence token for tokenizer",
    )
    parser.add_argument(
        "--ignore-chat-template",
        action="store_true",
        help="Use the raw prompt without the tokenizer's chat template.",
    )
    parser.add_argument(
        "--use-default-chat-template",
        action="store_true",
        help="Use the default chat template",
    )
    parser.add_argument(
        "--max-kv-size",
        type=int,
        default=None,
        help="Set the maximum key-value cache size",
    )
    parser.add_argument(
        "--prompt-cache-file",
        help="The file to save the prompt cache in",
        required=True,
    )
    parser.add_argument(
        "--prompt",
        required=True,
        help="Message to be processed by the model ('-' reads from stdin)",
    )
    parser.add_argument(
        "--kv-bits",
        type=int,
        help="Number of bits for KV cache quantization. "
        "Defaults to no quantization.",
        default=None,
    )
    parser.add_argument(
        "--kv-group-size",
        type=int,
        help="Group size for KV cache quantization.",
        default=64,
    )
    parser.add_argument(
        "--quantized-kv-start",
        help="When --kv-bits is set, start quantizing the KV cache "
        "from this step onwards.",
        type=int,
        default=DEFAULT_QUANTIZED_KV_START,
    )
    return parser


def main():
    parser = setup_arg_parser()
    args = parser.parse_args()

    # Building tokenizer_config
    tokenizer_config = {"trust_remote_code": True if args.trust_remote_code else None}
    if args.eos_token is not None:
        tokenizer_config["eos_token"] = args.eos_token

    model, tokenizer = load(
        args.model,
        adapter_path=args.adapter_path,
        tokenizer_config=tokenizer_config,
    )

    args.prompt = sys.stdin.read() if args.prompt == "-" else args.prompt

    if args.use_default_chat_template:
        if tokenizer.chat_template is None:
            tokenizer.chat_template = tokenizer.default_chat_template

    if not args.ignore_chat_template and tokenizer.chat_template is not None:
        messages = [{"role": "user", "content": args.prompt}]
        prompt = tokenizer.apply_chat_template(
            messages, add_generation_prompt=False, continue_final_message=True
        )

    else:
        prompt = tokenizer.encode(args.prompt)

    cache = make_prompt_cache(model, args.max_kv_size)
    y = mx.array(prompt)

    # Process the prompt
    start = time.time()
    max_msg_len = 0

    def callback(processed, total_tokens):
        current = time.time()
        speed = processed / (current - start)
        msg = f"\rProcessed {processed:6d} tokens ({speed:6.2f} tok/s)"
        nonlocal max_msg_len
        max_msg_len = max(max_msg_len, len(msg))
        print(msg + " " * (max_msg_len - len(msg)), end="", flush=True)

    for _ in generate_step(
        y,
        model,
        max_tokens=0,
        prompt_cache=cache,
        kv_bits=args.kv_bits,
        kv_group_size=args.kv_group_size,
        quantized_kv_start=args.quantized_kv_start,
        prompt_progress_callback=callback,
    ):
        pass

    print()
    print(f"Peak memory: {mx.metal.get_peak_memory() / 1e9:.3f} GB")

    print("Saving...")
    metadata = {}
    metadata["model"] = args.model
    metadata["chat_template"] = json.dumps(tokenizer.chat_template)
    metadata["tokenizer_config"] = json.dumps(tokenizer_config)
    save_prompt_cache(args.prompt_cache_file, cache, metadata)


if __name__ == "__main__":
    main()

>>>> llms/mlx_lm/generate.py
# Copyright © 2023-2024 Apple Inc.

import argparse
import json
import sys

import mlx.core as mx

from .models.cache import QuantizedKVCache, load_prompt_cache
from .sample_utils import make_sampler
from .utils import generate, load

DEFAULT_PROMPT = "hello"
DEFAULT_MAX_TOKENS = 100
DEFAULT_TEMP = 0.0
DEFAULT_TOP_P = 1.0
DEFAULT_MIN_P = 0.0
DEFAULT_MIN_TOKENS_TO_KEEP = 1
DEFAULT_SEED = None
DEFAULT_MODEL = "mlx-community/Llama-3.2-3B-Instruct-4bit"
DEFAULT_QUANTIZED_KV_START = 5000


def str2bool(string):
    return string.lower() not in ["false", "f"]


def setup_arg_parser():
    """Set up and return the argument parser."""
    parser = argparse.ArgumentParser(description="LLM inference script")
    parser.add_argument(
        "--model",
        type=str,
        help=(
            "The path to the local model directory or Hugging Face repo. "
            f"If no model is specified, then {DEFAULT_MODEL} is used."
        ),
        default=None,
    )
    parser.add_argument(
        "--adapter-path",
        type=str,
        help="Optional path for the trained adapter weights and config.",
    )
    parser.add_argument(
        "--extra-eos-token",
        type=str,
        default=(),
        nargs="+",
        help="Add tokens in the list of eos tokens that stop generation.",
    )
    parser.add_argument(
        "--system-prompt",
        default=None,
        help="System prompt to be used for the chat template",
    )
    parser.add_argument(
        "--prompt",
        "-p",
        default=DEFAULT_PROMPT,
        help="Message to be processed by the model ('-' reads from stdin)",
    )
    parser.add_argument(
        "--prefill-response",
        default=None,
        help="Prefill response to be used for the chat template",
    )
    parser.add_argument(
        "--max-tokens",
        "-m",
        type=int,
        default=DEFAULT_MAX_TOKENS,
        help="Maximum number of tokens to generate",
    )
    parser.add_argument(
        "--temp", type=float, default=DEFAULT_TEMP, help="Sampling temperature"
    )
    parser.add_argument(
        "--top-p", type=float, default=DEFAULT_TOP_P, help="Sampling top-p"
    )
    parser.add_argument(
        "--min-p", type=float, default=DEFAULT_MIN_P, help="Sampling min-p"
    )
    parser.add_argument(
        "--min-tokens-to-keep",
        type=int,
        default=DEFAULT_MIN_TOKENS_TO_KEEP,
        help="Minimum tokens to keep for min-p sampling.",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=DEFAULT_SEED,
        help="PRNG seed",
    )
    parser.add_argument(
        "--ignore-chat-template",
        action="store_true",
        help="Use the raw prompt without the tokenizer's chat template.",
    )
    parser.add_argument(
        "--use-default-chat-template",
        action="store_true",
        help="Use the default chat template",
    )
    parser.add_argument(
        "--chat-template-config",
        help="Additional config for `apply_chat_template`. Should be a dictionary of"
        " string keys to values represented as a JSON decodable string.",
        default=None,
    )
    parser.add_argument(
        "--verbose",
        type=str2bool,
        default=True,
        help="Log verbose output when 'True' or 'T' or only print the response when 'False' or 'F'",
    )
    parser.add_argument(
        "--max-kv-size",
        type=int,
        help="Set the maximum key-value cache size",
        default=None,
    )
    parser.add_argument(
        "--prompt-cache-file",
        type=str,
        default=None,
        help="A file containing saved KV caches to avoid recomputing them",
    )
    parser.add_argument(
        "--kv-bits",
        type=int,
        help="Number of bits for KV cache quantization. "
        "Defaults to no quantization.",
        default=None,
    )
    parser.add_argument(
        "--kv-group-size",
        type=int,
        help="Group size for KV cache quantization.",
        default=64,
    )
    parser.add_argument(
        "--quantized-kv-start",
        help="When --kv-bits is set, start quantizing the KV cache "
        "from this step onwards.",
        type=int,
        default=DEFAULT_QUANTIZED_KV_START,
    )
    parser.add_argument(
        "--draft-model",
        type=str,
        help="A model to be used for speculative decoding.",
        default=None,
    )
    parser.add_argument(
        "--num-draft-tokens",
        type=int,
        help="Number of tokens to draft when using speculative decoding.",
        default=3,
    )
    return parser


def main():
    parser = setup_arg_parser()
    args = parser.parse_args()

    if args.seed is not None:
        mx.random.seed(args.seed)

    # Load the prompt cache and metadata if a cache file is provided
    using_cache = args.prompt_cache_file is not None
    if using_cache:
        prompt_cache, metadata = load_prompt_cache(
            args.prompt_cache_file,
            return_metadata=True,
        )
        if isinstance(prompt_cache[0], QuantizedKVCache):
            if args.kv_bits is not None and args.kv_bits != prompt_cache[0].bits:
                raise ValueError(
                    "--kv-bits does not match the kv cache loaded from --prompt-cache-file."
                )
            if args.kv_group_size != prompt_cache[0].group_size:
                raise ValueError(
                    "--kv-group-size does not match the kv cache loaded from --prompt-cache-file."
                )

    # Building tokenizer_config
    tokenizer_config = (
        {} if not using_cache else json.loads(metadata["tokenizer_config"])
    )
    tokenizer_config["trust_remote_code"] = True

    model_path = args.model
    if using_cache:
        if model_path is None:
            model_path = metadata["model"]
        elif model_path != metadata["model"]:
            raise ValueError(
                f"Providing a different model ({model_path}) than that "
                f"used to create the prompt cache ({metadata['model']}) "
                "is an error."
            )
    model_path = model_path or DEFAULT_MODEL

    model, tokenizer = load(
        model_path,
        adapter_path=args.adapter_path,
        tokenizer_config=tokenizer_config,
    )
    for eos_token in args.extra_eos_token:
        tokenizer.add_eos_token(eos_token)

    template_kwargs = {}
    if args.chat_template_config is not None:
        template_kwargs = json.loads(args.chat_template_config)

    if args.use_default_chat_template:
        if tokenizer.chat_template is None:
            tokenizer.chat_template = tokenizer.default_chat_template
    elif using_cache:
        tokenizer.chat_template = json.loads(metadata["chat_template"])

    prompt = args.prompt.replace("\\n", "\n").replace("\\t", "\t")
    prompt = sys.stdin.read() if prompt == "-" else prompt
    if not args.ignore_chat_template and tokenizer.chat_template is not None:
        if args.system_prompt is not None:
            messages = [{"role": "system", "content": args.system_prompt}]
        else:
            messages = []
        messages.append({"role": "user", "content": prompt})

        has_prefill = args.prefill_response is not None
        if has_prefill:
            messages.append({"role": "assistant", "content": args.prefill_response})
        prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            continue_final_message=has_prefill,
            add_generation_prompt=not has_prefill,
            **template_kwargs,
        )

        # Treat the prompt as a suffix assuming that the prefix is in the
        # stored kv cache.
        if using_cache:
            messages[-1]["content"] = "<query>"
            test_prompt = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                continue_final_message=has_prefill,
                add_generation_prompt=not has_prefill,
            )
            prompt = prompt[test_prompt.index("<query>") :]
        prompt = tokenizer.encode(prompt, add_special_tokens=False)
    else:
        prompt = tokenizer.encode(prompt)

    if args.draft_model is not None:
        draft_model, draft_tokenizer = load(args.draft_model)
        if draft_tokenizer.vocab_size != tokenizer.vocab_size:
            raise ValueError("Draft model tokenizer does not match model tokenizer.")
    else:
        draft_model = None
    sampler = make_sampler(args.temp, args.top_p, args.min_p, args.min_tokens_to_keep)
    response = generate(
        model,
        tokenizer,
        prompt,
        max_tokens=args.max_tokens,
        verbose=args.verbose,
        sampler=sampler,
        max_kv_size=args.max_kv_size,
        prompt_cache=prompt_cache if using_cache else None,
        kv_bits=args.kv_bits,
        kv_group_size=args.kv_group_size,
        quantized_kv_start=args.quantized_kv_start,
        draft_model=draft_model,
        num_draft_tokens=args.num_draft_tokens,
    )
    if not args.verbose:
        print(response)


if __name__ == "__main__":
    main()

>>>> llms/mlx_lm/_version.py
# Copyright © 2023-2024 Apple Inc.

__version__ = "0.21.6"

>>>> llms/mlx_lm/merge.py
# Copyright © 2023-2024 Apple Inc.

import argparse
import glob
import shutil
from pathlib import Path
from typing import Optional

import mlx.core as mx
import mlx.nn as nn
import numpy as np
import yaml
from mlx.utils import tree_flatten, tree_map

from .utils import (
    fetch_from_hub,
    get_model_path,
    save_config,
    save_weights,
    upload_to_hub,
)


def configure_parser() -> argparse.ArgumentParser:
    """
    Configures and returns the argument parser for the script.

    Returns:
        argparse.ArgumentParser: Configured argument parser.
    """
    parser = argparse.ArgumentParser(description="Merge multiple models.")

    parser.add_argument("--config", type=str, help="Path to the YAML config.")
    parser.add_argument(
        "--mlx-path",
        type=str,
        default="mlx_merged_model",
        help="Path to save the MLX model.",
    )
    parser.add_argument(
        "--upload-repo",
        help="The Hugging Face repo to upload the model to.",
        type=str,
        default=None,
    )
    return parser


def slerp(t, w1, w2, eps=1e-5):
    """
    Spherical linear interpolation

    Args:
        t (float): Interpolation weight in [0.0, 1.0]
        w1 (mx.array): First input
        w2 (mx.array): Second input
        eps (float): Constant for numerical stability
    Returns:
        mx.array: Interpolated result
    """
    t = float(t)
    if t == 0:
        return w1
    elif t == 1:
        return w2
    # Normalize
    v1 = w1 / mx.linalg.norm(w1)
    v2 = w2 / mx.linalg.norm(w2)
    # Angle
    dot = mx.clip((v1 * v2).sum(), 0.0, 1.0)
    theta = mx.arccos(dot)
    sin_theta = mx.sin(theta + eps)
    s1 = mx.sin(theta * (1 - t)) / sin_theta
    s2 = mx.sin(theta * t) / sin_theta
    return s1 * w1 + s2 * w2


def merge_models(base_model: nn.Module, model: nn.Module, config: dict):
    method = config.get("method", None)
    if method != "slerp":
        raise ValueError(f"Merge method {method} not supported")

    num_layers = len(model.layers)

    def unpack_values(vals):
        if isinstance(vals, (int, float)):
            return np.full(num_layers, vals)
        bins = len(vals) - 1
        sizes = [num_layers // bins] * bins
        sizes[-1] = num_layers - sum(sizes[:-1])
        return np.concatenate(
            [np.linspace(v1, v2, s) for v1, v2, s in zip(vals[:-1], vals[1:], sizes)]
        )

    param_list = config["parameters"]["t"]
    params = {}
    filter_keys = set()
    for pl in param_list[:-1]:
        params[pl["filter"]] = unpack_values(pl["value"])
        filter_keys.add(pl["filter"])
    default = unpack_values(param_list[-1]["value"])

    for e in range(num_layers):
        bl = base_model.layers[e]
        l = model.layers[e]
        base_weights = bl.parameters()
        weights = l.parameters()
        for k, w1 in base_weights.items():
            w2 = weights[k]
            t = params.get(k, default)[e]
            base_weights[k] = tree_map(lambda x, y: slerp(t, x, y), w1, w2)
        base_model.update(base_weights)


def merge(
    config: str,
    mlx_path: str = "mlx_model",
    upload_repo: Optional[str] = None,
):
    with open(config, "r") as fid:
        merge_conf = yaml.safe_load(fid)
    print("[INFO] Loading")

    model_paths = merge_conf.get("models", [])
    if len(model_paths) < 2:
        raise ValueError(f"Expected at least 2 models, got {len(model_paths)}.")

    # Load all models
    base_hf_path = model_paths[0]
    base_path = get_model_path(base_hf_path)
    base_model, base_config, tokenizer = fetch_from_hub(base_path, lazy=True)
    models = []
    for mp in model_paths[1:]:
        model, model_config, _ = fetch_from_hub(get_model_path(mp), lazy=True)
        base_type = base_config["model_type"]
        model_type = model_config["model_type"]
        if base_type != model_type:
            raise ValueError(
                f"Can only merge models of the same type,"
                f" but got {base_type} and {model_type}."
            )
        models.append(model)

    # Merge models into base model
    for m in models:
        merge_models(base_model, m, merge_conf)

    # Save base model
    mlx_path = Path(mlx_path)
    weights = dict(tree_flatten(base_model.parameters()))
    del models, base_model
    save_weights(mlx_path, weights, donate_weights=True)
    py_files = glob.glob(str(base_path / "*.py"))
    for file in py_files:
        shutil.copy(file, mlx_path)

    tokenizer.save_pretrained(mlx_path)

    save_config(config, config_path=mlx_path / "config.json")

    if upload_repo is not None:
        upload_to_hub(mlx_path, upload_repo, base_hf_path)


def main():
    parser = configure_parser()
    args = parser.parse_args()
    merge(**vars(args))


if __name__ == "__main__":
    main()

>>>> llms/mlx_lm/server.py
# Copyright © 2023-2024 Apple Inc.

import argparse
import json
import logging
import platform
import time
import uuid
import warnings
from dataclasses import dataclass, field
from http.server import BaseHTTPRequestHandler, HTTPServer
from pathlib import Path
from typing import (
    Any,
    Dict,
    List,
    Literal,
    NamedTuple,
    Optional,
    Sequence,
    Tuple,
    Union,
)

import mlx.core as mx
from huggingface_hub import scan_cache_dir

from ._version import __version__
from .models.cache import make_prompt_cache
from .sample_utils import make_logits_processors, make_sampler
from .utils import load, stream_generate


def get_system_fingerprint():
    gpu_arch = mx.metal.device_info()["architecture"] if mx.metal.is_available() else ""
    return f"{__version__}-{mx.__version__}-{platform.platform()}-{gpu_arch}"


class StopCondition(NamedTuple):
    stop_met: bool
    trim_length: int


def stopping_criteria(
    tokens: List[int],
    stop_id_sequences: List[List[int]],
    eos_token_id: Union[int, None],
) -> StopCondition:
    """
    Determines whether the token generation should stop based on predefined
    conditions.

    Args:
        tokens (List[int]): The current sequence of generated tokens.
        stop_id_sequences (List[List[[int]]): A list of integer lists, each
          representing a sequence of token IDs. If the end of the `tokens`
          list matches any of these sequences, the generation should stop.
        eos_token_id (Union[int, None]): The token ID that represents the
          end-of-sequence. If the last token in `tokens` matches this, the
          generation should stop.

    Returns:
        StopCondition: A named tuple indicating whether the stop condition has
          been met (`stop_met`) and how many tokens should be trimmed from the
          end if it has (`trim_length`).
    """
    if tokens and tokens[-1] == eos_token_id:
        return StopCondition(stop_met=True, trim_length=0)

    for stop_ids in stop_id_sequences:
        if len(tokens) >= len(stop_ids):
            if tokens[-len(stop_ids) :] == stop_ids:
                return StopCondition(stop_met=True, trim_length=len(stop_ids))

    return StopCondition(stop_met=False, trim_length=0)


def sequence_overlap(s1: Sequence, s2: Sequence) -> bool:
    """
    Checks if a suffix of s1 has overlap with a prefix of s2

    Args:
        s1 (Sequence): The first sequence
        s2 (Sequence): The second sequence

    Returns:
        bool: If the two sequences have overlap
    """
    max_overlap = min(len(s1), len(s2))
    return any(s1[-i:] == s2[:i] for i in range(1, max_overlap + 1))


def convert_chat(messages: List[dict], role_mapping: Optional[dict] = None):
    default_role_mapping = {
        "system_prompt": (
            "A chat between a curious user and an artificial intelligence "
            "assistant. The assistant follows the given rules no matter what."
        ),
        "system": "ASSISTANT's RULE: ",
        "user": "USER: ",
        "assistant": "ASSISTANT: ",
        "stop": "\n",
    }
    role_mapping = role_mapping if role_mapping is not None else default_role_mapping

    prompt = ""
    for line in messages:
        role_prefix = role_mapping.get(line["role"], "")
        stop = role_mapping.get("stop", "")
        content = line.get("content", "")
        prompt += f"{role_prefix}{content}{stop}"

    prompt += role_mapping.get("assistant", "")
    return prompt.rstrip()


def process_message_content(messages):
    """
    Convert message content to a format suitable for `apply_chat_template`.

    The function operates on messages in place. It converts the 'content' field
    to a string instead of a list of text fragments.

    Args:
        message_list (list): A list of dictionaries, where each dictionary may
          have a 'content' key containing a list of dictionaries with 'type' and
          'text' keys.

    Raises:
        ValueError: If the 'content' type is not supported or if 'text' is missing.

    """
    for message in messages:
        content = message["content"]
        if isinstance(content, list):
            text_fragments = [
                fragment["text"] for fragment in content if fragment["type"] == "text"
            ]
            if len(text_fragments) != len(content):
                raise ValueError("Only 'text' content type is supported.")
            message["content"] = "".join(text_fragments)


@dataclass
class PromptCache:
    cache: List[Any] = field(default_factory=list)
    model_key: Tuple[str, Optional[str]] = ("", None)
    tokens: List[int] = field(default_factory=list)


class ModelProvider:
    def __init__(self, cli_args: argparse.Namespace):
        """Load models on demand and persist them across the whole process."""
        self.cli_args = cli_args
        self.model_key = None
        self.model = None
        self.tokenizer = None

        # Preload the default model if it is provided
        if self.cli_args.model is not None:
            self.load("default_model")

    def _validate_model_path(self, model_path: str):
        model_path = Path(model_path)
        if model_path.exists() and not model_path.is_relative_to(Path.cwd()):
            raise RuntimeError(
                "Local models must be relative to the current working dir."
            )

    # Added in adapter_path to load dynamically
    def load(self, model_path, adapter_path=None):
        if self.model_key == (model_path, adapter_path):
            return self.model, self.tokenizer

        # Remove the old model if it exists.
        self.model = None
        self.tokenizer = None
        self.model_key = None

        # Building tokenizer_config
        tokenizer_config = {
            "trust_remote_code": True if self.cli_args.trust_remote_code else None
        }
        if self.cli_args.chat_template:
            tokenizer_config["chat_template"] = self.cli_args.chat_template

        if model_path == "default_model" and self.cli_args.model is not None:
            model, tokenizer = load(
                self.cli_args.model,
                adapter_path=(
                    adapter_path if adapter_path else self.cli_args.adapter_path
                ),  # if the user doesn't change the model but adds an adapter path
                tokenizer_config=tokenizer_config,
            )
        else:
            self._validate_model_path(model_path)
            model, tokenizer = load(
                model_path, adapter_path=adapter_path, tokenizer_config=tokenizer_config
            )

        if self.cli_args.use_default_chat_template:
            if tokenizer.chat_template is None:
                tokenizer.chat_template = tokenizer.default_chat_template

        self.model_key = (model_path, adapter_path)
        self.model = model
        self.tokenizer = tokenizer

        return self.model, self.tokenizer


class APIHandler(BaseHTTPRequestHandler):
    def __init__(
        self,
        model_provider: ModelProvider,
        *args,
        prompt_cache: Optional[PromptCache] = None,
        system_fingerprint: Optional[str] = None,
        **kwargs,
    ):
        """
        Create static request specific metadata
        """
        self.created = int(time.time())
        self.model_provider = model_provider
        self.prompt_cache = prompt_cache or PromptCache()
        self.system_fingerprint = system_fingerprint or get_system_fingerprint()
        super().__init__(*args, **kwargs)

    def _set_cors_headers(self):
        self.send_header("Access-Control-Allow-Origin", "*")
        self.send_header("Access-Control-Allow-Methods", "*")
        self.send_header("Access-Control-Allow-Headers", "*")

    def _set_completion_headers(self, status_code: int = 200):
        self.send_response(status_code)
        self.send_header("Content-type", "application/json")
        self._set_cors_headers()

    def _set_stream_headers(self, status_code: int = 200):
        self.send_response(status_code)
        self.send_header("Content-type", "text/event-stream")
        self.send_header("Cache-Control", "no-cache")
        self._set_cors_headers()

    def do_OPTIONS(self):
        self._set_completion_headers(204)
        self.end_headers()

    def do_POST(self):
        """
        Respond to a POST request from a client.
        """
        endpoints = {
            "/v1/completions": self.handle_text_completions,
            "/v1/chat/completions": self.handle_chat_completions,
            "/chat/completions": self.handle_chat_completions,
        }

        if self.path not in endpoints:
            self._set_completion_headers(404)
            self.end_headers()
            self.wfile.write(b"Not Found")
            return

        # Fetch and parse request body
        content_length = int(self.headers["Content-Length"])
        raw_body = self.rfile.read(content_length)
        self.body = json.loads(raw_body.decode())
        indent = "\t"  # Backslashes can't be inside of f-strings
        logging.debug(f"Incoming Request Body: {json.dumps(self.body, indent=indent)}")
        assert isinstance(
            self.body, dict
        ), f"Request should be dict, but got {type(self.body)}"

        # Extract request parameters from the body
        self.stream = self.body.get("stream", False)
        self.stream_options = self.body.get("stream_options", None)
        self.requested_model = self.body.get("model", "default_model")
        self.adapter = self.body.get("adapters", None)
        self.max_tokens = self.body.get("max_completion_tokens", None)
        if self.max_tokens is None:
            self.max_tokens = self.body.get("max_tokens", 512)
        self.temperature = self.body.get("temperature", 0.0)
        self.top_p = self.body.get("top_p", 1.0)
        self.repetition_penalty = self.body.get("repetition_penalty", 1.0)
        self.repetition_context_size = self.body.get("repetition_context_size", 20)
        self.logit_bias = self.body.get("logit_bias", None)
        self.logprobs = self.body.get("logprobs", -1)
        self.validate_model_parameters()

        # Load the model if needed
        try:
            self.model, self.tokenizer = self.model_provider.load(
                self.requested_model, self.adapter
            )
        except:
            self._set_completion_headers(404)
            self.end_headers()
            self.wfile.write(b"Not Found")
            return

        # Get stop id sequences, if provided
        stop_words = self.body.get("stop")
        stop_words = stop_words or []
        stop_words = [stop_words] if isinstance(stop_words, str) else stop_words
        stop_id_sequences = [
            self.tokenizer.encode(stop_word, add_special_tokens=False)
            for stop_word in stop_words
        ]

        # Send header type
        (
            self._set_stream_headers(200)
            if self.stream
            else self._set_completion_headers(200)
        )

        # Call endpoint specific method
        prompt = endpoints[self.path]()
        self.handle_completion(prompt, stop_id_sequences)

    def validate_model_parameters(self):
        """
        Validate the model parameters passed in the request for the correct types and values.
        """
        if not isinstance(self.stream, bool):
            raise ValueError("stream must be a boolean")

        if not isinstance(self.max_tokens, int) or self.max_tokens < 0:
            raise ValueError("max_tokens must be a non-negative integer")

        if not isinstance(self.temperature, (float, int)) or self.temperature < 0:
            raise ValueError("temperature must be a non-negative float")

        if not isinstance(self.top_p, (float, int)) or self.top_p < 0 or self.top_p > 1:
            raise ValueError("top_p must be a float between 0 and 1")

        if (
            not isinstance(self.repetition_penalty, (float, int))
            or self.repetition_penalty < 0
        ):
            raise ValueError("repetition_penalty must be a non-negative float")

        if self.logprobs != -1 and not (0 < self.logprobs <= 10):
            raise ValueError(
                f"logprobs must be between 1 and 10 but got {self.logprobs:,}"
            )

        if (
            not isinstance(self.repetition_context_size, int)
            or self.repetition_context_size < 0
        ):
            raise ValueError("repetition_context_size must be a non-negative integer")

        if self.logit_bias is not None:
            if not isinstance(self.logit_bias, dict):
                raise ValueError("logit_bias must be a dict of int to float")

            try:
                self.logit_bias = {int(k): v for k, v in self.logit_bias.items()}
            except ValueError:
                raise ValueError("logit_bias must be a dict of int to float")

        if not isinstance(self.requested_model, str):
            raise ValueError("model must be a string")
        if self.adapter is not None and not isinstance(self.adapter, str):
            raise ValueError("adapter must be a string")

    def generate_response(
        self,
        text: str,
        finish_reason: Union[Literal["length", "stop"], None],
        prompt_token_count: Optional[int] = None,
        completion_token_count: Optional[int] = None,
        token_logprobs: Optional[List[float]] = None,
        top_tokens: Optional[List[Dict[int, float]]] = None,
        tokens: Optional[List[int]] = None,
    ) -> dict:
        """
        Generate a single response packet based on response type (stream or
        not), completion type and parameters.

        Args:
            text (str): Text generated by model
            finish_reason (Union[Literal["length", "stop"], None]): The reason the
              response is being sent: "length", "stop" or `None`.
            prompt_token_count (Optional[int]): The number of tokens in the prompt,
              used to populate the "usage" field (not used when stream).
            completion_token_count (Optional[int]): The number of tokens in the
              response, used to populate the "usage" field (not used when stream).
            token_logprobs (Optional[List[float]]): The log probabilities per token,
              in token order.
            top_tokens (Optional[List[Dict[int, float]]]): List of dictionaries mapping
              tokens to logprobs for the top N tokens at each token position.
            tokens (Optional[List[int]]): List of tokens to return with logprobs structure

        Returns:
            dict: A dictionary containing the response, in the same format as
              OpenAI's API.
        """
        token_logprobs = token_logprobs if token_logprobs else []
        top_logprobs = top_tokens if top_tokens else []

        # Static response
        response = {
            "id": self.request_id,
            "system_fingerprint": self.system_fingerprint,
            "object": self.object_type,
            "model": self.requested_model,
            "created": self.created,
            "choices": [
                {
                    "index": 0,
                    "logprobs": {
                        "token_logprobs": token_logprobs,
                        "top_logprobs": top_logprobs,
                        "tokens": tokens,
                    },
                    "finish_reason": finish_reason,
                }
            ],
        }

        if not self.stream:
            if not (
                isinstance(prompt_token_count, int)
                and isinstance(completion_token_count, int)
            ):
                raise ValueError(
                    "Response type is complete, but token counts not provided"
                )

            response["usage"] = {
                "prompt_tokens": prompt_token_count,
                "completion_tokens": completion_token_count,
                "total_tokens": prompt_token_count + completion_token_count,
            }

        choice = response["choices"][0]

        # Add dynamic response
        if self.object_type.startswith("chat.completion"):
            key_name = "delta" if self.stream else "message"
            choice[key_name] = {"role": "assistant", "content": text}
        elif self.object_type == "text_completion":
            choice.update(text=text)
        else:
            ValueError(f"Unsupported response type: {self.object_type}")

        return response

    def get_prompt_cache(self, prompt):
        cache_len = len(self.prompt_cache.tokens)
        if (
            self.prompt_cache.model_key != self.model_provider.model_key
            or cache_len >= len(prompt)
            or self.prompt_cache.tokens != prompt[:cache_len]
        ):
            self.prompt_cache.model_key = self.model_provider.model_key
            self.prompt_cache.cache = make_prompt_cache(self.model_provider.model)
        else:
            prompt = prompt[cache_len:]
        self.prompt_cache.tokens.extend(prompt)
        return prompt

    def handle_completion(
        self,
        prompt: List[int],
        stop_id_sequences: List[List[int]],
    ):
        """
        Generate a response to a prompt and send it to the client in a single batch.

        Args:
            prompt (List[int]): The tokenized prompt.
            stop_id_sequences (List[List[int]]): A list of stop words passed
              to the stopping_criteria function
        """
        tokens = []
        finish_reason = "length"
        stop_sequence_suffix = None
        if self.stream:
            self.end_headers()
            logging.debug(f"Starting stream:")
        else:
            logging.debug(f"Starting completion:")
        token_logprobs = []
        top_tokens = []

        prompt = self.get_prompt_cache(prompt)

        text = ""
        tic = time.perf_counter()
        sampler = make_sampler(self.temperature, top_p=self.top_p)
        logits_processors = make_logits_processors(
            self.logit_bias, self.repetition_penalty, self.repetition_context_size
        )
        for gen_response in stream_generate(
            model=self.model,
            tokenizer=self.tokenizer,
            prompt=prompt,
            max_tokens=self.max_tokens,
            sampler=sampler,
            logits_processors=logits_processors,
            prompt_cache=self.prompt_cache.cache,
        ):
            segment = gen_response.text
            text += segment
            logging.debug(text)
            token = gen_response.token
            logprobs = gen_response.logprobs
            tokens.append(token)

            if self.logprobs > 0:
                sorted_indices = mx.argpartition(-logprobs, kth=self.logprobs - 1)
                top_indices = sorted_indices[: self.logprobs]
                top_logprobs = logprobs[top_indices]
                top_token_info = zip(top_indices.tolist(), top_logprobs.tolist())
                top_tokens.append(tuple(top_token_info))

            token_logprobs.append(logprobs[token].item())

            stop_condition = stopping_criteria(
                tokens, stop_id_sequences, self.tokenizer.eos_token_id
            )
            if stop_condition.stop_met:
                finish_reason = "stop"
                if stop_condition.trim_length:
                    stop_sequence_suffix = self.tokenizer.decode(
                        tokens[-stop_condition.trim_length :]
                    )
                    text = text[: -len(stop_sequence_suffix)]
                break

            if self.stream:
                # If the end of tokens overlaps with a stop sequence, generate new
                # tokens until we know if the stop sequence is hit or not
                if any(
                    (
                        sequence_overlap(tokens, sequence)
                        for sequence in stop_id_sequences
                    )
                ):
                    continue
                elif segment:
                    response = self.generate_response(segment, None)
                    self.wfile.write(f"data: {json.dumps(response)}\n\n".encode())
                    self.wfile.flush()

        self.prompt_cache.tokens.extend(tokens)

        logging.debug(f"Prompt: {gen_response.prompt_tps:.3f} tokens-per-sec")
        logging.debug(f"Generation: {gen_response.generation_tps:.3f} tokens-per-sec")
        logging.debug(f"Peak memory: {gen_response.peak_memory:.3f} GB")

        if self.stream:
            response = self.generate_response(segment, finish_reason)
            self.wfile.write(f"data: {json.dumps(response)}\n\n".encode())
            self.wfile.flush()
            if self.stream_options is not None and self.stream_options["include_usage"]:
                response = self.completion_usage_response(len(prompt), len(tokens))
                self.wfile.write(f"data: {json.dumps(response)}\n\n".encode())
                self.wfile.flush()
            self.wfile.write("data: [DONE]\n\n".encode())
            self.wfile.flush()
        else:
            response = self.generate_response(
                text,
                finish_reason,
                len(prompt),
                len(tokens),
                token_logprobs=token_logprobs,
                top_tokens=top_tokens,
                tokens=tokens,
            )
            response_json = json.dumps(response).encode()
            indent = "\t"  # Backslashes can't be inside of f-strings
            logging.debug(f"Outgoing Response: {json.dumps(response, indent=indent)}")

            # Send an additional Content-Length header when it is known
            self.send_header("Content-Length", str(len(response_json)))
            self.end_headers()
            self.wfile.write(response_json)
            self.wfile.flush()

    def completion_usage_response(
        self,
        prompt_token_count: Optional[int] = None,
        completion_token_count: Optional[int] = None,
    ):
        response = {
            "id": self.request_id,
            "system_fingerprint": self.system_fingerprint,
            "object": "chat.completion",
            "model": self.requested_model,
            "created": self.created,
            "choices": [],
            "usage": {
                "prompt_tokens": prompt_token_count,
                "completion_tokens": completion_token_count,
                "total_tokens": prompt_token_count + completion_token_count,
            },
        }
        return response

    def handle_chat_completions(self) -> List[int]:
        """
        Handle a chat completion request.

        Returns:
            mx.array: A mx.array of the tokenized prompt from the request body
        """
        body = self.body
        assert "messages" in body, "Request did not contain messages"

        # Determine response type
        self.request_id = f"chatcmpl-{uuid.uuid4()}"
        self.object_type = "chat.completion.chunk" if self.stream else "chat.completion"
        if self.tokenizer.chat_template:
            messages = body["messages"]
            process_message_content(messages)
            prompt = self.tokenizer.apply_chat_template(
                messages,
                body.get("tools", None),
                add_generation_prompt=True,
            )
        else:
            prompt = convert_chat(body["messages"], body.get("role_mapping"))
            prompt = self.tokenizer.encode(prompt)

        return prompt

    def handle_text_completions(self) -> List[int]:
        """
        Handle a text completion request.

        Returns:
            mx.array: A mx.array of the tokenized prompt from the request body
        """
        # Determine response type
        self.request_id = f"cmpl-{uuid.uuid4()}"
        self.object_type = "text_completion"
        assert "prompt" in self.body, "Request did not contain a prompt"
        return self.tokenizer.encode(self.body["prompt"])

    def do_GET(self):
        """
        Respond to a GET request from a client.
        """
        if self.path == "/v1/models":
            self.handle_models_request()
        else:
            self._set_completion_headers(404)
            self.end_headers()
            self.wfile.write(b"Not Found")

    def handle_models_request(self):
        """
        Handle a GET request for the /v1/models endpoint.
        """
        self._set_completion_headers(200)
        self.end_headers()

        # Scan the cache directory for downloaded mlx models
        hf_cache_info = scan_cache_dir()
        downloaded_models = [
            repo for repo in hf_cache_info.repos if "mlx" in repo.repo_id
        ]

        # Create a list of available models
        models = [
            {
                "id": repo.repo_id,
                "object": "model",
                "created": self.created,
            }
            for repo in downloaded_models
        ]

        response = {"object": "list", "data": models}

        response_json = json.dumps(response).encode()
        self.wfile.write(response_json)
        self.wfile.flush()


def run(
    host: str,
    port: int,
    model_provider: ModelProvider,
    server_class=HTTPServer,
    handler_class=APIHandler,
):
    server_address = (host, port)
    prompt_cache = PromptCache()
    httpd = server_class(
        server_address,
        lambda *args, **kwargs: handler_class(
            model_provider,
            prompt_cache=prompt_cache,
            system_fingerprint=get_system_fingerprint(),
            *args,
            **kwargs,
        ),
    )
    warnings.warn(
        "mlx_lm.server is not recommended for production as "
        "it only implements basic security checks."
    )
    logging.info(f"Starting httpd at {host} on port {port}...")
    httpd.serve_forever()


def main():
    parser = argparse.ArgumentParser(description="MLX Http Server.")
    parser.add_argument(
        "--model",
        type=str,
        help="The path to the MLX model weights, tokenizer, and config",
    )
    parser.add_argument(
        "--adapter-path",
        type=str,
        help="Optional path for the trained adapter weights and config.",
    )
    parser.add_argument(
        "--host",
        type=str,
        default="127.0.0.1",
        help="Host for the HTTP server (default: 127.0.0.1)",
    )
    parser.add_argument(
        "--port",
        type=int,
        default=8080,
        help="Port for the HTTP server (default: 8080)",
    )
    parser.add_argument(
        "--trust-remote-code",
        action="store_true",
        help="Enable trusting remote code for tokenizer",
    )
    parser.add_argument(
        "--log-level",
        type=str,
        default="INFO",
        choices=["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
        help="Set the logging level (default: INFO)",
    )
    parser.add_argument(
        "--cache-limit-gb",
        type=int,
        default=None,
        help="Set the MLX cache limit in GB",
        required=False,
    )
    parser.add_argument(
        "--chat-template",
        type=str,
        default="",
        help="Specify a chat template for the tokenizer",
        required=False,
    )
    parser.add_argument(
        "--use-default-chat-template",
        action="store_true",
        help="Use the default chat template",
    )
    args = parser.parse_args()

    logging.basicConfig(
        level=getattr(logging, args.log_level.upper(), None),
        format="%(asctime)s - %(levelname)s - %(message)s",
    )

    if args.cache_limit_gb is not None:
        logging.debug(f"Setting cache limit to {args.cache_limit_gb} GB")
        mx.metal.set_cache_limit(args.cache_limit_gb * 1024 * 1024 * 1024)

    run(args.host, args.port, ModelProvider(args))


if __name__ == "__main__":
    main()

>>>> llms/mlx_lm/gguf.py
import re
from enum import IntEnum
from pathlib import Path
from typing import Iterable, Optional, Set, Tuple, Union

import mlx.core as mx
from transformers import AutoTokenizer


class TokenType(IntEnum):
    NORMAL = 1
    UNKNOWN = 2
    CONTROL = 3
    USER_DEFINED = 4
    UNUSED = 5
    BYTE = 6


class GGMLFileType(IntEnum):
    GGML_TYPE_F16 = 1


# copied from https://github.com/ggerganov/llama.cpp/blob/master/convert.py#L455
class HfVocab:
    def __init__(
        self,
        fname_tokenizer: Path,
        fname_added_tokens: Optional[Union[Path, None]] = None,
    ) -> None:
        self.tokenizer = AutoTokenizer.from_pretrained(
            fname_tokenizer,
            cache_dir=fname_tokenizer,
            local_files_only=True,
        )
        self.added_tokens_list = []
        self.added_tokens_dict = dict()
        self.added_tokens_ids = set()
        for tok, tokidx in sorted(
            self.tokenizer.get_added_vocab().items(), key=lambda x: x[1]
        ):
            if tokidx >= self.tokenizer.vocab_size:
                self.added_tokens_list.append(tok)
                self.added_tokens_dict[tok] = tokidx
                self.added_tokens_ids.add(tokidx)
        self.specials = {
            tok: self.tokenizer.get_vocab()[tok]
            for tok in self.tokenizer.all_special_tokens
        }
        self.special_ids = set(self.tokenizer.all_special_ids)
        self.vocab_size_base = self.tokenizer.vocab_size
        self.vocab_size = self.vocab_size_base + len(self.added_tokens_list)
        self.fname_tokenizer = fname_tokenizer
        self.fname_added_tokens = fname_added_tokens

    def hf_tokens(self) -> Iterable[Tuple[bytes, float, TokenType]]:
        reverse_vocab = {
            id: encoded_tok for encoded_tok, id in self.tokenizer.get_vocab().items()
        }
        for token_id in range(self.vocab_size_base):
            if token_id in self.added_tokens_ids:
                continue
            token_text = reverse_vocab[token_id]
            yield token_text, self.get_token_score(token_id), self.get_token_type(
                token_id, token_text, self.special_ids
            )

    def get_token_type(
        self, token_id: int, token_text: bytes, special_ids: Set[int]
    ) -> TokenType:
        if re.fullmatch(r"<0x[0-9A-Fa-f]{2}>", token_text):
            return TokenType.BYTE
        return TokenType.CONTROL if token_id in special_ids else TokenType.NORMAL

    def get_token_score(self, token_id: int) -> float:
        return -1000.0

    def added_tokens(self) -> Iterable[Tuple[bytes, float, TokenType]]:
        for text in self.added_tokens_list:
            if text in self.specials:
                toktype = self.get_token_type(self.specials[text], "", self.special_ids)
                score = self.get_token_score(self.specials[text])
            else:
                toktype = TokenType.USER_DEFINED
                score = -1000.0
            yield text, score, toktype

    def has_newline_token(self):
        return "<0x0A>" in self.tokenizer.vocab or "\n" in self.tokenizer.vocab

    def all_tokens(self) -> Iterable[Tuple[bytes, float, TokenType]]:
        yield from self.hf_tokens()
        yield from self.added_tokens()

    def __repr__(self) -> str:
        return f"<HfVocab with {self.vocab_size_base} base tokens and {len(self.added_tokens_list)} added tokens>"

    @staticmethod
    def load(path: Path) -> "HfVocab":
        added_tokens_path = path.parent / "added_tokens.json"
        return HfVocab(path, added_tokens_path if added_tokens_path.exists() else None)


def translate_weight_names(name):
    name = name.replace("model.layers.", "blk.")
    # for mixtral gate
    name = name.replace("block_sparse_moe.gate", "ffn_gate_inp")
    # for mixtral experts ffns
    pattern = r"block_sparse_moe\.experts\.(\d+)\.w1\.weight"
    replacement = r"ffn_gate.\1.weight"
    name = re.sub(pattern, replacement, name)
    pattern = r"block_sparse_moe\.experts\.(\d+)\.w2\.weight"
    replacement = r"ffn_down.\1.weight"
    name = re.sub(pattern, replacement, name)
    pattern = r"block_sparse_moe\.experts\.(\d+)\.w3\.weight"
    replacement = r"ffn_up.\1.weight"
    name = re.sub(pattern, replacement, name)

    name = name.replace("mlp.gate_proj", "ffn_gate")
    name = name.replace("mlp.down_proj", "ffn_down")
    name = name.replace("mlp.up_proj", "ffn_up")
    name = name.replace("self_attn.q_proj", "attn_q")
    name = name.replace("self_attn.k_proj", "attn_k")
    name = name.replace("self_attn.v_proj", "attn_v")
    name = name.replace("self_attn.o_proj", "attn_output")
    name = name.replace("input_layernorm", "attn_norm")
    name = name.replace("post_attention_layernorm", "ffn_norm")
    name = name.replace("model.embed_tokens", "token_embd")
    name = name.replace("model.norm", "output_norm")
    name = name.replace("lm_head", "output")
    return name


def permute_weights(weights, n_head, n_head_kv=None):
    if n_head_kv is not None and n_head != n_head_kv:
        n_head = n_head_kv
    reshaped = weights.reshape(
        n_head, 2, weights.shape[0] // n_head // 2, *weights.shape[1:]
    )
    swapped = reshaped.swapaxes(1, 2)
    final_shape = weights.shape
    return swapped.reshape(final_shape)


def prepare_metadata(config, vocab):
    metadata = {
        "general.name": "llama",
        "llama.context_length": (
            mx.array(config["max_position_embeddings"], dtype=mx.uint32)
            if config.get("max_position_embeddings") is not None
            else None
        ),
        "llama.embedding_length": (
            mx.array(config["hidden_size"], dtype=mx.uint32)
            if config.get("hidden_size") is not None
            else None
        ),
        "llama.block_count": (
            mx.array(config["num_hidden_layers"], dtype=mx.uint32)
            if config.get("num_hidden_layers") is not None
            else None
        ),
        "llama.feed_forward_length": (
            mx.array(config["intermediate_size"], dtype=mx.uint32)
            if config.get("intermediate_size") is not None
            else None
        ),
        "llama.rope.dimension_count": (
            mx.array(
                config["hidden_size"] // config["num_attention_heads"], dtype=mx.uint32
            )
            if config.get("hidden_size") is not None
            and config.get("num_attention_heads") is not None
            else None
        ),
        "llama.attention.head_count": (
            mx.array(config["num_attention_heads"], dtype=mx.uint32)
            if config.get("num_attention_heads") is not None
            else None
        ),
        "llama.attention.head_count_kv": (
            mx.array(
                config.get("num_key_value_heads", config["num_attention_heads"]),
                dtype=mx.uint32,
            )
            if config.get("num_attention_heads") is not None
            else None
        ),
        "llama.expert_count": (
            mx.array(config.get("num_local_experts", None), dtype=mx.uint32)
            if config.get("num_local_experts") is not None
            else None
        ),
        "llama.expert_used_count": (
            mx.array(config.get("num_experts_per_tok", None), dtype=mx.uint32)
            if config.get("num_experts_per_tok") is not None
            else None
        ),
        "llama.attention.layer_norm_rms_epsilon": (
            mx.array(config.get("rms_norm_eps", 1e-05))
            if config.get("rms_norm_eps") is not None
            else None
        ),
        "llama.rope.freq_base": (
            mx.array(config.get("rope_theta", 10000), dtype=mx.float32)
            if config.get("rope_theta") is not None
            else None
        ),
    }

    rope_scaling = config.get("rope_scaling")
    if rope_scaling is not None and (typ := rope_scaling.get("type")):
        rope_factor = rope_scaling.get("factor")
        f_rope_scale = rope_factor
        if typ == "linear":
            rope_scaling_type = "linear"
            metadata["llama.rope.scaling.type"] = rope_scaling_type
            metadata["llama.rope.scaling.factor"] = mx.array(f_rope_scale)

    metadata["general.file_type"] = mx.array(
        GGMLFileType.GGML_TYPE_F16.value,
        dtype=mx.uint32,
    )
    metadata["general.quantization_version"] = mx.array(
        GGMLFileType.GGML_TYPE_F16.value,
        dtype=mx.uint32,
    )
    metadata["general.name"] = config.get("_name_or_path", "llama").split("/")[-1]
    metadata["general.architecture"] = "llama"
    metadata["general.alignment"] = mx.array(32, dtype=mx.uint32)

    # add metadata for vocab
    metadata["tokenizer.ggml.model"] = "llama"
    tokens = []
    scores = []
    toktypes = []
    for text, score, toktype in vocab.all_tokens():
        tokens.append(text)
        scores.append(score)
        toktypes.append(toktype.value)
    assert len(tokens) == vocab.vocab_size
    metadata["tokenizer.ggml.tokens"] = tokens
    metadata["tokenizer.ggml.scores"] = mx.array(scores, dtype=mx.float32)
    metadata["tokenizer.ggml.token_type"] = mx.array(toktypes, dtype=mx.uint32)
    if vocab.tokenizer.bos_token_id is not None:
        metadata["tokenizer.ggml.bos_token_id"] = mx.array(
            vocab.tokenizer.bos_token_id, dtype=mx.uint32
        )
    if vocab.tokenizer.eos_token_id is not None:
        metadata["tokenizer.ggml.eos_token_id"] = mx.array(
            vocab.tokenizer.eos_token_id, dtype=mx.uint32
        )
    if vocab.tokenizer.unk_token_id is not None:
        metadata["tokenizer.ggml.unknown_token_id"] = mx.array(
            vocab.tokenizer.unk_token_id, dtype=mx.uint32
        )

    metadata = {k: v for k, v in metadata.items() if v is not None}
    return metadata


def convert_to_gguf(
    model_path: Union[str, Path],
    weights: dict,
    config: dict,
    output_file_path: str,
):
    if isinstance(model_path, str):
        model_path = Path(model_path)

    quantization = config.get("quantization", None)
    if quantization:
        raise NotImplementedError(
            "Conversion of quantized models is not yet supported."
        )
    print("Converting to GGUF format")
    # https://github.com/ggerganov/llama.cpp/blob/master/convert.py#L1182 seems relate to llama.cpp's multihead attention
    weights = {
        k: (
            permute_weights(
                v, config["num_attention_heads"], config["num_attention_heads"]
            )
            if "self_attn.q_proj.weight" in k
            else (
                permute_weights(
                    v, config["num_attention_heads"], config["num_key_value_heads"]
                )
                if "self_attn.k_proj.weight" in k
                else v
            )
        )
        for k, v in weights.items()
    }

    # rename weights for gguf format
    weights = {translate_weight_names(k): v for k, v in weights.items()}

    if not (model_path / "tokenizer.json").exists():
        raise ValueError("Tokenizer json not found")

    vocab = HfVocab.load(model_path)
    metadata = prepare_metadata(config, vocab)

    weights = {
        k: (
            v.astype(mx.float32).astype(mx.float16)
            if v.dtype == mx.bfloat16
            else v.astype(mx.float32) if "norm" in k else v
        )
        for k, v in weights.items()
    }

    output_file_path = output_file_path
    mx.save_gguf(output_file_path, weights, metadata)
    print(f"Converted GGUF model saved as: {output_file_path}")

>>>> llms/mlx_lm/tuner/lora.py
# Copyright © 2024 Apple Inc.

import math

import mlx.core as mx
import mlx.nn as nn

from ..models.switch_layers import QuantizedSwitchLinear, SwitchLinear


class LoRALinear(nn.Module):
    @staticmethod
    def from_base(
        linear: nn.Linear,
        r: int = 8,
        dropout: float = 0.0,
        scale: float = 20.0,
    ):
        # TODO remove when input_dims and output_dims are attributes
        # on linear and quantized linear
        output_dims, input_dims = linear.weight.shape
        if isinstance(linear, nn.QuantizedLinear):
            input_dims *= 32 // linear.bits
        lora_lin = LoRALinear(
            input_dims=input_dims,
            output_dims=output_dims,
            r=r,
            dropout=dropout,
            scale=scale,
        )
        lora_lin.linear = linear
        return lora_lin

    def fuse(self, de_quantize: bool = False):
        linear = self.linear
        bias = "bias" in linear
        weight = linear.weight
        is_quantized = isinstance(linear, nn.QuantizedLinear)

        # Use the same type as the linear weight if not quantized
        dtype = weight.dtype

        if is_quantized:
            dtype = linear.scales.dtype
            weight = mx.dequantize(
                weight,
                linear.scales,
                linear.biases,
                linear.group_size,
                linear.bits,
            )
        output_dims, input_dims = weight.shape
        fused_linear = nn.Linear(input_dims, output_dims, bias=bias)

        lora_b = (self.scale * self.lora_b.T).astype(dtype)
        lora_a = self.lora_a.T.astype(dtype)
        fused_linear.weight = weight + lora_b @ lora_a
        if bias:
            fused_linear.bias = linear.bias

        if is_quantized and not de_quantize:
            fused_linear = nn.QuantizedLinear.from_linear(
                fused_linear,
                linear.group_size,
                linear.bits,
            )

        return fused_linear

    def __init__(
        self,
        input_dims: int,
        output_dims: int,
        r: int = 8,
        dropout: float = 0.0,
        scale: float = 20.0,
        bias: bool = False,
    ):
        super().__init__()

        # Regular linear layer weights
        self.linear = nn.Linear(input_dims, output_dims, bias=bias)

        self.dropout = nn.Dropout(p=dropout)

        # Scale for low-rank update
        self.scale = scale

        # Low rank lora weights
        scale = 1 / math.sqrt(input_dims)
        self.lora_a = mx.random.uniform(
            low=-scale,
            high=scale,
            shape=(input_dims, r),
        )
        self.lora_b = mx.zeros(shape=(r, output_dims))

    def __call__(self, x):
        y = self.linear(x)
        z = (self.dropout(x) @ self.lora_a) @ self.lora_b
        return y + (self.scale * z).astype(x.dtype)


class LoRASwitchLinear(nn.Module):
    @staticmethod
    def from_base(
        linear: nn.Module,
        r: int = 8,
        dropout: float = 0.0,
        scale: float = 20.0,
    ):
        lora_lin = LoRASwitchLinear(
            input_dims=linear.input_dims,
            output_dims=linear.output_dims,
            num_experts=linear.num_experts,
            r=r,
            dropout=dropout,
            scale=scale,
        )
        lora_lin.linear = linear
        return lora_lin

    def fuse(self, de_quantize: bool = False):
        linear = self.linear
        bias = "bias" in linear
        weight = linear.weight
        is_quantized = isinstance(linear, QuantizedSwitchLinear)

        # Use the same type as the linear weight if not quantized
        dtype = weight.dtype

        if is_quantized:
            dtype = mx.float16
            weight = mx.dequantize(
                weight,
                linear.scales,
                linear.biases,
                linear.group_size,
                linear.bits,
            )
        num_experts, output_dims, input_dims = weight.shape
        fused_linear = SwitchLinear(input_dims, output_dims, num_experts, bias=bias)

        lora_b = (self.scale * self.lora_b).astype(dtype)
        lora_a = self.lora_a.reshape(num_experts, -1, input_dims).astype(dtype)
        fused_linear.weight = weight + lora_b @ lora_a
        if bias:
            fused_linear.bias = linear.bias

        if is_quantized and not de_quantize:
            fused_linear = fused_linear.to_quantized(linear.group_size, linear.bits)

        return fused_linear

    def __init__(
        self,
        input_dims: int,
        output_dims: int,
        num_experts: int,
        r: int = 8,
        dropout: float = 0.0,
        scale: float = 20.0,
        bias: bool = False,
    ):
        super().__init__()

        # Regular linear layer weights
        self.linear = SwitchLinear(input_dims, output_dims, num_experts, bias=bias)

        self.dropout = nn.Dropout(p=dropout)

        # Scale for low-rank update
        self.scale = scale

        # Low rank lora weights
        scale = 1 / math.sqrt(input_dims)
        self.lora_a = mx.random.uniform(
            low=-scale,
            high=scale,
            shape=(r * num_experts, input_dims),
        )
        self.lora_b = mx.zeros(shape=(num_experts, output_dims, r))
        self.num_experts = num_experts

    def __call__(self, x, indices):
        shape = x.shape[:-3] + (self.num_experts, -1)

        y = self.linear(x, indices)
        z = (self.dropout(x) @ self.lora_a.T).reshape(shape)
        z = mx.take_along_axis(z, indices[..., None], axis=-2)
        z = z[..., None, :] @ self.lora_b[indices].swapaxes(-2, -1)

        return y + (self.scale * z).astype(x.dtype)


class LoRAEmbedding(nn.Module):
    @staticmethod
    def from_base(
        embedding: nn.Embedding,
        r: int = 8,
        dropout: float = 0.0,
        scale: float = 20.0,
    ):
        num_embeddings, dims = embedding.weight.shape
        if isinstance(embedding, nn.QuantizedEmbedding):
            dims *= 32 // embedding.bits
        lora_embedding = LoRAEmbedding(
            num_embeddings=num_embeddings,
            dims=dims,
            r=r,
            dropout=dropout,
            scale=scale,
        )
        lora_embedding.embedding = embedding
        return lora_embedding

    def fuse(self, de_quantize: bool = False):
        embedding = self.embedding
        weight = embedding.weight
        is_quantized = isinstance(embedding, nn.QuantizedEmbedding)

        # Use the same type as the linear weight if not quantized
        dtype = weight.dtype

        if is_quantized:
            dtype = embedding.scales.dtype
            weight = mx.dequantize(
                weight,
                embedding.scales,
                embedding.biases,
                embedding.group_size,
                embedding.bits,
            )
        num_embeddings, dims = weight.shape
        fused_embedding = nn.Embedding(num_embeddings, dims)

        lora_a = (self.scale * self.lora_a).astype(dtype)
        lora_b = self.lora_b.astype(dtype)
        fused_embedding.weight = weight + lora_a @ lora_b

        if is_quantized and not de_quantize:
            fused_embedding = nn.QuantizedEmbedding.from_embedding(
                fused_embedding,
                embedding.group_size,
                embedding.bits,
            )

        return fused_embedding

    def __init__(
        self,
        num_embeddings: int,
        dims: int,
        r: int = 8,
        dropout: float = 0.0,
        scale: float = 20.0,
    ):
        super().__init__()

        # Regular embedding layer
        self.embedding = nn.Embedding(num_embeddings, dims)
        self.dropout = nn.Dropout(p=dropout)

        # Scale for low-rank update
        self.scale = scale

        # Low rank lora weights
        scale = 1 / math.sqrt(num_embeddings)
        self.lora_a = mx.random.uniform(
            low=-scale,
            high=scale,
            shape=(num_embeddings, r),
        )
        self.lora_b = mx.zeros(shape=(r, dims))

    def __call__(self, x):
        y = self.embedding(x)
        z = self.dropout(self.lora_a[x] @ self.lora_b)
        out = y + (self.scale * z).astype(y.dtype)
        return out

    def as_linear(self, x):
        y = self.embedding.as_linear(x)
        z = (self.dropout(x) @ self.lora_b.T) @ self.lora_a.T
        return y + (self.scale * z).astype(x.dtype)

>>>> llms/mlx_lm/tuner/trainer.py
# Copyright © 2024 Apple Inc.

import glob
import shutil
import time
from dataclasses import dataclass, field
from pathlib import Path
from typing import List, Optional, Tuple

import mlx.core as mx
import mlx.nn as nn
import numpy as np
from mlx.nn.utils import average_gradients
from mlx.utils import tree_flatten
from transformers import PreTrainedTokenizer

from .datasets import CompletionsDataset


def grad_checkpoint(layer):
    """
    Update all instances of type(layer) to use gradient checkpointing.
    """
    fn = type(layer).__call__

    def checkpointed_fn(model, *args, **kwargs):
        def inner_fn(params, *args, **kwargs):
            model.update(params)
            return fn(model, *args, **kwargs)

        return mx.checkpoint(inner_fn)(model.trainable_parameters(), *args, **kwargs)

    type(layer).__call__ = checkpointed_fn


@dataclass
class TrainingArgs:
    batch_size: int = field(default=4, metadata={"help": "Minibatch size."})
    iters: int = field(default=100, metadata={"help": "Iterations to train for."})
    val_batches: int = field(
        default=25,
        metadata={
            "help": "Number of validation batches, -1 uses the entire validation set."
        },
    )
    steps_per_report: int = field(
        default=10,
        metadata={"help": "Number of training steps between loss reporting."},
    )
    steps_per_eval: int = field(
        default=200, metadata={"help": "Number of training steps between validations."}
    )
    steps_per_save: int = field(
        default=100, metadata={"help": "Save the model every number steps"}
    )
    max_seq_length: int = field(
        default=2048, metadata={"help": "Maximum sequence length."}
    )
    adapter_file: str = field(
        default="adapters.safetensors",
        metadata={"help": "Save/load path for the trained adapter weights."},
    )
    grad_checkpoint: bool = field(
        default=False,
        metadata={"help": "Use gradient checkpointing to reduce memory use."},
    )


def default_loss(model, batch, lengths):
    inputs = batch[:, :-1]
    targets = batch[:, 1:]

    logits = model(inputs)
    logits = logits.astype(mx.float32)

    steps = mx.arange(1, targets.shape[1] + 1)
    mask = mx.logical_and(steps >= lengths[:, 0:1], steps <= lengths[:, 1:])

    ce = nn.losses.cross_entropy(logits, targets) * mask
    ntoks = mask.sum()
    ce = ce.sum() / ntoks

    return ce, ntoks


def iterate_batches(
    dataset,
    tokenizer,
    batch_size,
    max_seq_length,
    train=False,
):
    # Sort by length:
    idx = sorted(range(len(dataset)), key=lambda idx: len(dataset[idx]))
    if len(dataset) < batch_size:
        raise ValueError(
            f"Dataset must have at least batch_size={batch_size}"
            f" examples but only has {len(dataset)}."
        )

    # If running in distributed mode (N machines) then each one should skip N-1
    # samples
    step = mx.distributed.init().size()
    if batch_size % step != 0:
        raise ValueError("The batch size must be divisible by the number of workers")

    # Make the batches:
    batch_idx = [
        idx[i : i + batch_size : step]
        for i in range(0, len(idx) - batch_size + 1, batch_size)
    ]

    while True:
        indices = np.random.permutation(len(batch_idx))
        for i in indices:
            batch = [dataset[j] for j in batch_idx[i]]
            if len(batch[0]) == 2:
                batch, offsets = zip(*batch)
            else:
                offsets = [0] * len(batch)
            lengths = [len(x) for x in batch]
            if max(lengths) > max_seq_length:
                print(
                    f"[WARNING] Some sequences are longer than {max_seq_length} tokens. "
                    f"The longest sentence {max(lengths)} will be truncated to {max_seq_length}. "
                    "Consider pre-splitting your data to save memory."
                )

            # Pad to the nearest multiple of 8 or the maximum length
            pad_to = 8
            max_length_in_batch = pad_to * ((max(lengths) + pad_to - 1) // pad_to)
            max_length_in_batch = min(max_length_in_batch, max_seq_length)

            batch_arr = np.zeros((batch_size // step, max_length_in_batch), np.int32)

            for j in range(batch_size // step):
                truncated_length = min(lengths[j], max_seq_length)
                batch_arr[j, :truncated_length] = batch[j][:truncated_length]
                lengths[j] = (
                    truncated_length  # Update lengths to match truncated lengths
                )
            batch = mx.array(batch_arr)
            yield batch, mx.array(list(zip(offsets, lengths)))

        if not train:
            break


def evaluate(
    model,
    dataset,
    tokenizer,
    batch_size,
    num_batches,
    max_seq_length=2048,
    loss: callable = default_loss,
    iterate_batches: callable = iterate_batches,
):
    all_losses = mx.array(0.0)
    ntokens = mx.array(0)

    index_iterator = iter(range(num_batches)) if num_batches != -1 else iter(int, 1)

    for _, batch in zip(
        index_iterator,
        iterate_batches(
            dataset=dataset,
            tokenizer=tokenizer,
            batch_size=batch_size,
            max_seq_length=max_seq_length,
        ),
    ):
        losses, toks = loss(model, *batch)
        all_losses += losses * toks
        ntokens += toks
        mx.eval(all_losses, ntokens)

    all_losses = mx.distributed.all_sum(all_losses, stream=mx.cpu)
    ntokens = mx.distributed.all_sum(ntokens, stream=mx.cpu)

    return (all_losses / ntokens).item()


class TrainingCallback:

    def on_train_loss_report(self, train_info: dict):
        """Called to report training loss at specified intervals."""
        pass

    def on_val_loss_report(self, val_info: dict):
        """Called to report validation loss at specified intervals or the beginning."""
        pass


def train(
    model,
    tokenizer,
    optimizer,
    train_dataset,
    val_dataset,
    args: TrainingArgs = TrainingArgs(),
    loss: callable = default_loss,
    iterate_batches: callable = iterate_batches,
    training_callback: TrainingCallback = None,
):
    print(f"Starting training..., iters: {args.iters}")
    world = mx.distributed.init()
    world_size = world.size()
    rank = world.rank()
    if world_size > 1:
        print(f"Node {rank} of {world_size}")

    if args.grad_checkpoint:
        grad_checkpoint(model.layers[0])

    state = [model.state, optimizer.state]

    def step(batch):
        # Forward and backward pass
        (lvalue, toks), grad = loss_value_and_grad(model, *batch)

        # All reduce the gradients if running in distributed mode
        grad = average_gradients(grad)

        # Model update
        optimizer.update(model, grad)

        return lvalue, toks

    loss_value_and_grad = nn.value_and_grad(model, loss)

    losses = 0
    n_tokens = 0
    steps = 0
    trained_tokens = 0
    train_time = 0
    # Main training loop
    for it, batch in zip(
        range(1, args.iters + 1),
        iterate_batches(
            dataset=train_dataset,
            tokenizer=tokenizer,
            batch_size=args.batch_size,
            max_seq_length=args.max_seq_length,
            train=True,
        ),
    ):
        tic = time.perf_counter()
        # Report validation loss if needed, the first validation loss
        # is always measured before any training.
        if it == 1 or it % args.steps_per_eval == 0 or it == args.iters:
            tic = time.perf_counter()
            val_loss = evaluate(
                model=model,
                dataset=val_dataset,
                loss=loss,
                tokenizer=tokenizer,
                batch_size=args.batch_size,
                num_batches=args.val_batches,
                max_seq_length=args.max_seq_length,
                iterate_batches=iterate_batches,
            )
            val_time = time.perf_counter() - tic
            if rank == 0:
                print(
                    f"Iter {it}: "
                    f"Val loss {val_loss:.3f}, "
                    f"Val took {val_time:.3f}s",
                    flush=True,
                )

            if training_callback is not None:
                val_info = {
                    "iteration": it,
                    "val_loss": val_loss,
                    "val_time": val_time,
                }
                training_callback.on_val_loss_report(val_info)

            tic = time.perf_counter()

        lvalue, toks = step(batch)
        losses += lvalue
        n_tokens += toks
        steps += 1
        mx.eval(state, losses, n_tokens)
        train_time += time.perf_counter() - tic

        # Report training loss if needed
        if it % args.steps_per_report == 0 or it == args.iters:
            train_loss = mx.distributed.all_sum(losses, stream=mx.cpu).item()
            train_loss /= steps * mx.distributed.init().size()
            n_tokens = mx.distributed.all_sum(n_tokens, stream=mx.cpu).item()
            learning_rate = optimizer.learning_rate.item()
            it_sec = args.steps_per_report / train_time
            tokens_sec = float(n_tokens) / train_time
            trained_tokens += n_tokens
            peak_mem = mx.metal.get_peak_memory() / 1e9
            if rank == 0:
                print(
                    f"Iter {it}: Train loss {train_loss:.3f}, "
                    f"Learning Rate {learning_rate:.3e}, "
                    f"It/sec {it_sec:.3f}, "
                    f"Tokens/sec {tokens_sec:.3f}, "
                    f"Trained Tokens {trained_tokens}, "
                    f"Peak mem {peak_mem:.3f} GB",
                    flush=True,
                )

            if training_callback is not None:
                train_info = {
                    "iteration": it,
                    "train_loss": train_loss,
                    "learning_rate": learning_rate,
                    "iterations_per_second": it_sec,
                    "tokens_per_second": tokens_sec,
                    "trained_tokens": trained_tokens,
                    "peak_memory": peak_mem,
                }
                training_callback.on_train_loss_report(train_info)

            losses = 0
            n_tokens = 0
            steps = 0
            train_time = 0

        # Save adapter weights
        if it % args.steps_per_save == 0:
            adapter_weights = dict(tree_flatten(model.trainable_parameters()))
            mx.save_safetensors(str(args.adapter_file), adapter_weights)
            checkpoint = (
                Path(args.adapter_file).parent / f"{it:07d}_adapters.safetensors"
            )
            mx.save_safetensors(str(checkpoint), adapter_weights)
            print(
                f"Iter {it}: Saved adapter weights to "
                f"{args.adapter_file} and {checkpoint}."
            )

    # Save final weights
    adapter_weights = dict(tree_flatten(model.trainable_parameters()))
    mx.save_safetensors(str(args.adapter_file), adapter_weights)
    print(f"Saved final weights to {args.adapter_file}.")

>>>> llms/mlx_lm/tuner/utils.py
# Copyright © 2024 Apple Inc.
import json
import types
from pathlib import Path
from typing import Dict

import mlx.core as mx
import mlx.nn as nn
import mlx.optimizers as opt
from mlx.utils import tree_flatten, tree_unflatten

from ..models.switch_layers import QuantizedSwitchLinear, SwitchLinear
from .dora import DoRAEmbedding, DoRALinear
from .lora import LoRAEmbedding, LoRALinear, LoRASwitchLinear


def build_schedule(schedule_config: Dict):
    """
    Build a learning rate schedule from the given config.
    """
    schedule_fn = getattr(opt.schedulers, schedule_config["name"])
    arguments = schedule_config["arguments"]
    initial_lr = arguments[0]
    bound_schedule_fn = schedule_fn(*arguments)
    if warmup_steps := schedule_config.get("warmup", 0):
        warmup_init = schedule_config.get("warmup_init", 0.0)
        warmup_fn = opt.schedulers.linear_schedule(
            warmup_init, initial_lr, warmup_steps
        )
        return opt.schedulers.join_schedules(
            [warmup_fn, bound_schedule_fn], [warmup_steps + 1]
        )
    else:
        return bound_schedule_fn


def linear_to_lora_layers(
    model: nn.Module,
    num_layers: int,
    config: Dict,
    use_dora: bool = False,
):
    """
    Convert some of the models linear layers to lora layers.

    Args:
        model (nn.Module): The neural network model.
        num_layers (int): The number of blocks to convert to lora layers
        starting from the last layer.
        config (dict): More configuration parameters for LoRA, including the
          rank, scale, and optional layer keys.
        use_dora (bool): If True, uses DoRA instead of LoRA.
          Default: ``False``
    """

    def to_lora(layer):
        if isinstance(layer, (nn.Linear, nn.QuantizedLinear)):
            LoRALayer = DoRALinear if use_dora else LoRALinear
        elif isinstance(layer, (SwitchLinear, QuantizedSwitchLinear)):
            if use_dora:
                raise ValueError(f"{type(layer).__name__} doesn't support DoRA yet.")
            LoRALayer = LoRASwitchLinear
        elif isinstance(layer, (nn.Embedding, nn.QuantizedEmbedding)):
            LoRALayer = DoRAEmbedding if use_dora else LoRAEmbedding
        else:
            raise ValueError(
                f"Can't convert layer of type {type(layer).__name__} to LoRA"
            )

        return LoRALayer.from_base(
            layer,
            r=config["rank"],
            scale=config["scale"],
            dropout=config["dropout"],
        )

    keys = config.get("keys", None)
    if keys is not None:
        keys = set(keys)
    elif model.model_type in [
        "mistral",
        "llama",
        "phi",
        "mixtral",
        "nemotron",
        "stablelm",
        "hunyuan",
        "qwen2",
        "qwen2_moe",
        "phimoe",
        "gemma",
        "gemma2",
        "granite",
        "helium",
        "starcoder2",
        "cohere",
        "cohere2",
        "minicpm",
        "deepseek",
        "olmo2",
        "olmoe",
        "internlm3",
    ]:
        keys = set(["self_attn.q_proj", "self_attn.v_proj"])
        if model.model_type in ["mixtral", "phimoe"]:
            keys.add("block_sparse_moe.gate")
        if model.model_type == "qwen2_moe":
            keys.add("mlp.gate")
            keys.add("mlp.shared_expert_gate")
        if model.model_type == "olmoe":
            keys.add("mlp.gate")

    elif model.model_type == "gpt_bigcode":
        keys = set(["attn.c_attn"])
    elif model.model_type == "gpt2":
        keys = set(["attn.c_attn"])
    elif model.model_type == "gpt_neox":
        keys = set(["attention.query_key_value"])
    elif model.model_type == "olmo":
        keys = set(["att_proj"])
    elif model.model_type == "openelm":
        keys = set(["attn.qkv_proj"])
    elif model.model_type == "phi3":
        keys = set(["self_attn.qkv_proj"])
    elif model.model_type == "phi-msft":
        keys = set(["mixer.Wqkv", "moe.gate"])
    elif model.model_type == "dbrx":
        keys = set(["norm_attn_norm.attn.Wqkv", "ffn.router.layer"])
    elif model.model_type == "internlm2":
        keys = set(["attention.wqkv", "attention.wo"])
    elif model.model_type == "deepseek_v2":
        keys = set(
            [
                "self_attn.q_proj",
                "self_attn.q_a_proj",
                "self_attn.q_b_proj",
                "self_attn.kv_a_proj_with_mqa",
                "self_attn.kv_b_proj",
            ]
        )
    elif model.model_type == "mamba":
        keys = set(
            [
                "mixer.in_proj",
                "mixer.x_proj",
                "mixer.dt_proj",
                "mixer.out_proj",
            ]
        )
    elif model.model_type == "exaone":
        keys = set(["attn.attention.q_proj", "attn.attention.v_proj"])
    else:
        raise ValueError(f"Lora does not support {model.model_type}")

    for l in model.layers[-max(num_layers, 0) :]:
        lora_layers = [(k, to_lora(m)) for k, m in l.named_modules() if k in keys]
        if lora_layers:
            l.update_modules(tree_unflatten(lora_layers))

    lora_modules = [(k, to_lora(m)) for k, m in model.named_modules() if k in keys]
    if lora_modules:
        model.update_modules(tree_unflatten(lora_modules))


def load_adapters(model: nn.Module, adapter_path: str) -> nn.Module:
    """
    Load any fine-tuned adapters / layers.

    Args:
        model (nn.Module): The neural network model.
        adapter_path (str): Path to the adapter configuration file.

    Returns:
        nn.Module: The updated model with LoRA layers applied.
    """
    adapter_path = Path(adapter_path)
    if not adapter_path.exists():
        raise FileNotFoundError(f"The adapter path does not exist: {adapter_path}")
    with open(adapter_path / "adapter_config.json", "r") as fid:
        config = types.SimpleNamespace(**json.load(fid))
    fine_tune_type = getattr(config, "fine_tune_type", "lora")
    if fine_tune_type != "full":
        linear_to_lora_layers(
            model,
            config.num_layers,
            config.lora_parameters,
            use_dora=(fine_tune_type == "dora"),
        )
    model.load_weights(str(adapter_path / "adapters.safetensors"), strict=False)
    return model


def dequantize(model: nn.Module) -> nn.Module:
    """
    Dequantize the quantized linear layers in the model.

    Args:
        model (nn.Module): The model with quantized linear layers.

    Returns:
        nn.Module: The model with dequantized layers.
    """
    de_quantize_layers = []
    for name, module in model.named_modules():
        if isinstance(module, nn.QuantizedLinear):
            bias = "bias" in module
            weight = module.weight
            weight = mx.dequantize(
                weight,
                module.scales,
                module.biases,
                module.group_size,
                module.bits,
            ).astype(mx.float16)
            output_dims, input_dims = weight.shape
            linear = nn.Linear(input_dims, output_dims, bias=bias)
            linear.weight = weight
            if bias:
                linear.bias = module.bias
            de_quantize_layers.append((name, linear))
        if isinstance(module, nn.QuantizedEmbedding):
            weight = mx.dequantize(
                module.weight,
                module.scales,
                module.biases,
                module.group_size,
                module.bits,
            ).astype(mx.float16)
            num_embeddings, dims = weight.shape
            emb = nn.Embedding(num_embeddings, dims)
            emb.weight = weight
            de_quantize_layers.append((name, emb))

    if len(de_quantize_layers) > 0:
        model.update_modules(tree_unflatten(de_quantize_layers))
    return model


def remove_lora_layers(model: nn.Module) -> nn.Module:
    """
    Remove the LoRA layers from the model.

    Args:
        model (nn.Module): The model with LoRA layers.

    Returns:
        nn.Module: The model without LoRA layers.
    """
    reset_layers = []
    for name, module in model.named_modules():
        if isinstance(module, LoRALinear):
            reset_layers.append((name, module.linear))
    if len(reset_layers) > 0:
        model.update_modules(tree_unflatten(reset_layers))
    return model


def nparams(module):
    if hasattr(module, "bits"):
        n = 0 if not hasattr(module, "bias") else module.bias.size
        return n + module.weight.size * 32 // module.bits
    return sum(v.size for _, v in tree_flatten(module.parameters()))


def print_trainable_parameters(model):
    leaf_modules = tree_flatten(
        model.leaf_modules(), is_leaf=lambda m: isinstance(m, nn.Module)
    )
    total_p = sum(nparams(m) for _, m in leaf_modules) / 10**6
    trainable_p = (
        sum(v.size for _, v in tree_flatten(model.trainable_parameters())) / 10**6
    )
    print(
        f"Trainable parameters: {(trainable_p * 100 / total_p):.3f}% "
        f"({trainable_p:.3f}M/{total_p:.3f}M)"
    )

>>>> llms/mlx_lm/tuner/datasets.py
import itertools
import json
import types
from pathlib import Path
from typing import Any, Dict, List, Optional

from transformers import PreTrainedTokenizer


class Dataset:
    """
    Light-weight wrapper to hold a dataset.
    """

    def __init__(
        self,
        data: List[Dict[str, str]],
        tokenizer: PreTrainedTokenizer,
        text_key: str = "text",
    ):
        self._data = [tokenizer.encode(d[text_key]) for d in data]
        for d in self._data:
            if d[-1] != tokenizer.eos_token_id:
                d.append(tokenizer.eos_token_id)

    def __getitem__(self, idx: int):
        return self._data[idx]

    def __len__(self):
        return len(self._data)


class ChatDataset:
    """
    A dataset for chat data in the format of {"messages": [...]}
    https://platform.openai.com/docs/guides/fine-tuning/example-format
    """

    def __init__(
        self,
        data: List[Dict[str, str]],
        tokenizer: PreTrainedTokenizer,
        chat_key: str = "messages",
        mask_prompt: bool = False,
    ):
        self._data = []
        for d in data:
            messages = d[chat_key]
            tools = d.get("tools", None)
            tokens = tokenizer.apply_chat_template(messages, tools=tools)
            if mask_prompt:
                messages = messages[:-1]
                offset = len(tokenizer.apply_chat_template(messages, tools=tools))
                self._data.append((tokens, offset))
            else:
                self._data.append(tokens)

    def __getitem__(self, idx: int):
        return self._data[idx]

    def __len__(self):
        return len(self._data)


class CompletionsDataset:
    """
    A dataset for prompt-completion data in the format of {"prompt": ..., "completion": ...}
    or using user-provided keys for prompt and completion values
    https://platform.openai.com/docs/guides/fine-tuning/example-format
    """

    def __init__(
        self,
        data: List[Dict[str, str]],
        tokenizer: PreTrainedTokenizer,
        prompt_key: str,
        completion_key: str,
        mask_prompt: bool,
    ):
        self._data = []
        for d in data:
            tokens = tokenizer.apply_chat_template(
                [
                    {"role": "user", "content": d[prompt_key]},
                    {"role": "assistant", "content": d[completion_key]},
                ],
            )
            if mask_prompt:
                offset = len(
                    tokenizer.apply_chat_template(
                        [{"role": "user", "content": d[prompt_key]}]
                    )
                )
                self._data.append((tokens, offset))
            else:
                self._data.append(tokens)

    def __getitem__(self, idx: int):
        return self._data[idx]

    def __len__(self):
        return len(self._data)


class ConcatenatedDataset:
    def __init__(self, data: List[Any]):
        self._data = list(itertools.chain(*data))

    def __getitem__(self, idx: int):
        return self._data[idx]

    def __len__(self):
        return len(self._data)


def create_dataset(
    data,
    tokenizer: PreTrainedTokenizer,
    config,
):
    mask_prompt = getattr(config, "mask_prompt", False)
    prompt_feature = getattr(config, "prompt_feature", "prompt")
    text_feature = getattr(config, "text_feature", "text")
    completion_feature = getattr(config, "completion_feature", "completion")
    chat_feature = getattr(config, "chat_feature", "messages")
    sample = data[0]
    if prompt_feature in sample and completion_feature in sample:
        return CompletionsDataset(
            data, tokenizer, prompt_feature, completion_feature, mask_prompt
        )
    elif chat_feature in sample:
        return ChatDataset(
            data, tokenizer, chat_key=chat_feature, mask_prompt=mask_prompt
        )
    elif text_feature in sample:
        if mask_prompt:
            raise ValueError("Prompt masking not supported for text dataset.")
        return Dataset(data, tokenizer, text_key=text_feature)
    else:
        raise ValueError(
            "Unsupported data format, check the supported formats here:\n"
            "https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/LORA.md#data."
        )


def load_local_dataset(
    data_path: Path,
    tokenizer: PreTrainedTokenizer,
    config,
):
    def load_subset(path):
        if not path.exists():
            return []
        with open(path, "r") as fid:
            data = [json.loads(l) for l in fid]
        return create_dataset(data, tokenizer, config)

    names = ("train", "valid", "test")
    train, valid, test = [load_subset(data_path / f"{n}.jsonl") for n in names]
    return train, valid, test


def load_hf_dataset(
    data_id: str,
    tokenizer: PreTrainedTokenizer,
    config,
):
    from datasets import exceptions, load_dataset

    try:
        dataset = load_dataset(data_id)

        names = ("train", "valid", "test")

        train, valid, test = [
            (
                create_dataset(dataset[n], tokenizer, config)
                if n in dataset.keys()
                else []
            )
            for n in names
        ]

    except exceptions.DatasetNotFoundError:
        raise ValueError(f"Not found Hugging Face dataset: {data_id} .")

    return train, valid, test


def load_custom_hf_dataset(args, tokenizer: PreTrainedTokenizer):
    import datasets

    def create_hf_dataset(dataset_name, config, split, hf_config):
        ds = datasets.load_dataset(
            dataset_name,
            split=split,
            **hf_config,
        )
        return create_dataset(ds, tokenizer, config)

    dataset_collection = args.hf_dataset
    if isinstance(dataset_collection, dict):
        dataset_collection = [dataset_collection]

    collection = []
    for ds in dataset_collection:
        ds_name = ds["name"]
        print(f"Loading Hugging Face dataset {ds_name}.")
        ds["mask_prompt"] = getattr(args, "mask_prompt", False)
        config = types.SimpleNamespace(**ds)
        hf_config = ds.get("config", {})
        if args.train:
            train_split = ds.get("train_split", "train[:80%]")
            valid_split = ds.get("valid_split", "train[-10%:]")
            train = create_hf_dataset(
                ds_name,
                config,
                train_split,
                hf_config,
            )
            valid = create_hf_dataset(
                ds_name,
                config,
                valid_split,
                hf_config,
            )
        else:
            train, valid = [], []

        if args.test:
            test_split = ds.get("test_split")
            test = create_hf_dataset(
                ds_name,
                config,
                test_split,
                hf_config,
            )
        else:
            test = []

        collection.append((train, valid, test))

    if len(collection) == 1:
        return collection[0]

    # Otherwise concatenate them
    return tuple(map(ConcatenatedDataset, zip(*collection)))


def load_dataset(args, tokenizer: PreTrainedTokenizer):
    if getattr(args, "hf_dataset", False):
        train, valid, test = load_custom_hf_dataset(args, tokenizer)
    else:
        data_path = Path(args.data)
        if data_path.exists():
            train, valid, test = load_local_dataset(data_path, tokenizer, args)
        else:
            print(f"Loading Hugging Face dataset {args.data}.")
            train, valid, test = load_hf_dataset(args.data, tokenizer, args)

    if args.train and len(train) == 0:
        raise ValueError(
            "Training set not found or empty. Must provide training set for fine-tuning."
        )
    if args.train and len(valid) == 0:
        raise ValueError(
            "Validation set not found or empty. Must provide validation set for fine-tuning."
        )
    if args.test and len(test) == 0:
        raise ValueError(
            "Test set not found or empty. Must provide test set for evaluation."
        )
    return train, valid, test

>>>> llms/mlx_lm/tuner/dora.py
# Copyright © 2024 Apple Inc.

import math

import mlx.core as mx
import mlx.nn as nn


class DoRALinear(nn.Module):
    @staticmethod
    def from_base(
        linear: nn.Linear,
        r: int = 8,
        dropout: float = 0.0,
        scale: float = 20.0,
    ):
        # TODO remove when input_dims and output_dims are attributes
        # on linear and quantized linear
        output_dims, input_dims = linear.weight.shape
        if isinstance(linear, nn.QuantizedLinear):
            input_dims *= 32 // linear.bits
        dora_lin = DoRALinear(
            input_dims=input_dims,
            output_dims=output_dims,
            r=r,
            dropout=dropout,
            scale=scale,
        )
        dora_lin.set_linear(linear)
        return dora_lin

    def fuse(self, de_quantize: bool = False):
        linear = self.linear
        bias = "bias" in linear
        weight = self._dequantized_weight()

        # Use the same type as the linear weight
        dtype = weight.dtype

        output_dims, input_dims = weight.shape
        fused_linear = nn.Linear(input_dims, output_dims, bias=False)

        lora_b = (self.scale * self.lora_b.T).astype(dtype)
        lora_a = self.lora_a.T.astype(dtype)
        weight = weight + lora_b @ lora_a
        norm_scale = self.m / mx.linalg.norm(weight, axis=1)
        fused_linear.weight = norm_scale[:, None] * weight

        if bias:
            fused_linear.bias = linear.bias

        if self._is_quantized() and not de_quantize:
            fused_linear = nn.QuantizedLinear.from_linear(
                fused_linear,
                linear.group_size,
                linear.bits,
            )
        return fused_linear

    def __init__(
        self,
        input_dims: int,
        output_dims: int,
        r: int = 8,
        dropout: float = 0.0,
        scale: float = 20.0,
        bias: bool = False,
    ):
        super().__init__()

        # Regular linear layer weights
        self.set_linear(nn.Linear(input_dims, output_dims, bias=bias))
        self.dropout = nn.Dropout(p=dropout)

        # Scale for low-rank update
        self.scale = scale

        # Low rank lora weights
        scale = 1 / math.sqrt(input_dims)
        self.lora_a = mx.random.uniform(
            low=-scale,
            high=scale,
            shape=(input_dims, r),
        )
        self.lora_b = mx.zeros(shape=(r, output_dims))

    def set_linear(self, linear):
        """
        Set the self.linear layer and recompute self.m.
        """
        self.linear = linear
        self.m = mx.linalg.norm(self._dequantized_weight().astype(mx.float32), axis=1)

    def _dequantized_weight(self):
        """
        Return the weight of linear layer and dequantize it if is quantized
        """
        weight = self.linear.weight
        if self._is_quantized():
            weight = mx.dequantize(
                weight,
                self.linear.scales,
                self.linear.biases,
                self.linear.group_size,
                self.linear.bits,
            )
        return weight

    def _is_quantized(self):
        return isinstance(self.linear, nn.QuantizedLinear)

    def __call__(self, x):
        # Regular LoRA (without a bias)
        w = self._dequantized_weight()
        y = x @ w.T

        z = (self.dropout(x) @ self.lora_a) @ self.lora_b
        out = y + (self.scale * z).astype(x.dtype)

        # Compute the norm of the adapted weights
        adapted = w + (self.scale * self.lora_b.T) @ self.lora_a.T
        denom = mx.stop_gradient(mx.linalg.norm(adapted, axis=1))

        # Remove the norm and scale by the learned magnitude
        out = (self.m / denom).astype(x.dtype) * out

        if "bias" in self.linear:
            out = out + self.linear.bias
        return out


class DoRAEmbedding(nn.Module):
    def from_base(
        embedding: nn.Embedding,
        r: int = 8,
        dropout: float = 0.0,
        scale: float = 20.0,
    ):
        num_embeddings, dims = embedding.weight.shape

        # TODO support quantized weights in DoRALinear
        if isinstance(embedding, nn.QuantizedLinear):
            raise ValueError("DoRAEmbedding does not yet support quantization.")
        dora_embedding = DoRAEmbedding(
            num_embeddings=num_embeddings,
            dims=dims,
            r=r,
            dropout=dropout,
            scale=scale,
        )
        dora_embedding.set_embedding(embedding)
        return dora_embedding

    def fuse(self, de_quantize: bool = False):
        embedding = self.embedding
        weight = embedding.weight

        # Use the same type as the linear weight if not quantized
        dtype = weight.dtype

        num_embeddings, dims = weight.shape
        fused_embedding = nn.Embedding(num_embeddings, dims)

        lora_a = (self.scale * self.lora_a).astype(dtype)
        lora_b = self.lora_b.astype(dtype)
        weight = weight + lora_a @ lora_b
        norm_scale = self.m / mx.linalg.norm(weight, axis=1)
        fused_embedding.weight = norm_scale[:, None] * weight

        return fused_embedding

    def __init__(
        self,
        num_embeddings: int,
        dims: int,
        r: int = 8,
        dropout: float = 0.0,
        scale: float = 20.0,
    ):
        super().__init__()

        # Regular embedding layer weights
        self.set_embedding(nn.Embedding(num_embeddings, dims))
        self.dropout = nn.Dropout(p=dropout)

        # Scale for low-rank update
        self.scale = scale

        # Low rank lora weights
        scale = 1 / math.sqrt(num_embeddings)
        self.lora_a = mx.random.uniform(
            low=-scale,
            high=scale,
            shape=(num_embeddings, r),
        )
        self.lora_b = mx.zeros(shape=(r, dims))

    def set_embedding(self, embedding: nn.Module):
        self.embedding = embedding
        self.m = mx.linalg.norm(embedding.weight, axis=1)

    def __call__(self, x):
        y = self.embedding(x)
        z = self.scale * self.lora_a[x] @ self.lora_b
        out = y + self.dropout(z).astype(y.dtype)

        # Compute the norm of the adapted weights for the individual embeddings
        adapted = y + z
        denom = mx.stop_gradient(mx.linalg.norm(adapted, axis=-1))

        # Remove the norm and scale by the learned magnitude
        out = (self.m[x] / denom)[..., None] * out

        return out

    def as_linear(self, x):
        y = self.embedding.as_linear(x)
        z = (self.dropout(x) @ self.lora_b.T) @ self.lora_a.T
        out = y + (self.scale * z).astype(x.dtype)

        # Compute the norm of the adapted weights
        adapted = self.embedding.weight + (self.scale * self.lora_a) @ self.lora_b
        denom = mx.stop_gradient(mx.linalg.norm(adapted, axis=1))

        # Remove the norm and scale by the learned magnitude
        out = (self.m / denom) * out

        return out

>>>> lora/fuse.py
# Copyright © 2023-2024 Apple Inc.

import argparse
from pathlib import Path

import mlx.core as mx
import mlx.nn as nn
import utils
from mlx.utils import tree_flatten, tree_unflatten
from models import LoRALinear

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="LoRA or QLoRA finetuning.")
    parser.add_argument(
        "--model",
        default="mlx_model",
        help="The path to the local model directory or Hugging Face repo.",
    )
    parser.add_argument(
        "--save-path",
        default="lora_fused_model",
        help="The path to save the fused model.",
    )
    parser.add_argument(
        "--adapter-file",
        type=str,
        default="adapters.npz",
        help="Path to the trained adapter weights (npz or safetensors).",
    )
    parser.add_argument(
        "--hf-path",
        help=(
            "Path to the original Hugging Face model. This is "
            "required for upload if --model is a local directory."
        ),
        type=str,
        default=None,
    )
    parser.add_argument(
        "--upload-name",
        help="The name of model to upload to Hugging Face MLX Community.",
        type=str,
        default=None,
    )
    parser.add_argument(
        "-d",
        "--de-quantize",
        help="Generate a de-quantized model.",
        action="store_true",
    )

    print("Loading pretrained model")
    args = parser.parse_args()

    model, tokenizer, config = utils.load(args.model)

    # Load adapters and get number of LoRA layers
    adapters = list(mx.load(args.adapter_file).items())
    lora_layers = len([m for m in adapters if "q_proj.lora_a" in m[0]])

    # Freeze all layers other than LORA linears
    model.freeze()
    for l in model.model.layers[len(model.model.layers) - lora_layers :]:
        l.self_attn.q_proj = LoRALinear.from_linear(l.self_attn.q_proj)
        l.self_attn.v_proj = LoRALinear.from_linear(l.self_attn.v_proj)
        if hasattr(l, "block_sparse_moe"):
            l.block_sparse_moe.gate = LoRALinear.from_linear(l.block_sparse_moe.gate)

    model.update(tree_unflatten(adapters))
    fused_linears = [
        (n, m.to_linear())
        for n, m in model.named_modules()
        if isinstance(m, LoRALinear)
    ]

    model.update_modules(tree_unflatten(fused_linears))

    if args.de_quantize:
        de_quantize_layers = []
        for n, m in model.named_modules():
            if isinstance(m, nn.QuantizedLinear):
                bias = "bias" in m
                weight = m.weight
                weight = mx.dequantize(
                    weight,
                    m.scales,
                    m.biases,
                    m.group_size,
                    m.bits,
                ).astype(mx.float16)
                output_dims, input_dims = weight.shape
                linear = nn.Linear(input_dims, output_dims, bias=bias)
                linear.weight = weight
                if bias:
                    linear.bias = m.bias
                de_quantize_layers.append((n, linear))

        model.update_modules(tree_unflatten(de_quantize_layers))

    weights = dict(tree_flatten(model.parameters()))
    if args.de_quantize:
        config.pop("quantization", None)
    utils.save_model(args.save_path, weights, tokenizer, config)

    if args.upload_name is not None:
        hf_path = args.hf_path
        if not Path(args.model).exists():
            # If the model path doesn't exist, assume it's an HF repo
            hf_path = args.model
        elif hf_path is None:
            raise ValueError(
                "Must provide original Hugging Face repo to upload local model."
            )
        utils.upload_to_hub(args.save_path, args.upload_name, hf_path)

>>>> lora/data/wikisql.py
# Copyright © 2023 Apple Inc.

"""
Code to preprocess the WikiSQL dataset adapted from
https://github.com/salesforce/WikiSQL and
https://huggingface.co/sqllama/sqllama-V0/blob/main/wikisql.ipynb .
"""


import json
import os


def load():
    """
    Load all three splits of the WikiSQL dataset.
    """
    return (WikiSQL(dn) for dn in ["train", "dev", "test"])


class WikiSQL:
    def __init__(self, dataset, save_dir="/tmp"):
        valid_sets = ("train", "dev", "test")
        if dataset not in valid_sets:
            raise ValueError(f"Dataset must be in {valid_sets}, got {dataset}")
        data_dir = os.path.join(save_dir, "wikisql")
        self._maybe_download(data_dir)

        self._parse_tables(os.path.join(data_dir, f"data/{dataset}.tables.jsonl"))
        self._parse_queries(os.path.join(data_dir, f"data/{dataset}.jsonl"))

    def _maybe_download(self, data_dir):
        if not os.path.exists(data_dir):
            import io
            import tarfile
            from urllib import request

            url = "https://raw.githubusercontent.com/salesforce/WikiSQL/master/data.tar.bz2"
            r = request.urlopen(url)
            with tarfile.open(fileobj=io.BytesIO(r.read())) as tf:
                tf.extractall(data_dir)

    def _parse_tables(self, tables):
        self._tables = {}
        with open(tables) as f:
            for line in f:
                table = json.loads(line)
                self._tables[table["id"]] = {
                    "columns": table["header"],
                    "types": table["types"],
                    "desc": f"table: {table['id']}\ncolumns: {', '.join(table['header'])}",
                }

    def _parse_queries(self, queries):
        self._queries = []
        with open(queries) as f:
            for line in f:
                query = json.loads(line)
                table = self._tables[query["table_id"]]
                question = query["question"]
                answer = self.query_to_text(
                    query["sql"], query["table_id"], table["columns"], table["types"]
                )
                self._queries.append(
                    f"<s>{table['desc']}\nQ: {question}\nA: {answer}</s>"
                )

    def query_to_text(self, query, table, columns, types):
        aggregation_ops = ["", "MAX", "MIN", "COUNT", "SUM", "AVG"]
        condition_ops = ["=", ">", "<", "OP"]
        column = columns[query["sel"]]
        aggregation = (aggregation_ops[query["agg"]] + " ") if query["agg"] > 0 else ""
        sql = f"SELECT {aggregation}{column} FROM {table}"

        conditions = query["conds"]
        if conditions:
            cs = []
            for i, o, v in conditions:
                column = columns[i]
                op = condition_ops[o]

                if types[i] == "text":
                    value = f"'{v}'"
                else:
                    value = v
                cs.append(f"{column} {op} {value}")

            sql += " WHERE " + " AND ".join(cs)

        return sql

    def __getitem__(self, idx):
        return self._queries[idx]

    def __len__(self):
        return len(self._queries)


if __name__ == "__main__":
    datanames = ["train", "dev", "test"]
    sizes = [56355, 8421, 15878]
    for dataname, size in zip(datanames, sizes):
        len(WikiSQL(dataname)) == size, f"Wrong {dataname} set size."

    # Write the sets to jsonl
    import json

    train, dev, test = load()
    datasets = [
        (train, "train", 1000),
        (dev, "valid", 100),
        (test, "test", 100),
    ]
    for dataset, name, size in datasets:
        with open(f"data/{name}.jsonl", "w") as fid:
            for e, t in zip(range(size), dataset):
                # Strip the <s>, </s> since the tokenizer adds them
                json.dump({"text": t[3:-4]}, fid)
                fid.write("\n")

>>>> lora/data/valid.jsonl
{"text": "table: 1-10015132-11\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: What position does the player who played for butler cc (ks) play?\nA: SELECT Position FROM 1-10015132-11 WHERE School/Club Team = 'Butler CC (KS)'"}
{"text": "table: 1-10015132-11\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: How many schools did player number 3 play at?\nA: SELECT COUNT School/Club Team FROM 1-10015132-11 WHERE No. = '3'"}
{"text": "table: 1-10015132-11\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: What school did player number 21 play for?\nA: SELECT School/Club Team FROM 1-10015132-11 WHERE No. = '21'"}
{"text": "table: 1-10015132-11\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: Who is the player that wears number 42?\nA: SELECT Player FROM 1-10015132-11 WHERE No. = '42'"}
{"text": "table: 1-10015132-11\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: What player played guard for toronto in 1996-97?\nA: SELECT Player FROM 1-10015132-11 WHERE Position = 'Guard' AND Years in Toronto = '1996-97'"}
{"text": "table: 1-10015132-9\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: Who are all of the players on the Westchester High School club team?\nA: SELECT Player FROM 1-10015132-9 WHERE School/Club Team = 'Westchester High School'"}
{"text": "table: 1-10015132-9\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: What school/club team is Amir Johnson on?\nA: SELECT School/Club Team FROM 1-10015132-9 WHERE Player = 'Amir Johnson'"}
{"text": "table: 1-10015132-9\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: What are the total amount of numbers on the Toronto team in 2005-06?\nA: SELECT COUNT No. FROM 1-10015132-9 WHERE Years in Toronto = '2005-06'"}
{"text": "table: 1-10015132-9\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: What are the total number of positions on the Toronto team in 2006-07?\nA: SELECT COUNT Position FROM 1-10015132-9 WHERE Years in Toronto = '2006-07'"}
{"text": "table: 1-10015132-9\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: What are the nationality of the players on the Fresno State school/club team?\nA: SELECT Nationality FROM 1-10015132-9 WHERE School/Club Team = 'Fresno State'"}
{"text": "table: 1-10015132-9\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: What school/club team is Trey Johnson on?\nA: SELECT School/Club Team FROM 1-10015132-9 WHERE Player = 'Trey Johnson'"}
{"text": "table: 1-10026563-1\ncolumns: Entered office as Head of State or Government, Began time as senior G8 leader, Ended time as senior G8 leader, Person, Office\nQ: When did Jacques Chirac stop being a G8 leader?\nA: SELECT Ended time as senior G8 leader FROM 1-10026563-1 WHERE Person = 'Jacques Chirac'"}
{"text": "table: 1-10026563-1\ncolumns: Entered office as Head of State or Government, Began time as senior G8 leader, Ended time as senior G8 leader, Person, Office\nQ: When did the Prime Minister of Italy take office?\nA: SELECT Entered office as Head of State or Government FROM 1-10026563-1 WHERE Office = 'Prime Minister of Italy'"}
{"text": "table: 1-1008653-1\ncolumns: Country ( exonym ), Capital ( exonym ), Country ( endonym ), Capital ( endonym ), Official or native language(s) (alphabet/script)\nQ: What is the English name of the country whose official native language is Dutch Papiamento?\nA: SELECT Country ( exonym ) FROM 1-1008653-1 WHERE Official or native language(s) (alphabet/script) = 'Dutch Papiamento'"}
{"text": "table: 1-1008653-1\ncolumns: Country ( exonym ), Capital ( exonym ), Country ( endonym ), Capital ( endonym ), Official or native language(s) (alphabet/script)\nQ: What official or native languages are spoken in the country whose capital city is Canberra?\nA: SELECT Official or native language(s) (alphabet/script) FROM 1-1008653-1 WHERE Capital ( exonym ) = 'Canberra'"}
{"text": "table: 1-1008653-1\ncolumns: Country ( exonym ), Capital ( exonym ), Country ( endonym ), Capital ( endonym ), Official or native language(s) (alphabet/script)\nQ: What is the local name given to the city of Canberra?\nA: SELECT Capital ( endonym ) FROM 1-1008653-1 WHERE Capital ( exonym ) = 'Canberra'"}
{"text": "table: 1-1008653-1\ncolumns: Country ( exonym ), Capital ( exonym ), Country ( endonym ), Capital ( endonym ), Official or native language(s) (alphabet/script)\nQ: What is the local name given to the capital of Anguilla?\nA: SELECT Capital ( endonym ) FROM 1-1008653-1 WHERE Country ( endonym ) = 'Anguilla'"}
{"text": "table: 1-1008653-1\ncolumns: Country ( exonym ), Capital ( exonym ), Country ( endonym ), Capital ( endonym ), Official or native language(s) (alphabet/script)\nQ: What is the English name given to the city of St. John's?\nA: SELECT Capital ( exonym ) FROM 1-1008653-1 WHERE Capital ( endonym ) = 'St. John's'"}
{"text": "table: 1-1008653-1\ncolumns: Country ( exonym ), Capital ( exonym ), Country ( endonym ), Capital ( endonym ), Official or native language(s) (alphabet/script)\nQ: How many capital cities does Australia have?\nA: SELECT COUNT Capital ( endonym ) FROM 1-1008653-1 WHERE Country ( endonym ) = 'Australia'"}
{"text": "table: 1-10088101-1\ncolumns: No. in set, No. in series, Title, Directed by, Written by, Original air date, Production code\nQ: The episode with production code 9abx02 was originally aired on what date?\nA: SELECT Original air date FROM 1-10088101-1 WHERE Production code = '9ABX02'"}
{"text": "table: 1-10088101-1\ncolumns: No. in set, No. in series, Title, Directed by, Written by, Original air date, Production code\nQ: What is the episode number that has production code 8abx15?\nA: SELECT MIN No. in series FROM 1-10088101-1 WHERE Production code = '8ABX15'"}
{"text": "table: 1-10295819-2\ncolumns: Player, Highest singles ranking, Highest doubles ranking, First year played, Years played, Ties played, Total W\u2013L, Singles W\u2013L, Doubles W\u2013L\nQ: Name the minimum tiesplayed for 6 years\nA: SELECT MIN Ties played FROM 1-10295819-2 WHERE Years played = 6"}
{"text": "table: 1-10342194-3\ncolumns: District, Total amount of trees, Prevailing types, %, Amount of old trees, Amount of trees, that require replacement\nQ: What is the amount of trees, that require replacement when prevailing types, % is pine \u2014 29.37 poplar \u2014 26.12 acer negundo \u2014 13.2?\nA: SELECT Amount of trees, that require replacement FROM 1-10342194-3 WHERE Prevailing types, % = 'Pine \u2014 29.37 Poplar \u2014 26.12 Acer negundo \u2014 13.2'"}
{"text": "table: 1-10342194-3\ncolumns: District, Total amount of trees, Prevailing types, %, Amount of old trees, Amount of trees, that require replacement\nQ: What is the amount of trees, that require replacement when district is leninsky?\nA: SELECT Amount of trees, that require replacement FROM 1-10342194-3 WHERE District = 'Leninsky'"}
{"text": "table: 1-10342194-3\ncolumns: District, Total amount of trees, Prevailing types, %, Amount of old trees, Amount of trees, that require replacement\nQ: What is the district when the total amount of trees is smaller than 150817.6878461314 and amount of old trees is 1,928 (1.89%)?\nA: SELECT District FROM 1-10342194-3 WHERE Total amount of trees < 150817.6878461314 AND Amount of old trees = '1,928 (1.89%)'"}
{"text": "table: 1-10342194-3\ncolumns: District, Total amount of trees, Prevailing types, %, Amount of old trees, Amount of trees, that require replacement\nQ: What is the amount of trees, that require replacement when the district is motovilikhinsky?\nA: SELECT Amount of trees, that require replacement FROM 1-10342194-3 WHERE District = 'Motovilikhinsky'"}
{"text": "table: 1-10342194-3\ncolumns: District, Total amount of trees, Prevailing types, %, Amount of old trees, Amount of trees, that require replacement\nQ: What is the total amount of trees when district is leninsky?\nA: SELECT MAX Total amount of trees FROM 1-10342194-3 WHERE District = 'Leninsky'"}
{"text": "table: 1-10342194-3\ncolumns: District, Total amount of trees, Prevailing types, %, Amount of old trees, Amount of trees, that require replacement\nQ: What is the district when prevailing types, % is acer negundo \u2014 30.22 tilia \u2014 18.6 poplar \u2014 15.23?\nA: SELECT District FROM 1-10342194-3 WHERE Prevailing types, % = 'Acer negundo \u2014 30.22 Tilia \u2014 18.6 Poplar \u2014 15.23'"}
{"text": "table: 1-10429820-13\ncolumns: Iowa State vs., Overall Record, in Ames, at Opponents Venue, at Neutral Site, Last 5 Meetings, Last 10 Meetings, Current Streak, Since Beginning of Big 12\nQ: When the value of \"since beginning of big 12\" is synonymous with its' category, what are the in Ames values?\nA: SELECT in Ames FROM 1-10429820-13 WHERE Since Beginning of Big 12 = 'Since Beginning of Big 12'"}
{"text": "table: 1-1046170-5\ncolumns: Year, Division, League, Regular Season, Playoffs, U.S. Open Cup\nQ: what's the\u00a0u.s. open cup status\u00a0for regular season\u00a0of 4th, atlantic division \nA: SELECT U.S. Open Cup FROM 1-1046170-5 WHERE Regular Season = '4th, Atlantic Division'"}
{"text": "table: 1-1046170-5\ncolumns: Year, Division, League, Regular Season, Playoffs, U.S. Open Cup\nQ: how many division  did not qualify for u.s. open cup in 2003\nA: SELECT Division FROM 1-1046170-5 WHERE U.S. Open Cup = 'Did Not Qualify' AND Year = 2003"}
{"text": "table: 1-1046170-5\ncolumns: Year, Division, League, Regular Season, Playoffs, U.S. Open Cup\nQ: which round is u.s. open cup division semifinals\nA: SELECT U.S. Open Cup FROM 1-1046170-5 WHERE Playoffs = 'Division Semifinals'"}
{"text": "table: 1-1046170-5\ncolumns: Year, Division, League, Regular Season, Playoffs, U.S. Open Cup\nQ: what are all the playoffs for regular season is 1st, atlantic division\nA: SELECT Playoffs FROM 1-1046170-5 WHERE Regular Season = '1st, Atlantic Division'"}
{"text": "table: 1-1046170-5\ncolumns: Year, Division, League, Regular Season, Playoffs, U.S. Open Cup\nQ: what are all the playoffs for u.s. open cup in 1st round\nA: SELECT Playoffs FROM 1-1046170-5 WHERE U.S. Open Cup = '1st Round'"}
{"text": "table: 1-1061075-1\ncolumns: Season, Competition, Round, Opponents, 1st leg, 2nd leg, Aggregate\nQ: what is the total number of\u00a02nd leg\u00a0where\u00a0aggregate\u00a0is 7-2\nA: SELECT COUNT 2nd leg FROM 1-1061075-1 WHERE Aggregate = '7-2'"}
{"text": "table: 1-1061075-1\ncolumns: Season, Competition, Round, Opponents, 1st leg, 2nd leg, Aggregate\nQ:  what's the\u00a0aggregate\u00a0where\u00a01st leg\u00a0is 3\u20132\nA: SELECT Aggregate FROM 1-1061075-1 WHERE 1st leg = '3\u20132'"}
{"text": "table: 1-1061075-1\ncolumns: Season, Competition, Round, Opponents, 1st leg, 2nd leg, Aggregate\nQ:  what's the\u00a0competition\u00a0where\u00a0aggregate\u00a0is 4\u20137\nA: SELECT Competition FROM 1-1061075-1 WHERE Aggregate = '4\u20137'"}
{"text": "table: 1-1061075-1\ncolumns: Season, Competition, Round, Opponents, 1st leg, 2nd leg, Aggregate\nQ:  what's the\u00a0competition\u00a0where\u00a01st leg\u00a0is 4-1 (h)\nA: SELECT Competition FROM 1-1061075-1 WHERE 1st leg = '4-1 (h)'"}
{"text": "table: 1-1061075-1\ncolumns: Season, Competition, Round, Opponents, 1st leg, 2nd leg, Aggregate\nQ: what is the total number of\u00a0round\u00a0where\u00a0opponents\u00a0is haugar\nA: SELECT COUNT Round FROM 1-1061075-1 WHERE Opponents = 'Haugar'"}
{"text": "table: 1-1061075-1\ncolumns: Season, Competition, Round, Opponents, 1st leg, 2nd leg, Aggregate\nQ:  what's the\u00a01st leg\u00a0where\u00a0opponents\u00a0is galatasaray\nA: SELECT 1st leg FROM 1-1061075-1 WHERE Opponents = 'Galatasaray'"}
{"text": "table: 1-10706961-2\ncolumns: Rd, Name, Pole Position, Fastest Lap, Winning driver, Winning team, Report\nQ: What is the highest Rd that Tom Sneva had the pole position in?\nA: SELECT MAX Rd FROM 1-10706961-2 WHERE Pole Position = 'Tom Sneva'"}
{"text": "table: 1-10706961-2\ncolumns: Rd, Name, Pole Position, Fastest Lap, Winning driver, Winning team, Report\nQ: How many winning drivers were there in the race that had a fastest lap time of 56.920?\nA: SELECT COUNT Winning driver FROM 1-10706961-2 WHERE Fastest Lap = '56.920'"}
{"text": "table: 1-10706961-2\ncolumns: Rd, Name, Pole Position, Fastest Lap, Winning driver, Winning team, Report\nQ: How many reports are there in the race that Forsythe Racing won and Teo Fabi had the pole position in?\nA: SELECT COUNT Report FROM 1-10706961-2 WHERE Winning team = 'Forsythe Racing' AND Pole Position = 'Teo Fabi'"}
{"text": "table: 1-10706961-2\ncolumns: Rd, Name, Pole Position, Fastest Lap, Winning driver, Winning team, Report\nQ: Which Rd took place at the Indianapolis 500?\nA: SELECT Rd FROM 1-10706961-2 WHERE Name = 'Indianapolis 500'"}
{"text": "table: 1-10706961-2\ncolumns: Rd, Name, Pole Position, Fastest Lap, Winning driver, Winning team, Report\nQ: Which teams won when Bobby Rahal was their winning driver?\nA: SELECT Winning team FROM 1-10706961-2 WHERE Winning driver = 'Bobby Rahal'"}
{"text": "table: 1-10706961-2\ncolumns: Rd, Name, Pole Position, Fastest Lap, Winning driver, Winning team, Report\nQ: What was the fastest lap time in the Escort Radar Warning 200?\nA: SELECT Fastest Lap FROM 1-10706961-2 WHERE Name = 'Escort Radar Warning 200'"}
{"text": "table: 1-10707176-2\ncolumns: Rnd, Race Name, Circuit, City/Location, Date, Pole position, Winning driver, Winning team, Report\nQ: What report was there for the porsche north america?\nA: SELECT Report FROM 1-10707176-2 WHERE Winning team = 'Porsche North America'"}
{"text": "table: 1-10707176-2\ncolumns: Rnd, Race Name, Circuit, City/Location, Date, Pole position, Winning driver, Winning team, Report\nQ: What rnds were there for the phoenix international raceway?\nA: SELECT Rnd FROM 1-10707176-2 WHERE Circuit = 'Phoenix International Raceway'"}
{"text": "table: 1-10707176-2\ncolumns: Rnd, Race Name, Circuit, City/Location, Date, Pole position, Winning driver, Winning team, Report\nQ: Who was the pole position for the rnd equalling 12?\nA: SELECT Pole position FROM 1-10707176-2 WHERE Rnd = '12'"}
{"text": "table: 1-10707176-2\ncolumns: Rnd, Race Name, Circuit, City/Location, Date, Pole position, Winning driver, Winning team, Report\nQ: How many reports were the for the cleveland burke lakefront airport circut?\nA: SELECT COUNT Report FROM 1-10707176-2 WHERE Circuit = 'Cleveland Burke Lakefront Airport'"}
{"text": "table: 1-10707176-2\ncolumns: Rnd, Race Name, Circuit, City/Location, Date, Pole position, Winning driver, Winning team, Report\nQ: How many winning drivers were the for the rnd equalling 5?\nA: SELECT COUNT Winning driver FROM 1-10707176-2 WHERE Rnd = '5'"}
{"text": "table: 1-10706879-3\ncolumns: Rd, Name, Pole Position, Fastest Lap, Winning driver, Winning team, Report\nQ: The race tony bettenhausen 200 has what smallest rd?\nA: SELECT MIN Rd FROM 1-10706879-3 WHERE Name = 'Tony Bettenhausen 200'"}
{"text": "table: 1-10706879-3\ncolumns: Rd, Name, Pole Position, Fastest Lap, Winning driver, Winning team, Report\nQ: The winning team of the race, los angeles times 500 is who?\nA: SELECT Winning team FROM 1-10706879-3 WHERE Name = 'Los Angeles Times 500'"}
{"text": "table: 1-10706879-3\ncolumns: Rd, Name, Pole Position, Fastest Lap, Winning driver, Winning team, Report\nQ: How many winning drivers in the kraco twin 125 (r2) race were there?\nA: SELECT COUNT Winning driver FROM 1-10706879-3 WHERE Name = 'Kraco Twin 125 (R2)'"}
{"text": "table: 1-10706879-3\ncolumns: Rd, Name, Pole Position, Fastest Lap, Winning driver, Winning team, Report\nQ: What are the races that johnny rutherford has won?\nA: SELECT Name FROM 1-10706879-3 WHERE Winning driver = 'Johnny Rutherford'"}
{"text": "table: 1-10706879-3\ncolumns: Rd, Name, Pole Position, Fastest Lap, Winning driver, Winning team, Report\nQ: How many fastest laps were there for a rd that equals 10?\nA: SELECT COUNT Fastest Lap FROM 1-10706879-3 WHERE Rd = 10"}
{"text": "table: 1-10712301-5\ncolumns: Region, Operator, Licence award date, On air date, Closure date\nQ: What is the license award date for North East England?\nA: SELECT Licence award date FROM 1-10712301-5 WHERE Region = 'North East England'"}
{"text": "table: 1-10733530-3\ncolumns: Nation, Population (thousands), Internet subscriptions (2000) (thousands of users), Internet subscriptions (2008) (thousands of users), % growth (2000\u20132008), % Internet users\nQ: What is the percentage of growth in 2000-2008 in ethiopia?\nA: SELECT % growth (2000\u20132008) FROM 1-10733530-3 WHERE Nation = 'Ethiopia'"}
{"text": "table: 1-10733530-3\ncolumns: Nation, Population (thousands), Internet subscriptions (2000) (thousands of users), Internet subscriptions (2008) (thousands of users), % growth (2000\u20132008), % Internet users\nQ: Name the total number of percentage growth 2000-2008 of uganda?\nA: SELECT COUNT % growth (2000\u20132008) FROM 1-10733530-3 WHERE Nation = 'Uganda'"}
{"text": "table: 1-10733530-3\ncolumns: Nation, Population (thousands), Internet subscriptions (2000) (thousands of users), Internet subscriptions (2008) (thousands of users), % growth (2000\u20132008), % Internet users\nQ: What is the maximum percentage grown 2000-2008 in burundi\nA: SELECT MAX % growth (2000\u20132008) FROM 1-10733530-3 WHERE Nation = 'Burundi'"}
{"text": "table: 1-10798421-1\ncolumns: Village (German), Village (Slovenian), Number of people 1991, Percent of Slovenes 1991, Percent of Slovenes 1951\nQ: Provide me with the names of all the villages (German) that has 76.3% of Slovenes in 1951.\nA: SELECT Village (German) FROM 1-10798421-1 WHERE Percent of Slovenes 1951 = '76.3%'"}
{"text": "table: 1-10798421-1\ncolumns: Village (German), Village (Slovenian), Number of people 1991, Percent of Slovenes 1991, Percent of Slovenes 1951\nQ: Give me the minimum number of people in 1991 with 92.5% of Slovenes in 1991.\nA: SELECT MIN Number of people 1991 FROM 1-10798421-1 WHERE Percent of Slovenes 1991 = '92.5%'"}
{"text": "table: 1-10798421-1\ncolumns: Village (German), Village (Slovenian), Number of people 1991, Percent of Slovenes 1991, Percent of Slovenes 1951\nQ: Provide me with the name of all the village (German) that are part of the village (Slovenian) with sele srednji kot. \nA: SELECT Village (German) FROM 1-10798421-1 WHERE Village (Slovenian) = 'Sele Srednji Kot'"}
{"text": "table: 1-10798421-1\ncolumns: Village (German), Village (Slovenian), Number of people 1991, Percent of Slovenes 1991, Percent of Slovenes 1951\nQ: Provide me with the name of all the village (German) that are part of the village (Slovenian) with sele borovnica.\nA: SELECT Village (German) FROM 1-10798421-1 WHERE Village (Slovenian) = 'Sele Borovnica'"}
{"text": "table: 1-10798421-1\ncolumns: Village (German), Village (Slovenian), Number of people 1991, Percent of Slovenes 1991, Percent of Slovenes 1951\nQ: Provide me with the name of the village (German) where there is 96.9% Slovenes in 1951. \nA: SELECT Village (German) FROM 1-10798421-1 WHERE Percent of Slovenes 1951 = '96.9%'"}
{"text": "table: 1-10798421-1\ncolumns: Village (German), Village (Slovenian), Number of people 1991, Percent of Slovenes 1991, Percent of Slovenes 1951\nQ: Provide with the names of the village (German) that is part of village (Slovenian) with sele srednji kot.\nA: SELECT Village (German) FROM 1-10798421-1 WHERE Village (Slovenian) = 'Sele Srednji Kot'"}
{"text": "table: 1-10812293-3\ncolumns: Game, Date, Team, Score, High points, High rebounds, High assists, Location Attendance, Record\nQ: What was the score of the game on November 12?\nA: SELECT Score FROM 1-10812293-3 WHERE Date = 'November 12'"}
{"text": "table: 1-10812293-3\ncolumns: Game, Date, Team, Score, High points, High rebounds, High assists, Location Attendance, Record\nQ: Who had high assists when they played against San Antonio?\nA: SELECT High assists FROM 1-10812293-3 WHERE Team = 'San Antonio'"}
{"text": "table: 1-10812293-3\ncolumns: Game, Date, Team, Score, High points, High rebounds, High assists, Location Attendance, Record\nQ: Who scored the most points in game 4?\nA: SELECT High points FROM 1-10812293-3 WHERE Game = 4"}
{"text": "table: 1-10812293-3\ncolumns: Game, Date, Team, Score, High points, High rebounds, High assists, Location Attendance, Record\nQ: Where was the game on November 20?\nA: SELECT Location Attendance FROM 1-10812293-3 WHERE Date = 'November 20'"}
{"text": "table: 1-10935205-1\ncolumns: No. in season, No. in series, Title, Canadian airdate, US airdate, Production code\nQ: The canadian airdate of 11 february 2008 applied to what series number?\nA: SELECT COUNT No. in series FROM 1-10935205-1 WHERE Canadian airdate = '11 February 2008'"}
{"text": "table: 1-10935205-1\ncolumns: No. in season, No. in series, Title, Canadian airdate, US airdate, Production code\nQ: The U.S. airdate of 4 april 2008 had a production code of what?\nA: SELECT MAX Production code FROM 1-10935205-1 WHERE US airdate = '4 April 2008'"}
{"text": "table: 1-10935205-1\ncolumns: No. in season, No. in series, Title, Canadian airdate, US airdate, Production code\nQ: The episode titled \"don't stop believin'\" was what highest number of the season?\nA: SELECT MAX No. in season FROM 1-10935205-1 WHERE Title = '\"Don't Stop Believin'\"'"}
{"text": "table: 1-10935205-1\ncolumns: No. in season, No. in series, Title, Canadian airdate, US airdate, Production code\nQ: The U.S. airdate of 8 august 2008 also had canadian airdates of what?\nA: SELECT Canadian airdate FROM 1-10935205-1 WHERE US airdate = '8 August 2008'"}
{"text": "table: 1-10935205-1\ncolumns: No. in season, No. in series, Title, Canadian airdate, US airdate, Production code\nQ: The canadian airdate of 17 march 2008 had how many numbers in the season?\nA: SELECT COUNT No. in season FROM 1-10935205-1 WHERE Canadian airdate = '17 March 2008'"}
{"text": "table: 1-10935205-1\ncolumns: No. in season, No. in series, Title, Canadian airdate, US airdate, Production code\nQ: For the episode(s) aired in the U.S. on 4 april 2008, what were the names?\nA: SELECT Title FROM 1-10935205-1 WHERE US airdate = '4 April 2008'"}
{"text": "table: 1-10953197-5\ncolumns: No. in series, No. in season, Title, Director, Writer(s), Original air date, Production code\nQ: Who directed the episode \"Great Sexpectations (2)\"?\nA: SELECT Director FROM 1-10953197-5 WHERE Title = '\"Great Sexpectations (2)\"'"}
{"text": "table: 1-10975034-2\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: Which player from the 2004 CFL draft attended Wilfrid Laurier?\nA: SELECT Player FROM 1-10975034-2 WHERE College = 'Wilfrid Laurier'"}
{"text": "table: 1-10975034-2\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What position does Christian Leibl-Cote play?\nA: SELECT Position FROM 1-10975034-2 WHERE Player = 'Christian Leibl-Cote'"}
{"text": "table: 1-10975034-2\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What is the pick number for Northwestern college?\nA: SELECT MAX Pick # FROM 1-10975034-2 WHERE College = 'Northwestern'"}
{"text": "table: 1-10992-3\ncolumns: No, City district (Stadtteil), Area in km\u00b2, Population, Foreign nationals, Foreign nationals in %, Area district (Ortsbezirk)\nQ: How many foreigners in percentage terms had a population of 4.911?\nA: SELECT COUNT Foreign nationals in % FROM 1-10992-3 WHERE Population = '4.911'"}
{"text": "table: 1-10992-3\ncolumns: No, City district (Stadtteil), Area in km\u00b2, Population, Foreign nationals, Foreign nationals in %, Area district (Ortsbezirk)\nQ: What is the number of the city district of stadtteil where foreigners are 5.162?\nA: SELECT COUNT City district (Stadtteil) FROM 1-10992-3 WHERE Foreign nationals = '5.162'"}
{"text": "table: 1-10992-3\ncolumns: No, City district (Stadtteil), Area in km\u00b2, Population, Foreign nationals, Foreign nationals in %, Area district (Ortsbezirk)\nQ: What is the city where the number is 47?\nA: SELECT City district (Stadtteil) FROM 1-10992-3 WHERE No = '47'"}
{"text": "table: 1-11044765-1\ncolumns: School, Mascot, Location, League, Enrollment\nQ: Which leagues have Raiders as their mascot?\nA: SELECT League FROM 1-11044765-1 WHERE Mascot = 'Raiders'"}
{"text": "table: 1-11044765-1\ncolumns: School, Mascot, Location, League, Enrollment\nQ: Which leagues is the Galena school in?\nA: SELECT League FROM 1-11044765-1 WHERE School = 'Galena'"}
{"text": "table: 1-11044765-1\ncolumns: School, Mascot, Location, League, Enrollment\nQ: What city and state is the Lancers mascot located?\nA: SELECT Location FROM 1-11044765-1 WHERE Mascot = 'Lancers'"}
{"text": "table: 1-11044765-1\ncolumns: School, Mascot, Location, League, Enrollment\nQ: What city and state are the miners located in?\nA: SELECT Location FROM 1-11044765-1 WHERE Mascot = 'Miners'"}
{"text": "table: 1-11044765-1\ncolumns: School, Mascot, Location, League, Enrollment\nQ: Which school has the Raiders as their mascot?\nA: SELECT School FROM 1-11044765-1 WHERE Mascot = 'Raiders'"}
{"text": "table: 1-1121352-2\ncolumns: No., Date, Tournament, Winning score, To par, Margin of victory, Runner(s)-up\nQ: Where was the tournament dated nov 3, 2002?\nA: SELECT Tournament FROM 1-1121352-2 WHERE Date = 'Nov 3, 2002'"}
{"text": "table: 1-1121352-2\ncolumns: No., Date, Tournament, Winning score, To par, Margin of victory, Runner(s)-up\nQ: Where is the margin of victory dated mar 28, 2004?\nA: SELECT Margin of victory FROM 1-1121352-2 WHERE Date = 'Mar 28, 2004'"}
{"text": "table: 1-1121352-2\ncolumns: No., Date, Tournament, Winning score, To par, Margin of victory, Runner(s)-up\nQ: What is the to par dated may 4, 2003?\nA: SELECT To par FROM 1-1121352-2 WHERE Date = 'May 4, 2003'"}
{"text": "table: 1-1121352-2\ncolumns: No., Date, Tournament, Winning score, To par, Margin of victory, Runner(s)-up\nQ: What date were the runner ups pat hurst juli inkster?\nA: SELECT Date FROM 1-1121352-2 WHERE Runner(s)-up = 'Pat Hurst Juli Inkster'"}
{"text": "table: 1-11210576-4\ncolumns: Character, Fate, Actor, First Episode, Final Episode, Duration, Final Episode Count\nQ: what's the total number of\u00a0final epbeingode count\u00a0with\u00a0character\u00a0being rick stetler\nA: SELECT COUNT Final Episode Count FROM 1-11210576-4 WHERE Character = 'Rick Stetler'"}
{"text": "table: 1-11210576-4\ncolumns: Character, Fate, Actor, First Episode, Final Episode, Duration, Final Episode Count\nQ: what are all the actor where first episode is \"ambush\"\nA: SELECT Actor FROM 1-11210576-4 WHERE First Episode = '\"Ambush\"'"}
{"text": "table: 1-11210576-4\ncolumns: Character, Fate, Actor, First Episode, Final Episode, Duration, Final Episode Count\nQ: what's the\u00a0character\u00a0with\u00a0fate\u00a0being deceased: knife wound\nA: SELECT Character FROM 1-11210576-4 WHERE Fate = 'Deceased: Knife Wound'"}
{"text": "table: 1-11210576-4\ncolumns: Character, Fate, Actor, First Episode, Final Episode, Duration, Final Episode Count\nQ: what's the total number of\u00a0final epbeingode count\u00a0with\u00a0first epbeingode\u00a0being \"l.a.\"\nA: SELECT COUNT Final Episode Count FROM 1-11210576-4 WHERE First Episode = '\"L.A.\"'"}
{"text": "table: 1-11210576-4\ncolumns: Character, Fate, Actor, First Episode, Final Episode, Duration, Final Episode Count\nQ: what's the\u00a0actor\u00a0with\u00a0character\u00a0being judge joseph ratner\nA: SELECT Actor FROM 1-11210576-4 WHERE Character = 'Judge Joseph Ratner'"}
{"text": "table: 1-11210576-4\ncolumns: Character, Fate, Actor, First Episode, Final Episode, Duration, Final Episode Count\nQ: what's the\u00a0first epbeingode\u00a0with\u00a0final epbeingode\u00a0being \"rio\"\nA: SELECT First Episode FROM 1-11210576-4 WHERE Final Episode = '\"Rio\"'"}
{"text": "table: 1-11214772-2\ncolumns: Year, Champion, Score, Runner-Up, Location, Semi-Finalist #1, Semi-Finalist #2\nQ: Which team was the second semi finalist in 2007?\nA: SELECT Semi-Finalist #2 FROM 1-11214772-2 WHERE Year = 2007"}
{"text": "table: 1-11214772-2\ncolumns: Year, Champion, Score, Runner-Up, Location, Semi-Finalist #1, Semi-Finalist #2\nQ: How many teams were listed as runner up in 2005 and there the first semi finalist was Western Carolina?\nA: SELECT COUNT Runner-Up FROM 1-11214772-2 WHERE Semi-Finalist #1 = 'Western Carolina' AND Year = 2005"}

>>>> lora/data/test.jsonl
{"text": "table: 1-10015132-16\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: What is terrence ross' nationality\nA: SELECT Nationality FROM 1-10015132-16 WHERE Player = 'Terrence Ross'"}
{"text": "table: 1-10015132-16\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: What clu was in toronto 1995-96\nA: SELECT School/Club Team FROM 1-10015132-16 WHERE Years in Toronto = '1995-96'"}
{"text": "table: 1-10015132-16\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: which club was in toronto 2003-06\nA: SELECT School/Club Team FROM 1-10015132-16 WHERE Years in Toronto = '2003-06'"}
{"text": "table: 1-10015132-16\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: how many schools or teams had jalen rose\nA: SELECT COUNT School/Club Team FROM 1-10015132-16 WHERE Player = 'Jalen Rose'"}
{"text": "table: 1-10083598-1\ncolumns: No, Date, Round, Circuit, Pole Position, Fastest Lap, Race winner, Report\nQ: Where was Assen held?\nA: SELECT Round FROM 1-10083598-1 WHERE Circuit = 'Assen'"}
{"text": "table: 1-10083598-1\ncolumns: No, Date, Round, Circuit, Pole Position, Fastest Lap, Race winner, Report\nQ: What was the number of race that Kevin Curtain won?\nA: SELECT COUNT No FROM 1-10083598-1 WHERE Pole Position = 'Kevin Curtain'"}
{"text": "table: 1-10083598-1\ncolumns: No, Date, Round, Circuit, Pole Position, Fastest Lap, Race winner, Report\nQ: What was the date of the race in Misano?\nA: SELECT Date FROM 1-10083598-1 WHERE Circuit = 'Misano'"}
{"text": "table: 1-1013129-2\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: How many different positions did Sherbrooke Faucons (qmjhl) provide in the draft?\nA: SELECT COUNT Position FROM 1-1013129-2 WHERE College/junior/club team = 'Sherbrooke Faucons (QMJHL)'"}
{"text": "table: 1-1013129-2\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: What are the nationalities of the player picked from Thunder Bay Flyers (ushl)\nA: SELECT Nationality FROM 1-1013129-2 WHERE College/junior/club team = 'Thunder Bay Flyers (USHL)'"}
{"text": "table: 1-1013129-2\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: How many different college/junior/club teams provided a player to the Washington Capitals NHL Team?\nA: SELECT COUNT College/junior/club team FROM 1-1013129-2 WHERE NHL team = 'Washington Capitals'"}
{"text": "table: 1-1013129-3\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: How many different nationalities do the players of New Jersey Devils come from?\nA: SELECT COUNT Nationality FROM 1-1013129-3 WHERE NHL team = 'New Jersey Devils'"}
{"text": "table: 1-1013129-3\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: What's Dorain Anneck's pick number?\nA: SELECT Pick FROM 1-1013129-3 WHERE Player = 'Dorain Anneck'"}
{"text": "table: 1-1013129-3\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: What is the nationality of the player from Vancouver Canucks?\nA: SELECT Nationality FROM 1-1013129-3 WHERE NHL team = 'Vancouver Canucks'"}
{"text": "table: 1-1013129-3\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: What's the pick number of the player from Springfield Olympics (Nejhl)?\nA: SELECT Pick FROM 1-1013129-3 WHERE College/junior/club team = 'Springfield Olympics (NEJHL)'"}
{"text": "table: 1-1014206-2\ncolumns: #, Shipyard, Laid down, Launched, Commissioned, Fleet, Status\nQ: When were the ships launched that were laid down on september 1, 1964?\nA: SELECT Launched FROM 1-1014206-2 WHERE Laid down = 'September 1, 1964'"}
{"text": "table: 1-1014206-2\ncolumns: #, Shipyard, Laid down, Launched, Commissioned, Fleet, Status\nQ: List the # for ships commissioned on december 18, 1965.\nA: SELECT # FROM 1-1014206-2 WHERE Commissioned = 'December 18, 1965'"}
{"text": "table: 1-1014206-2\ncolumns: #, Shipyard, Laid down, Launched, Commissioned, Fleet, Status\nQ: List the # for ships commissioned on september 30, 1967.\nA: SELECT # FROM 1-1014206-2 WHERE Commissioned = 'September 30, 1967'"}
{"text": "table: 1-1014206-2\ncolumns: #, Shipyard, Laid down, Launched, Commissioned, Fleet, Status\nQ: When were ships laid down that were commissioned on october 29, 1965?\nA: SELECT Laid down FROM 1-1014206-2 WHERE Commissioned = 'October 29, 1965'"}
{"text": "table: 1-1015521-2\ncolumns: Equivalent NATO Rank Code, Rank in Spanish, Rank in English, Commonwealth equivalent, US Air Force equivalent\nQ:  What could a spanish coronel be addressed as in the commonwealth military?\nA: SELECT Commonwealth equivalent FROM 1-1015521-2 WHERE Rank in Spanish = 'Coronel'"}
{"text": "table: 1-1015521-2\ncolumns: Equivalent NATO Rank Code, Rank in Spanish, Rank in English, Commonwealth equivalent, US Air Force equivalent\nQ: Give me a list of all spanish officer titles that could receive recognition as group captain in english\nA: SELECT Rank in English FROM 1-1015521-2 WHERE Commonwealth equivalent = 'Group Captain'"}
{"text": "table: 1-1015521-2\ncolumns: Equivalent NATO Rank Code, Rank in Spanish, Rank in English, Commonwealth equivalent, US Air Force equivalent\nQ: If you are a pilot officer in the commonwealth then what will you called as in the US air force?\nA: SELECT US Air Force equivalent FROM 1-1015521-2 WHERE Commonwealth equivalent = 'Pilot Officer'"}
{"text": "table: 1-1015521-2\ncolumns: Equivalent NATO Rank Code, Rank in Spanish, Rank in English, Commonwealth equivalent, US Air Force equivalent\nQ: If you're a major general in the US air force then what ranking will you receive in the commonwealth's air force?\nA: SELECT Commonwealth equivalent FROM 1-1015521-2 WHERE US Air Force equivalent = 'Major General'"}
{"text": "table: 1-1015521-2\ncolumns: Equivalent NATO Rank Code, Rank in Spanish, Rank in English, Commonwealth equivalent, US Air Force equivalent\nQ: If you get a ranking as major in the  english military then what would the spanish military address you as? \nA: SELECT Rank in Spanish FROM 1-1015521-2 WHERE Rank in English = 'Major'"}
{"text": "table: 1-10182508-5\ncolumns: Rank Each wrestlers total number of days as champion are ranked highest to lowest; wrestlers with the same number mean that they are tied for that certain rank., Wrestler, # of reigns, Combined defenses, Combined days\nQ: Which wrestlers have had 2 reigns?\nA: SELECT Wrestler FROM 1-10182508-5 WHERE # of reigns = 2"}
{"text": "table: 1-10182508-5\ncolumns: Rank Each wrestlers total number of days as champion are ranked highest to lowest; wrestlers with the same number mean that they are tied for that certain rank., Wrestler, # of reigns, Combined defenses, Combined days\nQ: In terms of reigns, what is the lowest number listed?\nA: SELECT MIN # of reigns FROM 1-10182508-5"}
{"text": "table: 1-10182508-5\ncolumns: Rank Each wrestlers total number of days as champion are ranked highest to lowest; wrestlers with the same number mean that they are tied for that certain rank., Wrestler, # of reigns, Combined defenses, Combined days\nQ: What rank was Bryan Danielson in this chart?\nA: SELECT Rank Each wrestlers total number of days as champion are ranked highest to lowest; wrestlers with the same number mean that they are tied for that certain rank. FROM 1-10182508-5 WHERE Wrestler = 'Bryan Danielson'"}
{"text": "table: 1-10182508-5\ncolumns: Rank Each wrestlers total number of days as champion are ranked highest to lowest; wrestlers with the same number mean that they are tied for that certain rank., Wrestler, # of reigns, Combined defenses, Combined days\nQ: How many combined days did Go Shiozaki have?\nA: SELECT Combined days FROM 1-10182508-5 WHERE Wrestler = 'Go Shiozaki'"}
{"text": "table: 1-10182508-5\ncolumns: Rank Each wrestlers total number of days as champion are ranked highest to lowest; wrestlers with the same number mean that they are tied for that certain rank., Wrestler, # of reigns, Combined defenses, Combined days\nQ: What was Go Shiozaki's rank?\nA: SELECT MIN Rank Each wrestlers total number of days as champion are ranked highest to lowest; wrestlers with the same number mean that they are tied for that certain rank. FROM 1-10182508-5 WHERE Wrestler = 'Go Shiozaki'"}
{"text": "table: 1-1024710-2\ncolumns: Member, Electorate, Province, MPs term, Election date\nQ: Which province is grey and bell electorate in\nA: SELECT Province FROM 1-1024710-2 WHERE Electorate = 'Grey and Bell'"}
{"text": "table: 1-1024710-2\ncolumns: Member, Electorate, Province, MPs term, Election date\nQ: Which province is bay of islands in\nA: SELECT Province FROM 1-1024710-2 WHERE Electorate = 'Bay of Islands'"}
{"text": "table: 1-10294071-1\ncolumns: Player, Total W\u2013L, Singles W\u2013L, Doubles W\u2013L, Ties played, Debut, Years played\nQ: what is the total number of\u00a0total w\u2013l\u00a0where\u00a0doubles w\u2013l\u00a0is 11\u201311\nA: SELECT COUNT Total W\u2013L FROM 1-10294071-1 WHERE Doubles W\u2013L = '11\u201311'"}
{"text": "table: 1-10294071-1\ncolumns: Player, Total W\u2013L, Singles W\u2013L, Doubles W\u2013L, Ties played, Debut, Years played\nQ: what is the total number of\u00a0singles w\u2013l\u00a0where\u00a0doubles w\u2013l\u00a0is 11\u201314\nA: SELECT COUNT Singles W\u2013L FROM 1-10294071-1 WHERE Doubles W\u2013L = '11\u201314'"}
{"text": "table: 1-10294071-1\ncolumns: Player, Total W\u2013L, Singles W\u2013L, Doubles W\u2013L, Ties played, Debut, Years played\nQ:  what's the\u00a0total w\u2013l\u00a0where\u00a0player\u00a0is boro jovanovi\u0107 category:articles with hcards\nA: SELECT Total W\u2013L FROM 1-10294071-1 WHERE Player = 'Boro Jovanovi\u0107 Category:Articles with hCards'"}
{"text": "table: 1-10294071-1\ncolumns: Player, Total W\u2013L, Singles W\u2013L, Doubles W\u2013L, Ties played, Debut, Years played\nQ: what is the maximum\u00a0ties played\u00a0where\u00a0player\u00a0is josip palada category:articles with hcards\nA: SELECT MAX Ties played FROM 1-10294071-1 WHERE Player = 'Josip Palada Category:Articles with hCards'"}
{"text": "table: 1-10294071-1\ncolumns: Player, Total W\u2013L, Singles W\u2013L, Doubles W\u2013L, Ties played, Debut, Years played\nQ: what is the total number of\u00a0ties played\u00a0where\u00a0total w\u2013l\u00a0is 38\u201324\nA: SELECT COUNT Ties played FROM 1-10294071-1 WHERE Total W\u2013L = '38\u201324'"}
{"text": "table: 1-10333757-1\ncolumns: Calls, Frequency, Branding, Format, Market/Rank, Timeslot, Group owner\nQ: What is the Frequency at the Market/Rank of Burlington - Plattsburgh , Vermont - New York /143?\nA: SELECT COUNT Frequency FROM 1-10333757-1 WHERE Market/Rank = 'Burlington - Plattsburgh , Vermont - New York /143'"}
{"text": "table: 1-10333757-1\ncolumns: Calls, Frequency, Branding, Format, Market/Rank, Timeslot, Group owner\nQ: What is the Branding for Group Owner Qantam of Cape Cod, LLC?\nA: SELECT Branding FROM 1-10333757-1 WHERE Group owner = 'Qantam of Cape Cod, LLC'"}
{"text": "table: 1-10333757-1\ncolumns: Calls, Frequency, Branding, Format, Market/Rank, Timeslot, Group owner\nQ: What Branding does WRKO calls use?\nA: SELECT Branding FROM 1-10333757-1 WHERE Calls = 'WRKO'"}
{"text": "table: 1-10333757-1\ncolumns: Calls, Frequency, Branding, Format, Market/Rank, Timeslot, Group owner\nQ: What is the Format for Branding of 1290 wkbk w281au 104.1?\nA: SELECT Format FROM 1-10333757-1 WHERE Branding = '1290 WKBK W281AU 104.1'"}
{"text": "table: 1-10333757-1\ncolumns: Calls, Frequency, Branding, Format, Market/Rank, Timeslot, Group owner\nQ: Which Market/Rank is associated with WCRN calls?\nA: SELECT Market/Rank FROM 1-10333757-1 WHERE Calls = 'WCRN'"}
{"text": "table: 1-10333757-1\ncolumns: Calls, Frequency, Branding, Format, Market/Rank, Timeslot, Group owner\nQ: Which Frequency is used for WEGP calls?\nA: SELECT Frequency FROM 1-10333757-1 WHERE Calls = 'WEGP'"}
{"text": "table: 1-10408617-5\ncolumns: Scheme, Tariff code, BTs retail price (regulated), Approx premium, Prefixes\nQ: What is the regulated retail price for the tariff code ff0 prs?\nA: SELECT BTs retail price (regulated) FROM 1-10408617-5 WHERE Tariff code = 'ff0 PRS'"}
{"text": "table: 1-10408617-5\ncolumns: Scheme, Tariff code, BTs retail price (regulated), Approx premium, Prefixes\nQ: What is the premium associated with tariff code g9?\nA: SELECT Approx premium FROM 1-10408617-5 WHERE Tariff code = 'g9'"}
{"text": "table: 1-10408617-5\ncolumns: Scheme, Tariff code, BTs retail price (regulated), Approx premium, Prefixes\nQ: How many tariff codes have a bts retail price of 2p/min or inclusive?\nA: SELECT COUNT Tariff code FROM 1-10408617-5 WHERE BTs retail price (regulated) = '2p/min or inclusive'"}
{"text": "table: 1-10408617-5\ncolumns: Scheme, Tariff code, BTs retail price (regulated), Approx premium, Prefixes\nQ: How many tariff codes have a bts retail price of 2.553p/min?\nA: SELECT COUNT Tariff code FROM 1-10408617-5 WHERE BTs retail price (regulated) = '2.553p/min'"}
{"text": "table: 1-10408617-5\ncolumns: Scheme, Tariff code, BTs retail price (regulated), Approx premium, Prefixes\nQ: What prefixes are priced at pence per minute, fixed at all times with a premium of 3p/min?\nA: SELECT Prefixes FROM 1-10408617-5 WHERE Scheme = 'Pence per minute, fixed at all times' AND Approx premium = '3p/min'"}
{"text": "table: 1-10408617-5\ncolumns: Scheme, Tariff code, BTs retail price (regulated), Approx premium, Prefixes\nQ: What is the bts retail price (regulated) for tariff code g10?\nA: SELECT BTs retail price (regulated) FROM 1-10408617-5 WHERE Tariff code = 'g10'"}
{"text": "table: 1-10409754-5\ncolumns: Nominative, Old orthography, New orthography, /e/ or /\u00e6/ (IPA), Tone (Latvian notation: /~/ - level, /^/ - broken), Translation\nQ: What is the tone for gen.sing. plague?\nA: SELECT Tone (Latvian notation: /~/ - level, /^/ - broken) FROM 1-10409754-5 WHERE Translation = 'Gen.Sing. plague'"}
{"text": "table: 1-10432351-1\ncolumns: Star (Pismis24-#), Spectral type, Magnitude (M bol ), Temperature (K), Radius (R \u2609 ), Mass (M \u2609 )\nQ: What is the smallest possible radius?\nA: SELECT MIN Radius (R \u2609 ) FROM 1-10432351-1"}
{"text": "table: 1-10432351-1\ncolumns: Star (Pismis24-#), Spectral type, Magnitude (M bol ), Temperature (K), Radius (R \u2609 ), Mass (M \u2609 )\nQ: What are all the spectral types for star mismis24-# is 1sw?\nA: SELECT Spectral type FROM 1-10432351-1 WHERE Star (Pismis24-#) = '1SW'"}
{"text": "table: 1-10432351-1\ncolumns: Star (Pismis24-#), Spectral type, Magnitude (M bol ), Temperature (K), Radius (R \u2609 ), Mass (M \u2609 )\nQ: If a radius is 10, what  is the lowest possible mass?\nA: SELECT MIN Mass (M \u2609 ) FROM 1-10432351-1 WHERE Radius (R \u2609 ) = 10"}
{"text": "table: 1-105344-2\ncolumns: Year, Aircraft kilometers, Departures, Flying hours, Passengers, Seat factor, Employees, Profit/loss\nQ: What percentage of seats were filled in 2006?\nA: SELECT Seat factor FROM 1-105344-2 WHERE Year = 2006"}
{"text": "table: 1-105344-2\ncolumns: Year, Aircraft kilometers, Departures, Flying hours, Passengers, Seat factor, Employees, Profit/loss\nQ: How many hours were flown in each of the years where more than 64379058.0 kilometers were flown?\nA: SELECT Flying hours FROM 1-105344-2 WHERE Aircraft kilometers > 64379058.0"}
{"text": "table: 1-105344-2\ncolumns: Year, Aircraft kilometers, Departures, Flying hours, Passengers, Seat factor, Employees, Profit/loss\nQ: Of the years that had exactly 17096 departures, what is the greatest number of aircraft kilometers flown?\nA: SELECT MAX Aircraft kilometers FROM 1-105344-2 WHERE Departures = 17096"}
{"text": "table: 1-10548224-1\ncolumns: Year, Game or event, Date contested, League or governing body, Sport, Winning team, Losing team, Final score\nQ: Which winning team beat the New York Yankees?\nA: SELECT Winning team FROM 1-10548224-1 WHERE Losing team = 'New York Yankees'"}
{"text": "table: 1-10548224-1\ncolumns: Year, Game or event, Date contested, League or governing body, Sport, Winning team, Losing team, Final score\nQ: What was the final score for the game that was contested on February 1, 2009?\nA: SELECT Final score FROM 1-10548224-1 WHERE Date contested = 'February 1, 2009'"}
{"text": "table: 1-10548224-1\ncolumns: Year, Game or event, Date contested, League or governing body, Sport, Winning team, Losing team, Final score\nQ: What sport had a final score of 3-2?\nA: SELECT Sport FROM 1-10548224-1 WHERE Final score = '3-2'"}
{"text": "table: 1-10548224-1\ncolumns: Year, Game or event, Date contested, League or governing body, Sport, Winning team, Losing team, Final score\nQ: Who was the winning team of the game that was contested on February 1, 2009?\nA: SELECT Winning team FROM 1-10548224-1 WHERE Date contested = 'February 1, 2009'"}
{"text": "table: 1-10548224-1\ncolumns: Year, Game or event, Date contested, League or governing body, Sport, Winning team, Losing team, Final score\nQ: Who was the losing team of the game that was contested on February 1, 2004?\nA: SELECT Losing team FROM 1-10548224-1 WHERE Date contested = 'February 1, 2004'"}
{"text": "table: 1-1057262-2\ncolumns: Crop (kilotonnes), New South Wales, Victoria, Queensland, Western Australia, South Australia, Tasmania, Total\nQ: what's the minimum\u00a0total\u00a0with\u00a0crop (kilotonnes)\u00a0being s lupin\nA: SELECT MIN Total FROM 1-1057262-2 WHERE Crop (kilotonnes) = 's Lupin'"}
{"text": "table: 1-1057262-2\ncolumns: Crop (kilotonnes), New South Wales, Victoria, Queensland, Western Australia, South Australia, Tasmania, Total\nQ: what's the\u00a0new south wales\u00a0with\u00a0crop (kilotonnes)\u00a0being canola\nA: SELECT New South Wales FROM 1-1057262-2 WHERE Crop (kilotonnes) = 'Canola'"}
{"text": "table: 1-1057262-2\ncolumns: Crop (kilotonnes), New South Wales, Victoria, Queensland, Western Australia, South Australia, Tasmania, Total\nQ: what's the total number of\u00a0south australia\u00a0with\u00a0victoria\u00a0value of 2173\nA: SELECT COUNT South Australia FROM 1-1057262-2 WHERE Victoria = 2173"}
{"text": "table: 1-1057262-2\ncolumns: Crop (kilotonnes), New South Wales, Victoria, Queensland, Western Australia, South Australia, Tasmania, Total\nQ: what's the minimum\u00a0tasmania value\nA: SELECT MIN Tasmania FROM 1-1057262-2"}
{"text": "table: 1-1057262-2\ncolumns: Crop (kilotonnes), New South Wales, Victoria, Queensland, Western Australia, South Australia, Tasmania, Total\nQ: what's the total number of\u00a0tasmania\u00a0with\u00a0new south wales\u00a0crop of 190 kilotonnes\nA: SELECT COUNT Tasmania FROM 1-1057262-2 WHERE New South Wales = 190"}
{"text": "table: 1-1058787-1\ncolumns: Approximate Age, Virtues, Psycho Social Crisis, Significant Relationship, Existential Question [ not in citation given ], Examples\nQ: How many significant relationships list Will as a virtue?\nA: SELECT COUNT Significant Relationship FROM 1-1058787-1 WHERE Virtues = 'Will'"}
{"text": "table: 1-1058787-1\ncolumns: Approximate Age, Virtues, Psycho Social Crisis, Significant Relationship, Existential Question [ not in citation given ], Examples\nQ: Which examples ask the existential question \"Can I Love?\"\nA: SELECT Examples FROM 1-1058787-1 WHERE Existential Question [ not in citation given ] = 'Can I Love?'"}
{"text": "table: 1-1059743-2\ncolumns: Rank, Member Association, Points, Group stage, Play-off, AFC Cup\nQ: How many countries got 796.7 points?\nA: SELECT COUNT Rank FROM 1-1059743-2 WHERE Points = '796.7'"}
{"text": "table: 1-1059743-2\ncolumns: Rank, Member Association, Points, Group stage, Play-off, AFC Cup\nQ: In what group stage were 177.2 points awarded?\nA: SELECT COUNT Group stage FROM 1-1059743-2 WHERE Points = '177.2'"}
{"text": "table: 1-1059743-2\ncolumns: Rank, Member Association, Points, Group stage, Play-off, AFC Cup\nQ: What is the lowest group to earn 886.6 points?\nA: SELECT MIN Group stage FROM 1-1059743-2 WHERE Points = '886.6'"}
{"text": "table: 1-1059743-2\ncolumns: Rank, Member Association, Points, Group stage, Play-off, AFC Cup\nQ: How many countries earned 177.2 points?\nA: SELECT COUNT Member Association FROM 1-1059743-2 WHERE Points = '177.2'"}
{"text": "table: 1-10586064-2\ncolumns: County, Precincts, Lunsford, % Lunsford, McConnell, % McConnell, Total\nQ: If % lunsford is 51.82% what is the % mcconnell in Letcher?\nA: SELECT % McConnell FROM 1-10586064-2 WHERE % Lunsford = '51.82%'"}
{"text": "table: 1-10586064-2\ncolumns: County, Precincts, Lunsford, % Lunsford, McConnell, % McConnell, Total\nQ: What country had the total 18,900 (r)?\nA: SELECT County FROM 1-10586064-2 WHERE Total = '18,900 (R)'"}
{"text": "table: 1-10586064-2\ncolumns: County, Precincts, Lunsford, % Lunsford, McConnell, % McConnell, Total\nQ: When % mcconnell is 44.54% what are the total number of counties?\nA: SELECT COUNT County FROM 1-10586064-2 WHERE % McConnell = '44.54%'"}
{"text": "table: 1-10586064-2\ncolumns: County, Precincts, Lunsford, % Lunsford, McConnell, % McConnell, Total\nQ: If % mcconnell is 47.17% what is the total number of mcconnell ?\nA: SELECT COUNT McConnell FROM 1-10586064-2 WHERE % McConnell = '47.17%'"}
{"text": "table: 1-10586064-2\ncolumns: County, Precincts, Lunsford, % Lunsford, McConnell, % McConnell, Total\nQ: What is the county of precints 515?\nA: SELECT County FROM 1-10586064-2 WHERE Precincts = 515"}
{"text": "table: 1-10601843-2\ncolumns: Stadium, Capacity, City, Country, Tenant, Opening\nQ: Which city has a capacity of 41903?\nA: SELECT City FROM 1-10601843-2 WHERE Capacity = 41903"}
{"text": "table: 1-10601843-2\ncolumns: Stadium, Capacity, City, Country, Tenant, Opening\nQ: What is the maximum capacity of the Otkrytie Arena stadium?\nA: SELECT MAX Capacity FROM 1-10601843-2 WHERE Stadium = 'Otkrytie Arena'"}
{"text": "table: 1-10601843-2\ncolumns: Stadium, Capacity, City, Country, Tenant, Opening\nQ: When did the stadium where Bursaspor is the tenant open?\nA: SELECT MIN Opening FROM 1-10601843-2 WHERE Tenant = 'Bursaspor'"}
{"text": "table: 1-10601843-2\ncolumns: Stadium, Capacity, City, Country, Tenant, Opening\nQ: How many tenants are there in the city of Samsun?\nA: SELECT COUNT Tenant FROM 1-10601843-2 WHERE City = 'Samsun'"}
{"text": "table: 1-10610087-5\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date\nQ: what's the\u00a0original air date\u00a0with\u00a0title\u00a0 \"hell\"\nA: SELECT Original air date FROM 1-10610087-5 WHERE Title = '\"Hell\"'"}
{"text": "table: 1-10638523-1\ncolumns: Particulars and Characteristics, Shivalik Zone, Mid-Hill Zone, High hill zone, Trance- n Himalaya Zone\nQ: What is the percentage of the Shivalik Zone where the percentage of the Mid-Hill Zone is 10%?\nA: SELECT Shivalik Zone FROM 1-10638523-1 WHERE Mid-Hill Zone = '10%'"}
{"text": "table: 1-10638523-1\ncolumns: Particulars and Characteristics, Shivalik Zone, Mid-Hill Zone, High hill zone, Trance- n Himalaya Zone\nQ: For mid-hill zone  what is the altitude?\nA: SELECT Mid-Hill Zone FROM 1-10638523-1 WHERE Particulars and Characteristics = 'Altitude'"}
{"text": "table: 1-10638523-1\ncolumns: Particulars and Characteristics, Shivalik Zone, Mid-Hill Zone, High hill zone, Trance- n Himalaya Zone\nQ: What are the climatic conditions for the trance- n himalaya zone?\nA: SELECT Trance- n Himalaya Zone FROM 1-10638523-1 WHERE Particulars and Characteristics = 'Climatic conditions'"}
{"text": "table: 1-10638523-1\ncolumns: Particulars and Characteristics, Shivalik Zone, Mid-Hill Zone, High hill zone, Trance- n Himalaya Zone\nQ: What is the percentage of the  trance- n himalaya zone that corresponds with the high hill zone is 25%?\nA: SELECT Trance- n Himalaya Zone FROM 1-10638523-1 WHERE High hill zone = '25%'"}
{"text": "table: 1-10644188-3\ncolumns: Total tenure rank, Uninterrupted rank, Name, State represented, Dates of service, Total tenure time, Uninterrupted time\nQ: What is the state of Ted Stevens?\nA: SELECT State represented FROM 1-10644188-3 WHERE Name = 'Ted Stevens'"}
{"text": "table: 1-10682862-68\ncolumns: Country, Players, Standard, Minor, First title, Last title\nQ: What's the standard of the country who won its first title in 1992?\nA: SELECT MAX Standard FROM 1-10682862-68 WHERE First title = 1992"}
{"text": "table: 1-10682862-68\ncolumns: Country, Players, Standard, Minor, First title, Last title\nQ: What's the smallest number of players?\nA: SELECT MIN Players FROM 1-10682862-68"}
{"text": "table: 1-10682862-68\ncolumns: Country, Players, Standard, Minor, First title, Last title\nQ: In what year was the last last title received, by any of the countries?\nA: SELECT MAX Last title FROM 1-10682862-68"}
{"text": "table: 1-10710364-1\ncolumns: Religious group, Population % 1961, Population % 1971, Population % 1981, Population % 1991, Population % 2001\nQ: What religious groups made up 0.72% of the Indian population in 2001?\nA: SELECT Religious group FROM 1-10710364-1 WHERE Population % 2001 = '0.72%'"}
{"text": "table: 1-10718868-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: What is the original air date for episode 15 of season 6?\nA: SELECT Original air date FROM 1-10718868-2 WHERE No. in season = 15"}
{"text": "table: 1-10718868-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: How many episodes in season 6 titles \"Poppin' Tags\"?\nA: SELECT COUNT No. in season FROM 1-10718868-2 WHERE Title = '\"Poppin' Tags\"'"}
{"text": "table: 1-10753917-1\ncolumns: Season, Driver, Team, Engine, Poles, Wins, Podiums, Points, Margin of defeat\nQ: Which podiums did the Williams team have with a margin of defeat of 2?\nA: SELECT Podiums FROM 1-10753917-1 WHERE Team = 'Williams' AND Margin of defeat = '2'"}
{"text": "table: 1-10753917-1\ncolumns: Season, Driver, Team, Engine, Poles, Wins, Podiums, Points, Margin of defeat\nQ: How many drivers on the williams team had a margin of defeat of 2?\nA: SELECT COUNT Driver FROM 1-10753917-1 WHERE Team = 'Williams' AND Margin of defeat = '2'"}
{"text": "table: 1-10753917-1\ncolumns: Season, Driver, Team, Engine, Poles, Wins, Podiums, Points, Margin of defeat\nQ: How many seasons was clay regazzoni the driver?\nA: SELECT COUNT Season FROM 1-10753917-1 WHERE Driver = 'Clay Regazzoni'"}
{"text": "table: 1-10753917-1\ncolumns: Season, Driver, Team, Engine, Poles, Wins, Podiums, Points, Margin of defeat\nQ: Which margin of defeats had points of 30?\nA: SELECT Margin of defeat FROM 1-10753917-1 WHERE Points = '30'"}
{"text": "table: 1-10753917-1\ncolumns: Season, Driver, Team, Engine, Poles, Wins, Podiums, Points, Margin of defeat\nQ: Which podiums did the alfa romeo team have?\nA: SELECT Podiums FROM 1-10753917-1 WHERE Team = 'Alfa Romeo'"}
{"text": "table: 1-10797636-1\ncolumns: Village (German), Village (Slovene), Number of people 1991, Percent of Slovenes 1991, Percent of Slovenes 1951\nQ: What was the percent of slovenes 1951 for bach?\nA: SELECT Percent of Slovenes 1951 FROM 1-10797636-1 WHERE Village (German) = 'Bach'"}
{"text": "table: 1-10812403-4\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What college's team is the Saskatchewan Roughriders?\nA: SELECT College FROM 1-10812403-4 WHERE CFL Team = 'Saskatchewan Roughriders'"}
{"text": "table: 1-10812403-4\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What position did Calvin Mccarty play?\nA: SELECT Position FROM 1-10812403-4 WHERE Player = 'Calvin McCarty'"}
{"text": "table: 1-10812403-4\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: How many people were pick #30?\nA: SELECT COUNT Position FROM 1-10812403-4 WHERE Pick # = 30"}

>>>> lora/data/train.jsonl
{"text": "table: 1-1000181-1\ncolumns: State/territory, Text/background colour, Format, Current slogan, Current series, Notes\nQ: Tell me what the notes are for South Australia \nA: SELECT Notes FROM 1-1000181-1 WHERE Current slogan = 'SOUTH AUSTRALIA'"}
{"text": "table: 1-1000181-1\ncolumns: State/territory, Text/background colour, Format, Current slogan, Current series, Notes\nQ: What is the current series where the new series began in June 2011?\nA: SELECT Current series FROM 1-1000181-1 WHERE Notes = 'New series began in June 2011'"}
{"text": "table: 1-1000181-1\ncolumns: State/territory, Text/background colour, Format, Current slogan, Current series, Notes\nQ: What is the format for South Australia?\nA: SELECT Format FROM 1-1000181-1 WHERE State/territory = 'South Australia'"}
{"text": "table: 1-1000181-1\ncolumns: State/territory, Text/background colour, Format, Current slogan, Current series, Notes\nQ: Name the background colour for the Australian Capital Territory\nA: SELECT Text/background colour FROM 1-1000181-1 WHERE State/territory = 'Australian Capital Territory'"}
{"text": "table: 1-10007452-3\ncolumns: Order Year, Manufacturer, Model, Fleet Series (Quantity), Powertrain (Engine/Transmission), Fuel Propulsion\nQ: how many times is the fuel propulsion is cng?\nA: SELECT COUNT Fleet Series (Quantity) FROM 1-10007452-3 WHERE Fuel Propulsion = 'CNG'"}
{"text": "table: 1-10007452-3\ncolumns: Order Year, Manufacturer, Model, Fleet Series (Quantity), Powertrain (Engine/Transmission), Fuel Propulsion\nQ: what is the fuel propulsion where the fleet series (quantity) is 310-329 (20)?\nA: SELECT Fuel Propulsion FROM 1-10007452-3 WHERE Fleet Series (Quantity) = '310-329 (20)'"}
{"text": "table: 1-10007452-3\ncolumns: Order Year, Manufacturer, Model, Fleet Series (Quantity), Powertrain (Engine/Transmission), Fuel Propulsion\nQ: who is the manufacturer for the order year 1998?\nA: SELECT Manufacturer FROM 1-10007452-3 WHERE Order Year = '1998'"}
{"text": "table: 1-10007452-3\ncolumns: Order Year, Manufacturer, Model, Fleet Series (Quantity), Powertrain (Engine/Transmission), Fuel Propulsion\nQ: how many times is the model ge40lfr?\nA: SELECT COUNT Manufacturer FROM 1-10007452-3 WHERE Model = 'GE40LFR'"}
{"text": "table: 1-10007452-3\ncolumns: Order Year, Manufacturer, Model, Fleet Series (Quantity), Powertrain (Engine/Transmission), Fuel Propulsion\nQ: how many times is the fleet series (quantity) is 468-473 (6)?\nA: SELECT COUNT Order Year FROM 1-10007452-3 WHERE Fleet Series (Quantity) = '468-473 (6)'"}
{"text": "table: 1-10007452-3\ncolumns: Order Year, Manufacturer, Model, Fleet Series (Quantity), Powertrain (Engine/Transmission), Fuel Propulsion\nQ: what is the powertrain (engine/transmission) when the order year is 2000?\nA: SELECT Powertrain (Engine/Transmission) FROM 1-10007452-3 WHERE Order Year = '2000'"}
{"text": "table: 1-10006830-1\ncolumns: Aircraft, Description, Max Gross Weight, Total disk area, Max disk Loading\nQ: What if the description of a ch-47d chinook?\nA: SELECT Description FROM 1-10006830-1 WHERE Aircraft = 'CH-47D Chinook'"}
{"text": "table: 1-10006830-1\ncolumns: Aircraft, Description, Max Gross Weight, Total disk area, Max disk Loading\nQ: What is the max gross weight of the Robinson R-22?\nA: SELECT Max Gross Weight FROM 1-10006830-1 WHERE Aircraft = 'Robinson R-22'"}
{"text": "table: 1-10015132-1\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: What school did player number 6 come from?\nA: SELECT School/Club Team FROM 1-10015132-1 WHERE No. = '6'"}
{"text": "table: 1-10015132-1\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: What school did the player that has been in Toronto from 2012-present come from?\nA: SELECT School/Club Team FROM 1-10015132-1 WHERE Years in Toronto = '2012-present'"}
{"text": "table: 1-10015132-1\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: What school did the player that has been in Toronto from 2010-2012 go to?\nA: SELECT School/Club Team FROM 1-10015132-1 WHERE Years in Toronto = '2010-2012'"}
{"text": "table: 1-10015132-1\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: What position did the player from Baylor play?\nA: SELECT Position FROM 1-10015132-1 WHERE School/Club Team = 'Baylor'"}
{"text": "table: 1-10015132-14\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: Who played in the Toronto Raptors from 1995-96?\nA: SELECT Player FROM 1-10015132-14 WHERE Years in Toronto = '1995-96'"}
{"text": "table: 1-10015132-14\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: Which number was Patrick O'Bryant?\nA: SELECT No. FROM 1-10015132-14 WHERE Player = 'Patrick O'Bryant'"}
{"text": "table: 1-10015132-14\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: What school did Patrick O'Bryant play for?\nA: SELECT School/Club Team FROM 1-10015132-14 WHERE Player = 'Patrick O'Bryant'"}
{"text": "table: 1-10015132-14\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: How many number does Fordham school have?\nA: SELECT COUNT No. FROM 1-10015132-14 WHERE School/Club Team = 'Fordham'"}
{"text": "table: 1-10015132-14\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: Which school was in Toronto in 2001-02?\nA: SELECT School/Club Team FROM 1-10015132-14 WHERE Years in Toronto = '2001-02'"}
{"text": "table: 1-10015132-21\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: Which school did the player that played 2004-05 attend?\nA: SELECT School/Club Team FROM 1-10015132-21 WHERE Years in Toronto = '2004-05'"}
{"text": "table: 1-10015132-21\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: Which position does Loren Woods play?\nA: SELECT Position FROM 1-10015132-21 WHERE Player = 'Loren Woods'"}
{"text": "table: 1-10015132-21\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: What number is the player that played 1998-2001\nA: SELECT MIN No. FROM 1-10015132-21 WHERE Years in Toronto = '1998-2001'"}
{"text": "table: 1-10015132-21\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: Which country is the player that went to Georgetown from?\nA: SELECT Nationality FROM 1-10015132-21 WHERE School/Club Team = 'Georgetown'"}
{"text": "table: 1-10015132-21\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: Which school did Herb Williams go to?\nA: SELECT School/Club Team FROM 1-10015132-21 WHERE Player = 'Herb Williams'"}
{"text": "table: 1-10015132-3\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: When did the player from Hawaii play for Toronto?\nA: SELECT Years in Toronto FROM 1-10015132-3 WHERE School/Club Team = 'Hawaii'"}
{"text": "table: 1-10015132-3\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: During what period did Dell Curry play for Toronto?\nA: SELECT Years in Toronto FROM 1-10015132-3 WHERE Player = 'Dell Curry'"}
{"text": "table: 1-10015132-3\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: What's the number of the player from Boise State?\nA: SELECT No. FROM 1-10015132-3 WHERE School/Club Team = 'Boise State'"}
{"text": "table: 1-10015132-3\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: What's Dell Curry nationality?\nA: SELECT Nationality FROM 1-10015132-3 WHERE Player = 'Dell Curry'"}
{"text": "table: 1-10015132-7\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: which player is from georgia\nA: SELECT Player FROM 1-10015132-7 WHERE School/Club Team = 'Georgia'"}
{"text": "table: 1-10015132-7\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: what school is rudy gay from\nA: SELECT School/Club Team FROM 1-10015132-7 WHERE Player = 'Rudy Gay'"}
{"text": "table: 1-10015132-7\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: what nationality is the player who played from 1997-98\nA: SELECT Nationality FROM 1-10015132-7 WHERE Years in Toronto = '1997-98'"}
{"text": "table: 1-10015132-7\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: what position did the player from connecticut play\nA: SELECT Position FROM 1-10015132-7 WHERE School/Club Team = 'Connecticut'"}
{"text": "table: 1-10015132-2\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: During which years was Marcus Banks in Toronto?\nA: SELECT Years in Toronto FROM 1-10015132-2 WHERE Player = 'Marcus Banks'"}
{"text": "table: 1-10015132-2\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: Which positions were in Toronto in 2004?\nA: SELECT Position FROM 1-10015132-2 WHERE Years in Toronto = '2004'"}
{"text": "table: 1-10015132-2\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: What nationality is the player Muggsy Bogues?\nA: SELECT Nationality FROM 1-10015132-2 WHERE Player = 'Muggsy Bogues'"}
{"text": "table: 1-10015132-2\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: What years was the player Lonny Baxter in Toronto?\nA: SELECT Years in Toronto FROM 1-10015132-2 WHERE Player = 'Lonny Baxter'"}
{"text": "table: 1-10015132-2\ncolumns: Player, No., Nationality, Position, Years in Toronto, School/Club Team\nQ: How many players were with the school or club team La Salle?\nA: SELECT COUNT Player FROM 1-10015132-2 WHERE School/Club Team = 'La Salle'"}
{"text": "table: 1-10021158-3\ncolumns: Year, Tournaments played, Cuts made*, Wins, 2nd, Top 10s, Best finish, Earnings ($), Money list rank, Scoring average, Scoring rank\nQ: When the scoring rank was 117, what was the best finish?\nA: SELECT Best finish FROM 1-10021158-3 WHERE Scoring rank = '117'"}
{"text": "table: 1-10021158-3\ncolumns: Year, Tournaments played, Cuts made*, Wins, 2nd, Top 10s, Best finish, Earnings ($), Money list rank, Scoring average, Scoring rank\nQ: When the best finish was T69, how many people came in 2nd?\nA: SELECT 2nd FROM 1-10021158-3 WHERE Best finish = 'T69'"}
{"text": "table: 1-10021158-3\ncolumns: Year, Tournaments played, Cuts made*, Wins, 2nd, Top 10s, Best finish, Earnings ($), Money list rank, Scoring average, Scoring rank\nQ: How many wins were there when the money list rank was 183?\nA: SELECT COUNT Wins FROM 1-10021158-3 WHERE Money list rank = '183'"}
{"text": "table: 1-10021158-3\ncolumns: Year, Tournaments played, Cuts made*, Wins, 2nd, Top 10s, Best finish, Earnings ($), Money list rank, Scoring average, Scoring rank\nQ: When the money list rank was n/a, what was the scoring average?\nA: SELECT Scoring average FROM 1-10021158-3 WHERE Money list rank = 'n/a'"}
{"text": "table: 1-10021158-3\ncolumns: Year, Tournaments played, Cuts made*, Wins, 2nd, Top 10s, Best finish, Earnings ($), Money list rank, Scoring average, Scoring rank\nQ: What time was the highest for 2nd finishers?\nA: SELECT MAX 2nd FROM 1-10021158-3"}
{"text": "table: 1-1004033-1\ncolumns: Season, Player, Position, Nationality, Team, Draft Pick #, Draft Class, College\nQ: When did the Metrostars have their first Rookie of the Year winner?\nA: SELECT MIN Season FROM 1-1004033-1 WHERE Team = 'MetroStars'"}
{"text": "table: 1-1004033-1\ncolumns: Season, Player, Position, Nationality, Team, Draft Pick #, Draft Class, College\nQ: What college did the Rookie of the Year from the Columbus Crew attend?\nA: SELECT College FROM 1-1004033-1 WHERE Team = 'Columbus Crew'"}
{"text": "table: 1-1004033-1\ncolumns: Season, Player, Position, Nationality, Team, Draft Pick #, Draft Class, College\nQ: How many teams had a #1 draft pick that won the Rookie of the Year Award?\nA: SELECT COUNT Team FROM 1-1004033-1 WHERE Draft Pick # = '1'"}
{"text": "table: 1-1004033-1\ncolumns: Season, Player, Position, Nationality, Team, Draft Pick #, Draft Class, College\nQ: What position did the #10 draft pick play?\nA: SELECT Position FROM 1-1004033-1 WHERE Draft Pick # = '10'"}
{"text": "table: 1-10023387-1\ncolumns: Player, Years Played, Total W-L, Singles W-L, Doubles W-L\nQ: what's the\u00a0years played\u00a0with\u00a0singles w-l\u00a0of 3\u20132\nA: SELECT Years Played FROM 1-10023387-1 WHERE Singles W-L = '3\u20132'"}
{"text": "table: 1-10023387-1\ncolumns: Player, Years Played, Total W-L, Singles W-L, Doubles W-L\nQ: what's the\u00a0doubles w-l\u00a0for player\u00a0seol jae-min (none)\nA: SELECT Doubles W-L FROM 1-10023387-1 WHERE Player = 'Seol Jae-Min (none)'"}
{"text": "table: 1-10023387-1\ncolumns: Player, Years Played, Total W-L, Singles W-L, Doubles W-L\nQ: what's the\u00a0singles w-l\u00a0for kim doo-hwan\nA: SELECT Singles W-L FROM 1-10023387-1 WHERE Player = 'Kim Doo-Hwan'"}
{"text": "table: 1-10023387-1\ncolumns: Player, Years Played, Total W-L, Singles W-L, Doubles W-L\nQ: what's the total number of\u00a0singles w-l\u00a0with\u00a0doubles w-l\u00a0of 0\u20130 and\u00a0total w-l\u00a0of 3\u20131\nA: SELECT COUNT Singles W-L FROM 1-10023387-1 WHERE Doubles W-L = '0\u20130' AND Total W-L = '3\u20131'"}
{"text": "table: 1-10023387-1\ncolumns: Player, Years Played, Total W-L, Singles W-L, Doubles W-L\nQ: what's the\u00a0doubles w-l\u00a0with\u00a0years played\u00a0value of 1 (1968)\nA: SELECT Doubles W-L FROM 1-10023387-1 WHERE Years Played = '1 (1968)'"}
{"text": "table: 1-10023387-1\ncolumns: Player, Years Played, Total W-L, Singles W-L, Doubles W-L\nQ: what\u00a0years are played\u00a0for player\u00a0 im chung-yang\nA: SELECT Years Played FROM 1-10023387-1 WHERE Player = 'Im Chung-Yang'"}
{"text": "table: 1-10020178-1\ncolumns: Name, Canton, Height (meters), Crest length (meters), Type, Year of construction, Name of the Lake\nQ: What is the name of the 375 crest length?\nA: SELECT Name FROM 1-10020178-1 WHERE Crest length (meters) = 375"}
{"text": "table: 1-10020178-1\ncolumns: Name, Canton, Height (meters), Crest length (meters), Type, Year of construction, Name of the Lake\nQ: What is year of construction of spitallamm?\nA: SELECT MIN Year of construction FROM 1-10020178-1 WHERE Name = 'Spitallamm'"}
{"text": "table: 1-10020178-1\ncolumns: Name, Canton, Height (meters), Crest length (meters), Type, Year of construction, Name of the Lake\nQ: What is the canton of grande dixence?\nA: SELECT Canton FROM 1-10020178-1 WHERE Name = 'Grande Dixence'"}
{"text": "table: 1-10020178-1\ncolumns: Name, Canton, Height (meters), Crest length (meters), Type, Year of construction, Name of the Lake\nQ: What is the name where lago di luzzone is?\nA: SELECT Name FROM 1-10020178-1 WHERE Name of the Lake = 'Lago di Luzzone'"}
{"text": "table: 1-100518-1\ncolumns: Name, Direction, Mantra, Weapon, Consort, Graha (Planet), Guardian M\u0101t\u1e5bk\u0101\nQ: What is the  guardian m\u0101t\u1e5bk\u0101 for the guardian whose consort is Sv\u0101h\u0101?\nA: SELECT Guardian M\u0101t\u1e5bk\u0101 FROM 1-100518-1 WHERE Consort = 'Sv\u0101h\u0101'"}
{"text": "table: 1-100518-1\ncolumns: Name, Direction, Mantra, Weapon, Consort, Graha (Planet), Guardian M\u0101t\u1e5bk\u0101\nQ: Where the mantra is \"o\u1e43 ya\u1e43 v\u0101yuve nama\u1e25\", what is the direction of the guardian?\nA: SELECT Direction FROM 1-100518-1 WHERE Mantra = 'O\u1e43 Ya\u1e43 V\u0101yuve Nama\u1e25'"}
{"text": "table: 1-100518-1\ncolumns: Name, Direction, Mantra, Weapon, Consort, Graha (Planet), Guardian M\u0101t\u1e5bk\u0101\nQ: What weapon is used by the guardian whose consort is \u015bac\u012b?\nA: SELECT Weapon FROM 1-100518-1 WHERE Consort = '\u015aac\u012b'"}
{"text": "table: 1-100518-1\ncolumns: Name, Direction, Mantra, Weapon, Consort, Graha (Planet), Guardian M\u0101t\u1e5bk\u0101\nQ: What are the directions for the guardian whose weapon is kha\u1e0dga (sword)?\nA: SELECT Direction FROM 1-100518-1 WHERE Weapon = 'Kha\u1e0dga (sword)'"}
{"text": "table: 1-100518-1\ncolumns: Name, Direction, Mantra, Weapon, Consort, Graha (Planet), Guardian M\u0101t\u1e5bk\u0101\nQ: What are the weapons used by guardians for the direction East?\nA: SELECT Weapon FROM 1-100518-1 WHERE Direction = 'East'"}
{"text": "table: 1-100518-1\ncolumns: Name, Direction, Mantra, Weapon, Consort, Graha (Planet), Guardian M\u0101t\u1e5bk\u0101\nQ: What are the directions for the guardian whose graha (planet) is b\u1e5bhaspati (Jupiter)?\nA: SELECT Direction FROM 1-100518-1 WHERE Graha (Planet) = 'B\u1e5bhaspati (Jupiter)'"}
{"text": "table: 1-10054296-1\ncolumns: Member, Headquarters, Classification, Chapters, Founded, UCCFS\nQ: What is the number of chapters listed for the fraternity with a headquarters in Austin, Texas?\nA: SELECT MAX Chapters FROM 1-10054296-1 WHERE Classification = 'Fraternity' AND Headquarters = 'Austin, Texas'"}
{"text": "table: 1-10054296-1\ncolumns: Member, Headquarters, Classification, Chapters, Founded, UCCFS\nQ: What are the members listed with the sorority classification\nA: SELECT Member FROM 1-10054296-1 WHERE Classification = 'Sorority'"}
{"text": "table: 1-10054296-1\ncolumns: Member, Headquarters, Classification, Chapters, Founded, UCCFS\nQ: Name the member that has 12 chapters\nA: SELECT Member FROM 1-10054296-1 WHERE Chapters = 12"}
{"text": "table: 1-10054296-1\ncolumns: Member, Headquarters, Classification, Chapters, Founded, UCCFS\nQ: Where is the headquarters of Alpha Nu Omega\nA: SELECT Headquarters FROM 1-10054296-1 WHERE Member = 'Alpha Nu Omega'"}
{"text": "table: 1-1007688-1\ncolumns: Year, Typhus, Typhoid fever, Relapsing fever, Smallpox, Malaria\nQ: what is the number of relapsing fever when malaria is 3000\nA: SELECT MIN Relapsing fever FROM 1-1007688-1 WHERE Malaria = '3000'"}
{"text": "table: 1-1007688-1\ncolumns: Year, Typhus, Typhoid fever, Relapsing fever, Smallpox, Malaria\nQ: what is the typhoid fever number for the year 1934\nA: SELECT Typhoid fever FROM 1-1007688-1 WHERE Year = '1934'"}
{"text": "table: 1-1007688-1\ncolumns: Year, Typhus, Typhoid fever, Relapsing fever, Smallpox, Malaria\nQ: What are all the typhus number when smallpox is 4\nA: SELECT Typhus FROM 1-1007688-1 WHERE Smallpox = 4"}
{"text": "table: 1-1007688-1\ncolumns: Year, Typhus, Typhoid fever, Relapsing fever, Smallpox, Malaria\nQ: what is the number of smallpox when typhoid fever is 293\nA: SELECT MAX Smallpox FROM 1-1007688-1 WHERE Typhoid fever = 293"}
{"text": "table: 1-1007688-1\ncolumns: Year, Typhus, Typhoid fever, Relapsing fever, Smallpox, Malaria\nQ: what is the typhoid fever number for the year 1929\nA: SELECT Typhoid fever FROM 1-1007688-1 WHERE Year = '1929'"}
{"text": "table: 1-10082596-1\ncolumns: School, Location, Founded, Affiliation, Enrollment, Team Nickname, Primary conference\nQ: How many schools are in Bloomington, IN?\nA: SELECT COUNT Founded FROM 1-10082596-1 WHERE Location = 'Bloomington, IN'"}
{"text": "table: 1-10082596-1\ncolumns: School, Location, Founded, Affiliation, Enrollment, Team Nickname, Primary conference\nQ: How many of the schools are designated private/Presbyterian?\nA: SELECT COUNT Location FROM 1-10082596-1 WHERE Affiliation = 'Private/Presbyterian'"}
{"text": "table: 1-10082596-1\ncolumns: School, Location, Founded, Affiliation, Enrollment, Team Nickname, Primary conference\nQ: In what year was Lindenwood University founded?\nA: SELECT MIN Founded FROM 1-10082596-1 WHERE School = 'Lindenwood University'"}
{"text": "table: 1-10082596-1\ncolumns: School, Location, Founded, Affiliation, Enrollment, Team Nickname, Primary conference\nQ: How many of the schools listed are in Ames, IA?\nA: SELECT COUNT Primary conference FROM 1-10082596-1 WHERE Location = 'Ames, IA'"}
{"text": "table: 1-1008653-9\ncolumns: Country ( exonym ), Capital ( exonym ), Country ( endonym ), Capital ( endonym ), Official or native language(s) (alphabet/script)\nQ: What is the capital (endonym) where Douglas is the Capital (exonym)?\nA: SELECT Capital ( endonym ) FROM 1-1008653-9 WHERE Capital ( exonym ) = 'Douglas'"}
{"text": "table: 1-1008653-9\ncolumns: Country ( exonym ), Capital ( exonym ), Country ( endonym ), Capital ( endonym ), Official or native language(s) (alphabet/script)\nQ: How many countries (endonym) has the capital (endonym) of Jakarta?\nA: SELECT COUNT Country ( endonym ) FROM 1-1008653-9 WHERE Capital ( endonym ) = 'Jakarta'"}
{"text": "table: 1-1008653-9\ncolumns: Country ( exonym ), Capital ( exonym ), Country ( endonym ), Capital ( endonym ), Official or native language(s) (alphabet/script)\nQ: What is the country (exonym) where the official or native language(s) (alphabet/script) is Icelandic?\nA: SELECT Country ( exonym ) FROM 1-1008653-9 WHERE Official or native language(s) (alphabet/script) = 'Icelandic'"}
{"text": "table: 1-1008653-9\ncolumns: Country ( exonym ), Capital ( exonym ), Country ( endonym ), Capital ( endonym ), Official or native language(s) (alphabet/script)\nQ: In which country (endonym) is Irish English the official or native language(s) (alphabet/script)?\nA: SELECT Country ( endonym ) FROM 1-1008653-9 WHERE Official or native language(s) (alphabet/script) = 'Irish English'"}
{"text": "table: 1-1008653-9\ncolumns: Country ( exonym ), Capital ( exonym ), Country ( endonym ), Capital ( endonym ), Official or native language(s) (alphabet/script)\nQ: Which country (exonym) is the country (endonym) isle of man ellan vannin?\nA: SELECT Country ( exonym ) FROM 1-1008653-9 WHERE Country ( endonym ) = 'Isle of Man Ellan Vannin'"}
{"text": "table: 1-1009087-1\ncolumns: Season, Network, Season premiere, Season finale, TV season, Ranking, Viewers (in millions)\nQ: The season premiere aired on September 11, 2000 aired on how many networks? \nA: SELECT COUNT Network FROM 1-1009087-1 WHERE Season premiere = 'September 11, 2000'"}
{"text": "table: 1-1009087-1\ncolumns: Season, Network, Season premiere, Season finale, TV season, Ranking, Viewers (in millions)\nQ: What was the ranking of the season finale aired on May 8, 2006? \nA: SELECT Ranking FROM 1-1009087-1 WHERE Season finale = 'May 8, 2006'"}
{"text": "table: 1-1011906-1\ncolumns: Regional County Municipality (RCM), Population Canada 2011 Census, Land Area, Density (pop. per km2), Seat of RCM\nQ: what is the minimum\u00a0population canada 2011 census\u00a0with\u00a0seat of rcm\u00a0being cowansville\nA: SELECT MIN Population Canada 2011 Census FROM 1-1011906-1 WHERE Seat of RCM = 'Cowansville'"}
{"text": "table: 1-1011906-1\ncolumns: Regional County Municipality (RCM), Population Canada 2011 Census, Land Area, Density (pop. per km2), Seat of RCM\nQ: what's the\u00a0land area\u00a0with\u00a0seat of rcm\u00a0being granby\nA: SELECT Land Area FROM 1-1011906-1 WHERE Seat of RCM = 'Granby'"}
{"text": "table: 1-101196-1\ncolumns: County, English name, Irish name, Population, Irish speakers\nQ: What is the population for County Mayo with the English Name Carrowteige?\nA: SELECT Population FROM 1-101196-1 WHERE County = 'County Mayo' AND English name = 'Carrowteige'"}
{"text": "table: 1-101196-1\ncolumns: County, English name, Irish name, Population, Irish speakers\nQ: What is the Irish name listed with 62% Irish speakers?\nA: SELECT Irish name FROM 1-101196-1 WHERE Irish speakers = '62%'"}
{"text": "table: 1-101196-1\ncolumns: County, English name, Irish name, Population, Irish speakers\nQ: What is the population for the Irish Name Leitir meall\u00e1in?\nA: SELECT Population FROM 1-101196-1 WHERE Irish name = 'Leitir Meall\u00e1in'"}
{"text": "table: 1-101196-1\ncolumns: County, English name, Irish name, Population, Irish speakers\nQ: What is the county for the Irish name Carna?\nA: SELECT County FROM 1-101196-1 WHERE Irish name = 'Carna'"}
{"text": "table: 1-101196-1\ncolumns: County, English name, Irish name, Population, Irish speakers\nQ: How many County Kerry have 53% Irish speakers?\nA: SELECT COUNT English name FROM 1-101196-1 WHERE Irish speakers = '53%' AND County = 'County Kerry'"}
{"text": "table: 1-101196-1\ncolumns: County, English name, Irish name, Population, Irish speakers\nQ: What is the population for the English name Spiddal?\nA: SELECT Population FROM 1-101196-1 WHERE English name = 'Spiddal'"}
{"text": "table: 1-10118412-6\ncolumns: State/Territory, Asian American Population (2010 Census), Chinese, Filipino, Indian, Japanese, Korean, Vietnamese, Other Asian\nQ: What is the the Chinese population for the state that has a Filipino population of 1474707?\nA: SELECT MIN Chinese FROM 1-10118412-6 WHERE Filipino = 1474707"}
{"text": "table: 1-10118412-6\ncolumns: State/Territory, Asian American Population (2010 Census), Chinese, Filipino, Indian, Japanese, Korean, Vietnamese, Other Asian\nQ: How many States have an Indian population of 30947?\nA: SELECT COUNT Filipino FROM 1-10118412-6 WHERE Indian = 30947"}
{"text": "table: 1-10118412-6\ncolumns: State/Territory, Asian American Population (2010 Census), Chinese, Filipino, Indian, Japanese, Korean, Vietnamese, Other Asian\nQ: What is the highest Indian population?\nA: SELECT MAX Indian FROM 1-10118412-6"}
{"text": "table: 1-10121127-1\ncolumns: UN Operation name, UN Operation title, Location, Dates of Australian involvement, Number of Australians involved, Australian role\nQ: What is Australia's role in the UN operation Unama?\nA: SELECT Australian role FROM 1-10121127-1 WHERE UN Operation name = 'UNAMA'"}
{"text": "table: 1-10121127-1\ncolumns: UN Operation name, UN Operation title, Location, Dates of Australian involvement, Number of Australians involved, Australian role\nQ: What is the UN operation title with the UN operation name, Uncok?\nA: SELECT UN Operation title FROM 1-10121127-1 WHERE UN Operation name = 'UNCOK'"}
{"text": "table: 1-10121127-1\ncolumns: UN Operation name, UN Operation title, Location, Dates of Australian involvement, Number of Australians involved, Australian role\nQ: How many Australians were in the UN commission on Korea?\nA: SELECT COUNT Number of Australians involved FROM 1-10121127-1 WHERE UN Operation title = 'UN Commission on Korea'"}
{"text": "table: 1-10121127-1\ncolumns: UN Operation name, UN Operation title, Location, Dates of Australian involvement, Number of Australians involved, Australian role\nQ: When was it where 65 Australians were involved in the UN?\nA: SELECT Dates of Australian involvement FROM 1-10121127-1 WHERE Number of Australians involved = '65'"}
{"text": "table: 1-10120207-8\ncolumns: Season, Timeslot ( ET ), Season premiere, Season finale, TV season, Rank, Viewers (millions)\nQ: What year is the season with the 10.73 million views?\nA: SELECT TV season FROM 1-10120207-8 WHERE Viewers (millions) = '10.73'"}
{"text": "table: 1-10120207-8\ncolumns: Season, Timeslot ( ET ), Season premiere, Season finale, TV season, Rank, Viewers (millions)\nQ: What is the season year where the rank is 39?\nA: SELECT TV season FROM 1-10120207-8 WHERE Rank = '39'"}
{"text": "table: 1-10120207-8\ncolumns: Season, Timeslot ( ET ), Season premiere, Season finale, TV season, Rank, Viewers (millions)\nQ: What is the number of season premieres were 10.17 people watched?\nA: SELECT COUNT Season premiere FROM 1-10120207-8 WHERE Viewers (millions) = '10.17'"}
{"text": "table: 1-10120207-8\ncolumns: Season, Timeslot ( ET ), Season premiere, Season finale, TV season, Rank, Viewers (millions)\nQ: What is the year of the season that was 12?\nA: SELECT TV season FROM 1-10120207-8 WHERE Season = 12"}
{"text": "table: 1-1012730-1\ncolumns: Year, Starts, Wins, Top 5, Top 10, Poles, Avg. Start, Avg. Finish, Winnings, Position, Team(s)\nQ: In 2012 what was the average finish?\nA: SELECT Avg. Finish FROM 1-1012730-1 WHERE Year = 2012"}
{"text": "table: 1-1012730-1\ncolumns: Year, Starts, Wins, Top 5, Top 10, Poles, Avg. Start, Avg. Finish, Winnings, Position, Team(s)\nQ: How many wins happened in 1983?\nA: SELECT MIN Wins FROM 1-1012730-1 WHERE Year = 1983"}
{"text": "table: 1-1012730-1\ncolumns: Year, Starts, Wins, Top 5, Top 10, Poles, Avg. Start, Avg. Finish, Winnings, Position, Team(s)\nQ: How many top tens had an average start of 29.4?\nA: SELECT COUNT Top 10 FROM 1-1012730-1 WHERE Avg. Start = '29.4'"}
{"text": "table: 1-1012730-1\ncolumns: Year, Starts, Wins, Top 5, Top 10, Poles, Avg. Start, Avg. Finish, Winnings, Position, Team(s)\nQ: How many poles had an average finish of 19.1?\nA: SELECT MAX Poles FROM 1-1012730-1 WHERE Avg. Finish = '19.1'"}
{"text": "table: 1-1012730-1\ncolumns: Year, Starts, Wins, Top 5, Top 10, Poles, Avg. Start, Avg. Finish, Winnings, Position, Team(s)\nQ: How many starts did Hendrick motorsports have?\nA: SELECT MIN Starts FROM 1-1012730-1 WHERE Team(s) = 'Hendrick Motorsports'"}
{"text": "table: 1-1013129-10\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: NHL players are all centre in Florida panthers.\nA: SELECT Player FROM 1-1013129-10 WHERE Position = 'Centre' AND NHL team = 'Florida Panthers'"}
{"text": "table: 1-1013129-10\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: NHL team player San Jose Sharks is United States nationally.\nA: SELECT Player FROM 1-1013129-10 WHERE NHL team = 'San Jose Sharks' AND Nationality = 'United States'"}
{"text": "table: 1-1013129-10\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: All players are position mark polak.\nA: SELECT Position FROM 1-1013129-10 WHERE Player = 'Mark Polak'"}
{"text": "table: 1-1013129-10\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: Position in nhl team centre are all smaller pick than 243.0\nA: SELECT NHL team FROM 1-1013129-10 WHERE Position = 'Centre' AND Pick < 243.0"}
{"text": "table: 1-1013129-11\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: What college/junior/club teams do the players from the St. Louis Blues come from?\nA: SELECT College/junior/club team FROM 1-1013129-11 WHERE NHL team = 'St. Louis Blues'"}
{"text": "table: 1-1013129-11\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: What teams do the players from TPS (Finland) play for?\nA: SELECT NHL team FROM 1-1013129-11 WHERE College/junior/club team = 'TPS (Finland)'"}
{"text": "table: 1-1013129-11\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: What high school team did Doug Nolan play for?\nA: SELECT College/junior/club team FROM 1-1013129-11 WHERE Player = 'Doug Nolan'"}
{"text": "table: 1-1013129-11\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: What club team is Per Gustafsson play for?\nA: SELECT College/junior/club team FROM 1-1013129-11 WHERE Player = 'Per Gustafsson'"}
{"text": "table: 1-1013129-11\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: What is the nationality of Shayne Wright?\nA: SELECT Nationality FROM 1-1013129-11 WHERE Player = 'Shayne Wright'"}
{"text": "table: 1-10128185-2\ncolumns: Song, Mobiles, Northern Ireland, Northern England, Scotland, Southern England, Wales, Total\nQ: How many votes did Southern England cast whilst Northern Ireland cast 3?\nA: SELECT Southern England FROM 1-10128185-2 WHERE Northern Ireland = 3"}
{"text": "table: 1-10128185-2\ncolumns: Song, Mobiles, Northern Ireland, Northern England, Scotland, Southern England, Wales, Total\nQ: What was the lowest number of votes Scotland cast?\nA: SELECT MIN Scotland FROM 1-10128185-2"}
{"text": "table: 1-10128185-2\ncolumns: Song, Mobiles, Northern Ireland, Northern England, Scotland, Southern England, Wales, Total\nQ: What is the total number of votes if Scotland cast 35?\nA: SELECT COUNT Scotland FROM 1-10128185-2 WHERE Total = 35"}
{"text": "table: 1-10128185-2\ncolumns: Song, Mobiles, Northern Ireland, Northern England, Scotland, Southern England, Wales, Total\nQ: How many votes did Northern Ireland cast if the total was 35?\nA: SELECT Northern Ireland FROM 1-10128185-2 WHERE Total = 35"}
{"text": "table: 1-10128185-2\ncolumns: Song, Mobiles, Northern Ireland, Northern England, Scotland, Southern England, Wales, Total\nQ: How many votes did Wales cast when Northern England cast 6?\nA: SELECT MIN Wales FROM 1-10128185-2 WHERE Northern England = 6"}
{"text": "table: 1-1012730-2\ncolumns: Year, Starts, Wins, Top 5, Top 10, Poles, Avg. Start, Avg. Finish, Winnings, Position, Team(s)\nQ: What teams had 9 in the top 5 and 1 wins?\nA: SELECT Team(s) FROM 1-1012730-2 WHERE Top 5 = 9 AND Wins = 1"}
{"text": "table: 1-1013129-1\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: What teams did the player vadim sharifijanov play for?\nA: SELECT College/junior/club team FROM 1-1013129-1 WHERE Player = 'Vadim Sharifijanov'"}
{"text": "table: 1-1013129-1\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: What positions do the hartford whalers nhl team have?\nA: SELECT Position FROM 1-1013129-1 WHERE NHL team = 'Hartford Whalers'"}
{"text": "table: 1-1013129-1\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: What is the smallest pick for the player, brett lindros?\nA: SELECT MIN Pick FROM 1-1013129-1 WHERE Player = 'Brett Lindros'"}
{"text": "table: 1-1013129-1\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: What positions does the college/junior/club team, molot perm (russia) have?\nA: SELECT Position FROM 1-1013129-1 WHERE College/junior/club team = 'Molot Perm (Russia)'"}
{"text": "table: 1-1013129-1\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: The nhl team new york islanders is what nationality?\nA: SELECT Nationality FROM 1-1013129-1 WHERE NHL team = 'New York Islanders'"}
{"text": "table: 1-1013168-3\ncolumns: District, Vacator, Reason for change, Successor, Date successor seated\nQ: What is the name of the vacator for district Louisiana 1st?\nA: SELECT Vacator FROM 1-1013168-3 WHERE District = 'Louisiana 1st'"}
{"text": "table: 1-101336-1\ncolumns: Formula, Notation, T c (K), No. of Cu-O planes in unit cell, Crystal structure\nQ: What is the notion when the crystal structure is tetragonal and the formula is bi 2 sr 2 cacu 2 o 8\nA: SELECT Notation FROM 1-101336-1 WHERE Crystal structure = 'Tetragonal' AND Formula = 'Bi 2 Sr 2 CaCu 2 O 8'"}
{"text": "table: 1-101336-1\ncolumns: Formula, Notation, T c (K), No. of Cu-O planes in unit cell, Crystal structure\nQ: How many times is the formula tl 2 ba 2 cuo 6?\nA: SELECT No. of Cu-O planes in unit cell FROM 1-101336-1 WHERE Formula = 'Tl 2 Ba 2 CuO 6'"}
{"text": "table: 1-101336-1\ncolumns: Formula, Notation, T c (K), No. of Cu-O planes in unit cell, Crystal structure\nQ: What is the crystal structure for the formula yba 2 cu 3 o 7?\nA: SELECT Crystal structure FROM 1-101336-1 WHERE Formula = 'YBa 2 Cu 3 O 7'"}
{"text": "table: 1-101336-1\ncolumns: Formula, Notation, T c (K), No. of Cu-O planes in unit cell, Crystal structure\nQ: What is the number for t c (k) when the notation is tl-2212?\nA: SELECT COUNT T c (K) FROM 1-101336-1 WHERE Notation = 'Tl-2212'"}
{"text": "table: 1-10138926-1\ncolumns: #, City, 1981 Census, 1991 Census, 2001 Census, 2010 Est., Region\nQ: How many 2010 estimations have been done in the city of Cremona?\nA: SELECT COUNT 2010 Est. FROM 1-10138926-1 WHERE City = 'Cremona'"}
{"text": "table: 1-10138926-1\ncolumns: #, City, 1981 Census, 1991 Census, 2001 Census, 2010 Est., Region\nQ: What's the 2001 census of the region of Abruzzo where the 1871 census is bigger than 51092.0?\nA: SELECT MIN 2001 Census FROM 1-10138926-1 WHERE Region = 'Abruzzo' AND 1981 Census > 51092.0"}
{"text": "table: 1-10138926-1\ncolumns: #, City, 1981 Census, 1991 Census, 2001 Census, 2010 Est., Region\nQ: What's the 1991 census of the city of Carpi?\nA: SELECT MAX 1991 Census FROM 1-10138926-1 WHERE City = 'Carpi'"}
{"text": "table: 1-10138926-1\ncolumns: #, City, 1981 Census, 1991 Census, 2001 Census, 2010 Est., Region\nQ: How many 2001 censuses are there on number 13?\nA: SELECT COUNT 2001 Census FROM 1-10138926-1 WHERE # = 13"}
{"text": "table: 1-10138926-1\ncolumns: #, City, 1981 Census, 1991 Census, 2001 Census, 2010 Est., Region\nQ: What's the 1981 census of Livorno?\nA: SELECT 1981 Census FROM 1-10138926-1 WHERE City = 'Livorno'"}
{"text": "table: 1-1013129-8\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: Which NHL team has player Mike Loach?\nA: SELECT NHL team FROM 1-1013129-8 WHERE Player = 'Mike Loach'"}
{"text": "table: 1-1013129-8\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: What is the NHL team that has Peter Strom?\nA: SELECT NHL team FROM 1-1013129-8 WHERE Player = 'Peter Strom'"}
{"text": "table: 1-1013129-8\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: What team is Keith Mccambridge on?\nA: SELECT College/junior/club team FROM 1-1013129-8 WHERE Player = 'Keith McCambridge'"}
{"text": "table: 1-1013129-8\ncolumns: Pick, Player, Position, Nationality, NHL team, College/junior/club team\nQ: How many nationalities are the pick 193?\nA: SELECT COUNT Nationality FROM 1-1013129-8 WHERE Pick = 193"}
{"text": "table: 1-1013168-2\ncolumns: State (class), Vacator, Reason for change, Successor, Date of successors formal installation\nQ: Who was the succesor that was formally installed on November 8, 1978?\nA: SELECT Successor FROM 1-1013168-2 WHERE Date of successors formal installation = 'November 8, 1978'"}
{"text": "table: 1-1014319-1\ncolumns: Week, Dance/song, Horwood, Goodman, Dixon, Tonioli, Total, Result\nQ: How many songs received a 10 from Goodman and were rated by Tonioli?\nA: SELECT COUNT Tonioli FROM 1-1014319-1 WHERE Goodman = '10'"}
{"text": "table: 1-1014319-1\ncolumns: Week, Dance/song, Horwood, Goodman, Dixon, Tonioli, Total, Result\nQ: What score did Goodman give to all songs with safe results, which received a 7 from Horwood and have a total score of 31?\nA: SELECT Goodman FROM 1-1014319-1 WHERE Total = '31' AND Horwood = '7' AND Result = 'Safe'"}
{"text": "table: 1-1014319-1\ncolumns: Week, Dance/song, Horwood, Goodman, Dixon, Tonioli, Total, Result\nQ: What score did Dixon give to the song \"samba / young hearts run free\", which was in second place?\nA: SELECT Dixon FROM 1-1014319-1 WHERE Dance/song = 'Samba / Young Hearts Run Free' AND Result = 'Second place'"}
{"text": "table: 1-1014319-1\ncolumns: Week, Dance/song, Horwood, Goodman, Dixon, Tonioli, Total, Result\nQ: How many scores did Goodman give to \"samba / young hearts run free\", which was in second place?\nA: SELECT COUNT Goodman FROM 1-1014319-1 WHERE Result = 'Second place' AND Dance/song = 'Samba / Young Hearts Run Free'"}
{"text": "table: 1-1015421-1\ncolumns: Class, Operator, No. Built, Year Built, Cars per Set, Unit nos.\nQ: What year was number 7 built?\nA: SELECT Year Built FROM 1-1015421-1 WHERE No. Built = 7"}
{"text": "table: 1-1015914-24\ncolumns: Case/Suffix, we two, you and I, you two, them two (the two), who-two\nQ: What is we two when the case/suffix is loc.?\nA: SELECT we two FROM 1-1015914-24 WHERE Case/Suffix = 'loc.'"}
{"text": "table: 1-1015914-24\ncolumns: Case/Suffix, we two, you and I, you two, them two (the two), who-two\nQ: What is them two (the two) when we two is ngalbelpa?\nA: SELECT them two (the two) FROM 1-1015914-24 WHERE we two = 'ngalbelpa'"}
{"text": "table: 1-1015914-24\ncolumns: Case/Suffix, we two, you and I, you two, them two (the two), who-two\nQ: What is them two (the two) when you and i is ng\u0153balngu?\nA: SELECT them two (the two) FROM 1-1015914-24 WHERE you and I = 'ng\u0153balngu'"}
{"text": "table: 1-1015914-24\ncolumns: Case/Suffix, we two, you and I, you two, them two (the two), who-two\nQ: What is who-two where you and i is ng\u0153ban?\nA: SELECT who-two FROM 1-1015914-24 WHERE you and I = 'ng\u0153ban'"}
{"text": "table: 1-1015914-24\ncolumns: Case/Suffix, we two, you and I, you two, them two (the two), who-two\nQ: What is we two where you two is ngipen?\nA: SELECT we two FROM 1-1015914-24 WHERE you two = 'ngipen'"}
{"text": "table: 1-1015914-24\ncolumns: Case/Suffix, we two, you and I, you two, them two (the two), who-two\nQ: What is who-two when you two is ngipelngu?\nA: SELECT who-two FROM 1-1015914-24 WHERE you two = 'ngipelngu'"}
{"text": "table: 1-10160447-1\ncolumns: Position, Driver, Points, Winnings, Series\nQ: what's the\u00a0points\u00a0with\u00a0driver\u00a0 mark martin\nA: SELECT Points FROM 1-10160447-1 WHERE Driver = 'Mark Martin'"}
{"text": "table: 1-10160447-1\ncolumns: Position, Driver, Points, Winnings, Series\nQ: what's the\u00a0points\u00a0with\u00a0driver\u00a0 rusty wallace\nA: SELECT Points FROM 1-10160447-1 WHERE Driver = 'Rusty Wallace'"}
{"text": "table: 1-10160447-1\ncolumns: Position, Driver, Points, Winnings, Series\nQ: what's the total number of\u00a0position\u00a0with\u00a0driver\u00a0 robby gordon\nA: SELECT COUNT Position FROM 1-10160447-1 WHERE Driver = 'Robby Gordon'"}
{"text": "table: 1-10160447-1\ncolumns: Position, Driver, Points, Winnings, Series\nQ: what's the maximum\u00a0position\u00a0with\u00a0winnings\u00a0 $50,000\nA: SELECT MAX Position FROM 1-10160447-1 WHERE Winnings = '$50,000'"}
{"text": "table: 1-10236830-6\ncolumns: Nomination, Actors Name, Film Name, Director, Country\nQ: What actor was nominted for an award in the film Anastasiya Slutskaya?\nA: SELECT Actors Name FROM 1-10236830-6 WHERE Film Name = 'Anastasiya Slutskaya'"}
{"text": "table: 1-10236830-6\ncolumns: Nomination, Actors Name, Film Name, Director, Country\nQ: What was the film Falling up nominated for?\nA: SELECT Nomination FROM 1-10236830-6 WHERE Film Name = 'Falling Up'"}
{"text": "table: 1-10236830-6\ncolumns: Nomination, Actors Name, Film Name, Director, Country\nQ: What is the name of the actress that was nominated for best actress in a leading role in the film Chopin: Desire for love?\nA: SELECT Actors Name FROM 1-10236830-6 WHERE Film Name = 'Chopin: Desire for Love' AND Nomination = 'Best Actress in a Leading Role'"}
{"text": "table: 1-10236830-6\ncolumns: Nomination, Actors Name, Film Name, Director, Country\nQ: What is the name of the actress that was nominated for best actress in a leading role in the film Chopin: Desire for love?\nA: SELECT Actors Name FROM 1-10236830-6 WHERE Film Name = 'Chopin: Desire for Love' AND Nomination = 'Best Actress in a Leading Role'"}
{"text": "table: 1-10236830-6\ncolumns: Nomination, Actors Name, Film Name, Director, Country\nQ: Which films does the actor Alla Sergiyko star in?\nA: SELECT Film Name FROM 1-10236830-6 WHERE Actors Name = 'Alla Sergiyko'"}
{"text": "table: 1-10236830-6\ncolumns: Nomination, Actors Name, Film Name, Director, Country\nQ: Which nominations was the film 27 Stolen Kisses nominated for?\nA: SELECT Nomination FROM 1-10236830-6 WHERE Film Name = '27 Stolen Kisses'"}
{"text": "table: 1-10236830-4\ncolumns: Nomination, Actors Name, Film Name, Director, Country\nQ: Which actor from Serbia was nominated for best actor in a supporting role?\nA: SELECT Actors Name FROM 1-10236830-4 WHERE Nomination = 'Best Actor in a Supporting Role' AND Country = 'Serbia'"}
{"text": "table: 1-10236830-4\ncolumns: Nomination, Actors Name, Film Name, Director, Country\nQ: Vsevolod Shilovskiy is from what country?\nA: SELECT Country FROM 1-10236830-4 WHERE Actors Name = 'Vsevolod Shilovskiy'"}
{"text": "table: 1-10236830-4\ncolumns: Nomination, Actors Name, Film Name, Director, Country\nQ: Which nominations are connected to the film Totalitarian Romance?\nA: SELECT Nomination FROM 1-10236830-4 WHERE Film Name = 'Totalitarian Romance'"}
{"text": "table: 1-10236830-4\ncolumns: Nomination, Actors Name, Film Name, Director, Country\nQ: Srdjan Dragojevic worked on a film which earned what nomination?\nA: SELECT Nomination FROM 1-10236830-4 WHERE Director = 'Srdjan Dragojevic'"}
{"text": "table: 1-10236830-4\ncolumns: Nomination, Actors Name, Film Name, Director, Country\nQ: Which actors are from Ukraine?\nA: SELECT Actors Name FROM 1-10236830-4 WHERE Country = 'Ukraine'"}
{"text": "table: 1-10236830-1\ncolumns: Nomination, Actors Name, Film Name, Director, Country\nQ: What was the film that vadim ilyenko directed?\nA: SELECT Film Name FROM 1-10236830-1 WHERE Director = 'Vadim Ilyenko'"}
{"text": "table: 1-10236830-1\ncolumns: Nomination, Actors Name, Film Name, Director, Country\nQ: What was the actors name that vadim ilyenko directed?\nA: SELECT Actors Name FROM 1-10236830-1 WHERE Director = 'Vadim Ilyenko'"}
{"text": "table: 1-10236830-1\ncolumns: Nomination, Actors Name, Film Name, Director, Country\nQ: What was the actors name for fuchzhou and nomination was best non-professional actor?\nA: SELECT Actors Name FROM 1-10236830-1 WHERE Film Name = 'Fuchzhou' AND Nomination = 'Best Non-Professional Actor'"}
{"text": "table: 1-10236830-1\ncolumns: Nomination, Actors Name, Film Name, Director, Country\nQ: What film did michaylo ilyenko make with best actor in a supporting role?\nA: SELECT Film Name FROM 1-10236830-1 WHERE Director = 'Michaylo Ilyenko' AND Nomination = 'Best Actor in a Supporting Role'"}
{"text": "table: 1-10236830-1\ncolumns: Nomination, Actors Name, Film Name, Director, Country\nQ: What was the actor's name for best debut?\nA: SELECT Actors Name FROM 1-10236830-1 WHERE Nomination = 'Best Debut'"}
{"text": "table: 1-10236830-1\ncolumns: Nomination, Actors Name, Film Name, Director, Country\nQ: What was the number of nominations for natalia raskokoha?\nA: SELECT COUNT Nomination FROM 1-10236830-1 WHERE Actors Name = 'Natalia Raskokoha'"}
{"text": "table: 1-10240125-1\ncolumns: Season, Division, League Apps, League Goals, FA Cup Apps, FA Cup Goals, Total Apps, Total Goals\nQ: What is the highest value of Total Goals?\nA: SELECT MAX Total Goals FROM 1-10240125-1"}
{"text": "table: 1-10240125-1\ncolumns: Season, Division, League Apps, League Goals, FA Cup Apps, FA Cup Goals, Total Apps, Total Goals\nQ: When FA Cup Apps is 9 what is the smallest number of FA Cup Goals?\nA: SELECT MIN FA Cup Goals FROM 1-10240125-1 WHERE FA Cup Apps = 9"}
{"text": "table: 1-10240125-1\ncolumns: Season, Division, League Apps, League Goals, FA Cup Apps, FA Cup Goals, Total Apps, Total Goals\nQ: What is the smallest number of Total Goals?\nA: SELECT MIN Total Goals FROM 1-10240125-1"}
{"text": "table: 1-10264179-2\ncolumns: Round, Circuit, Date, Pole Position, Fastest Lap, Winning Driver, Winning Team\nQ: What circuit was the race where Hideki Mutoh had the fastest lap?\nA: SELECT Circuit FROM 1-10264179-2 WHERE Fastest Lap = 'Hideki Mutoh'"}
{"text": "table: 1-10269427-3\ncolumns: Episode #, Production code, Title, Directed by, Written by, Airdate\nQ: what is the minimum production code with title \"foreign exchange problem / turn about\"\nA: SELECT MIN Production code FROM 1-10269427-3 WHERE Title = '\"Foreign Exchange Problem / Turn About\"'"}
{"text": "table: 1-10269427-3\ncolumns: Episode #, Production code, Title, Directed by, Written by, Airdate\nQ: what is the episode # for title \"the yindianapolis 500 / personality problem\"\nA: SELECT Episode # FROM 1-10269427-3 WHERE Title = '\"The Yindianapolis 500 / Personality Problem\"'"}
{"text": "table: 1-10269427-3\ncolumns: Episode #, Production code, Title, Directed by, Written by, Airdate\nQ: what is the episode # for production code 227\nA: SELECT Episode # FROM 1-10269427-3 WHERE Production code = 227"}
{"text": "table: 1-10269427-3\ncolumns: Episode #, Production code, Title, Directed by, Written by, Airdate\nQ: who directed the movie written by is sib ventress / aydrea ten bosch\nA: SELECT Directed by FROM 1-10269427-3 WHERE Written by = 'Sib Ventress / Aydrea ten Bosch'"}
{"text": "table: 1-10269427-3\ncolumns: Episode #, Production code, Title, Directed by, Written by, Airdate\nQ: what is the production code with title \"skirting the issue / moon over my yinnie\"\nA: SELECT Production code FROM 1-10269427-3 WHERE Title = '\"Skirting the Issue / Moon Over my Yinnie\"'"}
{"text": "table: 1-10240125-2\ncolumns: Season, Division, League Apps, League Goals, FA Cup Apps, FA Cup Goals, Total Apps, Total Goals\nQ: Whatis the number of total goals maximum?\nA: SELECT MAX Total Goals FROM 1-10240125-2"}
{"text": "table: 1-10262329-1\ncolumns: Assembly Type, Adhesive Type, Time(Sec), Temp (\u00b0C), Pressure\nQ: HOW MANY TEMPERATURE INTERVALS ARE POSSIBLE TO USE WITH ACRYL? \nA: SELECT COUNT Temp (\u00b0C) FROM 1-10262329-1 WHERE Adhesive Type = 'Acryl'"}
{"text": "table: 1-1028356-3\ncolumns: Outcome, Year, Championship, Surface, Partner, Opponents, Score\nQ: How many matches where played with Jim Pugh?\nA: SELECT COUNT Opponents FROM 1-1028356-3 WHERE Partner = 'Jim Pugh'"}
{"text": "table: 1-1028356-3\ncolumns: Outcome, Year, Championship, Surface, Partner, Opponents, Score\nQ: What is the score with partner Jim Pugh?\nA: SELECT Score FROM 1-1028356-3 WHERE Partner = 'Jim Pugh'"}
{"text": "table: 1-1028356-3\ncolumns: Outcome, Year, Championship, Surface, Partner, Opponents, Score\nQ: How many matched scored 3\u20136, 7\u20136(5), 6\u20133?\nA: SELECT COUNT Surface FROM 1-1028356-3 WHERE Score = '3\u20136, 7\u20136(5), 6\u20133'"}
{"text": "table: 1-1028356-3\ncolumns: Outcome, Year, Championship, Surface, Partner, Opponents, Score\nQ: What is the score of the match with partner Jim Pugh?\nA: SELECT Score FROM 1-1028356-3 WHERE Partner = 'Jim Pugh'"}
{"text": "table: 1-1028356-3\ncolumns: Outcome, Year, Championship, Surface, Partner, Opponents, Score\nQ: What year was the championship in Wimbledon (2)?\nA: SELECT Year FROM 1-1028356-3 WHERE Championship = 'Wimbledon (2)'"}
{"text": "table: 1-1028356-3\ncolumns: Outcome, Year, Championship, Surface, Partner, Opponents, Score\nQ: What is the score of the match with opponents Gretchen Magers Kelly Jones?\nA: SELECT Score FROM 1-1028356-3 WHERE Opponents = 'Gretchen Magers Kelly Jones'"}
{"text": "table: 1-10284385-1\ncolumns: Begin Date, End Date, Representative, Date of birth, House term, State served, Party, Age (years, days)\nQ: How many birthdays does Earl Hanley Beshlin have?\nA: SELECT COUNT Date of birth FROM 1-10284385-1 WHERE Representative = 'Earl Hanley Beshlin'"}
{"text": "table: 1-10284385-1\ncolumns: Begin Date, End Date, Representative, Date of birth, House term, State served, Party, Age (years, days)\nQ: Which politican party has a birthday of November 10, 1880\nA: SELECT Party FROM 1-10284385-1 WHERE Date of birth = 'November 10, 1880'"}
{"text": "table: 1-10284385-1\ncolumns: Begin Date, End Date, Representative, Date of birth, House term, State served, Party, Age (years, days)\nQ: Which representative has a birthday of January 31, 1866?\nA: SELECT Representative FROM 1-10284385-1 WHERE Date of birth = 'January 31, 1866'"}
{"text": "table: 1-10295819-1\ncolumns: Player, Current singles ranking, Current doubles ranking, First year played, Ties played, Total W\u2013L, Singles W\u2013L, Doubles W\u2013L\nQ: What is the Singles W-L for the players named  Laurynas Grigelis?\nA: SELECT Singles W\u2013L FROM 1-10295819-1 WHERE Player = 'Laurynas Grigelis'"}
{"text": "table: 1-10295819-1\ncolumns: Player, Current singles ranking, Current doubles ranking, First year played, Ties played, Total W\u2013L, Singles W\u2013L, Doubles W\u2013L\nQ: What is the Current singles ranking for the player named Mantas Bugaili\u0161kis?\nA: SELECT Current singles ranking FROM 1-10295819-1 WHERE Player = 'Mantas Bugaili\u0161kis'"}
{"text": "table: 1-10312547-1\ncolumns: Character, 1954 Broadway, 1955 broadcast, 1960 broadcast, 1979 Broadway, 1990 Broadway, 1991 Broadway, 1998 Broadway, 1999 Broadway\nQ: How many playerd Mrs. Darling in the 1999 Broadway?\nA: SELECT COUNT 1999 Broadway FROM 1-10312547-1 WHERE Character = 'Mrs. Darling'"}
{"text": "table: 1-10312547-1\ncolumns: Character, 1954 Broadway, 1955 broadcast, 1960 broadcast, 1979 Broadway, 1990 Broadway, 1991 Broadway, 1998 Broadway, 1999 Broadway\nQ: Who played Peter Pan in the 1990 Broadway?\nA: SELECT 1990 Broadway FROM 1-10312547-1 WHERE Character = 'Peter Pan'"}
{"text": "table: 1-10312547-1\ncolumns: Character, 1954 Broadway, 1955 broadcast, 1960 broadcast, 1979 Broadway, 1990 Broadway, 1991 Broadway, 1998 Broadway, 1999 Broadway\nQ: Who played in the 1991 Broadway before Barbara McCulloh played the part in the 1999 Broadway?\nA: SELECT 1991 Broadway FROM 1-10312547-1 WHERE 1999 Broadway = 'Barbara McCulloh'"}
{"text": "table: 1-10312547-1\ncolumns: Character, 1954 Broadway, 1955 broadcast, 1960 broadcast, 1979 Broadway, 1990 Broadway, 1991 Broadway, 1998 Broadway, 1999 Broadway\nQ: Who played in the 1990 Broadway after Tom Halloran played the character in the 1955 Broadcast?\nA: SELECT 1990 Broadway FROM 1-10312547-1 WHERE 1955 broadcast = 'Tom Halloran'"}
{"text": "table: 1-10312547-1\ncolumns: Character, 1954 Broadway, 1955 broadcast, 1960 broadcast, 1979 Broadway, 1990 Broadway, 1991 Broadway, 1998 Broadway, 1999 Broadway\nQ: What character did Drake English play in the 1998 Broadway?\nA: SELECT Character FROM 1-10312547-1 WHERE 1998 Broadway = 'Drake English'"}
{"text": "table: 1-103084-4\ncolumns: Year, Broadcast date, BBC One total viewing, BBC One Rank, BBC Two total viewing, BBC Two Rank\nQ: What date was BBC One total viewing greater then 11616996.338225884?\nA: SELECT Broadcast date FROM 1-103084-4 WHERE BBC One total viewing > 11616996.338225884"}
{"text": "table: 1-103084-4\ncolumns: Year, Broadcast date, BBC One total viewing, BBC One Rank, BBC Two total viewing, BBC Two Rank\nQ: How many years did BBC One rank 20th?\nA: SELECT COUNT Year FROM 1-103084-4 WHERE BBC One Rank = '20th'"}
{"text": "table: 1-103084-4\ncolumns: Year, Broadcast date, BBC One total viewing, BBC One Rank, BBC Two total viewing, BBC Two Rank\nQ: What year was the BBC two total viewing 7,530,000?\nA: SELECT MIN Year FROM 1-103084-4 WHERE BBC Two total viewing = '7,530,000'"}
{"text": "table: 1-10321124-1\ncolumns: \u2193 Function / Genus \u2192, Shigella, Salmonella, Yersinia, Escherichia\nQ:  how many\u00a0\u2193 function / genus \u2192\u00a0with\u00a0escherichia\u00a0being espd\nA: SELECT COUNT \u2193 Function / Genus \u2192 FROM 1-10321124-1 WHERE Escherichia = 'EspD'"}
{"text": "table: 1-10321124-1\ncolumns: \u2193 Function / Genus \u2192, Shigella, Salmonella, Yersinia, Escherichia\nQ: what's the\u00a0salmonella\u00a0with\u00a0escherichia\u00a0being espd\nA: SELECT Salmonella FROM 1-10321124-1 WHERE Escherichia = 'EspD'"}
{"text": "table: 1-10321124-1\ncolumns: \u2193 Function / Genus \u2192, Shigella, Salmonella, Yersinia, Escherichia\nQ: what's the\u00a0\u2193 function / genus \u2192\u00a0with\u00a0shigella\u00a0being spa32\nA: SELECT \u2193 Function / Genus \u2192 FROM 1-10321124-1 WHERE Shigella = 'Spa32'"}
{"text": "table: 1-10321124-1\ncolumns: \u2193 Function / Genus \u2192, Shigella, Salmonella, Yersinia, Escherichia\nQ: what's the\u00a0salmonella\u00a0with\u00a0shigella\u00a0being ipgc\nA: SELECT Salmonella FROM 1-10321124-1 WHERE Shigella = 'IpgC'"}
{"text": "table: 1-10321124-1\ncolumns: \u2193 Function / Genus \u2192, Shigella, Salmonella, Yersinia, Escherichia\nQ: what's the\u00a0salmonella\u00a0with\u00a0escherichia\u00a0being sepb (escn)\nA: SELECT Salmonella FROM 1-10321124-1 WHERE Escherichia = 'SepB (EscN)'"}
{"text": "table: 1-10321124-1\ncolumns: \u2193 Function / Genus \u2192, Shigella, Salmonella, Yersinia, Escherichia\nQ: what's the\u00a0shigella\u00a0with\u00a0yersinia\u00a0being yscp\nA: SELECT Shigella FROM 1-10321124-1 WHERE Yersinia = 'YscP'"}
{"text": "table: 1-10321805-1\ncolumns: Year (Ceremony), Film title used in nomination, Original title, Director, Result\nQ: How many original titles did Marriage Italian-Style have? \nA: SELECT COUNT Original title FROM 1-10321805-1 WHERE Film title used in nomination = 'Marriage Italian-Style'"}
{"text": "table: 1-10321805-1\ncolumns: Year (Ceremony), Film title used in nomination, Original title, Director, Result\nQ: What year was a movie with the original title La Leggenda del Santo Bevitore submitted?\nA: SELECT Year (Ceremony) FROM 1-10321805-1 WHERE Original title = 'La leggenda del santo bevitore'"}
{"text": "table: 1-10335-1\ncolumns: Camp, Estimated deaths, Operational, Occupied territory, Current country of location, Primary means for mass killings\nQ: what's the\u00a0camp\u00a0with\u00a0estimated deaths\u00a0of 600,000\nA: SELECT Camp FROM 1-10335-1 WHERE Estimated deaths = '600,000'"}
{"text": "table: 1-10335-1\ncolumns: Camp, Estimated deaths, Operational, Occupied territory, Current country of location, Primary means for mass killings\nQ: what's the\u00a0operational period\u00a0with\u00a0camp\u00a0 sajmi\u0161te\nA: SELECT Operational FROM 1-10335-1 WHERE Camp = 'Sajmi\u0161te'"}
{"text": "table: 1-10335-1\ncolumns: Camp, Estimated deaths, Operational, Occupied territory, Current country of location, Primary means for mass killings\nQ: what's the\u00a0estimated deaths\u00a0with\u00a0operational period\u00a0of 17 march 1942 \u2013 end of june 1943\nA: SELECT Estimated deaths FROM 1-10335-1 WHERE Operational = '17 March 1942 \u2013 end of June 1943'"}
{"text": "table: 1-10335-1\ncolumns: Camp, Estimated deaths, Operational, Occupied territory, Current country of location, Primary means for mass killings\nQ: what's the\u00a0current country of location\u00a0with\u00a0operational period\u00a0\u00a0of summer of 1941 to 28 june 1944\nA: SELECT Current country of location FROM 1-10335-1 WHERE Operational = 'Summer of 1941 to 28 June 1944'"}
{"text": "table: 1-10335-1\ncolumns: Camp, Estimated deaths, Operational, Occupied territory, Current country of location, Primary means for mass killings\nQ: what's the\u00a0occupied territory\u00a0with\u00a0estimated deaths\u00a0of 600,000\nA: SELECT Occupied territory FROM 1-10335-1 WHERE Estimated deaths = '600,000'"}
{"text": "table: 1-10335-1\ncolumns: Camp, Estimated deaths, Operational, Occupied territory, Current country of location, Primary means for mass killings\nQ: what's the\u00a0occupied territory\u00a0with\u00a0operational\u00a0period of may 1940 \u2013 january 1945\nA: SELECT Occupied territory FROM 1-10335-1 WHERE Operational = 'May 1940 \u2013 January 1945'"}
{"text": "table: 1-10360823-1\ncolumns: Round, Choice, Overall, Player name, Position, College\nQ: Which overall pick was traded to the Cleveland Browns?\nA: SELECT Overall FROM 1-10360823-1 WHERE College = 'Traded to the Cleveland Browns'"}
{"text": "table: 1-10360823-1\ncolumns: Round, Choice, Overall, Player name, Position, College\nQ: Overall pick 240 was a pick in which round?\nA: SELECT Round FROM 1-10360823-1 WHERE Overall = 240"}
{"text": "table: 1-10360823-1\ncolumns: Round, Choice, Overall, Player name, Position, College\nQ: Which overall pick number went to college at Youngstown State?\nA: SELECT MIN Overall FROM 1-10360823-1 WHERE College = 'Youngstown State'"}
{"text": "table: 1-10360823-1\ncolumns: Round, Choice, Overall, Player name, Position, College\nQ: What position is played by pick 255 overall?\nA: SELECT Position FROM 1-10360823-1 WHERE Overall = 255"}
{"text": "table: 1-10360823-1\ncolumns: Round, Choice, Overall, Player name, Position, College\nQ: Which player was chosen in round 17?\nA: SELECT Player name FROM 1-10360823-1 WHERE Round = 17"}
{"text": "table: 1-10361453-2\ncolumns: Game, Date, Opponent, Result, Vikings points, Opponents, Record, Attendance\nQ: The record of 7-3 had the largest attendance of what?\nA: SELECT MAX Attendance FROM 1-10361453-2 WHERE Record = '7-3'"}
{"text": "table: 1-10361453-2\ncolumns: Game, Date, Opponent, Result, Vikings points, Opponents, Record, Attendance\nQ: The record of 9-4 was against which opponent?\nA: SELECT Opponent FROM 1-10361453-2 WHERE Record = '9-4'"}
{"text": "table: 1-10361453-2\ncolumns: Game, Date, Opponent, Result, Vikings points, Opponents, Record, Attendance\nQ: The game number of 8 had a record of what?\nA: SELECT Record FROM 1-10361453-2 WHERE Game = 8"}
{"text": "table: 1-10360656-1\ncolumns: Round, Choice, Overall, Player name, Position, College\nQ: What round was Steve Stonebreaker drafted?\nA: SELECT MAX Round FROM 1-10360656-1 WHERE Player name = 'Steve Stonebreaker'"}
{"text": "table: 1-10360656-1\ncolumns: Round, Choice, Overall, Player name, Position, College\nQ: Who was the top picki n the draft?\nA: SELECT MIN Choice FROM 1-10360656-1"}
{"text": "table: 1-10360656-1\ncolumns: Round, Choice, Overall, Player name, Position, College\nQ: What round was Bill Hill drafted?\nA: SELECT Choice FROM 1-10360656-1 WHERE Player name = 'Bill Hill'"}
{"text": "table: 1-10360656-1\ncolumns: Round, Choice, Overall, Player name, Position, College\nQ: What was the name of the quarterback drafted?\nA: SELECT Player name FROM 1-10360656-1 WHERE Position = 'Quarterback'"}
{"text": "table: 1-10361625-1\ncolumns: Round, Choice, Overall, Player name, Position, College\nQ: Where is the college where Keith Hartwig plays?\nA: SELECT College FROM 1-10361625-1 WHERE Player name = 'Keith Hartwig'"}
{"text": "table: 1-10361625-1\ncolumns: Round, Choice, Overall, Player name, Position, College\nQ: What is the name of the linebacker at Illinois college?\nA: SELECT Player name FROM 1-10361625-1 WHERE Position = 'Linebacker' AND College = 'Illinois'"}
{"text": "table: 1-10361625-1\ncolumns: Round, Choice, Overall, Player name, Position, College\nQ: What is the greatest round of overall 83?\nA: SELECT MAX Round FROM 1-10361625-1 WHERE Overall = 83"}
{"text": "table: 1-10361625-1\ncolumns: Round, Choice, Overall, Player name, Position, College\nQ: Which round did Tommy Kramer play in>\nA: SELECT Round FROM 1-10361625-1 WHERE Player name = 'Tommy Kramer'"}
{"text": "table: 1-10361625-1\ncolumns: Round, Choice, Overall, Player name, Position, College\nQ: What is Rice's collage score?\nA: SELECT Overall FROM 1-10361625-1 WHERE College = 'Rice'"}
{"text": "table: 1-10361230-1\ncolumns: Round, Choice, Overall, Player name, Position, College\nQ: Where does the defensive back position appear first?\nA: SELECT MIN Round FROM 1-10361230-1 WHERE Position = 'Defensive Back'"}
{"text": "table: 1-10361230-1\ncolumns: Round, Choice, Overall, Player name, Position, College\nQ: What is Bruce Cerone overall?\nA: SELECT MIN Overall FROM 1-10361230-1 WHERE Player name = 'Bruce Cerone'"}
{"text": "table: 1-10361230-1\ncolumns: Round, Choice, Overall, Player name, Position, College\nQ: Which player went to Emporia State?\nA: SELECT Player name FROM 1-10361230-1 WHERE College = 'Emporia State'"}
{"text": "table: 1-10361230-1\ncolumns: Round, Choice, Overall, Player name, Position, College\nQ: What is the highest choice?\nA: SELECT MAX Choice FROM 1-10361230-1"}
{"text": "table: 1-10361230-1\ncolumns: Round, Choice, Overall, Player name, Position, College\nQ: What college did Bill Cappleman go to?\nA: SELECT College FROM 1-10361230-1 WHERE Player name = 'Bill Cappleman'"}
{"text": "table: 1-1036189-1\ncolumns: Headstamp ID, Primer Annulus Color, Bullet Tip Color, Other Features, Functional Type\nQ: For the headstamp id of h2, what was the color of the bullet tip?\nA: SELECT Bullet Tip Color FROM 1-1036189-1 WHERE Headstamp ID = 'H2'"}
{"text": "table: 1-1036189-1\ncolumns: Headstamp ID, Primer Annulus Color, Bullet Tip Color, Other Features, Functional Type\nQ: For the functional type of light ball, what were the other features?\nA: SELECT Other Features FROM 1-1036189-1 WHERE Functional Type = 'Light Ball'"}
{"text": "table: 1-1036189-1\ncolumns: Headstamp ID, Primer Annulus Color, Bullet Tip Color, Other Features, Functional Type\nQ: How many primers annulus colors were there when the color of the bullet tip was white?\nA: SELECT COUNT Primer Annulus Color FROM 1-1036189-1 WHERE Bullet Tip Color = 'White'"}
{"text": "table: 1-1036189-1\ncolumns: Headstamp ID, Primer Annulus Color, Bullet Tip Color, Other Features, Functional Type\nQ: How many bullet tips colors had other features of a blue band on case base?\nA: SELECT COUNT Bullet Tip Color FROM 1-1036189-1 WHERE Other Features = 'Blue band on case base'"}
{"text": "table: 1-1037590-1\ncolumns: Year, Games, Games started, Completions, Attempts, Completion %, Yards, Yards/Attempt, Touchdowns, Interceptions, Rating\nQ: How many touchdowns were scored in the year with a completion percentage of 56.0?\nA: SELECT MIN Touchdowns FROM 1-1037590-1 WHERE Completion % = '56.0'"}
{"text": "table: 1-1037590-1\ncolumns: Year, Games, Games started, Completions, Attempts, Completion %, Yards, Yards/Attempt, Touchdowns, Interceptions, Rating\nQ: What number of completions are recorded for the year with 12 games started?\nA: SELECT Completions FROM 1-1037590-1 WHERE Games started = 12"}
{"text": "table: 1-1037590-1\ncolumns: Year, Games, Games started, Completions, Attempts, Completion %, Yards, Yards/Attempt, Touchdowns, Interceptions, Rating\nQ: How many years were there with 348 attempts?\nA: SELECT COUNT Yards FROM 1-1037590-1 WHERE Attempts = 348"}
{"text": "table: 1-10402018-1\ncolumns: Character, Australia & New Zealand (Sydney - first run, Melbourne, Auckland), London, Toronto / Broadway, Brazil, UK Tour, US Tour, Italy (Milan, Rome, Trieste)\nQ: How many characters is by Babs Rubenstein?\nA: SELECT COUNT London FROM 1-10402018-1 WHERE US Tour = 'Babs Rubenstein'"}
{"text": "table: 1-10402018-1\ncolumns: Character, Australia & New Zealand (Sydney - first run, Melbourne, Auckland), London, Toronto / Broadway, Brazil, UK Tour, US Tour, Italy (Milan, Rome, Trieste)\nQ: Which person is in the tronto/broadway and has a uk tour of n/a\nA: SELECT Toronto / Broadway FROM 1-10402018-1 WHERE UK Tour = 'n/a'"}
{"text": "table: 1-10402018-1\ncolumns: Character, Australia & New Zealand (Sydney - first run, Melbourne, Auckland), London, Toronto / Broadway, Brazil, UK Tour, US Tour, Italy (Milan, Rome, Trieste)\nQ: How many people play Frank in London?\nA: SELECT COUNT London FROM 1-10402018-1 WHERE Character = 'Frank'"}
{"text": "table: 1-10399701-2\ncolumns: School Year, Class A, Class AA, Class AAA, Class AAAA, Class AAAAA\nQ: Who was Class AAA during the school year of 2000-01?\nA: SELECT Class AAA FROM 1-10399701-2 WHERE School Year = '2000-01'"}
{"text": "table: 1-10399701-2\ncolumns: School Year, Class A, Class AA, Class AAA, Class AAAA, Class AAAAA\nQ: Who was Class AAA during the same year that Class A was (tie) Apple Springs/Texline?\nA: SELECT Class AAA FROM 1-10399701-2 WHERE Class A = '(tie) Apple Springs/Texline'"}
{"text": "table: 1-10399701-2\ncolumns: School Year, Class A, Class AA, Class AAA, Class AAAA, Class AAAAA\nQ: Who was Class AAAAA during the school year of 1995-96?\nA: SELECT Class AAAAA FROM 1-10399701-2 WHERE School Year = '1995-96'"}
{"text": "table: 1-10399701-2\ncolumns: School Year, Class A, Class AA, Class AAA, Class AAAA, Class AAAAA\nQ: Who was Class AAA during the same year that Class AAAAA was Brownsville Pace?\nA: SELECT Class AAA FROM 1-10399701-2 WHERE Class AAAAA = 'Brownsville Pace'"}
{"text": "table: 1-10399701-2\ncolumns: School Year, Class A, Class AA, Class AAA, Class AAAA, Class AAAAA\nQ: What was the total number of Class AAA during the same year that Class AAA was White Oak?\nA: SELECT COUNT Class AA FROM 1-10399701-2 WHERE Class AAA = 'White Oak'"}
{"text": "table: 1-10392906-2\ncolumns: Week, Date, Kickoff, Opponent, Final score, Team record, Game site, Attendance\nQ: How many records are listed on Friday, May 25?\nA: SELECT COUNT Team record FROM 1-10392906-2 WHERE Date = 'Friday, May 25'"}
{"text": "table: 1-10392906-2\ncolumns: Week, Date, Kickoff, Opponent, Final score, Team record, Game site, Attendance\nQ: How many opponents were played on Saturday, June 9?\nA: SELECT COUNT Opponent FROM 1-10392906-2 WHERE Date = 'Saturday, June 9'"}
{"text": "table: 1-10392906-2\ncolumns: Week, Date, Kickoff, Opponent, Final score, Team record, Game site, Attendance\nQ: In what week was the first game played at the Commerzbank-Arena?\nA: SELECT MIN Week FROM 1-10392906-2 WHERE Game site = 'Commerzbank-Arena'"}
{"text": "table: 1-10413597-5\ncolumns: No. in series, No. in season, Title, Setting, Directed by, Written by, U.S. viewers (million), Original air date\nQ: What was the original air date of an episode set in 1544?\nA: SELECT Original air date FROM 1-10413597-5 WHERE Setting = '1544'"}
{"text": "table: 1-10413597-5\ncolumns: No. in series, No. in season, Title, Setting, Directed by, Written by, U.S. viewers (million), Original air date\nQ: How many settings where there for episode 29 of the season?\nA: SELECT COUNT Setting FROM 1-10413597-5 WHERE No. in series = 29"}
{"text": "table: 1-10413597-5\ncolumns: No. in series, No. in season, Title, Setting, Directed by, Written by, U.S. viewers (million), Original air date\nQ: Who wrote the episode that was set in winter 1541/february 13, 1542?\nA: SELECT Written by FROM 1-10413597-5 WHERE Setting = 'Winter 1541/February 13, 1542'"}
{"text": "table: 1-10413597-4\ncolumns: No. in series, No. in season, Title, Setting, Directed by, Written by, Original air date\nQ: What episode number of the season was \"The Northern Uprising\"?\nA: SELECT No. in season FROM 1-10413597-4 WHERE Title = '\"The Northern Uprising\"'"}
{"text": "table: 1-10416547-1\ncolumns: Date, Album name, Track, Track title, Lyricist, Music genre/style, Major instrument(s), Lyrics theme/style, Duration\nQ: What is the name of the track that lasts 5:30?\nA: SELECT Track title FROM 1-10416547-1 WHERE Duration = '5:30'"}
{"text": "table: 1-10416547-1\ncolumns: Date, Album name, Track, Track title, Lyricist, Music genre/style, Major instrument(s), Lyrics theme/style, Duration\nQ: What is the album namethat has the track title Sweetness \u751c\u751c\u7684 (ti\u00e1n ti\u00e1n de)?\nA: SELECT Album name FROM 1-10416547-1 WHERE Track title = 'Sweetness \u751c\u751c\u7684 (Ti\u00e1n ti\u00e1n de)'"}
{"text": "table: 1-10416547-1\ncolumns: Date, Album name, Track, Track title, Lyricist, Music genre/style, Major instrument(s), Lyrics theme/style, Duration\nQ: What is the duration of the song where the major instrument is the piano and the date is 2004-02-03?\nA: SELECT Duration FROM 1-10416547-1 WHERE Major instrument(s) = 'Piano' AND Date = '2004-02-03'"}
{"text": "table: 1-10416547-1\ncolumns: Date, Album name, Track, Track title, Lyricist, Music genre/style, Major instrument(s), Lyrics theme/style, Duration\nQ: What is the total number of lyricist where the lyrics theme is romance and the song lasts 3:50?\nA: SELECT COUNT Lyricist FROM 1-10416547-1 WHERE Lyrics theme/style = 'Romance' AND Duration = '3:50'"}
{"text": "table: 1-10416547-1\ncolumns: Date, Album name, Track, Track title, Lyricist, Music genre/style, Major instrument(s), Lyrics theme/style, Duration\nQ: What is the major instrument of the song that lasts 4:32? \nA: SELECT Major instrument(s) FROM 1-10416547-1 WHERE Duration = '4:32'"}
{"text": "table: 1-10416547-1\ncolumns: Date, Album name, Track, Track title, Lyricist, Music genre/style, Major instrument(s), Lyrics theme/style, Duration\nQ: What is the total number of music genre/style in which the lyrics are a detective story?\nA: SELECT COUNT Music genre/style FROM 1-10416547-1 WHERE Lyrics theme/style = 'Detective story'"}
{"text": "table: 1-1046071-1\ncolumns: Year, Division, League, Regular Season, Playoffs, Open Cup\nQ: What is the playoffs for the usl pro select league?\nA: SELECT Playoffs FROM 1-1046071-1 WHERE League = 'USL Pro Select League'"}
{"text": "table: 1-1046071-1\ncolumns: Year, Division, League, Regular Season, Playoffs, Open Cup\nQ: What is the number of the division for the 1st round?\nA: SELECT COUNT Division FROM 1-1046071-1 WHERE Open Cup = '1st Round'"}
{"text": "table: 1-10420426-1\ncolumns: Season, Series, Team, Races, Wins, Poles, F/Laps, Podiums, Points, Position\nQ: What was the team where series is formula renault 2.0 nec?\nA: SELECT Team FROM 1-10420426-1 WHERE Series = 'Formula Renault 2.0 NEC'"}
{"text": "table: 1-10420426-1\ncolumns: Season, Series, Team, Races, Wins, Poles, F/Laps, Podiums, Points, Position\nQ: What is the total number of poles for arden international?\nA: SELECT COUNT Poles FROM 1-10420426-1 WHERE Team = 'Arden International'"}
{"text": "table: 1-10420426-1\ncolumns: Season, Series, Team, Races, Wins, Poles, F/Laps, Podiums, Points, Position\nQ: What is the number of wins for gp2 series for racing engineering?\nA: SELECT COUNT Wins FROM 1-10420426-1 WHERE Series = 'GP2 Series' AND Team = 'Racing Engineering'"}
{"text": "table: 1-10420426-1\ncolumns: Season, Series, Team, Races, Wins, Poles, F/Laps, Podiums, Points, Position\nQ: What is the number of podiums for season 2010 for campionato italiano superstars.\nA: SELECT COUNT Podiums FROM 1-10420426-1 WHERE Season = '2010' AND Series = 'Campionato Italiano Superstars'"}
{"text": "table: 1-10420426-1\ncolumns: Season, Series, Team, Races, Wins, Poles, F/Laps, Podiums, Points, Position\nQ: What is the podium for 144 points?\nA: SELECT Podiums FROM 1-10420426-1 WHERE Points = 144"}
{"text": "table: 1-10470082-3\ncolumns: No., #, Title, Director, Writer, US air date, Storyteller, Villains\nQ: How many writers had an US air date of september 25, 1993?\nA: SELECT COUNT Writer FROM 1-10470082-3 WHERE US air date = 'September 25, 1993'"}
{"text": "table: 1-10470082-3\ncolumns: No., #, Title, Director, Writer, US air date, Storyteller, Villains\nQ: How many villians were in No. 25?\nA: SELECT COUNT Villains FROM 1-10470082-3 WHERE No. = 25"}
{"text": "table: 1-1046454-1\ncolumns: Year, Division, League, Regular Season, Playoffs, Open Cup\nQ: what being the maximum\u00a0year\u00a0where\u00a0regular season\u00a0is 4th, northwest\nA: SELECT MAX Year FROM 1-1046454-1 WHERE Regular Season = '4th, Northwest'"}
{"text": "table: 1-1046454-1\ncolumns: Year, Division, League, Regular Season, Playoffs, Open Cup\nQ: what is the total number of\u00a0playoffs\u00a0where\u00a0regular season\u00a0is 6th, southwest\nA: SELECT COUNT Playoffs FROM 1-1046454-1 WHERE Regular Season = '6th, Southwest'"}
{"text": "table: 1-1046454-1\ncolumns: Year, Division, League, Regular Season, Playoffs, Open Cup\nQ: what is the maximum\u00a0division\nA: SELECT MAX Division FROM 1-1046454-1"}
{"text": "table: 1-1046454-1\ncolumns: Year, Division, League, Regular Season, Playoffs, Open Cup\nQ:  what's the\u00a0league\u00a0where\u00a0regular season\u00a0is 2nd, northwest\nA: SELECT League FROM 1-1046454-1 WHERE Regular Season = '2nd, Northwest'"}
{"text": "table: 1-1046454-1\ncolumns: Year, Division, League, Regular Season, Playoffs, Open Cup\nQ: what are all the regular season where year is 2011\nA: SELECT Regular Season FROM 1-1046454-1 WHERE Year = 2011"}
{"text": "table: 1-10470082-5\ncolumns: No., #, Title, Director, Writer, US air date, Storyteller, Villains\nQ: How many titles have the number 11\nA: SELECT COUNT Title FROM 1-10470082-5 WHERE # = 11"}
{"text": "table: 1-10470082-5\ncolumns: No., #, Title, Director, Writer, US air date, Storyteller, Villains\nQ: How many have Mrs. briar as a villain\nA: SELECT COUNT No. FROM 1-10470082-5 WHERE Villains = 'Mrs. Briar'"}
{"text": "table: 1-10470082-5\ncolumns: No., #, Title, Director, Writer, US air date, Storyteller, Villains\nQ: how many have the number 8\nA: SELECT COUNT No. FROM 1-10470082-5 WHERE # = 8"}
{"text": "table: 1-10470082-5\ncolumns: No., #, Title, Director, Writer, US air date, Storyteller, Villains\nQ: How many have the title \"the tale of the room for rent\"\nA: SELECT COUNT # FROM 1-10470082-5 WHERE Title = '\"The Tale of the Room for Rent\"'"}
{"text": "table: 1-10470082-6\ncolumns: No., #, Title, Director, Writer, US air date, Storyteller, Villains\nQ: What is the name of the episode told by Kiki and directed by Will Dixon?\nA: SELECT Title FROM 1-10470082-6 WHERE Storyteller = 'Kiki' AND Director = 'Will Dixon'"}
{"text": "table: 1-10470082-6\ncolumns: No., #, Title, Director, Writer, US air date, Storyteller, Villains\nQ: Who is the storyteller in the episode called \"The Tale of the Jagged Sign\"?\nA: SELECT Storyteller FROM 1-10470082-6 WHERE Title = '\"The Tale of the Jagged Sign\"'"}
{"text": "table: 1-10470082-6\ncolumns: No., #, Title, Director, Writer, US air date, Storyteller, Villains\nQ: Who wrote Episode #3?\nA: SELECT Writer FROM 1-10470082-6 WHERE # = 3"}
{"text": "table: 1-10470082-7\ncolumns: No., #, Title, Director, Writer, US air date, Storyteller, Villains\nQ: Who are the villains in the episode titled \"The Tale of the Forever Game\"?\nA: SELECT Villains FROM 1-10470082-7 WHERE Title = '\"The Tale of The Forever Game\"'"}
{"text": "table: 1-10470082-7\ncolumns: No., #, Title, Director, Writer, US air date, Storyteller, Villains\nQ: How many villains appeared in the episode titled \"The Tale of the Virtual Pets\"?\nA: SELECT COUNT Villains FROM 1-10470082-7 WHERE Title = '\"The Tale of The Virtual Pets\"'"}
{"text": "table: 1-10470082-7\ncolumns: No., #, Title, Director, Writer, US air date, Storyteller, Villains\nQ: Who are the villains in the episodes where Megan is the storyteller and Lorette LeBlanc is the director?\nA: SELECT Villains FROM 1-10470082-7 WHERE Storyteller = 'Megan' AND Director = 'Lorette LeBlanc'"}
{"text": "table: 1-10470082-7\ncolumns: No., #, Title, Director, Writer, US air date, Storyteller, Villains\nQ: What is the largest # for an episode that was written by Allison Lea Bingeman?\nA: SELECT MAX # FROM 1-10470082-7 WHERE Writer = 'Allison Lea Bingeman'"}
{"text": "table: 1-10477224-1\ncolumns: Sepal length, Sepal width, Petal length, Petal width, Species\nQ: Name the species when petal width is 2.0 and petal length is 4.9\nA: SELECT Species FROM 1-10477224-1 WHERE Petal width = '2.0' AND Petal length = '4.9'"}
{"text": "table: 1-10477224-1\ncolumns: Sepal length, Sepal width, Petal length, Petal width, Species\nQ: Name the sepal width for i.virginica with petal length of 5.1\nA: SELECT Sepal width FROM 1-10477224-1 WHERE Species = 'I.virginica' AND Petal length = '5.1'"}
{"text": "table: 1-10477224-1\ncolumns: Sepal length, Sepal width, Petal length, Petal width, Species\nQ: Name the number of species with sepal width of 3.4 and sepal length of 5.4\nA: SELECT COUNT Species FROM 1-10477224-1 WHERE Sepal width = '3.4' AND Sepal length = '5.4'"}
{"text": "table: 1-10477224-1\ncolumns: Sepal length, Sepal width, Petal length, Petal width, Species\nQ: Name the sepal length for sepal width of 2.8 and petal length of 5.1\nA: SELECT Sepal length FROM 1-10477224-1 WHERE Sepal width = '2.8' AND Petal length = '5.1'"}
{"text": "table: 1-10477224-1\ncolumns: Sepal length, Sepal width, Petal length, Petal width, Species\nQ: Name the sepal width when sepal length is 6.5 and petal width is 2.2\nA: SELECT Sepal width FROM 1-10477224-1 WHERE Sepal length = '6.5' AND Petal width = '2.2'"}
{"text": "table: 1-10477224-1\ncolumns: Sepal length, Sepal width, Petal length, Petal width, Species\nQ: Name the sepal lengh when sepal width is 2.9 and petal width 1.3\nA: SELECT Sepal length FROM 1-10477224-1 WHERE Sepal width = '2.9' AND Petal width = '1.3'"}
{"text": "table: 1-10470082-4\ncolumns: No., #, Title, Director, Writer, US air date, Storyteller, Villains\nQ: Who is the director and what number is the episode for episode #1 of Are You Afraid of the Dark season 3?\nA: SELECT COUNT Director FROM 1-10470082-4 WHERE # = 1"}
{"text": "table: 1-10470082-4\ncolumns: No., #, Title, Director, Writer, US air date, Storyteller, Villains\nQ: Who is the director of the episode whom Scott Peters is the writer?\nA: SELECT Director FROM 1-10470082-4 WHERE Writer = 'Scott Peters'"}
{"text": "table: 1-10470082-4\ncolumns: No., #, Title, Director, Writer, US air date, Storyteller, Villains\nQ: Who is the villain in episode #7?\nA: SELECT Villains FROM 1-10470082-4 WHERE # = 7"}
{"text": "table: 1-10470082-8\ncolumns: No., #, Title, Director, Writer, US air date, Storyteller, Villains\nQ: Who wrote episode #1 in season 7?\nA: SELECT COUNT Writer FROM 1-10470082-8 WHERE # = 1"}
{"text": "table: 1-10470082-8\ncolumns: No., #, Title, Director, Writer, US air date, Storyteller, Villains\nQ: When did the episode written by Jim Morris air?\nA: SELECT US air date FROM 1-10470082-8 WHERE Writer = 'Jim Morris'"}
{"text": "table: 1-10527215-3\ncolumns: Rd, Name, Pole Position, Fastest Lap, Winning driver, Winning team, Report\nQ: What was Datsun Twin 200's fastest lap?\nA: SELECT Fastest Lap FROM 1-10527215-3 WHERE Name = 'Datsun Twin 200'"}
{"text": "table: 1-10527215-3\ncolumns: Rd, Name, Pole Position, Fastest Lap, Winning driver, Winning team, Report\nQ: In the Datsun Twin 200 race, what was the fastest lap?\nA: SELECT Fastest Lap FROM 1-10527215-3 WHERE Name = 'Datsun Twin 200'"}
{"text": "table: 1-10527215-3\ncolumns: Rd, Name, Pole Position, Fastest Lap, Winning driver, Winning team, Report\nQ: What's the report for the True Value 500?\nA: SELECT Report FROM 1-10527215-3 WHERE Name = 'True Value 500'"}
{"text": "table: 1-10527215-3\ncolumns: Rd, Name, Pole Position, Fastest Lap, Winning driver, Winning team, Report\nQ: What was Johnny Rutherford's fastest lap while Al Unser was the pole position?\nA: SELECT Fastest Lap FROM 1-10527215-3 WHERE Winning driver = 'Johnny Rutherford' AND Pole Position = 'Al Unser'"}
{"text": "table: 1-10527215-3\ncolumns: Rd, Name, Pole Position, Fastest Lap, Winning driver, Winning team, Report\nQ: What's the report on Penske Racing winning while the pole position was Al Unser?\nA: SELECT COUNT Report FROM 1-10527215-3 WHERE Pole Position = 'Al Unser' AND Winning team = 'Penske Racing'"}
{"text": "table: 1-104858-1\ncolumns: Country, Membership (from 2010), Name of member organization, Year current Scouting organization joined WOSM, Year member organization was founded, Admits boys/girls\nQ: Which countries have a scouting organization that was founded in 1926, and joined WOSM in 1930?\nA: SELECT Country FROM 1-104858-1 WHERE Year current Scouting organization joined WOSM = '1930' AND Year member organization was founded = '1926'"}
{"text": "table: 1-104858-1\ncolumns: Country, Membership (from 2010), Name of member organization, Year current Scouting organization joined WOSM, Year member organization was founded, Admits boys/girls\nQ: Does Venezuela admit only boys, only girls, or both?\nA: SELECT Admits boys/girls FROM 1-104858-1 WHERE Country = 'Venezuela'"}
{"text": "table: 1-104858-1\ncolumns: Country, Membership (from 2010), Name of member organization, Year current Scouting organization joined WOSM, Year member organization was founded, Admits boys/girls\nQ: Which organizations were founded in 1972, but became WOSM members until 1977?\nA: SELECT Name of member organization FROM 1-104858-1 WHERE Year member organization was founded = '1972' AND Year current Scouting organization joined WOSM = '1977'"}
{"text": "table: 1-104858-1\ncolumns: Country, Membership (from 2010), Name of member organization, Year current Scouting organization joined WOSM, Year member organization was founded, Admits boys/girls\nQ: Does the Scout Association of Hong Kong admit boys, girls, or both?\nA: SELECT Admits boys/girls FROM 1-104858-1 WHERE Name of member organization = 'The Scout Association of Hong Kong'"}
{"text": "table: 1-104858-1\ncolumns: Country, Membership (from 2010), Name of member organization, Year current Scouting organization joined WOSM, Year member organization was founded, Admits boys/girls\nQ: Does the Ghana Scout Association (founded in 1912) admit boys, girls, or both?\nA: SELECT Admits boys/girls FROM 1-104858-1 WHERE Year member organization was founded = '1912' AND Name of member organization = 'The Ghana Scout Association'"}
{"text": "table: 1-10528691-4\ncolumns: Model, Introduction, Discontinued, CPU Speed, Print resolution (DPI) Resolution is given in dots per inch (DPI), Print speed (PPM), Standard memory, Maximum memory\nQ: What is the model number introduced May 1999?\nA: SELECT MAX Model FROM 1-10528691-4 WHERE Introduction = 'May 1999'"}
{"text": "table: 1-10528691-4\ncolumns: Model, Introduction, Discontinued, CPU Speed, Print resolution (DPI) Resolution is given in dots per inch (DPI), Print speed (PPM), Standard memory, Maximum memory\nQ: What is the print resolution (FPI) for December 2002?\nA: SELECT Print resolution (DPI) Resolution is given in dots per inch (DPI) FROM 1-10528691-4 WHERE Introduction = 'December 2002'"}
{"text": "table: 1-10528691-4\ncolumns: Model, Introduction, Discontinued, CPU Speed, Print resolution (DPI) Resolution is given in dots per inch (DPI), Print speed (PPM), Standard memory, Maximum memory\nQ: What is the maximum memory for the model discontinued in November 2001?\nA: SELECT Maximum memory FROM 1-10528691-4 WHERE Discontinued = 'November 2001'"}
{"text": "table: 1-1053802-1\ncolumns: Region/country, Local title, Network, Winners, Main presenters\nQ: What is main presenters of La Granja?\nA: SELECT Main presenters FROM 1-1053802-1 WHERE Local title = 'La Granja'"}
{"text": "table: 1-1053802-1\ncolumns: Region/country, Local title, Network, Winners, Main presenters\nQ: What is the main presenter of bulgaria?\nA: SELECT Main presenters FROM 1-1053802-1 WHERE Region/country = 'Bulgaria'"}
{"text": "table: 1-1053802-1\ncolumns: Region/country, Local title, Network, Winners, Main presenters\nQ: How many winners are there of farma?\nA: SELECT COUNT Winners FROM 1-1053802-1 WHERE Local title = 'Farma'"}
{"text": "table: 1-10556257-1\ncolumns: Season, Team, League Apps, League Goals, Cup Apps, Cup Goals\nQ: What is the most cup goals for seasson 1911-12?\nA: SELECT MAX Cup Goals FROM 1-10556257-1 WHERE Season = '1911-12'"}
{"text": "table: 1-10556257-1\ncolumns: Season, Team, League Apps, League Goals, Cup Apps, Cup Goals\nQ: What is the league apps for season 1923-24?\nA: SELECT League Apps FROM 1-10556257-1 WHERE Season = '1923-24'"}
{"text": "table: 1-10556257-1\ncolumns: Season, Team, League Apps, League Goals, Cup Apps, Cup Goals\nQ: What is the team for season 1911-12?\nA: SELECT Team FROM 1-10556257-1 WHERE Season = '1911-12'"}
{"text": "table: 1-10566855-1\ncolumns: Season, Premier, Runner-up, Score, Margin, Venue, Attendance\nQ: what's the minimum\u00a0attendance\u00a0with\u00a0score\u00a0 10.16 (76) \u2013 9.22 (76)\nA: SELECT MIN Attendance FROM 1-10566855-1 WHERE Score = '10.16 (76) \u2013 9.22 (76)'"}
{"text": "table: 1-10566855-1\ncolumns: Season, Premier, Runner-up, Score, Margin, Venue, Attendance\nQ: who's the\u00a0premier\u00a0with\u00a0in 1970\nA: SELECT Premier FROM 1-10566855-1 WHERE Season = 1970"}
{"text": "table: 1-10566855-1\ncolumns: Season, Premier, Runner-up, Score, Margin, Venue, Attendance\nQ: who are all the runner-up for premier in richmond\nA: SELECT Runner-up FROM 1-10566855-1 WHERE Premier = 'Richmond'"}
{"text": "table: 1-10566855-1\ncolumns: Season, Premier, Runner-up, Score, Margin, Venue, Attendance\nQ: what is the minimum attendance with score 8.16 (64) \u2013 8.12 (60)\nA: SELECT MIN Attendance FROM 1-10566855-1 WHERE Score = '8.16 (64) \u2013 8.12 (60)'"}
{"text": "table: 1-10568553-1\ncolumns: County, Location, Street Names, Milepost, Roads Intersected, Notes\nQ: How many mileposts are there on Anne Street?\nA: SELECT COUNT Milepost FROM 1-10568553-1 WHERE Street Names = 'Anne Street'"}
{"text": "table: 1-10568553-1\ncolumns: County, Location, Street Names, Milepost, Roads Intersected, Notes\nQ: Which street is 12.2 miles long?\nA: SELECT Street Names FROM 1-10568553-1 WHERE Milepost = '12.2'"}
{"text": "table: 1-10568553-1\ncolumns: County, Location, Street Names, Milepost, Roads Intersected, Notes\nQ: Where does Route 24 intersect?\nA: SELECT Location FROM 1-10568553-1 WHERE Roads Intersected = 'Route 24'"}
{"text": "table: 1-10568553-1\ncolumns: County, Location, Street Names, Milepost, Roads Intersected, Notes\nQ: Where is milepost 12.8?\nA: SELECT Location FROM 1-10568553-1 WHERE Milepost = '12.8'"}
{"text": "table: 1-1057262-1\ncolumns: Commodity, 2001-02, 2002-03, 2003-04, 2004-05, 2005-06, 2006-07\nQ: What is the minimum amount for wool for 2001-02?\nA: SELECT MIN 2001-02 FROM 1-1057262-1 WHERE Commodity = 'Wool'"}
{"text": "table: 1-1057316-1\ncolumns: Serial number, Wheel arrangement ( Whyte notation ), Build date, Operational owner(s), Disposition\nQ: Who were the operational owners during the construction date of April 1892?\nA: SELECT Operational owner(s) FROM 1-1057316-1 WHERE Build date = 'April 1892'"}
{"text": "table: 1-1057316-1\ncolumns: Serial number, Wheel arrangement ( Whyte notation ), Build date, Operational owner(s), Disposition\nQ: Where can you find Colorado and Southern Railway #9?\nA: SELECT Disposition FROM 1-1057316-1 WHERE Operational owner(s) = 'Colorado and Southern Railway #9'"}
{"text": "table: 1-1057316-1\ncolumns: Serial number, Wheel arrangement ( Whyte notation ), Build date, Operational owner(s), Disposition\nQ: What is the wheel arrangement for the train in Riverdale, Georgia?\nA: SELECT Wheel arrangement ( Whyte notation ) FROM 1-1057316-1 WHERE Disposition = 'Riverdale, Georgia'"}
{"text": "table: 1-1057316-1\ncolumns: Serial number, Wheel arrangement ( Whyte notation ), Build date, Operational owner(s), Disposition\nQ: When was the train 2053 built?\nA: SELECT Build date FROM 1-1057316-1 WHERE Serial number = '2053'"}
{"text": "table: 1-1057316-1\ncolumns: Serial number, Wheel arrangement ( Whyte notation ), Build date, Operational owner(s), Disposition\nQ: How many wheels does the train owned by Texas and New Orleans Railroad #319 have?\nA: SELECT COUNT Wheel arrangement ( Whyte notation ) FROM 1-1057316-1 WHERE Operational owner(s) = 'Texas and New Orleans Railroad #319'"}
{"text": "table: 1-10577579-3\ncolumns: Institution, Location, Men\u2019s Nickname, Women\u2019s Nickname, Founded, Type, Enrollment, Joined, Left, Current Conference, Classification\nQ: Which college has the men's nickname of the blazers?\nA: SELECT Institution FROM 1-10577579-3 WHERE Men\u2019s Nickname = 'Blazers'"}
{"text": "table: 1-10577579-3\ncolumns: Institution, Location, Men\u2019s Nickname, Women\u2019s Nickname, Founded, Type, Enrollment, Joined, Left, Current Conference, Classification\nQ: Name the joined for the wolfpack women's nickname\nA: SELECT Joined FROM 1-10577579-3 WHERE Women\u2019s Nickname = 'Wolfpack'"}
{"text": "table: 1-10577579-3\ncolumns: Institution, Location, Men\u2019s Nickname, Women\u2019s Nickname, Founded, Type, Enrollment, Joined, Left, Current Conference, Classification\nQ: Name the left of the Lady Pilots.\nA: SELECT Left FROM 1-10577579-3 WHERE Women\u2019s Nickname = 'Lady Pilots'"}
{"text": "table: 1-10577579-3\ncolumns: Institution, Location, Men\u2019s Nickname, Women\u2019s Nickname, Founded, Type, Enrollment, Joined, Left, Current Conference, Classification\nQ: Name the women's nickname when the enrollment is 1500 in mobile, Alabama.\nA: SELECT Women\u2019s Nickname FROM 1-10577579-3 WHERE Enrollment = 1500 AND Location = 'Mobile, Alabama'"}
{"text": "table: 1-10577579-3\ncolumns: Institution, Location, Men\u2019s Nickname, Women\u2019s Nickname, Founded, Type, Enrollment, Joined, Left, Current Conference, Classification\nQ: Which conference is in Jackson, Mississippi?\nA: SELECT Current Conference FROM 1-10577579-3 WHERE Location = 'Jackson, Mississippi'"}
{"text": "table: 1-10577579-3\ncolumns: Institution, Location, Men\u2019s Nickname, Women\u2019s Nickname, Founded, Type, Enrollment, Joined, Left, Current Conference, Classification\nQ: What is the men's nickname at the school that has the lady wildcats women's nickname?\nA: SELECT Men\u2019s Nickname FROM 1-10577579-3 WHERE Women\u2019s Nickname = 'Lady Wildcats'"}
{"text": "table: 1-10577579-2\ncolumns: Institution, Location, Mens Nickname, Womens Nickname, Founded, Type, Enrollment, Joined\nQ: What is the Mens Nickname for the member location of Jacksonville, florida?\nA: SELECT Mens Nickname FROM 1-10577579-2 WHERE Location = 'Jacksonville, Florida'"}
{"text": "table: 1-10577579-2\ncolumns: Institution, Location, Mens Nickname, Womens Nickname, Founded, Type, Enrollment, Joined\nQ: What is the enrollment for the institution that was founded in 1866 and is a private/(african methodist) type?\nA: SELECT MAX Enrollment FROM 1-10577579-2 WHERE Founded = 1866 AND Type = 'Private/(African Methodist)'"}
{"text": "table: 1-10577579-2\ncolumns: Institution, Location, Mens Nickname, Womens Nickname, Founded, Type, Enrollment, Joined\nQ: That is the year founded for the institution location of Nashville, Tennessee?\nA: SELECT MIN Founded FROM 1-10577579-2 WHERE Location = 'Nashville, Tennessee'"}
{"text": "table: 1-10577579-2\ncolumns: Institution, Location, Mens Nickname, Womens Nickname, Founded, Type, Enrollment, Joined\nQ: What is the year the institution Tougaloo College joined?\nA: SELECT Joined FROM 1-10577579-2 WHERE Institution = 'Tougaloo College'"}
{"text": "table: 1-10592536-8\ncolumns: Team, Outgoing manager, Manner of departure, Date of vacancy, Replaced by, Date of appointment, Position in table\nQ: What is the date of vacancy when the date of appointment is 28 november 2007 and replaced by is alex mcleish?\nA: SELECT Date of vacancy FROM 1-10592536-8 WHERE Date of appointment = '28 November 2007' AND Replaced by = 'Alex McLeish'"}
{"text": "table: 1-10592536-8\ncolumns: Team, Outgoing manager, Manner of departure, Date of vacancy, Replaced by, Date of appointment, Position in table\nQ: What is the date of appointment when the date of vacancy is 21 december 2007?\nA: SELECT Date of appointment FROM 1-10592536-8 WHERE Date of vacancy = '21 December 2007'"}
{"text": "table: 1-10592536-8\ncolumns: Team, Outgoing manager, Manner of departure, Date of vacancy, Replaced by, Date of appointment, Position in table\nQ: Who replaced when team is wigan athletic?\nA: SELECT Replaced by FROM 1-10592536-8 WHERE Team = 'Wigan Athletic'"}
{"text": "table: 1-10592536-8\ncolumns: Team, Outgoing manager, Manner of departure, Date of vacancy, Replaced by, Date of appointment, Position in table\nQ: What is the date of vacancy when the team is manchester city and replaced by is mark hughes?\nA: SELECT Date of vacancy FROM 1-10592536-8 WHERE Team = 'Manchester City' AND Replaced by = 'Mark Hughes'"}
{"text": "table: 1-10592536-8\ncolumns: Team, Outgoing manager, Manner of departure, Date of vacancy, Replaced by, Date of appointment, Position in table\nQ: What is the date of appointment when replaced by is roy hodgson?\nA: SELECT Date of appointment FROM 1-10592536-8 WHERE Replaced by = 'Roy Hodgson'"}
{"text": "table: 1-10592536-8\ncolumns: Team, Outgoing manager, Manner of departure, Date of vacancy, Replaced by, Date of appointment, Position in table\nQ: Who replaced when position in table is pre-season?\nA: SELECT Replaced by FROM 1-10592536-8 WHERE Position in table = 'Pre-season'"}
{"text": "table: 1-1059743-1\ncolumns: Rank, Member Association, Points, Group stage, Play-off, AFC Cup\nQ: How many games had a score value of 813.5 in post-season play?\nA: SELECT COUNT Play-off FROM 1-1059743-1 WHERE Points = '813.5'"}
{"text": "table: 1-1059743-1\ncolumns: Rank, Member Association, Points, Group stage, Play-off, AFC Cup\nQ: Did any team score games that totaled up to 860.5?\nA: SELECT Play-off FROM 1-1059743-1 WHERE Points = '860.5'"}
{"text": "table: 1-10595672-1\ncolumns: Date, Opponent, Home / Away, Score, High points, High rebounds, High assists, Location/Attendance, Record\nQ: What was the score of the game when the team reached a record of 6-9?\nA: SELECT Score FROM 1-10595672-1 WHERE Record = '6-9'"}
{"text": "table: 1-10581768-2\ncolumns: Institution, Nickname, Location, Founded, Type, Enrollment\nQ: What type institution is point park university\nA: SELECT Type FROM 1-10581768-2 WHERE Institution = 'Point Park University'"}
{"text": "table: 1-10581768-2\ncolumns: Institution, Nickname, Location, Founded, Type, Enrollment\nQ: How many institutions are located in wilmore, kentucky and private\nA: SELECT MAX Founded FROM 1-10581768-2 WHERE Type = 'Private' AND Location = 'Wilmore, Kentucky'"}
{"text": "table: 1-10581768-2\ncolumns: Institution, Nickname, Location, Founded, Type, Enrollment\nQ: point park university is what type of institution\nA: SELECT Type FROM 1-10581768-2 WHERE Institution = 'Point Park University'"}
{"text": "table: 1-10581768-2\ncolumns: Institution, Nickname, Location, Founded, Type, Enrollment\nQ: how many founded dates are listed for carlow university 1\nA: SELECT COUNT Founded FROM 1-10581768-2 WHERE Institution = 'Carlow University 1'"}
{"text": "table: 1-10610087-6\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date\nQ: Who wrote the episode titled \"black\"?\nA: SELECT Written by FROM 1-10610087-6 WHERE Title = '\"Black\"'"}
{"text": "table: 1-10610087-6\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date\nQ: Who are the writers for the episode \"solo\"?\nA: SELECT Written by FROM 1-10610087-6 WHERE Title = '\"Solo\"'"}
{"text": "table: 1-10610087-3\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date\nQ: what is the original air date of the episode no in season 9?\nA: SELECT Original air date FROM 1-10610087-3 WHERE No. in season = 9"}
{"text": "table: 1-10610087-3\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date\nQ: What is the title of the episode written by denis leary, peter tolan and evan reilly?\nA: SELECT Title FROM 1-10610087-3 WHERE Written by = 'Denis Leary, Peter Tolan and Evan Reilly'"}
{"text": "table: 1-10610087-3\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date\nQ: How many episodes were titles \"voicemail\"?\nA: SELECT COUNT Directed by FROM 1-10610087-3 WHERE Title = '\"Voicemail\"'"}
{"text": "table: 1-10602294-1\ncolumns: Name, Dates active, Peak classification, Windspeeds, Pressure, Areas affected, Damage (USD), Deaths, Refs\nQ: When was Kamba active?\nA: SELECT Dates active FROM 1-10602294-1 WHERE Name = 'Kamba'"}
{"text": "table: 1-10602294-1\ncolumns: Name, Dates active, Peak classification, Windspeeds, Pressure, Areas affected, Damage (USD), Deaths, Refs\nQ: What was the cyclone's pressure in the storm that death was equal to 95km/h (60mph)?\nA: SELECT Pressure FROM 1-10602294-1 WHERE Deaths = '95km/h (60mph)'"}
{"text": "table: 1-10602294-1\ncolumns: Name, Dates active, Peak classification, Windspeeds, Pressure, Areas affected, Damage (USD), Deaths, Refs\nQ: What were the active dates for the storm that had 185km/h (115mph) deaths?\nA: SELECT Dates active FROM 1-10602294-1 WHERE Deaths = '185km/h (115mph)'"}
{"text": "table: 1-10602294-1\ncolumns: Name, Dates active, Peak classification, Windspeeds, Pressure, Areas affected, Damage (USD), Deaths, Refs\nQ: What was the damage (usd) from the cyclones that measured 1003hPa (29.62inHg) pressure?\nA: SELECT Damage (USD) FROM 1-10602294-1 WHERE Pressure = '1003hPa (29.62inHg)'"}
{"text": "table: 1-10621256-1\ncolumns: Player, Matches, Inns, N/O, Runs, High Score, Average, 100, 50, Catches, Stump\nQ:  what's the\u00a0average\u00a0where\u00a0high score\u00a0is 120\nA: SELECT Average FROM 1-10621256-1 WHERE High Score = 120"}
{"text": "table: 1-10621256-1\ncolumns: Player, Matches, Inns, N/O, Runs, High Score, Average, 100, 50, Catches, Stump\nQ:  what's the\u00a0player\u00a0where\u00a050\u00a0is 2 and\u00a0n/o\u00a0is 0\nA: SELECT Player FROM 1-10621256-1 WHERE 50 = 2 AND N/O = 0"}
{"text": "table: 1-10621256-1\ncolumns: Player, Matches, Inns, N/O, Runs, High Score, Average, 100, 50, Catches, Stump\nQ:  what's the\u00a0player\u00a0where\u00a0inns\u00a0is 21\nA: SELECT Player FROM 1-10621256-1 WHERE Inns = 21"}
{"text": "table: 1-106367-2\ncolumns: General election, # of candidates, # of seats won, % of popular vote, Result\nQ: Which general election had a pq majority and a 44.75% of the popular vote?\nA: SELECT General election FROM 1-106367-2 WHERE Result = 'PQ majority' AND % of popular vote = '44.75%'"}
{"text": "table: 1-106367-2\ncolumns: General election, # of candidates, # of seats won, % of popular vote, Result\nQ: What is the least number of candidates running were there when 80 seats were won?\nA: SELECT MIN # of candidates FROM 1-106367-2 WHERE # of seats won = 80"}
{"text": "table: 1-106367-2\ncolumns: General election, # of candidates, # of seats won, % of popular vote, Result\nQ: How many seats were won in the election with 125 candidates?\nA: SELECT COUNT # of seats won FROM 1-106367-2 WHERE # of candidates = 125"}
{"text": "table: 1-10647639-1\ncolumns: Week, Date, Opponent, Result, Game site, Record, Attendance\nQ: How many weeks are there?\nA: SELECT MAX Week FROM 1-10647639-1"}
{"text": "table: 1-10647639-1\ncolumns: Week, Date, Opponent, Result, Game site, Record, Attendance\nQ: How many people attended the game against the indianapolis colts?\nA: SELECT COUNT Attendance FROM 1-10647639-1 WHERE Opponent = 'Indianapolis Colts'"}
{"text": "table: 1-10647639-1\ncolumns: Week, Date, Opponent, Result, Game site, Record, Attendance\nQ: On december 16, 1985, all the records were what?\nA: SELECT Record FROM 1-10647639-1 WHERE Date = 'December 16, 1985'"}
{"text": "table: 1-10646790-2\ncolumns: Week, Date, Opponent, Result, Stadium, Record, Attendance\nQ: How many results are there for the 0-4 record?\nA: SELECT COUNT Result FROM 1-10646790-2 WHERE Record = '0-4'"}
{"text": "table: 1-10646790-2\ncolumns: Week, Date, Opponent, Result, Stadium, Record, Attendance\nQ: How many weeks are there that include the date October 11, 1969.\nA: SELECT COUNT Week FROM 1-10646790-2 WHERE Date = 'October 11, 1969'"}
{"text": "table: 1-10646790-2\ncolumns: Week, Date, Opponent, Result, Stadium, Record, Attendance\nQ: How many weeks are there that include the date November 9, 1969.\nA: SELECT COUNT Week FROM 1-10646790-2 WHERE Date = 'November 9, 1969'"}
{"text": "table: 1-10646790-2\ncolumns: Week, Date, Opponent, Result, Stadium, Record, Attendance\nQ: How many records are there at the War Memorial Stadium?\nA: SELECT COUNT Record FROM 1-10646790-2 WHERE Stadium = 'War Memorial Stadium'"}
{"text": "table: 1-10646790-2\ncolumns: Week, Date, Opponent, Result, Stadium, Record, Attendance\nQ: What was the minimum attendance on December 7, 1969?\nA: SELECT MIN Attendance FROM 1-10646790-2 WHERE Date = 'December 7, 1969'"}
{"text": "table: 1-10647401-1\ncolumns: Week, Opponent, Result, Stadium, Record, Attendance\nQ: What week corresponds to the last one to be played at the memorial stadium?\nA: SELECT MAX Week FROM 1-10647401-1 WHERE Stadium = 'Memorial Stadium'"}
{"text": "table: 1-10647401-1\ncolumns: Week, Opponent, Result, Stadium, Record, Attendance\nQ: In which stadium is the week 5 game played?\nA: SELECT Stadium FROM 1-10647401-1 WHERE Week = 5"}
{"text": "table: 1-10664957-2\ncolumns: 1st players choice, 2nd players choice, Probability 1st player wins, Probability 2nd player wins, Probability of a draw\nQ: In Penney's game what is the probability where the 1st player wins if the probability of a draw is 8.28% and the 2nd player chooses B BR?\nA: SELECT Probability 1st player wins FROM 1-10664957-2 WHERE Probability of a draw = '8.28%' AND 2nd players choice = 'B BR'"}
{"text": "table: 1-10664957-2\ncolumns: 1st players choice, 2nd players choice, Probability 1st player wins, Probability 2nd player wins, Probability of a draw\nQ: If the first player chooses RB B, what is the second player's choices?\nA: SELECT 2nd players choice FROM 1-10664957-2 WHERE 1st players choice = 'RB B'"}
{"text": "table: 1-10664957-2\ncolumns: 1st players choice, 2nd players choice, Probability 1st player wins, Probability 2nd player wins, Probability of a draw\nQ: What is the probability where the second player wins where their choice is R RB and the first player has a 5.18% chance of winning?\nA: SELECT Probability 2nd player wins FROM 1-10664957-2 WHERE 2nd players choice = 'R RB' AND Probability 1st player wins = '5.18%'"}
{"text": "table: 1-10664957-2\ncolumns: 1st players choice, 2nd players choice, Probability 1st player wins, Probability 2nd player wins, Probability of a draw\nQ: What are the chances the first player will win if the 2nd player has an 80.11% chance of winning with the choice of R RB?\nA: SELECT Probability 1st player wins FROM 1-10664957-2 WHERE Probability 2nd player wins = '80.11%' AND 2nd players choice = 'R RB'"}
{"text": "table: 1-10664957-2\ncolumns: 1st players choice, 2nd players choice, Probability 1st player wins, Probability 2nd player wins, Probability of a draw\nQ: What are the chances that player 2 wins if player 1's choice is BB R?\nA: SELECT Probability 2nd player wins FROM 1-10664957-2 WHERE 1st players choice = 'BB R'"}
{"text": "table: 1-10664957-2\ncolumns: 1st players choice, 2nd players choice, Probability 1st player wins, Probability 2nd player wins, Probability of a draw\nQ: How high is the chance that player 1 wins if player 2 has an 88.29% chance of winning with the choice of R RB?\nA: SELECT Probability 1st player wins FROM 1-10664957-2 WHERE Probability 2nd player wins = '88.29%' AND 2nd players choice = 'R RB'"}
{"text": "table: 1-10650711-1\ncolumns: Pick #, NFL Team, Player, Position, College\nQ:  what is the\u00a0nfl team\u00a0where\u00a0player\u00a0is thane gash\nA: SELECT NFL Team FROM 1-10650711-1 WHERE Player = 'Thane Gash'"}
{"text": "table: 1-10650711-1\ncolumns: Pick #, NFL Team, Player, Position, College\nQ: what is the maximum\u00a0pick #\u00a0where\u00a0player\u00a0is anthony blaylock\nA: SELECT MAX Pick # FROM 1-10650711-1 WHERE Player = 'Anthony Blaylock'"}
{"text": "table: 1-10650711-1\ncolumns: Pick #, NFL Team, Player, Position, College\nQ:  what's the\u00a0nfl team\u00a0where\u00a0player\u00a0is clifford charlton\nA: SELECT NFL Team FROM 1-10650711-1 WHERE Player = 'Clifford Charlton'"}
{"text": "table: 1-10650711-1\ncolumns: Pick #, NFL Team, Player, Position, College\nQ:  what's the\u00a0position\u00a0where\u00a0player\u00a0is anthony blaylock\nA: SELECT Position FROM 1-10650711-1 WHERE Player = 'Anthony Blaylock'"}
{"text": "table: 1-10650711-1\ncolumns: Pick #, NFL Team, Player, Position, College\nQ: what is the minimum\u00a0pick #\u00a0where\u00a0position\u00a0is defensive tackle\nA: SELECT MIN Pick # FROM 1-10650711-1 WHERE Position = 'Defensive Tackle'"}
{"text": "table: 1-1067441-1\ncolumns: Province, Population (2004 estimate), Area (km\u00b2), Density, GDP (2003, PPS in mil. \u20ac ), GDP per cap. (2003, in \u20ac)\nQ: Which province has a density of 971.4?\nA: SELECT Province FROM 1-1067441-1 WHERE Density = '971.4'"}
{"text": "table: 1-1067441-1\ncolumns: Province, Population (2004 estimate), Area (km\u00b2), Density, GDP (2003, PPS in mil. \u20ac ), GDP per cap. (2003, in \u20ac)\nQ: What is Friesland's gdp per capita?\nA: SELECT MIN GDP per cap. (2003, in \u20ac) FROM 1-1067441-1 WHERE Province = 'Friesland'"}
{"text": "table: 1-1067441-1\ncolumns: Province, Population (2004 estimate), Area (km\u00b2), Density, GDP (2003, PPS in mil. \u20ac ), GDP per cap. (2003, in \u20ac)\nQ: What is the area of the place that has a population density of 331.4?\nA: SELECT MAX Area (km\u00b2) FROM 1-1067441-1 WHERE Density = '331.4'"}
{"text": "table: 1-1067441-1\ncolumns: Province, Population (2004 estimate), Area (km\u00b2), Density, GDP (2003, PPS in mil. \u20ac ), GDP per cap. (2003, in \u20ac)\nQ: Which province has a gdp of 38355\u20ac  million euros?\nA: SELECT Province FROM 1-1067441-1 WHERE GDP (2003, PPS in mil. \u20ac ) = 38355"}
{"text": "table: 1-1067441-1\ncolumns: Province, Population (2004 estimate), Area (km\u00b2), Density, GDP (2003, PPS in mil. \u20ac ), GDP per cap. (2003, in \u20ac)\nQ: What is the population estimate for the place that gad a 18496\u20ac  million euro gdp?\nA: SELECT Population (2004 estimate) FROM 1-1067441-1 WHERE GDP (2003, PPS in mil. \u20ac ) = 18496"}
{"text": "table: 1-10701133-1\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Featured character(s), Original air date, U.S. viewers (million)\nQ: What is the title when original air date is may15,2008?\nA: SELECT Title FROM 1-10701133-1 WHERE Original air date = 'May15,2008'"}
{"text": "table: 1-10701133-1\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Featured character(s), Original air date, U.S. viewers (million)\nQ: What is the highest no. in season?\nA: SELECT MAX No. in season FROM 1-10701133-1"}
{"text": "table: 1-10701133-1\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Featured character(s), Original air date, U.S. viewers (million)\nQ: Who directed the episode where u.s. viewers (million) is 12.90?\nA: SELECT Directed by FROM 1-10701133-1 WHERE U.S. viewers (million) = '12.90'"}
{"text": "table: 1-1067134-1\ncolumns: DVD Name, # of Ep, Region 1, Region 2, Region 4\nQ: How many episodes aired in Region 2 beginning May 26, 2008?\nA: SELECT MIN # of Ep FROM 1-1067134-1 WHERE Region 2 = 'May 26, 2008'"}
{"text": "table: 1-1067134-1\ncolumns: DVD Name, # of Ep, Region 1, Region 2, Region 4\nQ: What date did the DVD for season six come out in region 2?\nA: SELECT Region 2 FROM 1-1067134-1 WHERE DVD Name = 'Season Six'"}
{"text": "table: 1-1067134-1\ncolumns: DVD Name, # of Ep, Region 1, Region 2, Region 4\nQ: What is the least amount of season epidsodes?\nA: SELECT MIN # of Ep FROM 1-1067134-1"}
{"text": "table: 1-1067134-1\ncolumns: DVD Name, # of Ep, Region 1, Region 2, Region 4\nQ: What DVD season/name for region 2 was released August 22, 2010?\nA: SELECT DVD Name FROM 1-1067134-1 WHERE Region 2 = 'August 22, 2010'"}
{"text": "table: 1-10705060-1\ncolumns: Season, Series, Team Name, Races, Poles, Wins, Points, Position\nQ: How many points for 2005?\nA: SELECT COUNT Points FROM 1-10705060-1 WHERE Season = '2005'"}
{"text": "table: 1-10705060-1\ncolumns: Season, Series, Team Name, Races, Poles, Wins, Points, Position\nQ: what is the score for the dams?\nA: SELECT Points FROM 1-10705060-1 WHERE Team Name = 'DAMS'"}
{"text": "table: 1-10705060-1\ncolumns: Season, Series, Team Name, Races, Poles, Wins, Points, Position\nQ: how many positions in 2009?\nA: SELECT COUNT Position FROM 1-10705060-1 WHERE Season = '2009'"}
{"text": "table: 1-10705060-1\ncolumns: Season, Series, Team Name, Races, Poles, Wins, Points, Position\nQ: what is the least number of poles?\nA: SELECT MIN Poles FROM 1-10705060-1"}
{"text": "table: 1-10705060-1\ncolumns: Season, Series, Team Name, Races, Poles, Wins, Points, Position\nQ: Which series with 62 points?\nA: SELECT Series FROM 1-10705060-1 WHERE Points = 62"}
{"text": "table: 1-10705060-1\ncolumns: Season, Series, Team Name, Races, Poles, Wins, Points, Position\nQ: What is the total for 10th position?\nA: SELECT COUNT Points FROM 1-10705060-1 WHERE Position = '10th'"}
{"text": "table: 1-10707142-2\ncolumns: Rnd, Race Name, Circuit, City/Location, Date, Pole position, Winning driver, Winning team, Report\nQ: how many reports of races took place on october 16?\nA: SELECT COUNT Report FROM 1-10707142-2 WHERE Date = 'October 16'"}
{"text": "table: 1-10707142-2\ncolumns: Rnd, Race Name, Circuit, City/Location, Date, Pole position, Winning driver, Winning team, Report\nQ: what is the name of the report that lists the race name as long beach grand prix?\nA: SELECT Report FROM 1-10707142-2 WHERE Race Name = 'Long Beach Grand Prix'"}
{"text": "table: 1-10707142-2\ncolumns: Rnd, Race Name, Circuit, City/Location, Date, Pole position, Winning driver, Winning team, Report\nQ: what is the report called where the circuit took place at the nazareth speedway?\nA: SELECT Report FROM 1-10707142-2 WHERE Circuit = 'Nazareth Speedway'"}
{"text": "table: 1-10707142-2\ncolumns: Rnd, Race Name, Circuit, City/Location, Date, Pole position, Winning driver, Winning team, Report\nQ: what is the name of the race where newman/haas racing is the winning team and rick mears is at the pole position?\nA: SELECT Race Name FROM 1-10707142-2 WHERE Winning team = 'Newman/Haas Racing' AND Pole position = 'Rick Mears'"}
{"text": "table: 1-10707142-2\ncolumns: Rnd, Race Name, Circuit, City/Location, Date, Pole position, Winning driver, Winning team, Report\nQ: meadowlands sports complex is the circuit at which city/location?\nA: SELECT City/Location FROM 1-10707142-2 WHERE Circuit = 'Meadowlands Sports Complex'"}
{"text": "table: 1-10710364-2\ncolumns: Religious group, Population %, Growth (1991\u20132001), Sex ratio (total), Literacy (%), Work participation (%), Sex ratio (rural), Sex ratio (urban), Sex ratio (child)\nQ: What is the literacy rate for groups that grew 103.1% between 1991 and 2001?\nA: SELECT Literacy (%) FROM 1-10710364-2 WHERE Growth (1991\u20132001) = '103.1%'"}
{"text": "table: 1-10710364-2\ncolumns: Religious group, Population %, Growth (1991\u20132001), Sex ratio (total), Literacy (%), Work participation (%), Sex ratio (rural), Sex ratio (urban), Sex ratio (child)\nQ: What is the lowest sex ratio in rural areas?\nA: SELECT MIN Sex ratio (rural) FROM 1-10710364-2"}
{"text": "table: 1-10710364-2\ncolumns: Religious group, Population %, Growth (1991\u20132001), Sex ratio (total), Literacy (%), Work participation (%), Sex ratio (rural), Sex ratio (urban), Sex ratio (child)\nQ: What is the lowest child sex ratio in groups where employment is 31.3%?\nA: SELECT MIN Sex ratio (child) FROM 1-10710364-2 WHERE Work participation (%) = '31.3%'"}
{"text": "table: 1-10710364-2\ncolumns: Religious group, Population %, Growth (1991\u20132001), Sex ratio (total), Literacy (%), Work participation (%), Sex ratio (rural), Sex ratio (urban), Sex ratio (child)\nQ: What is the population percentage of the group where the rural sex ratio is 953?\nA: SELECT Population % FROM 1-10710364-2 WHERE Sex ratio (rural) = 953"}
{"text": "table: 1-10715317-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: What is the original air date for the title \"felonious monk\"?\nA: SELECT Original air date FROM 1-10715317-2 WHERE Title = '\"Felonious Monk\"'"}
{"text": "table: 1-10715317-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: What is the title of the episode directed by Peter Markle and written by Jerry Stahl?\nA: SELECT Title FROM 1-10715317-2 WHERE Directed by = 'Peter Markle' AND Written by = 'Jerry Stahl'"}
{"text": "table: 1-10715317-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: How many episodes were titled \"identity crisis\"?\nA: SELECT COUNT Directed by FROM 1-10715317-2 WHERE Title = '\"Identity Crisis\"'"}
{"text": "table: 1-10716893-3\ncolumns: Year, Network, Host, Pre-race analyst, Lap-by-lap, Color commentator(s), Pit reporters\nQ: Who does the lap-by-lap in 2011?\nA: SELECT Lap-by-lap FROM 1-10716893-3 WHERE Year = 2011"}
{"text": "table: 1-10716893-3\ncolumns: Year, Network, Host, Pre-race analyst, Lap-by-lap, Color commentator(s), Pit reporters\nQ: Which network has Marty Reid as host and lap-by-lap broadcaster?\nA: SELECT Network FROM 1-10716893-3 WHERE Lap-by-lap = 'Marty Reid' AND Host = 'Marty Reid'"}
{"text": "table: 1-10716893-3\ncolumns: Year, Network, Host, Pre-race analyst, Lap-by-lap, Color commentator(s), Pit reporters\nQ: How many pre-race analysis occur when Allen Bestwick does the lap-by-lap?\nA: SELECT COUNT Pre-race analyst FROM 1-10716893-3 WHERE Lap-by-lap = 'Allen Bestwick'"}
{"text": "table: 1-10718192-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: What's the highest season number of an episode in the series?\nA: SELECT MAX No. in season FROM 1-10718192-2"}
{"text": "table: 1-10718192-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: When did the episode titled \"Lucky Strike\" air for the first time?\nA: SELECT Original air date FROM 1-10718192-2 WHERE Title = '\"Lucky Strike\"'"}
{"text": "table: 1-10718192-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: Who was the writer of the episode titled \"One Hit Wonder\"?\nA: SELECT Written by FROM 1-10718192-2 WHERE Title = '\"One Hit Wonder\"'"}
{"text": "table: 1-10718192-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: What's the date of the first airing of the episode with series number 63?\nA: SELECT Original air date FROM 1-10718192-2 WHERE No. in series = 63"}
{"text": "table: 1-10718525-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: How many titles got a viewership of 26.53 million?\nA: SELECT COUNT Title FROM 1-10718525-2 WHERE U.S. viewers (millions) = '26.53'"}
{"text": "table: 1-10718525-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: How many viewers tuned into the show directed by Matt Earl Beesley?\nA: SELECT U.S. viewers (millions) FROM 1-10718525-2 WHERE Directed by = 'Matt Earl Beesley'"}
{"text": "table: 1-10718631-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: Who wrote episode 94?\nA: SELECT Written by FROM 1-10718631-2 WHERE No. in series = 94"}
{"text": "table: 1-10718631-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: Which episode was the number in the season where the number in the season is 10?\nA: SELECT No. in series FROM 1-10718631-2 WHERE No. in season = 10"}
{"text": "table: 1-10718631-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: How many episodes were in the season that had the epiosde of \"crow's feet\"?\nA: SELECT No. in season FROM 1-10718631-2 WHERE Title = '\"Crow's Feet\"'"}
{"text": "table: 1-10718631-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: When did the 113 episode air?\nA: SELECT Original air date FROM 1-10718631-2 WHERE No. in series = 113"}
{"text": "table: 1-10718631-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: How many titles were there for the 113 episode?\nA: SELECT COUNT Title FROM 1-10718631-2 WHERE No. in series = 113"}
{"text": "table: 1-10718984-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: What is the number in the season that Marlene Meyer wrote and 20.49 million people watched?\nA: SELECT MAX No. in season FROM 1-10718984-2 WHERE Written by = 'Marlene Meyer' AND U.S. viewers (millions) = '20.49'"}
{"text": "table: 1-10718984-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: When did the no. 23 show originally air?\nA: SELECT Original air date FROM 1-10718984-2 WHERE No. in season = 23"}
{"text": "table: 1-10725629-2\ncolumns: Rnd, Race Name, Circuit, City/Location, Date, Pole position, Fastest lap, Winning driver, Winning team, Report\nQ: Which circuits had a race on October 4?\nA: SELECT Circuit FROM 1-10725629-2 WHERE Date = 'October 4'"}
{"text": "table: 1-10725629-2\ncolumns: Rnd, Race Name, Circuit, City/Location, Date, Pole position, Fastest lap, Winning driver, Winning team, Report\nQ: In which reports does Michael Andretti have the pole position and Galles-Kraco Racing is the winning team?\nA: SELECT Report FROM 1-10725629-2 WHERE Pole position = 'Michael Andretti' AND Winning team = 'Galles-Kraco Racing'"}
{"text": "table: 1-10725629-2\ncolumns: Rnd, Race Name, Circuit, City/Location, Date, Pole position, Fastest lap, Winning driver, Winning team, Report\nQ: How many rounds were there of the Bosch Spark Plug Grand Prix?\nA: SELECT COUNT Rnd FROM 1-10725629-2 WHERE Race Name = 'Bosch Spark Plug Grand Prix'"}
{"text": "table: 1-10725629-2\ncolumns: Rnd, Race Name, Circuit, City/Location, Date, Pole position, Fastest lap, Winning driver, Winning team, Report\nQ: Which rounds were held on August 9?\nA: SELECT Rnd FROM 1-10725629-2 WHERE Date = 'August 9'"}
{"text": "table: 1-10725629-2\ncolumns: Rnd, Race Name, Circuit, City/Location, Date, Pole position, Fastest lap, Winning driver, Winning team, Report\nQ: On how many dates did the Michigan International Speedway hold a round?\nA: SELECT COUNT Date FROM 1-10725629-2 WHERE Circuit = 'Michigan International Speedway'"}
{"text": "table: 1-10722506-6\ncolumns: Conference, # of Bids, Record, Win %, Round of 32, Sweet Sixteen, Elite Eight, Final Four, Championship Game\nQ: Name the total number of bids of the sun belt conference\nA: SELECT COUNT # of Bids FROM 1-10722506-6 WHERE Conference = 'Sun Belt'"}
{"text": "table: 1-10722506-6\ncolumns: Conference, # of Bids, Record, Win %, Round of 32, Sweet Sixteen, Elite Eight, Final Four, Championship Game\nQ: Name the round of 32 in conference usa\nA: SELECT Round of 32 FROM 1-10722506-6 WHERE Conference = 'Conference USA'"}
{"text": "table: 1-10722506-6\ncolumns: Conference, # of Bids, Record, Win %, Round of 32, Sweet Sixteen, Elite Eight, Final Four, Championship Game\nQ: What is the record when round of 32 is 0 and metro atlantic conference?\nA: SELECT Record FROM 1-10722506-6 WHERE Round of 32 = 0 AND Conference = 'Metro Atlantic'"}
{"text": "table: 1-10722506-6\ncolumns: Conference, # of Bids, Record, Win %, Round of 32, Sweet Sixteen, Elite Eight, Final Four, Championship Game\nQ: What is the number of bids with elite eight larger than 1.0\nA: SELECT COUNT # of Bids FROM 1-10722506-6 WHERE Elite Eight > 1.0"}
{"text": "table: 1-10749143-2\ncolumns: Series #, Season #, Title, Directed by, Written by, Original air date, Production code, U.S. viewers (millions)\nQ: Who directed the episode with production code 7aff03?\nA: SELECT Directed by FROM 1-10749143-2 WHERE Production code = '7AFF03'"}
{"text": "table: 1-10749143-2\ncolumns: Series #, Season #, Title, Directed by, Written by, Original air date, Production code, U.S. viewers (millions)\nQ: What is the title of the episode wtih 10.34 million U.S viewers?\nA: SELECT Title FROM 1-10749143-2 WHERE U.S. viewers (millions) = '10.34'"}
{"text": "table: 1-10748727-1\ncolumns: Season, Series, Team Name, Races, Wins, Poles, F/Laps, Podiums, Points, Position\nQ: What place is the team that completed 6 races?\nA: SELECT Position FROM 1-10748727-1 WHERE Races = 6"}
{"text": "table: 1-10748727-1\ncolumns: Season, Series, Team Name, Races, Wins, Poles, F/Laps, Podiums, Points, Position\nQ: how much did the british formula three called \"fortec motorsport\" score?\nA: SELECT Points FROM 1-10748727-1 WHERE Series = 'British Formula Three' AND Team Name = 'Fortec Motorsport'"}
{"text": "table: 1-10748727-1\ncolumns: Season, Series, Team Name, Races, Wins, Poles, F/Laps, Podiums, Points, Position\nQ: how many races were in 2009 with 0 wins?\nA: SELECT Races FROM 1-10748727-1 WHERE Season = '2009' AND Wins = 0"}
{"text": "table: 1-10748727-1\ncolumns: Season, Series, Team Name, Races, Wins, Poles, F/Laps, Podiums, Points, Position\nQ: What years did art grand prix compete?\nA: SELECT Season FROM 1-10748727-1 WHERE Team Name = 'ART Grand Prix'"}
{"text": "table: 1-10748727-1\ncolumns: Season, Series, Team Name, Races, Wins, Poles, F/Laps, Podiums, Points, Position\nQ: What year had a score of 9?\nA: SELECT Season FROM 1-10748727-1 WHERE Points = '9'"}
{"text": "table: 1-10748727-1\ncolumns: Season, Series, Team Name, Races, Wins, Poles, F/Laps, Podiums, Points, Position\nQ: what is the greatest number of wins by japanese formula three?\nA: SELECT MAX Wins FROM 1-10748727-1 WHERE Series = 'Japanese Formula Three'"}
{"text": "table: 1-10749367-3\ncolumns: #, Air Date, Challenge, Winner, Test-taker, Passed?\nQ: Who took test #4?\nA: SELECT Test-taker FROM 1-10749367-3 WHERE # = 4"}
{"text": "table: 1-10749367-3\ncolumns: #, Air Date, Challenge, Winner, Test-taker, Passed?\nQ: What episode aired on 18 April 2007?\nA: SELECT MIN # FROM 1-10749367-3 WHERE Air Date = '18 April 2007'"}
{"text": "table: 1-10749367-3\ncolumns: #, Air Date, Challenge, Winner, Test-taker, Passed?\nQ: Who had the challenge of night driving?\nA: SELECT Test-taker FROM 1-10749367-3 WHERE Challenge = 'Night Driving'"}
{"text": "table: 1-10798928-1\ncolumns: Year (Ceremony), Film title used in nomination, Original title, Director, Result\nQ: How many directors were there for the film Course Completed?\nA: SELECT COUNT Director FROM 1-10798928-1 WHERE Film title used in nomination = 'Course Completed'"}
{"text": "table: 1-10798928-1\ncolumns: Year (Ceremony), Film title used in nomination, Original title, Director, Result\nQ: Who directed El Nido?\nA: SELECT Director FROM 1-10798928-1 WHERE Original title = 'El nido'"}
{"text": "table: 1-10798928-1\ncolumns: Year (Ceremony), Film title used in nomination, Original title, Director, Result\nQ: Who directed Dulcinea?\nA: SELECT Director FROM 1-10798928-1 WHERE Original title = 'Dulcinea'"}
{"text": "table: 1-10797463-1\ncolumns: Village (German), Village (Slovenian), Number of people 1991, Percent of Slovenes 1991, Percent of Slovenes 1951\nQ: What are the slovenian names of the villages that had 65.9% of slovenes in 1951?\nA: SELECT Village (Slovenian) FROM 1-10797463-1 WHERE Percent of Slovenes 1951 = '65.9%'"}
{"text": "table: 1-10797463-1\ncolumns: Village (German), Village (Slovenian), Number of people 1991, Percent of Slovenes 1991, Percent of Slovenes 1951\nQ: What are the slovenian names of the villages that had 16.7% of slovenes in 1991?\nA: SELECT Village (Slovenian) FROM 1-10797463-1 WHERE Percent of Slovenes 1991 = '16.7%'"}
{"text": "table: 1-10797463-1\ncolumns: Village (German), Village (Slovenian), Number of people 1991, Percent of Slovenes 1991, Percent of Slovenes 1951\nQ: How many villages had 21.7% of slovenes in 1991?\nA: SELECT COUNT Village (German) FROM 1-10797463-1 WHERE Percent of Slovenes 1991 = '21.7%'"}
{"text": "table: 1-10797463-1\ncolumns: Village (German), Village (Slovenian), Number of people 1991, Percent of Slovenes 1991, Percent of Slovenes 1951\nQ: what percent of slovenes did the village called \u010dahor\u010de in slovenian have in 1991?\nA: SELECT Percent of Slovenes 1991 FROM 1-10797463-1 WHERE Village (Slovenian) = '\u010cahor\u010de'"}
{"text": "table: 1-10797463-1\ncolumns: Village (German), Village (Slovenian), Number of people 1991, Percent of Slovenes 1991, Percent of Slovenes 1951\nQ: What is the slovenian name for the village that in german is known as st.margarethen?\nA: SELECT Village (Slovenian) FROM 1-10797463-1 WHERE Village (German) = 'St.Margarethen'"}
{"text": "table: 1-10812293-4\ncolumns: Game, Date, Team, Score, High points, High rebounds, High assists, Location Attendance, Record\nQ: For games on December 20, how many points did the scoring leaders get?\nA: SELECT High points FROM 1-10812293-4 WHERE Date = 'December 20'"}
{"text": "table: 1-10812293-4\ncolumns: Game, Date, Team, Score, High points, High rebounds, High assists, Location Attendance, Record\nQ: Who was the scoring leader and how many points did he get in games on December 23?\nA: SELECT High points FROM 1-10812293-4 WHERE Date = 'December 23'"}
{"text": "table: 1-10812938-3\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What is the pick # for the position de?\nA: SELECT Pick # FROM 1-10812938-3 WHERE Position = 'DE'"}
{"text": "table: 1-10812938-3\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: Which player went to college at Saint Mary's?\nA: SELECT Player FROM 1-10812938-3 WHERE College = 'Saint Mary's'"}
{"text": "table: 1-10812938-3\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What is the position for the player with cfl team Winnipeg blue bombers?\nA: SELECT Position FROM 1-10812938-3 WHERE CFL Team = 'Winnipeg Blue Bombers'"}
{"text": "table: 1-10812938-3\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: Which player went to college at Laval?\nA: SELECT Player FROM 1-10812938-3 WHERE College = 'Laval'"}
{"text": "table: 1-10812938-3\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What was the college for the player with the cfl team of Edmonton Eskimos (via calgary)?\nA: SELECT College FROM 1-10812938-3 WHERE CFL Team = 'Edmonton Eskimos (via Calgary)'"}
{"text": "table: 1-10812938-5\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What's the team of the player from St. Francis Xavier College?\nA: SELECT CFL Team FROM 1-10812938-5 WHERE College = 'St. Francis Xavier'"}
{"text": "table: 1-10812938-5\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What player is on the Montreal Alouettes CFl team?\nA: SELECT Player FROM 1-10812938-5 WHERE CFL Team = 'Montreal Alouettes'"}
{"text": "table: 1-10812938-5\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What's the pick number of the player from Toronto Argonauts?\nA: SELECT MIN Pick # FROM 1-10812938-5 WHERE CFL Team = 'Toronto Argonauts'"}
{"text": "table: 1-10812938-5\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What's the pick number of the player whose position is CB?\nA: SELECT Pick # FROM 1-10812938-5 WHERE Position = 'CB'"}
{"text": "table: 1-10812938-5\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What's the pick number of the player from New Mexico?\nA: SELECT MAX Pick # FROM 1-10812938-5 WHERE College = 'New Mexico'"}
{"text": "table: 1-10812938-5\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What player went to Ohio State College?\nA: SELECT Player FROM 1-10812938-5 WHERE College = 'Ohio State'"}
{"text": "table: 1-1081459-1\ncolumns: Number Range, Builder, Introduced, No. Built, Region, Withdrawn\nQ: What is the minimum introduced value for the Departmental region?\nA: SELECT MIN Introduced FROM 1-1081459-1 WHERE Region = 'Departmental'"}
{"text": "table: 1-1081459-1\ncolumns: Number Range, Builder, Introduced, No. Built, Region, Withdrawn\nQ: What is the smallest introduced value?\nA: SELECT MIN Introduced FROM 1-1081459-1"}
{"text": "table: 1-10812938-4\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: Which CFL Teams drafted an OL in 2006?\nA: SELECT CFL Team FROM 1-10812938-4 WHERE Position = 'OL'"}
{"text": "table: 1-10812938-4\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: Which college is aligned to the Saskatchewan Roughriders?\nA: SELECT College FROM 1-10812938-4 WHERE CFL Team = 'Saskatchewan Roughriders'"}
{"text": "table: 1-10812938-4\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What Position did the Hamilton Tiger-Cats (via Ottawa) pick in the 2006 Draft.\nA: SELECT Position FROM 1-10812938-4 WHERE CFL Team = 'Hamilton Tiger-Cats (via Ottawa)'"}
{"text": "table: 1-10812938-4\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What is the earliest pick listed in the table.\nA: SELECT MIN Pick # FROM 1-10812938-4"}
{"text": "table: 1-10842344-1\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, Production code, U.S. viewers (millions)\nQ: What episode number had production code e4423?\nA: SELECT MAX No. in season FROM 1-10842344-1 WHERE Production code = 'E4423'"}
{"text": "table: 1-10842344-1\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, Production code, U.S. viewers (millions)\nQ: What's the latest episode in a season where the U.S. viewers totaled 14.37 million?\nA: SELECT MAX No. in season FROM 1-10842344-1 WHERE U.S. viewers (millions) = '14.37'"}
{"text": "table: 1-10842344-1\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, Production code, U.S. viewers (millions)\nQ: Who directed the episode \"Escape\"?\nA: SELECT Directed by FROM 1-10842344-1 WHERE Title = '\"Escape\"'"}
{"text": "table: 1-10819266-8\ncolumns: Season, Episodes, Time slot (EST), Season premiere, Season finale, TV season, Rank, Viewers (in millions)\nQ: How many seasons is the season finale on May 26, 2010?\nA: SELECT COUNT Season FROM 1-10819266-8 WHERE Season finale = 'May 26, 2010'"}
{"text": "table: 1-10819266-8\ncolumns: Season, Episodes, Time slot (EST), Season premiere, Season finale, TV season, Rank, Viewers (in millions)\nQ: What episodes are there where the season premier is September 23, 2009?\nA: SELECT Episodes FROM 1-10819266-8 WHERE Season premiere = 'September 23, 2009'"}
{"text": "table: 1-10819266-8\ncolumns: Season, Episodes, Time slot (EST), Season premiere, Season finale, TV season, Rank, Viewers (in millions)\nQ: What is the season finale for season 4?\nA: SELECT Season finale FROM 1-10819266-8 WHERE Season = 4"}
{"text": "table: 1-10819266-8\ncolumns: Season, Episodes, Time slot (EST), Season premiere, Season finale, TV season, Rank, Viewers (in millions)\nQ: How many season premiers have a rank of #21?\nA: SELECT Season premiere FROM 1-10819266-8 WHERE Rank = '#21'"}
{"text": "table: 1-10819266-8\ncolumns: Season, Episodes, Time slot (EST), Season premiere, Season finale, TV season, Rank, Viewers (in millions)\nQ: What are the seasons where September 26, 2007 is the season premier?\nA: SELECT TV season FROM 1-10819266-8 WHERE Season premiere = 'September 26, 2007'"}
{"text": "table: 1-10818465-1\ncolumns: Model, RU, Max processors, Processor frequency, Max memory, Max disk capacity, GA Date\nQ: What max processor has a maximum memory of 256 gb?\nA: SELECT Max processors FROM 1-10818465-1 WHERE Max memory = '256 GB'"}
{"text": "table: 1-10818465-1\ncolumns: Model, RU, Max processors, Processor frequency, Max memory, Max disk capacity, GA Date\nQ: What is the max memory of the t5120 model?\nA: SELECT Max memory FROM 1-10818465-1 WHERE Model = 'T5120'"}
{"text": "table: 1-10818465-1\ncolumns: Model, RU, Max processors, Processor frequency, Max memory, Max disk capacity, GA Date\nQ: What is the lowest ru?\nA: SELECT MIN RU FROM 1-10818465-1"}
{"text": "table: 1-10818465-1\ncolumns: Model, RU, Max processors, Processor frequency, Max memory, Max disk capacity, GA Date\nQ: What ga date do the models with 1.0, 1.2, 1.4ghz processor frequencies have?\nA: SELECT GA Date FROM 1-10818465-1 WHERE Processor frequency = '1.0, 1.2, 1.4GHz'"}
{"text": "table: 1-10818465-1\ncolumns: Model, RU, Max processors, Processor frequency, Max memory, Max disk capacity, GA Date\nQ: What is the ga date of the t5120 model?\nA: SELECT GA Date FROM 1-10818465-1 WHERE Model = 'T5120'"}
{"text": "table: 1-10815352-1\ncolumns: League, Sport, Country, Season, Games, Average attendance, Total attendance\nQ: What is the sport of the La Liga league?\nA: SELECT Sport FROM 1-10815352-1 WHERE League = 'La Liga'"}
{"text": "table: 1-10815352-1\ncolumns: League, Sport, Country, Season, Games, Average attendance, Total attendance\nQ: What's the minimum total attendance of the Premier League association football?\nA: SELECT MIN Total attendance FROM 1-10815352-1 WHERE Sport = 'Association football' AND League = 'Premier League'"}
{"text": "table: 1-10815352-1\ncolumns: League, Sport, Country, Season, Games, Average attendance, Total attendance\nQ: What's the average attendance of the leagues in the season of 2013?\nA: SELECT MIN Average attendance FROM 1-10815352-1 WHERE Season = '2013'"}
{"text": "table: 1-10815352-1\ncolumns: League, Sport, Country, Season, Games, Average attendance, Total attendance\nQ: What's the total attendance of the leagues in season of 2010?\nA: SELECT COUNT Total attendance FROM 1-10815352-1 WHERE Season = '2010'"}
{"text": "table: 1-10874596-1\ncolumns: Year [e ] (Ceremony), Film title used in nomination, Original title, Director, Result\nQ: Who were the directors of the film submitted with the title Young T\u00f6rless?\nA: SELECT Director FROM 1-10874596-1 WHERE Film title used in nomination = 'Young T\u00f6rless'"}
{"text": "table: 1-10874596-1\ncolumns: Year [e ] (Ceremony), Film title used in nomination, Original title, Director, Result\nQ: What was the original title of the film submitted with the title A Woman in Flames?\nA: SELECT Original title FROM 1-10874596-1 WHERE Film title used in nomination = 'A Woman in Flames'"}
{"text": "table: 1-10874596-1\ncolumns: Year [e ] (Ceremony), Film title used in nomination, Original title, Director, Result\nQ: In what years was a film submitted with the title The Enigma of Kaspar Hauser?\nA: SELECT Year [e ] (Ceremony) FROM 1-10874596-1 WHERE Film title used in nomination = 'The Enigma of Kaspar Hauser'"}
{"text": "table: 1-10874596-1\ncolumns: Year [e ] (Ceremony), Film title used in nomination, Original title, Director, Result\nQ: Who were the directors of the film with the original title o.k.?\nA: SELECT Director FROM 1-10874596-1 WHERE Original title = 'o.k.'"}
{"text": "table: 1-1087659-2\ncolumns: Year, Division, League, Reg. Season, Playoffs, Avg. Attendance\nQ: What is the division for the division semifinals playoffs?\nA: SELECT Division FROM 1-1087659-2 WHERE Playoffs = 'Division Semifinals'"}
{"text": "table: 1-10908676-7\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date\nQ: What is the number in series of \"say uncle\"?\nA: SELECT No. in series FROM 1-10908676-7 WHERE Title = '\"Say Uncle\"'"}
{"text": "table: 1-10908676-7\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date\nQ: What is the title written by David Mamet?\nA: SELECT Title FROM 1-10908676-7 WHERE Written by = 'David Mamet'"}
{"text": "table: 1-10942714-1\ncolumns: Rank, English title, Chinese title, Average, Peak, Premiere, Finale, HK viewers\nQ: What was the finale for \u6f6e\u7206\u5927\u72c0\nA: SELECT Finale FROM 1-10942714-1 WHERE Chinese title = '\u6f6e\u7206\u5927\u72c0'"}
{"text": "table: 1-10942714-1\ncolumns: Rank, English title, Chinese title, Average, Peak, Premiere, Finale, HK viewers\nQ: What was the finale for  \u6f6e\u7206\u5927\u72c0\nA: SELECT Finale FROM 1-10942714-1 WHERE Chinese title = '\u6f6e\u7206\u5927\u72c0'"}
{"text": "table: 1-10942714-1\ncolumns: Rank, English title, Chinese title, Average, Peak, Premiere, Finale, HK viewers\nQ: How many viewers were there for the premier with 34\nA: SELECT HK viewers FROM 1-10942714-1 WHERE Premiere = 34"}
{"text": "table: 1-10942714-1\ncolumns: Rank, English title, Chinese title, Average, Peak, Premiere, Finale, HK viewers\nQ: How many are listed under \u6f6e\u7206\u5927\u72c0\nA: SELECT COUNT Peak FROM 1-10942714-1 WHERE Chinese title = '\u6f6e\u7206\u5927\u72c0'"}
{"text": "table: 1-10953197-2\ncolumns: No. in series, No. in season, Title, Director, Writer(s), Original air date, Production code\nQ: Who was the director of the episode with a production code of 2393059?\nA: SELECT Director FROM 1-10953197-2 WHERE Production code = '2393059'"}
{"text": "table: 1-10953197-2\ncolumns: No. in series, No. in season, Title, Director, Writer(s), Original air date, Production code\nQ: How many people wrote \"Michael's Game\"?\nA: SELECT COUNT Writer(s) FROM 1-10953197-2 WHERE Title = '\"Michael's Game\"'"}
{"text": "table: 1-10953197-2\ncolumns: No. in series, No. in season, Title, Director, Writer(s), Original air date, Production code\nQ: When did the episode title \"Duet For One\" air?\nA: SELECT Original air date FROM 1-10953197-2 WHERE Title = '\"Duet for One\"'"}
{"text": "table: 1-10935548-1\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, Production code, U.S. viewers (millions)\nQ: Which episode had 16.38 million U.S. viewers?\nA: SELECT Title FROM 1-10935548-1 WHERE U.S. viewers (millions) = '16.38'"}
{"text": "table: 1-10935548-1\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, Production code, U.S. viewers (millions)\nQ: What is the production code of the episode written by Jos\u00e9 Molina that aired on October 12, 2004?\nA: SELECT Production code FROM 1-10935548-1 WHERE Written by = 'Jos\u00e9 Molina' AND Original air date = 'October 12, 2004'"}
{"text": "table: 1-10935548-1\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, Production code, U.S. viewers (millions)\nQ: What was the original air date of the episode \"Quarry\"?\nA: SELECT Original air date FROM 1-10935548-1 WHERE Title = '\"Quarry\"'"}
{"text": "table: 1-10935548-1\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, Production code, U.S. viewers (millions)\nQ: Which episode was directed by Jean de Segonzac?\nA: SELECT Title FROM 1-10935548-1 WHERE Directed by = 'Jean de Segonzac'"}
{"text": "table: 1-10953197-3\ncolumns: No. in series, No. in season, Title, Director, Writer(s), Original air date, Production code\nQ: what are the original air dates with a production code of 2394087\nA: SELECT Original air date FROM 1-10953197-3 WHERE Production code = '2394087'"}
{"text": "table: 1-10953197-3\ncolumns: No. in series, No. in season, Title, Director, Writer(s), Original air date, Production code\nQ: Who are the writers for the title \"boxing sydney\"\nA: SELECT Writer(s) FROM 1-10953197-3 WHERE Title = '\"Boxing Sydney\"'"}
{"text": "table: 1-10953197-3\ncolumns: No. in series, No. in season, Title, Director, Writer(s), Original air date, Production code\nQ: What are the production codes for the title \"all about brooke\"\nA: SELECT Production code FROM 1-10953197-3 WHERE Title = '\"All About Brooke\"'"}
{"text": "table: 1-10953197-3\ncolumns: No. in series, No. in season, Title, Director, Writer(s), Original air date, Production code\nQ: Who are the writer(s) for the production code 2394084\nA: SELECT Writer(s) FROM 1-10953197-3 WHERE Production code = '2394084'"}
{"text": "table: 1-10953197-4\ncolumns: No. in series, No. in season, Title, Director, Writer(s), Original air date, Production code\nQ: What's the total number of episodes with the production code 2395113A?\nA: SELECT COUNT Title FROM 1-10953197-4 WHERE Production code = '2395113A'"}
{"text": "table: 1-10953197-4\ncolumns: No. in series, No. in season, Title, Director, Writer(s), Original air date, Production code\nQ: What's the number of the episode called \"Melrose Unglued\"?\nA: SELECT MAX No. in series FROM 1-10953197-4 WHERE Title = '\"Melrose Unglued\"'"}
{"text": "table: 1-10953197-4\ncolumns: No. in series, No. in season, Title, Director, Writer(s), Original air date, Production code\nQ: Who's the writer for the episode with a production code 2395114?\nA: SELECT Writer(s) FROM 1-10953197-4 WHERE Production code = '2395114'"}
{"text": "table: 1-10953197-4\ncolumns: No. in series, No. in season, Title, Director, Writer(s), Original air date, Production code\nQ: Who directed the episode titled \"Full Metal Betsy\"?\nA: SELECT Director FROM 1-10953197-4 WHERE Title = '\"Full Metal Betsy\"'"}
{"text": "table: 1-10953197-4\ncolumns: No. in series, No. in season, Title, Director, Writer(s), Original air date, Production code\nQ: What's the number of the episode with production code 2395118?\nA: SELECT No. in season FROM 1-10953197-4 WHERE Production code = '2395118'"}
{"text": "table: 1-10953197-4\ncolumns: No. in series, No. in season, Title, Director, Writer(s), Original air date, Production code\nQ: Who was the writer for the episode with production code 2395096?\nA: SELECT Writer(s) FROM 1-10953197-4 WHERE Production code = '2395096'"}
{"text": "table: 1-10932739-2\ncolumns: Planet, Planet Type, Semimajor Axis ( AU ), Orbital Period, Radial velocity (m/s), Detectable by:\nQ: What generation of spectrograph is most likely to detect a planet with a radial velocity of 0.089 m/s?\nA: SELECT Detectable by: FROM 1-10932739-2 WHERE Radial velocity (m/s) = '0.089'"}
{"text": "table: 1-10932739-2\ncolumns: Planet, Planet Type, Semimajor Axis ( AU ), Orbital Period, Radial velocity (m/s), Detectable by:\nQ: How long is the orbital period for the planet that has a semimajor axis of 5.20 au?\nA: SELECT Orbital Period FROM 1-10932739-2 WHERE Semimajor Axis ( AU ) = '5.20'"}
{"text": "table: 1-10932739-2\ncolumns: Planet, Planet Type, Semimajor Axis ( AU ), Orbital Period, Radial velocity (m/s), Detectable by:\nQ: What generation of spectrograph is Jupiter detected by?\nA: SELECT Detectable by: FROM 1-10932739-2 WHERE Planet = 'Jupiter'"}
{"text": "table: 1-10932739-2\ncolumns: Planet, Planet Type, Semimajor Axis ( AU ), Orbital Period, Radial velocity (m/s), Detectable by:\nQ: Which planet has an orbital period of 11.86 years?\nA: SELECT Planet FROM 1-10932739-2 WHERE Orbital Period = '11.86 years'"}
{"text": "table: 1-10953197-7\ncolumns: No. in series, No. in season, Title, Director, Writer(s), Original air date, Production code\nQ: who directed the production code 2398204\nA: SELECT Director FROM 1-10953197-7 WHERE Production code = '2398204'"}
{"text": "table: 1-10953197-7\ncolumns: No. in series, No. in season, Title, Director, Writer(s), Original air date, Production code\nQ: when did \"unpleasantville\" air?\nA: SELECT Original air date FROM 1-10953197-7 WHERE Title = '\"Unpleasantville\"'"}
{"text": "table: 1-10960039-1\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What is player Alexis Bwenge's pick number?\nA: SELECT Pick # FROM 1-10960039-1 WHERE Player = 'Alexis Bwenge'"}
{"text": "table: 1-10960039-1\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What player is pick #2?\nA: SELECT Player FROM 1-10960039-1 WHERE Pick # = 2"}
{"text": "table: 1-10960039-1\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: Which player's college is Saskatchewan?\nA: SELECT Player FROM 1-10960039-1 WHERE College = 'Saskatchewan'"}
{"text": "table: 1-10960039-1\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What is McMaster College's pick number?\nA: SELECT MIN Pick # FROM 1-10960039-1 WHERE College = 'McMaster'"}
{"text": "table: 1-10953197-6\ncolumns: No. in series, No. in season, Title, Director, Writer(s), Original air date, Production code\nQ: give the least number of times an episode was shown from 1997-1998\nA: SELECT MIN No. in season FROM 1-10953197-6"}
{"text": "table: 1-10953197-6\ncolumns: No. in series, No. in season, Title, Director, Writer(s), Original air date, Production code\nQ: when was the episode named \"the doctor is in... deep\" first broadcast \nA: SELECT Original air date FROM 1-10953197-6 WHERE Title = '\"The Doctor Is In... Deep\"'"}
{"text": "table: 1-10953197-6\ncolumns: No. in series, No. in season, Title, Director, Writer(s), Original air date, Production code\nQ: how many times does the episode called \"coop de grace\" appear \nA: SELECT COUNT No. in series FROM 1-10953197-6 WHERE Title = '\"Coop de Grace\"'"}
{"text": "table: 1-10953197-6\ncolumns: No. in series, No. in season, Title, Director, Writer(s), Original air date, Production code\nQ: what is season 6 sum of both the number of times processing ID 2397162 was assigned and the number of times chip chalmers managed an episode \nA: SELECT MAX No. in season FROM 1-10953197-6 WHERE Director = 'Chip Chalmers' AND Production code = '2397162'"}
{"text": "table: 1-10966926-2\ncolumns: Round, Choice, Player name, Position, Height, Weight, College\nQ: Which player went to Michigan State?\nA: SELECT Player name FROM 1-10966926-2 WHERE College = 'Michigan State'"}
{"text": "table: 1-10966926-2\ncolumns: Round, Choice, Player name, Position, Height, Weight, College\nQ: Which player went to college in Oklahoma?\nA: SELECT Player name FROM 1-10966926-2 WHERE College = 'Oklahoma'"}
{"text": "table: 1-10966926-2\ncolumns: Round, Choice, Player name, Position, Height, Weight, College\nQ: Which position does Colt Brennan play?\nA: SELECT Position FROM 1-10966926-2 WHERE Player name = 'Colt Brennan'"}
{"text": "table: 1-10966926-2\ncolumns: Round, Choice, Player name, Position, Height, Weight, College\nQ: What is the height of the person that weighs 320 pounds?\nA: SELECT Height FROM 1-10966926-2 WHERE Weight = 320"}
{"text": "table: 1-10975034-4\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: How many colleges have a DB position?\nA: SELECT COUNT College FROM 1-10975034-4 WHERE Position = 'DB'"}
{"text": "table: 1-10975034-4\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What is the maximum number of picks for the CFL team Calgary Stampeders?\nA: SELECT MAX Pick # FROM 1-10975034-4 WHERE CFL Team = 'Calgary Stampeders'"}
{"text": "table: 1-10975034-4\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: How many CFL teams are from York college?\nA: SELECT COUNT CFL Team FROM 1-10975034-4 WHERE College = 'York'"}
{"text": "table: 1-10975034-4\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What CFL teams are part of Simon Fraser college?\nA: SELECT CFL Team FROM 1-10975034-4 WHERE College = 'Simon Fraser'"}
{"text": "table: 1-10975034-4\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: Which players have a pick number of 27?\nA: SELECT Player FROM 1-10975034-4 WHERE Pick # = 27"}
{"text": "table: 1-10960039-6\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: How many times were players named brett ralph were selected?\nA: SELECT COUNT Pick # FROM 1-10960039-6 WHERE Player = 'Brett Ralph'"}
{"text": "table: 1-10960039-6\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What schools did lenard semajuste play for?\nA: SELECT College FROM 1-10960039-6 WHERE Player = 'Lenard Semajuste'"}
{"text": "table: 1-10960039-6\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What is the highest selection number for the saskatchewan roughriders team?\nA: SELECT MAX Pick # FROM 1-10960039-6 WHERE CFL Team = 'Saskatchewan Roughriders'"}
{"text": "table: 1-10960039-6\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: How many fb players were drafted?\nA: SELECT COUNT Pick # FROM 1-10960039-6 WHERE Position = 'FB'"}
{"text": "table: 1-10960039-6\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: How many players played for adams state school?\nA: SELECT COUNT Player FROM 1-10960039-6 WHERE College = 'Adams State'"}
{"text": "table: 1-10960039-6\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What teams drafted players that played for northwood school?\nA: SELECT CFL Team FROM 1-10960039-6 WHERE College = 'Northwood'"}
{"text": "table: 1-10975034-5\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What college did Craig Zimmer go to?\nA: SELECT College FROM 1-10975034-5 WHERE Player = 'Craig Zimmer'"}
{"text": "table: 1-10975034-5\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What is the pick number of regina?\nA: SELECT Pick # FROM 1-10975034-5 WHERE College = 'Regina'"}
{"text": "table: 1-10975034-5\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What is the player who is lb and cfl team is saskatchewan roughriders?\nA: SELECT Player FROM 1-10975034-5 WHERE Position = 'LB' AND CFL Team = 'Saskatchewan Roughriders'"}
{"text": "table: 1-10975034-5\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What is the cfl team that has a position of ol?\nA: SELECT CFL Team FROM 1-10975034-5 WHERE Position = 'OL'"}
{"text": "table: 1-10975034-5\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What is the number of position where the pick number is 43?\nA: SELECT COUNT Position FROM 1-10975034-5 WHERE Pick # = 43"}
{"text": "table: 1-10975034-5\ncolumns: Pick #, CFL Team, Player, Position, College\nQ: What is the cfl team with ryan folk?\nA: SELECT CFL Team FROM 1-10975034-5 WHERE Player = 'Ryan Folk'"}
{"text": "table: 1-10979230-5\ncolumns: Romaji title, Japanese title, Release date, Reference, Oricon\nQ: What release date is when kids-270 is a reference? \nA: SELECT Release date FROM 1-10979230-5 WHERE Reference = 'KIDS-270'"}
{"text": "table: 1-10979230-5\ncolumns: Romaji title, Japanese title, Release date, Reference, Oricon\nQ: what is the title where romaji is titles da.i.su.ki\nA: SELECT Japanese title FROM 1-10979230-5 WHERE Romaji title = 'Da.i.su.ki'"}
{"text": "table: 1-10979230-5\ncolumns: Romaji title, Japanese title, Release date, Reference, Oricon\nQ: what are the title in japanese where the reference is kids-430?\nA: SELECT Japanese title FROM 1-10979230-5 WHERE Reference = 'KIDS-430'"}
{"text": "table: 1-10979230-5\ncolumns: Romaji title, Japanese title, Release date, Reference, Oricon\nQ: who is the reference when romaji title is heartbreak sniper?\nA: SELECT Reference FROM 1-10979230-5 WHERE Romaji title = 'Heartbreak Sniper'"}
{"text": "table: 1-10979230-4\ncolumns: Romaji title, Japanese title, Release date, Reference, Oricon\nQ: What rank is \u611b\u306e\u30d0\u30ab on the Japanese singles chart?\nA: SELECT COUNT Oricon FROM 1-10979230-4 WHERE Japanese title = '\u611b\u306e\u30d0\u30ab'"}
{"text": "table: 1-10979230-4\ncolumns: Romaji title, Japanese title, Release date, Reference, Oricon\nQ: How many songs have mi-chemin as their Japanese name and romanji name?\nA: SELECT COUNT Romaji title FROM 1-10979230-4 WHERE Japanese title = 'Mi-Chemin'"}
{"text": "table: 1-1099080-1\ncolumns: Condition, Prothrombin time, Partial thromboplastin time, Bleeding time, Platelet count\nQ: What was the  partial thromboplastin time for factor x deficiency as seen in amyloid purpura\nA: SELECT Partial thromboplastin time FROM 1-1099080-1 WHERE Condition = 'Factor X deficiency as seen in amyloid purpura'"}
{"text": "table: 1-1099080-1\ncolumns: Condition, Prothrombin time, Partial thromboplastin time, Bleeding time, Platelet count\nQ: How many conditions have an unaffected prothrombin time and a prolonged bleeding time\nA: SELECT COUNT Condition FROM 1-1099080-1 WHERE Prothrombin time = 'Unaffected' AND Bleeding time = 'Prolonged'"}
{"text": "table: 1-1099080-1\ncolumns: Condition, Prothrombin time, Partial thromboplastin time, Bleeding time, Platelet count\nQ: What was the bleeding time for the factor x deficiency as seen in amyloid purpura\nA: SELECT Bleeding time FROM 1-1099080-1 WHERE Condition = 'Factor X deficiency as seen in amyloid purpura'"}
{"text": "table: 1-1099080-1\ncolumns: Condition, Prothrombin time, Partial thromboplastin time, Bleeding time, Platelet count\nQ: What conditions had both prolonged bleeding times and prolonged partial thromboplastin times\nA: SELECT Condition FROM 1-1099080-1 WHERE Partial thromboplastin time = 'Prolonged' AND Bleeding time = 'Prolonged'"}
{"text": "table: 1-1099080-1\ncolumns: Condition, Prothrombin time, Partial thromboplastin time, Bleeding time, Platelet count\nQ: What was the bleeding time for  factor xii deficiency\nA: SELECT Bleeding time FROM 1-1099080-1 WHERE Condition = 'Factor XII deficiency'"}
{"text": "table: 1-1099080-1\ncolumns: Condition, Prothrombin time, Partial thromboplastin time, Bleeding time, Platelet count\nQ: What were the bleeding times when both the platelet count was unaffected and the partial thromboplastin time was unaffected\nA: SELECT Bleeding time FROM 1-1099080-1 WHERE Partial thromboplastin time = 'Unaffected' AND Platelet count = 'Unaffected'"}
{"text": "table: 1-11019212-1\ncolumns: Location, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday\nQ: what's the\u00a0tuesday\u00a0time with\u00a0location\u00a0being millhopper\nA: SELECT Tuesday FROM 1-11019212-1 WHERE Location = 'Millhopper'"}
{"text": "table: 1-11019212-1\ncolumns: Location, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday\nQ: what's the\u00a0wednesday time\u00a0with\u00a0monday\u00a0being 10:00-8:00\nA: SELECT Wednesday FROM 1-11019212-1 WHERE Monday = '10:00-8:00'"}
{"text": "table: 1-11019212-1\ncolumns: Location, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday\nQ: what's the\u00a0thursday\u00a0time with\u00a0location\u00a0being hawthorne\nA: SELECT Thursday FROM 1-11019212-1 WHERE Location = 'Hawthorne'"}
{"text": "table: 1-11019212-1\ncolumns: Location, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday\nQ: what's the\u00a0saturday\u00a0time with\u00a0wednesday\u00a0being 10:00-5:00\nA: SELECT Saturday FROM 1-11019212-1 WHERE Wednesday = '10:00-5:00'"}
{"text": "table: 1-11019212-1\ncolumns: Location, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday\nQ: what's the\u00a0thursday\u00a0time with\u00a0sunday\u00a0being 1:00-5:00 and\u00a0tuesday\u00a0being 1:00-7:00\nA: SELECT Thursday FROM 1-11019212-1 WHERE Sunday = '1:00-5:00' AND Tuesday = '1:00-7:00'"}
{"text": "table: 1-11019212-1\ncolumns: Location, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday\nQ: what's the\u00a0monday\u00a0time with\u00a0tuesday\u00a0being 9:00-6:00\nA: SELECT Monday FROM 1-11019212-1 WHERE Tuesday = '9:00-6:00'"}
{"text": "table: 1-11056278-3\ncolumns: Rnd, Race Name, Pole position, Fastest lap, Winning driver, Winning team, Report\nQ: What are all the reports where Paul Tracy had the fastest lap?\nA: SELECT Report FROM 1-11056278-3 WHERE Fastest lap = 'Paul Tracy'"}
{"text": "table: 1-11056278-3\ncolumns: Rnd, Race Name, Pole position, Fastest lap, Winning driver, Winning team, Report\nQ: Who drove the fastest lap at the Tenneco Automotive Grand Prix of Detroit?\nA: SELECT Fastest lap FROM 1-11056278-3 WHERE Race Name = 'Tenneco Automotive Grand Prix of Detroit'"}
{"text": "table: 1-11056278-3\ncolumns: Rnd, Race Name, Pole position, Fastest lap, Winning driver, Winning team, Report\nQ: Who had the fastest lap in the races won by Max Papis?\nA: SELECT Fastest lap FROM 1-11056278-3 WHERE Winning driver = 'Max Papis'"}
{"text": "table: 1-11056278-3\ncolumns: Rnd, Race Name, Pole position, Fastest lap, Winning driver, Winning team, Report\nQ: In Round 6, how many winning drivers were there?\nA: SELECT COUNT Winning driver FROM 1-11056278-3 WHERE Rnd = 6"}
{"text": "table: 1-1104312-5\ncolumns: English name, Original name, Area in km\u00b2, Population at 2010 Census, Number of settlements and villages\nQ: What are the original names of the districts where the population in the 2010 census was 210450?\nA: SELECT Original name FROM 1-1104312-5 WHERE Population at 2010 Census = 210450"}
{"text": "table: 1-1104312-5\ncolumns: English name, Original name, Area in km\u00b2, Population at 2010 Census, Number of settlements and villages\nQ: What is the original name of the district with the current English name of South Bogor?\nA: SELECT Original name FROM 1-1104312-5 WHERE English name = 'South Bogor'"}
{"text": "table: 1-1104312-5\ncolumns: English name, Original name, Area in km\u00b2, Population at 2010 Census, Number of settlements and villages\nQ: What is the listed population from the 2010 census of West Bogor?\nA: SELECT MIN Population at 2010 Census FROM 1-1104312-5 WHERE English name = 'West Bogor'"}
{"text": "table: 1-1104312-5\ncolumns: English name, Original name, Area in km\u00b2, Population at 2010 Census, Number of settlements and villages\nQ: How many districts have an area of 17.72 KM2?\nA: SELECT COUNT English name FROM 1-1104312-5 WHERE Area in km\u00b2 = '17.72'"}
{"text": "table: 1-1104312-5\ncolumns: English name, Original name, Area in km\u00b2, Population at 2010 Census, Number of settlements and villages\nQ: What is the area in km2 for the district whose original name was Kecamatan Bogor Timur?\nA: SELECT Area in km\u00b2 FROM 1-1104312-5 WHERE Original name = 'Kecamatan Bogor Timur'"}
{"text": "table: 1-11066073-1\ncolumns: Pilot car No., Colour, Serial No., Engine No., Registration No.\nQ: What is the number of colour with the regisration number of mg-509?\nA: SELECT COUNT Colour FROM 1-11066073-1 WHERE Registration No. = 'MG-509'"}
{"text": "table: 1-11058032-1\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: What is the title of the episode directed by Mark Tinker?\nA: SELECT Title FROM 1-11058032-1 WHERE Directed by = 'Mark Tinker'"}
{"text": "table: 1-11058032-1\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: What episode in the season was directed by Jeff Melman?\nA: SELECT MIN No. in season FROM 1-11058032-1 WHERE Directed by = 'Jeff Melman'"}
{"text": "table: 1-11058032-1\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: How many episodes had 16.03 million viewers?\nA: SELECT COUNT No. in series FROM 1-11058032-1 WHERE U.S. viewers (millions) = '16.03'"}
{"text": "table: 1-11071897-1\ncolumns: Interregnum began, Interregnum ended, Duration, Count Palatine of Saxony, Count Palatine of the Rhine\nQ: What is the number of interregnum for duration 3 months, 6 days?\nA: SELECT COUNT Interregnum ended FROM 1-11071897-1 WHERE Duration = '3 months, 6 days'"}
{"text": "table: 1-11075747-4\ncolumns: Series #, Episode #, Title, Directed by, Written by, Original air date\nQ: Who directed Episode 8?\nA: SELECT Directed by FROM 1-11075747-4 WHERE Episode # = 8"}
{"text": "table: 1-11075747-4\ncolumns: Series #, Episode #, Title, Directed by, Written by, Original air date\nQ: Who directed the episode called \"Tell-tale Heart\"?\nA: SELECT Directed by FROM 1-11075747-4 WHERE Title = '\"Tell-Tale Heart\"'"}
{"text": "table: 1-11075747-4\ncolumns: Series #, Episode #, Title, Directed by, Written by, Original air date\nQ: What was the original air date for Series 36?\nA: SELECT Original air date FROM 1-11075747-4 WHERE Series # = 36"}
{"text": "table: 1-11075747-4\ncolumns: Series #, Episode #, Title, Directed by, Written by, Original air date\nQ: Who wrote Series 38?\nA: SELECT Written by FROM 1-11075747-4 WHERE Series # = 38"}
{"text": "table: 1-1108394-24\ncolumns: 1973 Democratic initial primary, Manhattan, The Bronx, Brooklyn, Queens, Richmond [Staten Is.], Total, %\nQ: What is the percentage for manhattan 45,901?\nA: SELECT COUNT % FROM 1-1108394-24 WHERE Manhattan = '45,901'"}
{"text": "table: 1-1108394-24\ncolumns: 1973 Democratic initial primary, Manhattan, The Bronx, Brooklyn, Queens, Richmond [Staten Is.], Total, %\nQ: Who won the 1973 democratic initial primary for queens of 19%?\nA: SELECT 1973 Democratic initial primary FROM 1-1108394-24 WHERE Queens = '19%'"}
{"text": "table: 1-1108394-24\ncolumns: 1973 Democratic initial primary, Manhattan, The Bronx, Brooklyn, Queens, Richmond [Staten Is.], Total, %\nQ: What is the manhattan for richmond 35%?\nA: SELECT Manhattan FROM 1-1108394-24 WHERE Richmond [Staten Is.] = '35%'"}
{"text": "table: 1-1108394-24\ncolumns: 1973 Democratic initial primary, Manhattan, The Bronx, Brooklyn, Queens, Richmond [Staten Is.], Total, %\nQ: What is the queens where richmond staten is 42%?\nA: SELECT Queens FROM 1-1108394-24 WHERE Richmond [Staten Is.] = '42%'"}
{"text": "table: 1-1108394-43\ncolumns: 1932 (before recount), party, Manhattan, The Bronx, Brooklyn, Queens, Richmond [Staten Is.], Total, %\nQ: what's the\u00a0party\u00a0with\u00a0brooklyn\u00a0value of 51.0%\nA: SELECT party FROM 1-1108394-43 WHERE Brooklyn = '51.0%'"}
{"text": "table: 1-1108394-43\ncolumns: 1932 (before recount), party, Manhattan, The Bronx, Brooklyn, Queens, Richmond [Staten Is.], Total, %\nQ: what's the\u00a0brooklyn\u00a0with\u00a0queens\u00a0value of 16.8%\nA: SELECT Brooklyn FROM 1-1108394-43 WHERE Queens = '16.8%'"}
{"text": "table: 1-1108394-43\ncolumns: 1932 (before recount), party, Manhattan, The Bronx, Brooklyn, Queens, Richmond [Staten Is.], Total, %\nQ: what is the minimum total\nA: SELECT MIN Total FROM 1-1108394-43"}
{"text": "table: 1-1108394-43\ncolumns: 1932 (before recount), party, Manhattan, The Bronx, Brooklyn, Queens, Richmond [Staten Is.], Total, %\nQ: what's the\u00a0%\u00a0with\u00a0total\u00a0value of 249887 and\u00a0queens\u00a0value of 6.8%\nA: SELECT % FROM 1-1108394-43 WHERE Total = 249887 AND Queens = '6.8%'"}
{"text": "table: 1-11094950-1\ncolumns: Team, Location, Joined, Conference, Division, Previous Conference\nQ: Which teams were in the central division and located in livonia?\nA: SELECT Team FROM 1-11094950-1 WHERE Division = 'Central' AND Location = 'Livonia'"}
{"text": "table: 1-11094950-1\ncolumns: Team, Location, Joined, Conference, Division, Previous Conference\nQ: Which teams are located in highland township?\nA: SELECT Team FROM 1-11094950-1 WHERE Location = 'Highland Township'"}
{"text": "table: 1-11094950-1\ncolumns: Team, Location, Joined, Conference, Division, Previous Conference\nQ: What conference was the churchill chargers team in?\nA: SELECT Conference FROM 1-11094950-1 WHERE Team = 'Churchill Chargers'"}
{"text": "table: 1-11111116-7\ncolumns: No. in season, No. in series, Title, Directed by, Written by, Original air date, U.S. viewers (million)\nQ: What was the titles of the episodes written by ken lazebnik?\nA: SELECT Title FROM 1-11111116-7 WHERE Written by = 'Ken LaZebnik'"}
{"text": "table: 1-11111116-7\ncolumns: No. in season, No. in series, Title, Directed by, Written by, Original air date, U.S. viewers (million)\nQ: Who directed an episode that had 2.81 million U.S. viewers?\nA: SELECT Directed by FROM 1-11111116-7 WHERE U.S. viewers (million) = '2.81'"}
{"text": "table: 1-11111116-7\ncolumns: No. in season, No. in series, Title, Directed by, Written by, Original air date, U.S. viewers (million)\nQ: What were the names of the episodes that had 3.02 million U.S. viewers?\nA: SELECT Title FROM 1-11111116-7 WHERE U.S. viewers (million) = '3.02'"}
{"text": "table: 1-11111116-7\ncolumns: No. in season, No. in series, Title, Directed by, Written by, Original air date, U.S. viewers (million)\nQ: What were the original air dates of the episode named \"winds of war\"?\nA: SELECT Original air date FROM 1-11111116-7 WHERE Title = '\"Winds of War\"'"}
{"text": "table: 1-11111116-7\ncolumns: No. in season, No. in series, Title, Directed by, Written by, Original air date, U.S. viewers (million)\nQ: Who directed episodes that had 2.61 million U.S. viewers?\nA: SELECT Directed by FROM 1-11111116-7 WHERE U.S. viewers (million) = '2.61'"}
{"text": "table: 1-11111116-8\ncolumns: No. in season, No. in series, Title, Directed by, Written by, Original air date, U.S. viewers (million)\nQ: How many millions of U.S. viewers watched \"Brace for Impact\"?\nA: SELECT U.S. viewers (million) FROM 1-11111116-8 WHERE Title = '\"Brace for Impact\"'"}
{"text": "table: 1-11111116-8\ncolumns: No. in season, No. in series, Title, Directed by, Written by, Original air date, U.S. viewers (million)\nQ: How many millions of U.S. viewers watched the episode that first aired on March 31, 2013?\nA: SELECT U.S. viewers (million) FROM 1-11111116-8 WHERE Original air date = 'March 31, 2013'"}
{"text": "table: 1-11111116-8\ncolumns: No. in season, No. in series, Title, Directed by, Written by, Original air date, U.S. viewers (million)\nQ: Who wrote the episodes that were viewed by 2.12 million viewers?\nA: SELECT Written by FROM 1-11111116-8 WHERE U.S. viewers (million) = '2.12'"}
{"text": "table: 1-11111116-6\ncolumns: No. in season, No. in series, Title, Directed by, Written by, Original air date, U.S. viewers (million)\nQ: The episode written by Rebecca Dameron aired on what date? \nA: SELECT Original air date FROM 1-11111116-6 WHERE Written by = 'Rebecca Dameron'"}
{"text": "table: 1-11111116-6\ncolumns: No. in season, No. in series, Title, Directed by, Written by, Original air date, U.S. viewers (million)\nQ: Which episode in the series drew 3.6 million U.S. viewers? \nA: SELECT MIN No. in series FROM 1-11111116-6 WHERE U.S. viewers (million) = '3.6'"}
{"text": "table: 1-11111116-6\ncolumns: No. in season, No. in series, Title, Directed by, Written by, Original air date, U.S. viewers (million)\nQ: Who wrote the episode that aired on April 17, 2011? \nA: SELECT Written by FROM 1-11111116-6 WHERE Original air date = 'April 17, 2011'"}
{"text": "table: 1-11111116-6\ncolumns: No. in season, No. in series, Title, Directed by, Written by, Original air date, U.S. viewers (million)\nQ: How many times did episode 79 originally air? \nA: SELECT COUNT Original air date FROM 1-11111116-6 WHERE No. in series = 79"}
{"text": "table: 1-11111116-6\ncolumns: No. in season, No. in series, Title, Directed by, Written by, Original air date, U.S. viewers (million)\nQ: How many millions of views in the country watched \"Line of Departure\"?\nA: SELECT U.S. viewers (million) FROM 1-11111116-6 WHERE Title = '\"Line of Departure\"'"}
{"text": "table: 1-11148572-1\ncolumns: Season, MLS Cup Winner, MLS Cup Runner-Up, MLS Supporters Shield Winner, MLS Supporters Shield Runner-Up\nQ: What is the name of the shield winner in which the mls cup winner and mls cup runner up is colorado rapids?\nA: SELECT MLS Cup Winner FROM 1-11148572-1 WHERE MLS Cup Runner-Up = 'Colorado Rapids'"}
{"text": "table: 1-11148572-1\ncolumns: Season, MLS Cup Winner, MLS Cup Runner-Up, MLS Supporters Shield Winner, MLS Supporters Shield Runner-Up\nQ: What is the name of the shield winner in which the mls cup winner and mls supporters shield runner up is Chivas usa?\nA: SELECT MLS Cup Winner FROM 1-11148572-1 WHERE MLS Supporters Shield Runner-Up = 'Chivas USA'"}
{"text": "table: 1-11148572-1\ncolumns: Season, MLS Cup Winner, MLS Cup Runner-Up, MLS Supporters Shield Winner, MLS Supporters Shield Runner-Up\nQ: who is the of the shield winnerin which the mls cup runner-up and mls cup winner is real salt lake?\nA: SELECT MLS Cup Runner-Up FROM 1-11148572-1 WHERE MLS Cup Winner = 'Real Salt Lake'"}
{"text": "table: 1-11148572-1\ncolumns: Season, MLS Cup Winner, MLS Cup Runner-Up, MLS Supporters Shield Winner, MLS Supporters Shield Runner-Up\nQ: Which shield winner has the mls cup runner up and the season is 2000?\nA: SELECT MLS Cup Runner-Up FROM 1-11148572-1 WHERE Season = 2000"}
{"text": "table: 1-1112176-1\ncolumns: Season, Division, League Apps (Sub), League Goals, FA Cup Apps (Sub), FA Cup Goals, FL Cup Apps (Sub), FL Cup Goals, Other Apps, Other Goals, Total Apps (Sub), Total Goals\nQ: League apps (sub) maximum?\nA: SELECT MAX League Apps (Sub) FROM 1-1112176-1"}
{"text": "table: 1-1112176-1\ncolumns: Season, Division, League Apps (Sub), League Goals, FA Cup Apps (Sub), FA Cup Goals, FL Cup Apps (Sub), FL Cup Goals, Other Apps, Other Goals, Total Apps (Sub), Total Goals\nQ: When total goals is 11 what was the league apps (sub)?\nA: SELECT MAX League Apps (Sub) FROM 1-1112176-1 WHERE Total Goals = 11"}
{"text": "table: 1-11129123-1\ncolumns: Episode Air Date, Audition City, Date, First Audition Venue, Callback Date, Callback Venue, Golden Tickets\nQ: Which city had the charleston area convention center as its callback location\nA: SELECT Audition City FROM 1-11129123-1 WHERE Callback Venue = 'Charleston Area Convention Center'"}
{"text": "table: 1-11129123-1\ncolumns: Episode Air Date, Audition City, Date, First Audition Venue, Callback Date, Callback Venue, Golden Tickets\nQ: When did the callbacks from  rancho bernardo inn air\nA: SELECT Episode Air Date FROM 1-11129123-1 WHERE Callback Venue = 'Rancho Bernardo Inn'"}
{"text": "table: 1-11147852-1\ncolumns: City of license/Market, Station, Channel TV ( DT ), Year of affiliation, Owned since\nQ: The station located in Albuquerque has been owned since what year?\nA: SELECT Owned since FROM 1-11147852-1 WHERE City of license/Market = 'Albuquerque'"}
{"text": "table: 1-11147852-1\ncolumns: City of license/Market, Station, Channel TV ( DT ), Year of affiliation, Owned since\nQ: What channels have stations that were affiliated in 2002?\nA: SELECT Channel TV ( DT ) FROM 1-11147852-1 WHERE Year of affiliation = '2002'"}
{"text": "table: 1-11147852-1\ncolumns: City of license/Market, Station, Channel TV ( DT ), Year of affiliation, Owned since\nQ: What market is KTFK-DT in?\nA: SELECT City of license/Market FROM 1-11147852-1 WHERE Station = 'KTFK-DT'"}
{"text": "table: 1-11167610-1\ncolumns: Trim, Engine, Turbo, Fuel Delivery, Power, Torque, Transmission, Performance\nQ:  what's the\u00a0engine\u00a0where\u00a0performance\u00a0is 0\u2013100km/h: 10.5s, vmax km/h (mph)\nA: SELECT Engine FROM 1-11167610-1 WHERE Performance = '0\u2013100km/h: 10.5s, VMax km/h (mph)'"}
{"text": "table: 1-11167610-1\ncolumns: Trim, Engine, Turbo, Fuel Delivery, Power, Torque, Transmission, Performance\nQ:  what's the\u00a0turbo\u00a0where\u00a0trim\u00a0is 2.0 20v\nA: SELECT Turbo FROM 1-11167610-1 WHERE Trim = '2.0 20v'"}
{"text": "table: 1-11167610-1\ncolumns: Trim, Engine, Turbo, Fuel Delivery, Power, Torque, Transmission, Performance\nQ:  what's the\u00a0torque\u00a0where\u00a0performance\u00a0is 0\u2013100km/h: 7.5s auto, vmax: km/h (mph)\nA: SELECT Torque FROM 1-11167610-1 WHERE Performance = '0\u2013100km/h: 7.5s auto, VMax: km/h (mph)'"}
{"text": "table: 1-11167610-1\ncolumns: Trim, Engine, Turbo, Fuel Delivery, Power, Torque, Transmission, Performance\nQ:  what's the\u00a0transmission\u00a0where\u00a0turbo\u00a0is yes (mitsubishi td04-16t )\nA: SELECT Transmission FROM 1-11167610-1 WHERE Turbo = 'Yes (Mitsubishi TD04-16t )'"}
{"text": "table: 1-11167610-1\ncolumns: Trim, Engine, Turbo, Fuel Delivery, Power, Torque, Transmission, Performance\nQ:  what's the\u00a0fuel delivery\u00a0where\u00a0power\u00a0is hp (kw) @6500 rpm\nA: SELECT Fuel Delivery FROM 1-11167610-1 WHERE Power = 'hp (kW) @6500 rpm'"}
{"text": "table: 1-11167610-1\ncolumns: Trim, Engine, Turbo, Fuel Delivery, Power, Torque, Transmission, Performance\nQ: \" what's the engine with turbo being yes (mitsubishi td04-15g ) \"\nA: SELECT Engine FROM 1-11167610-1 WHERE Turbo = 'Yes (Mitsubishi TD04-15g )'"}
{"text": "table: 1-11173827-1\ncolumns: Rank, English title, Chinese title, Average, Peak, Premiere, Finale, HK viewers\nQ: What is the english title that has finale as 33 and peak as 42?\nA: SELECT English title FROM 1-11173827-1 WHERE Finale = 33 AND Peak = 42"}
{"text": "table: 1-11173827-1\ncolumns: Rank, English title, Chinese title, Average, Peak, Premiere, Finale, HK viewers\nQ: What is the english title where the premiere is less than 30.0 and the finale is bigger than 36.0?\nA: SELECT English title FROM 1-11173827-1 WHERE Premiere < 30.0 AND Finale > 36.0"}
{"text": "table: 1-11173827-1\ncolumns: Rank, English title, Chinese title, Average, Peak, Premiere, Finale, HK viewers\nQ: What is the rank of the chinese title \u7de3\u4f86\u81ea\u6709\u6a5f?\nA: SELECT Rank FROM 1-11173827-1 WHERE Chinese title = '\u7de3\u4f86\u81ea\u6709\u6a5f'"}
{"text": "table: 1-11173827-1\ncolumns: Rank, English title, Chinese title, Average, Peak, Premiere, Finale, HK viewers\nQ: What amount is the number of hk viewers where chinese title is \u5341\u5144\u5f1f?\nA: SELECT HK viewers FROM 1-11173827-1 WHERE Chinese title = '\u5341\u5144\u5f1f'"}
{"text": "table: 1-11178271-1\ncolumns: #, Episode, Air Date, Rating, Share, 18\u201349 (Rating/Share), Viewers (m), Weekly Rank\nQ: What is the weekly rank with an air date is november 12, 2007?\nA: SELECT Weekly Rank FROM 1-11178271-1 WHERE Air Date = 'November 12, 2007'"}
{"text": "table: 1-11178271-1\ncolumns: #, Episode, Air Date, Rating, Share, 18\u201349 (Rating/Share), Viewers (m), Weekly Rank\nQ: What is the air date of the episode \"blowback\"?\nA: SELECT Air Date FROM 1-11178271-1 WHERE Episode = '\"Blowback\"'"}
{"text": "table: 1-11178271-1\ncolumns: #, Episode, Air Date, Rating, Share, 18\u201349 (Rating/Share), Viewers (m), Weekly Rank\nQ: What is the lowest weekly rank with an air date of november 26, 2007?\nA: SELECT MIN Weekly Rank FROM 1-11178271-1 WHERE Air Date = 'November 26, 2007'"}
{"text": "table: 1-11178271-1\ncolumns: #, Episode, Air Date, Rating, Share, 18\u201349 (Rating/Share), Viewers (m), Weekly Rank\nQ: What is the episode where 18-49 has a rating/share of 3.5/9\nA: SELECT Episode FROM 1-11178271-1 WHERE 18\u201349 (Rating/Share) = '3.5/9'"}
{"text": "table: 1-11178271-1\ncolumns: #, Episode, Air Date, Rating, Share, 18\u201349 (Rating/Share), Viewers (m), Weekly Rank\nQ: What is the viewers where the rating is 5.3?\nA: SELECT Viewers (m) FROM 1-11178271-1 WHERE Rating = '5.3'"}
{"text": "table: 1-11178271-1\ncolumns: #, Episode, Air Date, Rating, Share, 18\u201349 (Rating/Share), Viewers (m), Weekly Rank\nQ: What is the 18-49 rating/share where the viewers is 5.61?\nA: SELECT 18\u201349 (Rating/Share) FROM 1-11178271-1 WHERE Viewers (m) = '5.61'"}
{"text": "table: 1-11206787-5\ncolumns: Team, Stadium, Capacity, Highest, Lowest, Average\nQ: What is the highest of balmoor/\nA: SELECT Highest FROM 1-11206787-5 WHERE Stadium = 'Balmoor'"}
{"text": "table: 1-11206787-5\ncolumns: Team, Stadium, Capacity, Highest, Lowest, Average\nQ: What is the number of capacity at somerset park?\nA: SELECT COUNT Capacity FROM 1-11206787-5 WHERE Stadium = 'Somerset Park'"}
{"text": "table: 1-11206787-5\ncolumns: Team, Stadium, Capacity, Highest, Lowest, Average\nQ: What is the minimum capacity where airdrie united is?\nA: SELECT MIN Capacity FROM 1-11206787-5 WHERE Team = 'Airdrie United'"}
{"text": "table: 1-11206787-5\ncolumns: Team, Stadium, Capacity, Highest, Lowest, Average\nQ: What is the stadium for alloa athletic?\nA: SELECT Stadium FROM 1-11206787-5 WHERE Team = 'Alloa Athletic'"}
{"text": "table: 1-11206787-5\ncolumns: Team, Stadium, Capacity, Highest, Lowest, Average\nQ: What is the highest of ayr united?\nA: SELECT MIN Highest FROM 1-11206787-5 WHERE Team = 'Ayr United'"}
{"text": "table: 1-11206787-5\ncolumns: Team, Stadium, Capacity, Highest, Lowest, Average\nQ: What is the average?\nA: SELECT MIN Average FROM 1-11206787-5"}
{"text": "table: 1-11190568-7\ncolumns: Team, Outgoing manager, Manner of departure, Date of vacancy, Replaced by, Date of appointment, Position\nQ: When are team Galway's dates of appointment?\nA: SELECT Date of appointment FROM 1-11190568-7 WHERE Team = 'Galway'"}
{"text": "table: 1-11190568-7\ncolumns: Team, Outgoing manager, Manner of departure, Date of vacancy, Replaced by, Date of appointment, Position\nQ: When are the vacancy dates for outgoing manager Damien Fox?\nA: SELECT Date of vacancy FROM 1-11190568-7 WHERE Outgoing manager = 'Damien Fox'"}
{"text": "table: 1-11190568-7\ncolumns: Team, Outgoing manager, Manner of departure, Date of vacancy, Replaced by, Date of appointment, Position\nQ: When is the date of vacancy of Davy Fitzgerald being a replacement?\nA: SELECT Date of vacancy FROM 1-11190568-7 WHERE Replaced by = 'Davy FitzGerald'"}
{"text": "table: 1-11190568-7\ncolumns: Team, Outgoing manager, Manner of departure, Date of vacancy, Replaced by, Date of appointment, Position\nQ: Which team has the outgoing manager John Meyler?\nA: SELECT Team FROM 1-11190568-7 WHERE Outgoing manager = 'John Meyler'"}
{"text": "table: 1-11200856-1\ncolumns: Hand, 1 credit, 2 credits, 3 credits, 4 credits, 5 credits\nQ: How many times is 3 credits 180?\nA: SELECT COUNT 1 credit FROM 1-11200856-1 WHERE 3 credits = 180"}
{"text": "table: 1-11200856-1\ncolumns: Hand, 1 credit, 2 credits, 3 credits, 4 credits, 5 credits\nQ: What is the hand for 4 credits is 1600?\nA: SELECT Hand FROM 1-11200856-1 WHERE 4 credits = 1600"}
{"text": "table: 1-11200856-1\ncolumns: Hand, 1 credit, 2 credits, 3 credits, 4 credits, 5 credits\nQ: How many 3 credits are there with 5 credits of 5?\nA: SELECT COUNT 3 credits FROM 1-11200856-1 WHERE 5 credits = '5'"}
{"text": "table: 1-11200856-1\ncolumns: Hand, 1 credit, 2 credits, 3 credits, 4 credits, 5 credits\nQ: How many 4 credits is the hand two pair?\nA: SELECT COUNT 4 credits FROM 1-11200856-1 WHERE Hand = 'Two pair'"}
{"text": "table: 1-11210576-3\ncolumns: Character, Position, Actor, First Episode, Final Episode, Duration, Final Episode Count\nQ: What duration is listed for Christian de la Fuente?\nA: SELECT Duration FROM 1-11210576-3 WHERE Actor = 'Christian de la Fuente'"}
{"text": "table: 1-11210576-3\ncolumns: Character, Position, Actor, First Episode, Final Episode, Duration, Final Episode Count\nQ: What was the final episode for Dea Agent?\nA: SELECT Final Episode FROM 1-11210576-3 WHERE Position = 'DEA Agent'"}
{"text": "table: 1-11207040-6\ncolumns: Team, Outgoing manager, Manner of departure, Date of vacancy, Replaced by, Date of appointment\nQ: What days is greenock morton vacant?\nA: SELECT Date of vacancy FROM 1-11207040-6 WHERE Team = 'Greenock Morton'"}
{"text": "table: 1-11207040-6\ncolumns: Team, Outgoing manager, Manner of departure, Date of vacancy, Replaced by, Date of appointment\nQ: What are the dates of the outgoing manager colin hendry does appointments? \nA: SELECT Date of appointment FROM 1-11207040-6 WHERE Outgoing manager = 'Colin Hendry'"}
{"text": "table: 1-11207040-6\ncolumns: Team, Outgoing manager, Manner of departure, Date of vacancy, Replaced by, Date of appointment\nQ: What teams does jim mcinally manage?\nA: SELECT Team FROM 1-11207040-6 WHERE Outgoing manager = 'Jim McInally'"}
{"text": "table: 1-11207040-6\ncolumns: Team, Outgoing manager, Manner of departure, Date of vacancy, Replaced by, Date of appointment\nQ: What days are vacant that were replaced by john brown?\nA: SELECT Date of vacancy FROM 1-11207040-6 WHERE Replaced by = 'John Brown'"}
{"text": "table: 1-11206916-2\ncolumns: Team, Outgoing manager, Manner of departure, Date of vacancy, Replaced by, Date of appointment\nQ: What manner of departure is listed with an appointment date of 13 march 2008\nA: SELECT Manner of departure FROM 1-11206916-2 WHERE Date of appointment = '13 March 2008'"}
{"text": "table: 1-11206916-2\ncolumns: Team, Outgoing manager, Manner of departure, Date of vacancy, Replaced by, Date of appointment\nQ: What is the date of appointment for outgoing manager Campbell Money\nA: SELECT Date of appointment FROM 1-11206916-2 WHERE Outgoing manager = 'Campbell Money'"}
{"text": "table: 1-11207040-5\ncolumns: Team, Stadium, Capacity, Highest, Lowest, Average\nQ: What is the lowest attendance that East End Park has ever had?\nA: SELECT MIN Lowest FROM 1-11207040-5 WHERE Stadium = 'East End Park'"}
{"text": "table: 1-11207040-5\ncolumns: Team, Stadium, Capacity, Highest, Lowest, Average\nQ: What team plays at Palmerston Park?\nA: SELECT Team FROM 1-11207040-5 WHERE Stadium = 'Palmerston Park'"}
{"text": "table: 1-11207040-5\ncolumns: Team, Stadium, Capacity, Highest, Lowest, Average\nQ: What is the lowest attandance recorded at Cappielow?\nA: SELECT MIN Lowest FROM 1-11207040-5 WHERE Stadium = 'Cappielow'"}
{"text": "table: 1-11207040-5\ncolumns: Team, Stadium, Capacity, Highest, Lowest, Average\nQ: What is the highest attendance at a game played by St. Johnstone?\nA: SELECT MAX Highest FROM 1-11207040-5 WHERE Team = 'St. Johnstone'"}
{"text": "table: 1-11207040-5\ncolumns: Team, Stadium, Capacity, Highest, Lowest, Average\nQ: What is the highest attandence at a Hamilton Academical game?\nA: SELECT MIN Highest FROM 1-11207040-5 WHERE Team = 'Hamilton Academical'"}
{"text": "table: 1-11214772-1\ncolumns: Year, Champion, Score, Runner-Up, Location, Semi-Finalist #1, Semi-Finalist #2\nQ:  who is the\u00a0champion\u00a0where\u00a0semi-finalist #2\u00a0is na and\u00a0location\u00a0is morrisville, nc\nA: SELECT Champion FROM 1-11214772-1 WHERE Semi-Finalist #2 = 'NA' AND Location = 'Morrisville, NC'"}
{"text": "table: 1-11214772-1\ncolumns: Year, Champion, Score, Runner-Up, Location, Semi-Finalist #1, Semi-Finalist #2\nQ:  what's the\u00a0score\u00a0where\u00a0year\u00a0is 2007\nA: SELECT Score FROM 1-11214772-1 WHERE Year = '2007'"}
{"text": "table: 1-11214772-1\ncolumns: Year, Champion, Score, Runner-Up, Location, Semi-Finalist #1, Semi-Finalist #2\nQ: what is the total number of\u00a0semi-finalist #2\u00a0where\u00a0runner-up\u00a0is east carolina\nA: SELECT COUNT Semi-Finalist #2 FROM 1-11214772-1 WHERE Runner-Up = 'East Carolina'"}
{"text": "table: 1-11214772-1\ncolumns: Year, Champion, Score, Runner-Up, Location, Semi-Finalist #1, Semi-Finalist #2\nQ:  who is the\u00a0semi-finalist #1\u00a0where\u00a0runner-up\u00a0is elon university\nA: SELECT Semi-Finalist #1 FROM 1-11214772-1 WHERE Runner-Up = 'Elon University'"}
{"text": "table: 1-11214772-1\ncolumns: Year, Champion, Score, Runner-Up, Location, Semi-Finalist #1, Semi-Finalist #2\nQ:  who is the\u00a0runner-up\u00a0where\u00a0year\u00a0is 2004 and\u00a0champion\u00a0is north carolina state\nA: SELECT Runner-Up FROM 1-11214772-1 WHERE Year = '2004' AND Champion = 'North Carolina State'"}
{"text": "table: 1-11214772-1\ncolumns: Year, Champion, Score, Runner-Up, Location, Semi-Finalist #1, Semi-Finalist #2\nQ:  who is the\u00a0runner-up\u00a0where\u00a0location\u00a0is ellenton, fl and\u00a0year\u00a0is 2004\nA: SELECT Runner-Up FROM 1-11214772-1 WHERE Location = 'Ellenton, FL' AND Year = '2004'"}
{"text": "table: 1-11214212-1\ncolumns: Year, Numer of Jamaicans granted British citizenship, Naturalisation by residence, Naturalisation by marriage, Registration of a minor child, Registration by other means\nQ: what's the\u00a0naturalisation  by marriage\u00a0with\u00a0numer of jamaicans granted british citizenship\u00a0being 3165\nA: SELECT Naturalisation by marriage FROM 1-11214212-1 WHERE Numer of Jamaicans granted British citizenship = 3165"}
{"text": "table: 1-11214212-1\ncolumns: Year, Numer of Jamaicans granted British citizenship, Naturalisation by residence, Naturalisation by marriage, Registration of a minor child, Registration by other means\nQ:  how many\u00a0numer of jamaicans granted british citizenship\u00a0with\u00a0naturalisation  by marriage\u00a0being 1060\nA: SELECT COUNT Numer of Jamaicans granted British citizenship FROM 1-11214212-1 WHERE Naturalisation by marriage = 1060"}
{"text": "table: 1-11214212-1\ncolumns: Year, Numer of Jamaicans granted British citizenship, Naturalisation by residence, Naturalisation by marriage, Registration of a minor child, Registration by other means\nQ: what's the\u00a0naturalisation by marriage\u00a0with\u00a0regbeingtration of a minor child\u00a0being 114\nA: SELECT Naturalisation by marriage FROM 1-11214212-1 WHERE Registration of a minor child = 114"}
{"text": "table: 1-11214212-1\ncolumns: Year, Numer of Jamaicans granted British citizenship, Naturalisation by residence, Naturalisation by marriage, Registration of a minor child, Registration by other means\nQ: what's the\u00a0numer of jamaicans granted british  citizenship\u00a0with\u00a0naturalisation by residence\u00a0being 927\nA: SELECT Numer of Jamaicans granted British citizenship FROM 1-11214212-1 WHERE Naturalisation by residence = 927"}
{"text": "table: 1-11214212-1\ncolumns: Year, Numer of Jamaicans granted British citizenship, Naturalisation by residence, Naturalisation by marriage, Registration of a minor child, Registration by other means\nQ: what is the maximum\u00a0year\u00a0with\u00a0registration of a minor child\u00a0being 281\nA: SELECT MAX Year FROM 1-11214212-1 WHERE Registration of a minor child = 281"}
{"text": "table: 1-11220799-2\ncolumns: Episode Titles, First air date, Reward, Immunity, Exiled, Eliminated, Vote, Finish\nQ: How many episodes had their first air date on March 6, 2008?\nA: SELECT COUNT Episode Titles FROM 1-11220799-2 WHERE First air date = 'March 6, 2008'"}
{"text": "table: 1-11220799-2\ncolumns: Episode Titles, First air date, Reward, Immunity, Exiled, Eliminated, Vote, Finish\nQ: What were the results of episodes with the first air date of March 6, 2008?\nA: SELECT Finish FROM 1-11220799-2 WHERE First air date = 'March 6, 2008'"}
{"text": "table: 1-11230937-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: How many millions of viewers watched episode 15?\nA: SELECT U.S. viewers (millions) FROM 1-11230937-2 WHERE No. in season = 15"}
{"text": "table: 1-11230937-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: How many millions of viewers watched the \"Throwing Heat\" episode?\nA: SELECT U.S. viewers (millions) FROM 1-11230937-2 WHERE Title = '\"Throwing Heat\"'"}
{"text": "table: 1-11230937-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: How many millions of viewers watched the episode directed by Anthony Hemingway?\nA: SELECT U.S. viewers (millions) FROM 1-11230937-2 WHERE Directed by = 'Anthony Hemingway'"}
{"text": "table: 1-11222744-2\ncolumns: Year, Title, Format, Studio, Release Date, Copyright Information, Catalog Number\nQ: The Catalog number is 80809 what is the title?\nA: SELECT Title FROM 1-11222744-2 WHERE Catalog Number = '80809'"}
{"text": "table: 1-11222744-2\ncolumns: Year, Title, Format, Studio, Release Date, Copyright Information, Catalog Number\nQ: where title is beginning callanetics , what is the total of format ?\nA: SELECT COUNT Format FROM 1-11222744-2 WHERE Title = 'Beginning Callanetics'"}
{"text": "table: 1-11222744-2\ncolumns: Year, Title, Format, Studio, Release Date, Copyright Information, Catalog Number\nQ: where catalog number is 81258 , what are all the studio ?\nA: SELECT Studio FROM 1-11222744-2 WHERE Catalog Number = '81258'"}
{"text": "table: 1-11222744-2\ncolumns: Year, Title, Format, Studio, Release Date, Copyright Information, Catalog Number\nQ: where title is am/pm callanetics , what are all the copyright information?\nA: SELECT Copyright Information FROM 1-11222744-2 WHERE Title = 'AM/PM Callanetics'"}
{"text": "table: 1-11236195-2\ncolumns: Season, Grand FinalDate, WinningTeam, Score, LosingTeam, Location, GF Attendance, Clive Churchill Medal\nQ: What was the GF attendance at the location of Sydney Football Stadium, Sydney (6)?\nA: SELECT COUNT GF Attendance FROM 1-11236195-2 WHERE Location = 'Sydney Football Stadium, Sydney (6)'"}
{"text": "table: 1-11236195-2\ncolumns: Season, Grand FinalDate, WinningTeam, Score, LosingTeam, Location, GF Attendance, Clive Churchill Medal\nQ: Which losing team had a score of 24-12?\nA: SELECT LosingTeam FROM 1-11236195-2 WHERE Score = '24-12'"}
{"text": "table: 1-11236195-2\ncolumns: Season, Grand FinalDate, WinningTeam, Score, LosingTeam, Location, GF Attendance, Clive Churchill Medal\nQ: What was the losing team in the 1993 season?\nA: SELECT LosingTeam FROM 1-11236195-2 WHERE Season = 1993"}
{"text": "table: 1-1123802-1\ncolumns: Engine, Power, continuous, Critical altitude This is the highest altitude at which the engine can achieve its full continuous power rating. Above this altitude, power falls off with height as with a naturally aspirated engine . See Supercharger#Altitude effects for details., Power, takeoff, Compression ratio, Supercharger gear ratio, Octane rating, Dry weight\nQ: What was the compression ration when the engine was Wasp Jr. T1B2?\nA: SELECT Compression ratio FROM 1-1123802-1 WHERE Engine = 'Wasp Jr. T1B2'"}
{"text": "table: 1-1123802-1\ncolumns: Engine, Power, continuous, Critical altitude This is the highest altitude at which the engine can achieve its full continuous power rating. Above this altitude, power falls off with height as with a naturally aspirated engine . See Supercharger#Altitude effects for details., Power, takeoff, Compression ratio, Supercharger gear ratio, Octane rating, Dry weight\nQ: What is the compression ration when the continuous power is hp (kw) at 2,200 RPM and the octane rating is 80/87?\nA: SELECT Compression ratio FROM 1-1123802-1 WHERE Power, continuous = 'hp (kW) at 2,200 RPM' AND Octane rating = '80/87'"}
{"text": "table: 1-1123802-1\ncolumns: Engine, Power, continuous, Critical altitude This is the highest altitude at which the engine can achieve its full continuous power rating. Above this altitude, power falls off with height as with a naturally aspirated engine . See Supercharger#Altitude effects for details., Power, takeoff, Compression ratio, Supercharger gear ratio, Octane rating, Dry weight\nQ: What is the compression ratio when the continuous power is  hp (KW) at 2,200 RPM and the critical altitude is at sea level?\nA: SELECT COUNT Compression ratio FROM 1-1123802-1 WHERE Power, continuous = 'hp (kW) at 2,200 RPM' AND Critical altitude This is the highest altitude at which the engine can achieve its full continuous power rating. Above this altitude, power falls off with height as with a naturally aspirated engine . See Supercharger#Altitude effects for details. = 'sea level'"}
{"text": "table: 1-1123802-1\ncolumns: Engine, Power, continuous, Critical altitude This is the highest altitude at which the engine can achieve its full continuous power rating. Above this altitude, power falls off with height as with a naturally aspirated engine . See Supercharger#Altitude effects for details., Power, takeoff, Compression ratio, Supercharger gear ratio, Octane rating, Dry weight\nQ: When the engine is Wasp Jr. T1B2, what is the number needed for takeoff power?\nA: SELECT COUNT Power, takeoff FROM 1-1123802-1 WHERE Engine = 'Wasp Jr. T1B2'"}
{"text": "table: 1-1123802-1\ncolumns: Engine, Power, continuous, Critical altitude This is the highest altitude at which the engine can achieve its full continuous power rating. Above this altitude, power falls off with height as with a naturally aspirated engine . See Supercharger#Altitude effects for details., Power, takeoff, Compression ratio, Supercharger gear ratio, Octane rating, Dry weight\nQ: When critical altitude is sea level, what is the compression ration for a supercharger gear ratio of 7:1?\nA: SELECT Compression ratio FROM 1-1123802-1 WHERE Critical altitude This is the highest altitude at which the engine can achieve its full continuous power rating. Above this altitude, power falls off with height as with a naturally aspirated engine . See Supercharger#Altitude effects for details. = 'sea level' AND Supercharger gear ratio = '7:1'"}
{"text": "table: 1-11235334-2\ncolumns: #, Episode, Air Date, Timeslot, Viewers, Weekly Rank for Living\nQ: How many episodes aired on october 27, 2008\nA: SELECT COUNT Episode FROM 1-11235334-2 WHERE Air Date = 'October 27, 2008'"}
{"text": "table: 1-11235334-2\ncolumns: #, Episode, Air Date, Timeslot, Viewers, Weekly Rank for Living\nQ: The episode \"chapter five: dressed to kill\" had a weekly ranking of what?\nA: SELECT Weekly Rank for Living FROM 1-11235334-2 WHERE Episode = '\"Chapter Five: Dressed to Kill\"'"}
{"text": "table: 1-11235334-2\ncolumns: #, Episode, Air Date, Timeslot, Viewers, Weekly Rank for Living\nQ: what is the most # that aired on september 29, 2008?\nA: SELECT MAX # FROM 1-11235334-2 WHERE Air Date = 'September 29, 2008'"}
{"text": "table: 1-11236195-5\ncolumns: Season, Grand FinalDate, WinningTeam, Score, LosingTeam, Location, GF Attendance, Clive Churchill Medal\nQ: How many seasons did the canterbury bulldogs (8) win?\nA: SELECT COUNT Season FROM 1-11236195-5 WHERE WinningTeam = 'Canterbury Bulldogs (8)'"}
{"text": "table: 1-11236195-5\ncolumns: Season, Grand FinalDate, WinningTeam, Score, LosingTeam, Location, GF Attendance, Clive Churchill Medal\nQ: How many teams lost at the sydney football stadium, sydney (11)?\nA: SELECT COUNT LosingTeam FROM 1-11236195-5 WHERE Location = 'Sydney Football Stadium, Sydney (11)'"}
{"text": "table: 1-11236195-5\ncolumns: Season, Grand FinalDate, WinningTeam, Score, LosingTeam, Location, GF Attendance, Clive Churchill Medal\nQ: What was the date that the st. george-illawarra dragons lost?\nA: SELECT Grand FinalDate FROM 1-11236195-5 WHERE LosingTeam = 'St. George-Illawarra Dragons'"}
{"text": "table: 1-11236195-5\ncolumns: Season, Grand FinalDate, WinningTeam, Score, LosingTeam, Location, GF Attendance, Clive Churchill Medal\nQ: Brett kimmorley, who was chosen for the clive churchill medal belonged to what team?\nA: SELECT WinningTeam FROM 1-11236195-5 WHERE Clive Churchill Medal = 'Brett Kimmorley'"}
{"text": "table: 1-11244302-1\ncolumns: #, Episode, Air Date, Time slot (EST), Rating, Share, 18-49 (Rating/Share), Viewers (m), Rank (Overall)\nQ: What time slots have a 6.3 rating\nA: SELECT Time slot (EST) FROM 1-11244302-1 WHERE Rating = '6.3'"}
{"text": "table: 1-11244302-1\ncolumns: #, Episode, Air Date, Time slot (EST), Rating, Share, 18-49 (Rating/Share), Viewers (m), Rank (Overall)\nQ: What time slot is the episode \"the way we weren't\" in\nA: SELECT Time slot (EST) FROM 1-11244302-1 WHERE Episode = '\"The Way We Weren't\"'"}
{"text": "table: 1-11244302-1\ncolumns: #, Episode, Air Date, Time slot (EST), Rating, Share, 18-49 (Rating/Share), Viewers (m), Rank (Overall)\nQ: What time slot is the episode \"who's your daddy\" in\nA: SELECT Time slot (EST) FROM 1-11244302-1 WHERE Episode = '\"Who's Your Daddy\"'"}
{"text": "table: 1-11244302-1\ncolumns: #, Episode, Air Date, Time slot (EST), Rating, Share, 18-49 (Rating/Share), Viewers (m), Rank (Overall)\nQ: Which air date had an 11 share\nA: SELECT Air Date FROM 1-11244302-1 WHERE Share = 11"}
{"text": "table: 1-11244302-1\ncolumns: #, Episode, Air Date, Time slot (EST), Rating, Share, 18-49 (Rating/Share), Viewers (m), Rank (Overall)\nQ: Which air date had the 18-49 rating/share of 3.3/9\nA: SELECT Air Date FROM 1-11244302-1 WHERE 18-49 (Rating/Share) = '3.3/9'"}
{"text": "table: 1-11240028-3\ncolumns: Character, Portrayed by, Relationship, First appearance, Last appearance\nQ: Which characters had their first experience in the episode \"consequences\"?\nA: SELECT Character FROM 1-11240028-3 WHERE First appearance = '\"Consequences\"'"}
{"text": "table: 1-11240028-3\ncolumns: Character, Portrayed by, Relationship, First appearance, Last appearance\nQ: What episode had the last appearances of the late wife of mac taylor?\nA: SELECT Last appearance FROM 1-11240028-3 WHERE Relationship = 'Late wife of Mac Taylor'"}
{"text": "table: 1-11240028-3\ncolumns: Character, Portrayed by, Relationship, First appearance, Last appearance\nQ: Which characters were portrayed by reed garrett?\nA: SELECT Portrayed by FROM 1-11240028-3 WHERE Character = 'Reed Garrett'"}
{"text": "table: 1-11240028-3\ncolumns: Character, Portrayed by, Relationship, First appearance, Last appearance\nQ: How many characters were portrayed by the informant of don flack?\nA: SELECT COUNT Portrayed by FROM 1-11240028-3 WHERE Relationship = 'Informant of Don Flack'"}
{"text": "table: 1-11240028-3\ncolumns: Character, Portrayed by, Relationship, First appearance, Last appearance\nQ: What episode was the last appearance of the character, rikki sandoval?\nA: SELECT Last appearance FROM 1-11240028-3 WHERE Character = 'Rikki Sandoval'"}
{"text": "table: 1-11240028-1\ncolumns: Character, Portrayed by, First appearance, Last appearance, Duration, Episodes\nQ: On which episode did actress Sela Ward make her last appearance?\nA: SELECT Last appearance FROM 1-11240028-1 WHERE Portrayed by = 'Sela Ward'"}
{"text": "table: 1-11240028-1\ncolumns: Character, Portrayed by, First appearance, Last appearance, Duration, Episodes\nQ: Which actors first appeared in \"Zoo York\"?\nA: SELECT Portrayed by FROM 1-11240028-1 WHERE First appearance = '\"Zoo York\"'"}
{"text": "table: 1-11240028-1\ncolumns: Character, Portrayed by, First appearance, Last appearance, Duration, Episodes\nQ: How many episodes did actress Vanessa Ferlito appear in?\nA: SELECT Episodes FROM 1-11240028-1 WHERE Portrayed by = 'Vanessa Ferlito'"}
{"text": "table: 1-11240028-1\ncolumns: Character, Portrayed by, First appearance, Last appearance, Duration, Episodes\nQ: Which actors first appeared in episode \"Blink\" 1, 2, 3?\nA: SELECT Portrayed by FROM 1-11240028-1 WHERE First appearance = '\"Blink\" 1, 2, 3'"}
{"text": "table: 1-11240028-1\ncolumns: Character, Portrayed by, First appearance, Last appearance, Duration, Episodes\nQ: What was the duration of Robert Joy's portrayal?\nA: SELECT COUNT Duration FROM 1-11240028-1 WHERE Portrayed by = 'Robert Joy'"}
{"text": "table: 1-11240028-1\ncolumns: Character, Portrayed by, First appearance, Last appearance, Duration, Episodes\nQ: Which episode did actor A. J. Buckley last appear in?\nA: SELECT Last appearance FROM 1-11240028-1 WHERE Portrayed by = 'A. J. Buckley'"}
{"text": "table: 1-11250-4\ncolumns: Club, Position in 2012\u201313, First season in top division, Number of seasons in top division, Number of seasons in the Premier League, First season of current spell in top division, Top division titles, Last top division title\nQ: What is the least top division titles?\nA: SELECT MIN Top division titles FROM 1-11250-4"}
{"text": "table: 1-11250-4\ncolumns: Club, Position in 2012\u201313, First season in top division, Number of seasons in top division, Number of seasons in the Premier League, First season of current spell in top division, Top division titles, Last top division title\nQ: What is the least number of seasons in top division?\nA: SELECT MIN Number of seasons in top division FROM 1-11250-4"}
{"text": "table: 1-11253290-2\ncolumns: #, Episode, Rating, Share, Rating/Share (18-49), Viewers (millions), Rank (timeslot), Rank (night), Rank (week)\nQ: How many viewers (millions) were there for rank (week) 20?\nA: SELECT COUNT Viewers (millions) FROM 1-11253290-2 WHERE Rank (week) = '20'"}
{"text": "table: 1-11253290-2\ncolumns: #, Episode, Rating, Share, Rating/Share (18-49), Viewers (millions), Rank (timeslot), Rank (night), Rank (week)\nQ: What is the rank (timeslot) with the episode name \"dangerous liaisons\"?\nA: SELECT Rank (timeslot) FROM 1-11253290-2 WHERE Episode = '\"Dangerous Liaisons\"'"}
{"text": "table: 1-11253290-2\ncolumns: #, Episode, Rating, Share, Rating/Share (18-49), Viewers (millions), Rank (timeslot), Rank (night), Rank (week)\nQ: What is the lowest rank (night) for having viewers (millions) 5.25?\nA: SELECT MIN Rank (night) FROM 1-11253290-2 WHERE Viewers (millions) = '5.25'"}
{"text": "table: 1-11253290-2\ncolumns: #, Episode, Rating, Share, Rating/Share (18-49), Viewers (millions), Rank (timeslot), Rank (night), Rank (week)\nQ: How many times was the episode named \"conference call\"?\nA: SELECT COUNT # FROM 1-11253290-2 WHERE Episode = '\"Conference Call\"'"}
{"text": "table: 1-11253290-2\ncolumns: #, Episode, Rating, Share, Rating/Share (18-49), Viewers (millions), Rank (timeslot), Rank (night), Rank (week)\nQ: How many times was the rank (night) 11?\nA: SELECT COUNT Viewers (millions) FROM 1-11253290-2 WHERE Rank (night) = 11"}
{"text": "table: 1-11251601-2\ncolumns: Country, Carbon dioxide emissions per year (10 6 Tons) (2006), Percentage of global total, Avg. emission per km 2 of its land (tons), Carbon dioxide emissions per year (Tons per person) (2007)\nQ: WHAT WAS THE AMOUNT OF CARBON DIOXIDE EMISSIONS  IN 2006 IN THE COUNTRY WHOSE  CO2 EMISSIONS (TONS PER PERSON)  REACHED 1.4 IN 2OO7?\nA: SELECT Carbon dioxide emissions per year (10 6 Tons) (2006) FROM 1-11251601-2 WHERE Carbon dioxide emissions per year (Tons per person) (2007) = '1.4'"}
{"text": "table: 1-11251601-2\ncolumns: Country, Carbon dioxide emissions per year (10 6 Tons) (2006), Percentage of global total, Avg. emission per km 2 of its land (tons), Carbon dioxide emissions per year (Tons per person) (2007)\nQ: HOW MANY TONS OF CO2 EMISSIONS DID RUSSIA PRODUCE IN 2006?\nA: SELECT MAX Carbon dioxide emissions per year (10 6 Tons) (2006) FROM 1-11251601-2 WHERE Country = 'Russia'"}
{"text": "table: 1-11251601-2\ncolumns: Country, Carbon dioxide emissions per year (10 6 Tons) (2006), Percentage of global total, Avg. emission per km 2 of its land (tons), Carbon dioxide emissions per year (Tons per person) (2007)\nQ: WHAT PERCENTAGE OF GLOBAL TOTAL EMISSIONS DID INDIA PRODUCE?\nA: SELECT Percentage of global total FROM 1-11251601-2 WHERE Country = 'India'"}
{"text": "table: 1-11251601-2\ncolumns: Country, Carbon dioxide emissions per year (10 6 Tons) (2006), Percentage of global total, Avg. emission per km 2 of its land (tons), Carbon dioxide emissions per year (Tons per person) (2007)\nQ: HOW MUCH IS THE PERCENTAGE OF GLOBAL TOTAL EMISSIONS IN THE COUNTRY THAT PRODUCED 4.9 TONS PER PERSON IN 2007?\nA: SELECT Percentage of global total FROM 1-11251601-2 WHERE Carbon dioxide emissions per year (Tons per person) (2007) = '4.9'"}
{"text": "table: 1-11251601-2\ncolumns: Country, Carbon dioxide emissions per year (10 6 Tons) (2006), Percentage of global total, Avg. emission per km 2 of its land (tons), Carbon dioxide emissions per year (Tons per person) (2007)\nQ: WHAT WAS THE AVERAGE EMISSION PER KM 2 IN INDIA?\nA: SELECT MAX Avg. emission per km 2 of its land (tons) FROM 1-11251601-2 WHERE Country = 'India'"}
{"text": "table: 1-11251109-3\ncolumns: #, Episode, Air Date, Timeslot (EST), Season, Rating, Share, 18\u201349, Viewers (m), Rank (#)\nQ: What is the rank number that aired october 26, 2007?\nA: SELECT Rank (#) FROM 1-11251109-3 WHERE Air Date = 'October 26, 2007'"}
{"text": "table: 1-11251109-3\ncolumns: #, Episode, Air Date, Timeslot (EST), Season, Rating, Share, 18\u201349, Viewers (m), Rank (#)\nQ: What is the number of rank with the viewership of 5.96 million?\nA: SELECT COUNT Rank (#) FROM 1-11251109-3 WHERE Viewers (m) = '5.96'"}
{"text": "table: 1-11251109-3\ncolumns: #, Episode, Air Date, Timeslot (EST), Season, Rating, Share, 18\u201349, Viewers (m), Rank (#)\nQ: What is the viewership on november 9, 2007?\nA: SELECT Viewers (m) FROM 1-11251109-3 WHERE Air Date = 'November 9, 2007'"}
{"text": "table: 1-11254821-2\ncolumns: Finishing position, Points awarded (Platinum), Points awarded (Gold), Points awarded (Silver), Points awarded (Satellite)\nQ: How many platinum points were awarded when 6 gold points were awarded?\nA: SELECT MAX Points awarded (Platinum) FROM 1-11254821-2 WHERE Points awarded (Gold) = 6"}
{"text": "table: 1-11254821-2\ncolumns: Finishing position, Points awarded (Platinum), Points awarded (Gold), Points awarded (Silver), Points awarded (Satellite)\nQ: What was the range of finishing position for 15 awarded platinum points?\nA: SELECT Finishing position FROM 1-11254821-2 WHERE Points awarded (Platinum) = 15"}
{"text": "table: 1-11254821-2\ncolumns: Finishing position, Points awarded (Platinum), Points awarded (Gold), Points awarded (Silver), Points awarded (Satellite)\nQ: How many platinum points were awarded for 5th place?\nA: SELECT MAX Points awarded (Platinum) FROM 1-11254821-2 WHERE Finishing position = '5th'"}
{"text": "table: 1-11254821-2\ncolumns: Finishing position, Points awarded (Platinum), Points awarded (Gold), Points awarded (Silver), Points awarded (Satellite)\nQ: How many platinum points were awarded when 70 silver points were awarded?\nA: SELECT Points awarded (Platinum) FROM 1-11254821-2 WHERE Points awarded (Silver) = 70"}
{"text": "table: 1-11254821-2\ncolumns: Finishing position, Points awarded (Platinum), Points awarded (Gold), Points awarded (Silver), Points awarded (Satellite)\nQ: How many platinum points were awarded when 9 gold points were awarded?\nA: SELECT Points awarded (Platinum) FROM 1-11254821-2 WHERE Points awarded (Gold) = 9"}
{"text": "table: 1-11274401-2\ncolumns: No., Episode, Air Date, Timeslot, Rating, Share, 18\u201349 (Rating/Share), Viewers (m), Rank (#)\nQ: How did the episode rank that had 2.65 million viewers?\nA: SELECT Rank (#) FROM 1-11274401-2 WHERE Viewers (m) = '2.65'"}
{"text": "table: 1-11274401-2\ncolumns: No., Episode, Air Date, Timeslot, Rating, Share, 18\u201349 (Rating/Share), Viewers (m), Rank (#)\nQ: What was the share for the first episode that ranked 85?\nA: SELECT MIN Share FROM 1-11274401-2 WHERE Rank (#) = '85'"}
{"text": "table: 1-11274401-2\ncolumns: No., Episode, Air Date, Timeslot, Rating, Share, 18\u201349 (Rating/Share), Viewers (m), Rank (#)\nQ: Which timeslot did episode no. 15 hold?\nA: SELECT Timeslot FROM 1-11274401-2 WHERE No. = 15"}
{"text": "table: 1-11274401-3\ncolumns: No., Episode, Air Date, Timeslot, Rating, Share, 18\u201349 (Rating/Share), Viewers (m), Rank (#)\nQ: What was the timeslot for the episode that aired on May 12, 2009?\nA: SELECT Timeslot FROM 1-11274401-3 WHERE Air Date = 'May 12, 2009'"}
{"text": "table: 1-11274401-3\ncolumns: No., Episode, Air Date, Timeslot, Rating, Share, 18\u201349 (Rating/Share), Viewers (m), Rank (#)\nQ: What's the 18-49 (rating/share) of the episode that originally aired on May 5, 2009?\nA: SELECT 18\u201349 (Rating/Share) FROM 1-11274401-3 WHERE Air Date = 'May 5, 2009'"}
{"text": "table: 1-11274401-3\ncolumns: No., Episode, Air Date, Timeslot, Rating, Share, 18\u201349 (Rating/Share), Viewers (m), Rank (#)\nQ: What's the total number of episodes whose original airings were viewed by 1.82 million viewers?\nA: SELECT COUNT Air Date FROM 1-11274401-3 WHERE Viewers (m) = '1.82'"}
{"text": "table: 1-11274401-3\ncolumns: No., Episode, Air Date, Timeslot, Rating, Share, 18\u201349 (Rating/Share), Viewers (m), Rank (#)\nQ: What's the rating of the episode originally aired on May 5, 2009?\nA: SELECT Rating FROM 1-11274401-3 WHERE Air Date = 'May 5, 2009'"}
{"text": "table: 1-11274401-3\ncolumns: No., Episode, Air Date, Timeslot, Rating, Share, 18\u201349 (Rating/Share), Viewers (m), Rank (#)\nQ: What episode was seen by 2.05 million viewers?\nA: SELECT Episode FROM 1-11274401-3 WHERE Viewers (m) = '2.05'"}
{"text": "table: 1-11256021-1\ncolumns: Date, Founder, Extroversion Scales, People-task orientation scale, Introverted, Task-Oriented, Extroverted, Task-Oriented, Extroverted, Relationship-Oriented, Introverted, Relationship Oriented, Moderate\nQ:  what's the\u00a0extroverted, relationship-oriented\u00a0where\u00a0extroverted, task-oriented\u00a0is director\nA: SELECT Extroverted, Relationship-Oriented FROM 1-11256021-1 WHERE Extroverted, Task-Oriented = 'Director'"}
{"text": "table: 1-11256021-1\ncolumns: Date, Founder, Extroversion Scales, People-task orientation scale, Introverted, Task-Oriented, Extroverted, Task-Oriented, Extroverted, Relationship-Oriented, Introverted, Relationship Oriented, Moderate\nQ:  what's the\u00a0extroverted, relationship-oriented\u00a0where\u00a0moderate\u00a0is introverted sanguine\nA: SELECT Extroverted, Relationship-Oriented FROM 1-11256021-1 WHERE Moderate = 'Introverted Sanguine'"}
{"text": "table: 1-11256021-1\ncolumns: Date, Founder, Extroversion Scales, People-task orientation scale, Introverted, Task-Oriented, Extroverted, Task-Oriented, Extroverted, Relationship-Oriented, Introverted, Relationship Oriented, Moderate\nQ:  what's the\u00a0founder\u00a0where\u00a0moderate\u00a0is ether\nA: SELECT Founder FROM 1-11256021-1 WHERE Moderate = 'ether'"}
{"text": "table: 1-11256021-1\ncolumns: Date, Founder, Extroversion Scales, People-task orientation scale, Introverted, Task-Oriented, Extroverted, Task-Oriented, Extroverted, Relationship-Oriented, Introverted, Relationship Oriented, Moderate\nQ:  what's the\u00a0extroverted, relationship-oriented\u00a0where\u00a0date\u00a0is c. 1928\nA: SELECT Extroverted, Relationship-Oriented FROM 1-11256021-1 WHERE Date = 'c. 1928'"}
{"text": "table: 1-11256021-1\ncolumns: Date, Founder, Extroversion Scales, People-task orientation scale, Introverted, Task-Oriented, Extroverted, Task-Oriented, Extroverted, Relationship-Oriented, Introverted, Relationship Oriented, Moderate\nQ:  who is the\u00a0founder\u00a0where\u00a0date\u00a0is c. 1900\nA: SELECT Founder FROM 1-11256021-1 WHERE Date = 'c. 1900'"}
{"text": "table: 1-11256021-1\ncolumns: Date, Founder, Extroversion Scales, People-task orientation scale, Introverted, Task-Oriented, Extroverted, Task-Oriented, Extroverted, Relationship-Oriented, Introverted, Relationship Oriented, Moderate\nQ:  what's the\u00a0people-task orientation scale\u00a0where\u00a0extroverted, relationship-oriented\u00a0is team type\nA: SELECT People-task orientation scale FROM 1-11256021-1 WHERE Extroverted, Relationship-Oriented = 'Team Type'"}
{"text": "table: 1-11303072-5\ncolumns: Wicket, Runs, Batting partners, Batting team, Fielding team, Venue, Season\nQ: What is the batting team where the runs are 276?\nA: SELECT Batting team FROM 1-11303072-5 WHERE Runs = '276'"}
{"text": "table: 1-11303072-5\ncolumns: Wicket, Runs, Batting partners, Batting team, Fielding team, Venue, Season\nQ: Name the batting team at Durham\nA: SELECT Batting team FROM 1-11303072-5 WHERE Fielding team = 'Durham'"}
{"text": "table: 1-11303072-5\ncolumns: Wicket, Runs, Batting partners, Batting team, Fielding team, Venue, Season\nQ: What is the batting team with the batting partnets of thilina kandamby and rangana herath?\nA: SELECT Batting team FROM 1-11303072-5 WHERE Batting partners = 'Thilina Kandamby and Rangana Herath'"}
{"text": "table: 1-11303072-5\ncolumns: Wicket, Runs, Batting partners, Batting team, Fielding team, Venue, Season\nQ: What is the fielding team with 155 runs?\nA: SELECT Fielding team FROM 1-11303072-5 WHERE Runs = '155'"}
{"text": "table: 1-11303072-5\ncolumns: Wicket, Runs, Batting partners, Batting team, Fielding team, Venue, Season\nQ: What is the batting partners with runs of 226?\nA: SELECT Batting partners FROM 1-11303072-5 WHERE Runs = '226'"}
{"text": "table: 1-11303072-9\ncolumns: Rank, Dismissals, Player, Nationality, Catches, Stumpings, Career Span\nQ: What is the nationality of David Bairstow?\nA: SELECT Nationality FROM 1-11303072-9 WHERE Player = 'David Bairstow'"}
{"text": "table: 1-11303072-9\ncolumns: Rank, Dismissals, Player, Nationality, Catches, Stumpings, Career Span\nQ: What are the players whose rank is 2?\nA: SELECT Player FROM 1-11303072-9 WHERE Rank = 2"}
{"text": "table: 1-11303072-9\ncolumns: Rank, Dismissals, Player, Nationality, Catches, Stumpings, Career Span\nQ: How many stumpings has Paul Nixon in his career?\nA: SELECT Stumpings FROM 1-11303072-9 WHERE Player = 'Paul Nixon'"}
{"text": "table: 1-11303072-9\ncolumns: Rank, Dismissals, Player, Nationality, Catches, Stumpings, Career Span\nQ: Where is Adam Gilchrist from?\nA: SELECT Nationality FROM 1-11303072-9 WHERE Player = 'Adam Gilchrist'"}
{"text": "table: 1-1130632-1\ncolumns: No. in series, Title, Directed by, Written by, Featured character(s), Original air date, U.S. viewers (million)\nQ: What are the title that have 19.48 million u.s. viewers?\nA: SELECT Title FROM 1-1130632-1 WHERE U.S. viewers (million) = '19.48'"}
{"text": "table: 1-1130632-1\ncolumns: No. in series, Title, Directed by, Written by, Featured character(s), Original air date, U.S. viewers (million)\nQ: Which titles have 18.73 u.s. viewers.\nA: SELECT Title FROM 1-1130632-1 WHERE U.S. viewers (million) = '18.73'"}
{"text": "table: 1-1130632-1\ncolumns: No. in series, Title, Directed by, Written by, Featured character(s), Original air date, U.S. viewers (million)\nQ: Who wrote all the shows with 18.73 u.s. viewers?\nA: SELECT Written by FROM 1-1130632-1 WHERE U.S. viewers (million) = '18.73'"}
{"text": "table: 1-1131183-2\ncolumns: Rank ( WJC ), Rank (ARDA), Metro area, Number of Jews (WJC), Number of Jews (ASARB)\nQ: What is the rank where the area is Los Angeles?\nA: SELECT Rank ( WJC ) FROM 1-1131183-2 WHERE Metro area = 'Los Angeles'"}
{"text": "table: 1-1131183-2\ncolumns: Rank ( WJC ), Rank (ARDA), Metro area, Number of Jews (WJC), Number of Jews (ASARB)\nQ: What is the number of jews where the rank is 1?\nA: SELECT COUNT Number of Jews (WJC) FROM 1-1131183-2 WHERE Rank (ARDA) = 1"}
{"text": "table: 1-1131183-2\ncolumns: Rank ( WJC ), Rank (ARDA), Metro area, Number of Jews (WJC), Number of Jews (ASARB)\nQ: What is the number of jews asarb where the metro area is philadelphia?\nA: SELECT Number of Jews (ASARB) FROM 1-1131183-2 WHERE Metro area = 'Philadelphia'"}
{"text": "table: 1-11318462-5\ncolumns: Crew, Open 1st VIII, Open 2nd VIII, Open 3rd VIII, U16 1st VIII, U16 2nd VIII, U16 3rd VIII, U15 1st IV, U15 2nd IV, U15 3rd IV, U15 4th IV, U15 5th IV, U15 6th IV\nQ: what are all the open 1st viii with u15 6th iv being bgs\nA: SELECT Open 1st VIII FROM 1-11318462-5 WHERE U15 6th IV = 'BGS'"}
{"text": "table: 1-11318462-5\ncolumns: Crew, Open 1st VIII, Open 2nd VIII, Open 3rd VIII, U16 1st VIII, U16 2nd VIII, U16 3rd VIII, U15 1st IV, U15 2nd IV, U15 3rd IV, U15 4th IV, U15 5th IV, U15 6th IV\nQ: what are all the u16 2nd viii with u15 3rd iv being bbc\nA: SELECT U16 2nd VIII FROM 1-11318462-5 WHERE U15 3rd IV = 'BBC'"}
{"text": "table: 1-11318462-5\ncolumns: Crew, Open 1st VIII, Open 2nd VIII, Open 3rd VIII, U16 1st VIII, U16 2nd VIII, U16 3rd VIII, U15 1st IV, U15 2nd IV, U15 3rd IV, U15 4th IV, U15 5th IV, U15 6th IV\nQ: what are all the open 1st viii with u15 4th iv being gt\nA: SELECT Open 1st VIII FROM 1-11318462-5 WHERE U15 4th IV = 'GT'"}
{"text": "table: 1-11318462-5\ncolumns: Crew, Open 1st VIII, Open 2nd VIII, Open 3rd VIII, U16 1st VIII, U16 2nd VIII, U16 3rd VIII, U15 1st IV, U15 2nd IV, U15 3rd IV, U15 4th IV, U15 5th IV, U15 6th IV\nQ: how many crew had u15 3rd iv being bgs and u15 1st iv being acgs and open 1st viii being acgs\nA: SELECT COUNT Crew FROM 1-11318462-5 WHERE U15 3rd IV = 'BGS' AND U15 1st IV = 'ACGS' AND Open 1st VIII = 'ACGS'"}
{"text": "table: 1-11318462-5\ncolumns: Crew, Open 1st VIII, Open 2nd VIII, Open 3rd VIII, U16 1st VIII, U16 2nd VIII, U16 3rd VIII, U15 1st IV, U15 2nd IV, U15 3rd IV, U15 4th IV, U15 5th IV, U15 6th IV\nQ: what are all the u15 3rd iv with u15 4th iv being bbc\nA: SELECT U15 3rd IV FROM 1-11318462-5 WHERE U15 4th IV = 'BBC'"}
{"text": "table: 1-11318462-5\ncolumns: Crew, Open 1st VIII, Open 2nd VIII, Open 3rd VIII, U16 1st VIII, U16 2nd VIII, U16 3rd VIII, U15 1st IV, U15 2nd IV, U15 3rd IV, U15 4th IV, U15 5th IV, U15 6th IV\nQ: how many open 2nd viii had u15 3rd iv being gt\nA: SELECT COUNT Open 2nd VIII FROM 1-11318462-5 WHERE U15 3rd IV = 'GT'"}
{"text": "table: 1-11318462-29\ncolumns: School, Location, Enrolment, Founded, Denomination, Day/Boarding, School Colours, Abbreviation, In competition since\nQ: How many schools have an enrollment of 850?\nA: SELECT COUNT Founded FROM 1-11318462-29 WHERE Enrolment = 850"}
{"text": "table: 1-11318462-29\ncolumns: School, Location, Enrolment, Founded, Denomination, Day/Boarding, School Colours, Abbreviation, In competition since\nQ: What is the location of the school named Brisbane Girls' Grammar School?\nA: SELECT Location FROM 1-11318462-29 WHERE School = 'Brisbane Girls' Grammar School'"}
{"text": "table: 1-11318462-29\ncolumns: School, Location, Enrolment, Founded, Denomination, Day/Boarding, School Colours, Abbreviation, In competition since\nQ: How many schools are located in South Brisbane?\nA: SELECT COUNT School FROM 1-11318462-29 WHERE Location = 'South Brisbane'"}
{"text": "table: 1-11318462-29\ncolumns: School, Location, Enrolment, Founded, Denomination, Day/Boarding, School Colours, Abbreviation, In competition since\nQ: When was SPLC founded?\nA: SELECT MIN Founded FROM 1-11318462-29 WHERE Abbreviation = 'SPLC'"}
{"text": "table: 1-11318462-29\ncolumns: School, Location, Enrolment, Founded, Denomination, Day/Boarding, School Colours, Abbreviation, In competition since\nQ: What is the enrollment of STM which has been in competition since 1990?\nA: SELECT COUNT Enrolment FROM 1-11318462-29 WHERE In competition since = 1990 AND Abbreviation = 'STM'"}
{"text": "table: 1-1132568-3\ncolumns: Rd., Grand Prix, Pole Position, Fastest Lap, Winning Driver, Constructor, Report\nQ: What number is the Monaco Grand Prix?\nA: SELECT Rd. FROM 1-1132568-3 WHERE Grand Prix = 'Monaco Grand Prix'"}
{"text": "table: 1-1132568-3\ncolumns: Rd., Grand Prix, Pole Position, Fastest Lap, Winning Driver, Constructor, Report\nQ: Who is in the pole position for the French Grand Prix?\nA: SELECT Pole Position FROM 1-1132568-3 WHERE Grand Prix = 'French Grand Prix'"}
{"text": "table: 1-1132568-3\ncolumns: Rd., Grand Prix, Pole Position, Fastest Lap, Winning Driver, Constructor, Report\nQ: What are the numbers for the raceways that are constructed by Ferrari, with Michael Schumacher holding the fastest lap and pole position?\nA: SELECT Rd. FROM 1-1132568-3 WHERE Fastest Lap = 'Michael Schumacher' AND Constructor = 'Ferrari' AND Pole Position = 'Michael Schumacher'"}
{"text": "table: 1-1132568-3\ncolumns: Rd., Grand Prix, Pole Position, Fastest Lap, Winning Driver, Constructor, Report\nQ: How many on the list are called the Austrian Grand Prix?\nA: SELECT COUNT Rd. FROM 1-1132568-3 WHERE Grand Prix = 'Austrian Grand Prix'"}
{"text": "table: 1-1132568-3\ncolumns: Rd., Grand Prix, Pole Position, Fastest Lap, Winning Driver, Constructor, Report\nQ: What number is the Canadian Grand Prix on the list?\nA: SELECT Rd. FROM 1-1132568-3 WHERE Grand Prix = 'Canadian Grand Prix'"}
{"text": "table: 1-1132588-3\ncolumns: Rd., Grand Prix, Pole Position, Fastest Lap, Winning Driver, Constructor, Report\nQ: What is the rd for the canadian grand prix?\nA: SELECT Rd. FROM 1-1132588-3 WHERE Grand Prix = 'Canadian Grand Prix'"}
{"text": "table: 1-1132588-3\ncolumns: Rd., Grand Prix, Pole Position, Fastest Lap, Winning Driver, Constructor, Report\nQ: What is the fastest lap for the european grand prix?\nA: SELECT Fastest Lap FROM 1-1132588-3 WHERE Grand Prix = 'European Grand Prix'"}
{"text": "table: 1-1132588-3\ncolumns: Rd., Grand Prix, Pole Position, Fastest Lap, Winning Driver, Constructor, Report\nQ: What is the pole position for the ferrari at the austrian grand prix?\nA: SELECT Pole Position FROM 1-1132588-3 WHERE Constructor = 'Ferrari' AND Grand Prix = 'Austrian Grand Prix'"}
{"text": "table: 1-11326124-3\ncolumns: Edition, Zone, Round, Date, Against, Surface, Opponent, Outcome, Result\nQ: What was the result of round 2r?\nA: SELECT Outcome FROM 1-11326124-3 WHERE Round = '2R'"}
{"text": "table: 1-11326124-3\ncolumns: Edition, Zone, Round, Date, Against, Surface, Opponent, Outcome, Result\nQ: Who did Tina Pisnik verse?\nA: SELECT Against FROM 1-11326124-3 WHERE Opponent = 'Tina Pisnik'"}
{"text": "table: 1-11326124-3\ncolumns: Edition, Zone, Round, Date, Against, Surface, Opponent, Outcome, Result\nQ: How many rounds were 2r?\nA: SELECT COUNT Result FROM 1-11326124-3 WHERE Round = '2R'"}
{"text": "table: 1-11326124-3\ncolumns: Edition, Zone, Round, Date, Against, Surface, Opponent, Outcome, Result\nQ: Name the outcome for round 2r\nA: SELECT Outcome FROM 1-11326124-3 WHERE Round = '2R'"}
{"text": "table: 1-11354111-3\ncolumns: #, Episode, Air Date, Rating, Share, Rating/Share 18\u201349, Viewers (m), Timeslot Rank, Night Rank, Overall Rank\nQ: what's the night rank with viewers (m) of 6.63\nA: SELECT Night Rank FROM 1-11354111-3 WHERE Viewers (m) = '6.63'"}
{"text": "table: 1-11354111-3\ncolumns: #, Episode, Air Date, Rating, Share, Rating/Share 18\u201349, Viewers (m), Timeslot Rank, Night Rank, Overall Rank\nQ: what's the overall rank with viewers (m) of 7.44\nA: SELECT Overall Rank FROM 1-11354111-3 WHERE Viewers (m) = '7.44'"}
{"text": "table: 1-11354111-3\ncolumns: #, Episode, Air Date, Rating, Share, Rating/Share 18\u201349, Viewers (m), Timeslot Rank, Night Rank, Overall Rank\nQ: what's the overall rank with rating/share 18\u201349 of 2.1/5\nA: SELECT COUNT Overall Rank FROM 1-11354111-3 WHERE Rating/Share 18\u201349 = '2.1/5'"}
{"text": "table: 1-11354111-3\ncolumns: #, Episode, Air Date, Rating, Share, Rating/Share 18\u201349, Viewers (m), Timeslot Rank, Night Rank, Overall Rank\nQ: what's the night rank with rating of 6.2\nA: SELECT Night Rank FROM 1-11354111-3 WHERE Rating = '6.2'"}
{"text": "table: 1-11354111-3\ncolumns: #, Episode, Air Date, Rating, Share, Rating/Share 18\u201349, Viewers (m), Timeslot Rank, Night Rank, Overall Rank\nQ: what's the viewers (m) with episode of \"legacy\"\nA: SELECT Viewers (m) FROM 1-11354111-3 WHERE Episode = '\"Legacy\"'"}
{"text": "table: 1-1137142-1\ncolumns: Season, Group A Winner, Group B Winner, Group C Winner, Group D Winner\nQ: What is the number of group b winner for francavilla?\nA: SELECT COUNT Group B Winner FROM 1-1137142-1 WHERE Group C Winner = 'Francavilla'"}
{"text": "table: 1-1137142-1\ncolumns: Season, Group A Winner, Group B Winner, Group C Winner, Group D Winner\nQ: What is the group a winner for modena?\nA: SELECT Group A Winner FROM 1-1137142-1 WHERE Group B Winner = 'Modena'"}
{"text": "table: 1-1137142-1\ncolumns: Season, Group A Winner, Group B Winner, Group C Winner, Group D Winner\nQ: What is the group a winner for vis pesaro?\nA: SELECT Group A Winner FROM 1-1137142-1 WHERE Group C Winner = 'Vis Pesaro'"}
{"text": "table: 1-1137142-1\ncolumns: Season, Group A Winner, Group B Winner, Group C Winner, Group D Winner\nQ: What group a winner was for nocerina?\nA: SELECT Group A Winner FROM 1-1137142-1 WHERE Group D Winner = 'Nocerina'"}
{"text": "table: 1-1137142-1\ncolumns: Season, Group A Winner, Group B Winner, Group C Winner, Group D Winner\nQ: What was the group d winner for modena?\nA: SELECT Group D Winner FROM 1-1137142-1 WHERE Group B Winner = 'Modena'"}
{"text": "table: 1-1137695-3\ncolumns: Round, Grand Prix, Pole Position, Fastest Lap, Winning Driver, Winning Constructor, Report\nQ: Who had the fastest lap at the brazilian grand prix?\nA: SELECT Fastest Lap FROM 1-1137695-3 WHERE Grand Prix = 'Brazilian Grand Prix'"}
{"text": "table: 1-1137695-3\ncolumns: Round, Grand Prix, Pole Position, Fastest Lap, Winning Driver, Winning Constructor, Report\nQ: Who was on the pole position at the monaco grand prix?\nA: SELECT Pole Position FROM 1-1137695-3 WHERE Grand Prix = 'Monaco Grand Prix'"}
{"text": "table: 1-1137695-3\ncolumns: Round, Grand Prix, Pole Position, Fastest Lap, Winning Driver, Winning Constructor, Report\nQ: Who was the winning driver when Michael Schumacher had the pole and the fastest lap?\nA: SELECT Winning Driver FROM 1-1137695-3 WHERE Fastest Lap = 'Michael Schumacher' AND Pole Position = 'Michael Schumacher'"}
{"text": "table: 1-1137704-2\ncolumns: Round, Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Winning Constructor, Report\nQ: what are all the\u00a0location\u00a0where\u00a0date\u00a0is 5 april\nA: SELECT Location FROM 1-1137704-2 WHERE Date = '5 April'"}
{"text": "table: 1-1137704-2\ncolumns: Round, Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Winning Constructor, Report\nQ: what are all the\u00a0pole position\u00a0where\u00a0date\u00a0is 26 july\nA: SELECT Pole Position FROM 1-1137704-2 WHERE Date = '26 July'"}
{"text": "table: 1-1137704-2\ncolumns: Round, Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Winning Constructor, Report\nQ: who are all the\u00a0winning constructors\u00a0where\u00a0fastest lap\u00a0is riccardo patrese and\u00a0location\u00a0is interlagos\nA: SELECT Winning Constructor FROM 1-1137704-2 WHERE Fastest Lap = 'Riccardo Patrese' AND Location = 'Interlagos'"}
{"text": "table: 1-1137704-2\ncolumns: Round, Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Winning Constructor, Report\nQ: what are all the\u00a0report\u00a0where\u00a0winning constructor\u00a0is williams - renault and\u00a0grand prix\u00a0is south african grand prix\nA: SELECT Report FROM 1-1137704-2 WHERE Winning Constructor = 'Williams - Renault' AND Grand Prix = 'South African Grand Prix'"}
{"text": "table: 1-1137704-2\ncolumns: Round, Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Winning Constructor, Report\nQ: whatthe minimum\u00a0round\u00a0where\u00a0grand prix\u00a0is german grand prix\nA: SELECT MIN Round FROM 1-1137704-2 WHERE Grand Prix = 'German Grand Prix'"}
{"text": "table: 1-1137704-2\ncolumns: Round, Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Winning Constructor, Report\nQ: what of the total number of\u00a0date\u00a0where\u00a0grand prix\u00a0is portuguese grand prix\nA: SELECT COUNT Date FROM 1-1137704-2 WHERE Grand Prix = 'Portuguese Grand Prix'"}
{"text": "table: 1-1137707-2\ncolumns: Round, Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Winning Constructor, Report\nQ: What is the number of pole position with a round of 15?\nA: SELECT COUNT Pole Position FROM 1-1137707-2 WHERE Round = 15"}
{"text": "table: 1-1137707-2\ncolumns: Round, Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Winning Constructor, Report\nQ: What is the date of the circuit gilles villeneuve?\nA: SELECT Date FROM 1-1137707-2 WHERE Location = 'Circuit Gilles Villeneuve'"}
{"text": "table: 1-1137707-2\ncolumns: Round, Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Winning Constructor, Report\nQ: What is the location of thierry boutsen?\nA: SELECT Location FROM 1-1137707-2 WHERE Fastest Lap = 'Thierry Boutsen'"}
{"text": "table: 1-1137718-2\ncolumns: Rd., Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Constructor, Report\nQ: Who had the pole position at the German Grand Prix?\nA: SELECT Pole Position FROM 1-1137718-2 WHERE Grand Prix = 'German Grand Prix'"}
{"text": "table: 1-1137718-2\ncolumns: Rd., Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Constructor, Report\nQ: Which rd. occurred on 22 October?\nA: SELECT MIN Rd. FROM 1-1137718-2 WHERE Date = '22 October'"}
{"text": "table: 1-1137718-2\ncolumns: Rd., Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Constructor, Report\nQ: Who was the winning driver on 13 August?\nA: SELECT Winning Driver FROM 1-1137718-2 WHERE Date = '13 August'"}
{"text": "table: 1-1137718-2\ncolumns: Rd., Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Constructor, Report\nQ: What was the fastest lap at the Mexican Grand Prix?\nA: SELECT Fastest Lap FROM 1-1137718-2 WHERE Grand Prix = 'Mexican Grand Prix'"}
{"text": "table: 1-1137718-2\ncolumns: Rd., Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Constructor, Report\nQ: Which rd. took place at Hockenheimring?\nA: SELECT MIN Rd. FROM 1-1137718-2 WHERE Location = 'Hockenheimring'"}
{"text": "table: 1-1137718-2\ncolumns: Rd., Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Constructor, Report\nQ: How many drivers had the fastest lap at Silverstone?\nA: SELECT COUNT Fastest Lap FROM 1-1137718-2 WHERE Location = 'Silverstone'"}
{"text": "table: 1-11381701-3\ncolumns: Source, Date, Method, iOS, Android, BlackBerry, Symbian / Series 40, Bada, Windows, Other\nQ: What is the percentage of Android use when Windows is 1.15%?\nA: SELECT Android FROM 1-11381701-3 WHERE Windows = '1.15%'"}
{"text": "table: 1-11381701-3\ncolumns: Source, Date, Method, iOS, Android, BlackBerry, Symbian / Series 40, Bada, Windows, Other\nQ: On which dates was the value of Bada 0.05%?\nA: SELECT Date FROM 1-11381701-3 WHERE Bada = '0.05%'"}
{"text": "table: 1-11381701-3\ncolumns: Source, Date, Method, iOS, Android, BlackBerry, Symbian / Series 40, Bada, Windows, Other\nQ: When the value of \"other\" is 0.7%, what is the percentage for Windows?\nA: SELECT Windows FROM 1-11381701-3 WHERE Other = '0.7%'"}
{"text": "table: 1-11381701-3\ncolumns: Source, Date, Method, iOS, Android, BlackBerry, Symbian / Series 40, Bada, Windows, Other\nQ: When Symbian/Series 40 is 0.40%, what is the percentage of \"other\"?\nA: SELECT Other FROM 1-11381701-3 WHERE Symbian / Series 40 = '0.40%'"}
{"text": "table: 1-11381701-3\ncolumns: Source, Date, Method, iOS, Android, BlackBerry, Symbian / Series 40, Bada, Windows, Other\nQ: Which source shows Blackberry at 2.9%?\nA: SELECT Source FROM 1-11381701-3 WHERE BlackBerry = '2.9%'"}
{"text": "table: 1-11390711-4\ncolumns: English Name, Japanese orthography, Pronouciation, abbreviation, Provider(IAI), Foundation\nQ: Which colleges have the english abbreviation MTC?\nA: SELECT English Name FROM 1-11390711-4 WHERE abbreviation = 'MTC'"}
{"text": "table: 1-11390711-4\ncolumns: English Name, Japanese orthography, Pronouciation, abbreviation, Provider(IAI), Foundation\nQ: What is the Japanese orthography for the English name National Farmers Academy?\nA: SELECT Japanese orthography FROM 1-11390711-4 WHERE English Name = 'National Farmers Academy'"}
{"text": "table: 1-11390711-4\ncolumns: English Name, Japanese orthography, Pronouciation, abbreviation, Provider(IAI), Foundation\nQ: What is the abbreviation for the college pronounced \"k\u014dk\u016b daigakk\u014d\"?\nA: SELECT abbreviation FROM 1-11390711-4 WHERE Pronouciation = 'K\u014dk\u016b Daigakk\u014d'"}
{"text": "table: 1-11390711-4\ncolumns: English Name, Japanese orthography, Pronouciation, abbreviation, Provider(IAI), Foundation\nQ: How many providers were founded in 1964?\nA: SELECT COUNT Provider(IAI) FROM 1-11390711-4 WHERE Foundation = 1964"}
{"text": "table: 1-11390711-4\ncolumns: English Name, Japanese orthography, Pronouciation, abbreviation, Provider(IAI), Foundation\nQ: What is the Japanese orthography for National Fisheries University?\nA: SELECT Japanese orthography FROM 1-11390711-4 WHERE English Name = 'National Fisheries University'"}
{"text": "table: 1-11391954-3\ncolumns: Country, Total, Marathon (mens), Marathon (womens), Half Marathon (mens), Half Marathon (womens)\nQ: What is the minimum number for the half marathon (womens)?\nA: SELECT MIN Half Marathon (womens) FROM 1-11391954-3"}
{"text": "table: 1-11391954-3\ncolumns: Country, Total, Marathon (mens), Marathon (womens), Half Marathon (mens), Half Marathon (womens)\nQ: Whatis the total number of half marathon (mens) that represented kazakhstan?\nA: SELECT COUNT Half Marathon (mens) FROM 1-11391954-3 WHERE Country = 'Kazakhstan'"}
{"text": "table: 1-11391954-3\ncolumns: Country, Total, Marathon (mens), Marathon (womens), Half Marathon (mens), Half Marathon (womens)\nQ: What is amount of countries where half marathon (women) is larger than 1.0?\nA: SELECT COUNT Country FROM 1-11391954-3 WHERE Half Marathon (womens) > 1.0"}
{"text": "table: 1-11391954-3\ncolumns: Country, Total, Marathon (mens), Marathon (womens), Half Marathon (mens), Half Marathon (womens)\nQ: How many times is Moldova the winner of half marathon (womens)?\nA: SELECT COUNT Half Marathon (womens) FROM 1-11391954-3 WHERE Country = 'Moldova'"}
{"text": "table: 1-11391954-3\ncolumns: Country, Total, Marathon (mens), Marathon (womens), Half Marathon (mens), Half Marathon (womens)\nQ: Which country has half marathon (womens) that is larger than 1.0?\nA: SELECT Country FROM 1-11391954-3 WHERE Half Marathon (womens) > 1.0"}
{"text": "table: 1-1139087-2\ncolumns: Rd., Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Constructor, Report\nQ: What is the make of the car that won the brazilian grand prix?\nA: SELECT Constructor FROM 1-1139087-2 WHERE Grand Prix = 'Brazilian Grand Prix'"}
{"text": "table: 1-1139087-2\ncolumns: Rd., Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Constructor, Report\nQ: Who drove the fastest lap for round 8?\nA: SELECT Fastest Lap FROM 1-1139087-2 WHERE Rd. = 8"}
{"text": "table: 1-1139087-2\ncolumns: Rd., Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Constructor, Report\nQ: What day was the grand prix in jerez?\nA: SELECT Date FROM 1-1139087-2 WHERE Location = 'Jerez'"}
{"text": "table: 1-1139087-2\ncolumns: Rd., Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Constructor, Report\nQ: What event was in detroit?\nA: SELECT Grand Prix FROM 1-1139087-2 WHERE Location = 'Detroit'"}
{"text": "table: 1-1139087-2\ncolumns: Rd., Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Constructor, Report\nQ: How many events did nigel mansell drive the fastest and a mclaren - honda win?\nA: SELECT COUNT Grand Prix FROM 1-1139087-2 WHERE Constructor = 'McLaren - Honda' AND Fastest Lap = 'Nigel Mansell'"}
{"text": "table: 1-1139087-2\ncolumns: Rd., Grand Prix, Date, Location, Pole Position, Fastest Lap, Winning Driver, Constructor, Report\nQ: What day is the french grand prix\nA: SELECT Date FROM 1-1139087-2 WHERE Grand Prix = 'French Grand Prix'"}
{"text": "table: 1-1139835-3\ncolumns: Year, Winners, Grand Finalist, Scores, Venue, Crowd, Margin, Season Result\nQ:  who is the\u00a0winners\u00a0where\u00a0season result\u00a0is 7th\nA: SELECT Winners FROM 1-1139835-3 WHERE Season Result = '7th'"}
{"text": "table: 1-1139835-3\ncolumns: Year, Winners, Grand Finalist, Scores, Venue, Crowd, Margin, Season Result\nQ:  who is the\u00a0winners\u00a0where\u00a0season result\u00a0is 9th\nA: SELECT Winners FROM 1-1139835-3 WHERE Season Result = '9th'"}
{"text": "table: 1-1139835-3\ncolumns: Year, Winners, Grand Finalist, Scores, Venue, Crowd, Margin, Season Result\nQ:  what's the\u00a0grand finalist\u00a0where\u00a0winners\u00a0is collingwood\nA: SELECT Grand Finalist FROM 1-1139835-3 WHERE Winners = 'Collingwood'"}
{"text": "table: 1-1139835-3\ncolumns: Year, Winners, Grand Finalist, Scores, Venue, Crowd, Margin, Season Result\nQ:  who is the\u00a0season result\u00a0where\u00a0margin\u00a0is 51\nA: SELECT Season Result FROM 1-1139835-3 WHERE Margin = 51"}
{"text": "table: 1-1139835-3\ncolumns: Year, Winners, Grand Finalist, Scores, Venue, Crowd, Margin, Season Result\nQ:  who is the\u00a0grand finalist\u00a0where\u00a0scores\u00a0is 11.11 (77) \u2013 10.8 (68)\nA: SELECT Grand Finalist FROM 1-1139835-3 WHERE Scores = '11.11 (77) \u2013 10.8 (68)'"}
{"text": "table: 1-1139835-3\ncolumns: Year, Winners, Grand Finalist, Scores, Venue, Crowd, Margin, Season Result\nQ:  who is the\u00a0grand finalist\u00a0where\u00a0scores\u00a0is 8.9 (57) \u2013 7.12 (54)\nA: SELECT Grand Finalist FROM 1-1139835-3 WHERE Scores = '8.9 (57) \u2013 7.12 (54)'"}
{"text": "table: 1-1139835-1\ncolumns: Year, Winners, Grand Finalist, Scores, Venue, Crowd, Margin, Season Result\nQ: what was the crowd when the scores are 10.12 (72) \u2013 8.11 (59)?\nA: SELECT MAX Crowd FROM 1-1139835-1 WHERE Scores = '10.12 (72) \u2013 8.11 (59)'"}
{"text": "table: 1-1139835-1\ncolumns: Year, Winners, Grand Finalist, Scores, Venue, Crowd, Margin, Season Result\nQ: what is the venue where the scores are 15.13 (103) \u2013 8.4 (52)?\nA: SELECT Venue FROM 1-1139835-1 WHERE Scores = '15.13 (103) \u2013 8.4 (52)'"}
{"text": "table: 1-1139835-1\ncolumns: Year, Winners, Grand Finalist, Scores, Venue, Crowd, Margin, Season Result\nQ: what is the venue where the margin is 4?\nA: SELECT Venue FROM 1-1139835-1 WHERE Margin = 4"}
{"text": "table: 1-1139835-1\ncolumns: Year, Winners, Grand Finalist, Scores, Venue, Crowd, Margin, Season Result\nQ: what is the crowd when the grand finalist was south melbourne?\nA: SELECT Crowd FROM 1-1139835-1 WHERE Grand Finalist = 'South Melbourne'"}
{"text": "table: 1-1140067-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: What was the date for monaco grand prix?\nA: SELECT Date FROM 1-1140067-2 WHERE Race = 'Monaco Grand Prix'"}
{"text": "table: 1-1140067-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: What was the date for the pole position of alain prost?\nA: SELECT Date FROM 1-1140067-2 WHERE Pole Position = 'Alain Prost'"}
{"text": "table: 1-1140067-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: What is the race winer of the portuguese grand prix?\nA: SELECT Race Winner FROM 1-1140067-2 WHERE Race = 'Portuguese Grand Prix'"}
{"text": "table: 1-1140074-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: what's the\u00a0race winner\u00a0with\u00a0date\u00a0being 12 june\nA: SELECT Race Winner FROM 1-1140074-2 WHERE Date = '12 June'"}
{"text": "table: 1-1140074-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: what's the\u00a0constructor\u00a0with\u00a0location\u00a0being hockenheimring\nA: SELECT Constructor FROM 1-1140074-2 WHERE Location = 'Hockenheimring'"}
{"text": "table: 1-1140074-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: what's the\u00a0race winner\u00a0with\u00a0location\u00a0being jacarepagu\u00e1\nA: SELECT Race Winner FROM 1-1140074-2 WHERE Location = 'Jacarepagu\u00e1'"}
{"text": "table: 1-1140074-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: what's the total number of\u00a0race winner\u00a0with\u00a0rnd\u00a0being 10\nA: SELECT COUNT Race Winner FROM 1-1140074-2 WHERE Rnd = 10"}
{"text": "table: 1-1140074-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: what's the\u00a0pole position\u00a0with\u00a0location\u00a0being hockenheimring\nA: SELECT Pole Position FROM 1-1140074-2 WHERE Location = 'Hockenheimring'"}
{"text": "table: 1-1140074-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: what's the\u00a0report\u00a0with\u00a0rnd\u00a0being 4\nA: SELECT Report FROM 1-1140074-2 WHERE Rnd = 4"}
{"text": "table: 1-1139835-9\ncolumns: Season, Premier, Runner Up, Score, Venue, Attendance, Premiership\nQ: What venue has an attendance of 30824 at Essendon in 1984?\nA: SELECT Venue FROM 1-1139835-9 WHERE Premier = 'Essendon' AND Attendance = 30824"}
{"text": "table: 1-1139835-9\ncolumns: Season, Premier, Runner Up, Score, Venue, Attendance, Premiership\nQ: What other venue was a runner up to Hawthorn?\nA: SELECT Venue FROM 1-1139835-9 WHERE Runner Up = 'Hawthorn'"}
{"text": "table: 1-1139835-9\ncolumns: Season, Premier, Runner Up, Score, Venue, Attendance, Premiership\nQ: What is the other premiership when the runner up wis Geelong?\nA: SELECT Premiership FROM 1-1139835-9 WHERE Runner Up = 'Geelong'"}
{"text": "table: 1-1139835-9\ncolumns: Season, Premier, Runner Up, Score, Venue, Attendance, Premiership\nQ: Who are all the runner ups when the score is 9.12 (66) \u2013 5.6 (36)?\nA: SELECT Runner Up FROM 1-1139835-9 WHERE Score = '9.12 (66) \u2013 5.6 (36)'"}
{"text": "table: 1-1140073-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: Who had the fastest lap in the race where Patrick Tambay was on the pole?\nA: SELECT Fastest Lap FROM 1-1140073-2 WHERE Pole Position = 'Patrick Tambay'"}
{"text": "table: 1-1140073-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: What race had Nelson Piquet on the pole and was in N\u00fcrburgring?\nA: SELECT Race FROM 1-1140073-2 WHERE Pole Position = 'Nelson Piquet' AND Location = 'N\u00fcrburgring'"}
{"text": "table: 1-1140073-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: How many rounds did Patrick Tambay record the fastest lap?\nA: SELECT COUNT Rnd FROM 1-1140073-2 WHERE Fastest Lap = 'Patrick Tambay'"}
{"text": "table: 1-1140073-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: Which race is located in kyalami?\nA: SELECT Race FROM 1-1140073-2 WHERE Location = 'Kyalami'"}
{"text": "table: 1-1140077-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: What is the fastest lap with pole position of gilles villeneuve?\nA: SELECT Fastest Lap FROM 1-1140077-2 WHERE Pole Position = 'Gilles Villeneuve'"}
{"text": "table: 1-1140077-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: Who did the fastest lap in the dutch grand prix?\nA: SELECT Fastest Lap FROM 1-1140077-2 WHERE Race = 'Dutch Grand Prix'"}
{"text": "table: 1-1140077-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: Who did the fastest lap with the race winner john watson?\nA: SELECT Fastest Lap FROM 1-1140077-2 WHERE Race Winner = 'John Watson'"}
{"text": "table: 1-1140076-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: What is the constructor for 9 May?\nA: SELECT Constructor FROM 1-1140076-2 WHERE Date = '9 May'"}
{"text": "table: 1-1140076-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: What is the pole position for the race with the fastest lap by Nelson Piquet and the constructor is Ferrari?\nA: SELECT Pole Position FROM 1-1140076-2 WHERE Fastest Lap = 'Nelson Piquet' AND Constructor = 'Ferrari'"}
{"text": "table: 1-1140076-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: What is the report listed for the race in San Marino Grand Prix?\nA: SELECT Report FROM 1-1140076-2 WHERE Race = 'San Marino Grand Prix'"}
{"text": "table: 1-1140076-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: Who was the constructor in the location Monza?\nA: SELECT Constructor FROM 1-1140076-2 WHERE Location = 'Monza'"}
{"text": "table: 1-1140076-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: How many races had the pole position Alain Prost and the race winner Keke Rosberg?\nA: SELECT COUNT Race FROM 1-1140076-2 WHERE Pole Position = 'Alain Prost' AND Race Winner = 'Keke Rosberg'"}
{"text": "table: 1-1140080-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: what's the\u00a0report\u00a0with\u00a0location\u00a0 \u00f6sterreichring\nA: SELECT Report FROM 1-1140080-2 WHERE Location = '\u00d6sterreichring'"}
{"text": "table: 1-1140080-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: what's the\u00a0report\u00a0with\u00a0race\u00a0argentine grand prix\nA: SELECT Report FROM 1-1140080-2 WHERE Race = 'Argentine Grand Prix'"}
{"text": "table: 1-1140080-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: what's the minimum\u00a0rnd\u00a0with\u00a0race\u00a0 italian grand prix\nA: SELECT MIN Rnd FROM 1-1140080-2 WHERE Race = 'Italian Grand Prix'"}
{"text": "table: 1-1140080-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: what's the total number of\u00a0report\u00a0with\u00a0date\u00a0 29 april\nA: SELECT COUNT Report FROM 1-1140080-2 WHERE Date = '29 April'"}
{"text": "table: 1-1140080-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: what's the\u00a0race winner\u00a0with\u00a0constructor\u00a0 renault\nA: SELECT Race Winner FROM 1-1140080-2 WHERE Constructor = 'Renault'"}
{"text": "table: 1-1140080-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: what's the\u00a0date\u00a0with\u00a0rnd\u00a0 1\nA: SELECT Date FROM 1-1140080-2 WHERE Rnd = 1"}
{"text": "table: 1-1140083-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: How many days is the Monaco Grand Prix?\nA: SELECT COUNT Date FROM 1-1140083-2 WHERE Race = 'Monaco Grand Prix'"}
{"text": "table: 1-1140083-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: How many rounds were won with James Hunt as pole position and John Watson as  fastest lap?\nA: SELECT COUNT Rnd FROM 1-1140083-2 WHERE Pole Position = 'James Hunt' AND Fastest Lap = 'John Watson'"}
{"text": "table: 1-1140083-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: The Dijon-prenois had how many fastest laps?\nA: SELECT COUNT Fastest Lap FROM 1-1140083-2 WHERE Location = 'Dijon-Prenois'"}
{"text": "table: 1-1140083-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: What was the constructor for round 15?\nA: SELECT Constructor FROM 1-1140083-2 WHERE Rnd = 15"}
{"text": "table: 1-1140088-6\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: Who won the Brands Hatch circuit?\nA: SELECT Winning driver FROM 1-1140088-6 WHERE Circuit = 'Brands Hatch'"}
{"text": "table: 1-1140088-6\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: Who constructed the I Italian Republic Grand Prix?\nA: SELECT Constructor FROM 1-1140088-6 WHERE Race Name = 'I Italian Republic Grand Prix'"}
{"text": "table: 1-1140088-6\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: What race was held at Oulton Park?\nA: SELECT Race Name FROM 1-1140088-6 WHERE Circuit = 'Oulton Park'"}
{"text": "table: 1-1140088-6\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: Did the I Brazilian Grand Prix have a report?\nA: SELECT Report FROM 1-1140088-6 WHERE Race Name = 'I Brazilian Grand Prix'"}
{"text": "table: 1-1140085-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: what is the race where the pole position is niki lauda and the date is 27 april?\nA: SELECT Race FROM 1-1140085-2 WHERE Pole Position = 'Niki Lauda' AND Date = '27 April'"}
{"text": "table: 1-1140085-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: what is the date where the constructor is ferrari and the location is anderstorp?\nA: SELECT Date FROM 1-1140085-2 WHERE Constructor = 'Ferrari' AND Location = 'Anderstorp'"}
{"text": "table: 1-1140085-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: how many times is the pole position niki lauda and the race is monaco grand prix?\nA: SELECT COUNT Rnd FROM 1-1140085-2 WHERE Pole Position = 'Niki Lauda' AND Race = 'Monaco Grand Prix'"}
{"text": "table: 1-1140085-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: what is the report where the location is kyalami?\nA: SELECT Report FROM 1-1140085-2 WHERE Location = 'Kyalami'"}
{"text": "table: 1-1140085-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: who is the pole position for the rnd 3\nA: SELECT Pole Position FROM 1-1140085-2 WHERE Rnd = 3"}
{"text": "table: 1-1140085-2\ncolumns: Rnd, Race, Date, Location, Pole Position, Fastest Lap, Race Winner, Constructor, Report\nQ: what is the race where the fastest lap is by jean-pierre jarier?\nA: SELECT Race FROM 1-1140085-2 WHERE Fastest Lap = 'Jean-Pierre Jarier'"}
{"text": "table: 1-1140090-6\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: What circuit did Clay Regazzoni win?\nA: SELECT Circuit FROM 1-1140090-6 WHERE Winning driver = 'Clay Regazzoni'"}
{"text": "table: 1-1140090-6\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: What was the date when Chris Amon won?\nA: SELECT Date FROM 1-1140090-6 WHERE Winning driver = 'Chris Amon'"}
{"text": "table: 1-1140090-6\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: What circuit is the Vi Rhein-Pokalrennen race in?\nA: SELECT Circuit FROM 1-1140090-6 WHERE Race Name = 'VI Rhein-Pokalrennen'"}
{"text": "table: 1-1140103-6\ncolumns: #, Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: What date is listed at place 13\nA: SELECT Date FROM 1-1140103-6 WHERE # = 13"}
{"text": "table: 1-1140103-6\ncolumns: #, Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: What date has a solitudering circuit\nA: SELECT Date FROM 1-1140103-6 WHERE Circuit = 'Solitudering'"}
{"text": "table: 1-1140103-6\ncolumns: #, Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: How many dates have silverstone circuit\nA: SELECT COUNT Date FROM 1-1140103-6 WHERE Circuit = 'Silverstone'"}
{"text": "table: 1-1140103-6\ncolumns: #, Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: How many constructors are listed for the XVI BRDC international trophy race\nA: SELECT COUNT Constructor FROM 1-1140103-6 WHERE Race Name = 'XVI BRDC International Trophy'"}
{"text": "table: 1-1140105-6\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: What is the name of the circuit in which the race name is ii danish grand prix?\nA: SELECT Circuit FROM 1-1140105-6 WHERE Race Name = 'II Danish Grand Prix'"}
{"text": "table: 1-1140105-6\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: What is te name of the constructors dated 26 march?\nA: SELECT Constructor FROM 1-1140105-6 WHERE Date = '26 March'"}
{"text": "table: 1-1140105-6\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: What is the total amount of circuts dated 22 april?\nA: SELECT COUNT Circuit FROM 1-1140105-6 WHERE Date = '22 April'"}
{"text": "table: 1-1140105-6\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: what is the name of the constructor that has the circuit zeltweg airfield?\nA: SELECT Constructor FROM 1-1140105-6 WHERE Circuit = 'Zeltweg Airfield'"}
{"text": "table: 1-1140105-6\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: What is the name of the winning driver where the circuit name is posillipo?\nA: SELECT Winning driver FROM 1-1140105-6 WHERE Circuit = 'Posillipo'"}
{"text": "table: 1-1140105-6\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: What is the name of the circuit where the race xi Syracuse grand prix was held?\nA: SELECT Circuit FROM 1-1140105-6 WHERE Race Name = 'XI Syracuse Grand Prix'"}
{"text": "table: 1-1140111-5\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: What kind of report is for the Pau circuit?\nA: SELECT Report FROM 1-1140111-5 WHERE Circuit = 'Pau'"}
{"text": "table: 1-1140111-5\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: How many different kinds of reports are there for races that Juan Manuel Fangio won?\nA: SELECT COUNT Report FROM 1-1140111-5 WHERE Winning driver = 'Juan Manuel Fangio'"}
{"text": "table: 1-1140111-5\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: Who constructed the Syracuse circuit?\nA: SELECT Constructor FROM 1-1140111-5 WHERE Circuit = 'Syracuse'"}
{"text": "table: 1-1140116-5\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: What is the name of the race in the Modena circuit?\nA: SELECT Race Name FROM 1-1140116-5 WHERE Circuit = 'Modena'"}
{"text": "table: 1-1140116-5\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: What is the race name in the Monza circuit?\nA: SELECT Race Name FROM 1-1140116-5 WHERE Circuit = 'Monza'"}
{"text": "table: 1-1140116-5\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: When does V Madgwick Cup take place?\nA: SELECT Date FROM 1-1140116-5 WHERE Race Name = 'V Madgwick Cup'"}
{"text": "table: 1-1140116-5\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: Which driver won the race xiv el\u00e4intarhanajot?\nA: SELECT Winning driver FROM 1-1140116-5 WHERE Race Name = 'XIV El\u00e4intarhanajot'"}
{"text": "table: 1-1140116-5\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: Who won the Modena circuit?\nA: SELECT Winning driver FROM 1-1140116-5 WHERE Circuit = 'Modena'"}
{"text": "table: 1-1140113-5\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: How many constructors won the III Redex Trophy?\nA: SELECT COUNT Constructor FROM 1-1140113-5 WHERE Race Name = 'III RedeX Trophy'"}
{"text": "table: 1-1140113-5\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: What was the report of Mike Hawthorn's winning race?\nA: SELECT Report FROM 1-1140113-5 WHERE Winning driver = 'Mike Hawthorn'"}
{"text": "table: 1-1140113-5\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: What was the name of the race in Bordeaux?\nA: SELECT Race Name FROM 1-1140113-5 WHERE Circuit = 'Bordeaux'"}
{"text": "table: 1-1140117-5\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: What is the report for the race name V Ulster Trophy?\nA: SELECT Report FROM 1-1140117-5 WHERE Race Name = 'V Ulster Trophy'"}
{"text": "table: 1-1140117-5\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: What is the constructor for the Silverstone circuit?\nA: SELECT Constructor FROM 1-1140117-5 WHERE Circuit = 'Silverstone'"}
{"text": "table: 1-1140117-5\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: What's the report for the Silverstone circuit?\nA: SELECT Report FROM 1-1140117-5 WHERE Circuit = 'Silverstone'"}
{"text": "table: 1-1140117-5\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: What's the report for the race name, XIII Grand Prix de l'Albigeois?\nA: SELECT Report FROM 1-1140117-5 WHERE Race Name = 'XIII Grand Prix de l'Albigeois'"}
{"text": "table: 1-1140117-5\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: Which date did the race name XII Pau Grand Prix take place on?\nA: SELECT Date FROM 1-1140117-5 WHERE Race Name = 'XII Pau Grand Prix'"}
{"text": "table: 1-1140117-5\ncolumns: Race Name, Circuit, Date, Winning driver, Constructor, Report\nQ: Who was the winning driver for the goodwood circuit?\nA: SELECT Winning driver FROM 1-1140117-5 WHERE Circuit = 'Goodwood'"}
{"text": "table: 1-11411026-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: How many millions of U.S. viewers whatched episodes written by Krystal Houghton?\nA: SELECT U.S. viewers (millions) FROM 1-11411026-2 WHERE Written by = 'Krystal Houghton'"}
{"text": "table: 1-11411026-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: How many titles were directed in series 79?\nA: SELECT COUNT Directed by FROM 1-11411026-2 WHERE No. in series = 79"}
{"text": "table: 1-11411026-2\ncolumns: No. in series, No. in season, Title, Directed by, Written by, Original air date, U.S. viewers (millions)\nQ: Who wrote an episode watched by 19.01 million US viewers?\nA: SELECT Written by FROM 1-11411026-2 WHERE U.S. viewers (millions) = '19.01'"}
{"text": "table: 1-11404452-1\ncolumns: Series #, Episode title, Writer(s), Director, U.S. viewers (millions), Original air date\nQ: What are the titles of the episodes where Rodman Flender is the director?\nA: SELECT Episode title FROM 1-11404452-1 WHERE Director = 'Rodman Flender'"}
{"text": "table: 1-11404452-1\ncolumns: Series #, Episode title, Writer(s), Director, U.S. viewers (millions), Original air date\nQ: What is the original air date of the Jamie Babbit directed episode?\nA: SELECT Original air date FROM 1-11404452-1 WHERE Director = 'Jamie Babbit'"}
{"text": "table: 1-11404452-1\ncolumns: Series #, Episode title, Writer(s), Director, U.S. viewers (millions), Original air date\nQ: What is the original air date when there were 12.81 million u.s viewers?\nA: SELECT Original air date FROM 1-11404452-1 WHERE U.S. viewers (millions) = '12.81'"}
{"text": "table: 1-11404452-1\ncolumns: Series #, Episode title, Writer(s), Director, U.S. viewers (millions), Original air date\nQ: When did Shelia Lawrence join the series?\nA: SELECT MIN Series # FROM 1-11404452-1 WHERE Writer(s) = 'Shelia Lawrence'"}
{"text": "table: 1-11404452-1\ncolumns: Series #, Episode title, Writer(s), Director, U.S. viewers (millions), Original air date\nQ: Who was the director when there were 13.66 million u.s viewers?\nA: SELECT Director FROM 1-11404452-1 WHERE U.S. viewers (millions) = '13.66'"}
{"text": "table: 1-1143966-1\ncolumns: Season, Games, Won, Lost, Tied, Points, Pct %, Goals for, Goals against, Standing\nQ: Name the percentage where the amount won was 25\nA: SELECT Pct % FROM 1-1143966-1 WHERE Won = 25"}
{"text": "table: 1-1143966-1\ncolumns: Season, Games, Won, Lost, Tied, Points, Pct %, Goals for, Goals against, Standing\nQ: How many games were won with 2nd oha was standing and there were 62 games?\nA: SELECT Won FROM 1-1143966-1 WHERE Standing = '2nd OHA' AND Games = 62"}
{"text": "table: 1-11447995-2\ncolumns: Ward, Bello, Ben-Tahir, Doucet, Furtenbacher, Gauthier, Haydon, Larter, Lawrance, Libweshya, Liscumb\nQ: What is the Liscumb when Gauthier is 34?\nA: SELECT Liscumb FROM 1-11447995-2 WHERE Gauthier = '34'"}
{"text": "table: 1-11447995-2\ncolumns: Ward, Bello, Ben-Tahir, Doucet, Furtenbacher, Gauthier, Haydon, Larter, Lawrance, Libweshya, Liscumb\nQ: What is the Bello when Ben-Tahir is 296?\nA: SELECT Bello FROM 1-11447995-2 WHERE Ben-Tahir = '296'"}
{"text": "table: 1-11447995-2\ncolumns: Ward, Bello, Ben-Tahir, Doucet, Furtenbacher, Gauthier, Haydon, Larter, Lawrance, Libweshya, Liscumb\nQ: What is Ben-Tahir when Bello is 51?\nA: SELECT Ben-Tahir FROM 1-11447995-2 WHERE Bello = '51'"}
{"text": "table: 1-11447995-2\ncolumns: Ward, Bello, Ben-Tahir, Doucet, Furtenbacher, Gauthier, Haydon, Larter, Lawrance, Libweshya, Liscumb\nQ: What is Haydon when Larter is 11 and Libweshya is 4?\nA: SELECT Haydon FROM 1-11447995-2 WHERE Larter = '11' AND Libweshya = '4'"}
{"text": "table: 1-11447995-2\ncolumns: Ward, Bello, Ben-Tahir, Doucet, Furtenbacher, Gauthier, Haydon, Larter, Lawrance, Libweshya, Liscumb\nQ: What is Liscumb when Haydon is 1632?\nA: SELECT Liscumb FROM 1-11447995-2 WHERE Haydon = '1632'"}
{"text": "table: 1-11447995-2\ncolumns: Ward, Bello, Ben-Tahir, Doucet, Furtenbacher, Gauthier, Haydon, Larter, Lawrance, Libweshya, Liscumb\nQ: What is Doucet when Lawrance is 36?\nA: SELECT Doucet FROM 1-11447995-2 WHERE Lawrance = '36'"}
{"text": "table: 1-11449590-2\ncolumns: Week, Date, Opponent, Result, Kickoff [a ], Game site, TV, Attendance, Record\nQ: Which tv had the date december 7, 1986?\nA: SELECT TV FROM 1-11449590-2 WHERE Date = 'December 7, 1986'"}
{"text": "table: 1-11449590-2\ncolumns: Week, Date, Opponent, Result, Kickoff [a ], Game site, TV, Attendance, Record\nQ: Which kickoff has the opponent at new orleans saints?\nA: SELECT Kickoff [a ] FROM 1-11449590-2 WHERE Opponent = 'at New Orleans Saints'"}
{"text": "table: 1-11464746-1\ncolumns: House Name, Composition, Named after, Founded, Colours\nQ: How many houses are green?\nA: SELECT COUNT House Name FROM 1-11464746-1 WHERE Colours = 'Green'"}
{"text": "table: 1-11464746-1\ncolumns: House Name, Composition, Named after, Founded, Colours\nQ: What year was the house named gongola made?\nA: SELECT Founded FROM 1-11464746-1 WHERE House Name = 'Gongola'"}
{"text": "table: 1-11464746-1\ncolumns: House Name, Composition, Named after, Founded, Colours\nQ: What is the name of the green house?\nA: SELECT House Name FROM 1-11464746-1 WHERE Colours = 'Green'"}
{"text": "table: 1-11464746-1\ncolumns: House Name, Composition, Named after, Founded, Colours\nQ: What is the green house made of?\nA: SELECT Composition FROM 1-11464746-1 WHERE Colours = 'Green'"}
{"text": "table: 1-11464746-1\ncolumns: House Name, Composition, Named after, Founded, Colours\nQ: What is the benue house made of?\nA: SELECT Composition FROM 1-11464746-1 WHERE House Name = 'Benue'"}
{"text": "table: 1-11465521-2\ncolumns: Week, Date, Opponent, Result, Kickoff [a ], Game site, TV, Attendance, Record\nQ: In which week was the game against a team with a record of 3-6 played?\nA: SELECT COUNT Week FROM 1-11465521-2 WHERE Record = '3-6'"}
{"text": "table: 1-11465521-2\ncolumns: Week, Date, Opponent, Result, Kickoff [a ], Game site, TV, Attendance, Record\nQ: Which channel had the game against the Minnesota Vikings?\nA: SELECT TV FROM 1-11465521-2 WHERE Opponent = 'Minnesota Vikings'"}
{"text": "table: 1-11465521-2\ncolumns: Week, Date, Opponent, Result, Kickoff [a ], Game site, TV, Attendance, Record\nQ: How many opponents were there at the game with 64,087 people in attendance?\nA: SELECT COUNT Opponent FROM 1-11465521-2 WHERE Attendance = '64,087'"}
{"text": "table: 1-11452830-2\ncolumns: Week, Date, Opponent, Result, Kickoff [a ], Game site, TV, Attendance, Record\nQ: Where was the game played when the team's record was 1-3? \nA: SELECT Game site FROM 1-11452830-2 WHERE Record = '1-3'"}
{"text": "table: 1-11452830-2\ncolumns: Week, Date, Opponent, Result, Kickoff [a ], Game site, TV, Attendance, Record\nQ: How many times was there a kickoff in the September 4, 1988 game? \nA: SELECT COUNT Kickoff [a ] FROM 1-11452830-2 WHERE Date = 'September 4, 1988'"}
{"text": "table: 1-11452830-2\ncolumns: Week, Date, Opponent, Result, Kickoff [a ], Game site, TV, Attendance, Record\nQ: How many crowds watched the game where the record was 1-3? \nA: SELECT COUNT Attendance FROM 1-11452830-2 WHERE Record = '1-3'"}
{"text": "table: 1-1149495-1\ncolumns: Series, Year, Winner, Runner-up, Third place, Fourth place, Fifth place, Sixth place, Host\nQ: What year finished with Daniel Zueras as the runner-up?\nA: SELECT COUNT Year FROM 1-1149495-1 WHERE Runner-up = 'Daniel Zueras'"}
{"text": "table: 1-1149495-1\ncolumns: Series, Year, Winner, Runner-up, Third place, Fourth place, Fifth place, Sixth place, Host\nQ: How many people hosted the show in the year when Chenoa  ended up in fourth place?\nA: SELECT COUNT Host FROM 1-1149495-1 WHERE Fourth place = 'Chenoa'"}
{"text": "table: 1-1149495-1\ncolumns: Series, Year, Winner, Runner-up, Third place, Fourth place, Fifth place, Sixth place, Host\nQ: How many fourth places were there in 2003?\nA: SELECT COUNT Fourth place FROM 1-1149495-1 WHERE Year = '2003'"}
{"text": "table: 1-1147705-1\ncolumns: model, max. motive power, max. torque at rpm, engine displacement, engine type, engine configuration & notes 0-100km/h\nQ: What is the engine type when the max torque at rpm is n\u00b7m ( lbf\u00b7ft ) @ 4,800 Answers:?\nA: SELECT engine type FROM 1-1147705-1 WHERE max. torque at rpm = 'N\u00b7m ( lbf\u00b7ft ) @ 4,800'"}
{"text": "table: 1-1147705-1\ncolumns: model, max. motive power, max. torque at rpm, engine displacement, engine type, engine configuration & notes 0-100km/h\nQ: What is the engine configuration $notes 0-100km/h for the engine type b5244 t2?\nA: SELECT engine configuration & notes 0-100km/h FROM 1-1147705-1 WHERE engine type = 'B5244 T2'"}
{"text": "table: 1-1147705-1\ncolumns: model, max. motive power, max. torque at rpm, engine displacement, engine type, engine configuration & notes 0-100km/h\nQ: What is the engine displacement for the engine type b5254 t?\nA: SELECT engine displacement FROM 1-1147705-1 WHERE engine type = 'B5254 T'"}
{"text": "table: 1-1147705-1\ncolumns: model, max. motive power, max. torque at rpm, engine displacement, engine type, engine configuration & notes 0-100km/h\nQ: How many have are model 2.4 awd?\nA: SELECT COUNT engine type FROM 1-1147705-1 WHERE model = '2.4 AWD'"}
{"text": "table: 1-1147705-1\ncolumns: model, max. motive power, max. torque at rpm, engine displacement, engine type, engine configuration & notes 0-100km/h\nQ: How many engine b5204 t3?\nA: SELECT COUNT engine displacement FROM 1-1147705-1 WHERE engine type = 'B5204 T3'"}
{"text": "table: 1-1147705-1\ncolumns: model, max. motive power, max. torque at rpm, engine displacement, engine type, engine configuration & notes 0-100km/h\nQ: How many engine b5234 t3?\nA: SELECT COUNT model FROM 1-1147705-1 WHERE engine type = 'B5234 T3'"}
{"text": "table: 1-1147701-4\ncolumns: Model name, Power (ps), Torque (Nm@rpm), Displacement (cm\u00b3), Engine code, Comment\nQ:  how many\u00a0power (ps)\u00a0with\u00a0torque (nm@rpm)\u00a0being 240@2200-5000\nA: SELECT COUNT Power (ps) FROM 1-1147701-4 WHERE Torque (Nm@rpm) = '240@2200-5000'"}
{"text": "table: 1-1147701-4\ncolumns: Model name, Power (ps), Torque (Nm@rpm), Displacement (cm\u00b3), Engine code, Comment\nQ: what's the\u00a0comment\u00a0with\u00a0model name\u00a0being 2.4 (2001-2007)\nA: SELECT Comment FROM 1-1147701-4 WHERE Model name = '2.4 (2001-2007)'"}
{"text": "table: 1-1147701-4\ncolumns: Model name, Power (ps), Torque (Nm@rpm), Displacement (cm\u00b3), Engine code, Comment\nQ: what's the\u00a0model name\u00a0with\u00a0engine code\u00a0being b5204 t5\nA: SELECT Model name FROM 1-1147701-4 WHERE Engine code = 'B5204 T5'"}
{"text": "table: 1-1147701-4\ncolumns: Model name, Power (ps), Torque (Nm@rpm), Displacement (cm\u00b3), Engine code, Comment\nQ: what's the\u00a0dbeingplacement (cm\u00b3)\u00a0with\u00a0torque (nm@rpm)\u00a0being 350@1800-6000\nA: SELECT Displacement (cm\u00b3) FROM 1-1147701-4 WHERE Torque (Nm@rpm) = '350@1800-6000'"}
{"text": "table: 1-1147701-4\ncolumns: Model name, Power (ps), Torque (Nm@rpm), Displacement (cm\u00b3), Engine code, Comment\nQ: what's the\u00a0model name\u00a0with\u00a0torque (nm@rpm)\u00a0being 230@4500\nA: SELECT Model name FROM 1-1147701-4 WHERE Torque (Nm@rpm) = '230@4500'"}
{"text": "table: 1-1147701-4\ncolumns: Model name, Power (ps), Torque (Nm@rpm), Displacement (cm\u00b3), Engine code, Comment\nQ: what's the\u00a0model name\u00a0with\u00a0engine code\u00a0being b5254 t4\nA: SELECT Model name FROM 1-1147701-4 WHERE Engine code = 'B5254 T4'"}
{"text": "table: 1-1147701-5\ncolumns: Model name, Power (ps), Torque (Nm@rpm), Displacement (cm\u00b3), Engine code, Comment\nQ: Name the torque of the engine is d5244 t5\nA: SELECT Torque (Nm@rpm) FROM 1-1147701-5 WHERE Engine code = 'D5244 T5'"}
{"text": "table: 1-1147701-5\ncolumns: Model name, Power (ps), Torque (Nm@rpm), Displacement (cm\u00b3), Engine code, Comment\nQ: What is the model of the engine d5244 t?\nA: SELECT Model name FROM 1-1147701-5 WHERE Engine code = 'D5244 T'"}
{"text": "table: 1-1147701-5\ncolumns: Model name, Power (ps), Torque (Nm@rpm), Displacement (cm\u00b3), Engine code, Comment\nQ: What is the model of the enginge d5252 t?\nA: SELECT Model name FROM 1-1147701-5 WHERE Engine code = 'D5252 T'"}
{"text": "table: 1-1147701-5\ncolumns: Model name, Power (ps), Torque (Nm@rpm), Displacement (cm\u00b3), Engine code, Comment\nQ: What is the model of the engine d5244 t7?\nA: SELECT Model name FROM 1-1147701-5 WHERE Engine code = 'D5244 T7'"}
{"text": "table: 1-11545282-11\ncolumns: Player, No., Nationality, Position, Years for Jazz, School/Club Team\nQ: What is the position of number 47?\nA: SELECT Position FROM 1-11545282-11 WHERE No. = '47'"}
{"text": "table: 1-11545282-11\ncolumns: Player, No., Nationality, Position, Years for Jazz, School/Club Team\nQ: Name the position of Turkey\nA: SELECT Position FROM 1-11545282-11 WHERE Nationality = 'Turkey'"}
{"text": "table: 1-11545282-11\ncolumns: Player, No., Nationality, Position, Years for Jazz, School/Club Team\nQ: Who is player number 51?\nA: SELECT Player FROM 1-11545282-11 WHERE No. = '51'"}
{"text": "table: 1-11545282-11\ncolumns: Player, No., Nationality, Position, Years for Jazz, School/Club Team\nQ: What is the position for the years 1998-99\nA: SELECT Position FROM 1-11545282-11 WHERE Years for Jazz = '1998-99'"}
{"text": "table: 1-11545282-11\ncolumns: Player, No., Nationality, Position, Years for Jazz, School/Club Team\nQ: How many positions are for creighton?\nA: SELECT COUNT Position FROM 1-11545282-11 WHERE School/Club Team = 'Creighton'"}
{"text": "table: 1-11545282-12\ncolumns: Player, No., Nationality, Position, Years for Jazz, School/Club Team\nQ: Which player is from Marshall and played 1974-75?\nA: SELECT Player FROM 1-11545282-12 WHERE Years for Jazz = '1974-75' AND School/Club Team = 'Marshall'"}
{"text": "table: 1-11545282-12\ncolumns: Player, No., Nationality, Position, Years for Jazz, School/Club Team\nQ: Which country is the player that went to Oregon?\nA: SELECT Nationality FROM 1-11545282-12 WHERE School/Club Team = 'Oregon'"}
{"text": "table: 1-11545282-12\ncolumns: Player, No., Nationality, Position, Years for Jazz, School/Club Team\nQ: Which country is Jim Les from?\nA: SELECT Nationality FROM 1-11545282-12 WHERE Player = 'Jim Les'"}
{"text": "table: 1-11545282-12\ncolumns: Player, No., Nationality, Position, Years for Jazz, School/Club Team\nQ: Which number is the player from Minnesota?\nA: SELECT MAX No. FROM 1-11545282-12 WHERE School/Club Team = 'Minnesota'"}
{"text": "table: 1-11545282-18\ncolumns: Player, No., Nationality, Position, Years for Jazz, School/Club Team\nQ: Which player played for years 2000-02\nA: SELECT Player FROM 1-11545282-18 WHERE Years for Jazz = '2000-02'"}
{"text": "table: 1-11545282-18\ncolumns: Player, No., Nationality, Position, Years for Jazz, School/Club Team\nQ: Which school is Kirk Snyder from?\nA: SELECT School/Club Team FROM 1-11545282-18 WHERE Player = 'Kirk Snyder'"}

>>>> lora/lora.py
# Copyright © 2023-2024 Apple Inc.

import argparse
import json
import math
import sys
import time
from pathlib import Path

import mlx.core as mx
import mlx.nn as nn
import mlx.optimizers as optim
import numpy as np
import utils as lora_utils
from mlx.utils import tree_flatten
from models import LoRALinear

# Disable output buffering to see print statements in real-time
sys.stdout.reconfigure(line_buffering=True)


def build_parser():
    parser = argparse.ArgumentParser(description="LoRA or QLoRA finetuning.")
    parser.add_argument(
        "--model",
        default="mlx_model",
        help="The path to the local model directory or Hugging Face repo.",
    )
    # Generation args
    parser.add_argument(
        "--max-tokens",
        "-m",
        type=int,
        default=100,
        help="The maximum number of tokens to generate",
    )
    parser.add_argument(
        "--temp", type=float, default=0.8, help="The sampling temperature"
    )
    parser.add_argument(
        "--prompt",
        "-p",
        type=str,
        help="The prompt for generation",
        default=None,
    )

    # Training args
    parser.add_argument(
        "--train",
        action="store_true",
        help="Do training",
    )
    parser.add_argument(
        "--add-eos-token",
        type=int,
        default=1,
        help="Enable add_eos_token for tokenizer",
    )
    parser.add_argument(
        "--data",
        type=str,
        default="data/",
        help="Directory with {train, valid, test}.jsonl files",
    )
    parser.add_argument(
        "--lora-layers",
        type=int,
        default=16,
        help="Number of layers to fine-tune",
    )
    parser.add_argument("--batch-size", type=int, default=4, help="Minibatch size.")
    parser.add_argument(
        "--iters", type=int, default=1000, help="Iterations to train for."
    )
    parser.add_argument(
        "--val-batches",
        type=int,
        default=25,
        help="Number of validation batches, -1 uses the entire validation set.",
    )
    parser.add_argument(
        "--learning-rate", type=float, default=1e-5, help="Adam learning rate."
    )
    parser.add_argument(
        "--steps-per-report",
        type=int,
        default=10,
        help="Number of training steps between loss reporting.",
    )
    parser.add_argument(
        "--steps-per-eval",
        type=int,
        default=200,
        help="Number of training steps between validations.",
    )
    parser.add_argument(
        "--resume-adapter-file",
        type=str,
        default=None,
        help="Load path to resume training with the given adapter weights.",
    )
    parser.add_argument(
        "--adapter-file",
        type=str,
        default="adapters.npz",
        help="Save/load path for the trained adapter weights.",
    )
    parser.add_argument(
        "--save-every",
        type=int,
        default=100,
        help="Save the model every N iterations.",
    )
    parser.add_argument(
        "--test",
        action="store_true",
        help="Evaluate on the test set after training",
    )
    parser.add_argument(
        "--test-batches",
        type=int,
        default=500,
        help="Number of test set batches, -1 uses the entire test set.",
    )
    parser.add_argument("--seed", type=int, default=0, help="The PRNG seed")
    return parser


class Dataset:
    """
    Light-weight wrapper to hold lines from a jsonl file
    """

    def __init__(self, path: Path, key: str = "text"):
        if not path.exists():
            self._data = None
        else:
            with open(path, "r") as fid:
                self._data = [json.loads(l) for l in fid]
        self._key = key

    def __getitem__(self, idx: int):
        return self._data[idx][self._key]

    def __len__(self):
        return len(self._data)


def load(args):
    def load_and_check(name):
        dataset_path = Path(args.data) / f"{name}.jsonl"
        try:
            return Dataset(dataset_path)
        except Exception as e:
            print(f"Unable to build dataset {dataset_path} ({e})")
            raise

    names = ("train", "valid", "test")
    train, valid, test = (load_and_check(n) for n in names)

    if args.train and len(train) == 0:
        raise ValueError(
            "Training set not found or empty. Must provide training set for fine-tuning."
        )
    if args.train and len(valid) == 0:
        raise ValueError(
            "Validation set not found or empty. Must provide validation set for fine-tuning."
        )
    if args.test and len(test) == 0:
        raise ValueError(
            "Test set not found or empty. Must provide test set for evaluation."
        )
    return train, valid, test


def loss(model, inputs, targets, lengths):
    # Run model on inputs
    logits, _ = model(inputs)
    logits = logits.astype(mx.float32)

    # Mask padding tokens
    length_mask = mx.arange(inputs.shape[1])[None, :] < lengths[:, None]

    # Calculate the loss
    ce = nn.losses.cross_entropy(logits, targets) * length_mask
    ntoks = length_mask.sum()
    ce = ce.sum() / ntoks
    return ce, ntoks


def iterate_batches(dset, tokenizer, batch_size, train=False):
    # Shuffle indices
    while True:
        indices = np.arange(len(dset))
        if train:
            indices = np.random.permutation(indices)

        # Collect batches from dataset
        for i in range(0, len(indices) - batch_size + 1, batch_size):
            # Encode batch
            batch = [tokenizer.encode(dset[indices[i + j]]) for j in range(batch_size)]
            lengths = [len(x) for x in batch]

            # Check if any sequence is longer than 2048 tokens
            if max(lengths) > 2048:
                print(
                    "[WARNING] Some sequences are longer than 2048 tokens. "
                    "Consider pre-splitting your data to save memory."
                )

            # Pad to the max length
            batch_arr = np.zeros((batch_size, max(lengths)), np.int32)

            for j in range(batch_size):
                batch_arr[j, : lengths[j]] = batch[j]
            batch = mx.array(batch_arr)
            yield batch[:, :-1], batch[:, 1:], mx.array(lengths)

        if not train:
            break


def evaluate(model, dataset, loss, tokenizer, batch_size, num_batches):
    all_losses = []
    ntokens = 0

    # num_batches can be -1 to indicate the entire set
    index_iterator = iter(range(num_batches)) if num_batches != -1 else iter(int, 1)

    for it, batch in zip(
        index_iterator,
        iterate_batches(dataset, tokenizer, batch_size),
    ):
        losses, toks = loss(model, *batch)
        all_losses.append((losses * toks).item())
        ntokens += toks.item()

    return np.sum(all_losses) / ntokens


def train(model, train_set, val_set, optimizer, loss, tokenizer, args):
    # Create value and grad function for loss
    loss_value_and_grad = nn.value_and_grad(model, loss)

    losses = []
    n_tokens = 0

    # Main training loop
    start = time.perf_counter()
    for it, batch in zip(
        range(args.iters),
        iterate_batches(train_set, tokenizer, args.batch_size, train=True),
    ):
        # Forward and backward pass
        (lvalue, toks), grad = loss_value_and_grad(model, *batch)

        # Model update
        optimizer.update(model, grad)
        mx.eval(model.parameters(), optimizer.state, lvalue)

        # Record loss
        losses.append(lvalue.item())
        n_tokens += toks.item()

        # Report training loss if needed
        if (it + 1) % args.steps_per_report == 0:
            train_loss = np.mean(losses)

            stop = time.perf_counter()
            print(
                f"Iter {it + 1}: Train loss {train_loss:.3f}, "
                f"It/sec {args.steps_per_report / (stop - start):.3f}, "
                f"Tokens/sec {float(n_tokens) / (stop - start):.3f}"
            )
            losses = []
            n_tokens = 0
            start = time.perf_counter()

        # Report validation loss if needed
        if it == 0 or (it + 1) % args.steps_per_eval == 0:
            stop = time.perf_counter()
            val_loss = evaluate(
                model, val_set, loss, tokenizer, args.batch_size, args.val_batches
            )
            print(
                f"Iter {it + 1}: "
                f"Val loss {val_loss:.3f}, "
                f"Val took {(time.perf_counter() - stop):.3f}s"
            )

            start = time.perf_counter()

        # Save adapter weights if needed
        if (it + 1) % args.save_every == 0:
            mx.savez(
                args.adapter_file, **dict(tree_flatten(model.trainable_parameters()))
            )
            print(f"Iter {it + 1}: Saved adapter weights to {args.adapter_file}.")


def generate(model, prompt, tokenizer, args):
    print(prompt, end="", flush=True)

    prompt = mx.array(tokenizer.encode(prompt))

    tokens = []
    skip = 0
    for token, n in zip(
        lora_utils.generate(prompt, model, args.temp),
        range(args.max_tokens),
    ):
        if token == tokenizer.eos_token_id:
            break

        tokens.append(token.item())
        s = tokenizer.decode(tokens)
        if len(s) - skip > 1:
            print(s[skip:-1], end="", flush=True)
            skip = len(s) - 1
    print(tokenizer.decode(tokens)[skip:], flush=True)
    print("=" * 10)
    if len(tokens) == 0:
        print("No tokens generated for this prompt")
        return


if __name__ == "__main__":
    parser = build_parser()
    args = parser.parse_args()

    np.random.seed(args.seed)

    # Building tokenizer_config
    tokenizer_config = {}
    if args.train:
        tokenizer_config["add_eos_token"] = bool(args.add_eos_token)

    print("Loading pretrained model")
    model, tokenizer, _ = lora_utils.load(args.model, tokenizer_config)
    # Freeze all layers other than LORA linears
    model.freeze()
    for l in model.model.layers[len(model.model.layers) - args.lora_layers :]:
        l.self_attn.q_proj = LoRALinear.from_linear(l.self_attn.q_proj)
        l.self_attn.v_proj = LoRALinear.from_linear(l.self_attn.v_proj)
        if hasattr(l, "block_sparse_moe"):
            l.block_sparse_moe.gate = LoRALinear.from_linear(l.block_sparse_moe.gate)

    p = sum(v.size for _, v in tree_flatten(model.parameters())) / 10**6
    print(f"Total parameters {p:.3f}M")
    p = sum(v.size for _, v in tree_flatten(model.trainable_parameters())) / 10**6
    print(f"Trainable parameters {p:.3f}M")

    print("Loading datasets")
    train_set, valid_set, test_set = load(args)

    # Resume training the given adapters.
    if args.resume_adapter_file is not None:
        print(f"Loading pretrained adapters from {args.resume_adapter_file}")
        model.load_weights(args.resume_adapter_file, strict=False)

    if args.train:
        print("Training")
        opt = optim.Adam(learning_rate=args.learning_rate)

        # Train model
        train(model, train_set, valid_set, opt, loss, tokenizer, args)

        # Save adapter weights
        mx.savez(args.adapter_file, **dict(tree_flatten(model.trainable_parameters())))

    # Load the LoRA adapter weights which we assume should exist by this point
    if not Path(args.adapter_file).is_file():
        raise ValueError(
            f"Adapter file {args.adapter_file} missing. "
            "Use --train to learn and save the adapters.npz."
        )
    model.load_weights(args.adapter_file, strict=False)

    if args.test:
        print("Testing")
        model.eval()
        test_loss = evaluate(
            model,
            test_set,
            loss,
            tokenizer,
            args.batch_size,
            num_batches=args.test_batches,
        )
        test_ppl = math.exp(test_loss)

        print(f"Test loss {test_loss:.3f}, Test ppl {test_ppl:.3f}.")

    if args.prompt is not None:
        print("Generating")
        generate(model, args.prompt, tokenizer, args)

>>>> lora/utils.py
# Copyright © 2023-2024 Apple Inc.

import glob
import json
import logging
from pathlib import Path
from typing import Generator

import mlx.core as mx
import mlx.nn as nn
import models
import transformers
from huggingface_hub import snapshot_download


def fetch_from_hub(hf_path: str):
    model_path = snapshot_download(
        repo_id=hf_path,
        allow_patterns=["*.json", "*.safetensors", "tokenizer.model"],
    )
    weight_files = glob.glob(f"{model_path}/*.safetensors")
    if len(weight_files) == 0:
        raise FileNotFoundError("No safetensors found in {}".format(model_path))

    weights = {}
    for wf in weight_files:
        weights.update(mx.load(wf).items())

    config = transformers.AutoConfig.from_pretrained(hf_path)
    tokenizer = transformers.AutoTokenizer.from_pretrained(
        hf_path,
    )
    return weights, config.to_dict(), tokenizer


def upload_to_hub(path: str, name: str, hf_path: str):
    import os

    from huggingface_hub import HfApi, ModelCard, logging

    repo_id = f"mlx-community/{name}"

    card = ModelCard.load(hf_path)
    card.data.tags = ["mlx"] if card.data.tags is None else card.data.tags + ["mlx"]
    card.text = f"""
# {name}
This model was converted to MLX format from [`{hf_path}`]().
Refer to the [original model card](https://huggingface.co/{hf_path}) for more details on the model.
## Use with mlx
```bash
pip install mlx
git clone https://github.com/ml-explore/mlx-examples.git
cd mlx-examples/llms/hf_llm
python generate.py --model {repo_id} --prompt "My name is"
```
"""
    card.save(os.path.join(path, "README.md"))

    logging.set_verbosity_info()

    api = HfApi()
    api.create_repo(repo_id=repo_id, exist_ok=True)
    api.upload_folder(
        folder_path=path,
        repo_id=repo_id,
        repo_type="model",
        multi_commits=True,
        multi_commits_verbose=True,
    )


def make_shards(weights: dict, max_file_size_gibibyte: int = 15):
    max_file_size_bytes = max_file_size_gibibyte << 30
    shards = []
    shard, shard_size = {}, 0
    for k, v in weights.items():
        if shard_size + v.nbytes > max_file_size_bytes:
            shards.append(shard)
            shard, shard_size = {}, 0
        shard[k] = v
        shard_size += v.nbytes
    shards.append(shard)
    return shards


def save_model(save_dir: str, weights, tokenizer, config):
    save_dir = Path(save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)

    shards = make_shards(weights, max_file_size_gibibyte=5)
    shards_count = len(shards)
    shard_file_format = (
        "model-{:05d}-of-{:05d}.safetensors"
        if shards_count > 1
        else "model.safetensors"
    )

    total_size = sum(v.nbytes for v in weights.values())
    index_data = {"metadata": {"total_size": total_size}, "weight_map": {}}

    for i, shard in enumerate(shards):
        shard_name = shard_file_format.format(i + 1, shards_count)
        mx.save_safetensors(
            str(save_dir / shard_name), shard, metadata={"format": "mlx"}
        )
        for weight_name in shard.keys():
            index_data["weight_map"][weight_name] = shard_name
        del shard

    tokenizer.save_pretrained(save_dir)
    with open(save_dir / "config.json", "w") as fid:
        json.dump(config, fid, indent=4)

    index_data["weight_map"] = {
        k: index_data["weight_map"][k] for k in sorted(index_data["weight_map"])
    }
    with open(save_dir / "model.safetensors.index.json", "w") as f:
        json.dump(
            index_data,
            f,
            indent=4,
        )


def load(path_or_hf_repo: str, tokenizer_config={}):
    # If the path exists, it will try to load model form it
    # otherwise download and cache from the hf_repo and cache
    model_path = Path(path_or_hf_repo)
    if not model_path.exists():
        model_path = Path(
            snapshot_download(
                repo_id=path_or_hf_repo,
                allow_patterns=["*.json", "*.safetensors", "tokenizer.model"],
            )
        )

    with open(model_path / "config.json", "r") as f:
        config = json.loads(f.read())
        quantization = config.get("quantization", None)

    weight_files = glob.glob(str(model_path / "*.safetensors"))
    if len(weight_files) == 0:
        raise FileNotFoundError("No safetensors found in {}".format(model_path))

    weights = {}
    for wf in weight_files:
        weights.update(mx.load(wf).items())

    model_args = models.ModelArgs.from_dict(config)
    model = models.Model(model_args)
    if quantization is not None:
        class_predicate = (
            lambda p, m: isinstance(m, (nn.Linear, nn.Embedding))
            and f"{p}.scales" in weights
        )
        nn.quantize(
            model,
            **quantization,
            class_predicate=class_predicate,
        )

    model.load_weights(list(weights.items()))

    mx.eval(model.parameters())
    tokenizer = transformers.AutoTokenizer.from_pretrained(
        model_path, **tokenizer_config
    )
    return model, tokenizer, config


def generate(
    prompt: mx.array, model: nn.Module, temp: float = 0.0
) -> Generator[mx.array, None, None]:
    """
    Generate text based on the given prompt and model.

    Args:
        prompt (mx.array): The input prompt.
        model (nn.Module): The model to use for generation.
        temp (float): The temperature for sampling. If temp is 0, use max sampling.

    Yields:
        mx.array: The generated text.
    """

    def sample(logits: mx.array) -> mx.array:
        return (
            mx.argmax(logits, axis=-1)
            if temp == 0
            else mx.random.categorical(logits * (1 / temp))
        )

    y = prompt
    cache = None
    while True:
        logits, cache = model(y[None], cache=cache)
        logits = logits[:, -1, :]
        y = sample(logits)
        yield y

>>>> lora/README.md
# Fine-Tuning with LoRA or QLoRA

This is an example of using MLX to fine-tune an LLM with low rank adaptation
(LoRA) for a target task.[^lora] The example also supports quantized LoRA
(QLoRA).[^qlora] The example works with Llama and Mistral style models
available on Hugging Face.

> [!TIP]
> For a more fully featured LLM package, checkout [MLX
> LM](https://github.com/ml-explore/mlx-examples/tree/main/llms/mlx_lm).

In this example we'll use the WikiSQL[^wikisql] dataset to train the LLM to
generate SQL queries from natural language. However, the example is intended to
be general should you wish to use a custom dataset.

## Contents

* [Setup](#Setup)
  * [Convert](#convert)
* [Run](#Run)
  * [Fine-tune](#Fine-tune)
  * [Evaluate](#Evaluate)
  * [Generate](#Generate)
* [Results](#Results)
* [Fuse and Upload](#Fuse-and-Upload)
* [Custom Data](#Custom-Data)
* [Memory Issues](#Memory-Issues)


## Setup 

Install the dependencies:

```
pip install -r requirements.txt
```

### Convert

This step is optional if you want to quantize (for QLoRA) or change the default
data type of a pre-existing model.

You convert models using the `convert.py` script. This script takes a Hugging
Face repo as input and outputs a model directory (which you can optionally also
upload to Hugging Face).

To make a 4-bit quantized model, run:

```
python convert.py --hf-path <hf_repo> -q
```

For example, the following will make a 4-bit quantized Mistral 7B and by default
store it in `mlx_model`:

```
python convert.py --hf-path mistralai/Mistral-7B-v0.1 -q
```

For more options run:

```
python convert.py --help
```

You can upload new models to the [Hugging Face MLX
Community](https://huggingface.co/mlx-community) by specifying `--upload-name`
to `convert.py`.

## Run

The main script is `lora.py`. To see a full list of options run:

```
python lora.py --help
```

Note, in the following the `--model` argument can be any compatible Hugging
Face repo or a local path to a converted mdoel. 

### Fine-tune

To fine-tune a model use:

```
python lora.py --model <path_to_model> 
               --train 
               --iters 600
```

If `--model` points to a quantized model, then the training will use QLoRA,
otherwise it will use regular LoRA.

By default, the adapter weights are saved in `adapters.npz`. You can specify
the output location with `--adapter-file`.

You can resume fine-tuning with an existing adapter with `--resume-adapter-file
<path_to_adapters.npz>`. 

### Evaluate

To compute test set perplexity use:

```
python lora.py --model <path_to_model> 
               --adapter-file <path_to_adapters.npz> 
               --test
```

### Generate

For generation use:

```
python lora.py --model <path_to_model> 
               --adapter-file <path_to_adapters.npz> 
               --max-tokens 50 
               --prompt "table: 1-10015132-16
columns: Player, No., Nationality, Position, Years in Toronto, School/Club Team
Q: What is terrence ross' nationality
A: "
```

## Results

The initial validation loss for Llama 7B on the WikiSQL is 2.66 and the final
validation loss after 1000 iterations is 1.23. The table below shows the
training and validation loss at a few points over the course of training.

| Iteration | Train Loss | Validation Loss |
| --------- | ---------- | --------------- |
| 1         |    N/A     |      2.659      |
| 200       |    1.264   |      1.405      |
| 400       |    1.201   |      1.303      |
| 600       |    1.123   |      1.274      |
| 800       |    1.017   |      1.255      |
| 1000      |    1.070   |      1.230      |

The model trains at around 475 tokens per second on an M2 Ultra.

## Fuse and Upload

You can generate a fused model with the low-rank adapters included using the
`fuse.py` script. This script also optionally allows you to upload the fused
model to the [Hugging Face MLX
Community](https://huggingface.co/mlx-community).

To generate the fused model run:

```
python fuse.py
```

This will by default load the base model from `mlx_model/`, the adapters from
`adapters.npz`,  and save the fused model in the path `lora_fused_model/`. All
of these are configurable. You can see the list of options with:

```
python fuse.py --help
```

To upload a fused model, supply the `--upload-name` and `--hf-path` arguments
to `fuse.py`. The latter is the repo name of the original model, which is
useful for the sake of attribution and model versioning.

For example, to fuse and upload a model derived from Mistral-7B-v0.1, run: 

```
python fuse.py --upload-name My-4-bit-model --hf-repo mistralai/Mistral-7B-v0.1
```

## Custom Data

You can make your own dataset for fine-tuning with LoRA. You can specify the
dataset with `--data=<my_data_directory>`. Check the subdirectory `data/` to
see the expected format.

For fine-tuning (`--train`), the data loader expects a `train.jsonl` and a
`valid.jsonl` to be in the data directory. For evaluation (`--test`), the data
loader expects a `test.jsonl` in the data directory. Each line in the `*.jsonl`
file should look like:

```
{"text": "This is an example for the model."}
```

Note other keys will be ignored by the loader.

## Memory Issues

Fine-tuning a large model with LoRA requires a machine with a decent amount
of memory. Here are some tips to reduce memory use should you need to do so:

1. Try quantization (QLoRA). You can use QLoRA by generating a quantized model
   with `convert.py` and the `-q` flag. See the [Setup](#setup) section for
   more details. 

2. Try using a smaller batch size with `--batch-size`. The default is `4` so
   setting this to `2` or `1` will reduce memory consumption. This may slow
   things down a little, but will also reduce the memory use.

3. Reduce the number of layers to fine-tune with `--lora-layers`. The default
   is `16`, so you can try `8` or `4`. This reduces the amount of memory
   needed for back propagation. It may also reduce the quality of the
   fine-tuned model if you are fine-tuning with a lot of data.

4. Longer examples require more memory. If it makes sense for your data, one thing
   you can do is break your examples into smaller
   sequences when making the `{train, valid, test}.jsonl` files.

For example, for a machine with 32 GB the following should run reasonably fast:

```
python lora.py 
   --model mistralai/Mistral-7B-v0.1 
   --train 
   --batch-size 1 
   --lora-layers 4
```

The above command on an M1 Max with 32 GB runs at about 250 tokens-per-second.


[^lora]: Refer to the [arXiv paper](https://arxiv.org/abs/2106.09685) for more details on LoRA.
[^qlora]: Refer to the paper [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)
[^wikisql]: Refer to the [GitHub repo](https://github.com/salesforce/WikiSQL/tree/master) for more information about WikiSQL.

>>>> lora/convert.py
# Copyright © 2023-2024 Apple Inc.

import argparse
import copy

import mlx.core as mx
import mlx.nn as nn
import models
import utils
from mlx.utils import tree_flatten


def quantize(weights, config, args):
    quantized_config = copy.deepcopy(config)

    # Load the model:
    model = models.Model(models.ModelArgs.from_dict(config))
    model.load_weights(list(weights.items()))

    # Quantize the model:
    nn.quantize(
        model,
        args.q_group_size,
        args.q_bits,
    )

    # Update the config:
    quantized_config["quantization"] = {
        "group_size": args.q_group_size,
        "bits": args.q_bits,
    }
    quantized_weights = dict(tree_flatten(model.parameters()))

    return quantized_weights, quantized_config


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Convert Hugging Face model to MLX format"
    )
    parser.add_argument(
        "--hf-path",
        type=str,
        help="Path to the Hugging Face model.",
    )
    parser.add_argument(
        "--mlx-path",
        type=str,
        default="mlx_model",
        help="Path to save the MLX model.",
    )
    parser.add_argument(
        "-q",
        "--quantize",
        help="Generate a quantized model.",
        action="store_true",
    )
    parser.add_argument(
        "--q-group-size",
        help="Group size for quantization.",
        type=int,
        default=64,
    )
    parser.add_argument(
        "--q-bits",
        help="Bits per weight for quantization.",
        type=int,
        default=4,
    )
    parser.add_argument(
        "--dtype",
        help="Type to save the parameters, ignored if -q is given.",
        type=str,
        choices=["float16", "bfloat16", "float32"],
        default="float16",
    )
    parser.add_argument(
        "--upload-name",
        help="The name of model to upload to Hugging Face MLX Community",
        type=str,
        default=None,
    )

    args = parser.parse_args()

    print("[INFO] Loading")
    weights, config, tokenizer = utils.fetch_from_hub(args.hf_path)

    dtype = mx.float16 if args.quantize else getattr(mx, args.dtype)
    weights = {k: v.astype(dtype) for k, v in weights.items()}
    if args.quantize:
        print("[INFO] Quantizing")
        weights, config = quantize(weights, config, args)

    utils.save_model(args.mlx_path, weights, tokenizer, config)
    if args.upload_name is not None:
        utils.upload_to_hub(args.mlx_path, args.upload_name, args.hf_path)

>>>> lora/models.py
# Copyright © 2023 Apple Inc.

import inspect
import math
from dataclasses import dataclass
from typing import Dict, Optional, Tuple, Union

import mlx.core as mx
import mlx.nn as nn


@dataclass
class ModelArgs:
    hidden_size: int
    num_hidden_layers: int
    intermediate_size: int
    num_attention_heads: int
    rms_norm_eps: float
    vocab_size: int
    num_key_value_heads: int = None
    rope_theta: float = 10000
    rope_traditional: bool = False
    model_type: str = None
    rope_scaling: Optional[Dict[str, Union[float, str]]] = None

    def __post_init__(self):
        if self.num_key_value_heads is None:
            self.num_key_value_heads = self.num_attention_heads

        if self.rope_scaling:
            required_keys = {"factor", "type"}
            if not all(key in self.rope_scaling for key in required_keys):
                raise ValueError(f"rope_scaling must contain keys {required_keys}")

            if self.rope_scaling["type"] != "linear":
                raise ValueError("rope_scaling 'type' currently only supports 'linear'")

    @classmethod
    def from_dict(cls, params):
        return cls(
            **{
                k: v
                for k, v in params.items()
                if k in inspect.signature(cls).parameters
            }
        )


class LoRALinear(nn.Module):
    @staticmethod
    def from_linear(linear: nn.Linear, rank: int = 8):
        # TODO remove when input_dims and output_dims are attributes
        # on linear and quantized linear
        output_dims, input_dims = linear.weight.shape
        if isinstance(linear, nn.QuantizedLinear):
            input_dims *= 32 // linear.bits
        lora_lin = LoRALinear(input_dims, output_dims, rank)
        lora_lin.linear = linear
        return lora_lin

    def to_linear(self):
        linear = self.linear
        bias = "bias" in linear
        weight = linear.weight
        is_quantized = isinstance(linear, nn.QuantizedLinear)

        # Use the same type as the linear weight if not quantized
        dtype = weight.dtype

        if is_quantized:
            dtype = mx.float16
            weight = mx.dequantize(
                weight,
                linear.scales,
                linear.biases,
                linear.group_size,
                linear.bits,
            )
        output_dims, input_dims = weight.shape
        fused_linear = nn.Linear(input_dims, output_dims, bias=bias)

        lora_b = (self.scale * self.lora_b.T).astype(dtype)
        lora_a = self.lora_a.T.astype(dtype)
        fused_linear.weight = weight + lora_b @ lora_a
        if bias:
            fused_linear.bias = linear.bias

        if is_quantized:
            fused_linear = nn.QuantizedLinear.from_linear(
                fused_linear,
                linear.group_size,
                linear.bits,
            )

        return fused_linear

    def __init__(
        self,
        input_dims: int,
        output_dims: int,
        lora_rank: int = 8,
        bias: bool = False,
        scale: float = 20.0,
    ):
        super().__init__()

        # Regular linear layer weights
        self.linear = nn.Linear(input_dims, output_dims, bias=bias)

        # Scale for low-rank update
        self.scale = scale

        # Low rank lora weights
        scale = 1 / math.sqrt(input_dims)
        self.lora_a = mx.random.uniform(
            low=-scale,
            high=scale,
            shape=(input_dims, lora_rank),
        )
        self.lora_b = mx.zeros(shape=(lora_rank, output_dims))

    def __call__(self, x):
        dtype = self.linear.weight.dtype
        if isinstance(self.linear, nn.QuantizedLinear):
            dtype = self.linear.scales.dtype
        y = self.linear(x.astype(dtype))
        z = (x @ self.lora_a) @ self.lora_b
        return y + self.scale * z


class Attention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        dim = args.hidden_size
        self.n_heads = n_heads = args.num_attention_heads
        self.n_kv_heads = n_kv_heads = args.num_key_value_heads

        self.repeats = n_heads // n_kv_heads

        head_dim = args.hidden_size // n_heads
        self.scale = head_dim**-0.5

        self.q_proj = nn.Linear(dim, n_heads * head_dim, bias=False)
        self.k_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=False)
        self.v_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=False)
        self.o_proj = nn.Linear(n_heads * head_dim, dim, bias=False)
        rope_scale = (
            1 / args.rope_scaling["factor"]
            if args.rope_scaling is not None and args.rope_scaling["type"] == "linear"
            else 1
        )
        self.rope = nn.RoPE(
            head_dim,
            traditional=args.rope_traditional,
            base=args.rope_theta,
            scale=rope_scale,
        )

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Tuple[mx.array, mx.array]] = None,
    ) -> mx.array:
        B, L, D = x.shape

        queries, keys, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)

        # Prepare the queries, keys and values for the attention computation
        queries = queries.reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)
        keys = keys.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)
        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)

        if cache is not None:
            key_cache, value_cache = cache
            queries = self.rope(queries, offset=key_cache.shape[2])
            keys = self.rope(keys, offset=key_cache.shape[2])
            keys = mx.concatenate([key_cache, keys], axis=2)
            values = mx.concatenate([value_cache, values], axis=2)
        else:
            queries = self.rope(queries)
            keys = self.rope(keys)

        output = mx.fast.scaled_dot_product_attention(
            queries, keys, values, scale=self.scale, mask=mask
        )
        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)
        return self.o_proj(output), (keys, values)


class MLP(nn.Module):
    def __init__(self, dim, hidden_dim):
        super().__init__()
        self.gate_proj = nn.Linear(dim, hidden_dim, bias=False)
        self.down_proj = nn.Linear(hidden_dim, dim, bias=False)
        self.up_proj = nn.Linear(dim, hidden_dim, bias=False)

    def __call__(self, x) -> mx.array:
        return self.down_proj(nn.silu(self.gate_proj(x)) * self.up_proj(x))


class TransformerBlock(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.num_attention_heads = args.num_attention_heads
        self.hidden_size = args.hidden_size
        self.self_attn = Attention(args)
        self.mlp = MLP(args.hidden_size, args.intermediate_size)
        self.input_layernorm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)
        self.post_attention_layernorm = nn.RMSNorm(
            args.hidden_size, eps=args.rms_norm_eps
        )
        self.args = args

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Tuple[mx.array, mx.array]] = None,
    ) -> mx.array:
        r, cache = self.self_attn(self.input_layernorm(x), mask, cache)
        h = x + r
        r = self.mlp(self.post_attention_layernorm(h))
        out = h + r
        return out, cache


class LlamaModel(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.vocab_size = args.vocab_size
        self.num_hidden_layers = args.num_hidden_layers
        assert self.vocab_size > 0
        self.embed_tokens = nn.Embedding(args.vocab_size, args.hidden_size)
        self.layers = [
            TransformerBlock(args=args) for _ in range(args.num_hidden_layers)
        ]
        self.norm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)

    def __call__(
        self,
        inputs: mx.array,
        cache=None,
    ):
        h = self.embed_tokens(inputs)

        mask = None
        if h.shape[1] > 1:
            mask = nn.MultiHeadAttention.create_additive_causal_mask(h.shape[1])
            mask = mask.astype(h.dtype)

        if cache is None:
            cache = [None] * len(self.layers)

        for e, layer in enumerate(self.layers):
            h, cache[e] = layer(h, mask, cache[e])

        return self.norm(h), cache


class Model(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.model = LlamaModel(args)
        self.lm_head = nn.Linear(args.hidden_size, args.vocab_size, bias=False)

    def __call__(
        self,
        inputs: mx.array,
        cache=None,
    ):
        out, cache = self.model(inputs, cache)
        return self.lm_head(out), cache

>>>> t5/hf_t5.py
import argparse

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, T5EncoderModel


def embed(t5_model: str):
    batch = [
        "translate English to German: That is good.",
        "This is an example of T5 working on MLX.",
    ]

    tokenizer = AutoTokenizer.from_pretrained(t5_model)
    torch_model = T5EncoderModel.from_pretrained(t5_model)
    torch_tokens = tokenizer(batch, return_tensors="pt", padding=True)
    torch_forward = torch_model(**torch_tokens, output_hidden_states=True)
    torch_output = torch_forward.last_hidden_state.detach().numpy()

    print("\n TF BERT:")
    for input_str, embedding in list(zip(batch, torch_output)):
        print("Input:", input_str)
        print(embedding)
        print()


def generate(t5_model: str):
    prompt = "translate English to German: As much as six inches of rain could fall in the New York City region through Monday morning, and officials warned of flooding along the coast."
    tokenizer = AutoTokenizer.from_pretrained(t5_model)
    torch_model = AutoModelForSeq2SeqLM.from_pretrained(t5_model)
    torch_tokens = tokenizer(prompt, return_tensors="pt", padding=True).input_ids
    outputs = torch_model.generate(torch_tokens, do_sample=False, max_length=512)
    print(tokenizer.decode(outputs[0], skip_special_tokens=True))


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Run the T5 model using Hugging Face Transformers."
    )
    parser.add_argument(
        "--encode-only",
        action="store_true",
        help="Only run the encoder and print the embeddings.",
        default=False,
    )
    parser.add_argument(
        "--model",
        default="t5-small",
        help="The huggingface name of the T5 model to save.",
    )
    args = parser.parse_args()
    if args.encode_only:
        embed(args.model)
    else:
        generate(args.model)

>>>> t5/README.md
# T5

The T5 models are encoder-decoder models pre-trained on a mixture of
unsupervised and supervised tasks.[^1] These models work well on a variety of
tasks by prepending task-specific prefixes to the input, e.g.:
`translate English to German: …`, `summarize: ….`, etc.

This example also supports the FLAN-T5 models variants.[^2]

## Generate

Generate text with:

```sh
python t5.py --model t5-small --prompt "translate English to German: A tasty apple"
```

This should give the output: `Ein leckerer Apfel`

To see a list of options run:

```sh
python t5.py --help
```

The `<model>` can be any of the following:

| Model Name | Model Size  |
| ---------- | ----------
| t5-small   | 60 million  |
| t5-base    | 220 million |
| t5-large   | 770 million |
| t5-3b      | 3 billion   |
| t5-11b     | 11 billion  |

The FLAN variants can be specified with `google/flan-t5-small`,
`google/flan-t5-base`, etc. See the [Hugging Face
page](https://huggingface.co/docs/transformers/model_doc/flan-t5) for a
complete list of models.

[^1]: For more information on T5 see the [original paper](https://arxiv.org/abs/1910.10683)
   or the [Hugging Face page](https://huggingface.co/docs/transformers/model_doc/t5).
[^2]: For more information on FLAN-T5 see the [original paper](https://arxiv.org/abs/2210.11416).

>>>> t5/t5.py
import argparse
import json
from pathlib import Path
from time import perf_counter_ns
from types import SimpleNamespace
from typing import List, Optional, Tuple

import mlx.core as mx
import mlx.nn as nn
import numpy as np
from transformers import AutoTokenizer


class Tokenizer:
    def __init__(self, config, model_name):
        self._decoder_start_id = config.decoder_start_token_id
        self._tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            legacy=False,
            model_max_length=getattr(config, "n_positions", 512),
        )

    @property
    def eos_id(self) -> int:
        return self._tokenizer.eos_token_id

    @property
    def decoder_start_id(self) -> int:
        return self._decoder_start_id

    def encode(self, s: str) -> mx.array:
        return mx.array(
            self._tokenizer(
                s,
                return_tensors="np",
                return_attention_mask=False,
            )["input_ids"]
        )

    def decode(self, t: List[int], with_sep: bool = True) -> str:
        tokens = self._tokenizer.convert_ids_to_tokens(t)
        return "".join(t.replace("▁", " " if with_sep else "") for t in tokens)


def _relative_position_bucket(
    relative_position, bidirectional=True, num_buckets=32, max_distance=128
):
    """
    Adapted from HF Tensorflow:
    https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/modeling_t5.py

    Translate relative position to a bucket number for relative attention. The relative position is defined as
    memory_position - query_position, i.e. the distance in tokens from the attending position to the attended-to
    position. If bidirectional=False, then positive relative positions are invalid. We use smaller buckets for
    small absolute relative_position and larger buckets for larger absolute relative_positions. All relative
    positions >=max_distance map to the same bucket. All relative positions <=-max_distance map to the same bucket.
    This should allow for more graceful generalization to longer sequences than the model has been trained on

    Args:
        relative_position: an int32 Tensor
        bidirectional: a boolean - whether the attention is bidirectional
        num_buckets: an integer
        max_distance: an integer

    Returns:
        a Tensor with the same shape as relative_position, containing int32 values in the range [0, num_buckets)
    """
    relative_buckets = 0
    if bidirectional:
        num_buckets //= 2
        relative_buckets += (relative_position > 0).astype(mx.int16) * num_buckets
        relative_position = mx.abs(relative_position)
    else:
        relative_position = -mx.minimum(
            relative_position, mx.zeros_like(relative_position)
        )
    # now relative_position is in the range [0, inf)

    # half of the buckets are for exact increments in positions
    max_exact = num_buckets // 2
    is_small = relative_position < max_exact

    # The other half of the buckets are for logarithmically bigger bins in positions up to max_distance
    scale = (num_buckets - max_exact) / np.log(max_distance / max_exact)
    relative_position_if_large = max_exact + (
        mx.log(relative_position.astype(mx.float32) / max_exact) * scale
    ).astype(mx.int16)
    relative_position_if_large = mx.minimum(relative_position_if_large, num_buckets - 1)
    relative_buckets += mx.where(
        is_small, relative_position, relative_position_if_large
    )
    return relative_buckets


class RelativePositionBias(nn.Module):
    def __init__(self, config, bidirectional: bool):
        self.bidirectional = bidirectional
        self.num_buckets = config.relative_attention_num_buckets
        self.max_distance = getattr(config, "relative_attention_max_distance", 128)
        self.n_heads = config.num_heads
        self.embeddings = nn.Embedding(
            config.relative_attention_num_buckets, config.num_heads
        )

    def __call__(self, query_length: int, key_length: int, offset: int = 0):
        """Compute binned relative position bias"""
        context_position = mx.arange(offset, query_length)[:, None]
        memory_position = mx.arange(key_length)[None, :]

        # shape (query_length, key_length)
        relative_position = memory_position - context_position
        relative_position_bucket = _relative_position_bucket(
            relative_position,
            bidirectional=self.bidirectional,
            num_buckets=self.num_buckets,
            max_distance=self.max_distance,
        )

        # shape (query_length, key_length, num_heads)
        values = self.embeddings(relative_position_bucket)

        # shape (num_heads, query_length, key_length)
        return values.transpose(2, 0, 1)


class MultiHeadAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        inner_dim = config.d_kv * config.num_heads
        self.num_heads = config.num_heads
        self.query_proj = nn.Linear(config.d_model, inner_dim, bias=False)
        self.key_proj = nn.Linear(config.d_model, inner_dim, bias=False)
        self.value_proj = nn.Linear(config.d_model, inner_dim, bias=False)
        self.out_proj = nn.Linear(inner_dim, config.d_model, bias=False)

    def __call__(
        self,
        queries: mx.array,
        keys: mx.array,
        values: mx.array,
        mask: Optional[mx.array],
        cache: Optional[Tuple[mx.array, mx.array]] = None,
    ) -> [mx.array, Tuple[mx.array, mx.array]]:
        queries = self.query_proj(queries)
        keys = self.key_proj(keys)
        values = self.value_proj(values)

        num_heads = self.num_heads
        B, L, _ = queries.shape
        _, S, _ = keys.shape
        queries = queries.reshape(B, L, num_heads, -1).transpose(0, 2, 1, 3)
        keys = keys.reshape(B, S, num_heads, -1).transpose(0, 2, 3, 1)
        values = values.reshape(B, S, num_heads, -1).transpose(0, 2, 1, 3)

        if cache is not None:
            key_cache, value_cache = cache
            keys = mx.concatenate([key_cache, keys], axis=3)
            values = mx.concatenate([value_cache, values], axis=2)

        # Dimensions are [batch x num heads x sequence x hidden dim]
        scores = queries @ keys
        if mask is not None:
            scores = scores + mask.astype(scores.dtype)

        scores = mx.softmax(scores.astype(mx.float32), axis=-1).astype(scores.dtype)
        values_hat = (scores @ values).transpose(0, 2, 1, 3).reshape(B, L, -1)
        return self.out_proj(values_hat), (keys, values)


class DenseActivation(nn.Module):
    def __init__(self, config):
        super().__init__()
        mlp_dims = config.d_ff or config.d_model * 4
        self.gated = hasattr(config, "feed_forward_proj")
        activation = (
            "relu"
            if not self.gated
            else config.feed_forward_proj.removeprefix("gated-")
        )
        if self.gated:
            self.wi_0 = nn.Linear(config.d_model, mlp_dims, bias=False)
            self.wi_1 = nn.Linear(config.d_model, mlp_dims, bias=False)
        else:
            self.wi = nn.Linear(config.d_model, mlp_dims, bias=False)
        self.wo = nn.Linear(mlp_dims, config.d_model, bias=False)
        if activation == "relu":
            self.act = nn.relu
        elif activation == "gelu":
            self.act = nn.gelu
        elif activation == "silu":
            self.act = nn.silu
        else:
            raise ValueError(f"Unknown activation: {activation}")

    def __call__(self, x):
        if self.gated:
            hidden_act = self.act(self.wi_0(x))
            hidden_linear = self.wi_1(x)
            x = hidden_act * hidden_linear
        else:
            x = self.act(self.wi(x))
        return self.wo(x)


class TransformerEncoderLayer(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.attention = MultiHeadAttention(config)
        self.ln1 = nn.RMSNorm(config.d_model, eps=config.layer_norm_epsilon)
        self.ln2 = nn.RMSNorm(config.d_model, eps=config.layer_norm_epsilon)
        self.dense = DenseActivation(config)

    def __call__(self, x, mask):
        y = self.ln1(x)
        y, _ = self.attention(y, y, y, mask=mask)
        x = x + y

        y = self.ln2(x)
        y = self.dense(y)
        return x + y


class TransformerEncoder(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.layers = [
            TransformerEncoderLayer(config) for i in range(config.num_layers)
        ]
        self.ln = nn.RMSNorm(config.d_model, eps=config.layer_norm_epsilon)
        self.relative_attention_bias = RelativePositionBias(config, bidirectional=True)

    def __call__(self, x: mx.array):
        pos_bias = self.relative_attention_bias(x.shape[1], x.shape[1])
        for layer in self.layers:
            x = layer(x, mask=pos_bias)
        return self.ln(x)


class TransformerDecoderLayer(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.self_attention = MultiHeadAttention(config)
        self.cross_attention = MultiHeadAttention(config)
        self.ln1 = nn.RMSNorm(config.d_model, eps=config.layer_norm_epsilon)
        self.ln2 = nn.RMSNorm(config.d_model, eps=config.layer_norm_epsilon)
        self.ln3 = nn.RMSNorm(config.d_model, eps=config.layer_norm_epsilon)
        self.dense = DenseActivation(config)

    def __call__(
        self,
        x: mx.array,
        memory: mx.array,
        mask: mx.array,
        memory_mask: mx.array,
        cache: Optional[List[Tuple[mx.array, mx.array]]] = None,
    ):
        y = self.ln1(x)
        y, cache = self.self_attention(y, y, y, mask, cache)
        x = x + y

        y = self.ln2(x)
        y, _ = self.cross_attention(y, memory, memory, memory_mask)
        x = x + y

        y = self.ln3(x)
        y = self.dense(y)
        x = x + y

        return x, cache


class TransformerDecoder(nn.Module):
    def __init__(self, config):
        super().__init__()
        n_layers = getattr(config, "num_decoder_layers", config.num_layers)
        self.layers = [TransformerDecoderLayer(config) for i in range(n_layers)]
        self.ln = nn.RMSNorm(config.d_model, eps=config.layer_norm_epsilon)
        self.relative_attention_bias = RelativePositionBias(config, bidirectional=False)

    def __call__(self, x, memory, mask, memory_mask, cache=None):
        if cache is not None:
            offset = cache[0][0].shape[3]
        else:
            offset = 0
            cache = [None] * len(self.layers)

        T = offset + x.shape[1]
        pos_bias = self.relative_attention_bias(T, T, offset=offset)
        if mask is not None:
            mask += pos_bias
        else:
            mask = pos_bias

        for e, layer in enumerate(self.layers):
            x, cache[e] = layer(x, memory, mask, memory_mask, cache=cache[e])
        x = self.ln(x)

        return x, cache


class OutputHead(nn.Module):
    def __init__(self, config):
        self.linear = nn.Linear(config.d_model, config.vocab_size, bias=False)

    def __call__(self, inputs):
        return self.linear(inputs)


class T5(nn.Module):
    def __init__(self, config):
        self.wte = nn.Embedding(config.vocab_size, config.d_model)
        self.encoder = TransformerEncoder(config)
        self.decoder = TransformerDecoder(config)
        self.tie_word_embeddings = getattr(config, "tie_word_embeddings", True)
        if not self.tie_word_embeddings:
            self.lm_head = OutputHead(config)
        self.model_dim = config.d_model

    def encode(self, inputs: mx.array):
        return self.encoder(self.wte(inputs))

    def decode(
        self,
        inputs: mx.array,
        memory: mx.array,
        cache=None,
    ):
        inputs = self.wte(inputs)
        T = inputs.shape[1]
        if T > 1:
            mask = nn.MultiHeadAttention.create_additive_causal_mask(T)
            mask = mask.astype(inputs.dtype)
        else:
            mask = None

        y, cache = self.decoder(
            inputs, memory=memory, mask=mask, memory_mask=None, cache=cache
        )
        if not self.tie_word_embeddings:
            y = self.lm_head(y)
        else:
            y *= self.model_dim**-0.5
            y = y @ self.wte.weight.T
        return y, cache

    def __call__(
        self,
        inputs: mx.array,
        decoder_inputs: mx.array,
    ):
        return self.decode(decoder_inputs, self.encode(inputs))[0]

    @classmethod
    def sanitize(cls, weights):
        shared_replacement_patterns = [
            (".block.", ".layers."),
            (".k.", ".key_proj."),
            (".o.", ".out_proj."),
            (".q.", ".query_proj."),
            (".v.", ".value_proj."),
            ("shared.", "wte."),
            ("lm_head.", "lm_head.linear."),
            (".layer.0.layer_norm.", ".ln1."),
            (".layer.1.layer_norm.", ".ln2."),
            (".layer.2.layer_norm.", ".ln3."),
            (".final_layer_norm.", ".ln."),
            (
                "layers.0.layer.0.SelfAttention.relative_attention_bias.",
                "relative_attention_bias.embeddings.",
            ),
        ]

        encoder_replacement_patterns = [
            (".layer.0.SelfAttention.", ".attention."),
            (".layer.1.DenseReluDense.", ".dense."),
        ]

        decoder_replacement_patterns = [
            (".layer.0.SelfAttention.", ".self_attention."),
            (".layer.1.EncDecAttention.", ".cross_attention."),
            (".layer.2.DenseReluDense.", ".dense."),
        ]

        ignored_keys = [
            "decoder.layers.0.cross_attention.relative_attention_bias.weight"
        ]

        def replace_key(key: str) -> str:
            for old, new in shared_replacement_patterns:
                key = key.replace(old, new)
            if key.startswith("encoder."):
                for old, new in encoder_replacement_patterns:
                    key = key.replace(old, new)
            elif key.startswith("decoder."):
                for old, new in decoder_replacement_patterns:
                    key = key.replace(old, new)
            return key

        weights = {replace_key(k): v for k, v in weights.items()}
        for key in ignored_keys:
            if key in weights:
                del weights[key]
        return weights

    @classmethod
    def from_pretrained(
        cls, path_or_repo: str, dtype: mx.Dtype = mx.bfloat16
    ) -> tuple["T5", Tokenizer]:
        from huggingface_hub import snapshot_download

        path = Path(path_or_repo)
        if not path.exists():
            path = Path(
                snapshot_download(
                    repo_id=path_or_repo,
                    allow_patterns=["*.json", "*.safetensors", "*.model"],
                )
            )

        with open(path / "config.json", "r") as f:
            config = SimpleNamespace(**json.load(f))

        model = T5(config)
        weights = mx.load(str(path / "model.safetensors"))
        weights = cls.sanitize(weights)
        weights = {k: v.astype(dtype) for k, v in weights.items()}
        model.load_weights(list(weights.items()))
        return model, Tokenizer(config, "t5-base")


def generate(prompt: str, model: T5, tokenizer: Tokenizer, temp: Optional[float] = 0.0):
    def sample(logits):
        if temp == 0:
            return mx.argmax(logits, axis=-1)
        else:
            return mx.random.categorical(logits * (1 / temp))

    prompt = tokenizer.encode(prompt)
    decoder_inputs = mx.array([tokenizer.decoder_start_id])
    memory = model.encode(prompt)
    cache = None
    y = decoder_inputs
    while True:
        logits, cache = model.decode(y[None], memory, cache=cache)
        y = sample(logits[:, -1, :])
        yield y.squeeze()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="T5 Inference script")
    parser.add_argument(
        "--model",
        type=str,
        help="Name of the T5 model.",
        default="t5-small",
    )
    parser.add_argument(
        "--prompt",
        help="",
        default="translate English to German: That is good.",
    )
    parser.add_argument(
        "--encode-only",
        action="store_true",
        default=False,
        help="Whether to decode or not. If true, will output last layer of encoder.",
    )
    parser.add_argument(
        "--max-tokens",
        "-m",
        type=int,
        default=100,
        help="Maximum number of tokens to generate",
    )
    parser.add_argument(
        "--temp",
        help="The sampling temperature.",
        type=float,
        default=0.0,
    )
    parser.add_argument(
        "--dtype",
        help="The model data type.",
        type=str,
        choices=["float16", "bfloat16", "float32"],
        default="bfloat16",
    )

    parser.add_argument("--seed", type=int, default=0, help="The PRNG seed")
    args = parser.parse_args()

    mx.random.seed(args.seed)

    dtype = getattr(mx, args.dtype)
    model, tokenizer = T5.from_pretrained(args.model, dtype)

    if args.encode_only:
        print("[INFO] Encoding with T5...", flush=True)
        print(args.prompt, flush=True)
        encoder_output = model.encode(tokenizer.encode(args.prompt))
        print(encoder_output, flush=True)
        exit(0)

    print("[INFO] Generating with T5...", flush=True)
    print("Input: ", args.prompt, flush=True)

    start = perf_counter_ns()
    for token, n_tokens in zip(
        generate(args.prompt, model, tokenizer, args.temp), range(args.max_tokens)
    ):
        if token.item() == tokenizer.eos_id:
            break
        print(
            tokenizer.decode([token.item()], with_sep=n_tokens > 0),
            end="",
            flush=True,
        )

    n_tokens += 1
    end = perf_counter_ns()
    elapsed = (end - start) / 1.0e9
    print()
    print(f"Time: {elapsed:.2f} seconds, tokens/s: {n_tokens / elapsed:.2f}")

>>>> bert/README.md
# BERT

An implementation of BERT [(Devlin, et al., 2019)](https://aclanthology.org/N19-1423/) in MLX.

## Setup 

Install the requirements:

```
pip install -r requirements.txt
```

Then convert the weights with:

```
python convert.py 
    --bert-model bert-base-uncased 
    --mlx-model weights/bert-base-uncased.npz
```

## Usage

To use the `Bert` model in your own code, you can load it with:

```python
import mlx.core as mx
from model import Bert, load_model

model, tokenizer = load_model(
    "bert-base-uncased",
    "weights/bert-base-uncased.npz")

batch = ["This is an example of BERT working on MLX."]
tokens = tokenizer(batch, return_tensors="np", padding=True)
tokens = {key: mx.array(v) for key, v in tokens.items()}

output, pooled = model(**tokens)
```

The `output` contains a `Batch x Tokens x Dims` tensor, representing a vector
for every input token. If you want to train anything at the **token-level**,
use this.

The `pooled` contains a `Batch x Dims` tensor, which is the pooled
representation for each input. If you want to train a **classification**
model, use this.


## Test

You can check the output for the default model (`bert-base-uncased`) matches the
Hugging Face version with:

```
python test.py
```

>>>> bert/model.py
import argparse
from pathlib import Path
from typing import List, Optional, Tuple

import mlx.core as mx
import mlx.nn as nn
from mlx.utils import tree_unflatten
from transformers import AutoConfig, AutoTokenizer, PreTrainedTokenizerBase


class TransformerEncoderLayer(nn.Module):
    """
    A transformer encoder layer with (the original BERT) post-normalization.
    """

    def __init__(
        self,
        dims: int,
        num_heads: int,
        mlp_dims: Optional[int] = None,
        layer_norm_eps: float = 1e-12,
    ):
        super().__init__()
        mlp_dims = mlp_dims or dims * 4
        self.attention = nn.MultiHeadAttention(dims, num_heads, bias=True)
        self.ln1 = nn.LayerNorm(dims, eps=layer_norm_eps)
        self.ln2 = nn.LayerNorm(dims, eps=layer_norm_eps)
        self.linear1 = nn.Linear(dims, mlp_dims)
        self.linear2 = nn.Linear(mlp_dims, dims)
        self.gelu = nn.GELU()

    def __call__(self, x, mask):
        attention_out = self.attention(x, x, x, mask)
        add_and_norm = self.ln1(x + attention_out)

        ff = self.linear1(add_and_norm)
        ff_gelu = self.gelu(ff)
        ff_out = self.linear2(ff_gelu)
        x = self.ln2(ff_out + add_and_norm)

        return x


class TransformerEncoder(nn.Module):
    def __init__(
        self, num_layers: int, dims: int, num_heads: int, mlp_dims: Optional[int] = None
    ):
        super().__init__()
        self.layers = [
            TransformerEncoderLayer(dims, num_heads, mlp_dims)
            for i in range(num_layers)
        ]

    def __call__(self, x, mask):
        for layer in self.layers:
            x = layer(x, mask)

        return x


class BertEmbeddings(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)
        self.token_type_embeddings = nn.Embedding(
            config.type_vocab_size, config.hidden_size
        )
        self.position_embeddings = nn.Embedding(
            config.max_position_embeddings, config.hidden_size
        )
        self.norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)

    def __call__(
        self, input_ids: mx.array, token_type_ids: mx.array = None
    ) -> mx.array:
        words = self.word_embeddings(input_ids)
        position = self.position_embeddings(
            mx.broadcast_to(mx.arange(input_ids.shape[1]), input_ids.shape)
        )

        if token_type_ids is None:
            # If token_type_ids is not provided, default to zeros
            token_type_ids = mx.zeros_like(input_ids)

        token_types = self.token_type_embeddings(token_type_ids)

        embeddings = position + words + token_types
        return self.norm(embeddings)


class Bert(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.embeddings = BertEmbeddings(config)
        self.encoder = TransformerEncoder(
            num_layers=config.num_hidden_layers,
            dims=config.hidden_size,
            num_heads=config.num_attention_heads,
            mlp_dims=config.intermediate_size,
        )
        self.pooler = nn.Linear(config.hidden_size, config.hidden_size)

    def __call__(
        self,
        input_ids: mx.array,
        token_type_ids: mx.array = None,
        attention_mask: mx.array = None,
    ) -> Tuple[mx.array, mx.array]:
        x = self.embeddings(input_ids, token_type_ids)

        if attention_mask is not None:
            # convert 0's to -infs, 1's to 0's, and make it broadcastable
            attention_mask = mx.log(attention_mask)
            attention_mask = mx.expand_dims(attention_mask, (1, 2))

        y = self.encoder(x, attention_mask)
        return y, mx.tanh(self.pooler(y[:, 0]))


def load_model(
    bert_model: str, weights_path: str
) -> Tuple[Bert, PreTrainedTokenizerBase]:
    if not Path(weights_path).exists():
        raise ValueError(f"No model weights found in {weights_path}")

    config = AutoConfig.from_pretrained(bert_model)

    # create and update the model
    model = Bert(config)
    model.load_weights(weights_path)

    tokenizer = AutoTokenizer.from_pretrained(bert_model)

    return model, tokenizer


def run(bert_model: str, mlx_model: str, batch: List[str]):
    model, tokenizer = load_model(bert_model, mlx_model)

    tokens = tokenizer(batch, return_tensors="np", padding=True)
    tokens = {key: mx.array(v) for key, v in tokens.items()}

    return model(**tokens)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run the BERT model using MLX.")
    parser.add_argument(
        "--bert-model",
        type=str,
        default="bert-base-uncased",
        help="The huggingface name of the BERT model to save.",
    )
    parser.add_argument(
        "--mlx-model",
        type=str,
        default="weights/bert-base-uncased.npz",
        help="The path of the stored MLX BERT weights (npz file).",
    )
    parser.add_argument(
        "--text",
        type=str,
        default="This is an example of BERT working in MLX",
        help="The text to generate embeddings for.",
    )
    args = parser.parse_args()
    run(args.bert_model, args.mlx_model, args.text)

>>>> bert/test.py
import argparse
from typing import List

import model
import numpy as np
from transformers import AutoModel, AutoTokenizer


def run_torch(bert_model: str, batch: List[str]):
    tokenizer = AutoTokenizer.from_pretrained(bert_model)
    torch_model = AutoModel.from_pretrained(bert_model)
    torch_tokens = tokenizer(batch, return_tensors="pt", padding=True)
    torch_forward = torch_model(**torch_tokens)
    torch_output = torch_forward.last_hidden_state.detach().numpy()
    torch_pooled = torch_forward.pooler_output.detach().numpy()
    return torch_output, torch_pooled


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Run a BERT-like model for a batch of text."
    )
    parser.add_argument(
        "--bert-model",
        type=str,
        default="bert-base-uncased",
        help="The model identifier for a BERT-like model from Hugging Face Transformers.",
    )
    parser.add_argument(
        "--mlx-model",
        type=str,
        default="weights/bert-base-uncased.npz",
        help="The path of the stored MLX BERT weights (npz file).",
    )
    parser.add_argument(
        "--text",
        nargs="+",
        default=["This is an example of BERT working in MLX."],
        help="A batch of texts to process. Multiple texts should be separated by spaces.",
    )

    args = parser.parse_args()

    torch_output, torch_pooled = run_torch(args.bert_model, args.text)

    mlx_output, mlx_pooled = model.run(args.bert_model, args.mlx_model, args.text)

    if torch_pooled is not None and mlx_pooled is not None:
        assert np.allclose(
            torch_output, mlx_output, rtol=1e-4, atol=1e-5
        ), "Model output is different"
        assert np.allclose(
            torch_pooled, mlx_pooled, rtol=1e-4, atol=1e-5
        ), "Model pooled output is different"
        print("Tests pass :)")
    else:
        print("Pooled outputs were not compared due to one or both being None.")

>>>> bert/convert.py
import argparse

import numpy
from transformers import AutoModel


def replace_key(key: str) -> str:
    key = key.replace(".layer.", ".layers.")
    key = key.replace(".self.key.", ".key_proj.")
    key = key.replace(".self.query.", ".query_proj.")
    key = key.replace(".self.value.", ".value_proj.")
    key = key.replace(".attention.output.dense.", ".attention.out_proj.")
    key = key.replace(".attention.output.LayerNorm.", ".ln1.")
    key = key.replace(".output.LayerNorm.", ".ln2.")
    key = key.replace(".intermediate.dense.", ".linear1.")
    key = key.replace(".output.dense.", ".linear2.")
    key = key.replace(".LayerNorm.", ".norm.")
    key = key.replace("pooler.dense.", "pooler.")
    return key


def convert(bert_model: str, mlx_model: str) -> None:
    model = AutoModel.from_pretrained(bert_model)
    # save the tensors
    tensors = {
        replace_key(key): tensor.numpy() for key, tensor in model.state_dict().items()
    }
    numpy.savez(mlx_model, **tensors)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Convert BERT weights to MLX.")
    parser.add_argument(
        "--bert-model",
        type=str,
        default="bert-base-uncased",
        help="The huggingface name of the BERT model to save. Any BERT-like model can be specified.",
    )
    parser.add_argument(
        "--mlx-model",
        type=str,
        default="weights/bert-base-uncased.npz",
        help="The output path for the MLX BERT weights.",
    )
    args = parser.parse_args()

    convert(args.bert_model, args.mlx_model)

>>>> whisper/setup.py
# Copyright © 2024 Apple Inc.

import sys
from pathlib import Path

from setuptools import find_namespace_packages, setup

package_dir = Path(__file__).parent / "mlx_whisper"

with open(package_dir / "requirements.txt") as fid:
    requirements = [l.strip() for l in fid.readlines()]

sys.path.append(str(package_dir))

from _version import __version__

setup(
    name="mlx-whisper",
    version=__version__,
    description="OpenAI Whisper on Apple silicon with MLX and the Hugging Face Hub",
    long_description=open("README.md", encoding="utf-8").read(),
    long_description_content_type="text/markdown",
    readme="README.md",
    author_email="mlx@group.apple.com",
    author="MLX Contributors",
    url="https://github.com/ml-explore/mlx-examples",
    license="MIT",
    install_requires=requirements,
    packages=find_namespace_packages(),
    include_package_data=True,
    python_requires=">=3.8",
    entry_points={
        "console_scripts": [
            "mlx_whisper = mlx_whisper.cli:main",
        ]
    },
)

>>>> whisper/README.md
# Whisper

Speech recognition with Whisper in MLX. Whisper is a set of open source speech
recognition models from OpenAI, ranging from 39 million to 1.5 billion
parameters.[^1]

### Setup

Install [`ffmpeg`](https://ffmpeg.org/):

```
# on macOS using Homebrew (https://brew.sh/)
brew install ffmpeg
```

Install the `mlx-whisper` package with:

```
pip install mlx-whisper
```

### Run

#### CLI

At its simplest:

```sh
mlx_whisper audio_file.mp3
```

This will make a text file `audio_file.txt` with the results.

Use `-f` to specify the output format and `--model` to specify the model. There
are many other supported command line options. To see them all, run
`mlx_whisper -h`.

You can also pipe the audio content of other programs via stdin:

```sh
some-process | mlx_whisper -
```

The default output file name will be `content.*`. You can specify the name with
the `--output-name` flag.

#### API

Transcribe audio with:

```python
import mlx_whisper

text = mlx_whisper.transcribe(speech_file)["text"]
```

The default model is "mlx-community/whisper-tiny". Choose the model by
setting `path_or_hf_repo`. For example:

```python
result = mlx_whisper.transcribe(speech_file, path_or_hf_repo="models/large")
```

This will load the model contained in `models/large`. The `path_or_hf_repo` can
also point to an MLX-style Whisper model on the Hugging Face Hub. In this case,
the model will be automatically downloaded. A [collection of pre-converted
Whisper
models](https://huggingface.co/collections/mlx-community/whisper-663256f9964fbb1177db93dc)
are in the Hugging Face MLX Community.

The `transcribe` function also supports word-level timestamps. You can generate
these with:

```python
output = mlx_whisper.transcribe(speech_file, word_timestamps=True)
print(output["segments"][0]["words"])
```

To see more transcription options use:

```
>>> help(mlx_whisper.transcribe)
```

### Converting models

> [!TIP]
> Skip the conversion step by using pre-converted checkpoints from the Hugging
> Face Hub. There are a few available in the [MLX
> Community](https://huggingface.co/mlx-community) organization.

To convert a model, first clone the MLX Examples repo:

```
git clone https://github.com/ml-explore/mlx-examples.git
```

Then run `convert.py` from `mlx-examples/whisper`. For example, to convert the
`tiny` model use:

```
python convert.py --torch-name-or-path tiny --mlx-path mlx_models/tiny
```

Note you can also convert a local PyTorch checkpoint which is in the original
OpenAI format.

To generate a 4-bit quantized model, use `-q`. For a full list of options:

```
python convert.py --help
```

By default, the conversion script will make the directory `mlx_models`
and save the converted `weights.npz` and `config.json` there.

Each time it is run, `convert.py` will overwrite any model in the provided
path. To save different models, make sure to set `--mlx-path` to a unique
directory for each converted model. For example:

```bash
model="tiny"
python convert.py --torch-name-or-path ${model} --mlx-path mlx_models/${model}_fp16
python convert.py --torch-name-or-path ${model} --dtype float32 --mlx-path mlx_models/${model}_fp32
python convert.py --torch-name-or-path ${model} -q --q_bits 4 --mlx-path mlx_models/${model}_quantized_4bits
```

[^1]: Refer to the [arXiv paper](https://arxiv.org/abs/2212.04356), [blog post](https://openai.com/research/whisper), and [code](https://github.com/openai/whisper) for more details.

>>>> whisper/test.py
# Copyright © 2023-2024 Apple Inc.

import json
import os
import unittest
from dataclasses import asdict
from pathlib import Path

import mlx.core as mx
import mlx_whisper
import mlx_whisper.audio as audio
import mlx_whisper.decoding as decoding
import mlx_whisper.load_models as load_models
import numpy as np
import torch
from convert import convert, load_torch_model, quantize
from mlx.utils import tree_flatten

MODEL_NAME = "tiny"
MLX_FP32_MODEL_PATH = "mlx_models/tiny_fp32"
MLX_FP16_MODEL_PATH = "mlx_models/tiny_fp16"
MLX_4BITS_MODEL_PATH = "mlx_models/tiny_quantized_4bits"
TEST_AUDIO = "mlx_whisper/assets/ls_test.flac"


def _save_model(save_dir, weights, config):
    mlx_path = Path(save_dir)
    mlx_path.mkdir(parents=True, exist_ok=True)

    # Save weights
    np.savez(str(mlx_path / "weights.npz"), **weights)

    # Save config.json with model_type
    with open(str(mlx_path / "config.json"), "w") as f:
        config["model_type"] = "whisper"
        json.dump(config, f, indent=4)

    config.pop("model_type", None)


def load_torch_and_mlx():
    torch_model = load_torch_model(MODEL_NAME)

    fp32_model = convert(MODEL_NAME, dtype=mx.float32)
    config = asdict(fp32_model.dims)
    weights = dict(tree_flatten(fp32_model.parameters()))
    _save_model(MLX_FP32_MODEL_PATH, weights, config)

    fp16_model = convert(MODEL_NAME, dtype=mx.float16)
    config = asdict(fp16_model.dims)
    weights = dict(tree_flatten(fp16_model.parameters()))
    _save_model(MLX_FP16_MODEL_PATH, weights, config)

    args = type("", (), {})()
    args.q_group_size = 64
    args.q_bits = 4
    weights, config = quantize(weights, config, args)
    _save_model(MLX_4BITS_MODEL_PATH, weights, config)

    return torch_model, fp32_model, fp16_model


def forward_torch(model, mels, tokens):
    mels = torch.Tensor(mels).to(torch.float32)
    tokens = torch.Tensor(tokens).to(torch.int32)
    with torch.no_grad():
        logits = model.forward(mels, tokens)
    return logits.numpy()


def forward_mlx(model, mels, tokens):
    mels = mx.array(mels.transpose(0, 2, 1))
    tokens = mx.array(tokens, mx.int32)
    logits = model(mels, tokens)
    return np.array(logits)


class TestWhisper(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        _, cls.model, _ = load_torch_and_mlx()
        data = audio.load_audio(TEST_AUDIO)
        data = audio.pad_or_trim(data)
        cls.mels = audio.log_mel_spectrogram(data)

    def test_torch_mlx(self):
        np.random.seed(10)

        torch_model = load_torch_model(MODEL_NAME)
        dims = torch_model.dims

        mels = np.random.randn(1, dims.n_mels, 3_000)
        tokens = np.random.randint(0, dims.n_vocab, (1, 20))

        torch_logits = forward_torch(torch_model, mels, tokens)

        mlx_logits = forward_mlx(self.model, mels, tokens)

        self.assertTrue(np.allclose(torch_logits, mlx_logits, atol=1e-2, rtol=1e-2))

    def test_fp16(self):
        mlx_model = load_models.load_model(MLX_FP16_MODEL_PATH, mx.float16)
        dims = mlx_model.dims
        mels = mx.array(np.random.randn(1, 3_000, dims.n_mels), mx.float16)
        tokens = mx.array(np.random.randint(0, dims.n_vocab, (1, 20)), mx.int32)
        logits = mlx_model(mels, tokens)
        self.assertEqual(logits.dtype, mx.float16)

    def test_quantized_4bits(self):
        mlx_model = load_models.load_model(MLX_4BITS_MODEL_PATH, mx.float16)
        dims = mlx_model.dims
        mels = mx.array(np.random.randn(1, 3_000, dims.n_mels), mx.float16)
        tokens = mx.array(np.random.randint(0, dims.n_vocab, (1, 20)), mx.int32)
        logits = mlx_model(mels, tokens)
        # Here, we just test if 4-bit models can forward, as the quantized tiny models struggle with accurate transcription
        self.assertEqual(logits.dtype, mx.float16)

    def test_decode_lang(self):
        options = decoding.DecodingOptions(task="lang_id", fp16=False)
        result = decoding.decode(self.model, self.mels, options)
        self.assertEqual(result.language, "en")
        self.assertEqual(len(result.language_probs), 99)
        self.assertAlmostEqual(
            result.language_probs["en"], 0.9947282671928406, places=5
        )

    def test_decode_greedy(self):
        result = decoding.decode(self.model, self.mels, fp16=False)
        self.assertEqual(result.language, "en")
        self.assertEqual(
            result.tokens,
            [
                50364,
                1396,
                264,
                665,
                5133,
                23109,
                25462,
                264,
                6582,
                293,
                750,
                632,
                42841,
                292,
                370,
                938,
                294,
                4054,
                293,
                12653,
                356,
                50620,
                50620,
                23563,
                322,
                3312,
                13,
                50680,
            ],
        )
        self.assertEqual(
            result.text,
            (
                "Then the good soul openly sorted the boat and she "
                "had buoyed so long in secret and bravely stretched on alone."
            ),
        )
        self.assertAlmostEqual(result.avg_logprob, -0.4975455382774616, places=3)
        self.assertAlmostEqual(result.no_speech_prob, 0.009631240740418434, places=4)
        self.assertAlmostEqual(result.compression_ratio, 1.2359550561797752)

        # Small temp should give the same results
        result = decoding.decode(self.model, self.mels, temperature=1e-8, fp16=False)

        self.assertEqual(
            result.text,
            (
                "Then the good soul openly sorted the boat and she "
                "had buoyed so long in secret and bravely stretched on alone."
            ),
        )
        self.assertAlmostEqual(result.avg_logprob, -0.4975455382774616, places=3)
        self.assertAlmostEqual(result.no_speech_prob, 0.009631240740418434, places=4)
        self.assertAlmostEqual(result.compression_ratio, 1.2359550561797752)

    def test_transcribe(self):
        result = mlx_whisper.transcribe(
            TEST_AUDIO, path_or_hf_repo=MLX_FP32_MODEL_PATH, fp16=False
        )
        self.assertEqual(
            result["text"],
            (
                " Then the good soul openly sorted the boat and she "
                "had buoyed so long in secret and bravely stretched on alone."
            ),
        )

    def test_transcribe_alice(self):
        audio_file = os.path.join(
            os.path.expanduser("~"),
            ".cache/whisper/alice.mp3",
        )
        if not os.path.exists(audio_file):
            print("To run this test download the alice in wonderland audiobook:")
            print("bash path_to_whisper_repo/whisper/assets/download_alice.sh")
            return

        result = mlx_whisper.transcribe(
            audio_file, path_or_hf_repo=MLX_FP32_MODEL_PATH, fp16=False
        )
        self.assertEqual(len(result["text"]), 10920)
        self.assertEqual(result["language"], "en")
        self.assertEqual(len(result["segments"]), 77)

        expected_5 = {
            "id": 5,
            "seek": 2800,
            "start": 40.0,
            "end": 46.0,
            "text": " Oh my poor little feet, I wonder who will put on your shoes and stockings for you now tears.",
            "tokens": [
                50964,
                876,
                452,
                4716,
                707,
                3521,
                11,
                286,
                2441,
                567,
                486,
                829,
                322,
                428,
                6654,
                293,
                4127,
                1109,
                337,
                291,
                586,
                10462,
                13,
                51264,
            ],
            "temperature": 0.0,
            "avg_logprob": -0.19670599699020386,
            "compression_ratio": 1.5991379310344827,
            "no_speech_prob": 0.09746722131967545,
        }

        expected_73 = {
            "id": 73,
            "seek": 70700,
            "start": 707.0,
            "end": 715.0,
            "text": " let us get to the shore, and then I'll tell you my history, and you'll understand why it is that I hate cats and dogs.",
            "tokens": [
                50364,
                718,
                505,
                483,
                281,
                264,
                17805,
                11,
                293,
                550,
                286,
                603,
                980,
                291,
                452,
                2503,
                11,
                293,
                291,
                603,
                1223,
                983,
                309,
                307,
                300,
                286,
                4700,
                11111,
                293,
                7197,
                13,
                50764,
            ],
            "temperature": 0.0,
            "avg_logprob": -0.1350895343440594,
            "compression_ratio": 1.6208333333333333,
            "no_speech_prob": 0.009053784422576427,
        }

        def check_segment(seg, expected):
            for k, v in expected.items():
                if isinstance(v, float):
                    self.assertAlmostEqual(seg[k], v, places=2)
                else:
                    self.assertEqual(seg[k], v)

        # Randomly check a couple of segments
        check_segment(result["segments"][5], expected_5)
        check_segment(result["segments"][73], expected_73)

    def test_transcribe_word_level_timestamps_confidence_scores(self):
        result = mlx_whisper.transcribe(
            TEST_AUDIO,
            path_or_hf_repo=MLX_FP16_MODEL_PATH,
            word_timestamps=True,
        )

        # result predicted with openai-whisper
        expected_0 = [
            {
                "word": " Then",
                "start": 0.0,
                "end": 0.94,
                "probability": 0.855542778968811,
            },
            {
                "word": " the",
                "start": 0.94,
                "end": 1.12,
                "probability": 0.6500106453895569,
            },
            {
                "word": " good",
                "start": 1.12,
                "end": 1.32,
                "probability": 0.5503873825073242,
            },
            {
                "word": " soul",
                "start": 1.32,
                "end": 1.56,
                "probability": 0.46757155656814575,
            },
            {
                "word": " openly",
                "start": 1.56,
                "end": 2.0,
                "probability": 0.9840946793556213,
            },
            {
                "word": " sorted",
                "start": 2.0,
                "end": 2.38,
                "probability": 0.24167272448539734,
            },
            {
                "word": " the",
                "start": 2.38,
                "end": 2.58,
                "probability": 0.9875414967536926,
            },
            {
                "word": " boat",
                "start": 2.58,
                "end": 2.8,
                "probability": 0.5856029391288757,
            },
            {
                "word": " and",
                "start": 2.8,
                "end": 2.98,
                "probability": 0.913351833820343,
            },
            {
                "word": " she",
                "start": 2.98,
                "end": 3.1,
                "probability": 0.9913808703422546,
            },
            {
                "word": " had",
                "start": 3.1,
                "end": 3.32,
                "probability": 0.9952940344810486,
            },
            {
                "word": " buoyed",
                "start": 3.32,
                "end": 3.58,
                "probability": 0.6411589980125427,
            },
            {
                "word": " so",
                "start": 3.58,
                "end": 3.8,
                "probability": 0.9682658314704895,
            },
            {
                "word": " long",
                "start": 3.8,
                "end": 4.06,
                "probability": 0.9953522682189941,
            },
            {
                "word": " in",
                "start": 4.06,
                "end": 4.26,
                "probability": 0.6745936870574951,
            },
            {
                "word": " secret",
                "start": 4.26,
                "end": 4.56,
                "probability": 0.9905064702033997,
            },
            {
                "word": " and",
                "start": 4.56,
                "end": 4.9,
                "probability": 0.856008768081665,
            },
            {
                "word": " bravely",
                "start": 4.9,
                "end": 5.28,
                "probability": 0.8477402329444885,
            },
        ]

        def check_words(words, expected_words):
            for word, expected_word in zip(words, expected_words):
                for k, v in expected_word.items():
                    if isinstance(v, float):
                        self.assertAlmostEqual(word[k], v, places=1)
                    else:
                        self.assertEqual(word[k], v)

        # Randomly check a couple of segments
        check_words(result["segments"][0]["words"], expected_0)


class TestAudio(unittest.TestCase):
    def test_load(self):
        data = audio.load_audio(TEST_AUDIO)
        data_8k = audio.load_audio(TEST_AUDIO, 8000)
        n = 106640
        self.assertTrue(data.shape, (n,))
        self.assertTrue(data.dtype, np.float32)
        self.assertTrue(data_8k.shape, (n // 2,))

    def test_pad(self):
        data = audio.load_audio(TEST_AUDIO)
        data = audio.pad_or_trim(data, 20_000)
        self.assertTrue(data.shape, [20_000])

    def test_mel_spec(self):
        mels = audio.log_mel_spectrogram(TEST_AUDIO)
        self.assertTrue(mels.shape, [80, 400])
        self.assertTrue(mels.dtype, mx.float32)


if __name__ == "__main__":
    unittest.main()

>>>> whisper/MANIFEST.in
include mlx_whisper/requirements.txt
include mlx_whisper/assets/mel_filters.npz
include mlx_whisper/assets/multilingual.tiktoken
include mlx_whisper/assets/gpt2.tiktoken

>>>> whisper/convert.py
# Copyright © 2023-2024 Apple Inc.

import argparse
import copy
import hashlib
import json
import os
import urllib
import warnings
from dataclasses import asdict
from pathlib import Path
from typing import List

import mlx.core as mx
import mlx.nn as nn
import numpy as np
import torch
from mlx.utils import tree_flatten, tree_map, tree_unflatten
from mlx_whisper import torch_whisper
from mlx_whisper.whisper import ModelDimensions, Whisper
from tqdm import tqdm

_VALID_DTYPES = {"float16", "float32"}

_MODELS = {
    "tiny.en": "https://openaipublic.azureedge.net/main/whisper/models/d3dd57d32accea0b295c96e26691aa14d8822fac7d9d27d5dc00b4ca2826dd03/tiny.en.pt",
    "tiny": "https://openaipublic.azureedge.net/main/whisper/models/65147644a518d12f04e32d6f3b26facc3f8dd46e5390956a9424a650c0ce22b9/tiny.pt",
    "base.en": "https://openaipublic.azureedge.net/main/whisper/models/25a8566e1d0c1e2231d1c762132cd20e0f96a85d16145c3a00adf5d1ac670ead/base.en.pt",
    "base": "https://openaipublic.azureedge.net/main/whisper/models/ed3a0b6b1c0edf879ad9b11b1af5a0e6ab5db9205f891f668f8b0e6c6326e34e/base.pt",
    "small.en": "https://openaipublic.azureedge.net/main/whisper/models/f953ad0fd29cacd07d5a9eda5624af0f6bcf2258be67c92b79389873d91e0872/small.en.pt",
    "small": "https://openaipublic.azureedge.net/main/whisper/models/9ecf779972d90ba49c06d968637d720dd632c55bbf19d441fb42bf17a411e794/small.pt",
    "medium.en": "https://openaipublic.azureedge.net/main/whisper/models/d7440d1dc186f76616474e0ff0b3b6b879abc9d1a4926b7adfa41db2d497ab4f/medium.en.pt",
    "medium": "https://openaipublic.azureedge.net/main/whisper/models/345ae4da62f9b3d59415adc60127b97c714f32e89e936602e85993674d08dcb1/medium.pt",
    "large-v1": "https://openaipublic.azureedge.net/main/whisper/models/e4b87e7e0bf463eb8e6956e646f1e277e901512310def2c24bf0e11bd3c28e9a/large-v1.pt",
    "large-v2": "https://openaipublic.azureedge.net/main/whisper/models/81f7c96c852ee8fc832187b0132e569d6c3065a3252ed18e56effd0b6a73e524/large-v2.pt",
    "large-v3": "https://openaipublic.azureedge.net/main/whisper/models/e5b1a55b89c1367dacf97e3e19bfd829a01529dbfdeefa8caeb59b3f1b81dadb/large-v3.pt",
    "large": "https://openaipublic.azureedge.net/main/whisper/models/e5b1a55b89c1367dacf97e3e19bfd829a01529dbfdeefa8caeb59b3f1b81dadb/large-v3.pt",
    "large-v3-turbo": "https://openaipublic.azureedge.net/main/whisper/models/aff26ae408abcba5fbf8813c21e62b0941638c5f6eebfb145be0c9839262a19a/large-v3-turbo.pt",
    "turbo": "https://openaipublic.azureedge.net/main/whisper/models/aff26ae408abcba5fbf8813c21e62b0941638c5f6eebfb145be0c9839262a19a/large-v3-turbo.pt",
}

# base85-encoded (n_layers, n_heads) boolean arrays indicating the cross-attention heads that are
# highly correlated to the word-level timing, i.e. the alignment between audio and text tokens.
_ALIGNMENT_HEADS = {
    "tiny.en": b"ABzY8J1N>@0{>%R00Bk>$p{7v037`oCl~+#00",
    "tiny": b"ABzY8bu8Lr0{>%RKn9Fp%m@SkK7Kt=7ytkO",
    "base.en": b"ABzY8;40c<0{>%RzzG;p*o+Vo09|#PsxSZm00",
    "base": b"ABzY8KQ!870{>%RzyTQH3`Q^yNP!>##QT-<FaQ7m",
    "small.en": b"ABzY8>?_)10{>%RpeA61k&I|OI3I$65C{;;pbCHh0B{qLQ;+}v00",
    "small": b"ABzY8DmU6=0{>%Rpa?J`kvJ6qF(V^F86#Xh7JUGMK}P<N0000",
    "medium.en": b"ABzY8usPae0{>%R7<zz_OvQ{)4kMa0BMw6u5rT}kRKX;$NfYBv00*Hl@qhsU00",
    "medium": b"ABzY8B0Jh+0{>%R7}kK1fFL7w6%<-Pf*t^=N)Qr&0RR9",
    "large-v1": b"ABzY8r9j$a0{>%R7#4sLmoOs{s)o3~84-RPdcFk!JR<kSfC2yj",
    "large-v2": b"ABzY8zd+h!0{>%R7=D0pU<_bnWW*tkYAhobTNnu$jnkEkXqp)j;w1Tzk)UH3X%SZd&fFZ2fC2yj",
    "large-v3": b"ABzY8gWO1E0{>%R7(9S+Kn!D~%ngiGaR?*L!iJG9p-nab0JQ=-{D1-g00",
    "large": b"ABzY8gWO1E0{>%R7(9S+Kn!D~%ngiGaR?*L!iJG9p-nab0JQ=-{D1-g00",
    "large-v3-turbo": b"ABzY8j^C+e0{>%RARaKHP%t(lGR*)0g!tONPyhe`",
    "turbo": b"ABzY8j^C+e0{>%RARaKHP%t(lGR*)0g!tONPyhe`",
}


def _download(url: str, root: str) -> str:
    os.makedirs(root, exist_ok=True)

    expected_sha256 = url.split("/")[-2]
    download_target = os.path.join(root, os.path.basename(url))

    if os.path.exists(download_target) and not os.path.isfile(download_target):
        raise RuntimeError(f"{download_target} exists and is not a regular file")

    if os.path.isfile(download_target):
        with open(download_target, "rb") as f:
            model_bytes = f.read()
        if hashlib.sha256(model_bytes).hexdigest() == expected_sha256:
            return download_target
        else:
            warnings.warn(
                f"{download_target} exists, but the SHA256 checksum does not match; re-downloading the file"
            )

    with urllib.request.urlopen(url) as source, open(download_target, "wb") as output:
        with tqdm(
            total=int(source.info().get("Content-Length")),
            ncols=80,
            unit="iB",
            unit_scale=True,
            unit_divisor=1024,
        ) as loop:
            while True:
                buffer = source.read(8192)
                if not buffer:
                    break

                output.write(buffer)
                loop.update(len(buffer))

    with open(download_target, "rb") as fid:
        model_bytes = fid.read()
    if hashlib.sha256(model_bytes).hexdigest() != expected_sha256:
        raise RuntimeError(
            "Model has been downloaded but the SHA256 checksum does not not match. Please retry loading the model."
        )

    return download_target


def available_models() -> List[str]:
    """Returns the names of available models"""
    return list(_MODELS.keys())


def hf_to_pt(weights, config):
    config = {
        "n_mels": config["num_mel_bins"],
        "n_audio_ctx": config["max_source_positions"],
        "n_audio_state": config["d_model"],
        "n_audio_head": config["encoder_attention_heads"],
        "n_audio_layer": config["encoder_layers"],
        "n_vocab": config["vocab_size"],
        "n_text_ctx": config["max_target_positions"],
        "n_text_state": config["d_model"],
        "n_text_head": config["decoder_attention_heads"],
        "n_text_layer": config["decoder_layers"],
    }

    def remap(k):
        k = k.replace("model.", "")
        k = k.replace(".layers", ".blocks")
        k = k.replace(".self_attn", ".attn")
        k = k.replace(".attn_layer_norm", ".attn_ln")
        k = k.replace(".encoder_attn.", ".cross_attn.")
        k = k.replace(".encoder_attn_layer_norm", ".cross_attn_ln")
        k = k.replace(".final_layer_norm", ".mlp_ln")
        k = k.replace(".q_proj", ".query")
        k = k.replace(".k_proj", ".key")
        k = k.replace(".v_proj", ".value")
        k = k.replace(".out_proj", ".out")
        k = k.replace(".fc1", ".mlp1")
        k = k.replace(".fc2", ".mlp2")
        k = k.replace("embed_positions.weight", "positional_embedding")
        k = k.replace("decoder.embed_tokens", "decoder.token_embedding")
        k = k.replace("encoder.layer_norm", "encoder.ln_post")
        k = k.replace("decoder.layer_norm", "decoder.ln")
        return k

    # token embeddings are shared with output projection
    weights.pop("proj_out.weight", None)
    weights = {remap(k): v for k, v in weights.items()}
    return weights, config


def load_torch_weights_and_config(
    name_or_path: str,
    download_root: str = None,
):
    if download_root is None:
        download_root = os.path.join(os.path.expanduser("~"), ".cache/whisper")

    # todo: accept alignment_heads of local Pytorch checkpoint
    alignment_heads = None
    if name_or_path in _MODELS:
        alignment_heads = _ALIGNMENT_HEADS[name_or_path]
        name_or_path = _download(_MODELS[name_or_path], download_root)
    elif not Path(name_or_path).exists():
        # Try downloading from HF
        from huggingface_hub import snapshot_download

        name_or_path = snapshot_download(
            repo_id=name_or_path,
            allow_patterns=[
                "*.json",
                "pytorch_model.bin",
                "model.safetensors",
                "*.txt",
            ],
        )

    if name_or_path.endswith(".pt"):
        checkpoint = torch.load(name_or_path, map_location="cpu", weights_only=False)
        weights, config = checkpoint["model_state_dict"], checkpoint["dims"]
    else:
        name_or_path = Path(name_or_path)
        pt_path = name_or_path / "pytorch_model.bin"
        if pt_path.is_file():
            weights = torch.load(pt_path, map_location="cpu")
        else:
            weights = mx.load(str(name_or_path / "model.safetensors"))
        with open(name_or_path / "config.json", "r") as fp:
            config = json.load(fp)
        weights, config = hf_to_pt(weights, config)

    return weights, config, alignment_heads


def load_torch_model(
    name_or_path: str,
    download_root: str = None,
) -> torch_whisper.Whisper:
    """
    Load a Whisper ASR model

    Parameters
    ----------
    name_or_path : str
        one of the official model names listed by `whisper.available_models()` or
        a local Pytorch checkpoint which is in the original OpenAI format
    download_root: str
        path to download the model files; by default, it uses "~/.cache/whisper"

    Returns
    -------
    model : Whisper
        The Whisper ASR model instance
    """

    if download_root is None:
        download_root = os.path.join(os.path.expanduser("~"), ".cache/whisper")

    weights, config, alignment_heads = load_torch_weights_and_config(
        name_or_path, download_root
    )
    dims = torch_whisper.ModelDimensions(**config)
    model = torch_whisper.Whisper(dims)
    model.load_state_dict(weights)

    if alignment_heads is not None:
        model.set_alignment_heads(alignment_heads)

    return model


def convert(name_or_path: str, dtype: mx.Dtype = mx.float16):
    def remap(key, value):
        key = key.replace("mlp.0", "mlp1")
        key = key.replace("mlp.2", "mlp2")
        if "conv" in key and value.ndim == 3:
            value = value.swapaxes(1, 2)
        if isinstance(value, torch.Tensor):
            value = mx.array(value.detach())
        return key, value.astype(dtype)

    weights, config, alignment_heads = load_torch_weights_and_config(name_or_path)
    weights.pop("encoder.positional_embedding", None)
    weights = dict(remap(k, v) for k, v in weights.items())

    model_dims = ModelDimensions(**config)
    model = Whisper(model_dims, dtype)
    model.load_weights(list(weights.items()), strict=False)

    if alignment_heads is not None:
        model.set_alignment_heads(alignment_heads)

    return model


def upload_to_hub(path: str, name: str, torch_name_or_path: str):
    import os

    from huggingface_hub import HfApi, ModelCard, logging

    repo_id = f"mlx-community/{name}"
    text = f"""
---
library_name: mlx
---

# {name}
This model was converted to MLX format from [`{torch_name_or_path}`]().

## Use with mlx
```bash
pip install mlx-whisper
```

```python
import mlx_whisper

result = mlx_whisper.transcribe(
    "FILE_NAME",
    path_or_hf_repo={repo_id},
)
```
"""
    card = ModelCard(text)
    card.save(os.path.join(path, "README.md"))

    logging.set_verbosity_info()

    api = HfApi()
    api.create_repo(repo_id=repo_id, exist_ok=True)
    api.upload_folder(
        folder_path=path,
        repo_id=repo_id,
        repo_type="model",
    )


def quantize(weights, config, args):
    quantized_config = copy.deepcopy(config)

    # Load the model:
    model = Whisper(ModelDimensions(**config))
    weights = tree_map(mx.array, weights)
    model.update(tree_unflatten(list(weights.items())))

    # Quantize the model:
    nn.quantize(model, args.q_group_size, args.q_bits)

    # Update the config:
    quantized_config["quantization"] = {
        "group_size": args.q_group_size,
        "bits": args.q_bits,
    }
    quantized_weights = dict(tree_flatten(model.parameters()))

    return quantized_weights, quantized_config


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Convert Whisper weights to MLX.")
    parser.add_argument(
        "--torch-name-or-path",
        type=str,
        default="tiny",
        help="The name or path to the PyTorch model.",
    )
    parser.add_argument(
        "--mlx-path",
        type=str,
        default="mlx_models",
        help="The path to save the MLX model.",
    )
    parser.add_argument(
        "--dtype",
        type=str,
        default="float16",
        help="The dtype to save the MLX model.",
    )
    parser.add_argument(
        "-q",
        "--quantize",
        help="Generate a quantized model.",
        action="store_true",
    )
    parser.add_argument(
        "--q-group-size",
        help="Group size for quantization.",
        type=int,
        default=64,
    )
    parser.add_argument(
        "--q-bits",
        help="Bits per weight for quantization.",
        type=int,
        default=4,
    )
    parser.add_argument(
        "--upload-name",
        help="The name of model to upload to Hugging Face MLX Community",
        type=str,
        default=None,
    )

    args = parser.parse_args()

    assert (
        args.dtype in _VALID_DTYPES
    ), f"dtype {args.dtype} not found in {_VALID_DTYPES}"
    dtype = getattr(mx, args.dtype)

    print("[INFO] Loading")
    model = convert(args.torch_name_or_path, dtype)
    config = asdict(model.dims)
    weights = dict(tree_flatten(model.parameters()))

    if args.quantize:
        print("[INFO] Quantizing")
        weights, config = quantize(weights, config, args)

    mlx_path = Path(args.mlx_path)
    mlx_path.mkdir(parents=True, exist_ok=True)

    # Save weights
    print("[INFO] Saving")
    mx.save_safetensors(str(mlx_path / "weights.safetensors"), weights)

    # Save config.json with model_type
    with open(str(mlx_path / "config.json"), "w") as f:
        config["model_type"] = "whisper"
        json.dump(config, f, indent=4)

    if args.upload_name is not None:
        upload_to_hub(mlx_path, args.upload_name, args.torch_name_or_path)

>>>> whisper/mlx_whisper/torch_whisper.py
# Copyright © 2023 Apple Inc.

import base64
import gzip
from dataclasses import dataclass
from typing import Dict, Iterable, Optional

import numpy as np
import torch
import torch.nn.functional as F
from torch import Tensor, nn


@dataclass
class ModelDimensions:
    n_mels: int
    n_audio_ctx: int
    n_audio_state: int
    n_audio_head: int
    n_audio_layer: int
    n_vocab: int
    n_text_ctx: int
    n_text_state: int
    n_text_head: int
    n_text_layer: int


class LayerNorm(nn.LayerNorm):
    def forward(self, x: Tensor) -> Tensor:
        return super().forward(x.float()).type(x.dtype)


class Linear(nn.Linear):
    def forward(self, x: Tensor) -> Tensor:
        return F.linear(
            x,
            self.weight.to(x.dtype),
            None if self.bias is None else self.bias.to(x.dtype),
        )


class Conv1d(nn.Conv1d):
    def _conv_forward(
        self, x: Tensor, weight: Tensor, bias: Optional[Tensor]
    ) -> Tensor:
        return super()._conv_forward(
            x, weight.to(x.dtype), None if bias is None else bias.to(x.dtype)
        )


def sinusoids(length, channels, max_timescale=10000):
    """Returns sinusoids for positional embedding"""
    assert channels % 2 == 0
    log_timescale_increment = np.log(max_timescale) / (channels // 2 - 1)
    inv_timescales = torch.exp(-log_timescale_increment * torch.arange(channels // 2))
    scaled_time = torch.arange(length)[:, np.newaxis] * inv_timescales[np.newaxis, :]
    return torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)


class MultiHeadAttention(nn.Module):
    def __init__(self, n_state: int, n_head: int):
        super().__init__()
        self.n_head = n_head
        self.query = Linear(n_state, n_state)
        self.key = Linear(n_state, n_state, bias=False)
        self.value = Linear(n_state, n_state)
        self.out = Linear(n_state, n_state)

    def forward(
        self,
        x: Tensor,
        xa: Optional[Tensor] = None,
        mask: Optional[Tensor] = None,
        kv_cache: Optional[dict] = None,
    ):
        q = self.query(x)

        if kv_cache is None or xa is None or self.key not in kv_cache:
            # hooks, if installed (i.e. kv_cache is not None), will prepend the cached kv tensors;
            # otherwise, perform key/value projections for self- or cross-attention as usual.
            k = self.key(x if xa is None else xa)
            v = self.value(x if xa is None else xa)
        else:
            # for cross-attention, calculate keys and values once and reuse in subsequent calls.
            k = kv_cache[self.key]
            v = kv_cache[self.value]

        wv, qk = self.qkv_attention(q, k, v, mask)
        return self.out(wv), qk

    def qkv_attention(
        self, q: Tensor, k: Tensor, v: Tensor, mask: Optional[Tensor] = None
    ):
        n_batch, n_ctx, n_state = q.shape
        scale = (n_state // self.n_head) ** -0.25
        q = q.view(*q.shape[:2], self.n_head, -1).permute(0, 2, 1, 3) * scale
        k = k.view(*k.shape[:2], self.n_head, -1).permute(0, 2, 3, 1) * scale
        v = v.view(*v.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)

        qk = q @ k
        if mask is not None:
            qk = qk + mask[:n_ctx, :n_ctx]
        qk = qk.float()

        w = F.softmax(qk, dim=-1).to(q.dtype)
        return (w @ v).permute(0, 2, 1, 3).flatten(start_dim=2), qk.detach()


class ResidualAttentionBlock(nn.Module):
    def __init__(self, n_state: int, n_head: int, cross_attention: bool = False):
        super().__init__()

        self.attn = MultiHeadAttention(n_state, n_head)
        self.attn_ln = LayerNorm(n_state)

        self.cross_attn = (
            MultiHeadAttention(n_state, n_head) if cross_attention else None
        )
        self.cross_attn_ln = LayerNorm(n_state) if cross_attention else None

        n_mlp = n_state * 4
        self.mlp = nn.Sequential(
            Linear(n_state, n_mlp), nn.GELU(), Linear(n_mlp, n_state)
        )
        self.mlp_ln = LayerNorm(n_state)

    def forward(
        self,
        x: Tensor,
        xa: Optional[Tensor] = None,
        mask: Optional[Tensor] = None,
        kv_cache: Optional[dict] = None,
    ):
        x = x + self.attn(self.attn_ln(x), mask=mask, kv_cache=kv_cache)[0]
        if self.cross_attn:
            x = x + self.cross_attn(self.cross_attn_ln(x), xa, kv_cache=kv_cache)[0]
        x = x + self.mlp(self.mlp_ln(x))
        return x


class AudioEncoder(nn.Module):
    def __init__(
        self, n_mels: int, n_ctx: int, n_state: int, n_head: int, n_layer: int
    ):
        super().__init__()
        self.conv1 = Conv1d(n_mels, n_state, kernel_size=3, padding=1)
        self.conv2 = Conv1d(n_state, n_state, kernel_size=3, stride=2, padding=1)
        self.register_buffer("positional_embedding", sinusoids(n_ctx, n_state))

        self.blocks: Iterable[ResidualAttentionBlock] = nn.ModuleList(
            [ResidualAttentionBlock(n_state, n_head) for _ in range(n_layer)]
        )
        self.ln_post = LayerNorm(n_state)

    def forward(self, x: Tensor):
        """
        x : torch.Tensor, shape = (batch_size, n_mels, n_ctx)
            the mel spectrogram of the audio
        """
        x = F.gelu(self.conv1(x))
        x = F.gelu(self.conv2(x))
        x = x.permute(0, 2, 1)

        assert x.shape[1:] == self.positional_embedding.shape, "incorrect audio shape"
        x = (x + self.positional_embedding).to(x.dtype)

        for block in self.blocks:
            x = block(x)

        x = self.ln_post(x)
        return x


class TextDecoder(nn.Module):
    def __init__(
        self, n_vocab: int, n_ctx: int, n_state: int, n_head: int, n_layer: int
    ):
        super().__init__()

        self.token_embedding = nn.Embedding(n_vocab, n_state)
        self.positional_embedding = nn.Parameter(torch.empty(n_ctx, n_state))

        self.blocks: Iterable[ResidualAttentionBlock] = nn.ModuleList(
            [
                ResidualAttentionBlock(n_state, n_head, cross_attention=True)
                for _ in range(n_layer)
            ]
        )
        self.ln = LayerNorm(n_state)

        mask = torch.empty(n_ctx, n_ctx).fill_(-np.inf).triu_(1)
        self.register_buffer("mask", mask, persistent=False)

    def forward(self, x: Tensor, xa: Tensor, kv_cache: Optional[dict] = None):
        """
        x : torch.LongTensor, shape = (batch_size, <= n_ctx)
            the text tokens
        xa : torch.Tensor, shape = (batch_size, n_audio_ctx, n_audio_state)
            the encoded audio features to be attended on
        """
        offset = next(iter(kv_cache.values())).shape[1] if kv_cache else 0
        x = (
            self.token_embedding(x)
            + self.positional_embedding[offset : offset + x.shape[-1]]
        )
        x = x.to(xa.dtype)

        for block in self.blocks:
            x = block(x, xa, mask=self.mask, kv_cache=kv_cache)

        x = self.ln(x)
        logits = (
            x @ torch.transpose(self.token_embedding.weight.to(x.dtype), 0, 1)
        ).float()

        return logits


class Whisper(nn.Module):
    def __init__(self, dims: ModelDimensions):
        super().__init__()
        self.dims = dims
        self.encoder = AudioEncoder(
            self.dims.n_mels,
            self.dims.n_audio_ctx,
            self.dims.n_audio_state,
            self.dims.n_audio_head,
            self.dims.n_audio_layer,
        )
        self.decoder = TextDecoder(
            self.dims.n_vocab,
            self.dims.n_text_ctx,
            self.dims.n_text_state,
            self.dims.n_text_head,
            self.dims.n_text_layer,
        )
        # use the last half among the decoder layers for time alignment by default;
        # to use a specific set of heads, see `set_alignment_heads()` below.
        all_heads = torch.zeros(
            self.dims.n_text_layer, self.dims.n_text_head, dtype=torch.bool
        )
        all_heads[self.dims.n_text_layer // 2 :] = True
        self.register_buffer("alignment_heads", all_heads.to_sparse(), persistent=False)

    def set_alignment_heads(self, dump: bytes):
        array = np.frombuffer(
            gzip.decompress(base64.b85decode(dump)), dtype=bool
        ).copy()
        mask = torch.from_numpy(array).reshape(
            self.dims.n_text_layer, self.dims.n_text_head
        )
        self.register_buffer("alignment_heads", mask.to_sparse(), persistent=False)

    def embed_audio(self, mel: torch.Tensor):
        return self.encoder(mel)

    def logits(self, tokens: torch.Tensor, audio_features: torch.Tensor):
        return self.decoder(tokens, audio_features)

    def forward(
        self, mel: torch.Tensor, tokens: torch.Tensor
    ) -> Dict[str, torch.Tensor]:
        return self.decoder(tokens, self.encoder(mel))

    @property
    def device(self):
        return next(self.parameters()).device

    @property
    def is_multilingual(self):
        return self.dims.n_vocab >= 51865

    @property
    def num_languages(self):
        return self.dims.n_vocab - 51765 - int(self.is_multilingual)

    def install_kv_cache_hooks(self, cache: Optional[dict] = None):
        """
        The `MultiHeadAttention` module optionally accepts `kv_cache` which stores the key and value
        tensors calculated for the previous positions. This method returns a dictionary that stores
        all caches, and the necessary hooks for the key and value projection modules that save the
        intermediate tensors to be reused during later calculations.

        Returns
        -------
        cache : Dict[nn.Module, torch.Tensor]
            A dictionary object mapping the key/value projection modules to its cache
        hooks : List[RemovableHandle]
            List of PyTorch RemovableHandle objects to stop the hooks to be called
        """
        cache = {**cache} if cache is not None else {}
        hooks = []

        def save_to_cache(module, _, output):
            if module not in cache or output.shape[1] > self.dims.n_text_ctx:
                # save as-is, for the first token or cross attention
                cache[module] = output
            else:
                cache[module] = torch.cat([cache[module], output], dim=1).detach()
            return cache[module]

        def install_hooks(layer: nn.Module):
            if isinstance(layer, MultiHeadAttention):
                hooks.append(layer.key.register_forward_hook(save_to_cache))
                hooks.append(layer.value.register_forward_hook(save_to_cache))

        self.decoder.apply(install_hooks)
        return cache, hooks

>>>> whisper/mlx_whisper/assets/download_alice.sh
#!/bin/bash

audio_file=$HOME/.cache/whisper/alice.mp3
echo $audio_file
zipf=alice_in_wonderland_librivox_64kb_mp3.zip
url=https://www.archive.org/download/alice_in_wonderland_librivox/
curl -LO $url/$zipf
unzip $zipf
mv wonderland_ch_02_64kb.mp3 $audio_file
rm wonderland_* $zipf

>>>> whisper/mlx_whisper/assets/multilingual.tiktoken
IQ== 0
Ig== 1
Iw== 2
JA== 3
JQ== 4
Jg== 5
Jw== 6
KA== 7
KQ== 8
Kg== 9
Kw== 10
LA== 11
LQ== 12
Lg== 13
Lw== 14
MA== 15
MQ== 16
Mg== 17
Mw== 18
NA== 19
NQ== 20
Ng== 21
Nw== 22
OA== 23
OQ== 24
Og== 25
Ow== 26
PA== 27
PQ== 28
Pg== 29
Pw== 30
QA== 31
QQ== 32
Qg== 33
Qw== 34
RA== 35
RQ== 36
Rg== 37
Rw== 38
SA== 39
SQ== 40
Sg== 41
Sw== 42
TA== 43
TQ== 44
Tg== 45
Tw== 46
UA== 47
UQ== 48
Ug== 49
Uw== 50
VA== 51
VQ== 52
Vg== 53
Vw== 54
WA== 55
WQ== 56
Wg== 57
Ww== 58
XA== 59
XQ== 60
Xg== 61
Xw== 62
YA== 63
YQ== 64
Yg== 65
Yw== 66
ZA== 67
ZQ== 68
Zg== 69
Zw== 70
aA== 71
aQ== 72
ag== 73
aw== 74
bA== 75
bQ== 76
bg== 77
bw== 78
cA== 79
cQ== 80
cg== 81
cw== 82
dA== 83
dQ== 84
dg== 85
dw== 86
eA== 87
eQ== 88
eg== 89
ew== 90
fA== 91
fQ== 92
fg== 93
oQ== 94
og== 95
ow== 96
pA== 97
pQ== 98
pg== 99
pw== 100
qA== 101
qQ== 102
qg== 103
qw== 104
rA== 105
rg== 106
rw== 107
sA== 108
sQ== 109
sg== 110
sw== 111
tA== 112
tQ== 113
tg== 114
tw== 115
uA== 116
uQ== 117
ug== 118
uw== 119
vA== 120
vQ== 121
vg== 122
vw== 123
wA== 124
wQ== 125
wg== 126
ww== 127
xA== 128
xQ== 129
xg== 130
xw== 131
yA== 132
yQ== 133
yg== 134
yw== 135
zA== 136
zQ== 137
zg== 138
zw== 139
0A== 140
0Q== 141
0g== 142
0w== 143
1A== 144
1Q== 145
1g== 146
1w== 147
2A== 148
2Q== 149
2g== 150
2w== 151
3A== 152
3Q== 153
3g== 154
3w== 155
4A== 156
4Q== 157
4g== 158
4w== 159
5A== 160
5Q== 161
5g== 162
5w== 163
6A== 164
6Q== 165
6g== 166
6w== 167
7A== 168
7Q== 169
7g== 170
7w== 171
8A== 172
8Q== 173
8g== 174
8w== 175
9A== 176
9Q== 177
9g== 178
9w== 179
+A== 180
+Q== 181
+g== 182
+w== 183
/A== 184
/Q== 185
/g== 186
/w== 187
AA== 188
AQ== 189
Ag== 190
Aw== 191
BA== 192
BQ== 193
Bg== 194
Bw== 195
CA== 196
CQ== 197
Cg== 198
Cw== 199
DA== 200
DQ== 201
Dg== 202
Dw== 203
EA== 204
EQ== 205
Eg== 206
Ew== 207
FA== 208
FQ== 209
Fg== 210
Fw== 211
GA== 212
GQ== 213
Gg== 214
Gw== 215
HA== 216
HQ== 217
Hg== 218
Hw== 219
IA== 220
fw== 221
gA== 222
gQ== 223
gg== 224
gw== 225
hA== 226
hQ== 227
hg== 228
hw== 229
iA== 230
iQ== 231
ig== 232
iw== 233
jA== 234
jQ== 235
jg== 236
jw== 237
kA== 238
kQ== 239
kg== 240
kw== 241
lA== 242
lQ== 243
lg== 244
lw== 245
mA== 246
mQ== 247
mg== 248
mw== 249
nA== 250
nQ== 251
ng== 252
nw== 253
oA== 254
rQ== 255
IHQ= 256
IGE= 257
IHRo 258
aW4= 259
ZXI= 260
IHc= 261
IHM= 262
b3U= 263
IHRoZQ== 264
cmU= 265
b24= 266
YXQ= 267
ZW4= 268
IGM= 269
aXQ= 270
aXM= 271
IGI= 272
bmQ= 273
IGQ= 274
IG0= 275
IGg= 276
IG8= 277
aW5n 278
ZXM= 279
IHA= 280
IHRv 281
YW4= 282
IGY= 283
b3I= 284
bGw= 285
IEk= 286
IGw= 287
IHk= 288
YXI= 289
IGc= 290
IHlvdQ== 291
ZWQ= 292
IGFuZA== 293
IGlu 294
IG9m 295
YXM= 296
IG4= 297
b20= 298
aWM= 299
IHRoYXQ= 300
dXM= 301
ZXQ= 302
dmU= 303
YWw= 304
b3c= 305
bGU= 306
IGlz 307
IGU= 308
IGl0 309
b3Q= 310
J3M= 311
IGJl 312
aW9u 313
IFQ= 314
IHdo 315
IEE= 316
ZW50 317
IFM= 318
IHJl 319
YXk= 320
IHdl 321
IG9u 322
ZXJl 323
IGhh 324
dXQ= 325
YWM= 326
aWQ= 327
aWc= 328
b3M= 329
a2U= 330
dmVy 331
aW0= 332
INA= 333
IFRo 334
YW0= 335
YWxs 336
IGZvcg== 337
ZWw= 338
Y2g= 339
cm8= 340
IHRoaXM= 341
IHN0 342
IFc= 343
IHU= 344
YWQ= 345
b3V0 346
aXI= 347
bGQ= 348
Y3Q= 349
IGs= 350
aWY= 351
IGdv 352
Li4= 353
0L4= 354
aXRo 355
bHk= 356
aHQ= 357
cXU= 358
IC0= 359
IGRv 360
IGo= 361
IGhhdmU= 362
IEI= 363
IGFu 364
IHdpdGg= 365
IGFyZQ== 366
IHI= 367
IGRl 368
IHNl 369
IHNv 370
IHY= 371
c3Q= 372
aWxs 373
dXI= 374
IGxp 375
IE0= 376
ZXN0 377
b2Q= 378
YWxseQ== 379
J3Q= 380
dXN0 381
IGFz 382
IEM= 383
Y2U= 384
IG1l 385
0LA= 386
0LU= 387
aWw= 388
IEg= 389
IHdhcw== 390
dGVy 391
dGg= 392
IGNhbg== 393
YW50 394
IGNvbQ== 395
b3Vy 396
aWdodA== 397
IFk= 398
YXRpb24= 399
IEFuZA== 400
b2w= 401
IHNo 402
0YI= 403
b3A= 404
c2U= 405
IG5vdA== 406
IFNv 407
IG5l 408
dW4= 409
IGFi 410
IGxpa2U= 411
IGF0 412
IEQ= 413
aWU= 414
IGhl 415
IGNvbg== 416
IGNo 417
b3Jl 418
IGFs 419
IG9y 420
IHF1 421
IE8= 422
b21l 423
cmE= 424
dWw= 425
IE4= 426
cHA= 427
IHlvdXI= 428
b3VsZA== 429
IFA= 430
IGZy 431
Z2U= 432
ZXJz 433
J3Jl 434
0Lg= 435
IHRoZXk= 436
IHdoYXQ= 437
dXNl 438
IGFsbA== 439
IFRoZQ== 440
IEw= 441
ZXNz 442
ZW0= 443
IGtu 444
IGp1c3Q= 445
YXJ0 446
IHBybw== 447
dmVyeQ== 448
dW0= 449
IGxv 450
IOw= 451
IG15 452
b2s= 453
IGV4 454
YWI= 455
IHRoZXJl 456
IGJ1dA== 457
IGtub3c= 458
IHN1 459
IEc= 460
0YE= 461
IEU= 462
IG1h 463
0L7Q 464
IGVu 465
IGFib3V0 466
IEl0 467
aXN0 468
IHdvcg== 469
cmk= 470
aW5k 471
IG9uZQ== 472
YXRl 473
YW5k 474
aW5r 475
IGxl 476
b3J0 477
J20= 478
IEY= 479
aWNo 480
0YA= 481
aWRl 482
IGdldA== 483
IG91dA== 484
Li4u 485
IHdpbGw= 486
44E= 487
aXZl 488
0L0= 489
IGZyb20= 490
YWlu 491
IFdl 492
IHVw 493
cGU= 494
cmVz 495
Y2E= 496
IFI= 497
IGlm 498
IHBs 499
IGRvbg== 500
YWNr 501
IDE= 502
ICI= 503
IHRy 504
IHVz 505
IFdo 506
aXR5 507
IEo= 508
IFlvdQ== 509
IGhlcmU= 510
aGVy 511
IHNvbWU= 512
b3Vn 513
YWs= 514
YXJk 515
IGdvaW5n 516
IHVu 517
bWVudA== 518
IHRoaW5r 519
IHBl 520
ZW5k 521
ICg= 522
Y2F1c2U= 523
IHRpbQ== 524
YXN0 525
w6k= 526
IG91cg== 527
IHdhbnQ= 528
YW1l 529
aWVz 530
IOs= 531
dWQ= 532
aW5l 533
IHJlYWxseQ== 534
IHRl 535
IHNlZQ== 536
Y2k= 537
IGJ5 538
c28= 539
dXJl 540
b3Nl 541
IFs= 542
YXJl 543
IG1vcmU= 544
YWg= 545
b25l 546
Y2s= 547
b3BsZQ== 548
0LDQ 549
IHRoZW4= 550
IHRoaW5n 551
IHRoZW0= 552
dmVu 553
b3VuZA== 554
b3N0 555
b25n 556
ZWN0 557
IHJpZ2h0 558
YWc= 559
IGludA== 560
IHBlb3BsZQ== 561
IHdoZW4= 562
b3Vz 563
cGw= 564
IHRpbWU= 565
IGlt 566
IHdobw== 567
IDI= 568
YXA= 569
IGJlY2F1c2U= 570
aGluZw== 571
IG5v 572
aWNl 573
IGxvb2s= 574
IGhhcw== 575
IHdvdWxk 576
IGhvdw== 577
YWN0 578
IGZl 579
bnQ= 580
b3VnaA== 581
IHBy 582
IEJ1dA== 583
IHNheQ== 584
0YM= 585
IG5vdw== 586
IG1hbg== 587
IHZlcnk= 588
IHdvcms= 589
aXo= 590
IEs= 591
aXY= 592
aXR0 593
IGFy 594
ZXA= 595
IGNs 596
IHdoaWNo 597
IGNv 598
YW5z 599
J3Zl 600
IHNh 601
ZmY= 602
J2xs 603
IGFueQ== 604
IGFjdA== 605
IHll 606
YmVy 607
YWNo 608
YWdl 609
cGVy 610
IGFsc28= 611
ZmVy 612
IHRoZXNl 613
IGFk 614
0LXQ 615
dGhlcg== 616
YWNl 617
aWNr 618
YWtl 619
cmVhdA== 620
aXJl 621
dWU= 622
IGFn 623
IFU= 624
dWNo 625
aW9ucw== 626
cnk= 627
MDA= 628
bmE= 629
IGRpZA== 630
IHF1ZQ== 631
IGhhZA== 632
IGV2ZXJ5 633
IEhl 634
IGxh 635
IHdheQ== 636
IHNw 637
Ymxl 638
IFRoaXM= 639
YXNz 640
IHRoZWly 641
aXRl 642
IG5lZWQ= 643
IHBhcnQ= 644
IHdlcmU= 645
IGJhY2s= 646
aXA= 647
b3du 648
b21ldA== 649
YmU= 650
YXNl 651
IG1ha2U= 652
aXJzdA== 653
aWE= 654
ZW5jZQ== 655
YW5n 656
YW5r 657
IGdvdA== 658
IHByZQ== 659
IGNvbnQ= 660
IG90aGVy 661
cHQ= 662
IFRoYXQ= 663
b2c= 664
IGdvb2Q= 665
IGludG8= 666
YWxr 667
IGJlZW4= 668
IGFt 669
IG92ZXI= 670
dWFsbHk= 671
IOI= 672
7J0= 673
IHVuZA== 674
aGU= 675
d2F5 676
IGdy 677
0Yw= 678
IGRpZg== 679
IHBlcg== 680
0Y8= 681
IElu 682
IHR3 683
b25k 684
YXJz 685
aW50 686
b3Jt 687
IGxvdA== 688
IHdoZXJl 689
IMM= 690
IFY= 691
IHNvbWV0 692
0Ls= 693
ZW5z 694
IGd1 695
IGFj 696
dWc= 697
0Ys= 698
xLE= 699
IGZpcnN0 700
cmVl 701
IGhpcw== 702
aXR0bGU= 703
IGltcA== 704
IG1v 705
YXY= 706
IGxpdHRsZQ== 707
IFdoYXQ= 708
IG11Y2g= 709
IHo= 710
IOo= 711
YWJsZQ== 712
INC/ 713
IHBv 714
IGNvbXA= 715
bmU= 716
IGRpcw== 717
IGxldA== 718
YW5jZQ== 719
IGhlcg== 720
IHRoaW5ncw== 721
IHN0YXJ0 722
dWx0 723
IGFwcA== 724
IHJlcw== 725
IGZv 726
IGNvdWxk 727
IGludGVy 728
IHRob3Nl 729
IGRlcw== 730
IHdlbGw= 731
IHR3bw== 732
IGtpbmQ= 733
eHQ= 734
cmVzcw== 735
ZWx5 736
w6Q= 737
IGJy 738
IHRocg== 739
INCy 740
IGk= 741
aXNo 742
IGRpZmZlcg== 743
IHJv 744
IFN0 745
IHNvbWV0aGluZw== 746
IHRha2U= 747
IGJv 748
eXM= 749
IHNoZQ== 750
IHRhbGs= 751
bG8= 752
0Yc= 753
IGV2ZW4= 754
0Lo= 755
44A= 756
INC9 757
IGJ1 758
IElm 759
IGRvd24= 760
IENo 761
YWRl 762
YXRpb25z 763
IHVzZQ== 764
b3Jk 765
IG9mZg== 766
IGFjdHVhbGx5 767
IHNwZQ== 768
ZHU= 769
YXRlZA== 770
YXRlcg== 771
b3Nz 772
bmluZw== 773
w7w= 774
IGRvZXM= 775
INGB 776
IG5ldw== 777
IGJldA== 778
dmVs 779
Y2Vzcw== 780
cGxl 781
IGhhcHA= 782
dGluZw== 783
b25uYQ== 784
IGVz 785
IGRheQ== 786
IG9ubHk= 787
aWdu 788
a2F5 789
c2Vs 790
ZW50cw== 791
b3VudA== 792
aWxk 793
aWxl 794
IHNj 795
IGhpbQ== 796
IGFnYWlu 797
dmluZw== 798
IGdvbm5h 799
IGNvbW0= 800
IGhlbA== 801
b3RoZXI= 802
IGtl 803
aWNhbA== 804
IDM= 805
IGVs 806
IHRocm91Z2g= 807
IGNvbWU= 808
YXJr 809
ZGF5 810
aWVy 811
w7M= 812
IHRoYW4= 813
IFRoZXk= 814
IG1heQ== 815
IHNlcg== 816
7ZU= 817
IGNhbGw= 818
IGRpZmZlcmVudA== 819
IHNob3VsZA== 820
IFRoZXJl 821
YXJ5 822
IE5vdw== 823
44I= 824
dGhpbmc= 825
d2U= 826
b3J5 827
ZnRlcg== 828
IHB1dA== 829
b3Jz 830
aWFs 831
64s= 832
IHVuZGVy 833
IGluYw== 834
IFll 835
dWI= 836
Zm9ybQ== 837
IHZpZGU= 838
4Lg= 839
dmVycw== 840
IGZlZWw= 841
w6E= 842
b2R5 843
ZnQ= 844
Zm9yZQ== 845
IGVt 846
Z2V0 847
IHNhaWQ= 848
aXRpb24= 849
IHJlYw== 850
aW91cw== 851
YXRjaA== 852
IHRyeQ== 853
IGhlbHA= 854
IHNob3c= 855
0LQ= 856
IGJpdA== 857
dWxs 858
0LI= 859
0YLQvg== 860
Z3I= 861
IHBsYXk= 862
aWZl 863
YWls 864
IFllYWg= 865
IHF1ZXN0 866
IG1hbnk= 867
IHBlcnM= 868
IGdyZWF0 869
w60= 870
IGVzdA== 871
bmc= 872
IOKZ 873
dHk= 874
bGE= 875
IE9o 876
INc= 877
4K4= 878
IEJl 879
YWR5 880
IG1vc3Q= 881
Y3Rpb24= 882
IE5v 883
IGRvaW5n 884
IGJlaW5n 885
IHRvbw== 886
Y2Vz 887
IGJs 888
LiI= 889
IHJlbQ== 890
aXNz 891
b25z 892
Pj4= 893
cnU= 894
d24= 895
b250 896
aWI= 897
ZWxs 898
IHNt 899
b3Ro 900
dWFs 901
ID4+ 902
IHBo 903
bGVz 904
b2M= 905
ZnVs 906
IHNlYw== 907
aXNl 908
IGFkZA== 909
aWdo 910
ZXJ0 911
IHNhbWU= 912
4oA= 913
IG1lYW4= 914
IGZpbmQ= 915
ZWs= 916
IGVuZA== 917
LS0= 918
0Lw= 919
IHN0aWxs 920
YXo= 921
ICc= 922
IG1pbg== 923
IHllYXJz 924
dXJu 925
IGFyb3VuZA== 926
c2VsZg== 927
IHdy 928
YnM= 929
b3VnaHQ= 930
IOKZqg== 931
IGZs 932
YW5nZQ== 933
IGFmdGVy 934
IHBvaW50 935
bWVy 936
dmVk 937
IGxvbmc= 938
b3k= 939
5Lg= 940
IGNy 941
d2F5cw== 942
IHN5 943
IHRyYQ== 944
IDIw 945
YXZl 946
IGNoZQ== 947
IGVudA== 948
IGJlZm9yZQ== 949
cGg= 950
IGF0dA== 951
aWFu 952
aWx5 953
IHBlcnNvbg== 954
IGJpZw== 955
IHNjaA== 956
IHJlYWw= 957
IG5leHQ= 958
IGxvdmU= 959
IHZpZGVv 960
IExldA== 961
IGZpbg== 962
IG1haw== 963
aWJsZQ== 964
IHRvZGF5 965
ZXJt 966
IEFs 967
b3dlcg== 968
YW5u 969
aXg= 970
IHBhcg== 971
IHN0dWQ= 972
w7Y= 973
IGltcG9ydA== 974
dGU= 975
IGdpdmU= 976
dmVz 977
IGRpZQ== 978
IGRlYw== 979
IHRlbGw= 980
INC6 981
0YHRgg== 982
IHdoeQ== 983
aWNhbGx5 984
aWN0 985
cmVk 986
IGJhcw== 987
IHN1cmU= 988
IGJlbA== 989
YXRpbmc= 990
IHRhaw== 991
IHNldA== 992
IGxpZmU= 993
IGRpZG4= 994
2Kc= 995
b2I= 996
dW5k 997
YXRo 998
IG9w 999
INC+ 1000
YWl0 1001
IHdvcmxk 1002
IHN1cHA= 1003
aW8= 1004
IGNvdXI= 1005
INC4 1006
d2FyZA== 1007
0LXQvQ== 1008
IGFsd2F5cw== 1009
dXA= 1010
IGhhbmQ= 1011
IEhvdw== 1012
Y2lhbA== 1013
IGNvbnM= 1014
INE= 1015
IGluZA== 1016
IDQ= 1017
IEFz 1018
IGZ1bg== 1019
amVjdA== 1020
IGltcG9ydGFudA== 1021
IHN1cg== 1022
ZXc= 1023
YXRlcw== 1024
IDU= 1025
IGRp 1026
IG1hZGU= 1027
IGlucw== 1028
IGFzaw== 1029
IGV0 1030
IG51bQ== 1031
IGNhcg== 1032
IE9rYXk= 1033
IHNpbQ== 1034
aWs= 1035
IGxhc3Q= 1036
IEdv 1037
IG11cw== 1038
IHJlbA== 1039
dWxhcg== 1040
tOw= 1041
IFdlbGw= 1042
cGVjdA== 1043
IFRoYW5r 1044
IHRocmVl 1045
w6M= 1046
44M= 1047
IGludg== 1048
IGdlbg== 1049
bGlj 1050
IGhhcHBlbg== 1051
64o= 1052
aWVu 1053
ZXZlcg== 1054
0L7Qsg== 1055
IHN0cg== 1056
IEFsbA== 1057
IGluc3Q= 1058
IOKA 1059
IGRlZg== 1060
IHNs 1061
IG1pZ2h0 1062
dW5n 1063
IHllYXI= 1064
IG93bg== 1065
IGtlZXA= 1066
Ym9keQ== 1067
ZGVy 1068
INGC 1069
INC0 1070
IGFub3RoZXI= 1071
IG1vZA== 1072
IGV2 1073
IGd1eXM= 1074
IGFibGU= 1075
w6Nv 1076
cXVl 1077
aWRlbnQ= 1078
IFllcw== 1079
IGl0cw== 1080
IHBsYWNl 1081
IHByb2R1 1082
YXJu 1083
INC8 1084
IHJlcA== 1085
IGV4cGVy 1086
IGZhbQ== 1087
aXRpZXM= 1088
aWZpYw== 1089
IGhpZ2g= 1090
aWVk 1091
b29s 1092
aWV3 1093
0LXRgg== 1094
cmVu 1095
IGRvbmU= 1096
IC4uLg== 1097
64qU 1098
c3RlbQ== 1099
IFNl 1100
IGJldHRlcg== 1101
Y29tZQ== 1102
IGRlbA== 1103
IHR5 1104
IHVt 1105
IGhv 1106
IEFu 1107
IG1vbg== 1108
aW5ncw== 1109
IHNr 1110
IG9i 1111
Y29t 1112
YmxlbQ== 1113
b3Bl 1114
c3RhbmQ= 1115
J2Q= 1116
bWVudHM= 1117
IGVsZQ== 1118
IElz 1119
IGRh 1120
IHJlZw== 1121
bGVhc2U= 1122
aWtl 1123
YWxz 1124
aXpl 1125
6rA= 1126
IGNhcmU= 1127
IG5ldmVy 1128
7J20 1129
ZXNl 1130
IG1ldA== 1131
b2xvZw== 1132
IFdoZW4= 1133
dWNr 1134
0LXRgA== 1135
IMOp 1136
IGRhdA== 1137
w6c= 1138
IGV4YW0= 1139
aWxpdHk= 1140
IGRldA== 1141
Y3Jp 1142
IHVzZWQ= 1143
IERv 1144
IHRyYW5z 1145
ZWc= 1146
dGVu 1147
0Y4= 1148
Y3Vz 1149
IHNlY29uZA== 1150
IGJlc3Q= 1151
IGhhcmQ= 1152
IGlkZQ== 1153
IHByb2JsZW0= 1154
6rM= 1155
IFVu 1156
0YU= 1157
IM4= 1158
IHdhdGNo 1159
IFNo 1160
YXR0ZXI= 1161
IHByZXQ= 1162
IGRlcg== 1163
IGNvdXJzZQ== 1164
xZ8= 1165
YXRpdmU= 1166
aWNz 1167
IHF1ZXN0aW9u 1168
dXRl 1169
7Jc= 1170
IEZvcg== 1171
YXRoZXI= 1172
IGNvbA== 1173
aWVuZA== 1174
IO0= 1175
IFo= 1176
IGRvZXNu 1177
YXJjaA== 1178
IGludGVyZXN0 1179
IHBvbA== 1180
IGNvcg== 1181
aWVuY2U= 1182
IHByZXM= 1183
IGVhY2g= 1184
IHN5c3RlbQ== 1185
IGZhY3Q= 1186
aWVs 1187
YWJseQ== 1188
IGVy 1189
IHJ1bg== 1190
IOyd 1191
IHRvcA== 1192
bmVy 1193
IHRob3VnaHQ= 1194
IGVhcw== 1195
aWVudA== 1196
IGNyZQ== 1197
0Yg= 1198
IGNvbW11bg== 1199
eWU= 1200
cmVhZHk= 1201
bGxvdw== 1202
IGV2ZXJ5dGhpbmc= 1203
b21t 1204
IG1lZA== 1205
mpQ= 1206
IGNvdW50 1207
aXRz 1208
IGNvbXBs 1209
aGlw 1210
2YQ= 1211
b29r 1212
IHRvZ2V0 1213
IHRvZ2V0aGVy 1214
YW1w 1215
IGdhbWU= 1216
IGFscmVhZHk= 1217
0LDQuw== 1218
IGNhbGxlZA== 1219
YWxl 1220
xYI= 1221
IE15 1222
IHVuZGVyc3RhbmQ= 1223
IGRy 1224
IG1vbQ== 1225
aXRlZA== 1226
0L7Quw== 1227
IHVzaW5n 1228
enk= 1229
IG51bWJlcg== 1230
44CB 1231
Y2Vk 1232
IGNsZQ== 1233
0L3Qvg== 1234
64uk 1235
aW5jZQ== 1236
IGxvb2tpbmc= 1237
IHByZXR0eQ== 1238
IHByb2I= 1239
IFNoZQ== 1240
IHZl 1241
IGdldHRpbmc= 1242
IHdlZWs= 1243
IGVmZg== 1244
dWZm 1245
YWly 1246
dWVz 1247
ZXJu 1248
IFE= 1249
b3Vw 1250
ZW50aW9u 1251
IHNpZGU= 1252
0L7QvA== 1253
IGZvcm0= 1254
IGJ1cw== 1255
IGFzcw== 1256
IGVk 1257
YXNvbg== 1258
d2Vlbg== 1259
4oCm 1260
IHR1cm4= 1261
IGN1cg== 1262
IGNvbGw= 1263
IGRpcmU= 1264
IEdvZA== 1265
IDEw 1266
IGVxdQ== 1267
INCx 1268
IG9wZW4= 1269
IHN1Y2g= 1270
aXJk 1271
0LDQug== 1272
IGVhcg== 1273
xJk= 1274
Z2Fu 1275
IHBhcnRpYw== 1276
IGZyaWVuZA== 1277
IGV4cA== 1278
IGV4dA== 1279
IGhvbWU= 1280
IHdhdGVy 1281
IE9u 1282
0YLRjA== 1283
b3Jr 1284
INC/0YA= 1285
IG1vdmU= 1286
bmVzcw== 1287
ZW5zZQ== 1288
aG8= 1289
IGNoYXI= 1290
Y28= 1291
aW5z 1292
IGJvdGg= 1293
IDE5 1294
IGdyYQ== 1295
IGJldHdlZW4= 1296
4bs= 1297
IOyV 1298
YXNo 1299
IFJl 1300
YWk= 1301
YWx0aA== 1302
dXJlcw== 1303
ZW1iZXI= 1304
IGF2 1305
IHZlcg== 1306
w6o= 1307
b25leQ== 1308
IHRoYW5r 1309
IG1heWJl 1310
dWM= 1311
aW1l 1312
6rOg 1313
IGF3YXk= 1314
IG5hbWU= 1315
b3VzZQ== 1316
IGFjYw== 1317
IG11c2lj 1318
IGNoYW5nZQ== 1319
IHBhc3M= 1320
Z2Vy 1321
IGJ1aWxk 1322
IHZhbA== 1323
aW5lc3M= 1324
YW55 1325
IGZldw== 1326
tOs= 1327
dGE= 1328
IGxpc3Q= 1329
w6U= 1330
IG9sZA== 1331
IOye 1332
IHNvcnQ= 1333
IG1lbQ== 1334
IGNh 1335
Y2VwdA== 1336
IGdlbmVy 1337
IHllYWg= 1338
IHdoaWxl 1339
IGFueXRoaW5n 1340
cmlj 1341
Z3JhbQ== 1342
IGVpbg== 1343
Y3k= 1344
dXJpbmc= 1345
IERl 1346
IHBvd2Vy 1347
IGNvbWluZw== 1348
IHdvcmQ= 1349
IC0t 1350
IGJlbGll 1351
IGZvdW5k 1352
dG8= 1353
0L8= 1354
IG1lYW5z 1355
IGluZm9ybQ== 1356
INg= 1357
INGH 1358
IHNtYWxs 1359
MDAw 1360
IGNhbWU= 1361
IO2V 1362
d2g= 1363
IHdvcmtpbmc= 1364
IGV4YW1wbGU= 1365
IHBvcw== 1366
IGRlcA== 1367
6rI= 1368
5Lo= 1369
b3Rl 1370
IGRlbQ== 1371
7Kc= 1372
dHM= 1373
IHZhcg== 1374
YXV0 1375
IHRyaQ== 1376
Y2hu 1377
IGhlYWQ= 1378
IHdob2xl 1379
15k= 1380
emU= 1381
IHRyeWluZw== 1382
IHRlbQ== 1383
IGNvdQ== 1384
ZXRz 1385
IDY= 1386
IGZpbA== 1387
dmVsb3A= 1388
IGNhc2U= 1389
4K8= 1390
IHByb2JhYmx5 1391
IG9rYXk= 1392
IHBsYW4= 1393
IHNpdA== 1394
IHNjaG9vbA== 1395
IFRoZW4= 1396
uOs= 1397
bWU= 1398
IHByb2Nlc3M= 1399
IGZhcg== 1400
IHJlYWQ= 1401
IHBvc3M= 1402
IGJyZQ== 1403
IHNvbA== 1404
aWNodA== 1405
IHN1cHBvcnQ= 1406
IFRv 1407
ZXJ0YWlu 1408
IHN0YXJ0ZWQ= 1409
IGNhcA== 1410
IGxlZnQ= 1411
IGRhdGE= 1412
IHRpbWVz 1413
0LXQuw== 1414
IHdhbnRlZA== 1415
0LDQvQ== 1416
IHRhbGtpbmc= 1417
IGlzdA== 1418
IGhhdmluZw== 1419
dW1w 1420
IGNvbnRpbg== 1421
IHN1Yg== 1422
INC3 1423
cHI= 1424
64uI 1425
aW5h 1426
xbw= 1427
IGNyZWF0 1428
b2Rl 1429
15U= 1430
5pg= 1431
ISE= 1432
IHRlcm0= 1433
aXNt 1434
0L7QtA== 1435
IEJlY2F1c2U= 1436
IHdlbnQ= 1437
aWRlcg== 1438
IHByb3Y= 1439
IGNoaWxk 1440
IGRlbg== 1441
IGxpZ2h0 1442
YnI= 1443
s9C+ 1444
b2g= 1445
IGJvb2s= 1446
INk= 1447
dXRpb24= 1448
IEp1c3Q= 1449
ZW5l 1450
IGZvdXI= 1451
IHZpcw== 1452
6rCA 1453
IGhvcGU= 1454
IG1ha2luZw== 1455
IExl 1456
7JU= 1457
IG9wcA== 1458
YXU= 1459
IG1vbmV5 1460
IHByb2dyYW0= 1461
w6g= 1462
IHN0YW5k 1463
SU4= 1464
IHNpZ24= 1465
IGxlYXJu 1466
w6A= 1467
IERvbg== 1468
IHRlYW0= 1469
INC90LA= 1470
bHVk 1471
IHJlc3Q= 1472
aWNlcw== 1473
5pw= 1474
INGA 1475
IGF1dA== 1476
IGxlYWQ= 1477
YXRpb25hbA== 1478
ZGU= 1479
Z3k= 1480
IG5pY2U= 1481
IGRhcw== 1482
IGRpc3Q= 1483
IGh1bQ== 1484
IE9uZQ== 1485
5og= 1486
IGNvbWVz 1487
IGpv 1488
IGNlbnQ= 1489
IGV4cGw= 1490
IG1hcms= 1491
cmVlbg== 1492
bGVk 1493
Z2lu 1494
7JqU 1495
IGxldmVs 1496
IGNvbmY= 1497
dXNo 1498
IGRldmVsb3A= 1499
IHRlc3Q= 1500
ZW5n 1501
dmlvdXM= 1502
YXR1cmU= 1503
0LXQvA== 1504
cmV0 1505
IGpl 1506
IHN0dWZm 1507
IGNsYXNz 1508
b3dz 1509
IOq3 1510
IHNp 1511
IGxlcw== 1512
cm9w 1513
55o= 1514
IHBvcg== 1515
IHdhcg== 1516
7JeQ 1517
IGV2ZXJ5b25l 1518
IGdl 1519
IGNoZWNr 1520
b3R0 1521
IHNpbmc= 1522
IGFydA== 1523
IGZvbGxvdw== 1524
IDIwMQ== 1525
IEZy 1526
YWlz 1527
7JY= 1528
zrE= 1529
5bA= 1530
IMOg 1531
aW1lcw== 1532
IHJldA== 1533
IGNoYW5n 1534
IHB1Yg== 1535
IGluZg== 1536
IHRlY2hu 1537
YWRh 1538
aXZlcw== 1539
IGJlaA== 1540
5piv 1541
IGxvb2tz 1542
44CC 1543
0Lc= 1544
IFdoeQ== 1545
55qE 1546
IGVub3VnaA== 1547
IGJyYQ== 1548
aXRjaA== 1549
5Ls= 1550
IGFkdg== 1551
0LE= 1552
IHdpdGhvdXQ= 1553
d2Vy 1554
bWVyaWM= 1555
ZGVu 1556
IGNvbXBsZXQ= 1557
IGlkZWE= 1558
dGVycw== 1559
b2Nr 1560
IGRlZmlu 1561
IGV2ZXI= 1562
IGds 1563
IG9uY2U= 1564
IGJyaW5n 1565
IHNheWluZw== 1566
IGFucw== 1567
IGhlYXI= 1568
bmVjdA== 1569
IGxlc3M= 1570
Z28= 1571
cmVhbQ== 1572
YWRv 1573
7J4= 1574
IG1pbmQ= 1575
ZW50ZQ== 1576
IGZ1bGw= 1577
IGJhZA== 1578
IHdvbQ== 1579
IHNvbWVvbmU= 1580
IGR1 1581
IHdvbg== 1582
IGNvbnRybw== 1583
b3J0dW4= 1584
IGhlYWx0aA== 1585
IGNobw== 1586
IEFy 1587
IGNvbmM= 1588
IGluZm9ybWF0aW9u 1589
IHN0b3A= 1590
YXR0 1591
YXRlbHk= 1592
5L0= 1593
IGdyb3Vw 1594
INGD 1595
IHF1aXRl 1596
IHJlc3A= 1597
RVI= 1598
dWdodA== 1599
6rg= 1600
bWFu 1601
aXplZA== 1602
IEJy 1603
IHJlbWVtYmVy 1604
IGZhbWlseQ== 1605
IGJ1c2luZXNz 1606
YXc= 1607
IHNwZWM= 1608
IGF1 1609
IE9y 1610
xIU= 1611
IHNlZW4= 1612
IGxhcg== 1613
IDc= 1614
Z2c= 1615
YmVycw== 1616
IGRyYQ== 1617
IG1vbnRo 1618
IHNheXM= 1619
IGlzcw== 1620
IGxpdmU= 1621
IGxpbmU= 1622
IG1vbWVudA== 1623
IGV4Yw== 1624
ZWxz 1625
IHNvdW5k 1626
IGNvb2w= 1627
IGxvYw== 1628
IGNlcnRhaW4= 1629
IGRyaQ== 1630
0L7Rgg== 1631
YW1lcw== 1632
IG11c3Q= 1633
bnk= 1634
0LjRgg== 1635
IGtpZA== 1636
IGluY2x1ZA== 1637
7J2E 1638
YXRvcg== 1639
xJ8= 1640
aGE= 1641
YXJlZA== 1642
IHNlZW0= 1643
0Lk= 1644
7IQ= 1645
IGVsc2U= 1646
IOyg 1647
aXJs 1648
IDg= 1649
IHZv 1650
IHF1ZXN0aW9ucw== 1651
aW5lcw== 1652
ZWU= 1653
5oiR 1654
w7xy 1655
IEFtZXJpYw== 1656
IHN0b3J5 1657
IHNlcnY= 1658
dmVybg== 1659
YWdlcw== 1660
bGFuZA== 1661
IOKAkw== 1662
ZXJh 1663
IENhbg== 1664
IHBvcA== 1665
ZXRoZXI= 1666
IG5h 1667
IG9yZGVy 1668
IG1ha2Vz 1669
IHNpbmNl 1670
Y29u 1671
Y3Rvcg== 1672
IHRob3VnaA== 1673
IHByb2R1Y3Q= 1674
0LvQuA== 1675
IGxlZw== 1676
IG1lZXQ= 1677
YWxm 1678
0YHRjw== 1679
dW5jaA== 1680
aXRlcg== 1681
b3Zl 1682
15XX 1683
aWV0 1684
0LDQvA== 1685
aXRhbA== 1686
IHN1cGVy 1687
bGluZw== 1688
IHBheQ== 1689
IHBhcmE= 1690
IGpvYg== 1691
IEhlcmU= 1692
IHN3 1693
a3M= 1694
cHRpb24= 1695
bWE= 1696
IGJlbGlldmU= 1697
rOs= 1698
IHdhaXQ= 1699
0L7QuQ== 1700
IHVudA== 1701
IHF1aWNr 1702
aHI= 1703
INGN 1704
IFBybw== 1705
IG1lbg== 1706
4Lk= 1707
IGRheXM= 1708
IGdvZXM= 1709
IHNwZWFr 1710
IEF0 1711
ZW1lbnQ= 1712
IG1pc3M= 1713
IGF3 1714
IGRlc2lnbg== 1715
IHByb2plY3Q= 1716
0L7RgA== 1717
aWo= 1718
YW50cw== 1719
YXRz 1720
IENocg== 1721
IDk= 1722
IGN1dA== 1723
IHJlcXU= 1724
INC90LU= 1725
IE5vdA== 1726
YXN0ZXI= 1727
IG1pbGw= 1728
IHBhcnRpY3VsYXI= 1729
IHBpZQ== 1730
IHN0dWRlbnRz 1731
IGZpdmU= 1732
b3Vu 1733
IE5l 1734
IGdp 1735
IHBhcw== 1736
IGZyZWU= 1737
IFNw 1738
bGljaA== 1739
IHByb2Y= 1740
IGVuZw== 1741
IHByb3Q= 1742
IExpa2U= 1743
b3NlZA== 1744
IGNvbm5lY3Q= 1745
YXBw 1746
IOun 1747
aXRpbmc= 1748
IGJsbw== 1749
IGxvcw== 1750
aXN0cw== 1751
IGV4cGVyaWVuY2U= 1752
cmVudA== 1753
IHN0YXk= 1754
IGZvb2Q= 1755
dG9u 1756
cnVjdA== 1757
IGhpc3Q= 1758
dmlldw== 1759
aW5pbmc= 1760
bW9zdA== 1761
aXZlcnM= 1762
Ym8= 1763
44GE 1764
IFRy 1765
Z2Vu 1766
IHBsZWFzZQ== 1767
IGNvbW11bml0eQ== 1768
IGNl 1769
QU4= 1770
bm8= 1771
IGJvZHk= 1772
IGhvdXI= 1773
IHZlcnM= 1774
4bo= 1775
Y2Vy 1776
IOqw 1777
IHJlYXNvbg== 1778
IFJpZ2h0 1779
IGxhdGVy 1780
z4Q= 1781
IGhvdXNl 1782
IFg= 1783
0L7QvQ== 1784
IHN0YXRl 1785
Zmlj 1786
5aQ= 1787
xZs= 1788
aWVsZA== 1789
IHByaQ== 1790
IHBhc3Q= 1791
IHdhbGs= 1792
b2xvZ3k= 1793
ZXJpbmc= 1794
YW5uYQ== 1795
IHRlcg== 1796
IGhvbGQ= 1797
IG9yZ2Fu 1798
YmVu 1799
zr8= 1800
w7Nu 1801
IGVmZmVjdA== 1802
IHlvdXJzZWxm 1803
IHBsdXM= 1804
YWo= 1805
YW5kbw== 1806
dXJhbA== 1807
IHJvb20= 1808
bGVjdA== 1809
6rKM 1810
PyI= 1811
c2lkZQ== 1812
IGJlY29tZQ== 1813
0YY= 1814
IMI= 1815
b29k 1816
IGNvbnN0 1817
IG5pZ2h0 1818
dXRlcw== 1819
0LY= 1820
IGJyZWFr 1821
IHBhaW4= 1822
IHN0ZXA= 1823
aXJlZA== 1824
IG5vdGhpbmc= 1825
IHVudGls 1826
0ZY= 1827
0LDQsg== 1828
2Yo= 1829
IGR1cmluZw== 1830
7KeA 1831
bGVzcw== 1832
b2xs 1833
0L3Riw== 1834
zrk= 1835
ZmVjdA== 1836
aXZlcg== 1837
j4Q= 1838
aXRoZXI= 1839
eWluZw== 1840
IGJlZ2lu 1841
15nX 1842
aXZpZA== 1843
IMOn 1844
IHNhbA== 1845
IHRh 1846
IHBvdA== 1847
ICQ= 1848
IG1hcg== 1849
IGNsZWFy 1850
IGZhY2U= 1851
IGdyb3c= 1852
ICo= 1853
IGluc2lkZQ== 1854
IGZyaWVuZHM= 1855
IGxlYXZl 1856
ZW5u 1857
IGVhc3k= 1858
IGFyZWE= 1859
YWxpdHk= 1860
b3Vk 1861
IGVhdA== 1862
2YY= 1863
IHB1cg== 1864
b3Ju 1865
IHNhdw== 1866
IGFuc3dlcg== 1867
IGZyb250 1868
IGJlYXV0 1869
vOs= 1870
IG1hdHRlcg== 1871
IHNvbg== 1872
IE5ldw== 1873
IHJlc3VsdA== 1874
aWRlcw== 1875
Y2hl 1876
IGZ1dA== 1877
cHM= 1878
IGZvY3Vz 1879
IGludGVyZXN0aW5n 1880
5aU= 1881
IGFw 1882
Ii4= 1883
IGNyZWF0ZQ== 1884
0L7RgQ== 1885
IHByZXNz 1886
cm9zcw== 1887
IHBpY2s= 1888
bGluZQ== 1889
IHRvb2s= 1890
IE1heQ== 1891
cm93 1892
IGljaA== 1893
mOs= 1894
IHJlZg== 1895
IG1vcg== 1896
cmFjdA== 1897
YXJlbnQ= 1898
QVI= 1899
IGV4YWN0 1900
IHNwYWNl 1901
d29yaw== 1902
0L3QuA== 1903
IGJpcg== 1904
IGRldg== 1905
0LM= 1906
IHRvbGQ= 1907
IHB1YmxpYw== 1908
Y2lhbGx5 1909
IHZpZXc= 1910
IEhleQ== 1911
bWVk 1912
bGxv 1913
Y2M= 1914
IGZhYw== 1915
IGNvdXBsZQ== 1916
IGhlYXJ0 1917
bGVy 1918
IHJlYWR5 1919
IGFsbW9zdA== 1920
YXJpbmc= 1921
IGhhbGY= 1922
IE1l 1923
YXZvcg== 1924
aXF1ZQ== 1925
IGNoYXJhYw== 1926
IHByYWN0 1927
T04= 1928
YW5l 1929
IGls 1930
0L3QsA== 1931
IHZp 1932
bGlzaA== 1933
aGVhZA== 1934
IGxlYXN0 1935
IGJhc2ljYWxseQ== 1936
YXNlZA== 1937
cmlnaHQ= 1938
IHlldA== 1939
IHRha2luZw== 1940
IGNvdW50cnk= 1941
IHdpbg== 1942
IGlzbg== 1943
IHBvc3NpYmxl 1944
IGNhbQ== 1945
IGluY3Jl 1946
IHBhdA== 1947
IHdhbm5h 1948
IGNvbnNpZGVy 1949
IGFicw== 1950
IHdpdGhpbg== 1951
IGh1bWFu 1952
IHRoaW5raW5n 1953
IG9o 1954
oZw= 1955
IHF1aQ== 1956
YXNlcw== 1957
IDA= 1958
aXRlbHk= 1959
5LiN 1960
IGtpbGw= 1961
IG1pbA== 1962
IGludmVzdA== 1963
aXN0ZXI= 1964
IHN1Yw== 1965
aW9uYWw= 1966
ZWxm 1967
IHdoZXRoZXI= 1968
IGNvbnRyb2w= 1969
IGFnYWluc3Q= 1970
b3Rz 1971
64uI64uk 1972
aW9y 1973
IHByZXNlbnQ= 1974
INin 1975
IHdhdGNoaW5n 1976
dWJl 1977
ZXJ2 1978
IG5pY2h0 1979
IGdvdmVybg== 1980
IFRoZXNl 1981
IDo= 1982
dWl0 1983
dWdo 1984
IHdvcmtz 1985
b28= 1986
IHdpcg== 1987
IGFpcg== 1988
IFRl 1989
0LDQtw== 1990
aXNpb24= 1991
d2hlcmU= 1992
IHRvdA== 1993
am95 1994
7Is= 1995
IHZvbA== 1996
INC1 1997
IGNsb3Nl 1998
IEFk 1999
0Yk= 2000
aW5lZA== 2001
IHVuYQ== 2002
IOq3uOs= 2003
sOs= 2004
b3JyeQ== 2005
IGJybw== 2006
IGZpbG0= 2007
aWZ0 2008
MjA= 2009
IHR5cGU= 2010
IGhhcHBlbmVk 2011
IEFt 2012
IGdpcmw= 2013
IEFyZQ== 2014
d2FyZHM= 2015
IHBvdXI= 2016
IGNvbG9y 2017
ZWx0 2018
0LDRgQ== 2019
IHNlbnNl 2020
bGV4 2021
IFdpdGg= 2022
dXNz 2023
cmli 2024
IHJlc2U= 2025
IG5vcm0= 2026
IGZ1dHVyZQ== 2027
IGRlYWw= 2028
ZW5kaW5n 2029
ZXk= 2030
IHg= 2031
ZXJv 2032
IENs 2033
dWs= 2034
IHdoYXRldmVy 2035
c2VsdmVz 2036
IHlvdW5n 2037
7Io= 2038
IE1hcg== 2039
IENocmlzdA== 2040
IGd1ZXNz 2041
IHBlcmZvcm0= 2042
IGVuZXI= 2043
cm9u 2044
IGhpdA== 2045
IHdvbmQ= 2046
IGRpcmVjdA== 2047
IEV2ZXJ5 2048
IG9mdGVu 2049
IGZh 2050
IGFsb25n 2051
IGNsaWNr 2052
IExvb2s= 2053
IHNpdHU= 2054
IGhhcHB5 2055
ZWFk 2056
IGFnbw== 2057
IGVuYw== 2058
IG15c2VsZg== 2059
IGNvdmVy 2060
0L7QsQ== 2061
IG1pZA== 2062
IGNvc3Q= 2063
IHRlbg== 2064
IFNjaA== 2065
IGV4cGVjdA== 2066
IHdhc24= 2067
IHN0cm9uZw== 2068
aWZ1bA== 2069
IG9wcG9ydHVu 2070
aW5hbA== 2071
eWxl 2072
IHNoYXJl 2073
IHRydWU= 2074
IGFwcHJv 2075
IGNoYWxs 2076
IG1pbnV0ZXM= 2077
IGNoYW5u 2078
IOuC 2079
zrU= 2080
bGk= 2081
IG1lc3M= 2082
b3JpZXM= 2083
cGVjaWFsbHk= 2084
IHdyb25n 2085
IHllcw== 2086
IOyX 2087
aXJvbg== 2088
IGFsbG93 2089
IHN1YnM= 2090
IGZvcmU= 2091
IGZpZ2h0 2092
IHNvY2lhbA== 2093
IGNyYQ== 2094
YW5h 2095
IGFmZg== 2096
IGVzcw== 2097
IHdheXM= 2098
IHNob3J0 2099
IGZhbGw= 2100
IGxhdw== 2101
IFdobw== 2102
IGVuam95 2103
IGNhbA== 2104
IGFjY2Vzcw== 2105
ZmU= 2106
IG5vbg== 2107
IGFjcm9zcw== 2108
ZXJ5 2109
dmlvdXNseQ== 2110
IEV4 2111
aWRlZA== 2112
IGxpbms= 2113
IFBy 2114
IHRlcm1z 2115
YWNlcw== 2116
IGxhbmQ= 2117
YXppbmc= 2118
IDE1 2119
IG11bHQ= 2120
IHNwZWNpYWw= 2121
5YA= 2122
aXZpbmc= 2123
7J2A 2124
IHR5cA== 2125
IHN0ZQ== 2126
IMQ= 2127
IGZvcndhcmQ= 2128
5Y8= 2129
IGZyZQ== 2130
5aW9 2131
IHJlc2VhcmNo 2132
4K+N 2133
0LDRgg== 2134
IG1haW4= 2135
IHJlY29yZA== 2136
IGh1 2137
IGRlZmluaXRlbHk= 2138
IGVpdGhlcg== 2139
IGxpc3Rlbg== 2140
IGtleQ== 2141
IG1hcmtldA== 2142
INGH0YLQvg== 2143
aXphdGlvbg== 2144
IHZpZGVvcw== 2145
IGd1eQ== 2146
IGZpZw== 2147
IHN0cmE= 2148
IFBs 2149
dWxseQ== 2150
YW1vcw== 2151
IG1lbnRpb24= 2152
IHNvbmc= 2153
IGludGVybg== 2154
cmFs 2155
dXJz 2156
IGhvbg== 2157
IHZhbHVl 2158
IGJhcg== 2159
Y2xl 2160
0L7Qtg== 2161
xIc= 2162
nOs= 2163
IHp1 2164
0LjQvA== 2165
5L2g 2166
IHNpbmdsZQ== 2167
IGF1Y2g= 2168
Y3Vzcw== 2169
IGdldHM= 2170
IHNvbWV0aW1lcw== 2171
5b4= 2172
YW1i 2173
bW0= 2174
Y2luZw== 2175
IHBlcmZlY3Q= 2176
IEJs 2177
b3V0aA== 2178
7KA= 2179
IHNjaQ== 2180
cGFy 2181
IHJlZA== 2182
IHBvc3Q= 2183
IG1vdA== 2184
IGVsZWN0 2185
IEV1 2186
aXRpdmU= 2187
IFNvbWU= 2188
IGRlc2NyaQ== 2189
IGN1cnJlbnQ= 2190
w6lz 2191
IHRyZQ== 2192
IEVu 2193
IG1pdA== 2194
RU4= 2195
iOs= 2196
aXVt 2197
IGhlYXJk 2198
IHNpbXBsZQ== 2199
bGFy 2200
IGV2ZXJ5Ym9keQ== 2201
aWxhcg== 2202
IG5lZWRz 2203
IGRpZmZpYw== 2204
IEdvb2Q= 2205
dW1lbnQ= 2206
Y2VudA== 2207
IG9wZXI= 2208
0LDRgtGM 2209
ZXR5 2210
IGJsYWNr 2211
IGdpdmVu 2212
b25lcw== 2213
IHdlbA== 2214
6YA= 2215
IOyVhA== 2216
IDMw 2217
QVQ= 2218
IHN0YXQ= 2219
b3VjaA== 2220
IE1y 2221
0LDRgA== 2222
IHNobw== 2223
IGNvbmQ= 2224
15Q= 2225
bXk= 2226
IGNoaWxkcmVu 2227
IGV1 2228
0LXQtA== 2229
7JWE 2230
dGVybg== 2231
IHVo 2232
IGhhcg== 2233
IHByb20= 2234
IHB1bGw= 2235
cmV3 2236
IGNvbXBhbnk= 2237
IGJlYXV0aWZ1bA== 2238
dXN0b20= 2239
7ZWY 2240
0LrQuA== 2241
IHN0cmU= 2242
IGFtYXppbmc= 2243
cmllcw== 2244
IHN1Y2Nlc3M= 2245
IG1hY2g= 2246
bm90 2247
IGRpc2N1c3M= 2248
IG5hdA== 2249
pqw= 2250
IHVuZQ== 2251
IGRpZmZpY3VsdA== 2252
IHJpcw== 2253
zr0= 2254
IGNhbXA= 2255
IGJ1eQ== 2256
5LiA 2257
IG1hZw== 2258
cG8= 2259
IFlvdXI= 2260
IGJlaGluZA== 2261
aWNh 2262
xLFu 2263
IE9L 2264
IGxhbmc= 2265
IHdvbWVu 2266
IGVudg== 2267
IHJlY2U= 2268
IGNoYW5uZWw= 2269
aWFsbHk= 2270
dWxl 2271
IDEy 2272
dGhlcnM= 2273
IGJvdHQ= 2274
IHJlcG9ydA== 2275
ZW50bHk= 2276
ZnVsbHk= 2277
VGhl 2278
IHNlbnQ= 2279
IGV2ZW50 2280
IGVuZXJneQ== 2281
bHQ= 2282
IHdvcmRz 2283
YXJy 2284
ZGxl 2285
IGFoZWFk 2286
YXJkcw== 2287
2LE= 2288
5LqG 2289
IHRvb2w= 2290
Y29ub20= 2291
0LXRgQ== 2292
IGV4YWN0bHk= 2293
IGZhdm9y 2294
IGxvdw== 2295
IHByb3Blcg== 2296
IOyeiA== 2297
ICE= 2298
IHJlbGF0aW9ucw== 2299
IG1hcw== 2300
IGtpZHM= 2301
IGVudGlyZQ== 2302
dWRl 2303
2YU= 2304
IFdoZXJl 2305
IG9uZXM= 2306
IGNpdHk= 2307
b2x1dA== 2308
IHNpeA== 2309
YWJpbGl0eQ== 2310
w7Zy 2311
aWxp 2312
IEVz 2313
IGhhcHBlbnM= 2314
YWlucw== 2315
IG1vZGVs 2316
IHBpY3Q= 2317
IGVzcGVjaWFsbHk= 2318
IDEwMA== 2319
a3Q= 2320
IHNvb24= 2321
Ynk= 2322
cm9kdQ== 2323
IGFubg== 2324
IHN1YnNjcmk= 2325
IFF1 2326
IGF2YWls 2327
aW1lbnQ= 2328
IHZvYw== 2329
a2E= 2330
IDIwMA== 2331
YXBlcg== 2332
IEluZA== 2333
IOyn 2334
aG9y 2335
jbA= 2336
am9y 2337
0LjQuw== 2338
IHNxdQ== 2339
QVU= 2340
YXJuaW5n 2341
INCz 2342
SVM= 2343
INC7 2344
0LXQuQ== 2345
eWVz 2346
5YU= 2347
INCS 2348
IG9yaWc= 2349
0L7Qs9C+ 2350
IGFza2Vk 2351
aWx0 2352
0L7Qsw== 2353
IGNvbnRpbnVl 2354
IOyY 2355
cmFt 2356
IG90aGVycw== 2357
RVM= 2358
b2hu 2359
IGxheQ== 2360
IGJhc2Vk 2361
IHB1 2362
IGFwcGU= 2363
IGxpbQ== 2364
IHByb3A= 2365
gOs= 2366
bWlu 2367
IGhvdA== 2368
IExh 2369
IGZhc3Q= 2370
IHByb3RlY3Q= 2371
IGFtb3VudA== 2372
IGFxdQ== 2373
IGZ1bmQ= 2374
IGN1c3RvbQ== 2375
IGN1bHQ= 2376
IGhhbmRz 2377
IGhhdmVu 2378
IGF1ZA== 2379
IG91dHNpZGU= 2380
IEFmdGVy 2381
YXBz 2382
IGFuaW0= 2383
cGxveQ== 2384
IGhhdA== 2385
IEZpcnN0 2386
IHRyZWF0 2387
IGVw 2388
IG1hdGVy 2389
IGJ1aWxkaW5n 2390
IOuw 2391
5ZA= 2392
7ISc 2393
emE= 2394
dWdodGVy 2395
IFBl 2396
bmV5 2397
ZXRlcg== 2398
YXRpYw== 2399
IGVkdWM= 2400
6riw 2401
IG1vdg== 2402
k6Q= 2403
YW1h 2404
cmF0aW9u 2405
IHNu 2406
2Yg= 2407
IHN1bQ== 2408
IHBob3Q= 2409
INCd 2410
IC4= 2411
5pyJ 2412
IGZpbmlzaA== 2413
aXR0aW5n 2414
5a4= 2415
IGxhcmdl 2416
IOyW 2417
IHdoaXRl 2418
YXJh 2419
IG1haXM= 2420
IEhp 2421
IGRhbQ== 2422
INin2YQ= 2423
IGJveA== 2424
IEhlbGxv 2425
IHNsZQ== 2426
IG9wdA== 2427
cmllZA== 2428
pbw= 2429
IGFjdGl2 2430
IG7Do28= 2431
IENvbQ== 2432
IHBsYXlpbmc= 2433
VGg= 2434
IGF2YWlsYWJsZQ== 2435
IHBvcnQ= 2436
5Yg= 2437
IEFo 2438
IGxhcw== 2439
IGVhcmx5 2440
IHdvbmRlcg== 2441
sbA= 2442
IDE4 2443
Y3Vs 2444
IGZ1bmN0aW9u 2445
IG1vcm5pbmc= 2446
bGxl 2447
aWVudHM= 2448
dXg= 2449
IGNpcg== 2450
aXRpb25z 2451
IGRlZXA= 2452
IHBvbGl0 2453
eW9y 2454
bXA= 2455
YWtpbmc= 2456
jOs= 2457
IE1hbg== 2458
IG1pbGxpb24= 2459
IC8= 2460
IGluZGl2aWQ= 2461
IHBhbg== 2462
IGdvdmVybm1lbnQ= 2463
IHdyaXRl 2464
IFRvZA== 2465
YW1lbnQ= 2466
IM8= 2467
IHdpbmQ= 2468
IEVuZw== 2469
Y2hlbg== 2470
V2g= 2471
7Jw= 2472
IGlkZW50 2473
44Gn 2474
dmVudA== 2475
dXJjaA== 2476
IGh5 2477
IHlh 2478
IHRyYWQ= 2479
IHJlbGF0aW9uc2hpcA== 2480
w7o= 2481
IGRvdQ== 2482
T1I= 2483
IHN3ZQ== 2484
IG5lZw== 2485
aW5hdGlvbg== 2486
IHRleHQ= 2487
aXBw 2488
IGZpbmU= 2489
w6Fz 2490
IERy 2491
IENvbWU= 2492
IG1vbnRocw== 2493
LCI= 2494
0LXQvdC4 2495
IGhvdXJz 2496
IHBvZA== 2497
aXJ0 2498
IGludm9s 2499
IGNvbGxlY3Q= 2500
IGF1Zg== 2501
IHBh 2502
IGhpc3Rvcnk= 2503
bWI= 2504
aWZ5 2505
ID8= 2506
IGJlbG93 2507
YXN1cmU= 2508
YWJ5 2509
IGxhbmd1 2510
IGFudA== 2511
IGNvbWI= 2512
YXRv 2513
IGV4aXN0 2514
IOuL 2515
IHRha2Vz 2516
IGNoYXJhY3Rlcg== 2517
YWZm 2518
IGZpZWxk 2519
IGVjb25vbQ== 2520
aWVm 2521
IHBpZWNl 2522
5Zw= 2523
IHJlYWNo 2524
IOqy 2525
b255 2526
IG1hdGVyaWFs 2527
IGRpZw== 2528
IHBoeXM= 2529
IGltcHJv 2530
IHNpbWlsYXI= 2531
SUM= 2532
IG5ldA== 2533
eW4= 2534
IHBvc2l0aW9u 2535
w58= 2536
IGJlbmU= 2537
cmVhZA== 2538
IGxlYXJuaW5n 2539
dW1l 2540
IGNsZWFu 2541
0YLQvtGA 2542
IGNvb2s= 2543
IHNlZW1z 2544
IG9s 2545
IFVT 2546
IEplcw== 2547
IOCu 2548
ZW50aWFs 2549
aXZlcnNpdHk= 2550
YWN5 2551
INGP 2552
b2x1dGVseQ== 2553
cmVjdA== 2554
IFBsZWFzZQ== 2555
IHJlcHJlcw== 2556
IHRvdWNo 2557
bWVu 2558
INCw 2559
acOzbg== 2560
IFRoYW5rcw== 2561
IGFuZw== 2562
IG1ham9y 2563
IGl0c2VsZg== 2564
aWxscw== 2565
Iiw= 2566
aWFucw== 2567
IHNjcmVlbg== 2568
IGhvcg== 2569
IGtub3du 2570
IGVudmlyb24= 2571
IGZpbmFs 2572
IGZpZ3VyZQ== 2573
IFR3 2574
IGV5ZXM= 2575
IGltYWc= 2576
IHNlZWluZw== 2577
IGhhaXI= 2578
cmVt 2579
IGFwcGxpYw== 2580
ZW5kcw== 2581
cHV0 2582
IG5ld3M= 2583
IGNvbXBsZXRlbHk= 2584
dWdocw== 2585
IGtuZXc= 2586
aWZpZWQ= 2587
IEpl 2588
IERpZA== 2589
IHNpdHVhdGlvbg== 2590
IGZsbw== 2591
bXM= 2592
IHBob25l 2593
IGJhbGw= 2594
ZG8= 2595
IHBhcmVudA== 2596
IHNvcnJ5 2597
dXJ5 2598
0LjQvQ== 2599
aXBz 2600
0LDQtA== 2601
IGluc3RlYWQ= 2602
IGh1Z2U= 2603
IHR1 2604
IOOB 2605
IEdy 2606
IGRldGFpbA== 2607
INCf 2608
IGluZGl2aWR1YWw= 2609
IGZpcmU= 2610
IGNsb3M= 2611
IHdlcg== 2612
dW5l 2613
IHJ1bm5pbmc= 2614
IGNvbnZlcnM= 2615
IHJlY29tbQ== 2616
IGNvbW8= 2617
IHNvbWVib2R5 2618
IEpvaG4= 2619
IOydtA== 2620
IE91cg== 2621
cGxlcw== 2622
IFBo 2623
IGFuYWw= 2624
IDUw 2625
IG9mZmVy 2626
IDw= 2627
aXRpb25hbA== 2628
Z2VzdA== 2629
IHZvdXM= 2630
bGV0 2631
aWN5 2632
IGZlZWxpbmc= 2633
TEU= 2634
cm9z 2635
IHRoaXJk 2636
0L7Qug== 2637
IHNlcmllcw== 2638
IEFueQ== 2639
aXNlZA== 2640
b2xk 2641
IGRyYXc= 2642
IHNlcnZpY2U= 2643
IGNhbm5vdA== 2644
YmFs 2645
44GG 2646
IGxpdmluZw== 2647
xLFt 2648
IGRpZmZlcmVuY2U= 2649
IG9wcG9ydHVuaXR5 2650
IG5lYXI= 2651
b3J0aA== 2652
a2Vu 2653
IGxvY2Fs 2654
2Ko= 2655
IENvbg== 2656
IG9iamVjdA== 2657
IGRhc3M= 2658
44GZ 2659
kNc= 2660
IHF1aWNrbHk= 2661
cmFwaA== 2662
IGlzc3Vlcw== 2663
6YCZ 2664
IEFtZXJpY2Fu 2665
IHByZXA= 2666
ZW5jZXM= 2667
IHByb2Zlc3M= 2668
bGxpbmc= 2669
b2Y= 2670
IGZvb3Q= 2671
YnJl 2672
IHVzdWFsbHk= 2673
IGdlbmVyYWw= 2674
ZGE= 2675
YW5jZXM= 2676
IGRlc3Q= 2677
IG9jYw== 2678
IG1lbWJlcnM= 2679
IGRhbnM= 2680
IGVxdWFs 2681
enQ= 2682
IGJlY29t 2683
IG1vdmluZw== 2684
IHNwZWNpZmlj 2685
w61h 2686
IGZ1cg== 2687
IG5lY2Vzcw== 2688
IGNvbW1vbg== 2689
IGF0dGFjaw== 2690
INGN0YLQvg== 2691
IFRvZGF5 2692
IHVucw== 2693
IEd1 2694
aW9k 2695
IGFjY291bnQ= 2696
IGdyYW5k 2697
IHNlbGY= 2698
IEVs 2699
IHRhc3Q= 2700
IGNvbnRlbnQ= 2701
IGN1 2702
hOs= 2703
IE1heWJl 2704
IEplc3Vz 2705
b3Jlcw== 2706
cG9ydA== 2707
qbQ= 2708
IGdpdmVz 2709
IG5vcm1hbA== 2710
0YDRgw== 2711
IGltcGFjdA== 2712
w6Ry 2713
IGRpZXM= 2714
IGxhYg== 2715
c2g= 2716
aW9z 2717
IFByZXM= 2718
IFVuZA== 2719
IE9m 2720
IGZpbmFsbHk= 2721
IGRvbGw= 2722
IHZvY8Oq 2723
cGx5 2724
IEFn 2725
IHRha2Vu 2726
IGdyb3VuZA== 2727
Zm9ydA== 2728
IGdhdmU= 2729
IEluc3Q= 2730
IGxvc3Q= 2731
IHdvcmtlZA== 2732
IGxpdGVy 2733
IGlzc3Vl 2734
IGluZHVzdA== 2735
IHJldHVybg== 2736
IGhhcHBlbmluZw== 2737
IHdhbnRz 2738
0LjQsg== 2739
IHByb2JsZW1z 2740
IENhcg== 2741
nbw= 2742
IEFsc28= 2743
IHNpemU= 2744
IG9idmlvdXNseQ== 2745
IFN1 2746
IFNj 2747
IHJlY29tbWVuZA== 2748
b3VyY2Vz 2749
YXN0aWM= 2750
Li4uLg== 2751
IG1p 2752
bGllcg== 2753
IEV2ZW4= 2754
Y2lh 2755
IGh1cg== 2756
dmE= 2757
IG1hc3M= 2758
IHdvdWxkbg== 2759
dW50 2760
Y2tz 2761
IGZlbHQ= 2762
b3Nw 2763
bGlnaHQ= 2764
0L7Qu9GM 2765
bmll 2766
IGJvdHRvbQ== 2767
INCx0Ys= 2768
b3JlZA== 2769
aXNvbg== 2770
IGdyYWQ= 2771
IHVtYQ== 2772
IHZh 2773
IOyC 2774
cmVzc2lvbg== 2775
dWxhdGlvbg== 2776
SUQ= 2777
aWRlbmNl 2778
IGJ1cg== 2779
IGdvbmU= 2780
bHU= 2781
7Ja07A== 2782
IHJlZHU= 2783
IGph 2784
7J2Y 2785
aXRh 2786
IHNvZnQ= 2787
IMOnYQ== 2788
aWNv 2789
ZXJhbA== 2790
w7E= 2791
YWY= 2792
IHBvaW50cw== 2793
Z3U= 2794
IGTDqQ== 2795
YXB0 2796
YXg= 2797
IEFscmlnaHQ= 2798
IGNhbWVyYQ== 2799
IGFjaA== 2800
INC/0L4= 2801
IHNldmVy 2802
NTA= 2803
IHNpZQ== 2804
z4E= 2805
IG1hbA== 2806
IGNvbXB1dA== 2807
IG1pZGRsZQ== 2808
IGNvdWxkbg== 2809
bWluZw== 2810
IOyL 2811
IEhpcw== 2812
IGdhbWVz 2813
IGludHJvZHU= 2814
IGNlbGw= 2815
cG9y 2816
IHNsZWVw 2817
IOuz 2818
aWRpbmc= 2819
IG91 2820
IGRlZw== 2821
IGRyaW5r 2822
IGVudmlyb25tZW50 2823
IFVuaXRlZA== 2824
IHRhbGtlZA== 2825
IGNob29zZQ== 2826
IGpvdXI= 2827
ZWdl 2828
IE1pbg== 2829
IGludGU= 2830
IHJhdGhlcg== 2831
IG9mZmlj 2832
0LrQsA== 2833
YWNoaW5n 2834
IG1lbnRpb25lZA== 2835
IGZpbGw= 2836
IHRyYWNr 2837
IG5pZQ== 2838
IHV0 2839
INCy0Ys= 2840
aWJpbGl0eQ== 2841
IHZhYw== 2842
IHJhZA== 2843
IHBhY2s= 2844
IHNlbmQ= 2845
IERhcw== 2846
IEFi 2847
IGVuZ2luZQ== 2848
44GX 2849
IGNvbXBldA== 2850
w7Q= 2851
INCy0YE= 2852
IGRvb3I= 2853
IGxvbmdlcg== 2854
5bCN 2855
IGxhbmd1YWdl 2856
IGV4dHJh 2857
cGxheQ== 2858
IHdlYnM= 2859
dW1i 2860
cm9vbQ== 2861
55w= 2862
IGJlZ2lubmluZw== 2863
IHJlZmVy 2864
QU0= 2865
bmVu 2866
aWdoZXI= 2867
ZmFjZQ== 2868
ZXJj 2869
IGZvcmdldA== 2870
IGNvbW1lbnQ= 2871
0LXQug== 2872
0LvRjw== 2873
cm9y 2874
xbxl 2875
IEdl 2876
IGRhcms= 2877
IGFueW9uZQ== 2878
YW50ZQ== 2879
Z2Vz 2880
7Iq1 2881
0ZE= 2882
YmVk 2883
amU= 2884
cnVjdHVyZQ== 2885
IHByaW0= 2886
aWRh 2887
6KY= 2888
44G+ 2889
IG1peA== 2890
IHN0YXJ0aW5n 2891
IOydtOs= 2892
IHByb3ZpZGU= 2893
YWN0aW9u 2894
IG1vdGhlcg== 2895
IHBlcmlvZA== 2896
IHN0aWNr 2897
IFlvdVQ= 2898
IHRlY2hub2xvZ3k= 2899
6rk= 2900
IGJlZA== 2901
IGdpdmluZw== 2902
IGV4cGxhaW4= 2903
emVu 2904
aW1hdGU= 2905
IHJlcHJlc2VudA== 2906
bG9hZA== 2907
IEhvd2V2ZXI= 2908
IGxpdmVz 2909
dXRo 2910
aXJpdA== 2911
b2du 2912
IGxpaw== 2913
IHJlc3BvbnM= 2914
IHByaXY= 2915
IHRvbQ== 2916
w6fDo28= 2917
aWFt 2918
IGV4Y2l0ZWQ= 2919
IGNhcmQ= 2920
Z3JvdW5k 2921
INeU 2922
IHNlbnM= 2923
IHRlYWNo 2924
aWRv 2925
aG9k 2926
IGVwaXM= 2927
IHdlbGNvbWU= 2928
IHdhbGw= 2929
5Lk= 2930
IGNoYW5jZQ== 2931
aGVu 2932
INCh 2933
IMSR 2934
IHNpbXBseQ== 2935
INGC0LDQug== 2936
cmluZw== 2937
amE= 2938
Ym9vaw== 2939
IHNldmVyYWw= 2940
c3Rl 2941
IGNyZWF0ZWQ= 2942
INC+0YI= 2943
IHB1c2g= 2944
PT0= 2945
IGhpZ2hlcg== 2946
dWY= 2947
b3VyY2U= 2948
b2tl 2949
IG9ubGluZQ== 2950
IHJlbGU= 2951
IHRvbg== 2952
ZW5zaXZl 2953
IGZhdm9yaXRl 2954
0YPQtA== 2955
IGxvb2tlZA== 2956
IHZvbg== 2957
4oCU 2958
IGbDvHI= 2959
IGJ1dHRvbg== 2960
IGJpbGw= 2961
IGNoYW5nZXM= 2962
ISI= 2963
IHNsb3c= 2964
YWJsZXM= 2965
IGRlYXRo 2966
YW5kcw== 2967
YXRlZw== 2968
IHRoZW1zZWx2ZXM= 2969
44Gj 2970
IGNvcA== 2971
44Gu 2972
IHBlcnNvbmFs 2973
dWdoaW5n 2974
IDEx 2975
Z2Fy 2976
YWRlcw== 2977
IG5lZWRlZA== 2978
IHN0dWR5 2979
YWdlZA== 2980
0YHRgtCy 2981
aW5v 2982
IGRpc2M= 2983
a2k= 2984
IGFkZHJlc3M= 2985
16g= 2986
aXR0ZW4= 2987
ZXNvbWU= 2988
INC2 2989
pOs= 2990
dXJh 2991
IG11 2992
IGNvbnRpbnU= 2993
Zm9y 2994
IG1hdGNo 2995
44Gm 2996
IHN0cmFpZ2h0 2997
kOs= 2998
bmVycw== 2999
IGRvZw== 3000
IGRlYg== 3001
IENP 3002
IG9z 3003
Z2Vk 3004
Y2FtZQ== 3005
IGNvcnJlY3Q= 3006
ZXR0ZQ== 3007
IFNlZQ== 3008
IGluY2x1ZGluZw== 3009
IEV1cm8= 3010
ZXN0ZXI= 3011
IGp1bXA= 3012
IFdoaWNo 3013
INC60LDQug== 3014
c29u 3015
eWE= 3016
SU5H 3017
IGVpbmU= 3018
b3No 3019
ZW5jeQ== 3020
IG1lZGlh 3021
IHN1YnNjcmliZQ== 3022
6YI= 3023
IHByaW4= 3024
IGhhYg== 3025
IFBlcg== 3026
IFdhcw== 3027
IHBhZ2U= 3028
aXRvcg== 3029
IHRvd2FyZHM= 3030
IHRyaWVk 3031
ZW5nZQ== 3032
YXJ0bWVudA== 3033
IHZhcmk= 3034
IHBhcGVy 3035
IHBpY3R1cmU= 3036
IHZlcnNpb24= 3037
IGJyb3VnaHQ= 3038
d2FyZQ== 3039
IFN0YXRlcw== 3040
IHNpY2g= 3041
bGVkZ2U= 3042
IHBlcmNlbnQ= 3043
IGdvZA== 3044
ZWM= 3045
IENvbW0= 3046
IGRlY2lkZWQ= 3047
IHNlbGVjdA== 3048
7ZWc 3049
KS4= 3050
dXJpdHk= 3051
IGZ1cnRoZXI= 3052
IGNvbW1lbnRz 3053
bGVtZW50 3054
IGRyZWFt 3055
IGNlbnRlcg== 3056
bWk= 3057
IGNhcw== 3058
IHdvbWFu 3059
IHJvYWQ= 3060
IGZhaWw= 3061
IGJlY2FtZQ== 3062
bHVz 3063
aWxpdGllcw== 3064
44Gv 3065
IENv 3066
IG1hbmFnZQ== 3067
IHJlY29nbg== 3068
IGFjdGlvbg== 3069
IGJlbmVm 3070
IGVhcmxpZXI= 3071
15w= 3072
IHNwZWVk 3073
IG1lbnQ= 3074
IHNvY2k= 3075
IHNob290 3076
dWk= 3077
IMOk 3078
IGFwcGx5 3079
dm8= 3080
eGlt 3081
IGNhdXNl 3082
IHN1cnBy 3083
IGhhYmVu 3084
REk= 3085
IGZhdGhlcg== 3086
IE5leHQ= 3087
IFlvdVR1YmU= 3088
IGNvZGU= 3089
IHJvbGU= 3090
Z3Jlc3M= 3091
IGdyZWVu 3092
ZXR0 3093
IGJ1aWx0 3094
IGZsb3c= 3095
IGJhc2U= 3096
IHRyYWluaW5n 3097
IHJvdW5k 3098
IFdpbGw= 3099
IHBhdGg= 3100
IFJv 3101
IGludGVyZXN0ZWQ= 3102
7Ja0 3103
IHJlc3BlY3Q= 3104
IGNoYW5nZWQ= 3105
aXNzaW9u 3106
IHN0dWRlbnQ= 3107
b2dyYXBo 3108
IGFwcHJvYWNo 3109
IHNob3dz 3110
5bCx 3111
IHRhcg== 3112
IGNyaXQ= 3113
IGdsbw== 3114
7Iq164uI64uk 3115
IGRlYWQ= 3116
IFByZXNpZGVudA== 3117
IHRob3Vz 3118
IGJhbA== 3119
c3Rlcg== 3120
ZXg= 3121
IGFic29sdXRlbHk= 3122
IG1pYw== 3123
IHByYWN0aWNl 3124
IHF1YWxpdHk= 3125
IGxvd2Vy 3126
b2dsZQ== 3127
IHNlcGFy 3128
YmFsbA== 3129
bWVkaQ== 3130
IHJldmlldw== 3131
IEFwcA== 3132
IG9r 3133
4oCL 3134
IGV4cGVyaWVu 3135
IGNvbmNlcm4= 3136
ZW50aWFsbHk= 3137
bW9yZQ== 3138
IEpv 3139
YXBhbg== 3140
IEljaA== 3141
aXN0aWM= 3142
IGZhaXI= 3143
IHdlYnNpdGU= 3144
aXJlcw== 3145
IEJ5 3146
IHRyYXZlbA== 3147
IHJpc2s= 3148
IG1pcg== 3149
IGJvYXJk 3150
IHNlbg== 3151
IHBhcmVudHM= 3152
IFdvdw== 3153
IGZlZWQ= 3154
IHNhdmU= 3155
IHNlcmlvdXM= 3156
IGluaXQ= 3157
RUw= 3158
dW5kcmVk 3159
QVM= 3160
IHZhbg== 3161
b3Jyb3c= 3162
IHdvcnRo 3163
IHNlYXJjaA== 3164
IDE2 3165
IHBhcnRz 3166
0YHRgtGM 3167
IGNvbXBhbg== 3168
IG1vdmll 3169
IG1ldGhvZA== 3170
IGlsbA== 3171
IHdpc2g= 3172
ZHk= 3173
IGl0ZW0= 3174
IG1pbnVz 3175
YW5nZXI= 3176
IHZvaWNl 3177
IHNraW4= 3178
IGFyZWFz 3179
IGVpZ2h0 3180
IG9icw== 3181
ICw= 3182
0LDQuQ== 3183
IG9pbA== 3184
IGN5 3185
IGJhYnk= 3186
c3k= 3187
IGVtcGxveQ== 3188
IEtl 3189
IHBsYWNlcw== 3190
IGZpeA== 3191
IGVzdMOh 3192
44Go 3193
aXZlZA== 3194
IGxvdHM= 3195
IHNlYXNvbg== 3196
dW5r 3197
YWx0 3198
IHRhYmxl 3199
INCi 3200
w6I= 3201
IGF0dGVudGlvbg== 3202
44Gq 3203
IEhlcg== 3204
IGFnZQ== 3205
IHByYQ== 3206
YmFjaw== 3207
Y2ls 3208
IG5ldHdvcms= 3209
cml0 3210
IGRvYw== 3211
IGFyZW4= 3212
aWdlbg== 3213
IOuE 3214
2K8= 3215
ZW5kZXI= 3216
IHRvdGFs 3217
IHByaWNl 3218
IGNyYXp5 3219
7Jo= 3220
aXF1 3221
dGhvdWdo 3222
WW91 3223
2Yc= 3224
44KT 3225
z4U= 3226
IHNhdA== 3227
IGJp 3228
IERpZQ== 3229
IHNoYQ== 3230
IHRoYW5rcw== 3231
dWg= 3232
IHN0YWdl 3233
0LDQtg== 3234
IEZs 3235
IGxlYXY= 3236
IGJveQ== 3237
IGFm 3238
w7Zu 3239
IEdldA== 3240
IGFjY2VwdA== 3241
IGVudGVy 3242
IHR1cg== 3243
IHNpxJk= 3244
IGhvbmVzdA== 3245
44CM 3246
IHNhbQ== 3247
IHJlcGw= 3248
Z2luZw== 3249
IGRldmVsb3BtZW50 3250
IEFjdA== 3251
b3Jh 3252
44CN 3253
5L4= 3254
IGtub3dz 3255
IGltYWdl 3256
IExvcmQ= 3257
0LjRgtGM 3258
IHdlZWtz 3259
IHNleA== 3260
lOs= 3261
IGh1bmRyZWQ= 3262
IHNvdW5kcw== 3263
IGxlYXJuZWQ= 3264
IGJ1ZA== 3265
INGB0YI= 3266
IGluY3JlZA== 3267
4pk= 3268
IG5vcw== 3269
IGRyb3A= 3270
IGJlbg== 3271
INCY 3272
IHNhZmU= 3273
YXRh 3274
IGZ1Y2s= 3275
c29jaQ== 3276
IGRhbg== 3277
IGNyb3Nz 3278
MTA= 3279
bW8= 3280
dmVydA== 3281
IDE3 3282
emll 3283
5ZU= 3284
IGRvbQ== 3285
IEJv 3286
IHNldHRpbmc= 3287
IGludm9sdmVk 3288
YXJpbHk= 3289
IHNpbmQ= 3290
IHN1cw== 3291
IHdvcnJ5 3292
ZXRo 3293
6rmM 3294
IHN1bg== 3295
IGhpZXI= 3296
IGNlcnRhaW5seQ== 3297
b3Vs 3298
b3J0cw== 3299
IEVy 3300
IFVt 3301
IGNhdXM= 3302
IG5hdHVyYWw= 3303
IMO8 3304
IGNyeQ== 3305
IFNlYw== 3306
IHNvbQ== 3307
5rI= 3308
IGVkdWNhdGlvbg== 3309
0LDQtdGC 3310
IG11bHRpcA== 3311
IGFsb25l 3312
IGV5ZQ== 3313
IHJhdGU= 3314
IEV1cm9wZQ== 3315
6L8= 3316
bW9u 3317
IGZpdA== 3318
aXppbmc= 3319
cHBlZA== 3320
IHByZXNzdXJl 3321
dGhl 3322
0LjRgQ== 3323
aXRlcw== 3324
IEFm 3325
cmVjaQ== 3326
YXR0bGU= 3327
IHNlcnZpY2Vz 3328
IEdvb2dsZQ== 3329
6YE= 3330
IGNhc2Vz 3331
IGRyaXZl 3332
IGNoYWxsZW5n 3333
dXo= 3334
IE1v 3335
7Jy86w== 3336
dmFs 3337
5YCL 3338
IGZvbA== 3339
IOyi 3340
ZmZpYw== 3341
IHJh 3342
IHNpbg== 3343
IGJsdWU= 3344
IGFmZmVjdA== 3345
IG1pcw== 3346
IHNob3Q= 3347
INC+0LE= 3348
YXNpbmc= 3349
IHNpZ25pZmlj 3350
IENoZQ== 3351
IOqz 3352
IHBvc2l0aXZl 3353
7KM= 3354
IHdpZQ== 3355
IDQw 3356
b3JkaW5n 3357
IEZyb20= 3358
6rU= 3359
IGJyYW5k 3360
IHRydXN0 3361
IHBsZQ== 3362
IGNvbW11bmlj 3363
IHdlaWdodA== 3364
IGFza2luZw== 3365
IHRheA== 3366
IEphcGFu 3367
44Gf 3368
IO2VmA== 3369
b3Bz 3370
z4I= 3371
IHB1dHRpbmc= 3372
IHJvbGw= 3373
IEFtZXJpY2E= 3374
cmVn 3375
ntc= 3376
YXR1cmVz 3377
ZW5zaW9u 3378
IFNvbWV0 3379
IG9yaWdpbmFs 3380
cGluZw== 3381
IMWf 3382
IHByb2R1Y3Rz 3383
44O8 3384
IGNvbnRhY3Q= 3385
b2x1dGlvbg== 3386
IGdvYWw= 3387
IHBvdw== 3388
IHBlcmZvcm1hbmNl 3389
IGJsb29k 3390
YXRvcnM= 3391
IE1pY2g= 3392
IHRlbXBlcg== 3393
IERhbg== 3394
IHN1Z2c= 3395
0YLQuA== 3396
IGltbQ== 3397
IG9mZmljZQ== 3398
IGFycmk= 3399
IGNvbWZvcnQ= 3400
INCU 3401
IHN1Z2dlc3Q= 3402
IHBsYXQ= 3403
gpg= 3404
MTk= 3405
IG9t 3406
IHNldmVu 3407
IENlbnQ= 3408
aWxsZQ== 3409
IGNvbmNlcHQ= 3410
IGJhZw== 3411
w7xu 3412
aXZlbHk= 3413
IGRpdg== 3414
bW9z 3415
5ok= 3416
IGZlZWxz 3417
IGly 3418
YWtlcw== 3419
bGV5 3420
IHBhcnRpY2lw 3421
INCa 3422
Zmw= 3423
anVzdA== 3424
IHNpbA== 3425
IFBh 3426
QUw= 3427
IGdvdHRh 3428
IGZhbg== 3429
IGNoYWxsZW5nZQ== 3430
IGNvbXBhbmllcw== 3431
IFBlb3BsZQ== 3432
PC8= 3433
0L7Qtw== 3434
IHBlbg== 3435
aXNpbmc= 3436
IGF1cw== 3437
ZW1pYw== 3438
YW1lbnRl 3439
IG1lZXRpbmc= 3440
IHZpc2l0 3441
IHN1cHBvc2Vk 3442
IE9uY2U= 3443
0LTQsA== 3444
b3JsZA== 3445
MzA= 3446
VVM= 3447
IHZpb2w= 3448
IG5vdGljZQ== 3449
INCQ 3450
aGFu 3451
cGVk 3452
7Jg= 3453
aGg= 3454
IHRyb3U= 3455
IG1pbnV0ZQ== 3456
IFBhcg== 3457
cmF5 3458
IHRpdA== 3459
IHVwZA== 3460
IGJsb2Nr 3461
IGR1ZQ== 3462
YXVy 3463
IGZvcmNl 3464
IGNvdW4= 3465
IOKAlA== 3466
IHR5cGVz 3467
66c= 3468
IGxhdGU= 3469
IGltcHJvdmU= 3470
IOyI 3471
IGF2ZQ== 3472
dWxlcw== 3473
Y2w= 3474
YW1lZA== 3475
IGF3ZXNvbWU= 3476
IE9r 3477
IHZvdA== 3478
IG1hY2hpbmU= 3479
IGZvbGxvd2luZw== 3480
IG1lYXN1cmU= 3481
YWNpw7Nu 3482
dWVs 3483
Y2hhbg== 3484
IGFiaWxpdHk= 3485
IHRvdXQ= 3486
IGlkZWFz 3487
IGluY3JlYXNl 3488
IGVucw== 3489
INGF 3490
IOuq 3491
IGplc3Q= 3492
INCc 3493
IHRydXRo 3494
aHk= 3495
IHNwZW5k 3496
IHNjaWVuY2U= 3497
ZXRl 3498
IDE0 3499
IGVwaXNvZGU= 3500
IGFsZw== 3501
ZW5kZWQ= 3502
44GT 3503
YXJp 3504
bGxh 3505
IGZpc2g= 3506
IHRocm93 3507
bWl0 3508
5bk= 3509
IGNpcmM= 3510
IENhbA== 3511
IHRvdXI= 3512
IGRpcmVjdGlvbg== 3513
IG5vY2g= 3514
0LXQsg== 3515
w6lu 3516
IGNvdW50cmllcw== 3517
IGluZHVzdHJ5 3518
aW55 3519
aWNsZQ== 3520
IGZlZXQ= 3521
SXQ= 3522
IGxlYWRlcnM= 3523
ZXR6dA== 3524
IHN0YWZm 3525
55Q= 3526
IHB1cnA= 3527
aXRv 3528
PyE= 3529
IEph 3530
IHN0b3Jl 3531
ZXRpYw== 3532
IENoaW5h 3533
IOuQ 3534
IFVuaXZlcnNpdHk= 3535
ICM= 3536
IGRlY2lzaW9u 3537
IGFjaGll 3538
IGFjdHVhbA== 3539
dWx5 3540
IHNlY3Rpb24= 3541
IHJlc3VsdHM= 3542
IHN0YXI= 3543
IG1pc3Q= 3544
aWJseQ== 3545
IGRhZA== 3546
IG51bWJlcnM= 3547
b21i 3548
6Ko= 3549
IFNwZQ== 3550
IG1lcg== 3551
IDI1 3552
IGF1dG9t 3553
IGNvbGQ= 3554
2Kg= 3555
hJw= 3556
YWdlcg== 3557
IFRW 3558
IFNpZQ== 3559
IEhhdmU= 3560
IMW8ZQ== 3561
dWdn 3562
YWluZWQ= 3563
IHVwb24= 3564
IGxvZw== 3565
IGNvbXBsZXRl 3566
IGJyYWlu 3567
YWdpbmc= 3568
IE11cw== 3569
b3Zlcg== 3570
IGVhc2llcg== 3571
IGludGVncg== 3572
IG3DoXM= 3573
IHR1cm5lZA== 3574
IHN0cmk= 3575
aXZhbA== 3576
IGhlYXY= 3577
IFRI 3578
IHdyaXRpbmc= 3579
0YDQsA== 3580
5Zyo 3581
5aSn 3582
IGNsYQ== 3583
ZGluZw== 3584
IHRlbGxpbmc= 3585
0LjQtA== 3586
aWNhdGVk 3587
5Lul 3588
YWNodA== 3589
44GC 3590
aGFwcw== 3591
IFN0ZQ== 3592
IHJlc291cmNlcw== 3593
IGRhbm4= 3594
IHBhcnR5 3595
IM+E 3596
IHNhZg== 3597
aXNlcw== 3598
dHJl 3599
b2ludA== 3600
IGtub3dsZWRnZQ== 3601
IGFueW1vcmU= 3602
IGZseQ== 3603
IG1haW50 3604
0LjQug== 3605
5ZE= 3606
IHNlbGw= 3607
bGF1Z2hz 3608
IFlvcms= 3609
IGJpZW4= 3610
IG9k 3611
IGVhc2lseQ== 3612
IHJhbmdl 3613
IG9wdGlvbg== 3614
2Lk= 3615
IGFwcHJlY2k= 3616
b2Ny 3617
IGRldGVybQ== 3618
0YQ= 3619
IG1lYW5pbmc= 3620
IHNpdGU= 3621
IGRpc2Nv 3622
dmVyYWdl 3623
IGxvc2U= 3624
IGluc3RhbGw= 3625
IGVtb3Q= 3626
YW50bHk= 3627
w6R0 3628
IHRhbWI= 3629
IFdhcg== 3630
IEhv 3631
IEdlbg== 3632
ZW15 3633
0LXQtw== 3634
IFBvbA== 3635
IG1lc3NhZ2U= 3636
IG5vdGU= 3637
jIA= 3638
IGhldA== 3639
IGltbWVkaQ== 3640
IGF2bw== 3641
IGJvb2tz 3642
IGJlY29tZXM= 3643
cmVzaA== 3644
w6hz 3645
YXNvbnM= 3646
IGhpbXNlbGY= 3647
dXRz 3648
IGp1 3649
IGF3YXJl 3650
IHJlcXVpcmU= 3651
IHN5c3RlbXM= 3652
IEhhcg== 3653
IGFtb25n 3654
IGhvbQ== 3655
IGJyZWF0 3656
IHdlaXJk 3657
IOu2 3658
zrs= 3659
2Kk= 3660
aWZm 3661
b3Jpbmc= 3662
IHBsYXRmb3Jt 3663
IFRha2U= 3664
IGhlbHBz 3665
dXRpb25z 3666
IGZvcmc= 3667
IGx1Y2s= 3668
IEVuZ2xpc2g= 3669
IHdlYg== 3670
IG5lZ2F0aXZl 3671
IHR1dA== 3672
IGFib3Zl 3673
bmd0aA== 3674
IOqxsA== 3675
IHN0b3JpZXM= 3676
IGxvYWQ= 3677
IGJhY2tncm91bmQ= 3678
IHN3aXRjaA== 3679
Z2E= 3680
IHByaW5jaQ== 3681
IGZpbmFu 3682
IHZhcmlvdXM= 3683
IGzDoA== 3684
IGtpbmRz 3685
YWluaW5n 3686
IG5hdHVyZQ== 3687
INCe 3688
Y3o= 3689
IHByYXk= 3690
IGdhcg== 3691
aXJt 3692
ICY= 3693
IOyD 3694
bnM= 3695
IFJlcA== 3696
IEZl 3697
IHJldg== 3698
cmFuZA== 3699
IGxpa2VseQ== 3700
IHVuZGVyc3RhbmRpbmc= 3701
xLFy 3702
44GL 3703
IGZhbA== 3704
IDEz 3705
0YbQuA== 3706
IHN1ZA== 3707
IGJyb3RoZXI= 3708
IHBsYW50 3709
IHRocm91Z2hvdXQ= 3710
d2lzZQ== 3711
cHJl 3712
IGN1bHR1cmU= 3713
INmF 3714
IHdvbmRlcmZ1bA== 3715
IGFo 3716
cHBlcg== 3717
IHNvbGQ= 3718
IHN0YXJ0cw== 3719
IHdyaXR0ZW4= 3720
zq8= 3721
bmk= 3722
INeU1w== 3723
IERhdg== 3724
IHVsdA== 3725
IGFybQ== 3726
IHJvY2s= 3727
IHdlYXI= 3728
642w 3729
YW5v 3730
cmFn 3731
IHNxdWFyZQ== 3732
0LDQvdC4 3733
Y2FzdA== 3734
bGVicg== 3735
IGxpdGVyYWxseQ== 3736
IHBsYXllZA== 3737
IGhlYXQ= 3738
b25zZQ== 3739
cmljdA== 3740
IGluc3A= 3741
aWRz 3742
IHBvcHVsYXI= 3743
64+E 3744
IGNhdGNo 3745
IG1vdW50 3746
IGp1ZA== 3747
V2hhdA== 3748
0LXQsQ== 3749
UkE= 3750
YXVk 3751
0LrQvg== 3752
IHN1cmZhY2U= 3753
IGNvbnY= 3754
IHBpZWNlcw== 3755
T2g= 3756
5oA= 3757
IHN0eWxl 3758
cHBpbmc= 3759
IHJlYWRpbmc= 3760
IGNvbnZlcnNhdGlvbg== 3761
0L7Qvw== 3762
5L6G 3763
IEFnYWlu 3764
IGJhbms= 3765
dGltZQ== 3766
0YPRgg== 3767
ZXJ2ZQ== 3768
IEdyZWF0 3769
IGNhcHQ= 3770
0LDQsQ== 3771
YXlz 3772
IEZpbg== 3773
aWZpY2F0aW9u 3774
IMOkcg== 3775
0LDRjg== 3776
IGVnZw== 3777
IFdlbA== 3778
IHRhcmdldA== 3779
dWxh 3780
Y2hlcw== 3781
YW5p 3782
T08= 3783
aWNpb3Vz 3784
bm93 3785
z4M= 3786
Ym9hcmQ= 3787
IGdlbnRl 3788
IGRybw== 3789
IEV0 3790
IGRpbg== 3791
IGNvcw== 3792
IGF1dGhvcg== 3793
2LM= 3794
IG9jaA== 3795
IGVtYWls 3796
IHNwaXJpdA== 3797
IHNpdHRpbmc= 3798
bWFz 3799
IHN0cmVuZ3Ro 3800
IGJpZ2dlcg== 3801
IFdhaXQ= 3802
IG1hdA== 3803
IHBvbGljZQ== 3804
cmVzc2Vk 3805
IHdhaXRpbmc= 3806
aXNoaW5n 3807
IGRvbGxhcnM= 3808
aG9vZA== 3809
c3M= 3810
IGltYWdpbmU= 3811
aW5p 3812
IG1lcw== 3813
IGRpc2U= 3814
aWRnZQ== 3815
YWJvcg== 3816
IHBldA== 3817
IGhvcA== 3818
IEtpbmc= 3819
IGNvbXB1dGVy 3820
IGdvbGQ= 3821
IG51 3822
IGZpbmc= 3823
KSw= 3824
IHNlY3VyaXR5 3825
cnVjdGlvbg== 3826
IHNvbHV0aW9u 3827
ZXh0 3828
IHBhdHRlcg== 3829
aWNrZW4= 3830
dXJlZA== 3831
IHN0YW5kYXJk 3832
7Iuc 3833
IGRvdWJsZQ== 3834
zrc= 3835
IHdpZmU= 3836
aXNh 3837
IGRpcmVjdGx5 3838
YWNlZA== 3839
IGJ1bmNo 3840
IMK/ 3841
0LDQu9GM 3842
IHJlZ2FyZA== 3843
IHN3ZWV0 3844
IHVuaXF1ZQ== 3845
IOKZqw== 3846
IHRyYWlu 3847
IEdlcm0= 3848
zqw= 3849
UkU= 3850
IGJlaGF2 3851
IHByZWQ= 3852
7IM= 3853
c2V0 3854
IGRlc2NyaXB0aW9u 3855
w6ll 3856
IGNhdA== 3857
5ZM= 3858
IGNvbGxlZ2U= 3859
7Js= 3860
IGFwcGxpY2F0aW9u 3861
IFNlbg== 3862
YXNr 3863
IGNyZWQ= 3864
dWJsaWM= 3865
IG11bHRpcGxl 3866
IG5p 3867
IHByZXNpZGVudA== 3868
IGFkZGVk 3869
IHJvYg== 3870
IGFxdWk= 3871
IGhvc3A= 3872
IHRvb2xz 3873
IGd1bg== 3874
IGJhc2lj 3875
IGxpbmVz 3876
IHN0cnVjdHVyZQ== 3877
IFJ1c3M= 3878
IHRvdGFsbHk= 3879
IGJpZ2dlc3Q= 3880
IGVlbg== 3881
IGFyZw== 3882
INec 3883
IHBhcms= 3884
IERlcw== 3885
IGNlbGVicg== 3886
IGZhaXQ= 3887
0LXQvdGM 3888
IHN1ZmY= 3889
IHJlZ3VsYXI= 3890
qOs= 3891
IG1pbmU= 3892
IEtvcmU= 3893
IHByZXZpb3Vz 3894
IHBp 3895
IHNlZw== 3896
IHBvbGljeQ== 3897
INC60L4= 3898
IFRydW1w 3899
IHZhY2M= 3900
w7N3 3901
IFN5 3902
0LjRhw== 3903
aXR0ZXI= 3904
IHBvbGl0aWNhbA== 3905
cmFz 3906
IGFscw== 3907
0LXQu9GM 3908
IHNoYXBl 3909
YW56 3910
IG9udG8= 3911
IGFyY2g= 3912
IGFtYg== 3913
YWdyYW0= 3914
IFNt 3915
Y3Rpb25z 3916
IGpvaW4= 3917
Ym9y 3918
5Zs= 3919
IGZyYW1l 3920
oIc= 3921
IGNob2ljZQ== 3922
4K+B 3923
0YPRjg== 3924
IENvcg== 3925
IFN3 3926
SVQ= 3927
IHRlbmQ= 3928
IEVhcg== 3929
IHRvcg== 3930
IGV2ZW50cw== 3931
IGNsYWlt 3932
IERh 3933
IE1hcms= 3934
IGdyb3Vwcw== 3935
IGVhdGluZw== 3936
IFdvcmxk 3937
IHJlY2VudGx5 3938
IHRhc3Rl 3939
IHN1cnY= 3940
4KQ= 3941
IHNraWxscw== 3942
INC40Lc= 3943
aXR0ZWQ= 3944
IHNob3A= 3945
7J207A== 3946
IGVzdGFi 3947
IOuCmA== 3948
IHNlY29uZHM= 3949
IFRob3Nl 3950
IEVudA== 3951
IOyE 3952
ZXJzb24= 3953
IHRvd24= 3954
IGNhbmQ= 3955
IG9wdGlvbnM= 3956
IGluZw== 3957
VklE 3958
IGVuY291cg== 3959
IHLDqQ== 3960
4pmq 3961
IGVudHJl 3962
IG1vdmVtZW50 3963
IEJlbg== 3964
IGJpcnRo 3965
IHdoZQ== 3966
IGhhbmc= 3967
IEVt 3968
aWdl 3969
cm9sbA== 3970
IHVuZg== 3971
7II= 3972
IHJpZA== 3973
IHNwcmVhZA== 3974
IGhvc3Q= 3975
YWxk 3976
IEVk 3977
IGNvbnN1bQ== 3978
VU4= 3979
IG9waW4= 3980
aXRhcg== 3981
IE1lZA== 3982
IHN1YmplY3Q= 3983
IHBhbA== 3984
IGNhcnJ5 3985
IGFncmVl 3986
IFdoaWxl 3987
IGNhcmVlcg== 3988
IHNjaWVudA== 3989
IHN1ZGRlbg== 3990
IGZpbGU= 3991
emk= 3992
IGV4Y2VwdA== 3993
6bo= 3994
IHBvdGVudGlhbA== 3995
IEFub3RoZXI= 3996
IGNvbXBsZXg= 3997
IFNpbQ== 3998
ZW5kbw== 3999
IHJhaXM= 4000
IHBoeXNpY2Fs 4001
IGRhdGU= 4002
YWtlcg== 4003
IENvbA== 4004
IHBvd2VyZnVs 4005
IG1lbWJlcg== 4006
cmFw 4007
IHNwb3Q= 4008
IHNvdXJjZQ== 4009
IGZlbQ== 4010
w6lt 4011
IGVtcA== 4012
amk= 4013
aWV0eQ== 4014
IGluZmx1 4015
IGRyeQ== 4016
IGxvY2s= 4017
IHplcm8= 4018
IFVo 4019
IHJvdXQ= 4020
IHBvcnF1ZQ== 4021
IDI0 4022
IHRhbA== 4023
IGZvbGtz 4024
IGxhdW5jaA== 4025
IGNvbXBvbg== 4026
IFdlbGNvbWU= 4027
IGthbm4= 4028
w6Ru 4029
INGN0YI= 4030
ZWVz 4031
INmI 4032
IGFueXdheQ== 4033
IGF1ZGllbmNl 4034
5Lq6 4035
IHNsaWdodA== 4036
b25h 4037
IHVy 4038
IHJlbGln 4039
IGV4dHJlbQ== 4040
xLF6 4041
IE1h 4042
zrw= 4043
IMO2 4044
IGFsbG93cw== 4045
IGZhdA== 4046
IEZhY2U= 4047
IG5hdGlvbmFs 4048
IGludGVydmlldw== 4049
IE1j 4050
w6l0 4051
IGN1dGU= 4052
ZWxh 4053
IHNlY3JldA== 4054
IFdlc3Q= 4055
IERlcA== 4056
IGV4ZXJj 4057
IGhpc3Rvcg== 4058
IHByaW9y 4059
IDYw 4060
YXZh 4061
YWNoZXI= 4062
eW9uZA== 4063
IEhh 4064
IGVzdGU= 4065
aW5hcnk= 4066
IE5vcnRo 4067
b25zdA== 4068
IHNtYXJ0 4069
YW1z 4070
0LDQu9C4 4071
IGRhcg== 4072
ZXJlZA== 4073
IGZ1bm55 4074
IE9i 4075
IEJsYWNr 4076
IHJlbGF0ZWQ= 4077
IEJ1 4078
IHNvbWV3aGVyZQ== 4079
IFJlbQ== 4080
bmVz 4081
bWVudGU= 4082
IFJlYWxseQ== 4083
IGNyZWF0aW5n 4084
IGZhbWls 4085
IHNvY2lldHk= 4086
IGdlbA== 4087
IHRyYW5zZm9ybQ== 4088
xIM= 4089
IGluY2x1ZGU= 4090
IGhvbA== 4091
bGlrZQ== 4092
a28= 4093
YWlycw== 4094
INC/0L7QtA== 4095
IHBlcnNwZWN0 4096
IGJlcw== 4097
IHBhcnRpY3VsYXJseQ== 4098
IHNob3dpbmc= 4099
IFBhcnQ= 4100
IHF1YWw= 4101
bG9jaw== 4102
IHJlYWxpdHk= 4103
aG9sZA== 4104
aWN0aW9u 4105
b29u 4106
IHZpcg== 4107
44Gr 4108
aXRhcnk= 4109
IGRydWc= 4110
IGZlYXR1cmU= 4111
IHJlYXNvbnM= 4112
INep 4113
IHdyb3Rl 4114
IGZhbnQ= 4115
IGJhbmQ= 4116
2YM= 4117
ZW5h 4118
a2V5 4119
IGVhcnRo 4120
ZG9t 4121
IGZlYXR1cmVz 4122
IGZsb29y 4123
IHNwZWFraW5n 4124
IHRpcA== 4125
IEF1c3Q= 4126
IHN0b2Nr 4127
IGNodXJjaA== 4128
IHJhYw== 4129
7Jy866Gc 4130
4LiZ 4131
44KM 4132
a3k= 4133
IHJlc3BvbnNl 4134
24w= 4135
dWxhdGlvbnM= 4136
IHNsaWRl 4137
IGdyYWR1 4138
Y2lvdXM= 4139
IG1lYW50 4140
ID09 4141
INeQ1w== 4142
44U= 4143
IGtpbmRh 4144
IHNjZW5l 4145
IG11aXQ= 4146
IOqwgA== 4147
cmFzdA== 4148
cmVzdA== 4149
IHBsYXllcnM= 4150
d2E= 4151
IGJyb2Fk 4152
IHRvbW9ycm93 4153
b2NvbA== 4154
INGB0LI= 4155
IEJhcg== 4156
xLFr 4157
IHNlYQ== 4158
IHJlbW92ZQ== 4159
IHJlbWluZA== 4160
0L7QvNGD 4161
IFNpbmNl 4162
IGF2ZWM= 4163
Y2VsbA== 4164
0LjRhQ== 4165
IGRvY3VtZW50 4166
IOq3uOuf 4167
IG5laWdo 4168
YmVhdA== 4169
IHDDpQ== 4170
IGFzcGVjdA== 4171
IGRlZA== 4172
bGlzaGVk 4173
aWxz 4174
IG91cnNlbHZlcw== 4175
dWNl 4176
IGhleQ== 4177
INC/0YDQvg== 4178
ZW50eQ== 4179
IGFzc29jaQ== 4180
YWRvcw== 4181
dW1iZXI= 4182
IF0= 4183
6YKj 4184
bm92 4185
IOyZ 4186
0YPRhw== 4187
IGNvbmRpdGlvbg== 4188
64qU642w 4189
IHZhbHVlcw== 4190
IHNjZW4= 4191
bWluaXN0 4192
IGNhc3Q= 4193
IGdyb3dpbmc= 4194
IHVzZXI= 4195
IHJlc3BvbmQ= 4196
bGlt 4197
w6ly 4198
eW0= 4199
55yL 4200
b3Nlcw== 4201
c3ljaA== 4202
INGA0LDQtw== 4203
IGFwcGVhcg== 4204
IHByb2dyZXNz 4205
ZW5ndGg= 4206
IGphaw== 4207
IERpcw== 4208
IHBhdGllbnRz 4209
IFNlcg== 4210
IGdhcw== 4211
w6hyZQ== 4212
7Ja07JqU 4213
IHJlY2k= 4214
7J24 4215
IHNjYQ== 4216
ZXBlbmQ= 4217
0YHQug== 4218
0LDQvw== 4219
IGJhdHRlcg== 4220
IHZlaA== 4221
8J8= 4222
IGFjY29t 4223
IGJlYXQ= 4224
IHBhaW50 4225
IGNvbnRyaWI= 4226
IHNhZA== 4227
xrA= 4228
YWxlcw== 4229
IHRyZWU= 4230
YmE= 4231
IGJvcm4= 4232
aWNlZA== 4233
4K6V 4234
YmFuZA== 4235
IG1lY2hhbg== 4236
IERldA== 4237
IGNhcGl0YWw= 4238
IGRlbGl2ZXI= 4239
IGZlYXI= 4240
npg= 4241
IFNvdXRo 4242
IGJvdWdodA== 4243
IHN0cmVzcw== 4244
IHZvcg== 4245
Pz8= 4246
aWg= 4247
7JW8 4248
IGVyYQ== 4249
7J206w== 4250
0LDRjw== 4251
aXNpb25z 4252
aXZpdHk= 4253
IGhlbHBlZA== 4254
IGFzc2lzdA== 4255
IHBsYXllcg== 4256
cmFu 4257
IGltbWVkaWF0ZWx5 4258
IG1vdmVk 4259
Y2ll 4260
6rE= 4261
IGFubm91bg== 4262
5b8= 4263
7J6Q 4264
IHByb2R1Y3Rpb24= 4265
IHN1bW1lcg== 4266
IHR1bg== 4267
IHByb2dyYW1z 4268
R0g= 4269
YWxpbmc= 4270
aXJh 4271
ZWxlc3M= 4272
Lik= 4273
IGF2ZXJhZ2U= 4274
6KaB 4275
IGdsYXNz 4276
b21hbg== 4277
aWZpY2FsbHk= 4278
IOuLpA== 4279
IENvbmc= 4280
IFZlcg== 4281
IHRyaWNr 4282
IGJlZ2Fu 4283
IHZpbGw= 4284
6rGw 4285
aG93 4286
5q0= 4287
IHRpbGw= 4288
IDkw 4289
YmVydA== 4290
IOq4 4291
IHRlbXBlcmF0dXJl 4292
w7I= 4293
4LmI 4294
IGdyYXBo 4295
IOq3uA== 4296
IHJvdA== 4297
IG1vYg== 4298
QVk= 4299
YWVs 4300
IHJlcGU= 4301
IGRldmljZQ== 4302
IDE5OQ== 4303
IHRlbGU= 4304
IGtlcHQ= 4305
cGE= 4306
5pY= 4307
dmVyc2U= 4308
IHN0cmVhbQ== 4309
0LXRhw== 4310
ZXNzaW9u 4311
IHN0cnVnZw== 4312
eno= 4313
IGRlZ3JlZQ== 4314
IGhlbHBpbmc= 4315
IHNtZWxs 4316
IHBlcmhhcHM= 4317
cHJv 4318
IGNvbnRleHQ= 4319
IGlr 4320
INC/0LXRgA== 4321
IGNhbGN1bA== 4322
6bq8 4323
YmluZw== 4324
IHJlYWxpemU= 4325
bGFt 4326
IENoYXI= 4327
eXQ= 4328
IOydtOw= 4329
IGRhbmdlcg== 4330
IElt 4331
YWE= 4332
IGxvdmVk 4333
IHB1cnBvc2U= 4334
IGZpbmlzaGVk 4335
IHBlYWNl 4336
IG90 4337
IGdsb2JhbA== 4338
z4A= 4339
IGFiZXI= 4340
log= 4341
IGNoYXJhY3RlcnM= 4342
IG51cg== 4343
IGRhbWFnZQ== 4344
IGVtZXI= 4345
IHByZWM= 4346
IFdpcg== 4347
IGluc3RpdA== 4348
kdc= 4349
IGFsbG93ZWQ= 4350
Ym9u 4351
IHRvZA== 4352
0LXQs9C+ 4353
IGpldHp0 4354
IG1lZGlj 4355
IHNtYWxsZXI= 4356
Y2VlZA== 4357
IGxldmVscw== 4358
IGludGVsbA== 4359
V2U= 4360
IHNlbQ== 4361
IGN1cnJlbnRseQ== 4362
IG1vZGVybg== 4363
IGNvbnRyYWN0 4364
IGRldGFpbHM= 4365
b3J0dW5hdGVseQ== 4366
T1M= 4367
IHN0YXRlcw== 4368
IGFkanVzdA== 4369
YW50YWdl 4370
ZXo= 4371
IFZlcnk= 4372
IHNjYWxl 4373
IHJlbGVhc2U= 4374
IGZheg== 4375
IGlj 4376
aXR1ZGU= 4377
QUM= 4378
IFBhdA== 4379
aWRlbg== 4380
rZA= 4381
IHByZWZlcg== 4382
b2xvZ2ljYWw= 4383
IEZhY2Vib29r 4384
IOqwmQ== 4385
IC4u 4386
IE1ha2U= 4387
INC60L7RgtC+0YA= 4388
IERhdmlk 4389
IEFmcmlj 4390
IG1vZGU= 4391
IENpdHk= 4392
IHNoYWxs 4393
INGE 4394
aW1pbg== 4395
INC30LA= 4396
cm9t 4397
dWE= 4398
IGJleW9uZA== 4399
IGRpc3RyaWI= 4400
0LrRgw== 4401
IERvZXM= 4402
IHZpY3Q= 4403
cmF0ZQ== 4404
IHZhaQ== 4405
IHN1Y2Nlc3NmdWw= 4406
IGhvdXM= 4407
YWhh 4408
ZXN0cw== 4409
IEVzdA== 4410
IGRpc2NvdmVy 4411
IHRoZXJlZm9yZQ== 4412
Y2hh 4413
IGN1cA== 4414
IHBvcHVsYXRpb24= 4415
IEls 4416
c2M= 4417
IHNwZW50 4418
cmVs 4419
IHVzZWZ1bA== 4420
IHRhYg== 4421
5p0= 4422
IMU= 4423
IOygnA== 4424
IGNvbnNl 4425
IHF1YW50 4426
YXlh 4427
IGJvbg== 4428
5Y+v 4429
IENoaW4= 4430
IOqygw== 4431
b3VuZHM= 4432
0LXRiA== 4433
ZWxsZQ== 4434
IGljZQ== 4435
MjE= 4436
IGtpY2s= 4437
5LiL 4438
IHN0ZXBz 4439
IHRvbmlnaHQ= 4440
0L3Ri9C5 4441
cmVuY2g= 4442
Lic= 4443
IGdyYWI= 4444
IGltcGxlbWVudA== 4445
IOyImA== 4446
IG1pc3Npb24= 4447
IGNsZWFybHk= 4448
IGFwcHJlY2lhdGU= 4449
6IA= 4450
IGZyZXNo 4451
YXJt 4452
IFR3bw== 4453
IGV4ZWM= 4454
IHByb2plY3Rz 4455
IGNvbW11bml0aWVz 4456
cmlibGU= 4457
IHJlZ2lvbg== 4458
IGZyZXF1 4459
cm95 4460
IGhvd2V2ZXI= 4461
IHBhcnRuZXJz 4462
YW5j 4463
IG1pbmlt 4464
IGxhdA== 4465
IGZhbWlsaWVz 4466
IGV2aWRlbmNl 4467
IHB1bg== 4468
cmFmdA== 4469
IGxvc3M= 4470
IG1hcA== 4471
IGFueWJvZHk= 4472
IGNoYW5naW5n 4473
IHJ1bGVz 4474
IG9yZ2FuaXphdGlvbg== 4475
IGVzc2VudGlhbGx5 4476
IFJlZA== 4477
IGVsZW1lbnQ= 4478
5pc= 4479
IHZpcnQ= 4480
cmF0 4481
IHByaW50 4482
YW5kZXI= 4483
YXJlbg== 4484
ZW1vcw== 4485
zr/PhQ== 4486
IGNvbmRpdGlvbnM= 4487
YWJl 4488
IGRhbmNl 4489
0LjRgA== 4490
IGRvcw== 4491
0L7Rhw== 4492
IFF1ZQ== 4493
IHdhbGtpbmc= 4494
IHRybw== 4495
IGlk 4496
IGFkZGl0aW9uYWw= 4497
IGZ1bGx5 4498
IGZhbnM= 4499
IGFkZGl0aW9u 4500
IGxpa2Vk 4501
IMO8YmVy 4502
IGJvdw== 4503
ZGk= 4504
IG1hc3Rlcg== 4505
b2Zm 4506
KTo= 4507
bWJlcg== 4508
IOus 4509
5a8= 4510
5Yiw 4511
bGF1c2U= 4512
IG9kZXI= 4513
IHNhZmV0eQ== 4514
IHJlYWN0 4515
4K6/ 4516
YnQ= 4517
IGRpc2FwcA== 4518
IGdpcmxz 4519
U3Q= 4520
IEFuZw== 4521
IGZhaXRo 4522
IHR1cm5z 4523
IHRpZ2h0 4524
IG1vdXRo 4525
YW1p 4526
emVy 4527
IHdlYXA= 4528
INCx0YPQtA== 4529
IGhvc3BpdGFs 4530
cmFpZA== 4531
IG1pY3Jv 4532
IFN0YXRl 4533
IE1vc3Q= 4534
YWdu 4535
IGRlY2lkZQ== 4536
IHBhdGllbnQ= 4537
IGNvcm5lcg== 4538
IGRpZWQ= 4539
Tm8= 4540
IFN0dWQ= 4541
cmVuZA== 4542
ZW1wdA== 4543
IGxpZQ== 4544
IGxpZg== 4545
IEJlZm9yZQ== 4546
dMOz 4547
IFN1cGVy 4548
IGJlbGw= 4549
NjA= 4550
IHByaXZhdGU= 4551
IFBhdWw= 4552
IGdpYg== 4553
IGFncmU= 4554
tOyEnA== 4555
IHNpZw== 4556
IGludmVzdGln 4557
0Y/Rgg== 4558
ZW5pbmc= 4559
IGRpc3RhbmNl 4560
IHdhcm0= 4561
IGRpZ2l0YWw= 4562
5b6I 4563
aW5lcg== 4564
IHBhbmQ= 4565
IENPVklE 4566
0LPQvg== 4567
Z24= 4568
IHJhY2U= 4569
IHByb3Vk 4570
IHRlYWNoaW5n 4571
INGC0L4= 4572
7J6l 4573
IEFsbGFo 4574
SW4= 4575
IHdvb2Q= 4576
IGNvbG9ycw== 4577
IHdpcmQ= 4578
dWo= 4579
aWRhZA== 4580
IGN1c3RvbWVycw== 4581
IGNvbm5lY3RlZA== 4582
IGxheWVy 4583
IGFjaGlldmU= 4584
IHBlcnNwZWN0aXZl 4585
IENvbGw= 4586
2YI= 4587
IGNsb3Vk 4588
ISEh 4589
IGVuZGVk 4590
oIfqsow= 4591
IG1hbmFnZW1lbnQ= 4592
IHJpY2g= 4593
IHN1YnN0 4594
IHJlbW8= 4595
IHNlcnZl 4596
IHJlc2lzdA== 4597
IHRob3VnaHRz 4598
IGdyb3d0aA== 4599
aWxpYXI= 4600
IHJpZ2h0cw== 4601
IGNoYXJnZQ== 4602
IGNvbnNpc3Q= 4603
IHdlcmRlbg== 4604
IGVtYg== 4605
YW5kb20= 4606
IGh1cnQ= 4607
IGthbg== 4608
aWFz 4609
0LvQvg== 4610
IHNoaXQ= 4611
IGJlZw== 4612
IHJlY2VpdmVk 4613
aXRhdGlvbg== 4614
IG1lYXQ= 4615
IGlzc28= 4616
ZmZlZQ== 4617
IGZhbW91cw== 4618
IGNvbWZvcnRhYmxl 4619
SUw= 4620
IEJ5ZQ== 4621
6Kqq 4622
5YCR 4623
b3RoZXM= 4624
IG1lZGljYWw= 4625
IGVuam95ZWQ= 4626
IGhlYWx0aHk= 4627
IHd5 4628
Y2llcw== 4629
IGVmZm9ydA== 4630
IGRvY3Rvcg== 4631
IG1pbGl0YXJ5 4632
TEFV 4633
IGdybw== 4634
IGJhdHRsZQ== 4635
IGZlZA== 4636
IGNhcGFj 4637
IGFmcmFpZA== 4638
aXZpbA== 4639
INCy0YHQtQ== 4640
IGxlbmd0aA== 4641
eXNpcw== 4642
IGJlaQ== 4643
pO0= 4644
IG9yZ2FuaXo= 4645
b3Jn 4646
aW5j 4647
IGludGVyYWN0 4648
IENoaW5lc2U= 4649
IGFjY29yZGluZw== 4650
IGluY3JlZGlibGU= 4651
IGtpbGxlZA== 4652
IGRhdWdodGVy 4653
IM+A 4654
0YvQsg== 4655
IHNjaG9vbHM= 4656
IMKr 4657
bGxlcg== 4658
IHNob3VsZG4= 4659
bmFs 4660
IGNyaXM= 4661
IGNoaWNrZW4= 4662
IGZhc3Rlcg== 4663
IGV4dHJlbWVseQ== 4664
IG9wcG9z 4665
IG5vdXM= 4666
ICs= 4667
cmlh 4668
IGZpbmFuY2lhbA== 4669
IGV4Y2l0aW5n 4670
IGpvdXJuZXk= 4671
15nXnQ== 4672
oOs= 4673
IGRpc3BsYXk= 4674
IG1lbW9yeQ== 4675
IGhlYXZ5 4676
0L3QtQ== 4677
IHBhc3NlZA== 4678
0YDQuA== 4679
aWxlcw== 4680
IHBzeWNo 4681
IHNwZWNpZmljYWxseQ== 4682
IGVuZ2FnZQ== 4683
IGxlZA== 4684
b3JnZQ== 4685
IERlbQ== 4686
b3JkZXI= 4687
IDgw 4688
IGNyZWFt 4689
ZXN0ZXJkYXk= 4690
IGVkZ2U= 4691
INC/0L7Quw== 4692
IGJ1bGw= 4693
IGluZGlj 4694
IGt0w7M= 4695
IGhvcGVmdWxseQ== 4696
dW1lbnRz 4697
YWdlbg== 4698
0L3QvtCz0L4= 4699
IGhhdGU= 4700
Y2h0 4701
ODA= 4702
IGVmZmlj 4703
IOyngA== 4704
IGludGVybmV0 4705
IGJ1ZGdldA== 4706
IHByb3BlcnR5 4707
aWRheQ== 4708
IOya 4709
INC80L7Qtg== 4710
b2xh 4711
IHNob3dlZA== 4712
IE1vbg== 4713
IHRob3VzYW5k 4714
QVA= 4715
IHBvb3I= 4716
dXNlZA== 4717
IEphY2s= 4718
IHPDpQ== 4719
g70= 4720
IGVzYw== 4721
IHNvZnR3YXJl 4722
IHF1YXI= 4723
INio 4724
IG5lY2Vzc2FyaWx5 4725
b21lbg== 4726
aXk= 4727
IGV2ZW50dWFsbHk= 4728
aXNoZWQ= 4729
IGJyaWdodA== 4730
RUQ= 4731
IHNwbA== 4732
IGRlbWFuZA== 4733
IHRocmVhdA== 4734
IHNpcg== 4735
IHJlbGVhc2Vk 4736
Y2tldA== 4737
IOKAqw== 4738
IHJlcXVpcmVk 4739
IHZvdGU= 4740
7Lk= 4741
4K6k 4742
IGRldmVsb3BlZA== 4743
IOyCrA== 4744
YXRvcnk= 4745
IGRpcg== 4746
Y2FwZQ== 4747
IHNsaWdodGx5 4748
w6w= 4749
4LmJ 4750
cmVldA== 4751
IGRpc2Vhc2U= 4752
IGNvdXJ0 4753
IGl0ZW1z 4754
IEVhcnRo 4755
0YHRgtC4 4756
0LbQtQ== 4757
7LI= 4758
IGNoYWxsZW5nZXM= 4759
IEJyaXQ= 4760
IGRlc2lnbmVk 4761
MTI= 4762
IGhlYXJpbmc= 4763
IGxpc3RlbmluZw== 4764
em8= 4765
INGB0Ls= 4766
44Gn44GZ 4767
IHBlcm8= 4768
IHdlYXJpbmc= 4769
cGxpYw== 4770
IGNoZW0= 4771
IGJhbGFuY2U= 4772
IGJh 4773
IHJlY2VpdmU= 4774
aW1h 4775
IHNpZ25pZmljYW50 4776
INC80Ys= 4777
YW5jaA== 4778
IENy 4779
IENvdW4= 4780
6riI 4781
IGpvYnM= 4782
IG9mZmljaWFs 4783
IHBlcm0= 4784
b21z 4785
IG9wcG9ydHVuaXRpZXM= 4786
IG92ZXJhbGw= 4787
IGh1cw== 4788
b2Rlcw== 4789
IG5hdGlvbg== 4790
IFJlZw== 4791
IG9yZA== 4792
IHJlc3RhdXI= 4793
IOyG 4794
IG1lbA== 4795
dmlu 4796
IHdlbm4= 4797
IGvDtm4= 4798
5oM= 4799
IG9waW5pb24= 4800
44KC 4801
6Kw= 4802
IFNvbWV0aW1lcw== 4803
54I= 4804
0YnQtQ== 4805
YXNj 4806
T1U= 4807
IDIwMjA= 4808
IGRlbGljaW91cw== 4809
aWdlcg== 4810
IOyViA== 4811
b2xl 4812
IGhhbmRsZQ== 4813
IGNpdA== 4814
IO2VnA== 4815
IGbDtnI= 4816
b290aA== 4817
IG5lY2Vzc2FyeQ== 4818
IGluZGVwZW5k 4819
5oQ= 4820
aXN0ZW4= 4821
aGFt 4822
IMOpdA== 4823
44Oz 4824
IG11bHRp 4825
z4w= 4826
Pyk= 4827
IGNhbXB1cw== 4828
IHRvcGlj 4829
IHJhaW4= 4830
IHBhbmVs 4831
IFNhbQ== 4832
IGxhcmdlcg== 4833
YXVkaWVuY2U= 4834
IHBhaWQ= 4835
IGVjb25vbWlj 4836
b2x0 4837
IHN0cmVldA== 4838
IENvbnQ= 4839
IGRyaXZpbmc= 4840
IOyggA== 4841
IGhheQ== 4842
IHByb2Zlc3Npb25hbA== 4843
IEludGVybg== 4844
5bg= 4845
IGlucHV0 4846
IGNhdGVn 4847
IGNybw== 4848
IGxs 4849
RVQ= 4850
0YvQuQ== 4851
Kio= 4852
IFpl 4853
QkxF 4854
IOyk 4855
cmVlcw== 4856
INCv 4857
ZWRl 4858
aWVydA== 4859
IGZvbGQ= 4860
IGR1cg== 4861
IE5hdGlvbmFs 4862
IOyWtOs= 4863
YW5jZWQ= 4864
IGZhaXJl 4865
dXRlZA== 4866
IGtpbmc= 4867
IHdpbGQ= 4868
b2k= 4869
dXBiZWF0 4870
IHByZXZlbnQ= 4871
aXVz 4872
IMOo 4873
IHdpZGU= 4874
IHJpbmc= 4875
IHRpdGxl 4876
IHN0YW5kaW5n 4877
IGFsdGhvdWdo 4878
IGhp 4879
IHNhdWNl 4880
IHNpZGVz 4881
IGFuaW1hbHM= 4882
aWxpbmc= 4883
YXRpdmVz 4884
7JeQ7ISc 4885
IE92ZXI= 4886
IGRlc3A= 4887
IGNvbnNpZGVyZWQ= 4888
YXJpZXM= 4889
aWVycw== 4890
IGVpbmVu 4891
IHNpc3Rlcg== 4892
IOuV 4893
IFN1cmU= 4894
44KL 4895
cmllbmQ= 4896
YWlnbg== 4897
IHNob3du 4898
IHNhYw== 4899
IHNvbnQ= 4900
IGNlbnR1cnk= 4901
IHRpZW4= 4902
IM66 4903
IFNU 4904
5ZWK 4905
IG9sZGVy 4906
aWVt 4907
IHRydWx5 4908
IFNp 4909
IHdpbmRvdw== 4910
aXF1ZXM= 4911
YXJpbw== 4912
5rKS 4913
IGxvY2F0aW9u 4914
zro= 4915
IOyc 4916
dmk= 4917
YWd1ZQ== 4918
IFNvcnJ5 4919
IGRpc3A= 4920
IGhlbGw= 4921
IMOJ 4922
IHRyYWRl 4923
IGNyaXRpY2Fs 4924
IOqx 4925
IG5hbWVk 4926
IHByZXBhcmVk 4927
IEhvdXNl 4928
YWx1 4929
IHRvdWdo 4930
IHRyaXA= 4931
IHNhbmQ= 4932
Y2Vs 4933
w7x6 4934
IFB1dA== 4935
IGFwYXJ0 4936
aXNm 4937
dmlz 4938
IGxpYnI= 4939
YXZlbg== 4940
IHZpZQ== 4941
IGVmZmVjdGl2ZQ== 4942
4Liy 4943
IG1hZ24= 4944
IG11aXRv 4945
IOq1 4946
aGFs 4947
IGxpbWl0 4948
IG5pbmU= 4949
IHdpbGxpbmc= 4950
xLHFnw== 4951
c3A= 4952
0LXQsw== 4953
aGk= 4954
IGFsdA== 4955
IEphbg== 4956
IG9yaWdpbg== 4957
IFVz 4958
IGVsZW1lbnRz 4959
IHVzZXM= 4960
IGhlbHBmdWw= 4961
IGZsYXQ= 4962
IGZhbWlsaWFy 4963
IFBhcms= 4964
IGNvcmU= 4965
IGNsb3Nlcg== 4966
IGFjdGl2ZQ== 4967
IGFkbWluaXN0 4968
Q0U= 4969
0L3Ri9C1 4970
54Q= 4971
IHJlbGF0aXZl 4972
IG1lbnRhbA== 4973
IHJhbmRvbQ== 4974
IHBhcnRuZXI= 4975
IHV0aWw= 4976
cGhvbmU= 4977
IHJ1bGU= 4978
d3c= 4979
IOyglQ== 4980
IHNjaG9u 4981
IGNvZmZlZQ== 4982
SEE= 4983
IGNvbm5lY3Rpb24= 4984
IHVuaXQ= 4985
bGF1Z2hpbmc= 4986
bG9n 4987
IGFwcGw= 4988
0LvQsA== 4989
dXNpYw== 4990
IEJyYQ== 4991
IGFueXdoZXJl 4992
QVVESQ== 4993
IHNlcGFyYXRl 4994
Ym94 4995
IGRpdmlk 4996
IHRlc3Rpbmc= 4997
IHNpY2s= 4998
IHdlcmVu 4999
5LuW 5000
INec1w== 5001
IGFkdmFudGFnZQ== 5002
IHRyYW5zZmVy 5003
Jy4= 5004
IOu5 5005
IGZpbmRpbmc= 5006
0L3QvtC5 5007
IOyiiw== 5008
IGZvcnQ= 5009
IGVjb25vbXk= 5010
IGxhY2s= 5011
IGxlYXZpbmc= 5012
IGRpbQ== 5013
5Y4= 5014
IFJlcw== 5015
2K0= 5016
IGRpc2N1c3Npb24= 5017
0LXQvw== 5018
IGdlcw== 5019
ZHVjdA== 5020
IGNoYWlu 5021
IHVzZXJz 5022
ZWNo 5023
xYJh 5024
IGRpc2g= 5025
IGNhcmVmdWw= 5026
IHRlYWNoZXI= 5027
IG9wdGlt 5028
IGZsdQ== 5029
YXRpY2FsbHk= 5030
IHJlZmxlY3Q= 5031
IHRyZWF0bWVudA== 5032
ZWVk 5033
acSZ 5034
w7k= 5035
4K6+ 5036
IGVxdWlw 5037
IHBsYW5uaW5n 5038
IHNvbHZl 5039
44Gd 5040
IFRvbQ== 5041
IGF2b2lk 5042
IHBvdQ== 5043
IGdyZWF0ZXI= 5044
bGlu 5045
T0w= 5046
IEx1 5047
IE1vcmU= 5048
IGF0dHJhY3Q= 5049
w6pu 5050
dW5h 5051
IHBob3Rv 5052
ZXJhdGlvbg== 5053
IHBsYW5ldA== 5054
IGNvcHk= 5055
IHZpc3VhbA== 5056
aXJpbmc= 5057
IGludGVybmF0aW9uYWw= 5058
IGxhdWdoaW5n 5059
IHRoaWNr 5060
IGhvbGRpbmc= 5061
IGJyaW5naW5n 5062
IGxldHRlcg== 5063
IGJ1cm4= 5064
IGVmZmVjdHM= 5065
aXTDqQ== 5066
b3Vycw== 5067
T1Q= 5068
w6ptZQ== 5069
IFNjaG9vbA== 5070
15XXqg== 5071
cm9wcmk= 5072
bGln 5073
zrHOuQ== 5074
IGFkdWx0 5075
IHN1Z2Fy 5076
IHJpZGU= 5077
IGhpZ2hsaWdodA== 5078
IG5vYm9keQ== 5079
IDIx 5080
IGNoYXQ= 5081
INC/0YDQuA== 5082
IGlubm92 5083
dW5nZW4= 5084
IGF0dGFjaA== 5085
ZWRvbQ== 5086
5Yo= 5087
eWw= 5088
IGxlZ2Fs 5089
IHJpY2U= 5090
IGNvbGxhYm9y 5091
a2luZw== 5092
ZG93bg== 5093
5pk= 5094
44KK 5095
IGlo 5096
IEFj 5097
b3VzbHk= 5098
IHJhcA== 5099
IHNvbGlk 5100
IGdlbmVyYWxseQ== 5101
IHBhdHRlcm4= 5102
YWxp 5103
4Lit 5104
IHRyYW5zbA== 5105
aW50ZXI= 5106
YXVsdA== 5107
IOuo 5108
IGV4cHJlc3M= 5109
IGV4YW1wbGVz 5110
IGNob3Nl 5111
IHRlbGxz 5112
w61z 5113
YWludA== 5114
IFRlbGw= 5115
IE1pY2hhZWw= 5116
5qg= 5117
IE51bWJlcg== 5118
IHRhcA== 5119
IGV4cGVyaW1lbnQ= 5120
IGJlbmVmaXQ= 5121
IOyw 5122
IHNlcXU= 5123
IGV4cGVuc2l2ZQ== 5124
IGdlbmVyYXRpb24= 5125
IE1hbnk= 5126
IGFkZGluZw== 5127
IGtpbA== 5128
IGNhbXBhaWdu 5129
IEFudA== 5130
cmF3 5131
b21tZW4= 5132
IHNvdWw= 5133
am8= 5134
IEFjdHVhbGx5 5135
YW1t 5136
6rKg 5137
IG1heGlt 5138
IHNhbHQ= 5139
IGNydQ== 5140
IGNhbGxpbmc= 5141
44GM 5142
IGJhc2lz 5143
YmFu 5144
IGtlZXBpbmc= 5145
IE1vcg== 5146
ZWRz 5147
7IY= 5148
IHRvZG8= 5149
0LDQvNC4 5150
0L3Rjw== 5151
IGxpdmVk 5152
IER1 5153
44KJ 5154
5a62 5155
Zm9yY2U= 5156
5bm0 5157
ZmVyZW5jZQ== 5158
YWxh 5159
IG9jY3Vy 5160
c2s= 5161
IHJlY2VudA== 5162
IGNhcnM= 5163
IHRyYWRpdGlvbmFs 5164
ZW50bGU= 5165
sog= 5166
IGhlbGQ= 5167
IG5hY2g= 5168
IENlbnRlcg== 5169
ZXJlbg== 5170
IGJpbg== 5171
2YE= 5172
IGNvbW1l 5173
IHJldmU= 5174
IOyYpA== 5175
IGV4cGVjdGVk 5176
YWJpbA== 5177
IGZvY3VzZWQ= 5178
b3Y= 5179
IGlQ 5180
b3JpYWw= 5181
aXJv 5182
IGV0Yw== 5183
YW1pbmc= 5184
IFNvbg== 5185
IHllc3RlcmRheQ== 5186
IHN0cmF0ZQ== 5187
INGG 5188
IOuP 5189
cGVz 5190
IGFjdGl2aXR5 5191
IGFkdmljZQ== 5192
IG9wZW5pbmc= 5193
Zmlu 5194
IHJlbGE= 5195
6ZY= 5196
IGluc3RhbmNl 5197
IEV2ZXJ5b25l 5198
Ymw= 5199
cGVu 5200
IHZpc2lvbg== 5201
IEFsZXg= 5202
aWZvcm4= 5203
IHRpY2s= 5204
SGU= 5205
IHN0cmF0ZWd5 5206
IGtvbQ== 5207
UEU= 5208
IEds 5209
IGVsZWN0cmlj 5210
MTU= 5211
IGRhaWx5 5212
IGh1c2JhbmQ= 5213
IHN0YXRpb24= 5214
IGFuYWx5c2lz 5215
eW5hbQ== 5216
IGF0dGVtcHQ= 5217
IGJpbGxpb24= 5218
dmFudA== 5219
IGZvcnRo 5220
IG1hdGg= 5221
YWx5 5222
IGJlaGF2aW9y 5223
IE1hcw== 5224
a2Fu 5225
IERheQ== 5226
IGJsZXNz 5227
IGd1dA== 5228
IEhpZ2g= 5229
b3g= 5230
IGRyZXNz 5231
IGplZA== 5232
6K8= 5233
5ZY= 5234
IGV4cGVyaWVuY2Vz 5235
aXN0YQ== 5236
IGZpZ2h0aW5n 5237
5bc= 5238
INGB0Lo= 5239
IG1vc3RseQ== 5240
YXVzZQ== 5241
IHBpY3R1cmVz 5242
0LXQvdGC 5243
IG1hZA== 5244
IG1vZGVscw== 5245
0YjQtQ== 5246
IENvdW50 5247
xYQ= 5248
xYJv 5249
ZXB0 5250
T00= 5251
IEFO 5252
IHRyb3VibGU= 5253
NDA= 5254
IGJpcmQ= 5255
dWxhdGU= 5256
IG11cg== 5257
IHByb2R1Y2U= 5258
IG1hcnJpZWQ= 5259
Yml0 5260
IHRoZW9yeQ== 5261
7Zg= 5262
IGxlYWRlcg== 5263
IExhc3Q= 5264
QUE= 5265
6LU= 5266
IGltYWdlcw== 5267
IGV4cGFuZA== 5268
IFBvcg== 5269
IHB1cmNo 5270
IFNhbg== 5271
IENocmlzdG1hcw== 5272
IEF1c3RyYWw= 5273
IHdpZA== 5274
IE1pc3M= 5275
IGtub3dpbmc= 5276
IHpl 5277
c2hpcA== 5278
a3U= 5279
0YXQvtC0 5280
IEluc3RhZ3JhbQ== 5281
IEluZGlh 5282
IGVzdGE= 5283
IENhbGlmb3Ju 5284
IDcw 5285
IGRyYWc= 5286
IGJydXNo 5287
IG5hbWVz 5288
QW5k 5289
IHlv 5290
aWxsYQ== 5291
IHNjaGVk 5292
IGRlc3Ryb3k= 5293
eWVhcg== 5294
IHZhbW9z 5295
INmE 5296
w6dh 5297
IGZvcmdvdA== 5298
0LjQtQ== 5299
IHJhaXNl 5300
cmVtZQ== 5301
7ZW0 5302
IEdpdmU= 5303
IGNvbnRhaW4= 5304
cmFi 5305
IGdpZnQ= 5306
INGB0L8= 5307
IHJlcXVlc3Q= 5308
IHNodXQ= 5309
IGRlZ3JlZXM= 5310
IGJlbmVmaXRz 5311
0YvQtQ== 5312
IHN0dWRpZXM= 5313
IGVuZHM= 5314
IGV2ZXJ5d2hlcmU= 5315
IGhlcm8= 5316
b3Bo 5317
ZXJyeQ== 5318
IG1hdGVyaWFscw== 5319
ZW5lZA== 5320
TkE= 5321
5Y0= 5322
IG11eQ== 5323
IHdvcnNl 5324
5LuA 5325
IE1hZA== 5326
IGRlY2lzaW9ucw== 5327
aW9uZQ== 5328
IGZvcmVpZ24= 5329
bGF1Z2h0ZXI= 5330
aWJlcg== 5331
0LXQvdC40Y8= 5332
44WL 5333
IHJlYWxpemVk 5334
IGlnbg== 5335
IHdlYWs= 5336
IM68 5337
IHNjYXJlZA== 5338
IGFzc3Vt 5339
QUs= 5340
778= 5341
77+9 5342
IGNvdmVyZWQ= 5343
IFNhdA== 5344
INC+0L0= 5345
IGluZGl2aWR1YWxz 5346
IGNvbXBhcmVk 5347
MTE= 5348
IEFkZA== 5349
aWNsZXM= 5350
IGNlcnQ= 5351
cmFy 5352
IGJyaWVm 5353
IGFjdGl2aXRpZXM= 5354
IGZhYg== 5355
YmFy 5356
IGFzdA== 5357
IE90aGVy 5358
IGNsYXNzZXM= 5359
IG9n 5360
IG1pc3Npbmc= 5361
44Gg 5362
6Z0= 5363
d2Vycw== 5364
16k= 5365
IGludHJvZHVjZQ== 5366
IGVxdWF0aW9u 5367
44G+44GZ 5368
IG5vbQ== 5369
IHBhaW50aW5n 5370
dXNoaW5n 5371
IEFQ 5372
IGVuY291cmFnZQ== 5373
IHNoaXA= 5374
aXR0ZWU= 5375
aXZlcnNl 5376
b3Rh 5377
bmFt 5378
44O7 5379
IGV4ZXJjaXNl 5380
INCt 5381
IG5hcw== 5382
IHRob3VzYW5kcw== 5383
IENhbGlmb3JuaWE= 5384
IHNlcw== 5385
IHJvdw== 5386
nog= 5387
IHBhbmRlbWlj 5388
IHNraWxs 5389
YmVs 5390
IGRpcmVjdG9y 5391
IG1pbGs= 5392
IG51dA== 5393
IG1vdGlvbg== 5394
IGNsb3NlZA== 5395
6Kg= 5396
IGNyZWRpdA== 5397
YWhy 5398
IGNoZWVzZQ== 5399
IGFsdGVybg== 5400
aW1hdGVseQ== 5401
IHN1c3Q= 5402
IFRyYQ== 5403
IGdsYWQ= 5404
IGhpZ2hseQ== 5405
IHdh 5406
IHJlZHVjZQ== 5407
IGJsZQ== 5408
YWRvcg== 5409
aW5hdGVk 5410
aW9uZXM= 5411
Y2llbnQ= 5412
IGRlcGVuZGluZw== 5413
IHNoYXJpbmc= 5414
IGNhdWdodA== 5415
cmFlbA== 5416
IG1laHI= 5417
IHBhc3Npb24= 5418
55s= 5419
IHJ1 5420
IGZhcm0= 5421
VEk= 5422
YXZlcw== 5423
IFJvYg== 5424
IEJybw== 5425
IG1vdGl2 5426
cmV0Y2g= 5427
cnVwdA== 5428
IEJpZw== 5429
IGFsbGU= 5430
IGV0dA== 5431
dWJz 5432
IEphcGFuZXNl 5433
IEhhbGw= 5434
0LjQu9C4 5435
QVVESUJMRQ== 5436
56w= 5437
IGNlbGxz 5438
aWth 5439
ZWxpbmU= 5440
aWxlcg== 5441
IOyj 5442
IHNreQ== 5443
SU5BVURJQkxF 5444
ZW5kZQ== 5445
YXB0ZXI= 5446
IHBpbg== 5447
IGdhdGhlcg== 5448
aG9s 5449
bGVjdGlvbg== 5450
IHN5bg== 5451
IHBsdWc= 5452
cm91bmQ= 5453
IHVuaXZlcnNpdHk= 5454
aGli 5455
IGZhbnRhc3RpYw== 5456
a24= 5457
IGhvbGU= 5458
IFJlbWVtYmVy 5459
aW5jdA== 5460
YWtz 5461
Q0g= 5462
IGJyb2tlbg== 5463
IHN0cmF0ZWc= 5464
IGFsaXZl 5465
IHRhbms= 5466
IGNhcnQ= 5467
cmF0ZWQ= 5468
cmll 5469
IFN0ZXA= 5470
IEV2ZXJ5dGhpbmc= 5471
IGJvdW5k 5472
IHNvYnJl 5473
IGN1c3RvbWVy 5474
oYw= 5475
dXJn 5476
IEJpbGw= 5477
TGE= 5478
d2hhdA== 5479
IHJlYWN0aW9u 5480
IHNlc3Npb24= 5481
IHBsYW5z 5482
IOydtOugh+qyjA== 5483
IGRvd25sb2Fk 5484
7Jk= 5485
dWVy 5486
IGNhYg== 5487
IGluc3Ry 5488
aWZ5aW5n 5489
IE5pY2U= 5490
IHRlYW1z 5491
xLFs 5492
IGdvYWxz 5493
aXNjaA== 5494
IHRyYW5zcG9ydA== 5495
IGFuaW1hbA== 5496
IGNvc3Rz 5497
IGNhbGxz 5498
IHNlaHI= 5499
7Ig= 5500
cmlhbg== 5501
IGRpYWw= 5502
IHdlYXRoZXI= 5503
4LmA 5504
INCy0L7Rgg== 5505
IFBsYXk= 5506
IHNoYXJlZA== 5507
IHNtb290aA== 5508
YWJh 5509
IGxlYXZlcw== 5510
4K6p 5511
IGNvbmNlbnQ= 5512
IHNoaWZ0 5513
IOuQmA== 5514
IEdvdmVybg== 5515
IGRlbW9uc3Q= 5516
IGJ1dHRlcg== 5517
IOyXrA== 5518
IHNhdGlzZg== 5519
iOus 5520
IHJlY29nbml6ZQ== 5521
IEZyZW5jaA== 5522
IHZvbHVtZQ== 5523
w6RuZA== 5524
0YPQvA== 5525
IOynhA== 5526
IEtlZXA= 5527
b3dh 5528
aXBwZWQ= 5529
0YHRgtGA 5530
IGRldGVjdA== 5531
IM+D 5532
IGxpZnQ= 5533
IGNsb3RoZXM= 5534
IFN0b3A= 5535
w7U= 5536
bWV0 5537
IGNsaW4= 5538
IGFycg== 5539
ZnJpZW5k 5540
IHN0dWNr 5541
WWU= 5542
aGFuZA== 5543
dW1h 5544
IHNjcmk= 5545
IGZ1Y2tpbmc= 5546
Y3RvcnM= 5547
16o= 5548
IGpvaW5pbmc= 5549
IGNldHRl 5550
INij 5551
IFdoaXRl 5552
IGlocg== 5553
zq0= 5554
44Gt 5555
IGluY2x1ZGVk 5556
ZXNzbw== 5557
IGFjYWQ= 5558
YnVt 5559
IHNhYg== 5560
INC00LvRjw== 5561
6L+Z 5562
dWZhY3Q= 5563
IFJlcHVibGlj 5564
cmlt 5565
IHllbGxvdw== 5566
IGxpbWl0ZWQ= 5567
VEVS 5568
IFR5 5569
IG5vdGVz 5570
dmVzdA== 5571
0LjQtw== 5572
YWxlZA== 5573
IHBoYXNl 5574
YW5kYQ== 5575
IE1vbQ== 5576
Ukk= 5577
IGltbWVy 5578
bWFs 5579
IGluag== 5580
IHlhbmc= 5581
dWRpYmxl 5582
0LDQsw== 5583
IHNldHQ= 5584
IG1hZ2lj 5585
IGVuc3VyZQ== 5586
IHNwcmluZw== 5587
IHNob2Nr 5588
IHdoZWVs 5589
0L7Qs9C00LA= 5590
44KI 5591
IGNhbmNlcg== 5592
IHJvb3Q= 5593
0JA= 5594
Z2VuY3k= 5595
IOuN 5596
aWk= 5597
IG91dHB1dA== 5598
IGNvbW1pdA== 5599
IHdvcmtlcnM= 5600
7JWE7JqU 5601
INGB0LDQvA== 5602
dmV5 5603
IHBldQ== 5604
IGNpdmls 5605
aXNj 5606
IGJyaW5ncw== 5607
0YDQsNCy 5608
YW5pYQ== 5609
xIE= 5610
Y3JhZnQ= 5611
bWJvbA== 5612
IGludGVsbGln 5613
Ymk= 5614
YWNpbmc= 5615
eW91 5616
IGJlY29taW5n 5617
IERlcg== 5618
ZW1h 5619
5bCx5piv 5620
IGluZ3JlZA== 5621
IGNvbW1hbmQ= 5622
IHVwZGF0ZQ== 5623
IHByZW0= 5624
IG9wZW5lZA== 5625
hKQ= 5626
0LXQvdC40LU= 5627
IGdhcmQ= 5628
IHN0YXRlbWVudA== 5629
IHNjcmV3 5630
IHByb3Rl 5631
IGNhcmRz 5632
IHRhc2s= 5633
IGV2ZW5pbmc= 5634
IHN0aXRjaA== 5635
aW5lbg== 5636
IEJlcg== 5637
bWFyaw== 5638
IERhZA== 5639
INC10YHRgtGM 5640
INee1w== 5641
7JeI 5642
IGJhbg== 5643
IGNsaW0= 5644
IGZyZWVkb20= 5645
IG5vcm1hbGx5 5646
0LXRgdGM 5647
5aY= 5648
IHByb3ZpZGVk 5649
IOyekA== 5650
IOyVhOuLiA== 5651
IEtpbQ== 5652
aWVkZXI= 5653
7J2M 5654
IGNpdGl6 5655
IGJpa2U= 5656
IGJhaw== 5657
IG5vaXNl 5658
IGNsaW1hdGU= 5659
aXplcw== 5660
5b6M 5661
IGluY3JlYXNpbmc= 5662
IFRIRQ== 5663
IGxpcXU= 5664
IHBlcnNvbmFsbHk= 5665
ZWY= 5666
cmVzcA== 5667
IGxlZ3M= 5668
aW5kZXI= 5669
IHBlZA== 5670
IOunjg== 5671
IGRlcGVuZA== 5672
IHZhcmlldHk= 5673
IElzcmFlbA== 5674
IHdhc2g= 5675
5YY= 5676
IHF1aWV0 5677
IEphbWVz 5678
IEpldw== 5679
IGZvcmV2ZXI= 5680
IEludA== 5681
IGNvdW50ZXI= 5682
dXJhbmNl 5683
IEFueXdheQ== 5684
Y2FyZQ== 5685
IE9ubHk= 5686
Y2nDs24= 5687
YWRp 5688
IEV2 5689
64uI6rmM 5690
IM6x 5691
IHNsb3dseQ== 5692
INC+0LQ= 5693
IG5vdGljZWQ= 5694
aWVyZW4= 5695
IGZlbGw= 5696
INCR 5697
IG3Dqm1l 5698
IHdoZW5ldmVy 5699
ISk= 5700
IEh5 5701
5bw= 5702
b3Jkcw== 5703
dXNpb24= 5704
IFN0YXI= 5705
IO2Y 5706
IE1hYw== 5707
5LiK 5708
aXZlbg== 5709
IOyLnA== 5710
IOyXhg== 5711
IFR1cg== 5712
IGdlcg== 5713
cmlz 5714
IHZleg== 5715
INC70Y4= 5716
IHZlcnN1cw== 5717
2KfY 5718
b2NvbGF0ZQ== 5719
IHBsYW5l 5720
IHpv 5721
IHN1aXQ= 5722
VGhpcw== 5723
IG5lcnY= 5724
IEFjYw== 5725
0YPQtg== 5726
7IKs 5727
bmg= 5728
ZW1l 5729
IGF1c3M= 5730
IG1lYXM= 5731
IHRyw6hz 5732
z4k= 5733
0YHQu9C4 5734
IEFydA== 5735
IFNlY29uZA== 5736
0L7Qu9GM0LrQvg== 5737
Y2hv 5738
aXRlY3Q= 5739
0LXRgdGC 5740
IGJvc3M= 5741
IGluY29tZQ== 5742
oKQ= 5743
IHNoYWQ= 5744
IGFwcHJvcHJp 5745
IE1hbA== 5746
b3B0 5747
IGFydGlzdA== 5748
IHBsYXlz 5749
b3RoZXJz 5750
IEludGVy 5751
IHZpcnVz 5752
IGh1bmc= 5753
IGNvbnN0YW50 5754
IHNjcmlwdA== 5755
IHNub3c= 5756
dWxm 5757
a2V0 5758
IGRldmljZXM= 5759
IG1ldGFs 5760
aWdodHM= 5761
7IS4 5762
IHNhbGVz 5763
IHZlZ2V0 5764
IGNvbGxlY3Rpb24= 5765
IHZpYQ== 5766
a2Vy 5767
IGdvdHRlbg== 5768
T1c= 5769
acOpbg== 5770
IGFjY3Vy 5771
IHdhdmU= 5772
dWx0eQ== 5773
IEFpcg== 5774
IGxlYWRpbmc= 5775
aWNpbmc= 5776
IGNlbnRyYWw= 5777
IENocmlzdGlhbg== 5778
ZnI= 5779
IEFsdGhvdWdo 5780
IHNvbmdz 5781
IGZpZg== 5782
0L3Ri9GF 5783
IGJlbG9uZw== 5784
b3NzaWJsZQ== 5785
7LA= 5786
IHBob3Rvcw== 5787
aXNs 5788
IHJlbGF4 5789
c2E= 5790
VVNJQw== 5791
6rc= 5792
IG1hbnVmYWN0 5793
IFR3aXR0ZXI= 5794
IGRhbmdlcm91cw== 5795
IGh5ZA== 5796
bGVhcg== 5797
aWFudA== 5798
IOKApg== 5799
IHN1ZGRlbmx5 5800
IGxhdWdo 5801
IGFuZ2xl 5802
IEdvdA== 5803
IHdvcnJpZWQ= 5804
0L7QtQ== 5805
IHBhcA== 5806
IE1hcnQ= 5807
ZW5v 5808
IGJhdHRlcnk= 5809
INC/0L7RgQ== 5810
IGxpZ2h0cw== 5811
IGFybXM= 5812
IEFicw== 5813
bWVz 5814
4oCT 5815
dXNldW0= 5816
IHRlYQ== 5817
IE1pYw== 5818
IGZvcm1lcg== 5819
b2dyYXBoeQ== 5820
IGFwcGxpY2F0aW9ucw== 5821
IERpcmU= 5822
54S2 5823
IGZlZWRiYWNr 5824
aXRjaGVu 5825
eW9ydW0= 5826
dWVk 5827
aWd0 5828
xrDhuw== 5829
b3NpdGlvbg== 5830
IERlbA== 5831
IO2VmOs= 5832
IEJhY2s= 5833
YWRz 5834
IHByaW1l 5835
7KO8 5836
7KOg 5837
15E= 5838
IG11dA== 5839
XS4= 5840
INCX 5841
bG9j 5842
a2lu 5843
IGV4cGVydA== 5844
IGFscmlnaHQ= 5845
dW5ncw== 5846
IHN1cHBseQ== 5847
IGxlYWRlcnNoaXA= 5848
IEZyYQ== 5849
IHR5cGljYWxseQ== 5850
IHNlbA== 5851
IHRyZWVz 5852
IDIy 5853
aGFy 5854
IHdvcnN0 5855
IGJ1c3k= 5856
YW50bw== 5857
IFVw 5858
IEJhcw== 5859
IHByZXNlbnRhdGlvbg== 5860
IHN0cmFuZ2U= 5861
IHRoaW4= 5862
0YLQtQ== 5863
IHZlaGljbGU= 5864
INC00L4= 5865
Y2VsbGVudA== 5866
NzA= 5867
IHRpcmVk 5868
IGNyaXNpcw== 5869
IHRpbnk= 5870
YXN5 5871
IHJhbg== 5872
6Yc= 5873
IGZvcmNlcw== 5874
INC+0Yc= 5875
IGlkZW50aWZ5 5876
IGFzc2Vzcw== 5877
0LjRgtC1 5878
U0U= 5879
IGNyZWF0aXZl 5880
558= 5881
IGRlcGFydG1lbnQ= 5882
IGluaXRpYWw= 5883
5oiR5YCR 5884
IERhbQ== 5885
YWt0 5886
dmVyZQ== 5887
IGluZmVjdA== 5888
IHB1bXA= 5889
4bqh 5890
IHZpZWw= 5891
IHJhcmU= 5892
IGRvdA== 5893
YXNoaW9u 5894
ZW1wbA== 5895
IGZsZXg= 5896
IGtvbg== 5897
IHRydWNr 5898
IGxlY3Q= 5899
IHBsYXN0aWM= 5900
bGF3 5901
IGxpa2Vz 5902
IHJvdWdo 5903
IE1BVA== 5904
7Z6I 5905
IGNvbW1lcg== 5906
IGFzc2U= 5907
IGNha2U= 5908
IGFjdGlvbnM= 5909
IGFkbQ== 5910
IG90aGVyd2lzZQ== 5911
IEhlYWx0aA== 5912
IGNvbGxl 5913
4LmA4Lg= 5914
IHJ1Yg== 5915
5b6X 5916
5pQ= 5917
IHNjcg== 5918
IHp1bQ== 5919
IEhpbQ== 5920
IGNoYW1w 5921
IGNvbmNlcm5lZA== 5922
IDUwMA== 5923
IHBsYXRl 5924
IE91dA== 5925
IGRvbmM= 5926
IGVxdWlwbWVudA== 5927
IHRhdWdodA== 5928
bGxlZA== 5929
IO2Z 5930
aXZh 5931
IG1vdG9y 5932
wrs= 5933
IGd1aWRl 5934
5Yk= 5935
IHN0b3BwZWQ= 5936
IHJhdA== 5937
IGxhYm9y 5938
IGFpbQ== 5939
IHByZXBhcmU= 5940
INGI 5941
IHNob290aW5n 5942
YW5uZWQ= 5943
Y3JpcHQ= 5944
IGVuZW15 5945
IGRlcGVuZHM= 5946
IG5hdg== 5947
IGJlcg== 5948
IGxhbmRz 5949
IHVuaXZlcnM= 5950
aXU= 5951
IGZhY3Rvcg== 5952
b2tpbmc= 5953
IGNhcmJvbg== 5954
YnV0 5955
IExvdmU= 5956
ZWxk 5957
IM61 5958
IGdh 5959
IMOpcw== 5960
IGJyZWFk 5961
IHZvbHQ= 5962
7Yo= 5963
IHdhc3Rl 5964
IGtlZXBz 5965
5omA 5966
IHN0b3I= 5967
IGhvbm9y 5968
IHVubGVzcw== 5969
IGNvbHVt 5970
IOuMgA== 5971
IHBsYW50cw== 5972
WWVhaA== 5973
IGluY2x1ZGVz 5974
5Lit 5975
IG94 5976
IHBldXQ= 5977
66eM 5978
7IOB 5979
aXN0cnk= 5980
4Lix 5981
IERlcGFydG1lbnQ= 5982
YW50YQ== 5983
IGZpbmdlcg== 5984
IHN0cmV0Y2g= 5985
IHN5bWJvbA== 5986
IG5laWdoYm9y 5987
5qw= 5988
6rCE 5989
fn4= 5990
INGC0Ys= 5991
IEFiZXI= 5992
a2Vz 5993
IG1hc3NpdmU= 5994
IENI 5995
IFNhbA== 5996
16A= 5997
44KS 5998
IGR5bmFt 5999
YWNoZQ== 6000
IFByZQ== 6001
IG1vbml0b3I= 6002
ZW50ZWQ= 6003
RU8= 6004
IHJhaXNlZA== 6005
aXN0aWNz 6006
2qk= 6007
IHZvdQ== 6008
aXRlbg== 6009
obA= 6010
IGJ1c2luZXNzZXM= 6011
IGVhcm4= 6012
IG1vYmlsZQ== 6013
aWRhZGU= 6014
IGhhYmU= 6015
eXI= 6016
bGljdA== 6017
IGNvbmR1Y3Q= 6018
IGZlZGVyYWw= 6019
IHdv 6020
YnU= 6021
IG5vbmU= 6022
IHRlYWNoZXJz 6023
INin2YTY 6024
6YGT 6025
aWRlbnRz 6026
2KfZhA== 6027
IHRyZW5k 6028
0LXQtg== 6029
IGFsYnVt 6030
IG1pY2g= 6031
YmFzZWQ= 6032
4Li1 6033
IHRyYW5zaXRpb24= 6034
INC90L4= 6035
w7Vlcw== 6036
aG9zdA== 6037
ZWR5 6038
IFByb2Y= 6039
cGFu 6040
aWpu 6041
IGNhcGFjaXR5 6042
dW5kbw== 6043
INeR1w== 6044
IGJyZWF0aA== 6045
INC80LXQvQ== 6046
IG3DvA== 6047
7Zk= 6048
IEF1dA== 6049
aGluZ3Rvbg== 6050
IG5vcg== 6051
IGdhaW4= 6052
cG9pbnQ= 6053
WWVz 6054
INiq 6055
IE5h 6056
w6Vy 6057
IGnDpw== 6058
IE1hcnk= 6059
IHNwaW4= 6060
IGFudGk= 6061
5ZCn 6062
IHNvbWVob3c= 6063
IGxhd3M= 6064
IG1vbWVudHM= 6065
IGdyZQ== 6066
IG1vdmVz 6067
IFdvdWxk 6068
IHByZWRpY3Q= 6069
IHZyYQ== 6070
IDIwMTk= 6071
toQ= 6072
IGZ1bmRhbWVudA== 6073
MjU= 6074
IHB1cmU= 6075
IHdvdw== 6076
IGlzbGFuZA== 6077
IGludmVzdG1lbnQ= 6078
IGJhdGg= 6079
IFlh 6080
IGhhcmRlcg== 6081
IHRpcHM= 6082
5Zc= 6083
IGVsZWN0cm9u 6084
IEJvYg== 6085
IGJvbmQ= 6086
b2RpZXM= 6087
IEF1Zw== 6088
IGdpYnQ= 6089
IGNoYWly 6090
IHR3aWNl 6091
d29vZA== 6092
IGNsYXI= 6093
IG1hc2s= 6094
IGhvbmVzdGx5 6095
IDIwMTg= 6096
dGllcw== 6097
Jyw= 6098
IHBlbnM= 6099
IHN1cnByaXNlZA== 6100
IGNvbW11bmljYXRpb24= 6101
44Gj44Gm 6102
IHNwcg== 6103
IHdob3Nl 6104
IHN0YXJz 6105
15DX 6106
IOKAiw== 6107
IHByb3Blcmx5 6108
IGdyZXc= 6109
b3Npbmc= 6110
IGRpdmVycw== 6111
QUQ= 6112
IGVtcHQ= 6113
IGV4cHJlc3Npb24= 6114
4bq/ 6115
IFBhbA== 6116
44GK 6117
IGp1c3RpY2U= 6118
IHBhaXI= 6119
d28= 6120
IHNlYXQ= 6121
b3J0ZXI= 6122
IGxpbmtz 6123
IE1lcg== 6124
IHJlbmQ= 6125
0L3QvtC1 6126
dXBpZA== 6127
IEhlbA== 6128
IE1hcmNo 6129
IExv 6130
0YHRjA== 6131
IGhhc24= 6132
IGV2YWx1 6133
44GP 6134
5aSp 6135
aWxvcw== 6136
IGZ1bmRpbmc= 6137
IHZlbg== 6138
dWFu 6139
IE1hc3Rlcg== 6140
IE9s 6141
IEZyZQ== 6142
IHlhcA== 6143
IFNpcg== 6144
c2No 6145
IG1pc3Rha2U= 6146
YW1hbg== 6147
IGRpbm5lcg== 6148
IFdhc2hpbmd0b24= 6149
IG9yZ2FuaXphdGlvbnM= 6150
INC20LU= 6151
YXZpbmc= 6152
IHbDrQ== 6153
IGJpcnRoZGF5 6154
IGJlYXI= 6155
INmB 6156
IGFmZm9yZA== 6157
IHJldmVu 6158
IHJlbGF0aW9uc2hpcHM= 6159
cm91Z2g= 6160
IFRpbWU= 6161
IHRhZw== 6162
IFN1bg== 6163
dWFyeQ== 6164
IFBv 6165
Y2Fy 6166
YWJpbGl0aWVz 6167
IHByaXNvbg== 6168
IGxpYw== 6169
7KCV 6170
aWRkZW4= 6171
IHNwZWNpZXM= 6172
6bs= 6173
IGZpcm0= 6174
IHNjb3Jl 6175
IGRpdA== 6176
IHNwZWN0 6177
IHBlbA== 6178
IGNvbXBsaWNhdGVk 6179
5qij 6180
IHJhbms= 6181
IG9wcG9zaXRl 6182
IHBpY2tlZA== 6183
INC60L7QvQ== 6184
ZWxlcg== 6185
IG1pZw== 6186
IFNs 6187
IE5ldA== 6188
IG5lY2s= 6189
IEZyYW5jZQ== 6190
IHRlY2huaWNhbA== 6191
4Lih 6192
IG1pbGVz 6193
IHByaW1hcnk= 6194
IHNlaW4= 6195
c2Vz 6196
IGxhdWdocw== 6197
YnJh 6198
xZtjaQ== 6199
cmlhZ2U= 6200
IG5pYw== 6201
ZXRlcnM= 6202
IMOq 6203
b2xvZ2llcw== 6204
IElT 6205
cmFk 6206
dWRv 6207
xLFuZA== 6208
bWFy 6209
IGV4Y2g= 6210
IGNvbXBldGl0aW9u 6211
IGF1c3Np 6212
IFNlcnY= 6213
IHJlbnQ= 6214
IGNob2NvbGF0ZQ== 6215
IHdpZWRlcg== 6216
IG5lYXJseQ== 6217
IHNwZWVjaA== 6218
IHVuYw== 6219
IHBhcmFt 6220
IEJyaXRpc2g= 6221
IHJlbWFpbg== 6222
4LiB 6223
dXJ0 6224
INi5 6225
IGNyYWNr 6226
YWlscw== 6227
IHByb21pc2U= 6228
IHBheWluZw== 6229
acOf 6230
IGFkYXB0 6231
0LDQu9Cw 6232
IG1vdmllcw== 6233
IHdpcmU= 6234
n6w= 6235
5pyD 6236
IHRlcnJpYmxl 6237
IHPDsw== 6238
IHBlcmZlY3RseQ== 6239
5ZGi 6240
b3JkaW4= 6241
IGrDoQ== 6242
IGltcG9zc2libGU= 6243
IFRocmVl 6244
IG5o 6245
IHR1cm5pbmc= 6246
cnVt 6247
IEJlbA== 6248
aWdn 6249
IHJlc3BvbnNpYmxl 6250
0LjQuQ== 6251
IGluY3JlZGlibHk= 6252
d2k= 6253
aWFubw== 6254
IGh1bWFucw== 6255
IMOH 6256
IHNldHRpbmdz 6257
IGpveQ== 6258
b290 6259
IGRlYWxpbmc= 6260
aWxsZWQ= 6261
IHN1cnJvdW5k 6262
IGZvbGxvd2Vk 6263
IHBvc3NpYmx5 6264
IGluaXRp 6265
c3Rlbg== 6266
IHByb3M= 6267
IGNhbmRpZA== 6268
IGFzc2lnbg== 6269
IHZpb2xlbmNl 6270
V2VsbA== 6271
IHJpc2U= 6272
UFM= 6273
IHRhbWLDqW0= 6274
IOuTpA== 6275
aWFuY2U= 6276
eWFu 6277
IGF1ZGlv 6278
IEJldA== 6279
IEFtZXJpY2Fucw== 6280
IEFzcw== 6281
aXNjaGVu 6282
7J6F 6283
IHVsdGltYXRlbHk= 6284
IHBvbGlj 6285
IG1ham9yaXR5 6286
6YCZ5YCL 6287
IEZpbmFsbHk= 6288
ZXJhcA== 6289
IGd1YXJk 6290
IE1BVFQ= 6291
IGJyb3du 6292
0LzQuA== 6293
IGNoYQ== 6294
IEhvbHk= 6295
IG5lcnZvdXM= 6296
aXBwaW5n 6297
xJlk 6298
IFNh 6299
k5zr 6300
toA= 6301
bGll 6302
55yf 6303
IG51Yw== 6304
IEFwcg== 6305
6Zs= 6306
IEtvcmVh 6307
ZWdv 6308
IENhbmFkYQ== 6309
IGvDtm5uZW4= 6310
IGNvbXBhcg== 6311
IGdhbno= 6312
IE1haXM= 6313
IHRoZW1l 6314
IGtp 6315
IGRyYXdpbmc= 6316
YXpvbg== 6317
IE9mZg== 6318
dHQ= 6319
IFdpbmQ= 6320
IHRvZG9z 6321
IG9idmlvdXM= 6322
0L3QsNGP 6323
SU0= 6324
INCg 6325
d2VsbA== 6326
IGJsb3c= 6327
IGhvb2s= 6328
IGNpcmNsZQ== 6329
IOuztA== 6330
IGFyY2hpdGVjdA== 6331
IEty 6332
IGPDsw== 6333
IHByb3RlY3Rpb24= 6334
ZWdh 6335
5Yc= 6336
IHdhdGNoZWQ= 6337
IGFuc3dlcnM= 6338
IGRpZXQ= 6339
aXZv 6340
IHBvd2Rlcg== 6341
IHlvdXJz 6342
IGhpZ2hlc3Q= 6343
54K6 6344
RkY= 6345
5bo= 6346
IGJveXM= 6347
w7Z5bGU= 6348
IGx1bmNo 6349
6Kyd 6350
IElJ 6351
IHNldHM= 6352
IG1vbGU= 6353
24E= 6354
IHdpbnRlcg== 6355
IGx1Y2t5 6356
IHJlc3BvbnNpYmlsaXR5 6357
IHNpZ25hbA== 6358
IHdvbmRlcmluZw== 6359
IGF4 6360
IGNvb2tpbmc= 6361
0L7QstC+0YA= 6362
bGVn 6363
INC/0L7Rgg== 6364
IHN1cnByaXNl 6365
IGRlbW9jcg== 6366
IGxvb3A= 6367
IGphZw== 6368
IGN1cmlvdXM= 6369
IG1hcmtldGluZw== 6370
0J0= 6371
YXJvbg== 6372
IEFwcGxl 6373
IHZpcnR1YWw= 6374
IDE5OA== 6375
bm9vbg== 6376
IE1ldA== 6377
0L7RgdGC0L4= 6378
0L7QsdGL 6379
aXR1 6380
IEF3 6381
IGJ1eWluZw== 6382
IHJlc3RhdXJhbnQ= 6383
IEJ1ZA== 6384
IGRvdWJ0 6385
IGdyYW50 6386
IHZlcmQ= 6387
IGNhc2g= 6388
IGZhY3VsdHk= 6389
VGhhdA== 6390
IEVpbg== 6391
5aSa 6392
IHdlZA== 6393
aXRuZXNz 6394
IE1hZw== 6395
bmVs 6396
IG5hcnI= 6397
IGFjY2lkZW50 6398
IG1lZGl1bQ== 6399
ZW1lbnRz 6400
IGNyb3c= 6401
bmlnaHQ= 6402
7J28 6403
5Lmf 6404
IGxpYnJhcnk= 6405
0LDRjtGC 6406
IHRhbWJpw6lu 6407
IHJlZmVyZW5jZQ== 6408
IGZvdXJ0aA== 6409
aG91c2U= 6410
dmVudGlvbg== 6411
IGZpbGxlZA== 6412
IENvdXI= 6413
aWJy 6414
IG5n 6415
IGRldmVsb3Bpbmc= 6416
IHByb3ZpZGVz 6417
IHBvbGw= 6418
IHRyYWZmaWM= 6419
YXJlbnRseQ== 6420
4K6f 6421
IGZvcm1z 6422
IGNsaWVudA== 6423
IGdlbnRsZQ== 6424
IG11c3M= 6425
IENvbmdyZXNz 6426
IEluZGlhbg== 6427
Y2Vhbg== 6428
IHBpbA== 6429
IGN6eQ== 6430
c3Rvb2Q= 6431
dXR5 6432
IG7DpA== 6433
IHNwZW5kaW5n 6434
IGNvbnN0cnVjdGlvbg== 6435
aW5hdWRpYmxl 6436
IOuniA== 6437
iOustA== 6438
IOyDnQ== 6439
b21h 6440
b3Nlbg== 6441
YWdv 6442
IGxhcmdlc3Q= 6443
44WL44WL 6444
IHVuaXZlcnNl 6445
YmVz 6446
b3Nh 6447
INC10LPQvg== 6448
IGR1ZGU= 6449
IE1BUg== 6450
IGluZGVlZA== 6451
zrXOuQ== 6452
IG1hbmFnZWQ= 6453
IFNob3VsZA== 6454
U28= 6455
IGFwcGxpZWQ= 6456
IGZhaXJseQ== 6457
IERlbg== 6458
IGFuYWx5 6459
IGNvbnN0YW50bHk= 6460
0YHQvw== 6461
SG93 6462
IFNheQ== 6463
ZW5jaWVz 6464
IFBD 6465
IGVnZ3M= 6466
4K6w 6467
IGV0aA== 6468
IEVudMOjbw== 6469
aW5hcg== 6470
aW90 6471
IGN6 6472
IEV1cm9wZWFu 6473
44GI 6474
IEFN 6475
IGPDoQ== 6476
IHJhZGlv 6477
p4w= 6478
IGhpZGU= 6479
5LuK 6480
IFN0YXJ0 6481
IGNsdWI= 6482
IEhvcGU= 6483
IGVmZm9ydHM= 6484
bHVzaW9u 6485
IGNpdGllcw== 6486
aG9uZQ== 6487
IHJlYWNoZWQ= 6488
IGd1aWQ= 6489
cm9pZA== 6490
IGhhcm0= 6491
IGN1dHRpbmc= 6492
IGJ1bA== 6493
MTg= 6494
aWVzdA== 6495
IE1leA== 6496
IGlyb24= 6497
55+l 6498
IGFmdGVybm9vbg== 6499
IGhhbGw= 6500
IHByenk= 6501
IGdvc2g= 6502
IGluZmx1ZW5jZQ== 6503
INCy0LjQtA== 6504
IGluY3JlYXNlZA== 6505
IE1pbmlzdGVy 6506
IGRpc2Np 6507
IFBldGVy 6508
IHZlcnQ= 6509
IG1lbnU= 6510
IHNlbGxpbmc= 6511
dXJhbGx5 6512
IHF1b3Rl 6513
IMKh 6514
IGNvbnRpbnVlcw== 6515
bXByZQ== 6516
IMWfZXk= 6517
aXR1dGlvbg== 6518
INC90LDRgQ== 6519
Y2xlcw== 6520
IEdlcm1hbg== 6521
Y3p5 6522
INCj 6523
QmU= 6524
IGtpdGNoZW4= 6525
IFRyeQ== 6526
aXBl 6527
IGljb24= 6528
YXJw 6529
IHByb3ZpZGluZw== 6530
IFRyYW5z 6531
IHRlY2huaXF1ZQ== 6532
IGjDpHI= 6533
IGluZnJhc3Q= 6534
IHN1c3A= 6535
w7xjaw== 6536
aWNpcA== 6537
INCV 6538
IGNpbg== 6539
7Ja06w== 6540
IHByeg== 6541
IGNvbXBvbmVudA== 6542
IGJ5ZQ== 6543
IEJpYmxl 6544
aXplcg== 6545
Q2g= 6546
IHNvbHV0aW9ucw== 6547
IGFjY29tcGw= 6548
IDIwMTY= 6549
SUU= 6550
IFRh 6551
IGFzc3VtZQ== 6552
IGxpcXVpZA== 6553
IOuouQ== 6554
IHF1YXJ0ZXI= 6555
IGZlbWFsZQ== 6556
IFRoaW5r 6557
IHN0YXR1cw== 6558
aXR1dGU= 6559
IGNvYWNo 6560
IHJlaW4= 6561
IGNvbWJpbmF0aW9u 6562
6Lc= 6563
IFRlcg== 6564
IG9iamVjdHM= 6565
IGRpc3RyaWN0 6566
IG1ha2V1cA== 6567
IG11cmRlcg== 6568
d2Fz 6569
ZmVu 6570
IGJvd2w= 6571
IHB1Ymxpc2hlZA== 6572
IHNwb3J0cw== 6573
44Gh 6574
IGlkZW50aXR5 6575
IHNlZW1lZA== 6576
IGFjdGluZw== 6577
0LvRjg== 6578
cml4 6579
IHVwbG9hZA== 6580
IGhhc3Q= 6581
IGJvYXQ= 6582
IE1vZA== 6583
cmlv 6584
ID0= 6585
IGN5Y2xl 6586
r7g= 6587
IGxvdWQ= 6588
dXN0ZWQ= 6589
Y29taW5n 6590
IDIwMTc= 6591
IG9udA== 6592
IGxlZ2lzbA== 6593
IHN0cnVjdA== 6594
IFNvbWV0aGluZw== 6595
IGNvbmZsaWN0 6596
IHVwcGVy 6597
IG1hbmFnZXI= 6598
IG1vcnQ= 6599
IGZyYQ== 6600
IMSw 6601
IE1pa2U= 6602
IFdvcms= 6603
IG7Dsw== 6604
cGhlcmU= 6605
IOyCrOs= 6606
IExhbmQ= 6607
IGZpbHRlcg== 6608
IHByb21vdA== 6609
5rA= 6610
5pmC 6611
lbw= 6612
IHJlY29yZGluZw== 6613
150= 6614
IGFzc29jaWF0ZWQ= 6615
IGZ1ZWw= 6616
dW5kZXI= 6617
IGVsZWN0aW9u 6618
IGVtcGxveWVlcw== 6619
IENvbXA= 6620
0YDRg9Cz 6621
IFdv 6622
cm9s 6623
IHNhdmVk 6624
IEhvbg== 6625
IFZp 6626
5YiG 6627
YWNh 6628
cHJldA== 6629
IHdldA== 6630
IHN0dXBpZA== 6631
IGxhZA== 6632
IGZlc3Q= 6633
IHdha2U= 6634
INC40L0= 6635
IGdyZWF0ZXN0 6636
IEppbQ== 6637
IHNlcmlvdXNseQ== 6638
IOy5 6639
IGZlZWxpbmdz 6640
IDMwMA== 6641
aWF0aW9u 6642
IGJlYXV0eQ== 6643
IOyemA== 6644
IHNhbg== 6645
k6A= 6646
IC0o 6647
IGNvbnNjaW91cw== 6648
INC00LXQuw== 6649
Ynll 6650
55k= 6651
TWFu 6652
IGxldHM= 6653
IHNob2Vz 6654
eWQ= 6655
5LmI 6656
IGRpc2FwcGU= 6657
IENvdW50eQ== 6658
IFNjb3R0 6659
IGJ1dHQ= 6660
IGFxdcOt 6661
IGNvbmZpZw== 6662
cmVzcG9uZA== 6663
TEFVR0g= 6664
qeuLiOuLpA== 6665
IGRpdmlkZWQ= 6666
IGFjcXU= 6667
IHpvbmU= 6668
IGtvbW0= 6669
YcOnw6Nv 6670
7Kec 6671
Y3V0 6672
IDIz 6673
IG1heGltdW0= 6674
cm9n 6675
IHJ1bnM= 6676
IGNvbXBvbmVudHM= 6677
IGFycml2ZWQ= 6678
IGNvbmZpZGVudA== 6679
0YDQvtCy 6680
IGhlaWdodA== 6681
IHByb2NlZA== 6682
RU0= 6683
INCt0YLQvg== 6684
IE1lbg== 6685
IHRhbGtz 6686
IGNvbmZpZGVuY2U= 6687
IENocmlz 6688
IGxlYWRz 6689
IG5vc2U= 6690
ZmFsbA== 6691
YmI= 6692
IE5vdGhpbmc= 6693
aXNlcg== 6694
IGluZGVwZW5kZW50 6695
IG1pbm9y 6696
IHN5bQ== 6697
bGVu 6698
Y2llbmNl 6699
IGZhc2hpb24= 6700
IHNleHVhbA== 6701
IGJ1bg== 6702
aGVyZQ== 6703
IHNvaWw= 6704
IGRpZXNl 6705
IHNoYXA= 6706
IGVtcHR5 6707
IGpvdXJuYWw= 6708
YWdvbg== 6709
IFRoZWly 6710
IHdlZWtlbmQ= 6711
w610 6712
IGVycm9y 6713
IG5hcg== 6714
w7g= 6715
6Kk= 6716
YW5jeQ== 6717
IOyVig== 6718
IGZvcmVzdA== 6719
IGhhY2Vy 6720
IG1pc3NlZA== 6721
44GV 6722
5Y+v5Lul 6723
IGV2aWw= 6724
IHN0b3JhZ2U= 6725
IHNpbmdpbmc= 6726
aW5oYQ== 6727
IGtub2Nr 6728
IGltcHJlc3M= 6729
INC+0YfQtdC90Yw= 6730
IEdvbGQ= 6731
IFN1cg== 6732
IFBvcnQ= 6733
5Y67 6734
IExvbmQ= 6735
IGZhemVy 6736
b3R5 6737
b3Rv 6738
IGFueA== 6739
IFdpbGxpYW0= 6740
IGV4aXN0aW5n 6741
cGxhY2U= 6742
IENE 6743
zrM= 6744
IENvbGxlZ2U= 6745
bG9y 6746
IEVhc3Q= 6747
c2Vu 6748
ZmFjaA== 6749
b2Z0 6750
IGV4cGVyaWVuY2Vk 6751
IGxvdmVz 6752
aW1t 6753
IHBvbHk= 6754
IGVzc2U= 6755
7KQ= 6756
IEdyYW5k 6757
6Kc= 6758
Y2hlcg== 6759
IHZpY3RpbQ== 6760
IEdlcw== 6761
0LvRjA== 6762
dmlzaW9u 6763
IHRhbGw= 6764
IGxlbnM= 6765
INC30L3QsA== 6766
IEJvdGg= 6767
IOyy 6768
IHN1c3RhaW4= 6769
IGFyZ3VtZW50 6770
IGZhY3RvcnM= 6771
IGF1dG9tYXRpY2FsbHk= 6772
IGZydWl0 6773
IGxpYmVy 6774
IGFsZQ== 6775
IFByZXNz 6776
IEJh 6777
INCz0L4= 6778
IGh1bmRyZWRz 6779
dGhhdA== 6780
IFJpY2g= 6781
IHJlY2lwZQ== 6782
IElU 6783
6Ic= 6784
4bql 6785
IGRlc2NyaWJl 6786
IGRyaXZlcg== 6787
IE9jdA== 6788
IE1hdA== 6789
0LTQtQ== 6790
IG1lYWw= 6791
IGxhdGVzdA== 6792
IHRoZXJhcA== 6793
IGNvbXBhcmU= 6794
IEFtYXpvbg== 6795
IOyigA== 6796
IFJ1c3NpYQ== 6797
IHN0cmluZw== 6798
IGth 6799
IENvbW11bg== 6800
IGRpYQ== 6801
SXM= 6802
IG1pbGxpb25z 6803
IGNvcnBvcg== 6804
IGNvcnJlc3BvbmQ= 6805
IGZpeGVk 6806
IEpvZQ== 6807
2Y4= 6808
IHZpZXdz 6809
IHJpdmVy 6810
IHN0dWRpbw== 6811
aWdnZXI= 6812
IGZsYXZvcg== 6813
IHByZXNlbmNl 6814
IHVuaXRz 6815
IHNhdmluZw== 6816
YXZvdXI= 6817
IHBlc3Nv 6818
b3JpdGg= 6819
IGhlcnM= 6820
IE5hdA== 6821
YXNpb24= 6822
IEZyYW5r 6823
0L7RiA== 6824
xYJ5 6825
7YQ= 6826
IGVpbmVt 6827
IGZ1bmN0aW9ucw== 6828
dW1hbg== 6829
IG5vcnRo 6830
IOyghA== 6831
IGhvcnNl 6832
dmlk 6833
IHBsZWFzdXJl 6834
0LDRiA== 6835
w6llcw== 6836
aW5kYQ== 6837
IHRhaWw= 6838
IGV4cGxvcmU= 6839
U1Q= 6840
IGNvbW1lcmNpYWw= 6841
IER1cmluZw== 6842
YXJs 6843
XTo= 6844
Zml0 6845
IHJhdGVz 6846
5rM= 6847
TVVTSUM= 6848
IGhvdXNpbmc= 6849
IGVpbmVy 6850
IHNpdHVhdGlvbnM= 6851
5os= 6852
IGRlY3Jl 6853
IGFwcHJvcHJpYXRl 6854
0LXQvdC90L4= 6855
JS4= 6856
IGJhYw== 6857
IHdhdA== 6858
ZW5zaXR5 6859
w6Ro 6860
a25vd24= 6861
aXR6 6862
IGVtb3Rpb25hbA== 6863
ZXJ2YXRpb24= 6864
IGJsaW5k 6865
MTY= 6866
7YM= 6867
5aSn5a62 6868
IGpvaW5lZA== 6869
IGxvY2F0ZWQ= 6870
INGB0Lw= 6871
YWRhcw== 6872
YmVyZw== 6873
IGRlc3M= 6874
IGRlYXI= 6875
ZWRlbg== 6876
Y29z 6877
IGFkb3B0 6878
MTAw 6879
b3dl 6880
IENoZWNr 6881
aXNtbw== 6882
IHNpbXBs 6883
IGFuZ3J5 6884
INC80LXQvdGP 6885
IENhbQ== 6886
IHBhZA== 6887
IGF0dGVuZA== 6888
IHNhbXBsZQ== 6889
5pel 6890
IOyb 6891
IElO 6892
dWxvdXM= 6893
IFNhcg== 6894
IFNob3c= 6895
IGluZnJhc3RydWN0dXJl 6896
IEF1Z3VzdA== 6897
IGxlc3Nvbg== 6898
IG5pZXQ= 6899
5o4= 6900
IGZvaQ== 6901
IGJyb2tl 6902
dHI= 6903
55U= 6904
IDQ1 6905
IGdldw== 6906
0YPQvw== 6907
YXRp 6908
IG1haW50YWlu 6909
IGFydGlzdHM= 6910
aW5nZXI= 6911
5p2l 6912
ZXJ2ZWQ= 6913
SUE= 6914
IGVxdWFscw== 6915
IG9wZXJhdGlvbg== 6916
aWxseQ== 6917
IOuCtA== 6918
IGNyb3dk 6919
IGludGVybmFs 6920
IHRlc3Rz 6921
IFJvY2s= 6922
IENvbnM= 6923
IOuEiOustA== 6924
d2Fy 6925
IHNvdQ== 6926
IGNoYXJ0 6927
IEp1bmU= 6928
IEFwcmls 6929
Z2VudA== 6930
IHZlbnQ= 6931
IHF1YW5k 6932
IEtvcmVhbg== 6933
aW1v 6934
54k= 6935
aWRlcnM= 6936
IG1vdW50YWlu 6937
0YHRgtCw0LI= 6938
5pyI 6939
aWpr 6940
IGRpc2NvdmVyZWQ= 6941
IFN1bmQ= 6942
IFNpbA== 6943
IHNvbG8= 6944
wrQ= 6945
IHNjaG9s 6946
IEVhY2g= 6947
57U= 6948
IGJhcmU= 6949
IO2M 6950
IHbDrWRl 6951
IGluZ3JlZGllbnRz 6952
IEl0cw== 6953
nbzqs6A= 6954
IOyK 6955
z40= 6956
IExlZQ== 6957
IHNjYXJ5 6958
IHByaW5jaXA= 6959
IHNwaXJpdHVhbA== 6960
7IU= 6961
IEhvbGQ= 6962
5rKS5pyJ 6963
IGRlZmluZQ== 6964
IExlcw== 6965
IE5vcg== 6966
IEVuZA== 6967
IGJsb2c= 6968
IEdyZWVu 6969
0LDQtdGC0YHRjw== 6970
cGFydA== 6971
ZWxlcw== 6972
5LqL 6973
IFVuZGVy 6974
IHBhcnRl 6975
IDM1 6976
IHNlY3Rvcg== 6977
IFNlcHQ= 6978
IGF1dGg= 6979
4K6u 6980
b21pbg== 6981
IGNsaWVudHM= 6982
IGNp 6983
IEZyaWRheQ== 6984
ZXJhcw== 6985
IHR3ZQ== 6986
dWxhdGVk 6987
IGN1bHR1cmFs 6988
INGB0LLQvg== 6989
IOuNlA== 6990
IMO6 6991
IHBhcmNl 6992
4K6y 6993
IHRyYWRpdGlvbg== 6994
IGp1ZGdl 6995
IEdlbmVyYWw= 6996
IGRldGVybWluZQ== 6997
IElzbg== 6998
IFBM 6999
bmVhdGg= 7000
IG1hdHRlcnM= 7001
7ZW07A== 7002
IV0= 7003
0LDRhQ== 7004
IHBvb2w= 7005
IHZhcmlhYmxl 7006
IHZhY2NpbmU= 7007
IGNhdXNlZA== 7008
IHdlc3Q= 7009
IFllcA== 7010
ZmFzdA== 7011
IHBoaWxvcw== 7012
aG9yYQ== 7013
IGNvbnRpbnVlZA== 7014
IHVuZm9ydHVuYXRlbHk= 7015
44GN 7016
5pU= 7017
IGZsaWdodA== 7018
IHdyYXA= 7019
IGh1aA== 7020
IEFic29sdXRlbHk= 7021
IHBpbms= 7022
IHJlbWFpbnM= 7023
IG7DqQ== 7024
IGZsZQ== 7025
IFNvbA== 7026
IGxvc2luZw== 7027
IGFsZ29yaXRo 7028
IHJlcXVpcmVz 7029
IGZvdW5kYXRpb24= 7030
IEJ1cg== 7031
IHByb2Zlc3Npb24= 7032
IE1pZA== 7033
IOutkA== 7034
Y2Fu 7035
IE1pbA== 7036
IHlvdW5nZXI= 7037
IGFwcGVhcnM= 7038
dGVybQ== 7039
7ZWY6rOg 7040
YWNsZQ== 7041
IExvbmRvbg== 7042
IGVuZ2luZWVyaW5n 7043
4Lii 7044
IGFkdmVudA== 7045
7IS47JqU 7046
IOq4sA== 7047
IE1hag== 7048
0YDQtdC8 7049
aW5ndQ== 7050
IFVL 7051
dXJv 7052
c3Bl 7053
IHRlbnQ= 7054
IHJlcG9ydGVk 7055
IEFM 7056
SGV5 7057
IOunkA== 7058
IGRlbnQ= 7059
IEF1c3RyYWxpYQ== 7060
IEphbnVhcnk= 7061
s7Q= 7062
YWd1ZXM= 7063
YXJzaA== 7064
cmln 7065
IHRpZW5l 7066
4Lij 7067
zq4= 7068
IG1hY2hlbg== 7069
dW50ZQ== 7070
0YPRgQ== 7071
IGVsZWN0cg== 7072
IHR1dG9yaWFs 7073
IHBsYWNlZA== 7074
IOydtOqxsA== 7075
IENvdW5jaWw= 7076
7ZaI 7077
sOumrA== 7078
YWhyZW4= 7079
IOq3uOuemA== 7080
IHByb3Zl 7081
Zm9s 7082
IHF1ZXI= 7083
IGNoZWFw 7084
IEZhdGhlcg== 7085
IFBvd2Vy 7086
k5w= 7087
IHB1cnM= 7088
IGVzcA== 7089
IEJyZQ== 7090
6riw6w== 7091
b21hcw== 7092
5oOz 7093
0LjQu9GM 7094
IGdlaHQ= 7095
b3N0ZXI= 7096
6rO8 7097
IGZpbGVz 7098
INCn 7099
YmVsbA== 7100
IHdob20= 7101
IOuY 7102
IGV4Y2VsbGVudA== 7103
IGRhdGFi 7104
IGfDtg== 7105
IOynhOynnA== 7106
IGJlbGllZg== 7107
amV0 7108
IGphY2s= 7109
IHN3aW0= 7110
cmlhbA== 7111
dW1pbg== 7112
YXVj 7113
IHNvbGw= 7114
IGVzc2VudGlhbA== 7115
7ZWY64qU 7116
IGV2b2w= 7117
Y2hhZnQ= 7118
YWluZQ== 7119
dGhsZXQ= 7120
IGluY29y 7121
IHJlcG9ydHM= 7122
IGRlZmluaXRpb24= 7123
a2Vs 7124
IGNpcmN1bQ== 7125
IHByb2R1Y2Vk 7126
INeb 7127
YW50aWM= 7128
bmV0 7129
IGF3YXJk 7130
IGR1cmNo 7131
IHRyYW5zcA== 7132
IG1hbGU= 7133
pqzr 7134
IG1vb24= 7135
IEdlb3JnZQ== 7136
IGZseWluZw== 7137
acOz 7138
IHNvdXJjZXM= 7139
IHBsZW50eQ== 7140
IERlbW9jcg== 7141
Uk8= 7142
IDAw 7143
IHNlY3VyZQ== 7144
IEJpcg== 7145
cmFpbg== 7146
IHp1cg== 7147
IGVmZmljaWVudA== 7148
IHJlcGVhdA== 7149
IG1ldGhvZHM= 7150
IGNhbG0= 7151
IGRpc2N1c3NlZA== 7152
IOyeiOuKlA== 7153
IHNlcnZlcg== 7154
YW5pZQ== 7155
IEluc3RlYWQ= 7156
IGlkZWFs 7157
IGNvbnZlbg== 7158
IGhvcGluZw== 7159
IFRvcg== 7160
IGRlcHRo 7161
IGhlYXZlbg== 7162
RU5DRQ== 7163
IGhhYml0 7164
Z3JhZA== 7165
IGZsYWc= 7166
IGluZQ== 7167
IGto 7168
IExJ 7169
IGZhY2luZw== 7170
IEFV 7171
IFRpbQ== 7172
IGdlbQ== 7173
IEp1bA== 7174
IGVsYQ== 7175
aXp6YQ== 7176
IGZlbGxvdw== 7177
IHF1ZWw= 7178
IHNwb2tl 7179
IGNpdGl6ZW5z 7180
dWdl 7181
6YO9 7182
IHBhZ2Vz 7183
IGZhc2M= 7184
IHJlbGlnaW91cw== 7185
YXRlbg== 7186
IGNoYXB0ZXI= 7187
IFZhbA== 7188
IGNvbnN1bHQ= 7189
IE1pbGw= 7190
Z2w= 7191
b3Blcg== 7192
IGluZmlu 7193
IG1hcnJpYWdl 7194
IG1lZGljaW5l 7195
INC00LI= 7196
IGRvZ3M= 7197
IGluc3RydW1lbnQ= 7198
IEV4YWN0 7199
w6Fu 7200
IDIwMjE= 7201
IGZlcg== 7202
IHdlYWx0aA== 7203
IGdyYWRl 7204
0YvRhQ== 7205
IGNyaW1l 7206
IHRocmVhZA== 7207
IGVzc2E= 7208
IHdpbmU= 7209
Y29ob2w= 7210
cGhh 7211
4LiH 7212
b2d1ZQ== 7213
IGluc3VyYW5jZQ== 7214
YXJyYXRvcg== 7215
IFNlcHRlbWJlcg== 7216
IHZpZA== 7217
IFNwaXJpdA== 7218
IGdlc3Q= 7219
IFJ1c3NpYW4= 7220
IHByb3BlcnRpZXM= 7221
IGFydGljbGU= 7222
IHVuZGVybmVhdGg= 7223
eWVy 7224
IGpvaW50 7225
IHJlbGF0aXZlbHk= 7226
IGluY2g= 7227
IGRlc3BpdGU= 7228
IEdyZWU= 7229
IGNsYXNzaWM= 7230
IHN1cHBvcnRpbmc= 7231
IGluc3RydWN0 7232
bHVzaXZl 7233
IGRpYWdu 7234
5oo= 7235
IGFkbWluaXN0cmF0aW9u 7236
0LDQsdC+0YI= 7237
IE9wZW4= 7238
5omA5Lul 7239
INC/0L7Qug== 7240
IGRvbGxhcg== 7241
IGNvbnNlcXU= 7242
b2Jlcg== 7243
IEdlcm1hbnk= 7244
IHRlcnI= 7245
IFFV 7246
INCT 7247
574= 7248
IHN0cm9uZ2Vy 7249
yZk= 7250
INmK 7251
IGlQaG9uZQ== 7252
IGZhYnJpYw== 7253
w7xo 7254
IGVuZW0= 7255
5q8= 7256
IHN1YnQ= 7257
RUU= 7258
b25kZQ== 7259
IGNyZXc= 7260
IHJlbW92ZWQ= 7261
IGxhZHk= 7262
IHBvdGVudGlhbGx5 7263
INCd0L4= 7264
eWFs 7265
IHN5bXB0 7266
IGFybXk= 7267
IGludHJvZHVjZWQ= 7268
dGVz 7269
IGFzcGVjdHM= 7270
MTQ= 7271
IExvdQ== 7272
ICk= 7273
IGRlcGxveQ== 7274
cGV0 7275
IGhhbg== 7276
IFdhdGNo 7277
IHdlYXBvbnM= 7278
IHBoZW4= 7279
IHJlZ2lzdGVy 7280
IGVpbmZhY2g= 7281
IHNwb3J0 7282
IGJyaWRnZQ== 7283
IGlubmVy 7284
IG1pbmltdW0= 7285
IHdpdG5lc3M= 7286
IGVzbw== 7287
IHZpbGxhZ2U= 7288
IG93bmVy 7289
pqzqs6A= 7290
IHNjcmVhbQ== 7291
aWxlZA== 7292
IHBpdGNo 7293
YnJ1 7294
IGFkdmFuY2U= 7295
5LiN5piv 7296
IHN1cHBvc2U= 7297
IEF0dA== 7298
0LXRgtGB0Y8= 7299
IGRpZmZlcmVuY2Vz 7300
YWtlZA== 7301
IGludGVycHJldA== 7302
w6Y= 7303
aWVuZG8= 7304
IGFic29s 7305
INCx0YPQtNC10YI= 7306
IOuy 7307
IHRyaWFs 7308
IHRoaW5rcw== 7309
bHlpbmc= 7310
Y2VwdGlvbg== 7311
IEFmcmljYW4= 7312
IGNoZW1pY2Fs 7313
IHRhcGU= 7314
IGNvbnZlcnNhdGlvbnM= 7315
IGRpc3RyaWJ1dGlvbg== 7316
dGk= 7317
IEFJ 7318
IGZsYXNo 7319
IHVuZGVyc3Rvb2Q= 7320
IEdvdmVybm1lbnQ= 7321
5bCP 7322
IT8= 7323
IFNr 7324
6rGw6w== 7325
cmllcg== 7326
VFM= 7327
IEFjY29yZGluZw== 7328
0Y7Rgg== 7329
IHNwb25z 7330
0YLQvtCx0Ys= 7331
IHZhbHU= 7332
ZXJlbQ== 7333
aWNodGln 7334
IHJlc2lzdGFuY2U= 7335
IEdhbA== 7336
Z2VyeQ== 7337
IGJlZ2lucw== 7338
IGFkdmFuY2Vk 7339
IHJlbGV2YW50 7340
IHBvbGl0aWNz 7341
IEZhbQ== 7342
IMOnb2s= 7343
IE5ldmVy 7344
aWxsaW5n 7345
IGZvb3RiYWxs 7346
0LjQuA== 7347
IElE 7348
IEFmcmljYQ== 7349
IGZpbmdlcnM= 7350
INCx0L7Qu9GM 7351
IMOh 7352
IGNsaXA= 7353
IExhdA== 7354
44KE 7355
IOyngOq4iA== 7356
ZXNzZQ== 7357
IHZvb3I= 7358
IGFzaWRl 7359
5p4= 7360
IHRvd2FyZA== 7361
IGJhdA== 7362
IHZhbGlk 7363
IE1lbnM= 7364
IGNvbXBsZXRlZA== 7365
xLHEnw== 7366
IHBvZGNhc3Q= 7367
IEJvbg== 7368
25I= 7369
IEp1bHk= 7370
aWxh 7371
IHBhY2thZ2U= 7372
IHB1bGxlZA== 7373
Y2hhcg== 7374
IE1lbA== 7375
b2lz 7376
IHNvdXRo 7377
IOuU 7378
IGltcG9ydGFuY2U= 7379
IHB1c2hpbmc= 7380
IGlzb2w= 7381
IHN0YW5kcw== 7382
Y2lsbA== 7383
5Lw= 7384
IPCf 7385
b3Jp 7386
6rCB 7387
IGhvbWVz 7388
IGNvbmNlcm5z 7389
IGJpeg== 7390
5b0= 7391
Ymll 7392
IGJpcw== 7393
IGdlYXI= 7394
IE1T 7395
IGh1bg== 7396
IE1hdHQ= 7397
4bqj 7398
c2V5 7399
IFNlY3JldA== 7400
IG9kZA== 7401
IE1heA== 7402
b2xseQ== 7403
Zm9yZA== 7404
IFNI 7405
IHJlcGxhY2U= 7406
IG5hdmln 7407
IGluaQ== 7408
0LjRjw== 7409
IGdpYW50 7410
IG1hbmQ= 7411
IEhhcHA= 7412
VElPTg== 7413
Z3Vu 7414
aWFtbw== 7415
7J6F64uI64uk 7416
IGdhcA== 7417
IMOqdHJl 7418
IGNsYXNzcm9vbQ== 7419
IGh5cA== 7420
YWtp 7421
6K4= 7422
aXN0ZXJz 7423
YWNrcw== 7424
INGB0L4= 7425
IGJ1Zw== 7426
IGdyYXY= 7427
YW1pbg== 7428
IGV2ZXJ5ZGF5 7429
IOyhsA== 7430
IGdhcmRlbg== 7431
Y2VtYmVy 7432
IGVzdG8= 7433
5ZeO 7434
2Kw= 7435
n7A= 7436
5YE= 7437
IHJvbQ== 7438
IOygnOqwgA== 7439
IGZhbGxpbmc= 7440
IGZhdWx0 7441
ZWxseQ== 7442
IGNoZXN0 7443
INC70Lg= 7444
IHBvdGF0bw== 7445
IGJ1aWxkaW5ncw== 7446
IG9wZXJhdGluZw== 7447
IHBhcmU= 7448
d3I= 7449
RG9u 7450
IEZvdXI= 7451
IHZ1bA== 7452
IGzDoQ== 7453
IGZydXN0 7454
IERhbm4= 7455
b2xlcw== 7456
bnlh 7457
IOy2 7458
INGA0LDRgQ== 7459
15s= 7460
IGHDrQ== 7461
d29yZA== 7462
IHdlYXBvbg== 7463
IG9idA== 7464
IEZhbGw= 7465
IFN0ZXZl 7466
IG1peGVk 7467
IHBvZGU= 7468
IEFT 7469
IExlZw== 7470
IGRlc2M= 7471
IHNwbGl0 7472
IGVtZXJnZW5jeQ== 7473
IFNpbmc= 7474
IHByb2ZpdA== 7475
IHR5cGljYWw= 7476
IERvbmM= 7477
IGFubm91bmNl 7478
IFRleA== 7479
IHNhY3I= 7480
dGVybmFs 7481
IGNvbW1pdHRlZQ== 7482
aWdv 7483
IGRpYW0= 7484
cGhhcw== 7485
IGRlZmU= 7486
IFByb2Zlc3M= 7487
IGRlY2w= 7488
0YPRgA== 7489
MjI= 7490
b2xm 7491
IE1vbmQ= 7492
dXk= 7493
IGF5 7494
IGxlbQ== 7495
IGxvdmVseQ== 7496
IENvdWxk 7497
IGd1YXI= 7498
SEg= 7499
IGNhcmVmdWxseQ== 7500
IExpc3Rlbg== 7501
INC60YA= 7502
IHlvdXRo 7503
IFRoZXJlZm9yZQ== 7504
IGRyZWFtcw== 7505
IEplZmY= 7506
P10= 7507
IOuI 7508
REE= 7509
IGJvZGllcw== 7510
YXV4 7511
IHRlY2huaXF1ZXM= 7512
IG1lY2hhbmlzbQ== 7513
15M= 7514
INC+0L3QuA== 7515
IGRlc2lyZQ== 7516
w64= 7517
IFZv 7518
cXVlcw== 7519
INGD0LbQtQ== 7520
IFdob2E= 7521
IEdhbWU= 7522
IGhhbA== 7523
YW5pc2g= 7524
IHByYWN0aWNlcw== 7525
NTAw 7526
IHNvcnRz 7527
dXBz 7528
YXRlZnVs 7529
IGhlcnNlbGY= 7530
IGd1aXRhcg== 7531
IHByb3Bvcw== 7532
IHNpdGVz 7533
IGJlYWNo 7534
INei 7535
56ys 7536
0L3Rgw== 7537
IGRyYW0= 7538
IE5vdmU= 7539
VkU= 7540
cmFudA== 7541
IHBsb3Q= 7542
IOyXrOq4sA== 7543
IENh 7544
IGVzdGFibGlzaGVk 7545
IDIwMTU= 7546
IGluc3BpcmVk 7547
IGFubm91bmNlZA== 7548
5Liq 7549
INGC0YA= 7550
IDI2 7551
IHZveQ== 7552
IHRlY2g= 7553
7KCB 7554
IHByb2Nlc3Nlcw== 7555
b250bw== 7556
IFBhbg== 7557
IHJhcGlk 7558
aXN0YW4= 7559
IDE5Nw== 7560
IHJlbGlnaW9u 7561
IDI4 7562
IHNtaWxl 7563
IGJhYg== 7564
INqp 7565
IFZpcg== 7566
IHNjaGVkdWxl 7567
IGV4ZWN1dA== 7568
IHByb24= 7569
0Y0= 7570
INCd0YM= 7571
bXVzaWM= 7572
7JuQ 7573
IGdhbg== 7574
7Iug 7575
IGRlZmF1bHQ= 7576
IGJlbQ== 7577
2Yk= 7578
IGZvcmNlZA== 7579
IE9idmlvdXNseQ== 7580
IHN0b25l 7581
IHRpZQ== 7582
IGRyaW5raW5n 7583
IHNlcnZlZA== 7584
Q2F1c2U= 7585
IGNvbmZlcmVuY2U= 7586
IEV4YWN0bHk= 7587
44OI 7588
oJw= 7589
7JmA 7590
IFJh 7591
IGZha2U= 7592
IGRpZmY= 7593
44Gp 7594
IGNoYWxsZW5naW5n 7595
IOykkQ== 7596
z4c= 7597
5LuA6bq8 7598
IGludGVsbGlnZW5jZQ== 7599
cmV0ZQ== 7600
IHN0dWR5aW5n 7601
IGFwcG9pbnQ= 7602
IHRhbg== 7603
INC40Lw= 7604
IGN1cnZl 7605
IFRlYW0= 7606
IEF6 7607
INC30LQ= 7608
IE11c2lj 7609
ZmllbGQ= 7610
aXJhdGlvbg== 7611
IGZhaWxlZA== 7612
IG5vdmVs 7613
IGRpZmZlcmVudGx5 7614
IGVzY2FwZQ== 7615
IFlv 7616
IE9jdG9iZXI= 7617
xLF5b3I= 7618
IGRlc2NyaWJlZA== 7619
IGNvbnZlcnQ= 7620
YWNlbWVudA== 7621
IGhvdGVs 7622
aXNhdGlvbg== 7623
IHN1aXM= 7624
44GR 7625
5a2Q 7626
5oCO 7627
IHdhbGtlZA== 7628
MjAw 7629
IG5laWdoYm9yaG9vZA== 7630
aXNw 7631
IExvcw== 7632
IGhpZGRlbg== 7633
IDI3 7634
0LvQtQ== 7635
IHBocg== 7636
IElzbGFuZA== 7637
IFN0cmVldA== 7638
ZW5kYQ== 7639
aGlwcw== 7640
b3N1cmU= 7641
IGRlZmluZWQ= 7642
4Lin 7643
IHZpZGE= 7644
IGxhYmVs 7645
IEV2ZXJ5Ym9keQ== 7646
IGpva2U= 7647
aWFv 7648
2KfZhg== 7649
IGF0aGxldA== 7650
Li4uIg== 7651
IEZpcmU= 7652
RG8= 7653
IGRlZmVuc2U= 7654
IGVudGVydGFpbg== 7655
w6F0 7656
IHBvbGljaWVz 7657
IGFsY29ob2w= 7658
IEVuZ2luZQ== 7659
IGdhbA== 7660
IEp1ZA== 7661
IHZvbHVudGU= 7662
aWNrcw== 7663
ZXRh 7664
YWd0 7665
INeV 7666
IG3Dtg== 7667
MTM= 7668
IGVuY291bg== 7669
IGVo 7670
IG9yYW5nZQ== 7671
IGFic29y 7672
IHNwYWNlcw== 7673
IE5vdmVtYmVy 7674
6rWs 7675
aWF0 7676
IHRhbQ== 7677
Y2tub3c= 7678
IHN0b3Jt 7679
IERpcmVjdG9y 7680
IHByZWdu 7681
IOydvA== 7682
INC+0L8= 7683
IHJlc291cmNl 7684
IGJhcmQ= 7685
bmV3 7686
IERlY2VtYmVy 7687
dWl0cw== 7688
IHdlaWw= 7689
IGNvbnN0cnVjdA== 7690
c2k= 7691
bmlj 7692
IGZsb3Vy 7693
IHJlc3RyaWN0 7694
w7x0 7695
IGVudGlyZWx5 7696
IGJyZWFraW5n 7697
ZW50bGljaA== 7698
IHR3ZW50eQ== 7699
IGNhdXNlcw== 7700
IGVsZXY= 7701
IFNwcg== 7702
IEludGVybmV0 7703
IGtpc3M= 7704
IG9wZXJhdGlvbnM= 7705
c3p5 7706
IOuK 7707
IHNjaWVudGlzdHM= 7708
IGdyb3du 7709
IG93bmVycw== 7710
b3V0cw== 7711
IGNvdXJzZXM= 7712
IHVzdWFs 7713
IGlubg== 7714
IHRyYW5zbQ== 7715
w7Fv 7716
IG51ZXN0 7717
0LrQvtCy 7718
IGNhdGVnb3J5 7719
IExpZmU= 7720
IFBsdXM= 7721
IGF0bW9z 7722
d2hpbGU= 7723
IHJlY29yZHM= 7724
IGRlxJ8= 7725
64uk6rOg 7726
IOyCrOue 7727
IHJlcXVpcmVtZW50cw== 7728
aW5u 7729
IGltbWln 7730
IGRlZXBlcg== 7731
57Q= 7732
IGFwcHM= 7733
IGNvbGxlYWd1ZXM= 7734
xbx5 7735
IG9mZmVycw== 7736
IHTDoQ== 7737
IGNvbHVtbg== 7738
bGF1ZA== 7739
SVI= 7740
IE1z 7741
IGV4Y2hhbmdl 7742
bGFz 7743
IExhdw== 7744
IEpvbg== 7745
aXNzZQ== 7746
cm9nZW4= 7747
IG1vaQ== 7748
15c= 7749
IHNlbmRpbmc= 7750
IGhlbGxv 7751
0LXQtQ== 7752
xZvEhw== 7753
IHN1Y2NlZWQ= 7754
IHN1ZmZlcmluZw== 7755
IGFkdmVydA== 7756
IOyjvA== 7757
55+l6YGT 7758
IHJlY28= 7759
xLFuxLE= 7760
INC60L7QvA== 7761
YWxsZXk= 7762
IGZhaWx1cmU= 7763
aWVq 7764
IOuVjA== 7765
IGRydWdz 7766
IGN1YW5kbw== 7767
IOyWtOuW 7768
IEFib3V0 7769
IHF1YW5kbw== 7770
OTA= 7771
IEZlZA== 7772
MTc= 7773
U2g= 7774
aW5obw== 7775
IFN1bmRheQ== 7776
IFBoaWw= 7777
IGFjYWRlbWlj 7778
IEluYw== 7779
IG1haW50ZW4= 7780
5Ye6 7781
IHJld2FyZA== 7782
ZXJk 7783
IGNvbW1pdHRlZA== 7784
7Iqk 7785
0LPRgA== 7786
IHN0YW5kYXJkcw== 7787
IGthbA== 7788
IGludGVudGlvbg== 7789
IFpo 7790
IGFja25vdw== 7791
5L8= 7792
ID09PQ== 7793
b2d5 7794
5ac= 7795
IGZpbG1z 7796
aXNr 7797
IHRlZXRo 7798
IHN0cnVnZ2xl 7799
cmQ= 7800
dWVu 7801
IGRpc3M= 7802
IERhcg== 7803
YW15 7804
IGVuZW1pZXM= 7805
IHZlbG9j 7806
IENhbGw= 7807
dW1icw== 7808
0LjRgtC10LvRjA== 7809
IG9jZWFu 7810
w6lk 7811
7Jqw 7812
IHRyZW0= 7813
aWVudG8= 7814
0LXRiNGM 7815
ZmZpY2llbnQ= 7816
IGJvdHRsZQ== 7817
IGluc3RpdHV0aW9u 7818
ZXN0eQ== 7819
IEhhbg== 7820
aGFi 7821
64qY 7822
IGFycmVzdA== 7823
6YKE 7824
IGxldHRlcnM= 7825
b3VuY2U= 7826
7Yw= 7827
QW4= 7828
IGNyZWF0ZXM= 7829
IGNsb2Nr 7830
IGRlYnQ= 7831
IGFuY2llbnQ= 7832
aWZpY2F0aW9ucw== 7833
Z2k= 7834
QnV0 7835
IFR1 7836
a2w= 7837
IGJvcmRlcg== 7838
IG9vaw== 7839
IEJheQ== 7840
ZXN0YQ== 7841
IOuztOw= 7842
IHdyYQ== 7843
cHJlbmU= 7844
IOqyjA== 7845
YW5nbGU= 7846
IGJlbGlldmVk 7847
aWVuY3k= 7848
YWth 7849
IGNyaXRpYw== 7850
IGJvbWI= 7851
IGhhbQ== 7852
INCb 7853
6rWt 7854
IEd1eXM= 7855
cm9zb2Z0 7856
IGNyaW0= 7857
ZXRjaA== 7858
QVJS 7859
IHNpZ2h0 7860
0LjQvdCw 7861
IGFpbg== 7862
4buR 7863
aXNjaGU= 7864
IGF1eA== 7865
IG51bWVy 7866
IHN1cnZpdmU= 7867
QWxs 7868
QkM= 7869
IHN6 7870
n6zr 7871
IGphbQ== 7872
IENvdXJ0 7873
IGFsbGVz 7874
IHRyaWdnZXI= 7875
0J4= 7876
IGZvcm1hdA== 7877
IGRlY2FkZXM= 7878
IGNlcw== 7879
IHNpZ25z 7880
IHJvYm90 7881
IENodXJjaA== 7882
IGF6 7883
IHNvdXA= 7884
IFRleGFz 7885
dXRlbg== 7886
INGH0YLQvtCx0Ys= 7887
IG5laWdoYg== 7888
lteU 7889
IGNvbW11bmljYXRl 7890
xaE= 7891
IGVsaW1pbg== 7892
IGZyZXF1ZW5jeQ== 7893
aGVybg== 7894
aWRvcw== 7895
IGVtcGhhcw== 7896
IG1lc3NhZ2Vz 7897
IGdlbmRlcg== 7898
IFdlbm4= 7899
INCy0L4= 7900
IHByaWNlcw== 7901
b2xv 7902
INC/0L7QvQ== 7903
d2luZw== 7904
IEZpbA== 7905
0LDQtdC8 7906
IEN1cg== 7907
IGZhbHNl 7908
IGZpZWxkcw== 7909
IHPDqQ== 7910
MjQ= 7911
IG1hYw== 7912
dcWf 7913
IGxheWVycw== 7914
IGFkdm9j 7915
d2Fu 7916
IGthcg== 7917
IMWe 7918
IGRlY29y 7919
IHdhbGxz 7920
b2U= 7921
aXNzaW9ucw== 7922
IHJlc29s 7923
16I= 7924
IENhcm9s 7925
IFZpZGU= 7926
bGVlcA== 7927
IFlPVQ== 7928
IGZsaXA= 7929
IHN1cmdlcnk= 7930
IGNob3A= 7931
VVI= 7932
Liw= 7933
IGFnZW5jeQ== 7934
IHdhbnRpbmc= 7935
IHNvbGFy 7936
IGhvcml6 7937
IEFkYW0= 7938
IHN0YXlpbmc= 7939
b2xpYw== 7940
IGdyYXRlZnVs 7941
IHJlbWFyaw== 7942
IHRlY2hub2xvZ2llcw== 7943
IHByb3RlaW4= 7944
5b+D 7945
0LTQtdC7 7946
IE1vbnQ= 7947
IHNob3VsZGVy 7948
IHph 7949
cmV5 7950
IE9vaA== 7951
IHN0eQ== 7952
aWNhcg== 7953
0L7RgtGA 7954
IHJvdXRl 7955
IFR1cm4= 7956
IGJvbQ== 7957
IGRlYmF0ZQ== 7958
IHBvc3NpYmlsaXR5 7959
IO2VtOw= 7960
YXBh 7961
IGludmVudA== 7962
w7xybGljaA== 7963
IHByb2ZpbGU= 7964
IHNlbmlvcg== 7965
cHB5 7966
dmFz 7967
IG11bmRv 7968
YXRldmVy 7969
IGFwcGFyZW50bHk= 7970
ZW5lcg== 7971
15A= 7972
560= 7973
IHByZWNpcw== 7974
IGFsaWdu 7975
IGtuaWZl 7976
IFJvYmVydA== 7977
5Ys= 7978
IGZvb2w= 7979
IGludml0ZQ== 7980
dXNpbmc= 7981
IGNpcmN1bXN0 7982
IGNhcHR1cmU= 7983
IGRvdWdo 7984
IFNhbmQ= 7985
IHNldQ== 7986
IE5ld3M= 7987
IGJpdGU= 7988
IG5ldXQ= 7989
d2lkZQ== 7990
IGxlY3R1cmU= 7991
IOuYkA== 7992
IG9yaWdpbmFsbHk= 7993
IGNob2ljZXM= 7994
IEdhcg== 7995
IHZlcnNl 7996
IGxpdA== 7997
IDE5Ng== 7998
7ZWg 7999
IG1lYXN1cmVz 8000
w6fDtWVz 8001
d2F0ZXI= 8002
cml2ZQ== 8003
IHppam4= 8004
7YE= 8005
IEJ1cw== 8006
IGhlYg== 8007
0LXRhQ== 8008
IEthcg== 8009
IE7Do28= 8010
IGtpbGxpbmc= 8011
4K6q 8012
IG1pcnJvcg== 8013
bW9k 8014
IG1vbA== 8015
IGNyZWF0aW9u 8016
IGVzdGlt 8017
IGF0bW9zcGhlcmU= 8018
IGdhbQ== 8019
IHRhYmxlcw== 8020
aXNp 8021
IExpdHRsZQ== 8022
IHRhcw== 8023
IEVsZQ== 8024
w6ls 8025
IHNjZW5lcw== 8026
IHRvbmU= 8027
IGFmZmVjdGVk 8028
IEFVREk= 8029
IEJyb3du 8030
SWY= 8031
INmH 8032
IERhbmllbA== 8033
55yf55qE 8034
cXVlcg== 8035
Y2hp 8036
7ZWY6w== 8037
IG1pc3Rha2Vz 8038
IHNsYQ== 8039
44Kk 8040
IGVudHI= 8041
INC10YHQu9C4 8042
IHNob3V0 8043
IHBvcnRpb24= 8044
0Zc= 8045
IHByZXZpb3VzbHk= 8046
4buZ 8047
INC/0YDQtdC0 8048
0L7RgdGM 8049
IGhlYWRz 8050
544= 8051
5a0= 8052
5ZyL 8053
IGdyYXNz 8054
4Liw 8055
Y3JpYmU= 8056
IHF1w6k= 8057
IFNwYW5pc2g= 8058
IG9mZmVyZWQ= 8059
INCx0YvQu9C+ 8060
IENsb3Vk 8061
IHZlY3Rvcg== 8062
IEh1aA== 8063
IGthZA== 8064
aWZ0cw== 8065
IM69 8066
IGh1bmdyeQ== 8067
0KE= 8068
IHBhcmFsbA== 8069
QU5E 8070
IHbDrWRlbw== 8071
aXp6 8072
IG9jY3Vw 8073
IO2U 8074
IHNlZWs= 8075
aGVz 8076
IGRvb3Jz 8077
IGhvdXNlcw== 8078
IGNvbnNpZGVyaW5n 8079
IGdyYWR1YXRl 8080
IGZ1bGY= 8081
6KGM 8082
6KM= 8083
IGV4dHJlbWU= 8084
IGZsb3dlcnM= 8085
aXRhdGU= 8086
IFByaQ== 8087
IGZ1bmRhbWVudGFs 8088
0YfQsNGB 8089
6K+0 8090
IHRleHR1cmU= 8091
jZg= 8092
IEFORA== 8093
4K6x 8094
IFRlbQ== 8095
IG5hZGE= 8096
7KeE 8097
IGNlbGVicmF0ZQ== 8098
dW1z 8099
IHBpbGw= 8100
INC40LvQuA== 8101
Z29pbmc= 8102
IGhpcA== 8103
IHN1cHBvcnRlZA== 8104
IHBlcm1hbg== 8105
IGFncmVlbWVudA== 8106
IHR5bQ== 8107
IOuR 8108
k6TsnbQ= 8109
IHB1cmNoYXNl 8110
7ZQ= 8111
IFBsYW4= 8112
ZWdlbg== 8113
IHJlY292ZXI= 8114
UFU= 8115
IE1pY3Jvc29mdA== 8116
ZHVj 8117
IGhvbGVz 8118
IGRyb3BwZWQ= 8119
IHBpZw== 8120
IGVuZGluZw== 8121
IGF0dGFja3M= 8122
YmVj 8123
IHJlbg== 8124
IHJhcHA= 8125
IOyasOumrA== 8126
IHRlcnJvcg== 8127
INeZ 8128
IGVkaXQ= 8129
IGFv 8130
Ljwv 8131
IDIwMDA= 8132
IFVuaW9u 8133
IHNjaWVudGlmaWM= 8134
IHB1bmNo 8135
b3J0aW9u 8136
IHB1dHM= 8137
IE1vbmRheQ== 8138
IEplcg== 8139
RUM= 8140
IG1hdHJpeA== 8141
IGluc3RpdHV0aW9ucw== 8142
IG1vbnQ= 8143
IGV4aGli 8144
IHNwZWFrZXI= 8145
IG1ldGVycw== 8146
Ll0= 8147
IHNlcnZpbmc= 8148
IGRhdGFiYXNl 8149
IExBVQ== 8150
IGRhbW4= 8151
IHBvZGVy 8152
ISEhIQ== 8153
IO2WiA== 8154
IEFVRElFTkNF 8155
IGp1bg== 8156
IEFD 8157
IEl0YWw= 8158
c2Vj 8159
IFlvdW5n 8160
cnVjaw== 8161
b3V2ZQ== 8162
4LiE 8163
54g= 8164
IOunjOs= 8165
YWRpbmc= 8166
dXJhdGlvbg== 8167
IFBT 8168
0Jo= 8169
IFVuZg== 8170
6IE= 8171
b3JpYQ== 8172
IG1hbmlm 8173
IHNlbnRlbmNl 8174
IHNpZ25lZA== 8175
QlM= 8176
IHByb29m 8177
IE11c2xpbQ== 8178
IG51Y2xlYXI= 8179
INCz0L7QstC+0YA= 8180
IHdvbGw= 8181
IGZhdm91cg== 8182
IFdI 8183
IHZ1bG5lcg== 8184
IGNsb3NlbHk= 8185
IGluZGV4 8186
0YLQtdGA 8187
YWNoZWw= 8188
IGNhcGFibGU= 8189
IEJlcw== 8190
IGNyb2No 8191
ZWt0 8192
IHNoZWV0 8193
IHNlZXM= 8194
IG5hdHVyYWxseQ== 8195
IEVuZ2xhbmQ= 8196
IHBhcnRpY2lwYXRl 8197
IGV4aXN0cw== 8198
IHNoYXJw 8199
cHk= 8200
IGJyZWFrZmFzdA== 8201
Ym93 8202
IHR3aXN0 8203
56c= 8204
aW5hdGluZw== 8205
b3Rp 8206
IEZvdW5k 8207
IGRldXg= 8208
IHNlbGVjdGVk 8209
7KCE 8210
b3Npcw== 8211
IHByZXNlbnRlZA== 8212
IGxpbmVhcg== 8213
IOq0 8214
IGt1bg== 8215
6bue 8216
w7RuZw== 8217
IGLEmWQ= 8218
IHRlbXBvcg== 8219
IGNhYmxl 8220
INC/0YDQvtGB0YLQvg== 8221
0LrQtQ== 8222
INGC0LDQvA== 8223
IHdpbm5pbmc= 8224
6IO9 8225
mOuPhA== 8226
IDIwMTQ= 8227
IOyXrOs= 8228
IFVO 8229
IENsaWNr 8230
IHByZXBhcg== 8231
IFRP 8232
IHN1YQ== 8233
IEhhbQ== 8234
IGzDpA== 8235
IGFic29sdXRl 8236
IGVuZ2FnZWQ= 8237
5aaC 8238
IEhtbQ== 8239
IGRhc2g= 8240
VEE= 8241
w7Fvcw== 8242
IHNwbw== 8243
55Sf 8244
KV0= 8245
IHRlc3RlZA== 8246
IGJsYW5r 8247
IHJlamVjdA== 8248
IGFzc2lt 8249
IHJlYXI= 8250
IFN0cg== 8251
IGNyYXNo 8252
INC90LDRiA== 8253
0LjRgtGB0Y8= 8254
IGNvbG9u 8255
IFVudA== 8256
IENl 8257
IGFjaWQ= 8258
6Zc= 8259
IGtpdA== 8260
aWJpbGl0aWVz 8261
dXRv 8262
IHZhbHVhYmxl 8263
bGlzdA== 8264
IHBhcnRpZXM= 8265
IE1t 8266
IGNvbG91cg== 8267
IGNoYW0= 8268
IHN0ZWVs 8269
IEltcA== 8270
IGZ1bmRz 8271
IEROQQ== 8272
IEtlbg== 8273
aW5kZQ== 8274
7ZW07ISc 8275
44OD 8276
IEhhcHB5 8277
IFVzZQ== 8278
IExpZ2h0 8279
IGxpcA== 8280
IGF1dGhvcml0eQ== 8281
IExvbmc= 8282
IElyYW4= 8283
IGVsbA== 8284
IGNvb3JkaW4= 8285
IHN1Ym0= 8286
IHJlY29yZGVk 8287
0YPRiA== 8288
IGRlbHRh 8289
IHJlZm9ybQ== 8290
IFN0aWxs 8291
IG9wcG9u 8292
IGFsbG93aW5n 8293
IHBhdHRlcm5z 8294
IGxldHRpbmc= 8295
IHNsZWVwaW5n 8296
T2theQ== 8297
IHBpenph 8298
IMWb 8299
INC00L7Quw== 8300
IHRhbGVudA== 8301
ZW5zaW9ucw== 8302
IGVudmlyb25tZW50YWw= 8303
IHByb2Zlc3Nvcg== 8304
IHNob3Rz 8305
IGNvbnRhaW5z 8306
dWdhcg== 8307
eW8= 8308
j5k= 8309
IHNlcXVlbmNl 8310
zrnOsQ== 8311
YWRlcg== 8312
6aA= 8313
0LDRhw== 8314
2YbYpw== 8315
IElr 8316
IHRvdXM= 8317
dXJpZXM= 8318
IHBvdW5kcw== 8319
IGV4dGVybmFs 8320
aW1lbnRz 8321
IHZyYWltZW50 8322
7Iuk 8323
IGhhcHBpbmVzcw== 8324
IHByemU= 8325
ZXN0aWM= 8326
IGVzdGFibGlzaA== 8327
IEZsb3I= 8328
IHJpZw== 8329
IGhvbmV5 8330
IHB1bA== 8331
IHN5bXB0b21z 8332
IGJyb3dz 8333
0LXQu9C4 8334
IM+Ezr8= 8335
IHNoaXJ0 8336
IFRlY2hu 8337
IFByb2dyYW0= 8338
0LXQvNGD 8339
IHVwc2V0 8340
IGd1ZXN0 8341
YnVyZw== 8342
IHVubGlrZQ== 8343
IHNvbWV3aGF0 8344
IGhhbmdpbmc= 8345
YWU= 8346
IHJ1bQ== 8347
IHBob3RvZ3JhcGg= 8348
IExp 8349
5Zue 8350
IHN0YWJsZQ== 8351
IHZvbHRhZ2U= 8352
IEVsbA== 8353
IGVudHJlcHJlbmU= 8354
dXNlcw== 8355
YXNzZW4= 8356
rLg= 8357
IOunjuydtA== 8358
IGdob3N0 8359
IHNhZ2Vu 8360
IGNvbWJhdA== 8361
IGfDtnI= 8362
IENhcA== 8363
IHPDo28= 8364
IEthdA== 8365
IGZvcm1h 8366
IHN1bW0= 8367
IG1hcmNo 8368
IHZhc3Q= 8369
w7xr 8370
IGNvbW1pdG1lbnQ= 8371
aW1vcw== 8372
TGV0 8373
IGRlZGljYXRlZA== 8374
aXN0ZQ== 8375
bGF5 8376
6YCZ5qij 8377
IHRvcGljcw== 8378
IG1hY2hpbmVz 8379
IFBhcmlz 8380
IOydtOufsA== 8381
IG1pbmk= 8382
IG1hcmtldHM= 8383
IGtv 8384
zrQ= 8385
dmlsbGU= 8386
IGdvb2RuZXNz 8387
IGZyYW1ld29yaw== 8388
dWx0dXJl 8389
IGJhc2tldA== 8390
ZXNzYQ== 8391
0LDRhtC4 8392
dXN0ZXI= 8393
IOq5 8394
5L2G 8395
IGV4dGVudA== 8396
IE1lbnNjaGVu 8397
IGNvbnNpc3RlbnQ= 8398
IGF1dG8= 8399
cmlw 8400
IG1lcmU= 8401
4K+I 8402
0ZQ= 8403
IGVsbGU= 8404
jIDr 8405
b2tlbg== 8406
IHB1bGxpbmc= 8407
IGNvdw== 8408
b3V0aGVybg== 8409
IG1lZXRpbmdz 8410
IGNhZGE= 8411
0L3Ri9C8 8412
aWVudGU= 8413
IGJhc3Q= 8414
YW5pbmc= 8415
IGZvY3VzaW5n 8416
cm9hZA== 8417
IHJvb2Y= 8418
IFByb2Zlc3Nvcg== 8419
IFNQ 8420
0YDQsNC3 8421
IG5vb2Q= 8422
IDQwMA== 8423
IOydtOygnA== 8424
7J6I 8425
IE1vdW50 8426
0LXQudGH0LDRgQ== 8427
INeQ 8428
V2h5 8429
154= 8430
xLFuZGE= 8431
IHBvc2l0aW9ucw== 8432
w6htZQ== 8433
548= 8434
INC00YDRg9Cz 8435
aXlvcg== 8436
IHBhc3Npbmc= 8437
IGFzc2VtYg== 8438
IHNtb2tl 8439
IHRpbA== 8440
IG11c2V1bQ== 8441
0JQ= 8442
IFBlcnNvbg== 8443
0L3QuNC8 8444
bGVpY2g= 8445
IGludGVudA== 8446
IHNxdWU= 8447
IGNyYWZ0 8448
7IiY 8449
b3JzdW4= 8450
IDE1MA== 8451
IGJyb3RoZXJz 8452
dm9y 8453
IFNwZWFrZXI= 8454
aWNpYW5z 8455
IG9mZmljZXI= 8456
IGnDp2lu 8457
INGC0LXQsQ== 8458
IHNjcmF0Y2g= 8459
IGdlbmVyYXRl 8460
eWk= 8461
IGVtb3Rpb25z 8462
YXVz 8463
7LmY 8464
NDU= 8465
IExpbms= 8466
IFJlYWw= 8467
IGF0ZQ== 8468
INC90LDQtA== 8469
IG5hdGl2ZQ== 8470
4buH 8471
xLF5 8472
IGVub3Jt 8473
IGJsb2Nrcw== 8474
IGZhY2Vz 8475
YWNj 8476
aXZlbmVzcw== 8477
IGluY2hlcw== 8478
dWlz 8479
aGVpdA== 8480
IHN0cmVldHM= 8481
IHByb2JhYmlsaXR5 8482
YXNp 8483
IGltcGw= 8484
IOCk 8485
dXJkYXk= 8486
IGZhdXQ= 8487
b215 8488
IHBpcA== 8489
IGlsbHVzdA== 8490
4K6v 8491
IEp1bg== 8492
IGx5aW5n 8493
OTk= 8494
IG1lbW9yaWVz 8495
IHByYWN0aWNhbA== 8496
aWFuYQ== 8497
b25jZXM= 8498
IHZpZXdlcnM= 8499
IFRob21hcw== 8500
5ow= 8501
IEdpcmw= 8502
IFdoZXRoZXI= 8503
IGlubm92YXRpb24= 8504
IGRpc2FwcG9pbnQ= 8505
TXk= 8506
IHdpbm5lcg== 8507
IGln 8508
IHJhdGlv 8509
IEJsdWU= 8510
IFN1Yg== 8511
IGRvY3VtZW50cw== 8512
IGZvcm11bGE= 8513
IOup 8514
0Yo= 8515
IGFwcGVhcmVk 8516
dmFy 8517
YW5kb24= 8518
IHNwcmF5 8519
bWFr 8520
IFFVRVM= 8521
S0U= 8522
IHdlZGRpbmc= 8523
UmU= 8524
0LDRgtGM0YHRjw== 8525
IHVubw== 8526
IGdhbGw= 8527
7YSw 8528
Y2lv 8529
Y2Vycw== 8530
INC80L3QtQ== 8531
IHBlcHBlcg== 8532
44GX44Gf 8533
IEZlYnJ1 8534
IGFsdGVybmF0aXZl 8535
IGZ1 8536
IEJhc2ljYWxseQ== 8537
IFNtaXRo 8538
IGdhdGU= 8539
IFRhbQ== 8540
IFdoYXRldmVy 8541
IGFwcHJveGlt 8542
IGNvbmNlcnQ= 8543
IGp1aWNl 8544
IEVzcGVjaWFsbHk= 8545
IGR5bmFtaWM= 8546
UXU= 8547
b25kZXI= 8548
aXZlcnk= 8549
IGJhbmc= 8550
IHJ1bA== 8551
IFBhcnR5 8552
IHNjaG9sYXJz 8553
IGNyeWluZw== 8554
asSF 8555
0KI= 8556
IFFVRVNUSU9O 8557
cmlk 8558
IGFjY3VyYXRl 8559
w6dv 8560
IENvb2w= 8561
Y29pbg== 8562
IOyDgQ== 8563
IEZv 8564
IHByw7M= 8565
IFJvbWFu 8566
INCf0YA= 8567
IGNoZWNraW5n 8568
Pyc= 8569
IGF0dGFjaGVk 8570
IElzbGFt 8571
IGV4cGVydHM= 8572
16c= 8573
IENvbnN0 8574
0YDQsNC9 8575
IHNoYWRvdw== 8576
IGRlbGF5 8577
0JI= 8578
IG9yaWVudA== 8579
64I= 8580
ZWxsZW4= 8581
IGFzw60= 8582
0LrQuNC5 8583
IGhpc3RvcmljYWw= 8584
IHVuY29t 8585
b21w 8586
aG0= 8587
IGJpbA== 8588
IHBsYW5uZWQ= 8589
IFVuZm9ydHVuYXRlbHk= 8590
IFdpbmRvd3M= 8591
2LQ= 8592
IGVuY291bnRlcg== 8593
IOyDneqwgQ== 8594
IHJlZ2FyZGluZw== 8595
YXJyYXNz 8596
IHJlY292ZXJ5 8597
IEh1cg== 8598
IEVtcA== 8599
IHPDrQ== 8600
7ZWY6rKM 8601
IGRlZmVuZA== 8602
IGNldA== 8603
YXNzZQ== 8604
64uo 8605
b2tlcw== 8606
IHJlbW90ZQ== 8607
INiz 8608
IGFydHM= 8609
aXNjbw== 8610
YXVjb3Vw 8611
IE1leGljbw== 8612
INC/0L7QvA== 8613
IGNob3Nlbg== 8614
ZW1hdA== 8615
b2Rpbmc= 8616
IGZsb3dlcg== 8617
c3RhbmRpbmc= 8618
IEFzc29jaQ== 8619
dW1teQ== 8620
SUxM 8621
IGNhbWVyYXM= 8622
5YaN 8623
IOaIkQ== 8624
IEFyYWI= 8625
IFN1bQ== 8626
IHRlZ28= 8627
IGNyaW1pbmFs 8628
aWZvcm0= 8629
IHN0YWNr 8630
7ISx 8631
IERvbmFsZA== 8632
IE9sZA== 8633
IGR1c3Q= 8634
IEpvc2U= 8635
IGhlbQ== 8636
IGluY3JlYXNlcw== 8637
b3N0YQ== 8638
IGR5aW5n 8639
IFJpdmVy 8640
IG1vaXN0 8641
0YLQvtCy 8642
YXJlcw== 8643
IGRpc2NpcGw= 8644
cmFpdA== 8645
IEhhcw== 8646
eWdlbg== 8647
IFRyZQ== 8648
IOu0 8649
IGxhbmd1YWdlcw== 8650
IEhlbg== 8651
IDM2 8652
IERpc25leQ== 8653
aW50cw== 8654
IGFsZ28= 8655
IGZvb2Rz 8656
IHNldHVw 8657
bGFu 8658
IGVmZmVjdGl2ZWx5 8659
IHdoZXJldmVy 8660
5pyA 8661
IHVudGVy 8662
Zm9ybWF0aW9u 8663
IGhpdHM= 8664
IHByaW5jaXBsZQ== 8665
IHRhc3Rlcw== 8666
p4g= 8667
IHRyZWF0ZWQ= 8668
IHJlc29sdXRpb24= 8669
IHByaXZpbGU= 8670
IElQ 8671
67A= 8672
IHRlcnJpdA== 8673
IHBvd2Vycw== 8674
IO2D 8675
IFZpY3Q= 8676
IGJvdGhlcg== 8677
IENoYWly 8678
IG11c2NsZQ== 8679
IHNhbGU= 8680
IGRlY2VudA== 8681
IGNvdXA= 8682
IFNxdQ== 8683
IGNvYXN0 8684
IHJvZA== 8685
IEZyYW5j 8686
IGJhdGhyb29t 8687
IHNob3BwaW5n 8688
INC80L7QttC10YI= 8689
IGnFnw== 8690
IFN0YXk= 8691
Z3JhZGU= 8692
IGZvcm1lZA== 8693
IGJhxZ8= 8694
IGJyaWxs 8695
am91cg== 8696
7ZY= 8697
5Zug 8698
d2ll 8699
aWNhdGU= 8700
IOKAi+KAiw== 8701
IE5vcm0= 8702
4KU= 8703
IG1haW5seQ== 8704
IFNwYWNl 8705
IHRyZW1lbmQ= 8706
aXRp 8707
4K61 8708
VVQ= 8709
TXVzaWM= 8710
IEZlYnJ1YXJ5 8711
IGNvbnRyYXN0 8712
5a+5 8713
ZXN0aW5n 8714
IM60 8715
aW5naW5n 8716
INmG 8717
c3Nlbg== 8718
IEhvbWU= 8719
IHNoZWxs 8720
IEhheQ== 8721
IGFsbGVy 8722
IEFw 8723
IFdlc3Rlcm4= 8724
IFdvcmQ= 8725
IFBMQVk= 8726
IOuF 8727
IEFxdQ== 8728
IGVudHJ5 8729
IGxhdW5jaGVk 8730
IE1lbQ== 8731
IFBvdXI= 8732
IHp3ZQ== 8733
IFNvbWVvbmU= 8734
aW5nZQ== 8735
IFByb2I= 8736
bWJsZQ== 8737
IFJlbA== 8738
dXJ1 8739
IHJoeQ== 8740
IGdpZw== 8741
IGVuZ2FnZW1lbnQ= 8742
w7zFnw== 8743
44KH 8744
IG9mZmVyaW5n 8745
d2hlbA== 8746
IGFjdG9y 8747
IOWwjQ== 8748
QVBQ 8749
d2VzdA== 8750
IFJveQ== 8751
IHJldHVybmVk 8752
IHNpbHZlcg== 8753
cmF0aW5n 8754
IGVzdGFy 8755
IHNrZQ== 8756
IHRp 8757
aWNhdGlvbg== 8758
IGFubm95 8759
IGRlZXBseQ== 8760
7Jqp 8761
IG5hdMO8cmxpY2g= 8762
RUxM 8763
IENhdGg= 8764
IHJhaWw= 8765
0L3QvtCy 8766
IHByYXllcg== 8767
Y29s 8768
R0I= 8769
INCi0LDQug== 8770
IGdsYQ== 8771
IFdhdGVy 8772
0Y/RgtGM 8773
IE5vbg== 8774
w7R0 8775
YWdlcnM= 8776
IGh1Zw== 8777
IGRvY3RvcnM= 8778
YW5jaW5n 8779
IFRhbGs= 8780
emluZw== 8781
IGhhZG4= 8782
IGx1aQ== 8783
IGF0w6k= 8784
IOq3uOumrOqzoA== 8785
6rmM7KeA 8786
aWNp 8787
IGluY29ycG9y 8788
IERp 8789
emls 8790
YW55YQ== 8791
qoU= 8792
IMK7 8793
MzU= 8794
IGJlZXI= 8795
IGJlYXVjb3Vw 8796
IE1D 8797
IGVhcnM= 8798
b2dlbg== 8799
IFF1ZXN0 8800
ZWRh 8801
5pys 8802
IFNhdHVyZGF5 8803
IGZhbGxz 8804
c3Rvbg== 8805
Ymxlcw== 8806
IHRodXM= 8807
IOuEpA== 8808
4LmE 8809
IHRoZXJt 8810
IGRpdmVyc2l0eQ== 8811
IHNveQ== 8812
YXp1 8813
aW1w 8814
IHRlbGV2aXNpb24= 8815
6YGO 8816
INep15w= 8817
IHd1cg== 8818
IGVkZ2Vz 8819
IGxlc3NvbnM= 8820
IEF1ZA== 8821
44GX44Gm 8822
dm9pcg== 8823
YW1lbnRv 8824
IGV4cGxhaW5lZA== 8825
INC+0L3QsA== 8826
IHRlbXBz 8827
z44= 8828
VGhleQ== 8829
IHN1cnByaXNpbmc= 8830
0LDQvdC40Y8= 8831
IERyYWc= 8832
6Z2i 8833
IENsZQ== 8834
IG5hbQ== 8835
INC70Y7QtA== 8836
IGhhcmR3YXJl 8837
IHRodW1icw== 8838
IM66zrHOuQ== 8839
IFRvcA== 8840
IMOl 8841
6Zk= 8842
15XXqA== 8843
IOq3uOuemOyEnA== 8844
IEJ1ZGQ= 8845
dGhlcm4= 8846
IGludGVyZXN0cw== 8847
2LA= 8848
IGRldmVsb3BlcnM= 8849
IGhpdHRpbmc= 8850
IG9wcG9zZWQ= 8851
IGhlYXJ0cw== 8852
IEFuZHJvaWQ= 8853
IEhhbmQ= 8854
IHJlcHJlc2VudHM= 8855
Z2xpY2g= 8856
7Yq4 8857
IDMy 8858
IGRvbWlu 8859
IEFubg== 8860
5LiA5LiL 8861
IMOpdMOp 8862
IHpvb20= 8863
IGt0w7NyZQ== 8864
IGFkdWx0cw== 8865
IG9yZGVyZWQ= 8866
IHBpY2tpbmc= 8867
IEhvbmc= 8868
IGZpbG1pbmc= 8869
5oCd 8870
IHNlZWQ= 8871
IEFU 8872
IGNhbGN1bGF0ZQ== 8873
INC60L7Qs9C00LA= 8874
IE9z 8875
aWNpdA== 8876
IHJlbWFpbmluZw== 8877
IHNlZ3U= 8878
w7s= 8879
IOyYpOuKmA== 8880
IGFycml2ZQ== 8881
IGNvbmdy 8882
IGdyYW5kZQ== 8883
IGhlYWx0aGNhcmU= 8884
INC80L7QttC90L4= 8885
U0E= 8886
ZXN0ZQ== 8887
IGF3YXJlbmVzcw== 8888
IHNxdWFyZWQ= 8889
eHR1cmU= 8890
IEJlaW5n 8891
IHNvbGRpZXJz 8892
0YPQsQ== 8893
IHJldm9sdXRpb24= 8894
IHRyYWluZWQ= 8895
ZW5kZW4= 8896
6LA= 8897
IGRhbmNpbmc= 8898
IGluc3RhbGxlZA== 8899
cHJpc2U= 8900
IHZldGVy 8901
IG1lbm9z 8902
bmVsbA== 8903
IEJyb3RoZXI= 8904
IG51bg== 8905
IGltcG9ydGFudGx5 8906
YWxsZWQ= 8907
aWHFgg== 8908
YWJsZWQ= 8909
IFN5c3RlbQ== 8910
IFZvbA== 8911
IGVsZA== 8912
IGVtb3Rpb24= 8913
aWNhbg== 8914
IEJhbms= 8915
aWtlcw== 8916
IHZsb2c= 8917
INCy0L7Qtw== 8918
IHB1ZWRl 8919
7Jik 8920
IHRlZW4= 8921
IHNldmVyZQ== 8922
JSw= 8923
IGNsZWFuaW5n 8924
esSF 8925
l5A= 8926
IFRocm91Z2g= 8927
IFNldA== 8928
RVA= 8929
Ij8= 8930
IE1vdGhlcg== 8931
IGZpZ3VyZWQ= 8932
IG11ZA== 8933
INGW 8934
IE9mZmljZQ== 8935
IHJhdw== 8936
IGRlc3Ryb3llZA== 8937
ZW50YQ== 8938
IGFnZ3Jlc3M= 8939
INC+0YE= 8940
IOuqqOs= 8941
w6TDpA== 8942
IEFS 8943
IGNvcnJlY3RseQ== 8944
5YmN 8945
IHN0aXI= 8946
IGV4dHJhY3Q= 8947
IHZlaGljbGVz 8948
6ZaL 8949
IFJ1bg== 8950
INCy0YDQtdC8 8951
IHBhcmFsbGVs 8952
IGxhZw== 8953
anU= 8954
IGRhcmU= 8955
IE1vdA== 8956
b25v 8957
IGJlaW5ncw== 8958
IHN0cm8= 8959
IGV4Y3VzZQ== 8960
IGFscGhh 8961
IGFza3M= 8962
IHBvY2tldA== 8963
Li4uPw== 8964
IGtpdGE= 8965
w7xt 8966
IGFwcGVhcmFuY2U= 8967
b3JkYW4= 8968
IGluc2VydA== 8969
INC90LDRhw== 8970
m2k= 8971
IHRlbXBv 8972
IGZhY2lsaXR5 8973
IHZpc2libGU= 8974
5ZI= 8975
IFNjaWVuY2U= 8976
dXJvcw== 8977
INmB2Yo= 8978
IFZhbg== 8979
IHRlbnNpb24= 8980
IO2VoA== 8981
IGRlbGl2ZXJ5 8982
IHN0aW0= 8983
IHN1cnZleQ== 8984
IEdyYQ== 8985
IGJvbA== 8986
5qA= 8987
IHdlaXRlcg== 8988
w59lbg== 8989
5LiA5YCL 8990
IHByb2NlZWQ= 8991
IGltcHJlc3NpdmU= 8992
IFZvYw== 8993
aW91c2x5 8994
INC00LA= 8995
aGFsZQ== 8996
b2No 8997
IGdsdWU= 8998
cGhldA== 8999
Y29udA== 9000
IGZpdHM= 9001
IGJveGVz 9002
IGNvbnRyb2xz 9003
IENoaWxk 9004
IHNjZW5hcmlv 9005
IHRyb3A= 9006
IHByb2Nlc3Npbmc= 9007
INGC0L7Qu9GM0LrQvg== 9008
IGJpcmRz 9009
IENoaWM= 9010
INC90LDQvw== 9011
IDIwMTM= 9012
IG3DvHNzZW4= 9013
IEphZw== 9014
IHPEhQ== 9015
IHBlcmNl 9016
cmVo 9017
IEZvcmU= 9018
IGNvbmZ1c2Vk 9019
YWlyZQ== 9020
IGFjY29tcGxpc2g= 9021
IGNhc2E= 9022
Y2xvY2s= 9023
IGluZmx1ZW4= 9024
IFJP 9025
IGJvbmU= 9026
aWNpYW4= 9027
IFND 9028
IHN0cmF0ZWdpZXM= 9029
Z2g= 9030
0LTRgw== 9031
IGl0dQ== 9032
IHBlcnNvbmFsaXR5 9033
IGJhcmR6bw== 9034
IGFjY2VwdGVk 9035
IHN0b20= 9036
aWV2 9037
IEhpc3Q= 9038
IEF1cw== 9039
IOuwlOs= 9040
QVRPUg== 9041
5oSP 9042
b2ly 9043
IG1hZ2F6 9044
IGV4cGxhbg== 9045
IGNvcm4= 9046
IGlscw== 9047
IGNpcmN1aXQ= 9048
IGdheQ== 9049
aG9w 9050
44KD 9051
IGVxdWl2YWw= 9052
IGRpZXNlcg== 9053
ZXJ2ZXM= 9054
Y29tZXM= 9055
a2xpY2g= 9056
IOuVjOs= 9057
YWJldA== 9058
IGV4aGE= 9059
IG1hbm5lcg== 9060
IOKZquKZqg== 9061
w6lj 9062
w6Rs 9063
IGNvbmZpcm0= 9064
IGVudGVyZWQ= 9065
ZW1wbG8= 9066
IEZhcg== 9067
IG/DuQ== 9068
ZXNzaW9ucw== 9069
IG51cnM= 9070
IGVudMOjbw== 9071
IGFiYW5kb24= 9072
bGlmZQ== 9073
IHdpcw== 9074
TmFycmF0b3I= 9075
IOyWtA== 9076
VGhlcmU= 9077
IFJhbQ== 9078
YXN0ZQ== 9079
IGF0dHJpYg== 9080
IEF5 9081
IG1lc21v 9082
IM69zrE= 9083
6as= 9084
ZW5zZXM= 9085
IGNyb3A= 9086
INC30LTQtdGB0Yw= 9087
IFVudGls 9088
c3RlaW4= 9089
IG92ZW4= 9090
IHN1c3BlY3Q= 9091
aGV0 9092
IHB1aXM= 9093
IGNhcnJpZWQ= 9094
w6ln 9095
IERldg== 9096
ZW1z 9097
cmVlbnM= 9098
YmVycnk= 9099
IHRlbXBs 9100
IEJpdA== 9101
IHZhcmlhYmxlcw== 9102
IG92ZXJ3aGVs 9103
zrzOtQ== 9104
IGluaXRpYWxseQ== 9105
7JWY 9106
b3RoaW5n 9107
0LXRgtGM 9108
IEhpbGw= 9109
IGRlcGFydA== 9110
IG15c3Q= 9111
YXp6 9112
IGZsdWlk 9113
IERD 9114
IGNsaW5pY2Fs 9115
IFJ5YW4= 9116
IEZsb3JpZGE= 9117
IFRhaw== 9118
IGFueGlldHk= 9119
YnJv 9120
IGNpcmN1bXN0YW5jZXM= 9121
INmD 9122
IGV4aXN0ZW5jZQ== 9123
IHRvbmc= 9124
IDIwMTI= 9125
IFNlY3JldGFyeQ== 9126
IHNwaWN5 9127
IFso 9128
IFdpdGhvdXQ= 9129
IGZhY3Rz 9130
IHRvbnM= 9131
QXBw 9132
IFN0YW5k 9133
IGxpZXM= 9134
IEFE 9135
d2lu 9136
z4TOtQ== 9137
YXBwbGF1c2U= 9138
SVA= 9139
c3Rh 9140
IFN1cA== 9141
cGhvbmVz 9142
npE= 9143
cGll 9144
IFBvdA== 9145
IE5P 9146
6LW3 9147
INee 9148
INCU0LA= 9149
aWNhcw== 9150
IEly 9151
IHB1c2hlZA== 9152
IHVuY2xl 9153
INmF2YY= 9154
IGxvbg== 9155
IHByaW5jaXBsZXM= 9156
IEludGVybmF0aW9uYWw= 9157
IMOW 9158
xb4= 9159
IHNheWE= 9160
IOqzoA== 9161
IHJpYg== 9162
IHBhc3Rl 9163
IHdhcm5pbmc= 9164
IG11c2ljYWw= 9165
IGFncmVlZA== 9166
0L7RgNC8 9167
IGdhcmxpYw== 9168
IG94eWdlbg== 9169
7JiI 9170
QWw= 9171
IOunng== 9172
ZWxpbmVz 9173
TEFVU0U= 9174
576O 9175
Z3lwdA== 9176
R0U= 9177
Y2tlcg== 9178
dHU= 9179
IHNoZWw= 9180
IHN0YXllZA== 9181
INCz0L7QtA== 9182
IGxhcHQ= 9183
IE1hcnRpbg== 9184
IGludml0ZWQ= 9185
IGNvbmZpcg== 9186
IGVtYmFycmFzcw== 9187
YWNpb25lcw== 9188
IENhbXA= 9189
IGhvbGRz 9190
YXh5 9191
IGRpdmU= 9192
dWNrbGVz 9193
IGJvb3N0 9194
IHfDvHI= 9195
c3RhbA== 9196
INGA0LDQsdC+0YI= 9197
IGTDqWM= 9198
IG9mZmljZXJz 9199
IOyVhOs= 9200
b2xvZ2lzdA== 9201
157X 9202
IHNlZWRz 9203
IGJ1ZmY= 9204
IHVwZGF0ZXM= 9205
44KP 9206
ZGVk 9207
IGZyaWVuZGx5 9208
IGNvdW5jaWw= 9209
IFByb2JhYmx5 9210
IHBpYW5v 9211
IHJlZHVjZWQ= 9212
z4TOsQ== 9213
IGF1dGhlbnQ= 9214
IGV4cGxvcw== 9215
cGFzcw== 9216
IEhpdA== 9217
anVk 9218
IE5hdg== 9219
b21p 9220
IGNvbW1pc3Npb24= 9221
IGd5bQ== 9222
0J8= 9223
IHBvbg== 9224
0YDQvtGB 9225
IGludGVyZmFjZQ== 9226
IHN0cnVjdHVyZXM= 9227
IEplbg== 9228
IHlvaw== 9229
IG1ldQ== 9230
7KeA66eM 9231
bmVk 9232
IFdpZQ== 9233
IGlkZW50aWZpZWQ= 9234
IGNoYW5uZWxz 9235
xLFuYQ== 9236
IHBoaWxvc29w 9237
a2VpdA== 9238
IGJpdHM= 9239
ZW50ZXM= 9240
IGZyYWc= 9241
IEtpbmQ= 9242
IGRvY2g= 9243
IHNuZQ== 9244
aW5kaW5n 9245
IEpld2lzaA== 9246
0L7RgNC+0Yg= 9247
IGZ1ZQ== 9248
5pa5 9249
IO2P 9250
IG3EsQ== 9251
IGtlaW5l 9252
IGxvY2F0aW9ucw== 9253
55So 9254
IG1ldGVy 9255
IGJlZWY= 9256
44GY 9257
IG1hbmlw 9258
IHNvbm8= 9259
enpsZQ== 9260
57Y= 9261
IHBlcw== 9262
IGhvcnJpYmxl 9263
IFNu 9264
IGZhY3Rvcnk= 9265
IGZpZnRo 9266
IGNvb2tlZA== 9267
IG1vb2Q= 9268
IHZlbG9jaXR5 9269
IG9ibGln 9270
IGNvbm5lY3Rpb25z 9271
xJ9pbQ== 9272
IOqztQ== 9273
IGRvbWFpbg== 9274
IGFwcGx5aW5n 9275
IHJpZGlj 9276
IGNlbA== 9277
IGNoaWxkaG9vZA== 9278
IFRlc3Q= 9279
cmF0dWxhdGlvbnM= 9280
IFZpcmdpbg== 9281
IENFTw== 9282
INC/0Ls= 9283
IGFsZ29yaXRobQ== 9284
IGludGVyYWN0aW9u 9285
YWdh 9286
IGtpZGRpbmc= 9287
IHRvbWF0bw== 9288
IGNvbnRpbnVpbmc= 9289
bGFk 9290
c3RyZWFt 9291
0L7QttC1 9292
IOyYgQ== 9293
0LXQu9C+0LI= 9294
QkE= 9295
IG5hcA== 9296
IE5vYm9keQ== 9297
IHRodW1i 9298
IE9O 9299
IHJ1c2g= 9300
RFI= 9301
IHN0cmlrZQ== 9302
IGV2b2x1dGlvbg== 9303
aWNoZQ== 9304
IOy7 9305
IOq3uOufsA== 9306
2KfYqg== 9307
IGFr 9308
IHdpbmRvd3M= 9309
IGV4Y2Vzcw== 9310
44Gq44GE 9311
IGNvbmNsdWQ= 9312
IGVwaXNvZGVz 9313
IHN0cnVnZ2xpbmc= 9314
IERhdA== 9315
nbzr 9316
IGtleXM= 9317
IGtsZQ== 9318
5p6c 9319
IHZlZ2V0YWJsZXM= 9320
eXN0ZW0= 9321
w6puY2lh 9322
cmljaw== 9323
IHJldmVudWU= 9324
IEhhdw== 9325
IGxhbg== 9326
YW50ZXM= 9327
aW5peg== 9328
44GT44KM 9329
0LjRgdGC 9330
IHN1cA== 9331
qbTshJw= 9332
IG1vbWVudG8= 9333
aXN0bw== 9334
44Gk 9335
IEVyaWM= 9336
aW9ycw== 9337
YmFq 9338
IGludHJvZHVjdGlvbg== 9339
aXJ0eQ== 9340
IGRlY2s= 9341
cmVhbA== 9342
IE1hcmlv 9343
IGxvdmluZw== 9344
4LiU 9345
IHN1cHBvcnRz 9346
0LjRh9C10YE= 9347
IGluY2lkZW50 9348
dXRjaA== 9349
dXY= 9350
IGJvb20= 9351
0LXRgNGM 9352
INC90YPQtg== 9353
IGNvbWJpbmVk 9354
IExpbg== 9355
MjM= 9356
b3JhdGlvbg== 9357
bnRl 9358
IHNvcg== 9359
IGRpcnR5 9360
aWZlcg== 9361
IEFQSQ== 9362
IGNvbGxhYm9yYXRpb24= 9363
aWFibGU= 9364
IHByaW9yaXR5 9365
IEFsZQ== 9366
IFByaW4= 9367
IEV4Yw== 9368
IHZhaXM= 9369
IGdyYW4= 9370
IHN0b29k 9371
IHJlY3J1 9372
IE11cg== 9373
ZXNpcw== 9374
YXNw 9375
IGxvY2tlZA== 9376
IFBlcm8= 9377
IEhhcnJ5 9378
IHR1ZG8= 9379
IFRlbg== 9380
2LU= 9381
Zm9yY2VtZW50 9382
KSk= 9383
b2xp 9384
IOyduA== 9385
IHN1cHBs 9386
IGNyb2NoZXQ= 9387
IHBoZW5vbWVu 9388
bG9z 9389
YXRoYW4= 9390
IFN1cHA= 9391
IGVtYnI= 9392
IGJlaw== 9393
IFplaXQ= 9394
Z2VuZA== 9395
IHJvb21z 9396
qr0= 9397
VkVS 9398
bnljaA== 9399
IGRvbnQ= 9400
IGNhYmlu 9401
IGFjY291bnRz 9402
IEVhc3Rlcg== 9403
15XXnA== 9404
44Or 9405
IGZhY2lsaXRpZXM= 9406
YmVpdA== 9407
IGxpbmtlZA== 9408
IEdlcg== 9409
IHByb2dyYW1taW5n 9410
b3RpYw== 9411
IGRyYW1h 9412
IDI5 9413
IO2B 9414
IGluc3RydWN0aW9ucw== 9415
IGltcG9ydGFudGU= 9416
IHdhdmVz 9417
IGFpZA== 9418
Q0s= 9419
6rKg7Iq164uI64uk 9420
IE1pcg== 9421
IHRpZA== 9422
IEhvdA== 9423
IGFycmFuZ2U= 9424
IEJhYnk= 9425
IHRhY2s= 9426
INGJ 9427
7Z0= 9428
IHZlcnRpY2Fs 9429
IGhlZWw= 9430
IEN1dA== 9431
IG5hcnJvdw== 9432
IEFyaQ== 9433
IGtuZWU= 9434
IEJyYXppbA== 9435
IEZpdmU= 9436
IHBvc3RlZA== 9437
VUQ= 9438
IHJvbGxpbmc= 9439
zrg= 9440
IGNsYWltcw== 9441
IElucw== 9442
T0s= 9443
44GE44GG 9444
dWlu 9445
IEluc3RpdHV0ZQ== 9446
IGludGVuc2U= 9447
aWFy 9448
IE5pY2s= 9449
IHNlbGVjdGlvbg== 9450
IGxlZ2VuZA== 9451
IHVuaWZvcm0= 9452
w7pu 9453
IHN0dWRpZWQ= 9454
5aSq 9455
INCl 9456
IOyVjA== 9457
Z2Vycw== 9458
IGRvdw== 9459
IENT 9460
IGFnZW50 9461
IEF1Zg== 9462
6Ka6 9463
IGpvZw== 9464
IGFpcmNyYWZ0 9465
64uY 9466
IHZpdA== 9467
dWxz 9468
IHNlZ21lbnQ= 9469
IG9yZGVycw== 9470
IENsYXNz 9471
IGFwb2xvZw== 9472
IHBsYXRmb3Jtcw== 9473
IG15dGg= 9474
0LDQttC1 9475
IEJvb2s= 9476
IHNlbnNpdGl2ZQ== 9477
INC/0L7Qu9GD0Yc= 9478
IGRhbWl0 9479
IENhcHQ= 9480
c29sZQ== 9481
IGFyY2hpdGVjdHVyZQ== 9482
IFdpbA== 9483
IGluaGVy 9484
Y2Fw 9485
IEJveQ== 9486
5qyh 9487
IGJ1cm5pbmc= 9488
IFB1YmxpYw== 9489
IGJlaGFsZg== 9490
IOychA== 9491
IHRoZXJhcHk= 9492
dWJzY3JpYmU= 9493
IGludm9sdmU= 9494
IGV4cG9zZWQ= 9495
acWf 9496
5Lus 9497
w6p0cmU= 9498
IHRvaWw= 9499
IHNpbms= 9500
cGly 9501
5YM= 9502
SUk= 9503
IGFnZW5jaWVz 9504
IHE= 9505
IERvd24= 9506
YXVm 9507
IOunmw== 9508
44O744O7 9509
IHByb2M= 9510
b2tlZA== 9511
IHN0b3Jlcw== 9512
cG93ZXI= 9513
IFRoaW5ncw== 9514
IGFjY2Vzc2libGU= 9515
IHRlxbw= 9516
IEVkdWM= 9517
IHNwZWFrZXJz 9518
IFNhcmFo 9519
lJQ= 9520
IGRpdmVyc2U= 9521
7J6W 9522
IFVsdA== 9523
w6B5 9524
IENoaWNhZ28= 9525
U2hl 9526
YXRoeQ== 9527
IGVuYWJsZQ== 9528
IHRyYWRpbmc= 9529
IG11c2NsZXM= 9530
5ps= 9531
IENhcmU= 9532
IFVy 9533
IFNjb3Q= 9534
IHBocmFzZQ== 9535
RU5U 9536
IOqyvQ== 9537
IEphYw== 9538
cGFjaw== 9539
IGRldGVybWluZWQ= 9540
w7xuZA== 9541
IG5lZ290aQ== 9542
IHZpZMOp 9543
IHJveg== 9544
IFN1cw== 9545
IHJpZGluZw== 9546
aG1lbg== 9547
IERlZg== 9548
IENyZQ== 9549
44K5 9550
IFdhbGw= 9551
aWdhbg== 9552
IHNlbXByZQ== 9553
0ZbQtA== 9554
IGRyaXZlbg== 9555
IGZvb3RhZ2U= 9556
IGZvbmQ= 9557
IFdheQ== 9558
w6Rt 9559
IE9iYW1h 9560
IFNlcnZpY2U= 9561
IDc1 9562
IERhcms= 9563
IOq3vOs= 9564
IENhdA== 9565
2Lc= 9566
6Yw= 9567
IGp1Zw== 9568
IGV0d2Fz 9569
IGJyZWF0aGluZw== 9570
4buD 9571
5YW2 9572
IFdlYg== 9573
5LmL 9574
6LWw 9575
IGZvaXM= 9576
IGxpZ2h0aW5n 9577
IERB 9578
IG9ic3Q= 9579
IGxldXI= 9580
54++ 9581
IEVneXB0 9582
IEFybXk= 9583
aWNpZGU= 9584
0LDRgtC4 9585
IOuLpOs= 9586
IGFwYXJ0bWVudA== 9587
IGNoaWVm 9588
IFdlZA== 9589
IG5ldHdvcmtz 9590
IGJhdHQ= 9591
5rg= 9592
IEx1Yw== 9593
IG5pY2VseQ== 9594
IHZlcmI= 9595
4Li0 9596
7LY= 9597
b3NpdA== 9598
IHJldmVhbGVk 9599
IHRhdA== 9600
IHRpZWQ= 9601
4buB 9602
IGFuaW1hdGlvbg== 9603
IHJvbGVz 9604
7Iqk7Q== 9605
IHZlcnNpb25z 9606
0YfQuNGC 9607
IHRhc2tz 9608
r7w= 9609
IHJlc2M= 9610
c2hl 9611
IGxvb3Nl 9612
IGPhuw== 9613
IGNvaXNh 9614
IGFsZXJ0 9615
IG5pbg== 9616
IFNBTQ== 9617
IHRyYWJhag== 9618
aXJ1cw== 9619
VEg= 9620
xqE= 9621
b2dldGhlcg== 9622
IFRhaQ== 9623
IGZpZ3VyZXM= 9624
INeQ16o= 9625
IGNyZWVw 9626
IGludmVzdGlnYXRpb24= 9627
IHJlY29tbWVuZGVk 9628
IEFr 9629
IHJlc2lkZW50cw== 9630
0YHRgtCy0L4= 9631
c2VjdA== 9632
0LDQvdC40LU= 9633
IG1pbmRz 9634
dWluZw== 9635
5bE= 9636
b3dpbmc= 9637
IG5vZw== 9638
IHJheg== 9639
2KfYsQ== 9640
IHF1b3Q= 9641
INC40YU= 9642
IHNlZA== 9643
IGFwcGxhdWQ= 9644
IGNvdmVyYWdl 9645
dm9s 9646
IFJlYw== 9647
xJs= 9648
INCy0YHRkQ== 9649
IGV4cGVjdGluZw== 9650
IG9wZXJhdGU= 9651
IGNvbnZlcg== 9652
IFN1Y2g= 9653
IFJhZA== 9654
IFByaW1l 9655
IHB1cnBsZQ== 9656
IDIwMTA= 9657
IOyViOs= 9658
IGV4ZW0= 9659
IGNvbXBhcmlzb24= 9660
IGxhbmRzY2FwZQ== 9661
IG5laXRoZXI= 9662
IEVo 9663
64U= 9664
IHN0b21hY2g= 9665
IGNhc28= 9666
w6Ju 9667
IHBlcmNlbnRhZ2U= 9668
d2ljaA== 9669
aXRhbg== 9670
IGts 9671
IGV4cGFucw== 9672
INin2YTZhQ== 9673
IG9jY2FzaW9u 9674
cmV0cw== 9675
aWduaW5n 9676
IGtpbG9tZXQ= 9677
6Lef 9678
IGd1c3Q= 9679
Y3pl 9680
IHVyYmFu 9681
IGFncmlj 9682
IGFzc2lzdGFuY2U= 9683
IHN1cmY= 9684
aW1ldGVy 9685
IHBldGl0 9686
IGFzc2Vzc21lbnQ= 9687
IG1hbnVhbA== 9688
IGltcHJvdmVk 9689
YnN0 9690
IHBpbG90 9691
IE1hcnM= 9692
IHZpZWxl 9693
IENvbmdyYXR1bGF0aW9ucw== 9694
IGFyZ3Vl 9695
IHdpcmtsaWNo 9696
IGNsaWNraW5n 9697
UklT 9698
IGxvZ28= 9699
IG91dGNvbWU= 9700
IENlbnRyYWw= 9701
IEpp 9702
IGdhbWluZw== 9703
IGNvbnNlcnY= 9704
IHVsdGltYXRl 9705
IFZl 9706
IFdhbA== 9707
YXJv 9708
5oSf 9709
c3Rhcg== 9710
IGNvbnN1bWVy 9711
IHRyYXZlbGluZw== 9712
aW1lcg== 9713
IDEwMDA= 9714
0L3QuNC6 9715
IHByaW5jaXBhbA== 9716
IHNha2U= 9717
0ZbQsg== 9718
IG1vdXNl 9719
YXJpb3M= 9720
IHJlbGF0aW9u 9721
6Ieq 9722
IG1vcmFs 9723
5ZWm 9724
IHRoZXRh 9725
d3k= 9726
IGthbQ== 9727
IGVpZw== 9728
IGdvbGRlbg== 9729
16Q= 9730
IGFtcGw= 9731
IHZ1 9732
c3Ry 9733
cm9ycw== 9734
IHdoZXJlYXM= 9735
aXphcg== 9736
IGFkbWluaXN0cg== 9737
IG7Ds3M= 9738
IFByZXQ= 9739
IEFjYWQ= 9740
YW5naW5n 9741
YmFnZQ== 9742
w6l0YWl0 9743
dXJp 9744
IGhlYWxpbmc= 9745
IHRpcG8= 9746
IG1hcnJ5 9747
0YPQsg== 9748
IGVzdGF0ZQ== 9749
dXU= 9750
7JQ= 9751
IEJlc3Q= 9752
IHN1ZmZlcg== 9753
IDE5NA== 9754
IGJhY3Rlcg== 9755
INCS0L7Rgg== 9756
IE9t 9757
IGR6 9758
6LY= 9759
7KY= 9760
IG9sZHU= 9761
IHBoeXNpY2FsbHk= 9762
IExvdWlz 9763
ZXRpbWU= 9764
Y2FzZQ== 9765
IHBpZXI= 9766
7KCc 9767
dmFu 9768
IGFzc2V0cw== 9769
IOuB 9770
dmV0 9771
0LjQsQ== 9772
IHByb21vdGU= 9773
IGNvbmdyYXQ= 9774
dWVzZGF5 9775
IGR1dHk= 9776
IFZpZGVv 9777
2K4= 9778
IEpvaG5zb24= 9779
a3Rpb24= 9780
IFZvY8Oq 9781
44CL 9782
IGFp 9783
IGFubnVhbA== 9784
IEpvc2g= 9785
aXR0ZQ== 9786
IEpP 9787
IHNsaWRlcw== 9788
IGFuYw== 9789
uYQ= 9790
dGVlbg== 9791
IGNhcnJ5aW5n 9792
bHltcA== 9793
ZWRpbmc= 9794
IGZybw== 9795
IGFkbWl0 9796
cmVy 9797
IG9mZmljaWFscw== 9798
cHRpb25z 9799
Z2Fs 9800
IGhldXRl 9801
IHZvaWNlcw== 9802
IGJhbGxz 9803
IGd1ZXN0cw== 9804
YW5uZXI= 9805
44CK 9806
aXNoZXI= 9807
IE1S 9808
IFJpY2hhcmQ= 9809
IHJvdWdobHk= 9810
bMSx 9811
IHZpY3Rvcnk= 9812
IGFsZ3Vu 9813
IE1ycw== 9814
xZtjaWU= 9815
IFVr 9816
IGV5 9817
IFdhcnM= 9818
IGJyYW5jaA== 9819
YXN0eQ== 9820
IFByaW5jZQ== 9821
0LXQutGC 9822
IHJlY29nbml6ZWQ= 9823
IG11Y2hv 9824
IExlYXZl 9825
Y29ubmVjdA== 9826
IHNwZWxs 9827
IHRvdWNoZWQ= 9828
IGFnZW5kYQ== 9829
6L4= 9830
YXJpYQ== 9831
IEtvbmc= 9832
b2dh 9833
IHBhcmFtZXRlcnM= 9834
64uk6w== 9835
IGluc3RhbnQ= 9836
IHJlZ3Vs 9837
Q29u 9838
IGVkaXRvcg== 9839
IERpc3Q= 9840
IHVua25vd24= 9841
IHB1bmlzaA== 9842
IGV4cGVjdGF0aW9ucw== 9843
IGNyeXB0 9844
IGRpdmlkZQ== 9845
YWtlbg== 9846
IE1lc3M= 9847
IGh5cGVy 9848
IFByb2plY3Q= 9849
aWtp 9850
IGFnb3Jh 9851
IGFidXNl 9852
IGNhdXNpbmc= 9853
IGNvbnZpbg== 9854
IExB 9855
IGNvbmNlbnRyYXRpb24= 9856
IGJyZWFrcw== 9857
dXJlcg== 9858
IGNvbmNyZXRl 9859
IGZvcm1hbA== 9860
IGJldGE= 9861
aXRvcnM= 9862
IENoYW1w 9863
IGhlYWRpbmc= 9864
IEJsbw== 9865
IHByZW5k 9866
IFNlbmF0ZQ== 9867
IGFkdmVudHVyZQ== 9868
b3Nv 9869
IG9wZW5z 9870
IFBMQVlJTkc= 9871
IFNV 9872
dXJlbg== 9873
aWt0 9874
INC70Y7QsQ== 9875
IEZvbGxvdw== 9876
IEJpZGVu 9877
ZWxu 9878
IFNreQ== 9879
ZXRpbmc= 9880
IEV4dA== 9881
0L3Rg9GO 9882
IOyZnA== 9883
IHNocg== 9884
ZWxsYQ== 9885
IERpdg== 9886
IHRyYW5zZm9ybWF0aW9u 9887
IGhvdXNlaG9sZA== 9888
ZXRyeQ== 9889
6KE= 9890
IERlc3A= 9891
IGNvdXJhZ2U= 9892
IHBhcmtpbmc= 9893
IGV0dMOk 9894
Y2Fs 9895
bHlu 9896
IGxhaWQ= 9897
IHRyaWVz 9898
aXJ0cw== 9899
aWdh 9900
IHJlY2FsbA== 9901
aWZpZXI= 9902
z4HOsQ== 9903
IGFhbg== 9904
IGJ1dHRvbnM= 9905
IHJlYWNoaW5n 9906
IOq3vOuNsA== 9907
IHNwYXJr 9908
IFNvY2lhbA== 9909
INC10YnQtQ== 9910
IGNhbmFs 9911
IGNyaXRlcg== 9912
IGt0w7NyeQ== 9913
IHRlbmVtb3M= 9914
gqw= 9915
INC90LXRgg== 9916
IHR1YmU= 9917
YWNsZXM= 9918
0LjRiA== 9919
IGRlxJ9pbA== 9920
IHN0YW1w 9921
IGluZmw= 9922
IGFob3Jh 9923
IHRyYWls 9924
IG1peHR1cmU= 9925
IFJvbGw= 9926
IHJvdXRpbmU= 9927
IGNvdW50eQ== 9928
IGVuam95aW5n 9929
0L3QvtGB0YLRjA== 9930
ZXJlcw== 9931
IHB1cnBvc2Vz 9932
IFNhbnRh 9933
IGJyZWFzdA== 9934
w6RuZw== 9935
IHdyaXRlcg== 9936
5Yw= 9937
0YDQvg== 9938
IG5lbQ== 9939
aWNvcw== 9940
0LDRgdGC 9941
IGRldGFpbGVk 9942
IHJldmVyc2U= 9943
IFJlYWR5 9944
IGRpc3RyYWN0 9945
IEFsb3Jz 9946
dXR0ZXI= 9947
IGRlc2VydmU= 9948
IFJvbg== 9949
0L3QvtC8 9950
IG9ic2Vydg== 9951
IGxvZ2lj 9952
IFB5 9953
IEtldmlu 9954
44Gd44GG 9955
pbQ= 9956
2YrZhg== 9957
IHNrYQ== 9958
IHRhY3Q= 9959
IGhvbGlkYXk= 9960
IGJ1bXA= 9961
INC80L7Qsw== 9962
IGRlaXg= 9963
7YU= 9964
IHdvcnNoaXA= 9965
Q2w= 9966
IHN1Y2s= 9967
INGB0LXQsQ== 9968
IGFwcGxhdXNl 9969
IEVw 9970
INC80L4= 9971
IHBhdGNo 9972
4bqt 9973
IGxhZGllcw== 9974
IGJyb2FkY2FzdA== 9975
IGlsbGVn 9976
IG5hcnJhdGl2ZQ== 9977
b3NzYQ== 9978
QVJSQVRPUg== 9979
IHNhbmc= 9980
IG1vdmVtZW50cw== 9981
IHBhcnRuZXJzaGlw 9982
IG9yZ2FuaXplZA== 9983
IG5vZGU= 9984
ZXN0eWxl 9985
IE1lZw== 9986
IGluZHVzdHJpYWw= 9987
IGdvbA== 9988
IGJvcmluZw== 9989
5Yqg 9990
44GU 9991
IGN1dHM= 9992
IHJlY29u 9993
YXNh 9994
IGltcHJlc3Npb24= 9995
7Jq0 9996
Z2ll 9997
TUE= 9998
hrU= 9999
IGVkaXRpbmc= 10000
cm9udA== 10001
IGZvbGxvd3M= 10002
IEl0YWxpYW4= 10003
0YDQvtC0 10004
IOqwmeydgA== 10005
IOuwqQ== 10006
IHBhcnRpY2xlcw== 10007
IEJvYXJk 10008
15nXqg== 10009
anVu 10010
cm9uaWM= 10011
IGVq 10012
IM+Ezrc= 10013
15XXkw== 10014
Y2lvbg== 10015
aXR0eQ== 10016
IFR1ZXNkYXk= 10017
dW1lcw== 10018
IFByb3Q= 10019
ZWRlcg== 10020
IHBlc3NvYXM= 10021
INC90L7Qsg== 10022
IHNraXA= 10023
IG9iamVjdGl2ZQ== 10024
w61hcw== 10025
IGRlc2s= 10026
IExvb2tz 10027
dW5kZW4= 10028
IHByaW1hcmlseQ== 10029
aW1lbnRv 10030
IHJlcG9ydGluZw== 10031
IGhhY2U= 10032
IGNoZWNrZWQ= 10033
6Zg= 10034
IOuztOs= 10035
IHNtZWxscw== 10036
IGFjdG9ycw== 10037
IEFzaWE= 10038
aWzDoA== 10039
IHJlY2VpdmluZw== 10040
IHRheGVz 10041
IGdyYWNl 10042
IGNvbXBldGl0aXZl 10043
IGRpdmlzaW9u 10044
IGVzcGVy 10045
IHdoZWVscw== 10046
IGtvbW10 10047
IHRyZW1lbmRvdXM= 10048
IGVzcGU= 10049
Li4uKQ== 10050
IOyehQ== 10051
IGxpc3RlZA== 10052
w6RsbA== 10053
IHVudXM= 10054
IEhvbGx5 10055
IGd1aWRhbmNl 10056
IGN1Yg== 10057
IGludGVsbGVjdA== 10058
INCx0YvQuw== 10059
IHJlZ2FyZGxlc3M= 10060
IFN0YW4= 10061
5rKh 10062
IGNvbmNsdXNpb24= 10063
YWNhxJ8= 10064
IGxvbA== 10065
IEJhdA== 10066
IG1hbmlmZXN0 10067
IENoaWVm 10068
IHNoYW1l 10069
IG91dGNvbWVz 10070
IG1haWw= 10071
IGt1cg== 10072
zrnOug== 10073
ZXR6 10074
IHByZXBhcmluZw== 10075
Mjc= 10076
IFF1ZWVu 10077
4K6z 10078
IOu5hA== 10079
IHRpc3M= 10080
IGNvbnNjaW91c25lc3M= 10081
IHBhbnRz 10082
IG1lbHQ= 10083
dWNodA== 10084
aW5o 10085
7JuM 10086
IHZvdHJl 10087
IG1vZHVsZQ== 10088
b3d5 10089
IG1vbnN0ZXI= 10090
IOuG 10091
IGVsZWN0cm9uaWM= 10092
IGNlbnRyZQ== 10093
IHN0b3Bz 10094
IHRvdQ== 10095
IOut 10096
IGxhbWI= 10097
IGNvbnNlcXVlbmNlcw== 10098
IHN0cmF3 10099
IGltcGVy 10100
IGV4dGVuZA== 10101
44Gj44Gf 10102
IGFuc3dlcmVk 10103
IE1haA== 10104
IExBVVJB 10105
aWZ0aW5n 10106
dWF0ZQ== 10107
5YWI 10108
IFVTQg== 10109
IEFuZHJldw== 10110
44Kr 10111
IEZyZWQ= 10112
IERF 10113
IEdlb3Jn 10114
57s= 10115
w6xuaA== 10116
IGRyYXdu 10117
IGxpcHM= 10118
Ymly 10119
IG1heW9y 10120
aW1p 10121
IGVuY29yZQ== 10122
5ZCD 10123
Zm9ydGFibGU= 10124
dXJzZGF5 10125
IEZvcm0= 10126
IGJsYW1l 10127
IHNob3dlcg== 10128
IGNvbnRhaW5lcg== 10129
c3RlcnM= 10130
dWRlcw== 10131
IFRheQ== 10132
4Lil 10133
IOyYiA== 10134
IHZvbQ== 10135
IGJhc3M= 10136
IExhYg== 10137
aXNzYQ== 10138
IGRpbWVuc2lvbg== 10139
IGV4ZWN1dGl2ZQ== 10140
IFJvbQ== 10141
6rKM7JqU 10142
IERvY3Rvcg== 10143
IGRlbGl2ZXJlZA== 10144
IGdhbmc= 10145
IGNlcg== 10146
IHBpdA== 10147
ZWxp 10148
IGV4dHJhb3Jk 10149
amFy 10150
IGRlcml2 10151
IGlsbG5lc3M= 10152
IGd1bnM= 10153
IDIwMTE= 10154
IGFpcnBvcnQ= 10155
0JU= 10156
IGF0dGl0dWRl 10157
IGdyYXQ= 10158
IFdy 10159
IE5BUlJBVE9S 10160
IOyalA== 10161
IHJlbmV3 10162
IGNvc2E= 10163
IGNvbnRyb2xsZWQ= 10164
b21teQ== 10165
b25kcw== 10166
IGVzZQ== 10167
w6RjaA== 10168
IHZlbmQ= 10169
ZGFt 10170
IGFyZ3U= 10171
IGFjY2VsZXI= 10172
IG5haWw= 10173
aWVuZQ== 10174
7IOd 10175
IGVuY29udA== 10176
ZXNlYXJjaA== 10177
6aE= 10178
IGdvb2Rz 10179
IGZpc2hpbmc= 10180
QVBQTEFVU0U= 10181
IE5BUw== 10182
ZWN0aW9u 10183
IHRlbXBsZQ== 10184
bGljaGU= 10185
IGtleWJvYXJk 10186
562J 10187
IGRlc2Rl 10188
IGVkdWNhdGlvbmFs 10189
IE5pZ2h0 10190
MzM= 10191
IGJyZWF0aGU= 10192
bGljaGVu 10193
dGht 10194
acOocmU= 10195
4Lia 10196
bGFyxLE= 10197
IGFsaQ== 10198
IGNvbXBvcw== 10199
IHNlbnNvcg== 10200
IOu2gOs= 10201
IG5ld3Nw 10202
IEJ1bmQ= 10203
IE1p 10204
IHBlcmZvcm1pbmc= 10205
IGRydW0= 10206
QkU= 10207
IHBvcms= 10208
IGNvYWw= 10209
ZW5nZXI= 10210
IHJhbQ== 10211
IOuyiA== 10212
54S25b6M 10213
0LjRgNC+0LI= 10214
IFBvcA== 10215
IHBob25lcw== 10216
IGZhY2ls 10217
IHRyYWNrcw== 10218
b250ZQ== 10219
IG9yZ2FuaWM= 10220
IGRpYWxvZ3Vl 10221
IEhhdmluZw== 10222
IFBvc3Q= 10223
IHBheW1lbnQ= 10224
IGFycmF5 10225
IGludGVuZGVk 10226
w7pz 10227
IGJhcnM= 10228
IHJldmlld3M= 10229
bGFuZHM= 10230
IGtpbmdkb20= 10231
IHN0YWdlcw== 10232
IG1vdW50YWlucw== 10233
IGR1bg== 10234
IGRlY2ly 10235
xI0= 10236
IGJhbmtz 10237
IHRocm93aW5n 10238
IOuquw== 10239
IGFuZ2Vy 10240
INGB0LXQudGH0LDRgQ== 10241
IGRpc3R1cg== 10242
IGh1bWFuaXR5 10243
IGVsZXM= 10244
IHNob3VsZGVycw== 10245
IFBlcmZlY3Q= 10246
IGZhbmN5 10247
IGJyaWxsaWFudA== 10248
IGluc3BpcmF0aW9u 10249
aG1t 10250
5b+r 10251
IGxpZA== 10252
VUw= 10253
IG3DpQ== 10254
aW5kaQ== 10255
6Ig= 10256
IHNoaWVsZA== 10257
IOyYpOs= 10258
Q1Q= 10259
YWdpbmU= 10260
dWJlcg== 10261
IEJS 10262
IHF1ZXN0bw== 10263
INC30LDQug== 10264
IEtub3c= 10265
IHRhbmc= 10266
7ZWp64uI64uk 10267
IGJhcmVseQ== 10268
IFNF 10269
IG1hcmdpbg== 10270
cmVp 10271
0LDRgtC10LvRjA== 10272
IGNvbnRy 10273
IHbDoA== 10274
IGxlZ2l0 10275
0Jg= 10276
a2lucw== 10277
0YDQtdC0 10278
IEFzaA== 10279
IGFkdmlz 10280
IEdyZWVr 10281
0YPQug== 10282
IHNoYWtl 10283
aWRhZGVz 10284
0LDRgdGM 10285
IGNvbnZlbnRpb24= 10286
IGNvbnRlc3Q= 10287
TVM= 10288
IFllYXI= 10289
IHJlcHJlc2VudGF0aW9u 10290
aW5kZW4= 10291
ZW5kYXI= 10292
IHByb3N0 10293
IEh1bWFu 10294
IEN5 10295
YW5nZWQ= 10296
UEE= 10297
IGF4aXM= 10298
IHRoZW9yZQ== 10299
YXR6 10300
IO2VmOqzoA== 10301
IGVscw== 10302
IFJlc2VhcmNo 10303
IGJlbmVmaWM= 10304
IGRlbnNpdHk= 10305
aW5kbw== 10306
7Jy8 10307
aW1kaQ== 10308
IHJlc2VhcmNoZXJz 10309
6rGw65Og 10310
aWdocw== 10311
ZGFu 10312
IGRpY2U= 10313
IG1hYXI= 10314
IHN1Ym1pdA== 10315
IGR1bWI= 10316
IGJpag== 10317
YXdheQ== 10318
IFBhc3M= 10319
IGV4dGVuc2lvbg== 10320
IGNydXNo 10321
IGNvdmVyaW5n 10322
ZWRp 10323
Ym9ybg== 10324
aW5hdGlvbnM= 10325
INGB0LTQtdC7 10326
0LLQtdGA 10327
IE90aGVyd2lzZQ== 10328
aXN0YW50 10329
0LDQudGC0LU= 10330
IHRhbnRv 10331
IHBlcmZvcm1lZA== 10332
INC30LDQvw== 10333
YWxv 10334
IEZvdW5kYXRpb24= 10335
IHByb3RvY29s 10336
IFpv 10337
bWF5 10338
IGhhY2s= 10339
IGJ1ZGR5 10340
bWFkZQ== 10341
IGFkcw== 10342
IGZhc2NpbmF0aW5n 10343
IGVxdWl2YWxlbnQ= 10344
Z2Vs 10345
IGFyYw== 10346
INGH0LXQu9C+0LI= 10347
IHByb3Bvc2Vk 10348
IG5vdHJl 10349
YW5nZXM= 10350
IGNvdW5zZWw= 10351
YWxsYQ== 10352
IDMx 10353
d2VldA== 10354
yJk= 10355
IGVsZWN0cmljaXR5 10356
IHRveA== 10357
xYJhZA== 10358
IOy0 10359
IGRpZmZpY3VsdHk= 10360
oNeZ 10361
bmVzZGF5 10362
0LjRgdGM 10363
IGFsbGVn 10364
IEdP 10365
IHF1aXQ= 10366
IEhlcnI= 10367
IGVzdMOhbg== 10368
IGdpcmxmcmllbmQ= 10369
IHRlbmc= 10370
aWZpY2lhbA== 10371
IEphbQ== 10372
IGNhbmNlbA== 10373
IGZyZXF1ZW50bHk= 10374
SVY= 10375
5a+m 10376
IGNsb3Npbmc= 10377
IGRlY2FkZQ== 10378
IHJlcHJlc2VudGVk 10379
IENhbmFk 10380
INC60L7RgtC+0YDRi9C1 10381
IGVzdGFtb3M= 10382
IFRodXJzZGF5 10383
IEdh 10384
IExpdmU= 10385
bGVt 10386
YmJsZQ== 10387
U09O 10388
IDIwMDg= 10389
IGRpY2g= 10390
IEF3ZXNvbWU= 10391
IGNvbmNlcHRz 10392
UEVBSw== 10393
IGxpdGVyYXR1cmU= 10394
IE9seW1w 10395
0LvQsNC0 10396
IG5vc3Q= 10397
dml0 10398
IEVudGVy 10399
b3JkZXJz 10400
aWNraW5n 10401
bmllag== 10402
IGV1Y2g= 10403
IFRob3VnaA== 10404
IGJhZ3M= 10405
IGxpbWl0cw== 10406
IHN0YWtl 10407
g6U= 10408
IG9j 10409
IFZpcw== 10410
IDEyMA== 10411
IG51ZQ== 10412
IGNvbmNl 10413
IGRpc2Fn 10414
56g= 10415
IGFudGljaXA= 10416
oIg= 10417
c2w= 10418
IHZvdGluZw== 10419
IGV4cG9zdXJl 10420
IENvbW11bml0eQ== 10421
IEp1c3RpY2U= 10422
b3JuZXk= 10423
c3p5c3Q= 10424
IGZyaWVk 10425
7Iuc6w== 10426
IFdpbg== 10427
IEA= 10428
IEhvcGVmdWxseQ== 10429
ZXN6 10430
IG1vbmRl 10431
IGNvbWJpbmU= 10432
Z21lbnQ= 10433
IHJlY29tbWVuZGF0aW9ucw== 10434
IHByZWduYW50 10435
7Iud 10436
cmFm 10437
IGx1 10438
6ICB 10439
5LuA5LmI 10440
ZG9vcg== 10441
0LDQt9GL0LI= 10442
dWVnbw== 10443
IGltcHJvdmVtZW50 10444
IHRyaW0= 10445
IGVpZ2Vu 10446
IGFwcHJveGltYXRlbHk= 10447
INCy0LDQvA== 10448
YXdh 10449
INGB0L7QsQ== 10450
IGNvcm9u 10451
IG9uZ29pbmc= 10452
IGhlcw== 10453
IGluanVyeQ== 10454
IGZyYW5r 10455
IGthZGFy 10456
cmVuY3k= 10457
IENvbG9y 10458
IEdydQ== 10459
IGRpcA== 10460
0YDRiw== 10461
IHRlYXJz 10462
Z3Q= 10463
IFBE 10464
IHBhdXNl 10465
b3Nj 10466
IHVzdGVk 10467
IFdvbw== 10468
IHdpxJk= 10469
6KaL 10470
IGRlbm4= 10471
IFBldA== 10472
IG92ZXJjb21l 10473
IOuCtOqwgA== 10474
IE1vdmU= 10475
IGxpY2Vuc2U= 10476
IHJlcGVhdGVk 10477
4K+H 10478
IGNhdGVnb3JpZXM= 10479
IG5vb2RsZXM= 10480
IGZsb29k 10481
IE1hc3M= 10482
IG51dHM= 10483
IEplc3M= 10484
IElo 10485
IGNoYW5jZXM= 10486
kJg= 10487
IGRvbmRl 10488
SUc= 10489
IGFuZGVyZQ== 10490
IGJvbmVz 10491
7J6R 10492
IGVmZmljaWVuY3k= 10493
IG1vZGVy 10494
cm9hdA== 10495
IOydtOqyjA== 10496
aWxsZXI= 10497
IG9tZWdh 10498
INC/0L7Qsg== 10499
IEdyb3Vw 10500
IHByb2R1Y2luZw== 10501
YW1v 10502
IHBhcnRpY2lwYW50cw== 10503
dXBw 10504
aWZpY2U= 10505
IGZvcnR1bg== 10506
aWV0bmFt 10507
YWNhaw== 10508
IEtv 10509
bWnFnw== 10510
IGphaWw= 10511
IEpvbmVz 10512
xZtteQ== 10513
IERldXRz 10514
IGJyaWVmbHk= 10515
IFRhbA== 10516
IFBlcmhhcHM= 10517
IFJ1Yg== 10518
IEtu 10519
64uk64qU 10520
csOp 10521
IHZvY8Oqcw== 10522
IENoYXJsZXM= 10523
0LXRgtC1 10524
cmllcnM= 10525
IGhlYWw= 10526
YW50ZWU= 10527
IGRlbW9jcmFjeQ== 10528
IGxvYW4= 10529
IGNoZWY= 10530
0Y/QvA== 10531
IHVuY29tZm9ydGFibGU= 10532
IGV0ZXJu 10533
YXBwaW5n 10534
IHJlcGFpcg== 10535
cm90 10536
IFRhcg== 10537
IGNvdmVycw== 10538
b21pbmc= 10539
IEV0aA== 10540
IM6t 10541
0YfQvdC+ 10542
IGFmdGVyd2FyZHM= 10543
INCy0LXRgA== 10544
IGRhaGE= 10545
IGtuZWVz 10546
IG9yZGluYXJ5 10547
w7xs 10548
Z2Fz 10549
IHRpY2tldA== 10550
IOyggOuKlA== 10551
IOyeiOyKteuLiOuLpA== 10552
Y2h0ZQ== 10553
TXI= 10554
IHNpc3Q= 10555
aHVp 10556
6re46w== 10557
7Jes 10558
IHZhcnk= 10559
IG1lbW9y 10560
IGNvbnRyb2xsZXI= 10561
IGLEmWR6aWU= 10562
IG1pbmlzdGVy 10563
15I= 10564
Zmxvdw== 10565
QUg= 10566
IHRvd2Vy 10567
55A= 10568
IHNjYXI= 10569
5oOF 10570
IFBlbg== 10571
IHBhw61z 10572
15g= 10573
7J246w== 10574
IGVuZXJn 10575
IHN3b3Jk 10576
IHBhcGVycw== 10577
0LjQu9Cw 10578
IFdlZG5lc2RheQ== 10579
IEZvcmNl 10580
IGV4dHJhb3JkaW5hcnk= 10581
IExha2U= 10582
IOqwgOs= 10583
IEJlYXV0 10584
IHJlYXNvbmFibGU= 10585
IGNvbnRyaWJ1dGU= 10586
IHBsZWFzZWQ= 10587
IHVwZGF0ZWQ= 10588
IHBpw7k= 10589
ZWxv 10590
IHNpZ25pZmljYW50bHk= 10591
IGJvdA== 10592
IGdlbmVyYXRpb25z 10593
IHByb3RlY3RlZA== 10594
5ZOI 10595
IGhpZGluZw== 10596
IElsbA== 10597
IG5ldXRyYWw= 10598
XSw= 10599
z4TOvw== 10600
IHRvbmd1ZQ== 10601
VGhhbms= 10602
IOqzhA== 10603
IHBheXM= 10604
zq/OvQ== 10605
IGFwcGxl 10606
MDE= 10607
ZXJr 10608
aWVyYQ== 10609
IGplZw== 10610
IFN1YnNjcmliZQ== 10611
IHRoZWF0ZXI= 10612
IHN0cm9uZ2x5 10613
IOyGjA== 10614
INC/0YDQsNCy 10615
dWNreQ== 10616
IEppbg== 10617
a3dhcmQ= 10618
6rG0 10619
IG9wcG9uZW50 10620
IFNP 10621
IGhvbHk= 10622
IGZpbGxpbmc= 10623
Ol0= 10624
IGhpag== 10625
0Jw= 10626
IGJpc3M= 10627
IGJsZW5k 10628
IGltcGxpYw== 10629
IOy9 10630
bGxlaWNodA== 10631
2YrYqQ== 10632
YXNhbnQ= 10633
ZXJ0ZQ== 10634
IFNhbWU= 10635
IGludGVyaW9y 10636
U2U= 10637
IGJlbmNo 10638
IHBvY28= 10639
IG1hcmtz 10640
IHdpbnM= 10641
5ZaU 10642
IM6z 10643
IGRpc3RpbmN0 10644
IEFzaWFu 10645
IG1vbGVj 10646
IEphY2tzb24= 10647
IGVhc3Q= 10648
IHBoeXNpY3M= 10649
aW1hbA== 10650
IHBlYWs= 10651
YXJpYW4= 10652
ZXBz 10653
IG5lYXQ= 10654
INCy0LDRgQ== 10655
dXJuaW5n 10656
IHN5bnRo 10657
IHJldmVhbA== 10658
xbo= 10659
Z29u 10660
bmlz 10661
YXRpdg== 10662
IExhcw== 10663
IHB5 10664
IE1hamVzdHk= 10665
IFZhbGxleQ== 10666
IGVuZg== 10667
IGdlbnM= 10668
IHJvb3Rz 10669
ZXpl 10670
YmV0 10671
IGFjdHM= 10672
6Zo= 10673
6JA= 10674
IHBoaWxvc29waHk= 10675
IG1hdGNoZXM= 10676
nWk= 10677
IGp1xbw= 10678
IGRlc3Blcg== 10679
IEVkdWNhdGlvbg== 10680
IHNwb3Rz 10681
IHJlZ2lvbnM= 10682
QXI= 10683
IE5hbQ== 10684
ZWVu 10685
IGRpYWdyYW0= 10686
IHJlbHk= 10687
IHRlbnM= 10688
IGRhdGluZw== 10689
IGNvYXQ= 10690
IEhvcg== 10691
IGFja25vd2xlZGdl 10692
IFByZXR0eQ== 10693
INC/0L7Qvw== 10694
IHZvaXI= 10695
IGZhdm91cml0ZQ== 10696
IG1vxbw= 10697
IGtt 10698
IERP 10699
IGZlcnQ= 10700
IOuPhA== 10701
IFBhYw== 10702
IGZvbnQ= 10703
IGZpbmRz 10704
IEl0YWx5 10705
INC60L7Quw== 10706
IGNvbXBhc3M= 10707
67M= 10708
bGlhbWVudA== 10709
IG5vdGlvbg== 10710
IGluamVjdA== 10711
IHdpc2RvbQ== 10712
IMOc 10713
IE1vb24= 10714
IEJ1c2luZXNz 10715
cmljcw== 10716
IFlvdXQ= 10717
IGZvcmdpdmU= 10718
IGZpbmFuY2U= 10719
aWxv 10720
2KM= 10721
YWhs 10722
IGRlbW8= 10723
IGNsaW1i 10724
IGV4cG9ydA== 10725
5aA= 10726
IHN1Y2Nlc3NmdWxseQ== 10727
IEZlcg== 10728
cGVjdGVk 10729
ZGVt 10730
IHJldGlyZQ== 10731
IGxhcHRvcA== 10732
IHNwaXI= 10733
IEFzc29jaWF0aW9u 10734
INCz0Ls= 10735
IFNlbA== 10736
IO2VnOs= 10737
IGVtcGxveWVl 10738
IG1vbHQ= 10739
Ukw= 10740
0K8= 10741
IGNvbnRyYQ== 10742
IHVn 10743
IEJhbGw= 10744
IEphdmE= 10745
w6lyaWU= 10746
IHByb2NlZHVyZQ== 10747
IGdyaWQ= 10748
IOuKkOs= 10749
IGJlbHQ= 10750
INGN0YLQvtCz0L4= 10751
dXJk 10752
IGNvbXByZWg= 10753
IGRldmVsb3Blcg== 10754
INGN0YLQvtC8 10755
5Zg= 10756
Y3I= 10757
IOuT 10758
IHNwb2tlbg== 10759
cmVuY2U= 10760
IHRlcm1pbg== 10761
IGFnZ3Jlc3NpdmU= 10762
IGJpc3NjaGVu 10763
IGhhc3Rh 10764
IEJyaWFu 10765
IENvbW1pc3Npb24= 10766
IFl1 10767
IHByb21pc2Vk 10768
IGVxdWl0eQ== 10769
aWtv 10770
dmVydHk= 10771
IHJlcGxhY2Vk 10772
IEhlbHA= 10773
IHBvc2U= 10774
IE1pZGRsZQ== 10775
IGtpbQ== 10776
IG1laW4= 10777
IENvdW5jaWxs 10778
INCS0YE= 10779
b3Jv 10780
IEJlcm4= 10781
IGJleg== 10782
IGFuYWx5dA== 10783
YW5nZW4= 10784
IOyLtg== 10785
IEdsbw== 10786
IHF1YWQ= 10787
0YLQsA== 10788
IHNwZWFrcw== 10789
7JiI7JqU 10790
IOyXrOufrOs= 10791
ZnJlZQ== 10792
0L3Rlg== 10793
cmljaA== 10794
IOuvuA== 10795
IERpZXM= 10796
YWJi 10797
pbg= 10798
IGRlcHJlc3Npb24= 10799
IHJldGFpbA== 10800
hOuTpA== 10801
IFZvdXM= 10802
IExhdGlu 10803
4bk= 10804
IOyii+yVhA== 10805
IHRvcnQ= 10806
IGNvbXB1dGVycw== 10807
IHNlYXJjaGluZw== 10808
IHR1Yg== 10809
YXRlbGw= 10810
IG1lcmM= 10811
IGdsYXNzZXM= 10812
cGVyc29u 10813
IGRpc2hlcw== 10814
IGd1YXJhbnRlZQ== 10815
IG1lZw== 10816
c20= 10817
IFdhbGs= 10818
7Jy866m0 10819
IGZvbGRlcg== 10820
IE1pdA== 10821
IHRpbWluZw== 10822
IGFic3Q= 10823
IExvZw== 10824
44Kv 10825
IGFwcHJvdmVk 10826
IFVTQQ== 10827
0LLQtdGC 10828
IHdpc2U= 10829
ZXNzZWQ= 10830
IGRvdWI= 10831
IHJlc2lkZW50 10832
IGdlbmVyYXRlZA== 10833
IHN0YXlz 10834
IGV4cGxhbmF0aW9u 10835
IHBvaXNvbg== 10836
YXRyZQ== 10837
IGluc2FuZQ== 10838
IHJlZmVycmVk 10839
YWlyZXM= 10840
IFRSQQ== 10841
IHNlaQ== 10842
IGlubm9j 10843
QWg= 10844
IG1hbnQ= 10845
aHVz 10846
IG91dGVy 10847
Z2Vi 10848
b2ljZQ== 10849
IGRpc2N1c3Npbmc= 10850
IGNvbnZlbmllbnQ= 10851
X18= 10852
IGF2b2ly 10853
IHNoYXBlcw== 10854
IGdyYXk= 10855
IGRlbnRybw== 10856
IG1hY2h0 10857
IDE5NQ== 10858
2Y8= 10859
IGFkZHM= 10860
dXRpbmc= 10861
IGNhcGFiaWxpdGllcw== 10862
IHNlY3Rpb25z 10863
IHR1bmU= 10864
IENhdXNl 10865
YXJkZQ== 10866
INGB0LrQsNC3 10867
YXZpcnVz 10868
IFJF 10869
IHR1bmVk 10870
IGxlYWY= 10871
dGVyaW9y 10872
IENhcHRhaW4= 10873
INis 10874
IGNob29zaW5n 10875
aGlu 10876
Z2dpbmc= 10877
dmlldA== 10878
IHJlZ3JldA== 10879
MjY= 10880
b25kZXJu 10881
IGJvbnVz 10882
IFJheQ== 10883
QXM= 10884
IHRvcm4= 10885
IEhpZXI= 10886
IEVV 10887
IHJpc2tz 10888
IGFtYQ== 10889
IFlldA== 10890
IGNoYXJhY3RlcmlzdGljcw== 10891
IOqwkA== 10892
IFNlbmF0b3I= 10893
IFZhbW9z 10894
IHJvc2U= 10895
IGNvcnBvcmF0ZQ== 10896
Z2hhbg== 10897
IGNlbnRlcnM= 10898
c3RhaXJz 10899
IG5pdA== 10900
IHVudXN1YWw= 10901
IFRvbnk= 10902
IEdS 10903
IFdpbGQ= 10904
IFNpbWlsYXI= 10905
IHRvZGFz 10906
5YGa 10907
IGhvcml6b250 10908
bWVs 10909
IHN0cmljdA== 10910
IGN1YWw= 10911
IHdyaXQ= 10912
IGV4dGVuZGVk 10913
IO2VmOuKlA== 10914
IHJlbGllZg== 10915
IG9uaW9u 10916
IGJhYmllcw== 10917
IGRpZmVy 10918
IGludGVncmF0ZWQ= 10919
w7x6aWs= 10920
ZXBpbmc= 10921
LS0tLQ== 10922
IG1lbnM= 10923
IHN0cmF0ZWdpYw== 10924
ZmluaXRlbHk= 10925
IGVpZ2VudGxpY2g= 10926
V2hv 10927
5Zyw 10928
IHs= 10929
IOS9oA== 10930
IFRyaQ== 10931
IHBvaW50ZWQ= 10932
8J0= 10933
bmFtZW50 10934
0LXRhg== 10935
IHByaWRl 10936
IFJlcHVibGljYW4= 10937
IHNhbXBsZXM= 10938
IGRvbWVzdGlj 10939
TFk= 10940
dmV6 10941
IHdlYmluYXI= 10942
2KfZhQ== 10943
IGVuaA== 10944
IHN1Z2dlc3RlZA== 10945
IG1laW5l 10946
IHB1ZWQ= 10947
b3Jlbg== 10948
cmly 10949
IGhlYXZpbHk= 10950
IGluc3RydWN0aW9u 10951
IG1pY3JvcGhvbmU= 10952
IGlndWFs 10953
IElyYQ== 10954
IHZ1bG5lcmFibGU= 10955
IFZpcmdpbmlh 10956
IGNvbnRpbnVvdXM= 10957
IHBvdmVydHk= 10958
IGJsYWRl 10959
5LiJ 10960
IHJlbGF0ZQ== 10961
IGNhcmE= 10962
IEdvaW5n 10963
IHJlZ2lvbmFs 10964
IEZ1Y2s= 10965
IHRvdw== 10966
IE11c2V1bQ== 10967
cmFudHM= 10968
INCx0LXQtw== 10969
bGFpbQ== 10970
IGNoYW1waW9u 10971
dGxl 10972
w61u 10973
ZW5jaWE= 10974
IGRpZXNlbQ== 10975
IERpZw== 10976
bWF0ZXM= 10977
IGludmVzdGluZw== 10978
IEpvcmRhbg== 10979
IGludGVncmF0aW9u 10980
IO2O 10981
4Lir 10982
ZW5zdXM= 10983
IEFyY2g= 10984
IHBlbmNpbA== 10985
0LDQu9GM0L3Qvg== 10986
aXNzZW4= 10987
IEth 10988
IHJvY2tz 10989
IHJhdGluZw== 10990
IHJlZnVnZQ== 10991
IGFwcg== 10992
ZXRlZA== 10993
IGFzc2lzdGFudA== 10994
IG1lYW5pbmdmdWw= 10995
IHBlcm1hbmVudA== 10996
IGhpbGw= 10997
IHdzenlzdA== 10998
IHdvdW5k 10999
IEF0bA== 11000
IGxha2U= 11001
IEZvcnQ= 11002
6Kyd6Kyd 11003
IHJlZHVjdGlvbg== 11004
IHZpdg== 11005
IHNvdXI= 11006
IGVjb3M= 11007
IGhheg== 11008
IHN0ZWFs 11009
IG15c3Rlcg== 11010
INCa0LDQug== 11011
INGN0YLQuA== 11012
IFZpZXRuYW0= 11013
IGFudGVz 11014
IGNvbm5lY3Rpbmc= 11015
6ZaT 11016
IERhdmU= 11017
IGLDtnlsZQ== 11018
IENhc3Q= 11019
TGU= 11020
IGN1bA== 11021
IGdlbnJl 11022
66eQ 11023
IGNvbXBsYWlu 11024
IGh1cnJ5 11025
YXJ0ZQ== 11026
Z3JlZw== 11027
IG1vbml0b3Jpbmc= 11028
IGRlc2VydA== 11029
INGB0L7Qsg== 11030
ZWxpbmc= 11031
IFN1cHJlbWU= 11032
IGdpYmk= 11033
IGxhcmc= 11034
IG5hdGlvbnM= 11035
IFRvaw== 11036
IG5lZWRsZQ== 11037
5rU= 11038
IGFzbGVlcA== 11039
IGNvbXVu 11040
IEpld3M= 11041
IGFjaGlldmVk 11042
IGV4aXQ= 11043
IGRpc2Vhc2Vz 11044
bGluZXM= 11045
44GL44KJ 11046
cmllbmRz 11047
IHJlY3Q= 11048
IHNjYW4= 11049
44Gv44GE 11050
IGh1cnRz 11051
esSZ 11052
IExvb2tpbmc= 11053
44K3 11054
7ZI= 11055
dWx0dXJhbA== 11056
4buT 11057
aW5lbnQ= 11058
IHB1ZXM= 11059
IGNoZWVyaW5n 11060
p4A= 11061
YWdnZXI= 11062
IGFkYQ== 11063
TGF1Z2h0ZXI= 11064
IFdvbWVu 11065
6KOh 11066
6Ks= 11067
IG9jY3VycmVk 11068
IHNlYXRz 11069
6ICM 11070
IGVtcG93ZXI= 11071
dW51 11072
ZWxsaW5n 11073
QkVS 11074
ZW5zaW9uYWw= 11075
IGNvbnNvbGU= 11076
YXNoaW5n 11077
IGVpbm1hbA== 11078
ZmFyZQ== 11079
IOuPvA== 11080
IHNlc3Npb25z 11081
2ZA= 11082
IHJpZGljdWxvdXM= 11083
w61hbg== 11084
IEhlbnJ5 11085
IEhvbA== 11086
IGNvbGxlY3RlZA== 11087
IGRpc2N1c3Npb25z 11088
RGU= 11089
IGRpc2FiaWxpdHk= 11090
IO2b 11091
IHN1YnNjcmliZXJz 11092
IGlyZ2VuZA== 11093
IGZlbA== 11094
IGRpcmVjdGlvbnM= 11095
IG1hbnVmYWN0dXJpbmc= 11096
IFJvZA== 11097
IOyWmA== 11098
4LiX 11099
5piO 11100
IGNyaXRlcmlh 11101
IG1vbGQ= 11102
6Kmx 11103
IGVudGVyaW5n 11104
cmlq 11105
aXNlbg== 11106
IFBhcmE= 11107
aWV2ZQ== 11108
IGNoYXJnZWQ= 11109
IGpvdQ== 11110
IGNhdHM= 11111
0LvQtdC0 11112
YWRheXM= 11113
0LDQvdC+0LI= 11114
asSZ 11115
dmF0aW9u 11116
IGFzdHJvbg== 11117
aXRhbHM= 11118
IEJyYW5k 11119
IEthbg== 11120
IHBsYWlu 11121
IGFuZGVyZW4= 11122
YW5kZQ== 11123
0Y/Qtw== 11124
IHRvbGVy 11125
xYJlbQ== 11126
IHByw6k= 11127
0LzQvtGC0YA= 11128
YWdlbWVudA== 11129
dWN0 11130
Y2jDqQ== 11131
IEVuZXI= 11132
YWrEhQ== 11133
IO2VtOs= 11134
IHN0YQ== 11135
IHJpbmdz 11136
IHRvaWxldA== 11137
IENyYQ== 11138
IGV4cGVyaWVuY2luZw== 11139
IHNsaXA= 11140
IHNhbmR3aWNo 11141
IFVzaW5n 11142
IHNwZWN0cnVt 11143
IFJvcw== 11144
YXBzZQ== 11145
IEpheQ== 11146
0LzRgw== 11147
5rOV 11148
RXg= 11149
IHJlY29nbml0aW9u 11150
IERpZG4= 11151
dWRh 11152
YWpl 11153
ZXN0bHk= 11154
IGZlbWlu 11155
aXR1cmU= 11156
0YDQsNGC 11157
IGhpcmU= 11158
IG5vd2hlcmU= 11159
5L2N 11160
4bqn 11161
IHdpbmc= 11162
IHNhdg== 11163
IFNlY3VyaXR5 11164
IHJ1cmFs 11165
IEZ1bg== 11166
YXllcg== 11167
IGFjY3Vz 11168
IG1t 11169
IEpvc2VwaA== 11170
IHNjcmVlbnM= 11171
IGJvcnJvdw== 11172
IHN3aW5n 11173
IDQ4 11174
IHRvdWNoaW5n 11175
dGhpcw== 11176
aW50ZW5kbw== 11177
6YM= 11178
0KA= 11179
IFNjb3RsYW5k 11180
IEphc29u 11181
IFZlbg== 11182
IGV4Y2VwdGlvbg== 11183
IG5lYXJieQ== 11184
IGJyb3dzZXI= 11185
YW5nZXJz 11186
IFNpbg== 11187
z4DOvw== 11188
5L2G5piv 11189
b3NwZWw= 11190
IHd1cmRl 11191
IGRydW5r 11192
7Zo= 11193
7IaN 11194
44OJ 11195
IOyKpO0= 11196
IExpZQ== 11197
b2Nv 11198
IExlYWd1ZQ== 11199
IGlnbm9yZQ== 11200
IDop 11201
IGxhbmRpbmc= 11202
INi52YQ= 11203
IFRhZw== 11204
Mjg= 11205
IGRyYWZ0 11206
IGFlcg== 11207
IOq3uOuDpQ== 11208
IHBlbnNl 11209
INC00LDQttC1 11210
IGJlZHJvb20= 11211
IG5hag== 11212
7KeA6rOg 11213
aWdlbm91cw== 11214
IGRlYWxz 11215
ZWxsbw== 11216
5LqM 11217
IHBvc2l0 11218
6rs= 11219
IHZpc2l0ZWQ= 11220
aWZpZXM= 11221
IHByZW1p 11222
IGNhbnQ= 11223
IFJpY2s= 11224
IHJhaXNpbmc= 11225
IHBlcm1pc3Npb24= 11226
IHB1Ymw= 11227
dW5jaQ== 11228
IGJlbmQ= 11229
IGNoYW1waW9ucw== 11230
ZGll 11231
IGF3ZnVs 11232
IGp1bXBpbmc= 11233
IGxsZWc= 11234
IHN1c3RhaW5hYmxl 11235
IFRvdA== 11236
IGNhbmR5 11237
5YCZ 11238
IHNhdGlzZmllZA== 11239
IHBpcGU= 11240
IGNvY2s= 11241
2LY= 11242
c3RvbmU= 11243
IG1vbWVudHVt 11244
INCd0LA= 11245
IGFsb3Jz 11246
IHJldHVybnM= 11247
YW1tZW4= 11248
564= 11249
0YvQvA== 11250
YXdu 11251
b3R0ZWQ= 11252
IHdvbGxlbg== 11253
aWN0ZWQ= 11254
IGNhbmRpZGF0ZXM= 11255
IExhZHk= 11256
IHlpZWxk 11257
IG1haW50ZW5hbmNl 11258
ZmZlY3Q= 11259
IGV4cGFuc2lvbg== 11260
IExFRA== 11261
IGRhcmtuZXNz 11262
IG91dGZpdA== 11263
7JWI 11264
INC40YHQvw== 11265
cnVwdGlvbg== 11266
44GE44G+44GZ 11267
IGVuZ2FnaW5n 11268
IGluc2lnaHQ= 11269
IEFsd2F5cw== 11270
IGdlZg== 11271
cmFr 11272
IHBpeA== 11273
6Ka65b6X 11274
IHF1YW50aXR5 11275
IGluaw== 11276
IEtpbmdkb20= 11277
IGNvcnQ= 11278
5bi4 11279
IGdvdmVybm1lbnRz 11280
IHByb3Rlc3Q= 11281
cG9vbg== 11282
INGC0L7Qs9C+ 11283
5a6D 11284
dWNoZW4= 11285
cXVhbGl0eQ== 11286
IFBvcnF1ZQ== 11287
IENsdWI= 11288
IHJpdA== 11289
IGFydGljbGVz 11290
Qkk= 11291
aWdpYmxl 11292
IGRpc2FzdGVy 11293
0LjQsw== 11294
INC90LjQug== 11295
2YfYpw== 11296
66W8 11297
YXJldA== 11298
IHVuYWJsZQ== 11299
IMOu 11300
IGVyc3Q= 11301
INeg 11302
dmFyZA== 11303
IGFubm95aW5n 11304
IEtpcg== 11305
0LXRgNC2 11306
ZW5uaXM= 11307
IHVuY2VydGFpbg== 11308
MzY= 11309
w7Zz 11310
IGVjb3N5c3RlbQ== 11311
emVk 11312
asOg 11313
c3Vu 11314
7Ja07ISc 11315
IMW8ZWJ5 11316
IG1hcHM= 11317
64KY 11318
5YWo 11319
IEp1c3Rpbg== 11320
IHRyYXNo 11321
IGVub3Jtb3Vz 11322
IHN0YXRlZA== 11323
IGJyYW5kcw== 11324
IHlvdXQ= 11325
INGH0LXQu9C+0LLQtdC6 11326
IE1hdHRo 11327
IHRyYW5zcG9ydGF0aW9u 11328
IGxlZ2lzbGF0aW9u 11329
IHByb3ZpZGVycw== 11330
INit 11331
IG1hZ2F6aW5l 11332
IHNlaGVu 11333
IERlc3BpdGU= 11334
IHBhc3Nlcw== 11335
5oiQ 11336
IGFsdGVy 11337
YWRhbg== 11338
IGZhcm1lcnM= 11339
6LCi 11340
IGNvbmZpcm1lZA== 11341
IGVzYQ== 11342
aXRvcw== 11343
IHJvYWRz 11344
VklT 11345
IHdvcmtlcg== 11346
IGRlc2lnbnM= 11347
IFNvdmlldA== 11348
YnJpZA== 11349
IHByYWN0aWNpbmc= 11350
IOu2gA== 11351
IFNlYQ== 11352
44Op 11353
INC/0YDQvtC0 11354
IGNoaWxs 11355
IGxlbW9u 11356
7JeQ64qU 11357
IGZsZXhpYmxl 11358
IEV4Y3VzZQ== 11359
IHRlcnJpdG9yeQ== 11360
5ZWP 11361
44G/ 11362
IGx1eA== 11363
IGxpZmV0aW1l 11364
IGRpc3Rpbmd1 11365
IFRpbWVz 11366
IGdyb3Nz 11367
ZW56 11368
IHNjcm9sbA== 11369
bcSxxZ8= 11370
Y2lw 11371
o7w= 11372
RFA= 11373
IHB1Ymxpc2g= 11374
IGViZW4= 11375
IHJlZ2lzdA== 11376
IGVkaXRpb24= 11377
IExF 11378
IGNoYXJnaW5n 11379
dXRhdGlvbg== 11380
eXJpY3M= 11381
aWRhcw== 11382
IM6/ 11383
INC60L7RgA== 11384
IFRvbg== 11385
55uu 11386
IHdob2V2ZXI= 11387
IEZveA== 11388
5omL 11389
6rGw65Og7JqU 11390
IGZvdWdodA== 11391
IGRyaWxs 11392
IEFmZ2hhbg== 11393
fiE= 11394
IFRvbw== 11395
IHNlY29uZGFyeQ== 11396
csOk 11397
IEhlYWQ= 11398
aW5uZW4= 11399
IHlhcm4= 11400
INC90LDQvA== 11401
IHdpZHRo 11402
IGVuZ2luZWVy 11403
acSF 11404
IHdpbmdz 11405
IOuVjOusuA== 11406
IHRyYXVtYQ== 11407
IHJlcHJvZHU= 11408
IGNoaXA= 11409
IHBhc3Npb25hdGU= 11410
IGF3a3dhcmQ= 11411
IO2K 11412
0LDQttC0 11413
IEJpdGNvaW4= 11414
IGtow7RuZw== 11415
IHLDsw== 11416
cmVjdGlvbg== 11417
INCz0LTQtQ== 11418
IFVzdWFsbHk= 11419
IGltcGxlbWVudGF0aW9u 11420
IGdhbWVwbGF5 11421
IG15c3Rlcnk= 11422
INC+0Lo= 11423
IGHDsW9z 11424
YW5keQ== 11425
0LjQvNC4 11426
IHByaXZhY3k= 11427
YWNv 11428
44KB 11429
IGR1bXA= 11430
IFBheQ== 11431
IGRpcGw= 11432
IGZ1cm4= 11433
IHNoaXBz 11434
TEE= 11435
INGF0L7RgNC+0Yg= 11436
IGVj 11437
IGRyb3Bz 11438
Y2hs 11439
MzI= 11440
IG9ic2VydmU= 11441
IERldmVsb3A= 11442
TcO8emlr 11443
YW5uZWw= 11444
b3dhxIc= 11445
IGZhY2Vk 11446
w6Fs 11447
IHZpY3RpbXM= 11448
IGdpZnRz 11449
IGJvb3Q= 11450
w59l 11451
cm9k 11452
IDIwMDk= 11453
w7ZydA== 11454
IHVuaXZlcnNhbA== 11455
IG5vdXZl 11456
IGJveWZyaWVuZA== 11457
IGNldGVyYQ== 11458
0YHRgtCw 11459
J1M= 11460
IG5pdmU= 11461
IGNydWNpYWw= 11462
IHN1cnZl 11463
IGNvaW4= 11464
IHNvbmRlcm4= 11465
IHNoYWRl 11466
IGx1Z2Fy 11467
IHN1cmVseQ== 11468
IG1heA== 11469
IGltcHJvdmluZw== 11470
5Zug54K6 11471
IHdlbg== 11472
INeR 11473
IOyWtOw= 11474
IGVuZm9yY2VtZW50 11475
aWJs 11476
IGxpdg== 11477
bGVyaQ== 11478
IG1lam9y 11479
IENhcm9saW5h 11480
IHZhcw== 11481
IGNvbXByb20= 11482
IGRpcnQ= 11483
IHVwZ3JhZGU= 11484
IEJlbGw= 11485
IHJlc3RhdXJhbnRz 11486
IHRyYXA= 11487
IHRlYXM= 11488
YmxpYw== 11489
IEdyZWc= 11490
c2Fu 11491
IG93 11492
dWVzdA== 11493
IHByb3Bvc2Fs 11494
IFJldA== 11495
ZnJvbnQ= 11496
IHBhc3NhZ2U= 11497
IHN1cnJvdW5kaW5n 11498
IMO6bHQ= 11499
IHVwY29taW5n 11500
IGhvcnJvcg== 11501
IGNsb3RoaW5n 11502
IOyVvQ== 11503
IGRpbA== 11504
cm9tZQ== 11505
IElk 11506
IFJvYWQ= 11507
INGN0YLQvtGC 11508
Y2hhaW4= 11509
INCx0YvRgtGM 11510
IE9mZmlj 11511
INCd0LU= 11512
IGluc2Fu 11513
IGRlY3JlYXNl 11514
INGF0L7Rgg== 11515
YnVpbGQ= 11516
IERyYWdvbg== 11517
5YI= 11518
IGludmVzdG9ycw== 11519
YW50aQ== 11520
IHNhY3JpZmljZQ== 11521
IHRyb29wcw== 11522
IEJhZA== 11523
IHBhc3N3b3Jk 11524
IGNvbnN0cmE= 11525
4LiV 11526
IMOHYQ== 11527
YWRvdw== 11528
dGhyb3VnaA== 11529
0YbQsA== 11530
Q2Fu 11531
IGNhbmRpZGF0ZQ== 11532
IGFudGli 11533
7JeF 11534
IHRhc3R5 11535
2YjZhg== 11536
IEluZg== 11537
IEJhbmc= 11538
acOfdA== 11539
aW5pdHk= 11540
ZmF0aGVy 11541
IGNvbnRyb3ZlcnM= 11542
IFBhaw== 11543
aWx0eQ== 11544
6rWs6w== 11545
IGxpZ2h0ZXI= 11546
IGZhbGxlbg== 11547
IHp1cw== 11548
IEd1YXJk 11549
IGNvdHQ= 11550
IEZyZWU= 11551
IGluaXRpYXRpdmU= 11552
YWxvdXM= 11553
IG5vdGlmaWNhdGlvbg== 11554
IE1lZGlj 11555
IENvbW1pdHRlZQ== 11556
7Jew 11557
IFdvb2Q= 11558
IG11c2g= 11559
dWx1bQ== 11560
6LI= 11561
YWhhaA== 11562
IHN1ZmZpY2llbnQ= 11563
IHNpbmdlcg== 11564
0LrQvtC5 11565
QUxJ 11566
w6R0dA== 11567
IFBS 11568
IExhcg== 11569
Y3VsZXM= 11570
aWVtcG8= 11571
IHVuZXg= 11572
IGludGVncmFs 11573
IHRyYW5zbWlzc2lvbg== 11574
IGljaQ== 11575
0YPRhQ== 11576
Z2lj 11577
IE5pbnRlbmRv 11578
IENvcA== 11579
IFRydXN0 11580
ZW5hcw== 11581
IGFiaWxpdGllcw== 11582
IGNoaXBz 11583
cGF0 11584
IGFuY2hl 11585
0LvQtdC9 11586
IGFwcHJvYWNoZXM= 11587
IHRob3I= 11588
IHNpc3RlcnM= 11589
IGRyaXZlcnM= 11590
IGFsbGE= 11591
MDM= 11592
IHJ1YmJlcg== 11593
IG7DpQ== 11594
QUNL 11595
IGRpc2FwcGVhcg== 11596
6rCc 11597
IGNvbXBlbnM= 11598
IHZpYnI= 11599
56yR 11600
R08= 11601
IHNpemVz 11602
IHRyYWNraW5n 11603
7ZmU 11604
IOyEuA== 11605
IGltcGFjdHM= 11606
aWJpbA== 11607
ZmlzaA== 11608
QlI= 11609
IGFycm93 11610
IGxhcmdlbHk= 11611
YW5ueQ== 11612
IGxhd3llcg== 11613
5oCO6bq8 11614
am91cnM= 11615
2ro= 11616
dmlh 11617
IGRlbGxh 11618
IG1hdGhlbWF0 11619
IE1pbmU= 11620
IEtvbGw= 11621
2LI= 11622
IENyb3Nz 11623
IDY1 11624
IGdyYWM= 11625
IGludm9sdmVz 11626
IGRlbGlnaHQ= 11627
IEhvbGx5d29vZA== 11628
IGltbWVkaWF0ZQ== 11629
b25pYw== 11630
IGxhZG8= 11631
IGJ1bGxldA== 11632
IE5vdGU= 11633
IHVubG9jaw== 11634
IGRpc2NvdW50 11635
IHJpc2luZw== 11636
cHJlc3M= 11637
IHBhY2U= 11638
IHNob3J0ZXI= 11639
IHRlbmVy 11640
Z2Vvbg== 11641
IG1hbmFnaW5n 11642
IGNlcmU= 11643
Q2hy 11644
V2hlbg== 11645
YWNoZW4= 11646
IOyT 11647
IEh1bg== 11648
IG9mdA== 11649
IDI1MA== 11650
aWVydW5n 11651
IHN0YWJpbA== 11652
IENvbm5lY3Q= 11653
IHlhbmk= 11654
IGRvd250 11655
zrzOsQ== 11656
IHZvY2Fs 11657
zr3OsQ== 11658
IGxlYW4= 11659
IHZpZMOpbw== 11660
IEZhbWlseQ== 11661
cmVzZW50 11662
IGFtb3VudHM= 11663
7KeB 11664
Y2xhc3M= 11665
IHZpYg== 11666
IEF2 11667
YXJzZQ== 11668
IGdlbnRsZW1lbg== 11669
IHNlZWtpbmc= 11670
IHVuaW9u 11671
IHJlZ3VsYXJseQ== 11672
5o8= 11673
IEphaHI= 11674
IEZvb2Q= 11675
IFByb2JsZW0= 11676
IGFydGlmaWNpYWw= 11677
IFNpeA== 11678
IGltcHJlc3NlZA== 11679
IHRvb3Ro 11680
IEto 11681
IHlhcmQ= 11682
IO2VtA== 11683
IG93bmVk 11684
IOuPmQ== 11685
7LKt 11686
IHRvZGE= 11687
IHBvcnRmb2w= 11688
IOuCqA== 11689
b3JnZW91cw== 11690
IGRhdGVz 11691
0L7Qu9GM0Lc= 11692
0LXRh9C90L4= 11693
IGNvbmZpZ3VyYXRpb24= 11694
IHJlcXVpcmVtZW50 11695
IGJlbGx5 11696
IHBhaW5mdWw= 11697
IGRlbW9uc3RyYXRl 11698
IGdsZWljaA== 11699
IHZpc2l0aW5n 11700
IENvbmY= 11701
IGRhbA== 11702
2ZE= 11703
IGFtZW5k 11704
IEZ1cg== 11705
5q+U 11706
IHZpdGFs 11707
4buL 11708
IG1hdGU= 11709
IE91 11710
IGxlZ2FjeQ== 11711
dXN0aW5n 11712
IGFjY29tbW9k 11713
IHF1b2k= 11714
YXVlbg== 11715
IGxpZmVzdHlsZQ== 11716
Q0M= 11717
w6TDpG4= 11718
YXJ0ZW4= 11719
IG1pbmhh 11720
csOz 11721
IOuqqA== 11722
IGZvcm1hdGlvbg== 11723
IHRyYWlsZXI= 11724
cGVyb3I= 11725
INCz0YA= 11726
IHVk 11727
enU= 11728
IGtvbW1lbg== 11729
IGNhdmU= 11730
IENvdW5jaWxsb3I= 11731
IHRocm93bg== 11732
IHRyaWNrcw== 11733
TEFVR0hURVI= 11734
IEFjYWRlbXk= 11735
cm93ZA== 11736
oZ0= 11737
7KCA 11738
IEltYWdpbmU= 11739
IGluZm9ybWVk 11740
cm9waA== 11741
IGxpZw== 11742
IHNrdWxs 11743
YWJldGg= 11744
IGZ1bmN0aW9uYWw= 11745
ZXJlaw== 11746
T24= 11747
6aY= 11748
IGFuY2VzdA== 11749
IHNhZmVseQ== 11750
IEhU 11751
64u5 11752
IGRhdg== 11753
IGRyaXZlcw== 11754
QW1lcmlj 11755
IHRpcmU= 11756
IHNhaXM= 11757
w6FyaQ== 11758
YXZvcnM= 11759
IGNvcnJlc3BvbmRpbmc= 11760
IHByw6lz 11761
Y2hlc3Q= 11762
IGJhY3Rlcmlh 11763
IGluZmVjdGlvbg== 11764
dXNhbA== 11765
IGF2ZXo= 11766
IGJhc2tldGJhbGw= 11767
IHN1cHBsaWVz 11768
IGV4cGVydGlzZQ== 11769
oKU= 11770
ZmE= 11771
IHRpZW1wbw== 11772
IFdhbnQ= 11773
IHNpbGx5 11774
IHVwcA== 11775
IGVsZWN0ZWQ= 11776
IGZpcmVk 11777
INiv 11778
IHVuaXZlcnNpdGllcw== 11779
YWxsZQ== 11780
IGphY2tldA== 11781
wrA= 11782
IHRyYXY= 11783
bHM= 11784
IGRlZmVhdA== 11785
IGNvZ24= 11786
IGVxdWF0aW9ucw== 11787
dWtp 11788
IFNoZXI= 11789
IHRoaXJ0eQ== 11790
IHN0cmVhbWluZw== 11791
b3Ryb3M= 11792
IFByb2R1 11793
bmVq 11794
IGRlc2lnbmVy 11795
IOuKkOuC 11796
IHBhaW50ZWQ= 11797
cmFpbmU= 11798
bWFpbA== 11799
IHZlc3M= 11800
IHJoeXRobQ== 11801
bGVzaA== 11802
IDk5 11803
IGFpbmRh 11804
Y2hpZWQ= 11805
IMOpdGFpdA== 11806
IGFmZmVjdHM= 11807
6aM= 11808
IEZpbmQ= 11809
IMOpbA== 11810
IHBvdGF0b2Vz 11811
IHBhZw== 11812
INC/0LDRgA== 11813
YXJ0cw== 11814
IE5hY2g= 11815
IDMz 11816
IEhhcmQ= 11817
IElyYXE= 11818
IG9waW5pb25z 11819
d2l0aA== 11820
ZXJtYW4= 11821
w70= 11822
6K0= 11823
IFNQRUFL 11824
rLw= 11825
IHN0YWJpbGl0eQ== 11826
IEhF 11827
IGNhcHR1cmVk 11828
IGJ1Y2tz 11829
IG1hc2tz 11830
IGNvbXBldGU= 11831
IGZvcmdvdHRlbg== 11832
0LvRjtGH 11833
c2VxdQ== 11834
IOyEoA== 11835
aWxsaW9u 11836
IGdyYXBoaWNz 11837
IGh1Yg== 11838
IOyXsA== 11839
ZW1wb3I= 11840
IGNyb3du 11841
IHdpZGVy 11842
IG9jY3Vycw== 11843
RFM= 11844
5oE= 11845
IEJhdHRsZQ== 11846
44Gq44KT 11847
IGR1YWw= 11848
IDYwMA== 11849
YXRoZXJz 11850
4LmB 11851
IHNldHRsZQ== 11852
IGF2YWl0 11853
IGRvaXM= 11854
0LrQuNGF 11855
YWRvcmVz 11856
IMOz 11857
bmVnbw== 11858
IEdlb3JnaWE= 11859
IFJvZw== 11860
IGRpdm9y 11861
IFNvbmc= 11862
IFNwZWNpYWw= 11863
IG11bg== 11864
IHByZXRlbmQ= 11865
TUFO 11866
IHZpb2xlbnQ= 11867
IGJlc2lkZXM= 11868
dnk= 11869
IE5heg== 11870
Mjk= 11871
IHN3ZWF0 11872
IHp3 11873
IEh1 11874
IEJ1aWxk 11875
IGhvcm0= 11876
IENhcmQ= 11877
IOycoA== 11878
IHJlY29tbWVuZGF0aW9u 11879
Y2FsbGVk 11880
c3RpY2s= 11881
IFBvbGljZQ== 11882
IGNvbnN1bWVycw== 11883
IGdyb2Nlcg== 11884
IHN0dW4= 11885
INCS0Ys= 11886
0KM= 11887
IERhdGE= 11888
IHN1YnN0YW50 11889
aXJlY3Q= 11890
4LI= 11891
INCy0Lc= 11892
IEFybQ== 11893
IHNlbWVzdGVy 11894
IEJyYWQ= 11895
IG91cnM= 11896
INC60L7RgtC+0YDRi9C5 11897
p2E= 11898
IGdyYW1z 11899
IGV4ZXJjaXNlcw== 11900
NzU= 11901
IHN3ZWFy 11902
IGluY2VudA== 11903
z4HOvw== 11904
IGlsbGVnYWw= 11905
5L2V 11906
IERhbW4= 11907
IG7Dug== 11908
IG5lY2Vz 11909
IGN1eg== 11910
aWNvbg== 11911
IGhvcnM= 11912
IENvbW8= 11913
5L2c 11914
IOuRkA== 11915
IG92ZXJzZQ== 11916
IGhhcnZlc3Q= 11917
IHRocmV3 11918
INC/0L7RgtC+0LzRgw== 11919
15nXlA== 11920
IG90cm8= 11921
INC/0LXRgNCy 11922
IHNjb3Bl 11923
IGdsb3J5 11924
IE1pY2hpZ2Fu 11925
IGFzc3VtaW5n 11926
INGD0LQ= 11927
IGJvbGQ= 11928
Z3Vl 11929
bW90aGVy 11930
IHdhcmVu 11931
6Kyb 11932
INil 11933
IEthbQ== 11934
aXNwaWVs 11935
IHRvdWpvdXJz 11936
IGNvbnN0aXR1dGlvbg== 11937
IH4= 11938
IGZyYW5rbHk= 11939
b2xlbg== 11940
b25zY2lvdXM= 11941
IHfDvHJkZQ== 11942
dGhvbg== 11943
IE9G 11944
7J6Q6w== 11945
dW5kYQ== 11946
IOaYrw== 11947
INC/0L7RgA== 11948
IGVtcGxveW1lbnQ= 11949
0ZHRgg== 11950
44GB 11951
IHN0ZWFt 11952
IERJ 11953
IHByb2Zlc3Npb25hbHM= 11954
IGVuZ2luZWVycw== 11955
IFhpYQ== 11956
56s= 11957
7JiB 11958
IER1bg== 11959
IGJpdGNo 11960
IEZvcmQ= 11961
5LiN6KaB 11962
c2VjdGlvbg== 11963
IHZpY2U= 11964
IExhdGVy 11965
b3N0b24= 11966
0Y3Rgg== 11967
16Y= 11968
IEF6dXJl 11969
cGxpbmc= 11970
IDE4MA== 11971
IENyZWF0 11972
SVNIQQ== 11973
IGJ1ZW5v 11974
7Z2s 11975
cnVw 11976
bGVycw== 11977
IFlhbmc= 11978
IEhB 11979
YmF0 11980
IENhdGhvbGlj 11981
IGFjY2VudA== 11982
IG1peGluZw== 11983
Y2tldHM= 11984
IGVuaGFuY2U= 11985
w7xocg== 11986
w6pz 11987
IO2W 11988
IHN3aW1taW5n 11989
IGPhu6dh 11990
IEVsaXo= 11991
IGltbXVuZQ== 11992
INCx0L7Quw== 11993
IGZhcmU= 11994
IEdhYg== 11995
16E= 11996
IHNhdGVsbA== 11997
IEFueXRoaW5n 11998
IGFzc2V0 11999
IHNjaGVkdWw= 12000
IHJhZGljYWw= 12001
IHp3ZWk= 12002
IE1F 12003
cmVsYXRlZA== 12004
IHNlcGFyYXRlZA== 12005
IExpYnI= 12006
IGdyaXA= 12007
IOCuqg== 12008
6KiA 12009
IGJlYW5z 12010
IE9w 12011
7IaM 12012
IHBvdW5k 12013
IGVudHJhbmNl 12014
z4Y= 12015
IE5pZQ== 12016
IFJlcHVibGljYW5z 12017
IGF0b20= 12018
IHBlcnNvbmFz 12019
IEFsaQ== 12020
w6Rocg== 12021
5aSW 12022
IGRyYW1hdGlj 12023
IEZpbmU= 12024
IHJlbWluZHM= 12025
6Jk= 12026
IGTDqWrDoA== 12027
IGFmZm9yZGFibGU= 12028
IGJyYW4= 12029
aWVybw== 12030
YWxhcg== 12031
Y3U= 12032
IFw= 12033
IG1vxbxl 12034
IGJsYXN0 12035
IHJlY3k= 12036
ZmlyZQ== 12037
IGxsZQ== 12038
INCy0YDQtdC80Y8= 12039
IFdX 12040
IHZz 12041
IER1ZGU= 12042
IFJvbWU= 12043
IGdyZWV0 12044
IEhldA== 12045
Y2lhcw== 12046
IOuLuQ== 12047
bGVzc2x5 12048
IHByZW1pdW0= 12049
IGV4cGVyaW1lbnRz 12050
YXRhcg== 12051
w6lyaQ== 12052
IG9mZmljaWFsbHk= 12053
IGZlZQ== 12054
4LmH 12055
INGH0LXQvA== 12056
cmVh 12057
IHRveQ== 12058
T1A= 12059
IFRheWxvcg== 12060
IE1jQw== 12061
aWxleQ== 12062
IEJhaw== 12063
IGFsdW0= 12064
IFVudGVy 12065
IG1hZ2ljYWw= 12066
IHRyYWJhbA== 12067
IHZvdGVz 12068
aXRhZ2U= 12069
IG1lY2hhbmljYWw= 12070
aG4= 12071
44G+44GX44Gf 12072
INC40L3RgtC10YA= 12073
5LuK5aSp 12074
IGhpbnQ= 12075
IGF1dGhvcml0aWVz 12076
IE5BU0E= 12077
aXZlcnNhcnk= 12078
INC/0L7Rhw== 12079
cmFj 12080
IFNQRUFLRVI= 12081
w7Z0 12082
IGZyYW1lcw== 12083
IGdvb2RieWU= 12084
IGNoZXI= 12085
aHU= 12086
IG5ldXI= 12087
5a6a 12088
IE1hY2g= 12089
IEhlbGw= 12090
IGZlc3RpdmFs 12091
64WE 12092
dXRh 12093
IG11c2hyb29t 12094
IHRhbnQ= 12095
IHRhdHRv 12096
IGRlbGV0ZQ== 12097
IGRpeg== 12098
IHbDpA== 12099
IHNldmVudA== 12100
IFF1aWNr 12101
IGJha2luZw== 12102
IGFzc2VtYmx5 12103
R28= 12104
IERyZWFt 12105
IExhZA== 12106
6Z2e 12107
w6J5 12108
YWdz 12109
IGdyYXZpdHk= 12110
IOynkQ== 12111
ZW1wbG95 12112
IGRpZXNlcw== 12113
IGRpc2NvdmVyeQ== 12114
0YHRgtCy0LA= 12115
IGhlYmJlbg== 12116
IGdlcmFkZQ== 12117
IERS 12118
ICcn 12119
IHRlY2huaWNhbGx5 12120
INCf0L4= 12121
IHByaXZpbGVnZQ== 12122
IEV2ZXI= 12123
IFNlcnZpY2Vz 12124
dXJhbg== 12125
IGNvbnN1bXB0aW9u 12126
IFJldg== 12127
IFNoYWxs 12128
YXNzZXI= 12129
toDthLA= 12130
IHJhY2lhbA== 12131
IFlvdXR1YmU= 12132
IFByYQ== 12133
0YHRgtCy0LXQvQ== 12134
Y2Vr 12135
5rQ= 12136
YXNoYQ== 12137
INuB 12138
kZw= 12139
YWhu 12140
SUNL 12141
IGRyaW5rcw== 12142
IGNhcmI= 12143
44K/ 12144
IDY0 12145
IE1tbQ== 12146
IGVsZWN0cmljYWw= 12147
IGZydWl0cw== 12148
IEhE 12149
w7Fh 12150
IERlZmluaXRlbHk= 12151
IOuwmw== 12152
IGZhaXM= 12153
cmF0aW9ucw== 12154
IGNvZQ== 12155
YWh1 12156
IEZhaXI= 12157
IGVhdGVu 12158
IGZpcg== 12159
IEF1 12160
0YPQvQ== 12161
dWxhdGluZw== 12162
aW5nbHk= 12163
IHZhY2NpbmVz 12164
IGRyYWdvbg== 12165
IHBvaW50aW5n 12166
IHBlbG8= 12167
b3J0ZXJz 12168
IHdvcmtvdXQ= 12169
0LjQvNC10YA= 12170
bW9uZA== 12171
IE5vcGU= 12172
INeW15Q= 12173
INmC 12174
IGFkb3B0ZWQ= 12175
YnVs 12176
IHNhbnM= 12177
IHBvc3NpYmlsaXRpZXM= 12178
IHBlbmQ= 12179
IHphbWFu 12180
aG91 12181
IHNoYXJlcw== 12182
IGNhbGVuZGFy 12183
IHBlcnNvbmE= 12184
IHNlYWw= 12185
IGdlbmU= 12186
IHN0b3JlZA== 12187
INC/0L7Qtw== 12188
IGx5cmljcw== 12189
IGluc3RydW1lbnRz 12190
IE1B 12191
gOydtA== 12192
IGNsb3Vkcw== 12193
aG90 12194
4bqv 12195
IOqwmeyVhOyalA== 12196
IEVtcGlyZQ== 12197
IGJpbw== 12198
d2luZA== 12199
aWVnbw== 12200
IEV1cm9w 12201
IOWlvQ== 12202
ZWRnZQ== 12203
IGJhY2t3YXJkcw== 12204
IOyngOs= 12205
IHF1ZWVu 12206
IHNoaW5l 12207
IMOnxLFr 12208
IGNhZA== 12209
IE9k 12210
IOyCrOuejA== 12211
IGJ1YmJsZQ== 12212
w7Rp 12213
emVz 12214
IHJlYWN0aW9ucw== 12215
IGp1ZGdtZW50 12216
IERlbW9jcmF0cw== 12217
IGNvc2Fz 12218
YXNoZWQ= 12219
INC00L7Qu9C2 12220
xZtuaWU= 12221
6rQ= 12222
IGV4ZW1wbGU= 12223
TVA= 12224
4buv 12225
IFZlcnM= 12226
IHJlc2ls 12227
IG3DoQ== 12228
xYRzdA== 12229
YmVsaWV2 12230
IFZvcg== 12231
IHNjaGVtZQ== 12232
b25kYQ== 12233
IHBvZGVtb3M= 12234
IGNoYXJnZXM= 12235
IGRlc3RpbmF0aW9u 12236
IEt5 12237
IFNT 12238
IHNpbGVuY2U= 12239
IGVtYmVk 12240
bmF0 12241
4bubaQ== 12242
QU5U 12243
Z2dlZA== 12244
IHJlZHVjaW5n 12245
IHVnbHk= 12246
IG1pbQ== 12247
0YPQtNCw 12248
MzQ= 12249
IGNvcmQ= 12250
INGC0L7QttC1 12251
IExpc2E= 12252
IMO2bg== 12253
IENocmlzdGlhbnM= 12254
dW1ibGVz 12255
b2xvZ2lzdHM= 12256
YXph 12257
IHRlbmRz 12258
IENvb2s= 12259
IGdlc2FndA== 12260
IO2VmOuCmA== 12261
IFRlcw== 12262
ZXJlbW9ueQ== 12263
INC90YPQttC90L4= 12264
IE1BUklTSEE= 12265
IGVucm9sbA== 12266
IENyeQ== 12267
RVNT 12268
IFNhZA== 12269
IGltcGxlbWVudGVk 12270
IGTDrWE= 12271
w5w= 12272
IHBpc3Q= 12273
RHI= 12274
IHNhYmU= 12275
IFNvY2k= 12276
w6RyZQ== 12277
INC60YLQvg== 12278
IEZyYW5jaXNjbw== 12279
IOyepQ== 12280
IGNyZWF0dXJlcw== 12281
YXdz 12282
IGVhcm5lZA== 12283
IGNoZWFwZXI= 12284
IGRsYQ== 12285
IHdhcm4= 12286
c2NoZQ== 12287
IGJsYWg= 12288
IG51dHI= 12289
6Lw= 12290
IGdvcmdlb3Vz 12291
IEFuZ2VsZXM= 12292
IGdlbWFjaHQ= 12293
IGhvbWVsZXNz 12294
b2dyYXBoaWM= 12295
IFRhaXdhbg== 12296
IFNvbQ== 12297
IEhhZA== 12298
YWN0aW9ucw== 12299
IHBvc3Rz 12300
IG91dHJh 12301
IE1lYW4= 12302
a2Fy 12303
IGNvdXM= 12304
IGJyYWNr 12305
0LjRgtGM0YHRjw== 12306
IGJlbGlldmVz 12307
IHN1aWNpZGU= 12308
IGVxdWFsbHk= 12309
IGNhcmVz 12310
0L7QttC90L4= 12311
IHN0ZW0= 12312
IE11Y2g= 12313
IHByb2R1Y2Vy 12314
15XXkA== 12315
IHByb3RlY3Rpbmc= 12316
IFRSQVZJUw== 12317
IGludGVydmlld3M= 12318
IGFsaWVu 12319
IEFzaw== 12320
IHNvbGU= 12321
Q08= 12322
IFN1ZA== 12323
IHN1cnZpdg== 12324
IHNrZXRjaA== 12325
IHfFgmE= 12326
IGNvbG9j 12327
IGFwb2xvZ2l6ZQ== 12328
d2VpZ2h0 12329
IDU1 12330
ID4= 12331
IGhlcm9lcw== 12332
IEJvc3Rvbg== 12333
IGRlcGVuZGVudA== 12334
IG1vdGl2YXRpb24= 12335
ZmxpeA== 12336
IHNlYW0= 12337
0LrQuNC1 12338
IGRyYWlu 12339
b2RlZA== 12340
IGd1aWx0eQ== 12341
IEplbm4= 12342
aW5nZW4= 12343
IGdyYW50ZWQ= 12344
IEtlbGx5 12345
IFNhdg== 12346
IFVuY2xl 12347
IEhvbmVzdGx5 12348
RUxJ 12349
IG5hdmlnYXRl 12350
IGJsZXNzZWQ= 12351
Y29yZQ== 12352
IGVhcm5pbmc= 12353
IHNpZ25hbHM= 12354
IGRpc2s= 12355
aWFscw== 12356
IGFnZXM= 12357
5oU= 12358
IHBhcnRpY2xl 12359
INGH0LXRgA== 12360
IGNhbm4= 12361
IHRpZXI= 12362
IHN0YXRlbWVudHM= 12363
6rOg7JqU 12364
IOuVjOusuOyXkA== 12365
IENobw== 12366
IHBvbGFy 12367
YW7Dpw== 12368
IEtlbm4= 12369
IE5p 12370
IEZpZ2h0 12371
b3JnYW4= 12372
6ZU= 12373
IENoYQ== 12374
IFPDrQ== 12375
44Oq 12376
IHNsaWM= 12377
IGNlcnRpZmlj 12378
IHRlbXBsYXRl 12379
IEZlZGVyYWw= 12380
IGNvbnNpZGVyYXRpb24= 12381
IGV4cGxv 12382
IE1haW4= 12383
IE5F 12384
IGFsb25nc2lkZQ== 12385
IGRyZXNzZWQ= 12386
IFBvaW50 12387
IGVudmlyb25tZW50cw== 12388
IHByw7N4aW0= 12389
IGRhYXI= 12390
IHByb21wdA== 12391
IHB1cnN1ZQ== 12392
IGVudGVydGFpbm1lbnQ= 12393
IHRocm9hdA== 12394
IHByb2JsZW1h 12395
IG1hcnQ= 12396
7Lw= 12397
IHByb3ZpZGVy 12398
2Iw= 12399
INeX 12400
aW50ZQ== 12401
bWFraW5n 12402
IHN0cm9rZQ== 12403
IHRpc3N1ZQ== 12404
VW4= 12405
IHByZWNpb3Vz 12406
IEFydHM= 12407
aW5raW5n 12408
INCe0L0= 12409
INC40YE= 12410
bmFo 12411
INCV0YHQu9C4 12412
IGNvcm5lcnM= 12413
IHRyaWNreQ== 12414
aW5jaA== 12415
bGlqaw== 12416
IHByZXNzaW5n 12417
bGV2ZWw= 12418
QU5H 12419
IHJhZGlhdGlvbg== 12420
7ISg 12421
IGNvbmZyb250 12422
IHZldA== 12423
IHJlcHJlc2VudGF0aXZl 12424
IHByb3BhZw== 12425
IGNyYXA= 12426
IERlYw== 12427
IHJhbXA= 12428
0LXQv9C10YDRjA== 12429
dcOpcw== 12430
ZXNzZW4= 12431
Y3JpcHRpb24= 12432
IGJpbGxz 12433
IE1hdHRoZXc= 12434
IGFuaW1l 12435
4bqldA== 12436
IGxvd2VzdA== 12437
aGFz 12438
c2NyZWVu 12439
b2dyYXA= 12440
0LDQu9C+ 12441
aW50b24= 12442
IEphaA== 12443
6ICF 12444
aXTDoA== 12445
IGtheQ== 12446
IHJvdGF0aW9u 12447
IFdlcmU= 12448
YWJlaQ== 12449
IHRyaWFscw== 12450
IGxldmVy 12451
aWdodHk= 12452
IHNwb29u 12453
IGh1bnQ= 12454
Y2xpbmc= 12455
IGRpc20= 12456
INCx0L7Qu9GM0Yg= 12457
IGFzc2F1bHQ= 12458
IO2YlQ== 12459
IHdlZWtseQ== 12460
IG1pc21v 12461
IGdlbmV0aWM= 12462
dWxwdA== 12463
IFN0dWRlbnQ= 12464
IHJlYWxpc3RpYw== 12465
IGF1dGhlbnRpYw== 12466
5omT 12467
YXN0YQ== 12468
IGFycmVzdGVk 12469
IGd1aWRlbGluZXM= 12470
INec15A= 12471
INC00LDQsg== 12472
IENvbWluZw== 12473
ZsO8cg== 12474
IHJlcXVlc3Rz 12475
g5A= 12476
IGFuYWx5emU= 12477
IGludGVyZXNz 12478
IGhhbHQ= 12479
IE9wZXI= 12480
b25vbQ== 12481
IGR1Y2s= 12482
IHdpdGhk 12483
c2Vy 12484
IM+M 12485
IEhpc3Rvcnk= 12486
IHlvdXR1YmU= 12487
44KN 12488
IHNhYmVy 12489
d2Fsaw== 12490
Zm9udA== 12491
IG92ZXJ2aWV3 12492
Mzk= 12493
w7x5 12494
ZXR0aQ== 12495
IGZyb3plbg== 12496
IGZsZXNo 12497
xJ9p 12498
IFBN 12499
IOyZgA== 12500
6aI= 12501
0YbQuNC4 12502
IOq4sOs= 12503
7YGs 12504
IHByb3Nl 12505
b29vbw== 12506
cmF0ZXM= 12507
V1M= 12508
IGF1dG9tYXRpYw== 12509
IGNvbGxlY3Rpbmc= 12510
xZE= 12511
IG5laWdoYm9ycw== 12512
wrsu 12513
IEV4cGw= 12514
IGNpcmN1bA== 12515
Y292ZXI= 12516
d2Vn 12517
IHN0aWNrcw== 12518
IGVsbGVy 12519
IHd3dw== 12520
IGRvcm0= 12521
IEV4cGVy 12522
IHN0YXRpc3RpY3M= 12523
IGVtYWlscw== 12524
IGdyYXZl 12525
aW1peg== 12526
SFM= 12527
IHVpdA== 12528
LCc= 12529
IGxhc2Vy 12530
6Ik= 12531
INGC0LXQvA== 12532
0YvRiA== 12533
0YnRkQ== 12534
IGdlbmF1 12535
IHRpZW5lbg== 12536
IG1lZGl0YXRpb24= 12537
IE9yZ2Fu 12538
IGVzdGltYXRl 12539
IOustOw= 12540
bGV0cw== 12541
IG7DoHk= 12542
IG1pbmRzZXQ= 12543
IHJlc29u 12544
IG3DqXM= 12545
IG51bWVyb3Vz 12546
IHZpZWxsZWljaHQ= 12547
IFRoaXJk 12548
dW91cw== 12549
IERlYWQ= 12550
0LDQvdC0 12551
SE4= 12552
IHJhY2luZw== 12553
IGFnZW50cw== 12554
IFV0 12555
IHRlYXI= 12556
IEhQ 12557
IGNoZW1pc3RyeQ== 12558
IHN1cnZpdmFs 12559
5paw 12560
IGNvbnZpbmNlZA== 12561
IDs= 12562
IHJlZ3VsYXRpb25z 12563
IEVT 12564
5ZKM 12565
MzAw 12566
IGVuc2U= 12567
IOy1 12568
IGRpY3Q= 12569
R0E= 12570
IGFow60= 12571
5YuV 12572
IHRlag== 12573
INC+0YHRgg== 12574
IEVsZWN0 12575
IGludGVsbGVjdHVhbA== 12576
IGJpYXM= 12577
IGJ1cmRlbg== 12578
54K5 12579
IOyWtOuWuw== 12580
IGNoZWVy 12581
IHNvcGg= 12582
IHBvcnRmb2xpbw== 12583
dWJh 12584
IGVzdG9z 12585
VFY= 12586
Rm9y 12587
IGFzaA== 12588
IGtvbW1lcg== 12589
IGNvbGxlY3RpdmU= 12590
IHdyZXN0 12591
IEpldHp0 12592
IFdhdA== 12593
cmVpY2g= 12594
IHByaW1lcg== 12595
YWN0aXZl 12596
IG1pZQ== 12597
aWNrZWQ= 12598
IGh1bnRpbmc= 12599
IHRlc3RpbQ== 12600
IGNvbXBhc3Npb24= 12601
INix 12602
IGJydXQ= 12603
IHNhbGFk 12604
0L7QsdGJ0LU= 12605
IHNvbHZpbmc= 12606
IGZsb2F0aW5n 12607
57c= 12608
IGF0dHJhY3RpdmU= 12609
2YjZhA== 12610
IHBlcmQ= 12611
aWZmZXI= 12612
IHNjdWxwdA== 12613
aGho 12614
IFdlZWs= 12615
IGVudGh1cw== 12616
IG5hZA== 12617
IG1lcmNo 12618
IO2ZlQ== 12619
IG1pbGU= 12620
5aW95LqG 12621
IM64 12622
IOuCmOs= 12623
6YeN 12624
Mzg= 12625
IGNoYWlucw== 12626
IEFsbW9zdA== 12627
IHRpY2tldHM= 12628
cmlu 12629
IEND 12630
IGRpc3RyaWJ1dGVk 12631
YWJldGVz 12632
IHRlbXBlcmF0dXJlcw== 12633
IGdhaW5lZA== 12634
IGZsZXhpYmlsaXR5 12635
IHNjcmVhbWluZw== 12636
IGFicm9hZA== 12637
dW5v 12638
IGVudHJlcHJlbmV1cnM= 12639
IE5ldHdvcms= 12640
IENhbmFkaWFu 12641
IHByZXY= 12642
IHPDtg== 12643
INGC0LXQsdGP 12644
IFBva2U= 12645
IFBvZA== 12646
IFR1cmtleQ== 12647
54++5Zyo 12648
IGFic3RyYWN0 12649
IHNuYWtl 12650
IEFteQ== 12651
IOuKkOuCjA== 12652
IGJyYXZl 12653
IOyeiOyWtOyalA== 12654
IEthbA== 12655
IDIwMDc= 12656
w6FyaW8= 12657
IG1hcmtlZA== 12658
Z2luZXM= 12659
IGFsbG9j 12660
T05H 12661
IHNjaWVudGlzdA== 12662
IGVzY2E= 12663
IHJhY2lzbQ== 12664
15HX 12665
IFNhbXM= 12666
IFBlbm4= 12667
IGxvYWRz 12668
IOCuqA== 12669
w7xiZXI= 12670
TWU= 12671
aXjDsg== 12672
IHBlcsOy 12673
YW5uZQ== 12674
IGV4cHJlc3NlZA== 12675
0LzQtdGA 12676
IG1vZXQ= 12677
IHJldHVybmluZw== 12678
bmlh 12679
IGV4cG9u 12680
UHJv 12681
IGxveWFs 12682
TUw= 12683
IGxhbXA= 12684
IHNoeQ== 12685
IGNvbXBvc2l0aW9u 12686
IEx5 12687
IG1hZ25ldGlj 12688
IHByZW1pZXI= 12689
IG1lYXN1cmVk 12690
IHN1bW1hcnk= 12691
IGF0dGFja2Vk 12692
IGZpbmlzaGluZw== 12693
0Jc= 12694
56U= 12695
IHNpdHM= 12696
IGh5ZHJvZ2Vu 12697
IG1haQ== 12698
IERldXRzY2g= 12699
YXPEsQ== 12700
IG9idGFpbg== 12701
dmll 12702
IHNvaXQ= 12703
IOuwlA== 12704
IGxhbmU= 12705
IGNvbnNlZ3U= 12706
0LLQvg== 12707
IGVhc2U= 12708
YWtpbg== 12709
IEZh 12710
IHVudHVr 12711
IGJ1cnN0 12712
IGN1bQ== 12713
YWzEsW0= 12714
w7pibGlj 12715
aWRp 12716
IFJveWFs 12717
IEtvbg== 12718
IGNvbW1vbmx5 12719
IHJlbW92aW5n 12720
IGp1cg== 12721
aWxpYg== 12722
IGFuY2g= 12723
7ZaJ 12724
xrDhu6M= 12725
INCc0Ys= 12726
IEFudGg= 12727
IFPDpQ== 12728
IGludGVycnVwdA== 12729
IHN0ZXJl 12730
IE9T 12731
b255bQ== 12732
dGVyeQ== 12733
IE1hcmlh 12734
6rKD 12735
IGV4cGxvcmluZw== 12736
IHRyYW5zcGFyZW50 12737
IGZhdGU= 12738
IEp1bmc= 12739
IGdydXA= 12740
IGRhcmtlcg== 12741
IERvdWc= 12742
IG1hbmU= 12743
5pS+ 12744
4bqhaQ== 12745
ZHJp 12746
bG9vaw== 12747
IERlc2lnbg== 12748
IHR1dGFq 12749
IGhvcml6b250YWw= 12750
cmVvbg== 12751
b3J0ZQ== 12752
IENvcnJlY3Q= 12753
IFN0ZXZlbg== 12754
IHZpbmU= 12755
MDI= 12756
acSH 12757
IHNpZW1wcmU= 12758
IEtleQ== 12759
5YOP 12760
IEdhbWVz 12761
IG5hYXI= 12762
IHNob2NrZWQ= 12763
ZWx2ZQ== 12764
IFJvc2U= 12765
7Ius 12766
IHN0b3BwaW5n 12767
b2hs 12768
IE1peA== 12769
IHN1ZmZlcmVk 12770
IHNpZ21h 12771
IHdlYWtuZXNz 12772
IE93 12773
4Li14LmI 12774
SUY= 12775
IOCuhQ== 12776
YWRlZA== 12777
IE5ldGZsaXg= 12778
YW5lcw== 12779
IHJlbWFpbmVk 12780
aXJ5 12781
IHJpcA== 12782
ZWxsdA== 12783
IHNpbGVudA== 12784
IHByb3Zlbg== 12785
IHRveGlj 12786
IGFsdW1pbg== 12787
IG11bHRpcGw= 12788
YWxhbmQ= 12789
IDM0 12790
MDY= 12791
IEJydQ== 12792
IOygleunkA== 12793
SnVzdA== 12794
Ym95 12795
IHNob2U= 12796
IGNyZWF0dXJl 12797
IGhlYWRlZA== 12798
INC+0YLQug== 12799
5rE= 12800
IGVzc2VuY2U= 12801
IHJlbWFya2FibGU= 12802
IG7Dum1lcg== 12803
IGRyZXc= 12804
IHB1enpsZQ== 12805
IExpYnJhcnk= 12806
IEZ1 12807
YXNoZXM= 12808
a2s= 12809
IElzdA== 12810
prA= 12811
IEJyeQ== 12812
IGNlcmVtb255 12813
IOCujg== 12814
IGNyaQ== 12815
ZXF1 12816
44Ki 12817
IHByaXpl 12818
IGRpbWVuc2lvbnM= 12819
b2dyYW0= 12820
IGxlYXRoZXI= 12821
IHBvcHVsYXRpb25z 12822
dXVt 12823
IHZlZ2Fu 12824
0Y/QtA== 12825
IGPDs21v 12826
5YQ= 12827
IHN0cmlw 12828
5aM= 12829
IHZhY2F0aW9u 12830
hZU= 12831
IG1lYWxz 12832
aWxpcHA= 12833
IGVudHM= 12834
YXJhbQ== 12835
cmljaHQ= 12836
IGdyYWlu 12837
IFNwYWlu 12838
IGNoZWVr 12839
IEFmZg== 12840
SU9O 12841
IEJyaW5n 12842
IDM4 12843
aWVsZW4= 12844
dWx1 12845
INCx0L7Qu9GM0YjQtQ== 12846
IGFubm91bmNlbWVudA== 12847
INGC0YPRgg== 12848
IFByb3BoZXQ= 12849
YXJkbw== 12850
Mzc= 12851
IHdva2U= 12852
IHRyYW5zbGF0aW9u 12853
IE5PVA== 12854
IENM 12855
IGTDvMWf 12856
0YbRlg== 12857
YWNlcg== 12858
IExvYw== 12859
IHBlcmNlcHRpb24= 12860
Tk8= 12861
IGRpZXNlbg== 12862
TG9vaw== 12863
aGVhcnQ= 12864
YXZlZA== 12865
IGJvdW5kYXJ5 12866
IGZsb3dz 12867
0ZHQvA== 12868
IGFyZ3VtZW50cw== 12869
IGVsZWN0aW9ucw== 12870
xLFz 12871
IGhlY2s= 12872
IHN1aXRhYmxl 12873
IGZpYmVy 12874
IFN0cmE= 12875
eHk= 12876
IEh1bQ== 12877
IG1vbnRobHk= 12878
dXBlcg== 12879
IGdvbGY= 12880
IGxhdGVseQ== 12881
IEdhcmQ= 12882
IFJlbg== 12883
IEFzdA== 12884
IEZhbnQ= 12885
0LDRgdGB 12886
IG9ic2Vy 12887
66Gc 12888
IGVhc2llc3Q= 12889
jZTr 12890
IHdlYnNpdGVz 12891
cG9s 12892
IGNvY29u 12893
IOCuhw== 12894
IFZlZw== 12895
IHdhbGtz 12896
IGludHJv 12897
IGRpcmVjdGVk 12898
IEFubmE= 12899
IOuTpOyWtA== 12900
IEVhc3Rlcm4= 12901
IFNhaW50 12902
IEJvdw== 12903
IHJvYXN0 12904
IFVSTA== 12905
IGplZGVu 12906
dXJhcw== 12907
YWph 12908
IHNlbWk= 12909
IHJhcGlkbHk= 12910
IHRhcmdldHM= 12911
IENvbnRyb2w= 12912
IGJhaA== 12913
IHJlZmxlY3Rpb24= 12914
IGNyZWF0aXZpdHk= 12915
aG9sZGVycw== 12916
IOyYrOs= 12917
IGFtb25nc3Q= 12918
IGZlZWRpbmc= 12919
0Y3RgtC+0LzRgw== 12920
INCy0LjQtNC1 12921
IOunjOuTpA== 12922
IFNtYXJ0 12923
IHJlbGlhYmxl 12924
IHZlemVz 12925
INeo 12926
Y2h1Y2tsZXM= 12927
YXppb25l 12928
IFdpbGxpYW1z 12929
IGHDpw== 12930
IHNsZWU= 12931
0LXRiQ== 12932
IHRpbWVsaW5l 12933
IHRob3JvdWdo 12934
4buN 12935
IE90 12936
4bqhbg== 12937
IGltYWdpbmF0aW9u 12938
IG1lY2hhbmljcw== 12939
cmlzdA== 12940
IGNsYWltZWQ= 12941
z4TOtw== 12942
w6p0ZQ== 12943
IEh1cnJ5 12944
IGlQYWQ= 12945
IGNvbnN0cnU= 12946
IENsYQ== 12947
IEFscw== 12948
5Lya 12949
dXR6 12950
IGN1bHR1cmVz 12951
IOyWtOuWu+qyjA== 12952
IGJlbG9uZ3M= 12953
IHllcg== 12954
IERvZXNu 12955
IGdlb21ldA== 12956
IGJpZA== 12957
IGZvYW0= 12958
IGhvYg== 12959
IEJyaXRhaW4= 12960
IHN1YnN0YW5jZQ== 12961
IGFubml2ZXJzYXJ5 12962
IOuEiA== 12963
IG5vdGVk 12964
IGdvdmVybm9y 12965
IHN0b2Nrcw== 12966
MzE= 12967
IGRpeWU= 12968
7Iqk6w== 12969
IHJlYg== 12970
emVs 12971
IG11bHRpcGx5 12972
IG9wZXJhdG9y 12973
hKTsmpQ= 12974
IHdhdGVycw== 12975
IGTDpHI= 12976
IHVuc2Vy 12977
IEVsaXphYmV0aA== 12978
6auY 12979
IGluY3JlYXNpbmdseQ== 12980
IEdybw== 12981
IGVuZ2luZXM= 12982
aXJz 12983
2Ks= 12984
IHRyZWFzdXJl 12985
UEM= 12986
aW5jdGlvbg== 12987
aXJp 12988
IGFjY3Vt 12989
IHZhcmlhdGlvbg== 12990
IHBvbQ== 12991
IHRpdGxlcw== 12992
IEZlc3Q= 12993
w7Nz 12994
IGVsZGVy 12995
bnlt 12996
cnVu 12997
0Y/Qsg== 12998
IGlubm92YXRpdmU= 12999
IG5vbWJyZQ== 13000
IGNvaW5j 13001
IGZyYW5jaA== 13002
IGVudG9uY2Vz 13003
IG5pY2h0cw== 13004
IGV4Y2x1c2l2ZQ== 13005
IENoZWVycw== 13006
IEJp 13007
dWpl 13008
5q2h 13009
IHBvaw== 13010
IFByZW0= 13011
IHJvY2tldA== 13012
RUxJUEU= 13013
IGhvc3BpdGFscw== 13014
cml1bQ== 13015
IGp1c3Rl 13016
IGhhbW1lcg== 13017
IHF1YW50dW0= 13018
IHJlc3BvbnNlcw== 13019
bGx5 13020
ZW5kaQ== 13021
IGFjdGl2ZWx5 13022
IGZyaWRnZQ== 13023
aWF0ZQ== 13024
bG9uZw== 13025
IHF1ZW0= 13026
IGRlYXRocw== 13027
IHN1cGVyaW9y 13028
Y2tlbg== 13029
7J207JeQ 13030
a3RvcA== 13031
IGdhdGhlcmVk 13032
o6g= 13033
IGRhenU= 13034
IHJlY2lwZXM= 13035
IGJ1eno= 13036
Y2Vu 13037
IGFueXRpbWU= 13038
b25zZW5zZQ== 13039
IGNpcmNsZXM= 13040
IHNvbHZlZA== 13041
IOyLoA== 13042
IGNvcm9uYXZpcnVz 13043
IEx1a2U= 13044
IGJ1YmI= 13045
IGNvbnRlbXBvcg== 13046
cnp5 13047
IEphbmU= 13048
INC00L7QvA== 13049
IHNjcmV3cw== 13050
IGh5YnJpZA== 13051
IGNhc3VhbA== 13052
IHNlbGJzdA== 13053
YmVpbmc= 13054
IMSQ 13055
IENvbHVtYg== 13056
INGF0L7Rhw== 13057
IGJ1Y2tldA== 13058
IGV2YWx1YXRl 13059
IGlkb2w= 13060
IHJlcHV0YXRpb24= 13061
IOyGjOs= 13062
2YjYsQ== 13063
IGhlY2hv 13064
IHBvZW0= 13065
IHN1YmplY3Rz 13066
cGxhbnQ= 13067
IEJlaA== 13068
IFNwZWFraW5n 13069
IGJhdHRlcmllcw== 13070
IGZvbGxvd2Vycw== 13071
w7Zs 13072
IGdlbnRseQ== 13073
IHNpeHQ= 13074
IHBhcmFtZXRlcg== 13075
IGlra2U= 13076
IFRvdXI= 13077
IERK 13078
b3R0ZQ== 13079
IEphaHJlbg== 13080
IHByZXBhcmF0aW9u 13081
INC00YPQvA== 13082
IDgwMA== 13083
Y29w 13084
aWtpbmc= 13085
IOusuA== 13086
INC90YM= 13087
INC70LXRgg== 13088
5ZCM 13089
IElkZQ== 13090
IOyhsOq4iA== 13091
IGxhdWdodGVy 13092
IG1vbGVjdWxlcw== 13093
IFJlc3Q= 13094
IG9ic2VydmVk 13095
ZHppZQ== 13096
IGFkdmVydGlzaW5n 13097
ZXJ0bw== 13098
IG1vaW5z 13099
IE1JVA== 13100
IGV4Y2l0 13101
IHR1bQ== 13102
IHR5bA== 13103
IGludmVzdGVk 13104
IHBoYXJt 13105
IHVuZXhwZWN0ZWQ= 13106
IHBoaQ== 13107
b3R5cGU= 13108
d2Vpc2U= 13109
IGdlw6c= 13110
am91cmQ= 13111
IGhvcnNlcw== 13112
bsSF 13113
PSI= 13114
IFNN 13115
IGZpYg== 13116
IGNsaXBz 13117
55W2 13118
5aaC5p6c 13119
IHJlZ2ltZQ== 13120
IHJvdGF0ZQ== 13121
cm91 13122
bmlr 13123
IGFybW9y 13124
8J+Y 13125
0LXRgNCw 13126
5bqm 13127
IE9jaA== 13128
IHJpY2h0aWc= 13129
w7x6ZWw= 13130
YW5lb3VzbHk= 13131
bWVr 13132
6Yyv 13133
IFhpYW8= 13134
IGV4aXN0ZWQ= 13135
d29ydGg= 13136
44Gj44Go 13137
IG5hdWdodA== 13138
IGhlacOfdA== 13139
IEJhbA== 13140
IHJlc2lk 13141
aXZvdA== 13142
b21hdGlj 13143
IGhpcmVk 13144
IGdyYWR1YWxseQ== 13145
IG9uaW9ucw== 13146
IGNvbXBhdA== 13147
IGludGlt 13148
IGpldw== 13149
IGNvbnRyaWJ1dGlvbg== 13150
IElyZQ== 13151
YWNqaQ== 13152
IHNsaWNl 13153
IGltbXVu 13154
IFJ1cw== 13155
IGdyb3dz 13156
IFNpbWlsYXJseQ== 13157
IGhhcmRlc3Q= 13158
IHN0cnVjaw== 13159
IG1lYXN1cmVtZW50 13160
Li4uXQ== 13161
dGhleQ== 13162
IOyggOs= 13163
IHNuZWFr 13164
IGFwcGxpZXM= 13165
INC90LXQvA== 13166
5pM= 13167
15HXqA== 13168
INCn0YLQvg== 13169
IG91dHJv 13170
IGlubm9jZW50 13171
IG1vZw== 13172
IFNhbXN1bmc= 13173
IG1lcmN5 13174
IGhhbmRsaW5n 13175
IGludGVydmVudGlvbg== 13176
aWRheXM= 13177
Z290 13178
IGN1cnJpYw== 13179
IGJvdW5kYXJpZXM= 13180
IGNvbmZ1c2luZw== 13181
nbzripQ= 13182
5oc= 13183
IHN0aXRjaGVz 13184
w612ZWw= 13185
IHR1bm5lbA== 13186
aXTDpA== 13187
IGdvc3Q= 13188
aW15 13189
IGN6YXM= 13190
IG3DqQ== 13191
IGNhdGFs 13192
IFNpbW9u 13193
IExJQU0= 13194
bWlj 13195
INCk 13196
IGV5ZWw= 13197
aXNhcw== 13198
IENQVQ== 13199
IERvdQ== 13200
IG7DpGNo 13201
IGluZmluaXR5 13202
IHJpZg== 13203
IFBlYWNl 13204
IEN1 13205
IG1pbmltYWw= 13206
IGxpc3RlbmVk 13207
IHBvbGU= 13208
aGFsYg== 13209
IGxvYWRlZA== 13210
IHN0ZWFkeQ== 13211
IEJlc2lkZXM= 13212
w6pt 13213
IGxhcA== 13214
IGNvb3A= 13215
IGZyaWVuZHNoaXA= 13216
d29ybGQ= 13217
IGdlaA== 13218
IHR5bGtv 13219
IExhdXJh 13220
IHN1cnJvdW5kZWQ= 13221
IEV2ZW50 13222
IGNoYXA= 13223
IFdvbmRlcg== 13224
YnJlYWs= 13225
IGRyb3Zl 13226
IGJyb2FkZXI= 13227
IGNoaQ== 13228
Rmk= 13229
IGdlaGVu 13230
IHdlc3Rlcm4= 13231
IGludGVsbGlnZW50 13232
IHBlcnNpc3Q= 13233
IGZvdW5kZWQ= 13234
44GT44Go 13235
IGhpc3Rvcmlj 13236
IGZyw6U= 13237
Y2tzw6U= 13238
IGhhbmR5 13239
IHN5bXA= 13240
IHJvd3M= 13241
IG51dHJp 13242
YnVy 13243
IExlb24= 13244
IHNpc3RlbWE= 13245
IGV4dGVuc2l2ZQ== 13246
INGD0LI= 13247
7Y8= 13248
IG5pZ2h0cw== 13249
IGPDoWM= 13250
IGNvdW50aW5n 13251
IE11c3Q= 13252
YWxsb3c= 13253
0LXRgdGB 13254
TW9t 13255
INC90LDQtNC+ 13256
IGJhcnJlbA== 13257
44Oe 13258
QVJE 13259
IGluc3RhbGxhdGlvbg== 13260
IGluc2VjdA== 13261
IOuFuOs= 13262
dWrEhQ== 13263
IMSRaQ== 13264
IHBhY2tlZA== 13265
IGZpY3Rpb24= 13266
Tm93 13267
IFlheQ== 13268
IHBlcnQ= 13269
cm9ucw== 13270
dW5kZQ== 13271
YWNoZXM= 13272
IHN0eWxlcw== 13273
IGFwcsOocw== 13274
b2t1 13275
IFZpY2U= 13276
xLFuxLF6 13277
Y29tbQ== 13278
IGFzc2lnbmVk 13279
IGludGVyYWN0aW9ucw== 13280
IGFjYWI= 13281
RkVMSVBF 13282
IHJlc2N1ZQ== 13283
IGluZHVzdHJpZXM= 13284
IEFuZHk= 13285
IHByYWlzZQ== 13286
IGZsYW1l 13287
IHNuYWNr 13288
7YI= 13289
54E= 13290
IHN3bw== 13291
cmVuZGVy 13292
IGJvYXJkcw== 13293
INGC0L7QvA== 13294
ZW5uZQ== 13295
IHBhc3Rh 13296
IGRldmls 13297
IEZlbA== 13298
IGhhdHRl 13299
IGNvbGxlZw== 13300
ZWg= 13301
7Ls= 13302
44GT44Gu 13303
IHByb2R1Y3RpdmU= 13304
Zm9yd2FyZA== 13305
0LjQvw== 13306
IHNtYXJ0cGhvbmU= 13307
IGludmlz 13308
IGJ1bQ== 13309
IHdob2E= 13310
7J6E 13311
IG9ja3PDpQ== 13312
IExhbmc= 13313
IFN5cmlh 13314
IHNlc2k= 13315
zq/OsQ== 13316
IGFwcHJvdmFs 13317
NDg= 13318
INC+0LTQuNC9 13319
IOuW 13320
IEhhcnI= 13321
IEFkbWluaXN0 13322
INek 13323
IERlYW4= 13324
Zmk= 13325
IGNpdGl6ZW4= 13326
IHNoYXJr 13327
MDU= 13328
IGJvaWw= 13329
IGluZGljYXRl 13330
5aE= 13331
QXJl 13332
IGxheW91dA== 13333
IHJlZnI= 13334
IFBhY2lmaWM= 13335
QUFBQQ== 13336
IEF1c3RyYWxpYW4= 13337
Z3Jlc3Npb24= 13338
Vm9pY2U= 13339
0LDQu9GB0Y8= 13340
IHNoZWx0ZXI= 13341
VG8= 13342
YXVwdA== 13343
IGV2YWx1YXRpb24= 13344
YXBvcg== 13345
IGN1cnJlbmN5 13346
INC80L3QvtCz0L4= 13347
aWdvcw== 13348
44Gw 13349
IG9jdA== 13350
IHJveWFs 13351
6LM= 13352
YXNpbA== 13353
IENoaWxkcmVu 13354
IHJpZW4= 13355
IOuTnOs= 13356
IGJhcnJpZXI= 13357
IGVqZW1wbG8= 13358
IGVr 13359
TkQ= 13360
ZXNw 13361
0LXQvdCw 13362
IHBpYw== 13363
IGtpbGxlcg== 13364
IGludGVncmF0ZQ== 13365
IGZld2Vy 13366
IGRpc2FiaWxpdGllcw== 13367
IC4uLi4= 13368
IHRyaWFuZ2xl 13369
IGZlZXM= 13370
IHdpZGVseQ== 13371
ZW1p 13372
IG92ZXJ3aGVsbWluZw== 13373
IHpvbWI= 13374
IGJlcmU= 13375
IGhvb2Q= 13376
IEF5ZQ== 13377
IEhhcnZhcmQ= 13378
ZXY= 13379
IM+Ezr/PhQ== 13380
IGN1cHM= 13381
IEF1Y2g= 13382
em9uYQ== 13383
IDE5OTA= 13384
IHdlacOf 13385
IGNydW5jaA== 13386
5qU= 13387
INC30LDQsg== 13388
IG1lYXN1cmluZw== 13389
IHN0YXRpb25z 13390
IFN0ZXBoZW4= 13391
IHNob3J0bHk= 13392
IHNpZ25pbmc= 13393
IGNvbWVkeQ== 13394
b21v 13395
IHN1Z2dlc3Rpb25z 13396
IHNpZ25hdHVyZQ== 13397
INC/0YDQuNCy 13398
IGRpc29yZGVy 13399
YXNrYQ== 13400
IHdvcmxkcw== 13401
IHByZWNpc2VseQ== 13402
bm9ybQ== 13403
cmF2 13404
IENpdmls 13405
SW50ZXI= 13406
IENlcnRhaW4= 13407
IGluanVyZWQ= 13408
IHN1Z2dlc3Rz 13409
IEdvbGRlbg== 13410
IGN5YmVy 13411
INi0 13412
IHRlbXBvcmFyeQ== 13413
IGNvb3Blcg== 13414
IHZvdGVk 13415
IG91Z2h0 13416
4bqleQ== 13417
eHVhbA== 13418
IHBhbmVscw== 13419
IDk1 13420
IGhhbmRzb21l 13421
INC/0YDQvtCy 13422
IHBlcm1pdA== 13423
IGtlaW4= 13424
IGJhZGx5 13425
IG5vdGlmaWNhdGlvbnM= 13426
aXph 13427
IE5vdGljZQ== 13428
IGluY2x1c2l2ZQ== 13429
IGFuc3dlcmluZw== 13430
IO2X 13431
dWxk 13432
7YWM 13433
IG5vd2FkYXlz 13434
IDM3 13435
IGJvbHQ= 13436
IHN0YXRpYw== 13437
IEhvcA== 13438
IGF2YW50 13439
YWpv 13440
IOunm+yeiA== 13441
IGZpZnR5 13442
IEZpbmFs 13443
IHNjb3Jlcw== 13444
IFRhcA== 13445
IGN5bA== 13446
IGNvbnZpbmNl 13447
IGFueXdheXM= 13448
b2Rh 13449
IOyVvA== 13450
IHNlcnZlcw== 13451
INGC0LDQutC+0Lk= 13452
IFpvb20= 13453
IHNhdmluZ3M= 13454
dWxv 13455
IHNvdXRoZXJu 13456
dmlld2Vy 13457
IGhvamU= 13458
IHNlamE= 13459
IHJlcHJlc2VudGluZw== 13460
iOuNmA== 13461
bGlr 13462
IFNvbWVib2R5 13463
IGJlYXN0 13464
IHN0aWNraW5n 13465
IGluc2lzdA== 13466
IHRhbGVudGVk 13467
IGV4cGxhaW5pbmc= 13468
IGF0dG9ybmV5 13469
6YOo 13470
IHN0YWlycw== 13471
IERvZw== 13472
7Ys= 13473
IGNpZw== 13474
IHNoYXBlZA== 13475
IHNvbnM= 13476
z4HOuQ== 13477
dXR0 13478
IOyU 13479
IHBhcmFk 13480
7J24642w 13481
IGhvcm4= 13482
IEpvdXI= 13483
YW5ubw== 13484
IHdvcmxkd2lkZQ== 13485
5Yqb 13486
IHBhcnRpY2lwYXRpb24= 13487
poQ= 13488
IG3Ds3c= 13489
IGJ1cm5lZA== 13490
IHdyaXRlcnM= 13491
YWxsYWg= 13492
IEZ1bmQ= 13493
IGNsZXZlcg== 13494
IExldXRl 13495
Ymlu 13496
IGJlYXRpbmc= 13497
Zm9vdA== 13498
IOybkA== 13499
IFN0dWRpbw== 13500
IHZhZw== 13501
YmV5 13502
cnpl 13503
IG9wcG9zaXRpb24= 13504
INC20LjQtw== 13505
d2hv 13506
IOqxtA== 13507
IHRyYWNl 13508
INC00LXQvdGM 13509
IGVwaWQ= 13510
IGdlc2No 13511
IE5hcg== 13512
IEJF 13513
0YPQuQ== 13514
IFNpZ24= 13515
ZWRseQ== 13516
IGNsYXk= 13517
IGluc3RhbnRseQ== 13518
IGdhdGhlcmluZw== 13519
IEdhbGF4eQ== 13520
IGJvcmVk 13521
IEJ1ZGRo 13522
Y8Op 13523
IG1hbQ== 13524
IHNsb3Bl 13525
IOuLpOydjA== 13526
IHNjaMO2bg== 13527
IHBpcg== 13528
Z2Vm 13529
YW1lcg== 13530
IGjDtg== 13531
IGNvbGxlYWd1ZQ== 13532
IHByZXNlbnRz 13533
YWRpdW0= 13534
IOCutQ== 13535
IGZhbGFy 13536
YmVlcA== 13537
IGRyaWVk 13538
aXNtcw== 13539
IHJvcGU= 13540
IHdvcmtzaG9w 13541
IGVzdHVk 13542
IGJhbmRz 13543
IHRoZW1lcw== 13544
5YWs 13545
2YrYsQ== 13546
5ZCO 13547
IHJlbWluZGVy 13548
0YLRgw== 13549
IEJo 13550
IGNvY29udXQ= 13551
INGB0YLQvg== 13552
IENoYW5uZWw= 13553
IGltbWlncmF0aW9u 13554
w6Rz 13555
Li4uLi4= 13556
5Li7 13557
55m9 13558
c3RvcA== 13559
INC60LDRgA== 13560
IGNvaW5z 13561
INGH0LDRgQ== 13562
IGRlc3RydWN0aW9u 13563
bGluZWQ= 13564
IGJhcnJpZXJz 13565
YW50aW5l 13566
IHByaW50ZWQ= 13567
IGNvbmdyYXR1bGF0aW9ucw== 13568
IEhlYXJ0 13569
IGlucXU= 13570
dGhh 13571
IGhhcmRseQ== 13572
IEF2ZW4= 13573
IHRpbmhh 13574
IFNvbnk= 13575
IE5G 13576
IGdyYWR1YXRlcw== 13577
IHNxdWVlemU= 13578
ZXJlbXk= 13579
z4TOuQ== 13580
IGVwaWM= 13581
IEp1 13582
IG9sbQ== 13583
IExhdWdodGVy 13584
IGJlbGllZnM= 13585
IENydQ== 13586
IFRydWU= 13587
IFNvdWw= 13588
b3dlZW4= 13589
IHJvbWFudGlj 13590
INC30LI= 13591
IGFub3M= 13592
IFl1cA== 13593
6Zi/ 13594
ZGlt 13595
IGluZmVy 13596
INC30LDQvA== 13597
IHNvYw== 13598
dWth 13599
IHByZWNpc2U= 13600
IGRyb3BwaW5n 13601
IGNsdWU= 13602
IGVycm9ycw== 13603
Y2hhcmdl 13604
IFB1 13605
b21ldGVy 13606
IGxhbWJkYQ== 13607
YWNpb25hbA== 13608
IERvbmc= 13609
IGNoYW1iZXI= 13610
IHRoYW5rZnVs 13611
IE51 13612
IEhhd2Fp 13613
IGluZm8= 13614
IGFjdGl2YXRl 13615
IFF1YWw= 13616
IHF1ZWQ= 13617
0YPQu9GM 13618
IGNsb3Ro 13619
5Zac 13620
IHdpY2h0aWc= 13621
NTU= 13622
IG90cmE= 13623
b2dyYXBoZXI= 13624
IGN1cmlvcw== 13625
IDE5ODA= 13626
IGVtcHJlcw== 13627
ZGVzcw== 13628
ZXVy 13629
IGNsdXN0ZXI= 13630
YXJ0ZXI= 13631
b2JpbGU= 13632
IFlhbg== 13633
IEFkdg== 13634
IGRpc2NpcGxpbmU= 13635
IOygleuPhA== 13636
IFBsYWNl 13637
IFNlbGVjdA== 13638
VEU= 13639
INCx0YvQu9Cw 13640
IHdoaXM= 13641
IGJheQ== 13642
IERvcg== 13643
ZW5jaW5n 13644
IHJlcGV0 13645
IGZpY2Fy 13646
cGFk 13647
IGZvZw== 13648
dXlvcg== 13649
IHNuYXA= 13650
aWJ0 13651
IHNvYmll 13652
IGFwcG9pbnRtZW50 13653
IFJ5 13654
IGNlaWxpbmc= 13655
b3Vyc2U= 13656
IHdyaXRlcw== 13657
IEFmZ2hhbmlzdGFu 13658
IG1vcw== 13659
YXpl 13660
IHBlbmFs 13661
IGNyeXN0YWw= 13662
SUNF 13663
6rCQ 13664
6Z8= 13665
IFRlc2xh 13666
IHRoZW9yaWVz 13667
IGFwcGVhbA== 13668
IG5ld3NwYXBlcg== 13669
IGNvb2tpZXM= 13670
5qk= 13671
INin2YTZhA== 13672
IG1hag== 13673
IEdldHRpbmc= 13674
a29tbWVu 13675
IEhlYXZlbg== 13676
ZWxscw== 13677
IGRpdmluZQ== 13678
xKs= 13679
IGFrdA== 13680
IGhvcGVz 13681
IENoZW4= 13682
d2VnZW4= 13683
Kioq 13684
IEZyYWdl 13685
INC90Lg= 13686
4Li5 13687
bWluaXN0ZXI= 13688
bmVzb3Rh 13689
d2hpY2g= 13690
IGV4cGxpY2l0 13691
IHZlcmRhZA== 13692
IGdyYWR1YXRlZA== 13693
IFBoaWxpcHA= 13694
UUw= 13695
IE1J 13696
IGRldm90 13697
IGN1cmU= 13698
IGNsb3Nlc3Q= 13699
IMOE 13700
IHNleHk= 13701
44Gb 13702
IERlYXRo 13703
b2tv 13704
dWd1 13705
IEFubmU= 13706
aXRhcmlhbg== 13707
ZXNh 13708
0LXQs9C+0LQ= 13709
IER1cg== 13710
IDAwMA== 13711
emVpdA== 13712
IHRvdXJuYW1lbnQ= 13713
IG1lbGhvcg== 13714
4Liq 13715
IGluZHU= 13716
IGZsYXc= 13717
IHdhcnM= 13718
IE1pbmQ= 13719
IElyb24= 13720
0YLQsNC6 13721
IFZS 13722
IHNpeg== 13723
IFNvdXRoZXJu 13724
IOq3uOufrOs= 13725
IGF3YWs= 13726
IOyVng== 13727
IGN1YmU= 13728
YmVsaWV2YWJsZQ== 13729
aWZhbGw= 13730
ZGlz 13731
IGFiYW5kb25lZA== 13732
bWluZA== 13733
IHBhcmw= 13734
IGNsYXNzaWNhbA== 13735
6Is= 13736
4buZdA== 13737
IEF1dG8= 13738
IEJvcg== 13739
56k= 13740
NDAw 13741
IFNvY2lldHk= 13742
IHN1YnRsZQ== 13743
IG1pc3Npb25z 13744
IHJlbWVtYmVyZWQ= 13745
IEVpdGhlcg== 13746
IGRhZsO8cg== 13747
T1JE 13748
IGludGVuc2l0eQ== 13749
RVNJTg== 13750
IEN1cA== 13751
IHJhcmVseQ== 13752
IHRveXM= 13753
IENoYXJsaWU= 13754
4buf 13755
IGdsYXViZQ== 13756
IHJvdW5kcw== 13757
VElO 13758
IGNhcGFiaWxpdHk= 13759
IGRlcml2YXRpdmU= 13760
IHJlZmVycmluZw== 13761
IGTDpQ== 13762
IFRBTEk= 13763
IGNvdHRvbg== 13764
IGNvbmZlcg== 13765
IGNvbHVtbnM= 13766
IGxpYmVyYWw= 13767
IG51bmNh 13768
IM68zrU= 13769
IGluZG8= 13770
aWJlbg== 13771
IEJlaXNwaWVs 13772
IOq3uOughw== 13773
INGD0Yc= 13774
IGhveQ== 13775
IGZyeQ== 13776
IFNjb3R0aXNo 13777
6Io= 13778
IGNpdg== 13779
IGNvbnNlcnZhdGl2ZQ== 13780
IGFpcnBs 13781
IHNhcg== 13782
cnVz 13783
IGludmVzdG1lbnRz 13784
IGluZmluaXRl 13785
IOCulQ== 13786
IFRBTElFU0lO 13787
IEdhcnk= 13788
dWVsbA== 13789
INCw0Lo= 13790
IENpcg== 13791
IHJpdHVhbA== 13792
ID4+Pg== 13793
IHRlbXB0 13794
IFRlY2g= 13795
IFBva2Vtb24= 13796
IGltcHJvdmVtZW50cw== 13797
IHNwYXJl 13798
IHRyYW5zbGF0ZQ== 13799
IHNvbnJh 13800
IEZpbG0= 13801
d29ydA== 13802
INC80Lg= 13803
IHBlcmlvZHM= 13804
IGplYWxvdXM= 13805
44GE44GE 13806
IHRpcg== 13807
TUk= 13808
IGNvbmR1Y3RlZA== 13809
IOyViOuFlQ== 13810
MDk= 13811
IFBvbGl0 13812
IFdoZXJlYXM= 13813
IG1vaXN0dXJl 13814
IHNpbnM= 13815
IGthcA== 13816
INGN0Lo= 13817
IGJlbmlt 13818
IGVsaW1pbmF0ZQ== 13819
IGF0aGxldGVz 13820
IE1hbmFnZXI= 13821
IGZlYXR1cmVk 13822
YXBvcmU= 13823
5Lqb 13824
IOuwnA== 13825
IHBlcmY= 13826
IFRodXM= 13827
IGRlYnV0 13828
0L7QsdGA 13829
IHNlw7E= 13830
IG15c3RlcmlvdXM= 13831
d29yZHM= 13832
lOqwgA== 13833
IGNoZWNrcw== 13834
IHZvbHVudGVlcg== 13835
IHdhc2hpbmc= 13836
IE1hcnZlbA== 13837
IEFC 13838
aXNzb3Jz 13839
ISc= 13840
IEZ1bGw= 13841
eWVvbg== 13842
IHdlaWdo 13843
IEpPSE4= 13844
IHZvcw== 13845
IHByb2NlZHVyZXM= 13846
IGFkZHJlc3NlZA== 13847
IEJlcmxpbg== 13848
cHV0ZXI= 13849
IEJhbg== 13850
IG1lZGljYXRpb24= 13851
IGRyb25l 13852
INGD0LE= 13853
IEplYW4= 13854
IGNhcHM= 13855
IGRpc2FwcG9pbnRlZA== 13856
IHdvcmU= 13857
IOq1rQ== 13858
IG9yZ2FuaXpl 13859
IEhhbGxvd2Vlbg== 13860
IGZhbnRhc3k= 13861
eWFyZA== 13862
IG5vc290cm9z 13863
IGp1bXBlZA== 13864
IHBob3RvZ3JhcGh5 13865
IE5hbWU= 13866
cmVj 13867
QUI= 13868
IGJsZXNzaW5n 13869
IFNodXQ= 13870
IGJpdHRlcg== 13871
cG9w 13872
44Gd44KM 13873
IGRlaQ== 13874
IGZ1bGZpbGw= 13875
55CG 13876
IGRlbmdhbg== 13877
IGJlbG8= 13878
IE1lYW53aGlsZQ== 13879
IGRlcG9pcw== 13880
IGRpYWJldGVz 13881
IGJ1bmQ= 13882
IFplYWxhbmQ= 13883
IGRpZ2VzdA== 13884
IHRpcmVz 13885
IGRvZA== 13886
YWduZQ== 13887
4bq/dA== 13888
IHBlZWw= 13889
INC30LDQsQ== 13890
IG5vZGVz 13891
IHRyZW5kcw== 13892
IFN3aXRjaA== 13893
IEF3YXJk 13894
IE9yaWc= 13895
IEhhbA== 13896
IGVzdGFz 13897
IDM2MA== 13898
IHNpbXVsdA== 13899
IGNvbWlj 13900
IG3DoA== 13901
IGJhbGFuY2Vk 13902
IFByaW5jZXNz 13903
IGtpbG9tZXRlcnM= 13904
4bup 13905
IHBhcnRpcg== 13906
7KSR 13907
c29mdA== 13908
IFZpZXc= 13909
IGJpb2xvZ2ljYWw= 13910
aW5zdA== 13911
NDQ= 13912
IG1hbmVyYQ== 13913
IGNvbXByZWhlbnNpdmU= 13914
IFNhYg== 13915
IGNyaW1lcw== 13916
eWVycw== 13917
IENvbXBhbnk= 13918
IFBob3Q= 13919
IHBvdWNv 13920
aWFj 13921
IGJlaW0= 13922
aW5hdGU= 13923
IHN1YnNlcXU= 13924
IE1heW9y 13925
IGNlbnR1cmllcw== 13926
w6hyZXM= 13927
7J6W7JWE7JqU 13928
IOq3uOufvA== 13929
IEZyYXU= 13930
IE9I 13931
IOuBnQ== 13932
IE5haA== 13933
IFNlcmllcw== 13934
IG92ZXJuaWdodA== 13935
7ZKI 13936
IOKAog== 13937
IHRyYXZl 13938
YXR0ZXJlZA== 13939
IHdhcnJp 13940
IEdydW5k 13941
IEluZG9uZXM= 13942
IHNjcmE= 13943
b2J5 13944
IEJyb29r 13945
IGN1cnM= 13946
IOu4 13947
IGV4cGxhaW5z 13948
cmFtYXRpYw== 13949
IHBhcnRpY2lwYXRpbmc= 13950
IG1pbnV0 13951
IGNvbnRyYWN0cw== 13952
IGdlZ2Vu 13953
IGRpc2FwcGVhcmVk 13954
IFNO 13955
IHJvYnVzdA== 13956
YXBo 13957
IHNocmlt 13958
IGRldmFzdA== 13959
Y29wZQ== 13960
IG1lZXRz 13961
IHBlYWNlZnVs 13962
bWF0ZQ== 13963
IHdlbGQ= 13964
INeq 13965
ZG9u 13966
0YPRgtGM 13967
IHJlZ2lzdGVyZWQ= 13968
IE5paw== 13969
amlu 13970
IGNhdg== 13971
IGVjaHQ= 13972
aW94 13973
IGZsb3dpbmc= 13974
0L3QvtGB0YLQuA== 13975
IHRvZQ== 13976
IGVudGl0eQ== 13977
0L7QstCw 13978
Zml0cw== 13979
IFBhdHJpY2s= 13980
0YLRgA== 13981
IGxldmVyYWdl 13982
IGNvcnJlbA== 13983
aWFo 13984
IHN0cmluZ3M= 13985
aXN0aW5jdA== 13986
IGd1ZQ== 13987
YXJjaHk= 13988
IHRlbmdv 13989
xLFtxLF6 13990
IG9yYml0 13991
5Li6 13992
INC10YnRkQ== 13993
Y2FrZQ== 13994
INec15Q= 13995
IE1pbm5lc290YQ== 13996
IGJyYWtl 13997
b3dpZQ== 13998
IGNyYXc= 13999
6riw66W8 14000
IHByb2dyYW1tZQ== 14001
INGB0LvRg9GH 14002
5Y+q 14003
aWVuY2Vz 14004
IE91aQ== 14005
IFBlcnM= 14006
aW1pZW50bw== 14007
IEludmVzdA== 14008
IHNsb3dlcg== 14009
5pmC5YCZ 14010
IEJldGg= 14011
IG51cnNl 14012
IFNwcmluZw== 14013
U3A= 14014
IHVuZW1wbG95 14015
0LTQuA== 14016
IGdlbml1cw== 14017
IEFhcm9u 14018
IOq3uOufrA== 14019
IGVp 14020
44GX44KH 14021
IHRhbmtz 14022
IGF1am91cmQ= 14023
IGNvbXBsZXhpdHk= 14024
INGA0LXRiA== 14025
IG9sZGVzdA== 14026
IGxldHo= 14027
5YWl 14028
IHBoZW5vbWVub24= 14029
cHJpbnQ= 14030
IEJ1bmRlcw== 14031
aXRhdA== 14032
6ruY 14033
IDQy 14034
IFdp 14035
IGluY29t 14036
IGdlaw== 14037
IGVtYnJhY2U= 14038
IHRpZXM= 14039
b3V0ZQ== 14040
IGRvc2U= 14041
IEZyaWVuZHM= 14042
0YvRgg== 14043
0LXQs9C+0LTQvdGP 14044
IG9yZw== 14045
hOuhnA== 14046
w7Nn 14047
IGV4Y2VlZA== 14048
IGdvZHM= 14049
IOqxsOyYiOyalA== 14050
IHNvY2lldA== 14051
IFVuaXZlcnM= 14052
aXTDpHQ= 14053
IHdvcmRlbg== 14054
IHNtb2tpbmc= 14055
IGludGVucw== 14056
YWJ1bA== 14057
ZW1pYQ== 14058
6JE= 14059
NDc= 14060
Zmx5 14061
IDIwMDY= 14062
IFNlcmlvdXNseQ== 14063
IHByemV6 14064
5rw= 14065
Y3Jl 14066
IG5hbg== 14067
IG1vZGVz 14068
0L7QstCw0YLRjA== 14069
IEhhbmc= 14070
ZW1lbg== 14071
IGJlbmVmaWNpYWw= 14072
IHZvdGVycw== 14073
IEJyb2Fk 14074
IGJlbnQ= 14075
V293 14076
IG11bA== 14077
5ZOl 14078
IFVD 14079
IGRhbWFnZWQ= 14080
IFVrcmFpbmU= 14081
IHdpcGU= 14082
IHN0b25lcw== 14083
IG1hbmFnZXJz 14084
IHJhYg== 14085
0YHRgtGA0L4= 14086
bGF0 14087
IGRlY2U= 14088
IGdyYXBoaWM= 14089
IGZvc3M= 14090
IGRpc2FncmVl 14091
IEFtZW4= 14092
IHNlY3JldHM= 14093
aG9sZQ== 14094
aW5rbGU= 14095
IGZvcnR1bmF0ZQ== 14096
IOyx 14097
7JyE 14098
6JCs 14099
IGhhYml0cw== 14100
IGJ1cmllZA== 14101
IGhpbg== 14102
IHZpcnR1YWxseQ== 14103
b2xhcw== 14104
IFJQ 14105
IFRhYg== 14106
bG93 14107
IHNhY3JpZmlj 14108
IGVzdGltYXRlZA== 14109
b2xu 14110
2Ys= 14111
Y3Vy 14112
IEZlZWw= 14113
IGNhc3RsZQ== 14114
IHVzZWxlc3M= 14115
IGRpc2c= 14116
IEphY29i 14117
IGdhYW4= 14118
IHVwc2lkZQ== 14119
IHBhcmVjZQ== 14120
44Oz44M= 14121
IHNoaXBwaW5n 14122
IENS 14123
IGRpc3J1cHQ= 14124
YWN0ZXI= 14125
VU5E 14126
ZnU= 14127
5a6M 14128
IFBpY2s= 14129
IENoYXJs 14130
IEJ1bGw= 14131
IGVudGVycHJpc2U= 14132
IHB1bmlzaG1lbnQ= 14133
YWNraW5n 14134
IGZyYWN0aW9u 14135
IHRhYmxldA== 14136
IGNob3Jk 14137
IHNpbWlsYXJseQ== 14138
5YW25a+m 14139
IFRvcm9udG8= 14140
IGNvdXJ0cw== 14141
xJ9s 14142
ZXN6Y3pl 14143
IHByb25vdW4= 14144
IFNpc3Rlcg== 14145
IE1Q 14146
IGdyZWF0bHk= 14147
IERhbms= 14148
aWNvcA== 14149
IGdhcmJhZ2U= 14150
IHJlc29sdmU= 14151
IFNhZg== 14152
IEd1bg== 14153
IGNvbXBvdW5k 14154
IOuwsA== 14155
IE11c2lr 14156
4pmr 14157
IGNoYW9z 14158
IFdoZW5ldmVy 14159
IGV1cm9z 14160
IG9yY2hlc3Q= 14161
IHJlZnJpZ2Vy 14162
YWxhbg== 14163
4Li3 14164
IEFtYXppbmc= 14165
IHB1ZA== 14166
YWdhbg== 14167
IGplc3pjemU= 14168
aXN5 14169
IGFjY3VyYWN5 14170
IEFtYQ== 14171
aXNvZGU= 14172
64yA 14173
IGludGVycHJldGF0aW9u 14174
IExpYmVy 14175
5rc= 14176
Y2Ft 14177
IGV2b2x2ZWQ= 14178
IEtheQ== 14179
0YbRiw== 14180
IGNyZWF0b3I= 14181
aXRhcw== 14182
IGFsYXJt 14183
IGNlbGVicmF0aW9u 14184
emVudA== 14185
IGZ1bmNpb24= 14186
IG92 14187
dW1ibGluZw== 14188
ICU= 14189
4LiI 14190
IHJlc3RyaWN0aW9ucw== 14191
INC90LDQsg== 14192
IEtpbmRlcg== 14193
IGJhbmFuYQ== 14194
0YzRjw== 14195
IGRpYW1ldGVy 14196
IG5vcnRoZXJu 14197
dXJlcnM= 14198
IFBhcw== 14199
5oiR55qE 14200
IHdvcmtmb3JjZQ== 14201
IGp1bmc= 14202
IGd1YXJhbnRl 14203
IGVxdWlsaWI= 14204
IHN1aXRl 14205
IGV1cm8= 14206
IGRlbGliZXI= 14207
U3Rl 14208
IGRvd250b3du 14209
IGNoaW4= 14210
IGNvZGVz 14211
ZWRpYQ== 14212
IHNoZWVw 14213
cmVzaG9sZA== 14214
d25pZQ== 14215
w7Ni 14216
IHVuZGVybHlpbmc= 14217
bGlh 14218
amVy 14219
z4DPjA== 14220
550= 14221
dGhyb3A= 14222
IHphcA== 14223
IHZhY3V1bQ== 14224
IEhhYg== 14225
IHdyYXBwZWQ= 14226
7KI= 14227
IGludmVudG9yeQ== 14228
0LzQsA== 14229
IGNvb3Jk 14230
IHBsYXRlcw== 14231
IHN5bW0= 14232
VGU= 14233
IHfFgmHFm25pZQ== 14234
IHJlYWNoZXM= 14235
IGxvbmVseQ== 14236
U2NyaXB0 14237
bGVl 14238
ZXNzZXI= 14239
IOqxuA== 14240
IEdlc2No 14241
IE1vdmluZw== 14242
IHLDqXA= 14243
IFZpbGw= 14244
5ZCI 14245
IFJhY2hlbA== 14246
IHRlbW9z 14247
T05F 14248
IHN0cmFpbg== 14249
IGFuZ2Vs 14250
IGbDpQ== 14251
VHI= 14252
IGFjaG8= 14253
IGhpZ2hsaWdodHM= 14254
IFdlcg== 14255
IENhcmw= 14256
IGJsdXI= 14257
IHJlZ2FyZHM= 14258
wrc= 14259
0LjQu9GB0Y8= 14260
IHJlY3Jl 14261
IFlhbmk= 14262
VUNL 14263
oLg= 14264
IGVsZWN0cm9ucw== 14265
IFNwaWVs 14266
IHZlZA== 14267
2r4= 14268
IGJlYW0= 14269
IGlkaW90 14270
65Ok 14271
0L3QsNGH 14272
aWRk 14273
IHNraQ== 14274
aXRhdGl2ZQ== 14275
IGh5cG90aGVz 14276
44Gn44GZ44Gt 14277
ZW50ZXI= 14278
IOyVhOuLiOs= 14279
IGlocmU= 14280
IHByZXZpZXc= 14281
YW5nZWw= 14282
IGRlbW9u 14283
IGR1cw== 14284
IGRpYw== 14285
IEtvbQ== 14286
TEVZ 14287
Li4uIQ== 14288
IHNpZWh0 14289
IFNvbmlj 14290
IHRlbmhv 14291
YW5hcw== 14292
IGRpZ2l0 14293
IE1hYXI= 14294
IHVuZGVyZ3JhZA== 14295
b3VuY2Vy 14296
dWZmeQ== 14297
IGNvbnZlcnNpb24= 14298
IGRpc2Nvbm5lY3Q= 14299
IGVjaG8= 14300
b21lcg== 14301
IGN1cnJpY3VsdW0= 14302
IHBlcmNow6k= 14303
IHdhbmQ= 14304
Li4/ 14305
IHJvbGxlZA== 14306
IGVudHJlcHJlbmV1cg== 14307
IHRoZW9yZXQ= 14308
INGJ0L4= 14309
IGluc2lnaHRz 14310
IHp1c2FtbWVu 14311
b2lu 14312
cmV0dA== 14313
cHJvZHU= 14314
IHZpc2l0b3Jz 14315
ZW91cw== 14316
IGdyYW5kbW90aGVy 14317
IGh1bW9y 14318
INC90LjRhQ== 14319
emVuaWE= 14320
aW5zb24= 14321
IHJlc2V0 14322
IGJhc2ViYWxs 14323
IG1hdGNoaW5n 14324
64uk6rCA 14325
IHB1bnRv 14326
7KE= 14327
IHJlZGU= 14328
IGFkZHJlc3Npbmc= 14329
IGZvcmVjYXN0 14330
IEJvbA== 14331
IGNvbG9yZWQ= 14332
IGRvY3VtZW50YXRpb24= 14333
IGV4cGVjdGF0aW9u 14334
IE5vcnRoZXJu 14335
IGNyZW8= 14336
IOCumg== 14337
Zm9u 14338
IHVuc2VyZQ== 14339
VU0= 14340
IGNvcGllcw== 14341
IGV4cGFuZGVk 14342
IHZldGVyYW5z 14343
IEFsbQ== 14344
INCy0L7QvtCx0YnQtQ== 14345
IHBzeWNob2xvZ2ljYWw= 14346
IG5vc3Nv 14347
IHBheW1lbnRz 14348
aW1ldGVycw== 14349
IC0tPg== 14350
IEplbm5pZmVy 14351
IHZvbHVudGVlcnM= 14352
b3NzZQ== 14353
b3Jpb3Vz 14354
INCx0YvQu9C4 14355
6II= 14356
IEVzcw== 14357
d3M= 14358
IEJD 14359
IElD 14360
V29tYW4= 14361
IHZvbnQ= 14362
IGV0aG5pYw== 14363
RU5O 14364
0LjQvNC+ 14365
IGxvYg== 14366
IG91aQ== 14367
Y3M= 14368
IHJlaGU= 14369
IOyggQ== 14370
IGNoaWNr 14371
w7pzaWNh 14372
IGtvbnQ= 14373
IERpc3RyaWN0 14374
IHBpbGU= 14375
INCw0LI= 14376
0LXQudGB0YLQsg== 14377
IMKj 14378
IGlzc3VlZA== 14379
INC60L7QvNC/ 14380
IHByb3NwZXI= 14381
IHByb2ZvdW5k 14382
IERlYXI= 14383
IOOBkw== 14384
IGZ1bmRlZA== 14385
IGJpc2E= 14386
npjr 14387
158= 14388
IOydmA== 14389
IHR3ZWx2ZQ== 14390
IENoYW1waW9ucw== 14391
6Z2e5bi4 14392
0YHQuw== 14393
IDIwMDU= 14394
cG0= 14395
IG9uZGU= 14396
IGRpZmbDqQ== 14397
IENoYWxs 14398
IGRpZmZpY3VsdGllcw== 14399
IGdhcmFnZQ== 14400
IGTDoQ== 14401
w7xuaw== 14402
IOusvA== 14403
IHRyYW4= 14404
IHN1Ym1pdHRlZA== 14405
enc= 14406
2YjYpw== 14407
IGFyaw== 14408
IOyEsQ== 14409
IGdyb2Nlcnk= 14410
0L7QvdCw 14411
aWVyZQ== 14412
IGFlc3Q= 14413
IGV4aGliaXRpb24= 14414
IHLDqXM= 14415
IGNvbnNpc3RlbmN5 14416
IGNvb2tpZQ== 14417
0L3QtdC5 14418
IHJlcGxhY2VtZW50 14419
5rK5 14420
IFNlbQ== 14421
IOyCrOyaqQ== 14422
ODAw 14423
IGdlbmVz 14424
IHRyYW5zYWN0aW9u 14425
IEVM 14426
IGR1cmFudGU= 14427
aWJsZXM= 14428
IEVhdA== 14429
dGFpbA== 14430
aXNzYW5jZQ== 14431
IHRvc3M= 14432
IHN1cnZpdmVk 14433
IG9mZmljZXM= 14434
IHN1cHBvcnRpdmU= 14435
V2hlcmU= 14436
IHRvdXRlcw== 14437
IOuniQ== 14438
IGpva2Vz 14439
aWVyb24= 14440
YXBlcnM= 14441
IG1hdHVyZQ== 14442
IE1hcnNo 14443
IHNpZG8= 14444
a2luZA== 14445
IHJlYWxtZW50ZQ== 14446
IENoZWY= 14447
IHF1ZWxxdWU= 14448
IGp1ZGdlcw== 14449
ZWZ0 14450
RVJT 14451
IGpldA== 14452
IHBlcnNvbnM= 14453
6Ls= 14454
aXphdGlvbnM= 14455
cmlr 14456
IHNob3Bz 14457
IFd5 14458
IGVsZWc= 14459
cXXDqA== 14460
cXVvaQ== 14461
IGp1Z2E= 14462
IO2VnOuyiA== 14463
IFF1ZXN0aW9u 14464
IEdsb2JhbA== 14465
IOyVveqwhA== 14466
IFN0YXRpb24= 14467
5o6l 14468
IE9oaW8= 14469
IHN0aWNreQ== 14470
IHN0cmVzc2Vk 14471
IGfDvG4= 14472
IO2d 14473
0YHRgtGD0L8= 14474
6aGM 14475
IFBoRA== 14476
aW1tZXI= 14477
IG1lbnRvcg== 14478
IGludmVudGVk 14479
IHJldW4= 14480
IGluZXZpdA== 14481
IHBvbMOtdA== 14482
IGV4ZWN1dGU= 14483
IFN0b3J5 14484
IG91dHN0YW5kaW5n 14485
IGd1ZXI= 14486
IFJhaW4= 14487
IGNob3Nlcw== 14488
IFRpdA== 14489
INGB0LXRgA== 14490
IFNpbmdhcG9yZQ== 14491
IE5vbmU= 14492
IGNocm9uaWM= 14493
sOuNsA== 14494
IGVnbw== 14495
5qC3 14496
RVNU 14497
44GC44KK 14498
IFdhbmc= 14499
IE5BVA== 14500
IGF1Zw== 14501
IGRlc2t0b3A= 14502
IGV0ZXJuYWw= 14503
IOyCrOyLpA== 14504
IENvbnN0aXR1dGlvbg== 14505
7IKs6w== 14506
15nXnA== 14507
cHJlcw== 14508
INCi0Ys= 14509
IGludGVyZg== 14510
IGxpc3Rz 14511
IGZpZ2h0cw== 14512
ZnRlbg== 14513
IElvd2E= 14514
IG1vdGl2YXRlZA== 14515
IEhvc3A= 14516
IGVsc2V3aGVyZQ== 14517
IHBhdGhz 14518
IGluc3RhbmNlcw== 14519
Qmw= 14520
cmFuZ2U= 14521
4bux 14522
IFNpdA== 14523
bWFuYQ== 14524
IOyLnOyekQ== 14525
IG3DrG5o 14526
YW5zYXM= 14527
IHNuYQ== 14528
IHBoaWxvc29waA== 14529
IHBhc3Nl 14530
xrDhu51p 14531
YWto 14532
ZW50YWw= 14533
IGlobg== 14534
cnVjdG9y 14535
INCy0LDRiA== 14536
IGdlbmVyb3Vz 14537
IHBpdm90 14538
0L/QvtC7 14539
IGphbWFpcw== 14540
IGNvbWVudA== 14541
IExldw== 14542
b2R6aQ== 14543
IFhib3g= 14544
INCy0L7QtA== 14545
IGNvbnNlbnQ= 14546
ieyepQ== 14547
IGRpc3Bhcg== 14548
bGFzcw== 14549
IEdvdmVybm9y 14550
QmVpZmFsbA== 14551
IOqwnA== 14552
IGJlbG92ZWQ= 14553
16DXlQ== 14554
c2VsbA== 14555
IGhvbm9yZWQ= 14556
bGVo 14557
IHfDpHJl 14558
dW50aW5n 14559
IGZyYXVk 14560
IFJBTQ== 14561
6rG4 14562
IGtpbGxz 14563
IGVjb25vbWljcw== 14564
MDQ= 14565
0L/QtdGA 14566
IGNvaXNhcw== 14567
INC40LPRgA== 14568
w61t 14569
IG3DtmNodGU= 14570
IOy1nA== 14571
IHN0aW11bA== 14572
IGZhc3Rlc3Q= 14573
bHY= 14574
IGfDqW4= 14575
IFNvdW5kcw== 14576
IDE5NzA= 14577
IGhvbWV3b3Jr 14578
c3BlYWtpbmc= 14579
IGVuY291cmFnaW5n 14580
IHF1ZXJ5 14581
IHJldmVycw== 14582
cHJvZml0 14583
IGR5 14584
IOyekQ== 14585
64qU642w7JqU 14586
IHNvYXA= 14587
IEdhbGw= 14588
IENO 14589
IEFucw== 14590
IGZpYw== 14591
YW5rcw== 14592
IGRlc3NlcnQ= 14593
IOyggO2drA== 14594
IE1ha2luZw== 14595
IGNvbWXDpw== 14596
6rOE 14597
IGFzc29jaWF0aW9u 14598
RGFk 14599
aGVl 14600
IGhvZ3k= 14601
IGFwcm8= 14602
IGludmlzaWJsZQ== 14603
QW1lcmljYW4= 14604
7Y4= 14605
IHZpYmU= 14606
IGVtaXNzaW9ucw== 14607
IGFkdm9jYXRl 14608
IGtpY2tlZA== 14609
IHZlbA== 14610
IHN1bW1hcg== 14611
IGZyZWFraW5n 14612
Y2hyb24= 14613
IHBpbmNo 14614
IHdzenlzdGs= 14615
aXNjYWw= 14616
IHByb3ZlZA== 14617
IG1pbmRmdWw= 14618
IHTDpA== 14619
IG5vaXNlcw== 14620
IGlzb2xhdGVk 14621
IGNyb3NzZWQ= 14622
IOqwlQ== 14623
IHZvaWzDoA== 14624
IGNob3Jl 14625
IFJB 14626
Q29t 14627
IHJlbGF4ZWQ= 14628
YXRybw== 14629
IHByZXZlbnRpb24= 14630
Vm9pY2VvdmVy 14631
T0Q= 14632
IENvdmlk 14633
IHNlcGFyYXRpb24= 14634
IC1b 14635
0LjRh9C10LPQvg== 14636
55m8 14637
IFNE 14638
YmxlZXA= 14639
IGluZGVwZW5kZW5jZQ== 14640
IHBhcnRpYWw= 14641
IGFsZ29yaXRobXM= 14642
IEFueW9uZQ== 14643
IGFzc29jaWF0ZQ== 14644
aHVt 14645
aWN1bGFy 14646
IGLhuqFu 14647
IGJhdHRsZXM= 14648
R29vZA== 14649
QXBwbGF1c2U= 14650
IGJhc3RhbnRl 14651
IGFkdmFudA== 14652
IFN3ZWV0 14653
IHJlZnVzZWQ= 14654
44K4 14655
INGC0LXQsdC1 14656
cGxldA== 14657
IGVuY291cmFnZWQ= 14658
5ZOm 14659
IG1pcmFjbGU= 14660
IEJ1bg== 14661
IFZhcg== 14662
cmltaW5hdGlvbg== 14663
ZWxlY3Q= 14664
IE11bHQ= 14665
IGRlbGl2ZXJpbmc= 14666
ZWluZw== 14667
IGNt 14668
bmVobWVu 14669
IExpbmU= 14670
IOunjA== 14671
ZW5jZWQ= 14672
IFNvdW5k 14673
IENvbnRpbg== 14674
aWpk 14675
VU5H 14676
a2xl 14677
IHRocmVzaG9sZA== 14678
IGNvbXBhY3Q= 14679
YWR0 14680
IHRvZXM= 14681
IFB1cg== 14682
b3duZWQ= 14683
bWVudGVk 14684
IGRlc2lnbmluZw== 14685
IHZhY2NpbmF0ZWQ= 14686
IGV4aGF1c3Q= 14687
IGJhc2ljcw== 14688
IGNvbnNpc3Rz 14689
IEd1eQ== 14690
YWN6eQ== 14691
IG3DrQ== 14692
d29u 14693
5a6z 14694
IDg1 14695
5oI= 14696
IG11bQ== 14697
IGlnbm9y 14698
IHByaW50aW5n 14699
YWN1bGFy 14700
cG93 14701
IGV4cGFuZGluZw== 14702
IGdpcg== 14703
IENhYg== 14704
7Zi4 14705
0YLRjNGB0Y8= 14706
IOyXrOufrOu2hA== 14707
IGFuZ2xlcw== 14708
IHRlcm1pbmFs 14709
IFdvbg== 14710
IEludGVyZXN0aW5n 14711
IGNyb3NzaW5n 14712
IGJvbmRz 14713
IHB1ZWRlbg== 14714
IG9yYg== 14715
bGFyxLFu 14716
IGNyZWVweQ== 14717
IG51dHJpdGlvbg== 14718
IGFsbGllcw== 14719
IHdpcmVsZXNz 14720
IGRlc2lyZWQ= 14721
IGNvbXB1dGU= 14722
IEFyaXpvbmE= 14723
IEJlYXV0aWZ1bA== 14724
IHByb2R1Y2Vz 14725
IG51ZXN0cm8= 14726
dGVk 14727
IGVsaWdpYmxl 14728
INGB0L7Qtw== 14729
aWNpYWw= 14730
IEhlcm8= 14731
IGNvbnN1bWU= 14732
IHJvYm90cw== 14733
IHB1cmNoYXNlZA== 14734
Y2Npw7Nu 14735
IGl6 14736
xrDhu6Nj 14737
zq/Ovc6xzrk= 14738
INij2YY= 14739
IHNoYWRvd3M= 14740
IE1lZGlh 14741
IHByaW5jZXNz 14742
IGtsYXI= 14743
IHdvb2Rlbg== 14744
IHVzYXI= 14745
IGfDvHplbA== 14746
IHNsb3Q= 14747
cmFkZQ== 14748
IOuS 14749
IGhhcm1vbg== 14750
IGluZ3JlZGllbnQ= 14751
b3JzaGlw 14752
ZWtp 14753
IGdyYW5kZmF0aGVy 14754
IGV4Y2l0ZW1lbnQ= 14755
IHBvbGl0aWNpYW5z 14756
Li4h 14757
IG91dHM= 14758
IHNlcGFyYXRlbHk= 14759
INGP0Lo= 14760
IFdlbHQ= 14761
IFBvdw== 14762
amFu 14763
IG9yaWVudGF0aW9u 14764
5Y+L 14765
TEM= 14766
YWdlbQ== 14767
24zaug== 14768
5ZCX 14769
IGJyYW5jaGVz 14770
YWRlbg== 14771
cmVudGU= 14772
IElocg== 14773
YXNt 14774
IGVzdMOjbw== 14775
IE5pYw== 14776
IHNsYXZl 14777
IGNvbXByZXNz 14778
Y3Jvd2Q= 14779
IGNsaW1iaW5n 14780
IE1hbmFnZW1lbnQ= 14781
IEJhaA== 14782
IHBhbmlj 14783
IGtvcg== 14784
IGNvb2xpbmc= 14785
IGJpbmQ= 14786
INC30LDQtA== 14787
IHJhY2s= 14788
IGVudGl0 14789
IHNlbmRz 14790
IHlvdXJzZWx2ZXM= 14791
ZGVz 14792
IE11c2xpbXM= 14793
IO2a 14794
aXNtYQ== 14795
Y3ljbGU= 14796
dW5rdA== 14797
IENvcmU= 14798
IGluanVyaWVz 14799
IGlkZW50aWNhbA== 14800
0LrQsNGP 14801
IERldXRzY2hsYW5k 14802
INC10LU= 14803
aXNhbg== 14804
IHRydWM= 14805
bGV0b24= 14806
IGJhY2t1cA== 14807
IHVsdHJh 14808
IGFidW5k 14809
aWxsZXVycw== 14810
IGJ5xYJv 14811
5YWD 14812
b3J0ZWQ= 14813
IGVhcnRocXU= 14814
INC60Ls= 14815
IG9ic2VydmF0aW9u 14816
IG1haW50ZW5hbnQ= 14817
ZWxlbg== 14818
IHNldHRsZWQ= 14819
IHBlbGE= 14820
IEVjb25vbQ== 14821
INU= 14822
IHN0ZWVyaW5n 14823
IEFMTA== 14824
IENoZXI= 14825
IHBhdGllbmNl 14826
IFNub3c= 14827
IGJvcg== 14828
IHdvcnRoeQ== 14829
IGPDoWk= 14830
INen 14831
IM66zrE= 14832
ZG9n 14833
IEthcmVu 14834
aWxsZXM= 14835
zrI= 14836
IGFncmljdWx0dXJl 14837
15XXnw== 14838
IFNlYW4= 14839
IHNlbnNvcnM= 14840
7ZW06w== 14841
YWdo 14842
IHB1YmxpY2x5 14843
IHBldXg= 14844
IEFsZXhhbmRlcg== 14845
IHByaW9yaXQ= 14846
IGxhenk= 14847
YXJkb24= 14848
YXR0ZXJpbmc= 14849
IGNvc3R1bWU= 14850
2LPYqg== 14851
6L+Y 14852
IHVudw== 14853
0Js= 14854
IHRoaWNrbmVzcw== 14855
cXVpdG8= 14856
Z3VudA== 14857
aXN0YXM= 14858
bmV5cw== 14859
IOuQmOqyjA== 14860
IEJyYXNpbA== 14861
IHRva2Vu 14862
IGFmZmlsaQ== 14863
bG9u 14864
IGbDpXI= 14865
IEJlYWNo 14866
IHdpdGNo 14867
IFNldmVu 14868
IHBhbnQ= 14869
zrvOuw== 14870
IGNhcHRhaW4= 14871
5Z0= 14872
IHZldXQ= 14873
IHBvdXZvaXI= 14874
YWN6 14875
IEJhcmI= 14876
IHV0aWxpdHk= 14877
IGNvbnRlbXBvcmFyeQ== 14878
IG9idGFpbmVk 14879
IHBhaW50aW5ncw== 14880
ZWFy 14881
IHBlYW4= 14882
IE9n 14883
IGN1c3Q= 14884
0LvQtdC8 14885
gpjr 14886
IElzc28= 14887
IGFjb250ZQ== 14888
IFRlbGU= 14889
IEFzc2lzdGFudA== 14890
w4k= 14891
7ZaI7Iq164uI64uk 14892
IGNvdW50cw== 14893
IGJ1Y2s= 14894
IERlZXA= 14895
IHRhY2tsZQ== 14896
IGhhcnNo 14897
IGRlY2lkZXM= 14898
6Zec 14899
LuKAiw== 14900
6YKK 14901
IEFuZ2Vs 14902
IGxheWluZw== 14903
IGNhbG9yaWVz 14904
IGNvbnRyb2xsaW5n 14905
IGFkdmFudGFnZXM= 14906
INGN0YLQvtC5 14907
IGFwcHJvYWNoaW5n 14908
IHRocmVhdHM= 14909
YWthbg== 14910
ZW1hdGlj 14911
bWFubg== 14912
6rO1 14913
bXVtYmxlcw== 14914
YWNpw7M= 14915
IG1haW50YWluaW5n 14916
IGZvdW5kZXI= 14917
bGFo 14918
ZmlnaHQ= 14919
IGFkbWl0dGVk 14920
4oCmLg== 14921
lYw= 14922
YWJvbA== 14923
IHVzYWdl 14924
IG5vbnNlbnNl 14925
IFBhbGVzdA== 14926
IGNvbnRyZQ== 14927
IERlbW9jcmF0aWM= 14928
IEVS 14929
amVrdA== 14930
IGFyYml0 14931
INCz0L7Quw== 14932
IE1pY2hlbGxl 14933
aWNoZXI= 14934
ZXNo 14935
IFBobw== 14936
0LrQvtC8 14937
NDk= 14938
IEVuZXJneQ== 14939
zr/PjQ== 14940
IGNlbnRz 14941
IHJlZmVycw== 14942
IGdvc3BlbA== 14943
IFNoYQ== 14944
IFNoYXJl 14945
15nXoA== 14946
IGNsaW5pYw== 14947
IOuEow== 14948
IGVxdWFsaXR5 14949
dWdz 14950
IHNoZWQ= 14951
IHBsYW5lcw== 14952
IHRvdXRl 14953
cmVjaw== 14954
IHN0cmFuZA== 14955
IGJpb2xvZ3k= 14956
IGxlYWd1ZQ== 14957
IFBvaw== 14958
IG7Dum1lcm8= 14959
IENvYXN0 14960
IGNvbnNpc3RlbnRseQ== 14961
IG51Y2xl 14962
T09PTw== 14963
IG9iamV0 14964
IGNob3I= 14965
IGdpbmdlcg== 14966
IGRhYmVp 14967
IGNvb3BlcmF0aW9u 14968
4K+NLg== 14969
bnRlbg== 14970
56Q= 14971
bMOg 14972
7JaR 14973
cmFkbw== 14974
IHBhc3NpdmU= 14975
IGdsb3Zlcw== 14976
IHVuZGVyZ3JvdW5k 14977
IGxvZ2ljYWw= 14978
IGtldA== 14979
IGZ1bmN0aW9uYWxpdHk= 14980
uOumrA== 14981
IHBvcnRhbA== 14982
ZWxsZXI= 14983
15nXqA== 14984
IFRlZA== 14985
IEdyZQ== 14986
kJw= 14987
IHBlcnNvbm5lbA== 14988
IGVtZXJnaW5n 14989
IEbDvHI= 14990
IG1lYW50aW1l 14991
dXNhbGVt 14992
IENsZWFy 14993
IHRyYXBwZWQ= 14994
IOyasA== 14995
IGRpc3Bs 14996
IG1ldHRyZQ== 14997
IG11bmljaXA= 14998
IHdpdGhkcmF3 14999
IHNwYXQ= 15000
dW5lcw== 15001
IGFjY2Vzc2liaWxpdHk= 15002
5oiR5Lus 15003
IGFwYXJl 15004
IHByb3NwZWN0 15005
INC90LDQtw== 15006
IGNvcHBlcg== 15007
IFBSTw== 15008
z4XPhA== 15009
IGF0dGFja2luZw== 15010
IFZpbg== 15011
IFN0b25l 15012
IGludmVzdGlnYXRl 15013
c3R5bGU= 15014
IM67 15015
66Gd 15016
66eI 15017
IGluc3BlY3Q= 15018
IGxpdmVy 15019
0LDQu9C40YHRjA== 15020
IHNlcmE= 15021
aGFsdGVu 15022
ZW1hbg== 15023
IG1pbmlzdHJ5 15024
Jyc= 15025
IGRvdHM= 15026
44WL44WL44WL44WL 15027
0YPRgdGC 15028
IEphaw== 15029
QUtF 15030
IGdhcHM= 15031
dWNrZXI= 15032
INC40L3RgtC10YDQtdGB 15033
IEVtaWx5 15034
IGludGVydmFs 15035
IHRlbmRlcg== 15036
IFRlY2hub2xvZ3k= 15037
Z2FtZQ== 15038
IHRyaWI= 15039
2YTYpw== 15040
IERldmVsb3BtZW50 15041
2YXYpw== 15042
IHdyaXN0 15043
IGZpcmVz 15044
IHRhcmdldGVk 15045
7KCQ 15046
IHNvZA== 15047
7ZqM 15048
IG9sZHXEnw== 15049
IHNlYXNvbnM= 15050
dmVudGlvbnM= 15051
INC90LXQs9C+ 15052
IHNvbWV0aW1l 15053
0LvQuNCy 15054
bsOp 15055
IHTDug== 15056
IERldXM= 15057
IGV4ZWN1dGlvbg== 15058
w6Fw 15059
IENoYW5nZQ== 15060
IEluZGVlZA== 15061
IHJlZ3VsYXRpb24= 15062
IEh1bmc= 15063
w6lpcw== 15064
IHdpc2hlcw== 15065
IGpheno= 15066
IHN0cnVjdHVyYWw= 15067
IGJsb3dpbmc= 15068
IGJ5xIc= 15069
IHRoZXJtYWw= 15070
cGhhbnQ= 15071
0YDRg9C3 15072
0LDQvdGC 15073
IFB1bGw= 15074
IGNvbmZ1c2lvbg== 15075
0L3Ri9C80Lg= 15076
IHNjZW5hcmlvcw== 15077
7KCB7Jy866Gc 15078
INC00LXRgg== 15079
IHRhdHRvbw== 15080
IGF1dHJl 15081
IGhlYXRpbmc= 15082
IHRyZWF0aW5n 15083
INC/0L7QvdC40Lw= 15084
IGV4Y2x1cw== 15085
IExPTA== 15086
d2Vhcg== 15087
YWdsZQ== 15088
IHp1csO8Y2s= 15089
IHJhdGlvbmFs 15090
c3U= 15091
IGRldGVy 15092
IE5hdGl2ZQ== 15093
4K6V4K6z 15094
YWNoZWQ= 15095
IOOD 15096
IEVudG9uY2Vz 15097
IGhvcmE= 15098
7J207JeQ7JqU 15099
IGxpdGU= 15100
w6s= 15101
IHNpeHRo 15102
INCx0L7Qu9C10LU= 15103
YWN0b3I= 15104
IHBzeWNob2xvZ3k= 15105
55u4 15106
IGRlbWFuZHM= 15107
IHBlZXI= 15108
IG5ld2x5 15109
IFdXRQ== 15110
RG9uYWxk 15111
IEJveA== 15112
IHBpbmU= 15113
IGxvYWRpbmc= 15114
IE5pY28= 15115
IHPFgg== 15116
b21tZQ== 15117
QVJU 15118
IHJlY3J1aXQ= 15119
IGJ1Z3M= 15120
YXJlbnRz 15121
INC/0YDQvtCx 15122
IEluc2lkZQ== 15123
aXBwZXI= 15124
ZHJhbWF0aWM= 15125
IHBsYW5ldHM= 15126
b3JkZQ== 15127
IHlvZ2E= 15128
Y2hpbGQ= 15129
IE1hcmll 15130
IOOBgg== 15131
IEJM 15132
IGZpbG1lZA== 15133
IHJlZnJlc2g= 15134
IHRvbWF0b2Vz 15135
IGZldA== 15136
UXXDqQ== 15137
ICEh 15138
IOuCtOs= 15139
cmluZQ== 15140
IGludGVyYWN0aXZl 15141
c2Fs 15142
YW5uYWg= 15143
cGV6 15144
57aT 15145
IHVuZGVyc3RhbmRz 15146
IFRva3lv 15147
IGxpYnJhcmllcw== 15148
IHJlYWRlcg== 15149
kZA= 15150
b3o= 15151
IEVuZGU= 15152
IEZsbw== 15153
IG1pbGQ= 15154
IHBvZXRyeQ== 15155
INC20LjQsg== 15156
5oSb 15157
IGJlaGF2ZQ== 15158
IGRvZW4= 15159
IFN1c2Fu 15160
cGFnZQ== 15161
cmFoYW0= 15162
IGNvbW11bmljYXRpb25z 15163
IHR1bmluZw== 15164
IHBhYw== 15165
IGFueGlvdXM= 15166
SU8= 15167
TWFyaw== 15168
IGhpw6c= 15169
Ym9va3M= 15170
IHBpc3M= 15171
IGVuYWJsZWQ= 15172
YWNoZWxvcg== 15173
IEZPUg== 15174
IMOpYw== 15175
IFRS 15176
aWxzdA== 15177
aGF0 15178
IOydjA== 15179
IHR5Y2g= 15180
IGphcg== 15181
IGJ1aWxkcw== 15182
IEFyZ2VudA== 15183
IGludGVybWVkaQ== 15184
IGxvdQ== 15185
IGFyYQ== 15186
IGFzc2lnbm1lbnQ= 15187
IGNhYmluZXQ= 15188
IHJldGlyZW1lbnQ= 15189
44G7 15190
IGRpc2FibGVk 15191
cmljYQ== 15192
IGF3YXJkcw== 15193
IGJvb3Rz 15194
IGFja25vd2xlZA== 15195
IHRoeQ== 15196
IOq1rA== 15197
IHN5bmQ= 15198
0L3QuNC5 15199
aWx0b24= 15200
IHByb2Js 15201
IEZhbA== 15202
IHZlcmRhZGU= 15203
IDcwMA== 15204
IExlYXJuaW5n 15205
b2N1cw== 15206
IHBhbGFjZQ== 15207
Tm90 15208
dGFpbg== 15209
Y20= 15210
IG1hZ25ldA== 15211
aW5jb2xu 15212
IGZpZ3VyaW5n 15213
IEx5bg== 15214
IEJvc3M= 15215
IFZP 15216
IGRpYWdub3Npcw== 15217
IGVxdWlwcGVk 15218
d2F0Y2g= 15219
aW5vcw== 15220
YWRlcnM= 15221
IHNoZWxm 15222
IG9yZ2FuaXM= 15223
IG5vZA== 15224
IGvEsXo= 15225
cHBlcnM= 15226
IHJlc3RvcmU= 15227
IGFydGlj 15228
IFZvaWNl 15229
xLF5b3J1bQ== 15230
6rKp 15231
IHNwcmVhZGluZw== 15232
IGhpcHM= 15233
IHdhcmQ= 15234
dXJlYXU= 15235
IGludGVyc2VjdGlvbg== 15236
NjY= 15237
IDM5 15238
57M= 15239
IHdhaXRlZA== 15240
7LQ= 15241
aGhoaA== 15242
IGR5cw== 15243
IEVO 15244
IGJhdGNo 15245
IGNhZg== 15246
IG1hcmtlcg== 15247
5aSn5a625aW9 15248
b3JhYmxl 15249
w7NyaWE= 15250
IHN0ZXBwZWQ= 15251
IGNlbGVicmF0aW5n 15252
0LDQvdCw 15253
IHdvcm4= 15254
IEZvbA== 15255
IHBsYQ== 15256
IGF0dGVtcHRz 15257
IHR3ZWV0 15258
IHJ1c3Q= 15259
Z2VuY2U= 15260
7Ya1 15261
IHJldmVs 15262
IHJlY2VwdA== 15263
ZW5lc3M= 15264
ICgo 15265
44O844M= 15266
IeKAiw== 15267
IOyGkA== 15268
IGluZmx1ZW5jZWQ= 15269
0LjQtg== 15270
INC60L7QvdC10YfQvdC+ 15271
IGNvbGxlZ2Vz 15272
aW9uaQ== 15273
IHNhZw== 15274
QW5u 15275
b2xhcg== 15276
IGV4cHJlc3Npb25z 15277
IHN1aXRz 15278
IG93bmVyc2hpcA== 15279
ZWxhbmQ= 15280
cGllY2U= 15281
5oCO5LmI 15282
IGRlc3B1w6lz 15283
IHRlbA== 15284
IGluc3VsdA== 15285
IOq1ieyepQ== 15286
IFNtYWxs 15287
IEZS 15288
b2th 15289
YmVycmllcw== 15290
IEFudG9u 15291
0LXQu9GP 15292
0Y/RgQ== 15293
IHZhbHZl 15294
YWN0cw== 15295
IHdvb2Rz 15296
4K6j 15297
IGN1bHRpdg== 15298
IGbDoQ== 15299
44Go44GE44GG 15300
IGNoZWVycw== 15301
IGFzc3VtcHRpb24= 15302
IGZpdG5lc3M= 15303
w61jdWw= 15304
IHBvZHI= 15305
IHdlaXQ= 15306
IEhpbmQ= 15307
IGRpZ24= 15308
INC30L0= 15309
IHNxdWFk 15310
IGRlc3Rybw== 15311
Y2VyZQ== 15312
c2hpcnQ= 15313
aW1tdA== 15314
ZW5nZXJz 15315
IHPDpA== 15316
a8WCYWQ= 15317
IMiZ 15318
IG9jY2Fz 15319
IOykhA== 15320
IHByb2Nlc3Nvcg== 15321
IERN 15322
IERhZGR5 15323
IHNvb25lcg== 15324
IHN0cmFpZ2h0Zm9yd2FyZA== 15325
IGRlcGFydG1lbnRz 15326
IENocm9tZQ== 15327
IHdvcmtwbGFjZQ== 15328
IFB5dGhvbg== 15329
IG1lbmc= 15330
IERBTg== 15331
IEljZQ== 15332
IOuIiA== 15333
IEdp 15334
IGhpcmluZw== 15335
IGxhbmRlZA== 15336
IGRlbW9jcmF0aWM= 15337
aWVkeg== 15338
44GY44KD 15339
IHNldg== 15340
aWNpYQ== 15341
IGVzcGVjaWFs 15342
IE5vdXM= 15343
IGjDpHQ= 15344
IGJvdQ== 15345
cGVydA== 15346
aWVzeg== 15347
5ZGA 15348
IHZpbA== 15349
xZtsaQ== 15350
IMOubg== 15351
IGxvc3Nlcw== 15352
6ZW3 15353
IHRvYXN0 15354
IHJlYWxt 15355
IEF1c3Rpbg== 15356
IEluZm9ybWF0aW9u 15357
IHJlc3VtZQ== 15358
IGNoYXNl 15359
IHNhbGFyeQ== 15360
IOu2hA== 15361
0LvQuNGH 15362
INGB0LvQtdC0 15363
IEZ1cnRoZXI= 15364
IGNhcmluZw== 15365
IHZpZw== 15366
IHZhbG9y 15367
6L+Z5Liq 15368
INGH0LA= 15369
IGFuYWx5dGljcw== 15370
IGdsb2Jl 15371
IE1BTg== 15372
IG5lbA== 15373
7J207JW8 15374
n7w= 15375
IG95 15376
7ZWY7IS47JqU 15377
amVu 15378
IHRyb3VibGVz 15379
YWhhaGE= 15380
IGNodXJjaGVz 15381
dWV0 15382
IG1lYXN1cmVtZW50cw== 15383
Ymls 15384
7L0= 15385
aWZ1bGx5 15386
0LjQvdGD 15387
IFdpbHNvbg== 15388
prQ= 15389
IO2MjA== 15390
IOywqA== 15391
IHDDumJsaWM= 15392
IEplcnVzYWxlbQ== 15393
IG5haWxz 15394
IHNwaW5l 15395
IGhlbW9z 15396
IHpu 15397
cXVpcw== 15398
IExlYmVu 15399
IHJlZmVyZW5jZXM= 15400
SVRI 15401
aXBlcg== 15402
INGB0LXQsdGP 15403
7IE= 15404
IFdh 15405
c3RhdGU= 15406
p50= 15407
5YWx 15408
IEdlbmVy 15409
IGFjdHJlc3M= 15410
IEVuam95 15411
4LmD 15412
INeS 15413
IGluZmVjdGVk 15414
IHNoYWtpbmc= 15415
IG5pY2s= 15416
4Li4 15417
IGZvdA== 15418
IGFjY29tcGxpc2hlZA== 15419
dWtl 15420
IHNoZWV0cw== 15421
IGZlbmNl 15422
IG51cnNpbmc= 15423
IGludHJvZHVjaW5n 15424
IGZlYXQ= 15425
T25l 15426
VE8= 15427
IGNsdWJz 15428
IEJydWNl 15429
b25nZQ== 15430
Y2hhbmdl 15431
IEJhdG1hbg== 15432
5Y+w 15433
IE9mZmljZXI= 15434
IGh5ZHJv 15435
IHN1cHBsZW1lbnQ= 15436
IGNlbGE= 15437
IGxvbmdlc3Q= 15438
IGNvbXBldGluZw== 15439
IGNvbmhl 15440
Z2l2aW5n 15441
IGJyYWlucw== 15442
IGxvYW5z 15443
IHdhZ2U= 15444
IENsaW50b24= 15445
IHPEgw== 15446
YW5lb3Vz 15447
IGxvcmQ= 15448
0YDRg9C2 15449
IHF1aXo= 15450
IHN0aWZm 15451
IExHQg== 15452
c3o= 15453
TUU= 15454
bWFyZQ== 15455
dGhlcmU= 15456
IG7DpHI= 15457
IE1hbmQ= 15458
bGFzdA== 15459
IGRhZw== 15460
IGhhbGZ3YXk= 15461
IEJhbmQ= 15462
IOuLpOyLnA== 15463
IEFyZW4= 15464
IGlsZQ== 15465
UE4= 15466
ZW50bw== 15467
IGFsZ3Vt 15468
IHNvY2Nlcg== 15469
IGJsb2NrZWQ= 15470
IEpvbmF0aGFu 15471
IHNldw== 15472
IFRlc3RhbWVudA== 15473
IHZhbGU= 15474
IGJlaGF2aQ== 15475
5aeL 15476
IGNvbm5h 15477
SUNI 15478
IGF1ZGllbmNlcw== 15479
bWw= 15480
YW1tYWQ= 15481
IOyCtOw= 15482
SUdI 15483
IHJhY2Vz 15484
ZW1lZA== 15485
IG3hu5l0 15486
w68= 15487
IG92ZXJz 15488
IGRlY2xhcmVk 15489
IHNhbmE= 15490
IFVuYQ== 15491
INGA0LU= 15492
dWNrcw== 15493
IHBhaXJz 15494
IGFuZ2U= 15495
TmU= 15496
IHVwcw== 15497
YXZ5 15498
w7hy 15499
cmVlaw== 15500
IGJlaGF2aW9ycw== 15501
IHJlZmxlY3RlZA== 15502
IHByaW9yaXRpZXM= 15503
IGNvbmR1 15504
IHJldHJlYXQ= 15505
IGV4cGVuc2Vz 15506
IOu0kA== 15507
IHRyaXBsZQ== 15508
IOq1ieyepe2eiA== 15509
w6RsdA== 15510
IGluZGlnZW5vdXM= 15511
IG1pbmluZw== 15512
IGFjY2VwdGFibGU= 15513
IHJ1aW4= 15514
Q0E= 15515
dWluZQ== 15516
IHBpcGVsaW5l 15517
Y3RpYw== 15518
w6p0 15519
INCy0YHQtdCz0L4= 15520
IGJvdW4= 15521
IERpZ2l0YWw= 15522
IEJvb20= 15523
0YbQtQ== 15524
INC70YPRhw== 15525
IGFzYw== 15526
jIDroZw= 15527
IEdvb2RieWU= 15528
IHJlbmRlcg== 15529
ZW5leg== 15530
YXJyZQ== 15531
IFRIQVQ= 15532
Ym91cg== 15533
aWNpw7Nu 15534
44Kt 15535
RXZlcnk= 15536
IHdpcmVz 15537
IFBhcmxpYW1lbnQ= 15538
bnVuZw== 15539
YXRldXI= 15540
IFNhdmU= 15541
IFBoeXM= 15542
IGFtb3I= 15543
IEV2ZQ== 15544
IGZyaWdodA== 15545
IGdhbW1h 15546
IG1pY3Jvcw== 15547
bWl0dA== 15548
IENvZGU= 15549
IEJleQ== 15550
cGxlZA== 15551
INC40YHQv9C+0LvRjNC3 15552
55c= 15553
7IOJ 15554
5aW5 15555
IG1vbmV0 15556
IEphaHJl 15557
IGx1eHVyeQ== 15558
IGRlYWY= 15559
IGJldHJheQ== 15560
IOqysA== 15561
0LjQutC4 15562
IGRlZmVhdGVk 15563
IHVuZGVydA== 15564
IHdlZw== 15565
IGNvb2xlcg== 15566
44GV44KT 15567
aWFtaQ== 15568
6YKE5pyJ 15569
IEplc3NpY2E= 15570
IEpveQ== 15571
IHNvcGhpc3RpYw== 15572
0LXQvdC40Lg= 15573
8J2Y 15574
IGNoaWxp 15575
IFR5cGU= 15576
IHByb3RlaW5z 15577
IHByZXNlbnRpbmc= 15578
YWxpYQ== 15579
7Jq4 15580
IE1ham9y 15581
IG1vbGVjdWxl 15582
dW1lcg== 15583
IGNvbGxhcHNl 15584
IEFueXdheXM= 15585
IE1vdW50YWlu 15586
YW50ZWQ= 15587
44CQ 15588
INCy0LjQtNC10L4= 15589
5rC0 15590
QXVk 15591
IGNvbnF1 15592
IHZvbGw= 15593
IGtuaXQ= 15594
IG1lbWJy 15595
IE1hcmtldA== 15596
IGRhcmk= 15597
IGNhbGN1bGF0ZWQ= 15598
0LPQuA== 15599
IHNocmltcA== 15600
IE11 15601
INC/0YDQvtGC 15602
IOyYgeyDgQ== 15603
IHByb2R1Y3Rpdml0eQ== 15604
IGNvZ25pdGl2ZQ== 15605
IEhlYg== 15606
aWN0aW9ucw== 15607
6rK9 15608
IGNyw6k= 15609
ZsO2cg== 15610
IHByYXlpbmc= 15611
YXNoaQ== 15612
IFRpaw== 15613
w7Ny 15614
d2Vu 15615
0YzRjg== 15616
aXhv 15617
ICgi 15618
INGC0LXQuw== 15619
IOyWtOuWpA== 15620
INC/0LXRgNC10LQ= 15621
IERyaXZl 15622
44CR 15623
IEVxdQ== 15624
IGVxdWlsaWJyaXVt 15625
IGRlc2NyaWJlcw== 15626
0L3QtdC1 15627
NDI= 15628
IEN1cnJlbnQ= 15629
eXk= 15630
IGFic29yYg== 15631
IHNvbGRpZXI= 15632
ZGVycw== 15633
IHRlc3RpbW9ueQ== 15634
IGRlY2xpbmU= 15635
nOuhnA== 15636
Z2FnZQ== 15637
IGluc3BpcmU= 15638
bGFwcGluZw== 15639
IHNwaW5uaW5n 15640
IHNsYXZlcnk= 15641
IGZhY2lhbA== 15642
IHRyYWRpdGlvbnM= 15643
w6FyaW9z 15644
IEhvc3BpdGFs 15645
IG5lc3Q= 15646
IOuIhA== 15647
IHRvaQ== 15648
IGZlYXJz 15649
7IWo 15650
IE11aA== 15651
IGdyYWR1YXRpb24= 15652
IGltcGFjdGVk 15653
IGF1bnQ= 15654
IExldHM= 15655
IGFsdW1pbnVt 15656
IGRvbWluYW50 15657
IERhdmlz 15658
IE5hdnk= 15659
IGNvbXB0 15660
b3BsZXM= 15661
IGVzdGF2YQ== 15662
6KU= 15663
IHNjYWw= 15664
IHByZXNlcnZl 15665
IE9wcA== 15666
IHByYWN0aWNhbGx5 15667
IG1hZ25pdHVkZQ== 15668
IGZpdHRpbmc= 15669
IGNvb3JkaW5hdGU= 15670
IGZ1cm5pdHVyZQ== 15671
IEZhbWls 15672
IGV4cGxvc2lvbg== 15673
IGRvY3VtZW50YXJ5 15674
IFNjcmlwdA== 15675
IHBvcnRyYXk= 15676
bWF0 15677
IHNjaGVkdWxlZA== 15678
IGR5bmFtaWNz 15679
cGh5 15680
YWt5 15681
IFVJ 15682
Q2hl 15683
IGNvbnRpbnVvdXNseQ== 15684
IFByb3Y= 15685
5bCR 15686
0YPQtw== 15687
cmFo 15688
IGdlcm5l 15689
cHJvb2Y= 15690
IHNlY3JldGFyeQ== 15691
IFBhdHJlb24= 15692
c2NyZWFt 15693
IEtpZHM= 15694
4buTaQ== 15695
IGtn 15696
IHVuY2VydGFpbnR5 15697
INC60LDQttC0 15698
IG1pdGln 15699
IHJlYWRz 15700
5bey 15701
IFJ1 15702
IHByaWVzdA== 15703
INC90LXQtA== 15704
IGxpbWl0YXRpb25z 15705
IGZsb2F0 15706
NjAw 15707
IFRveQ== 15708
IEppbW15 15709
IG9mZmVuc2l2ZQ== 15710
ZW5p 15711
IFhp 15712
IGV5ZWJy 15713
IFR1cms= 15714
IGFjY2lkZW50YWxseQ== 15715
IG9obmU= 15716
IFNhdWQ= 15717
OTU= 15718
IER1dGNo 15719
0LDQvdGB 15720
IFNlYXR0bGU= 15721
IOuTsQ== 15722
Y2hlY2s= 15723
a8SZ 15724
IGNvbnRyaWJ1dGlvbnM= 15725
IGJlc2lkZQ== 15726
IHF1aW5kaQ== 15727
IGZsZXc= 15728
5pe2 15729
2LDYpw== 15730
IExP 15731
IHdhaXN0 15732
IEVW 15733
IGhvbGlkYXlz 15734
am9u 15735
IG1pc3VuZGVy 15736
0Y/QvQ== 15737
IGJvdXQ= 15738
IGRpbWlu 15739
4bq9 15740
w7Ns 15741
IEdyYWNl 15742
IGlucHV0cw== 15743
IGRlbnk= 15744
IGZvcm1pbmc= 15745
IEJpbGQ= 15746
IGFkZXF1 15747
IGZvbGs= 15748
IHJlamVjdGVk 15749
c2VtYg== 15750
IGZydXN0cmF0ZWQ= 15751
b3Blbg== 15752
IEJldHRlcg== 15753
aWxvbg== 15754
IHRvd2Vs 15755
IGRpZmZlcmVudGlhbA== 15756
IHNhY3JlZA== 15757
IHNhaWw= 15758
6YeM 15759
ZW50aW1lcw== 15760
IGdlbnRsZW1hbg== 15761
IGljb25pYw== 15762
IGNvbXBhcmluZw== 15763
IHNhZ3Q= 15764
IHRleHRz 15765
IGdyYW5kbWE= 15766
IHJvbGxz 15767
IGNvbnRlbnRz 15768
5LiN5aW9 15769
0L7RgdGB 15770
IHN1c3BlbnNpb24= 15771
cm9pdA== 15772
prw= 15773
IGFzc2V6 15774
IGRvcnQ= 15775
IE1hdGg= 15776
IFZpY3Rvcg== 15777
IEphdmFTY3JpcHQ= 15778
5LiN5bCN 15779
IGVuaGFu 15780
xZk= 15781
IEJ1c2g= 15782
IHByb21vdGlvbg== 15783
IGtpbg== 15784
IG1vbnN0ZXJz 15785
IENvbG9yYWRv 15786
IM6y 15787
7ZW07JqU 15788
5q2j 15789
aWZmZXJlbnQ= 15790
IG5ha2Vk 15791
IHByb2Q= 15792
ZXRpY3M= 15793
IFdvbWFu 15794
IHRyZWF0bWVudHM= 15795
IGVzdG95 15796
dsOp 15797
IGxpZnRpbmc= 15798
IHlhcHQ= 15799
IFJvYmVy 15800
IOy5nA== 15801
IHN1YnN0aXR1dGU= 15802
YWt1 15803
cmlkZ2U= 15804
IOqxsOs= 15805
IHJlc3BvbmRlZA== 15806
IGLDqQ== 15807
IEVuZ2luZWVy 15808
IHRyYW5zZmVycmVk 15809
67I= 15810
IGhhYmVy 15811
b29w 15812
IFdF 15813
IHZlc3Q= 15814
IGZvcnR5 15815
IERT 15816
IDIwMDQ= 15817
IGNvYWNoaW5n 15818
bm9t 15819
IEJhYg== 15820
IG5vc3Nh 15821
IEpha2U= 15822
IGd5 15823
IGRlbGVn 15824
IOyeoA== 15825
INC60YDQsNGB 15826
IHN0YW5kcG9pbnQ= 15827
IGRpc2Fk 15828
IGFydHdvcms= 15829
QWQ= 15830
aWxsbw== 15831
IMSRxrDhu6Nj 15832
IFByb20= 15833
IExpYg== 15834
IGNyaXRpY2lzbQ== 15835
IGNvbnRhY3Rz 15836
0YDQsNC8 15837
IGFjaGlldmVtZW50 15838
0JTQsA== 15839
IGRpc3NvbA== 15840
IFZlZ2Fz 15841
IHN0cmVhbXM= 15842
IEtlbnQ= 15843
INi52YTZiQ== 15844
IHJhZGl1cw== 15845
IHN1Y2tz 15846
IEFjaA== 15847
IGZp 15848
b3VzdA== 15849
INC70Y7QtNC4 15850
IHBhbGV0dGU= 15851
IEhheg== 15852
IEFudGhvbnk= 15853
IHRlbWE= 15854
IENvcw== 15855
IHNhZmVy 15856
zrHPgg== 15857
IGNvbnRyYWQ= 15858
IG1haW9y 15859
IGluZmxhdGlvbg== 15860
IFNpbHZlcg== 15861
IGF0dGVuZGluZw== 15862
7ZWc7YWM 15863
YXJ0bw== 15864
IGFwcGxhdWRpbmc= 15865
IGNvbXB1dGluZw== 15866
IEhhdA== 15867
5rs= 15868
a25vdw== 15869
bWFrZXJz 15870
IGNvbm9j 15871
IGVkdWNhdGVk 15872
IG1vZGlmaWVk 15873
IGluY2x1c2lvbg== 15874
bWVudGFs 15875
npA= 15876
aXNpYQ== 15877
IM+Azr/PhQ== 15878
IGF1bg== 15879
IElyZWxhbmQ= 15880
IGvDtg== 15881
IGNvbXBsaWFuY2U= 15882
IGluc3BpcmluZw== 15883
0LjRgtC10LvRjNC90L4= 15884
IGRpc3Bvcw== 15885
7LCo 15886
IHdpcA== 15887
cmljYWw= 15888
cmF3ZA== 15889
IHRyZXM= 15890
IG1vYmls 15891
b2x1dGlvbnM= 15892
Qk8= 15893
IGJvdW5jZQ== 15894
IGFzc3VtZWQ= 15895
IE1lZGljYWw= 15896
IGZpc2NhbA== 15897
IG5nxrDhu51p 15898
aXRpb25hbGx5 15899
IHN0b2xlbg== 15900
IEJN 15901
IG1lY2hhbmlzbXM= 15902
zrXOrw== 15903
IHF1YWxpZmllZA== 15904
IOyekOs= 15905
dWdodGVycw== 15906
IEhJVg== 15907
IExvdHM= 15908
IHNlcnZlcnM= 15909
IGNhcnI= 15910
IFRvZ2V0aGVy 15911
IGF0dHJhY3RlZA== 15912
IGty 15913
5oiR5piv 15914
dGh1cg== 15915
aW5pbg== 15916
IEhhbGY= 15917
yJs= 15918
IFBhcA== 15919
IHJlbWluZGVk 15920
QUxM 15921
IGhlbG1ldA== 15922
IGJvdHRsZXM= 15923
IHByb2Zlc3NvcnM= 15924
IHNlaW5l 15925
xYLEhQ== 15926
44OP 15927
IOqxsOyVvA== 15928
INei15w= 15929
ZnVu 15930
IEJpcmQ= 15931
IGZpZ2h0ZXI= 15932
IOuUsOs= 15933
IFRvb2w= 15934
IHRpbg== 15935
aW5vaXM= 15936
67aE 15937
15nXnw== 15938
IENBUg== 15939
5ZCN 15940
aXJzdHk= 15941
IG91dGRvb3I= 15942
IE5T 15943
44WO 15944
ZmZlbg== 15945
IGx1ZA== 15946
SGVsbG8= 15947
IHJvbGxlcg== 15948
aWVsZQ== 15949
IFBvbGFuZA== 15950
IGFwYQ== 15951
ZXhw 15952
IGNlcnRpZmljYXRl 15953
IFRvd24= 15954
0LDRjtGC0YHRjw== 15955
aWxkZQ== 15956
IGRldGVybWlu 15957
UFI= 15958
IGZyZWV6ZQ== 15959
IG1haW5zdHJlYW0= 15960
IG9iamVjdGl2ZXM= 15961
Ymxv 15962
IHRha2ll 15963
5ZOI5ZOI 15964
IOuwlOuhnA== 15965
ZWxldA== 15966
IElW 15967
IEZhc3Q= 15968
IGRlcmU= 15969
ZW1w 15970
IERyYQ== 15971
IOyeiOyXiA== 15972
IGRpc2NyaW1pbmF0aW9u 15973
IM61zq/Ovc6xzrk= 15974
bmVjZXNz 15975
5q4= 15976
xLHEn8Sx 15977
IHBvc3Rpbmc= 15978
d2nFm2NpZQ== 15979
IGx1Yg== 15980
IG9saXZl 15981
IHJpbQ== 15982
IG1vZGVsaW5n 15983
IGHDsW8= 15984
IFBha2lzdGFu 15985
IG92ZXJs 15986
IGluZmxhbQ== 15987
TkU= 15988
7JeQ6rKM 15989
IGF0dGVuZGVk 15990
IGRlYWx0 15991
IEFsdA== 15992
IExpbmNvbG4= 15993
IGF3YWtl 15994
IGZpbHRlcnM= 15995
IFdpdGhpbg== 15996
Y3p5d2nFm2NpZQ== 15997
IHPDuw== 15998
IEpvaG5ueQ== 15999
IGludGVncml0eQ== 16000
IGlzb2xhdGlvbg== 16001
IEVhc3k= 16002
INC/0YDQuNC9 16003
IEFsaWNl 16004
IHNtaWxpbmc= 16005
ZW5peA== 16006
LC4uLg== 16007
zrY= 16008
IGJlZ3Vu 16009
IGpld2Vs 16010
IGNvbnZlbnRpb25hbA== 16011
IHN0YXRpc3Q= 16012
IGhhbmRlZA== 16013
IGlycmU= 16014
IHByb2hpYg== 16015
IHNhdGVsbGl0ZQ== 16016
6aaZ 16017
IEluZHVzdA== 16018
IHRyYWdlZA== 16019
IHRyYXZh 16020
IGlobQ== 16021
IGNydWVs 16022
IEFnb3Jh 16023
IERvYw== 16024
IHpvbmVz 16025
IG1hbGw= 16026
IHRyYXk= 16027
15XXoA== 16028
IGlycml0 16029
IGthbnM= 16030
IEJlYXQ= 16031
dWRnZQ== 16032
aWVsbGU= 16033
IHRydXN0ZWQ= 16034
IGJpa2Vz 16035
INGD0L8= 16036
IE1lbWJlcg== 16037
d2ljaw== 16038
IGNyZWF0b3Jz 16039
IGhlcml0YWdl 16040
aW5kaXN0aW5jdA== 16041
IHJlc3Vy 16042
ZW5uZW4= 16043
Q29tZQ== 16044
IGZpcmluZw== 16045
IEJ1ZW5v 16046
INCi0L4= 16047
aWthbg== 16048
ZXR0ZXM= 16049
IGtlcw== 16050
IHRyaXBz 16051
IGRpdm9yY2U= 16052
IEts 16053
IGNvbnNvbA== 16054
a2VlcA== 16055
6riw6rCA 16056
IFJlcG9ydA== 16057
IGhvc3Rpbmc= 16058
IGRpYW1vbmQ= 16059
IGNvbXBsaWM= 16060
IGhlbGljb3A= 16061
IGRlcHVpcw== 16062
ZHM= 16063
IENoYW4= 16064
0Y/Quw== 16065
IHNjaXNzb3Jz 16066
aWxhdGlvbg== 16067
IHByb3BvcnRpb24= 16068
RVJF 16069
INmI2KfZhA== 16070
aW50YQ== 16071
IG11Y2hhcw== 16072
dWF0aW9u 16073
aXRpcw== 16074
5oqK 16075
0Y/RiQ== 16076
IG5paW4= 16077
IGVtcGhhc2l6ZQ== 16078
dWVsYQ== 16079
IHByb2R1Y2Vycw== 16080
IHJ6ZQ== 16081
w6RuZGVy 16082
RVRI 16083
5ro= 16084
IGNvbnN0aXR1 16085
5Zu9 16086
IHBlcmZvcm1hbmNlcw== 16087
aXN0bGU= 16088
Z292 16089
IExpdGVy 16090
IGluY29ycG9yYXRl 16091
IGVkdWNhdGU= 16092
IE5pbg== 16093
7Kq9 16094
2YfZhQ== 16095
ZWxlcmF0aW9u 16096
15XXkQ== 16097
IHlhxZ8= 16098
b3JvdXM= 16099
IENhcw== 16100
IGdyYW50cw== 16101
64ql 16102
YW1lbA== 16103
IOq3uOugh+qyjA== 16104
IEVzdGU= 16105
0YXQvtC00LjRgg== 16106
INC/0L7RgdC70LU= 16107
IGdlbnQ= 16108
IGZvY3VzZXM= 16109
YWxpdGllcw== 16110
IFJo 16111
67O0 16112
5rCR 16113
IERhbmNl 16114
cnI= 16115
IGFtZXI= 16116
IHV0aWxpemU= 16117
IGzDrQ== 16118
IEFtb25n 16119
IHByZWduYW5jeQ== 16120
IGxvb3Bz 16121
0LDQu9C+0YHRjA== 16122
IE1vaA== 16123
IGNhdGNoaW5n 16124
IGdsb2I= 16125
IGFqdWQ= 16126
IFs/ 16127
IEFuYWw= 16128
bG9va2luZw== 16129
IHN1cmZhY2Vz 16130
IHByb2dyZXNzaXZl 16131
IHZpcmFs 16132
MDg= 16133
zr4= 16134
S0E= 16135
IMW8eQ== 16136
IHBpY2tz 16137
YW5ub24= 16138
IGJ1bGs= 16139
IFJvc3M= 16140
IGRlc2NyaWJpbmc= 16141
IEdlbA== 16142
IGxvY2FsbHk= 16143
IGVuZGxlc3M= 16144
IG1hc3NhZ2U= 16145
IGNsZWFuZWQ= 16146
IHRyYXZlbGVk 16147
0LXQvdGL 16148
IHNlbnRpbWVudA== 16149
aWdtYQ== 16150
IE5hcw== 16151
IGNoZW1pY2Fscw== 16152
IHJpZ2h0ZW91cw== 16153
IE1hZ2lj 16154
IHJlbGF0ZXM= 16155
IHRydWNrcw== 16156
IDE5NjA= 16157
5Yil 16158
IGFwcGV0 16159
IHNuYWNrcw== 16160
IFN1bW1lcg== 16161
IHnDvHo= 16162
IHByaXM= 16163
IE1leGljYW4= 16164
IHRyYW5zcGFyZW4= 16165
IG1pbm9yaXR5 16166
IHZlcnRl 16167
IGxhc3Nlbg== 16168
NDY= 16169
0LvQtdC6 16170
w6lw 16171
INGE0LjQu9GM 16172
IGl5aQ== 16173
IHNwYW4= 16174
7ZWY7KeA 16175
IGluZGljYXRlZA== 16176
cXVhcg== 16177
IHNjaG9sYXJzaGlw 16178
IExHQlQ= 16179
IGhpc3RvcmljYWxseQ== 16180
w7PFgg== 16181
IG1pbmlzdA== 16182
IHBlbmV0 16183
IFJhcA== 16184
IGNvbnNlcnZhdGlvbg== 16185
55u0 16186
IEhvbmV5 16187
IEJlaQ== 16188
aWRlbA== 16189
IHJlc3BvbnNpYmlsaXRpZXM= 16190
IG1lc3N5 16191
IEV4Y2VwdA== 16192
T1JF 16193
IGluaXRpYXRpdmVz 16194
IGp1bmlvcg== 16195
IGRlc2lnbmVycw== 16196
IGV4cGxvcmF0aW9u 16197
IHNwb25zb3I= 16198
IG1vYmlsaXR5 16199
IGludGVn 16200
bGFuZG8= 16201
IGJhcms= 16202
IGluZGljYXRlcw== 16203
4LY= 16204
IGVtcGxveWVy 16205
5a6J 16206
IGNvdXNpbg== 16207
IGJvaWxpbmc= 16208
IGNocm9t 16209
IMOnYWw= 16210
IHBlcnBldA== 16211
IGNvbnRhaW5lZA== 16212
IHBhcmtz 16213
0Ks= 16214
IEVuZ2luZWVyaW5n 16215
UGxlYXNl 16216
IFN0YXJ0aW5n 16217
aGVybw== 16218
IGxhd3llcnM= 16219
6KW/ 16220
IHpk 16221
IGZyYW5jaGlzZQ== 16222
cmFnZQ== 16223
IGludHVpdA== 16224
IEdM 16225
cmVhY2g= 16226
IEVsbGU= 16227
IG5oxrA= 16228
IE5vcmQ= 16229
IGJlYW4= 16230
MDc= 16231
IHBsZWFzYW50 16232
5b2T 16233
dmlyb24= 16234
IGdyYWRpZW50 16235
enVz 16236
IEVN 16237
IGVzc2F5 16238
7JeQ7JqU 16239
4bq/bg== 16240
bnU= 16241
4bur 16242
IMOJcw== 16243
IGRlbm9taW4= 16244
IEdpcmxz 16245
IHBlcnNvbm5lcw== 16246
INin2YTYow== 16247
YmlsZA== 16248
IFN0YXQ= 16249
IGNvbXBsaW1lbnQ= 16250
IEthdGU= 16251
IG9wdGltYWw= 16252
IGhpZA== 16253
2K/Zig== 16254
IHF1aWNrZXI= 16255
d2FsbA== 16256
RW4= 16257
SU5F 16258
Pz8/ 16259
7LK0 16260
IEFjdGlvbg== 16261
5Z8= 16262
IHBlbmFsdHk= 16263
IEtheg== 16264
Jz8= 16265
IGNyaWVk 16266
IGNhbnZhcw== 16267
ZnRl 16268
IGV4Y2x1ZA== 16269
uOuhnA== 16270
IGVtcGhhc2lz 16271
IGVuenk= 16272
IEhvdQ== 16273
IG92ZXJzZWFz 16274
w61hbW9z 16275
5bir 16276
w7ZnbGljaA== 16277
IGhlYWRwaG9uZXM= 16278
Y24= 16279
IEFnZQ== 16280
IGFrYW4= 16281
IGNoYXJhY3RlcmlzdGlj 16282
7ZWY66m0 16283
Z2V0cw== 16284
IOu2iA== 16285
IHJpdmFs 16286
IGJvcmRlcnM= 16287
ZW1lbnRl 16288
ZW3DoXM= 16289
IHlvbA== 16290
IGNvbXBl 16291
ZW5kZXJz 16292
xLFuZGFu 16293
IG3DtmdsaWNo 16294
IGJ1YmJsZXM= 16295
bmF0dXJhbA== 16296
IGFybWVk 16297
IGVsYWJvcg== 16298
IOydtOuyiA== 16299
IHdhc2hlZA== 16300
zr/Phc68zrU= 16301
6KuL 16302
IGZsYXZvcnM= 16303
IGV4aXN0ZQ== 16304
IHByZXN0 16305
IFRoZW1h 16306
0L7Qv9GA0L7RgQ== 16307
ZXJvbg== 16308
VUU= 16309
ZXJp 16310
IGNvbmNlcg== 16311
IGFpeMOy 16312
5YWp 16313
IHByb3RlY3RpdmU= 16314
INC30L3QsNGO 16315
IOuCoA== 16316
IElJSQ== 16317
IG1lZXI= 16318
IFNob3A= 16319
bGxp 16320
IE9yZGVy 16321
IE1Z 16322
IEdob3N0 16323
44KC44GG 16324
YWRlbA== 16325
IHN0b2xl 16326
IHJlbGVhc2luZw== 16327
IENvbW1lbnQ= 16328
IHRyYWlucw== 16329
66qF 16330
IHdpc3Nlbg== 16331
ZW5zZWQ= 16332
IGRlc2NlbmQ= 16333
IGZpZXI= 16334
IHJhZGk= 16335
IHBlcnN1 16336
56I= 16337
INC80L0= 16338
IERlc3Q= 16339
IHdvcnJpZXM= 16340
aXRldA== 16341
YmFz 16342
IHN0YWI= 16343
bmFtZQ== 16344
b3JpYw== 16345
IENsb3Nl 16346
IGFsdW1uaQ== 16347
IFNlbGY= 16348
ZmZl 16349
aXRhdGluZw== 16350
YXRoZXJpbmU= 16351
IFJpZ2h0cw== 16352
IGVsbG9z 16353
IHdhcnJhbnQ= 16354
IG5lcnZl 16355
IHZlZ2V0YWJsZQ== 16356
IFRlaWw= 16357
IOqwmeydtA== 16358
Ulk= 16359
IHN1c3RhaW5hYmlsaXR5 16360
IHN0ZWh0 16361
IGJyaWQ= 16362
YWRhxZ8= 16363
IHR2 16364
IGR1cmF0aW9u 16365
IHBlc3NvYQ== 16366
IG1ldHJpY3M= 16367
IGFkYW0= 16368
Y2Fz 16369
0LDRgNC4 16370
IGV2aWRlbnQ= 16371
IGRpc3BsYXllZA== 16372
2KfYpg== 16373
IHJlY2s= 16374
IEJ1ZGRoYQ== 16375
IGRlbGU= 16376
IERpZWdv 16377
b3NwaA== 16378
IGJsYQ== 16379
IE1paw== 16380
dWxhdG9y 16381
IDIwMDE= 16382
IHByb21vdGluZw== 16383
eWNo 16384
IEVY 16385
IGxhc3RseQ== 16386
IG91dGxpbmU= 16387
IHNwaXJpdHM= 16388
IHZldXg= 16389
IHN1YnRyYWN0 16390
IMWfaW1kaQ== 16391
IHBpbnM= 16392
IGJ1cmdlcg== 16393
IG1vbHRv 16394
IGhhYsOtYQ== 16395
IOuwmA== 16396
aWd1 16397
ZXJzdA== 16398
IG5lbg== 16399
IGJhY29u 16400
aXRpb3Vz 16401
IGNhcnJpZXM= 16402
IHByb21pc2Vz 16403
bmRl 16404
IExlZnQ= 16405
IExpbQ== 16406
5qM= 16407
IDQ0 16408
IGNhcmVlcnM= 16409
IOyjvOs= 16410
IHNwZWVkcw== 16411
cXXDqQ== 16412
bWFk 16413
bWFya2V0 16414
aXNtZQ== 16415
IDIwMDM= 16416
IHJlY2Vzcw== 16417
IEpVRA== 16418
IHJhY2lzdA== 16419
IFNjaGw= 16420
IHBhcmxlcg== 16421
IG90cm9z 16422
aXNoZXM= 16423
IGNvbnZlcnRlZA== 16424
YWFhYQ== 16425
0LDQvdC40Lg= 16426
IEFyaw== 16427
IENoYW5jZQ== 16428
IGVsZW1lbnRhcnk= 16429
zrXOvQ== 16430
aW5rcw== 16431
SW50ZXJ2aWV3ZXI= 16432
IGZyZWVseQ== 16433
YWxhaA== 16434
IOuLpOuluA== 16435
IHJlcXVlc3RlZA== 16436
IHRvcnF1ZQ== 16437
bm/Fm2Np 16438
b3VyZWQ= 16439
IFN0YWZm 16440
IHN0YWlu 16441
IEFsYW4= 16442
IHZlcmU= 16443
IFdpbnRlcg== 16444
IGRlZmVjdA== 16445
aWVkeQ== 16446
IGJlYXRz 16447
IGjDoQ== 16448
dW1u 16449
b29ucw== 16450
aXR1ZGVz 16451
IHNlaXQ= 16452
b2x5 16453
IHJlc2Vydg== 16454
IGV4dHI= 16455
IHBoeXNpY2lhbg== 16456
dmlzb3I= 16457
IGhhbmRmdWw= 16458
IE5hdGlvbnM= 16459
IOyii+ydgA== 16460
dWNjZXNz 16461
IHVwc3RhaXJz 16462
IFNxdWFyZQ== 16463
IGhlaW4= 16464
IFNlYXNvbg== 16465
b2xpcw== 16466
IHByaW5jZQ== 16467
IGRlZmVuc2l2ZQ== 16468
570= 16469
INC80LXRgdGC 16470
0ZbQuQ== 16471
INin2YY= 16472
dW1ibGU= 16473
6rmM7JqU 16474
IGFzc2Fzcw== 16475
IGNpcmN1bGFy 16476
IHF1YWxpdGllcw== 16477
IGhtbQ== 16478
IGJsb3du 16479
IExpeg== 16480
IEt1cg== 16481
IFNB 16482
IGZpbmRpbmdz 16483
IGNvbG91cnM= 16484
IGRlbGxl 16485
IElS 16486
IEF0aA== 16487
IER1Yg== 16488
IE94 16489
INiu 16490
IHBvY2tldHM= 16491
IGdyaWxs 16492
IHN3aXRjaGluZw== 16493
IHByZWZlcnJlZA== 16494
IFdhbGVz 16495
IGV4ZW1wbG8= 16496
IGNob3BwZWQ= 16497
IHZhY2NpbmF0aW9u 16498
IG5ldXJv 16499
IHNwZWNpZnk= 16500
aXZvcw== 16501
IHNlcsOh 16502
IHppZQ== 16503
IOCurg== 16504
IHJlc3VsdGluZw== 16505
IFVnaA== 16506
IG1lc3NlZA== 16507
Q0Q= 16508
IHBhYXI= 16509
IGNvbWVy 16510
IGNvdWNo 16511
IEZlc3RpdmFs 16512
IDQ5 16513
dm91cw== 16514
emVucw== 16515
56iu 16516
IEtlbm5lZHk= 16517
IFRz 16518
IOuztOyX 16519
IGRlbW9uc3RyYXRpb24= 16520
IHVudG8= 16521
IGZydXN0cmF0aW5n 16522
IGxhYm9yYXRvcnk= 16523
IGVneQ== 16524
IGJlYXV0aWZ1bGx5 16525
IOyerOs= 16526
IGFsZ3U= 16527
IMO2eWxl 16528
5L2g55yL 16529
IFBI 16530
IGZvcnR1bmU= 16531
IGNsZWFuZXI= 16532
IFJvYmlu 16533
IHNhdXM= 16534
IEdlbGQ= 16535
IGthdA== 16536
b2Jz 16537
IG9sdXI= 16538
IG1hdHQ= 16539
IHF1ZXN0YQ== 16540
IHN1Z2dlc3Rpb24= 16541
ZW5jZXI= 16542
0L7RgdGC 16543
IHJhZGFy 16544
IOyeoQ== 16545
aXNoYQ== 16546
4K6o 16547
44KT44Gq 16548
amVz 16549
IHZlZWw= 16550
7IKw 16551
IGF1dGhvcnM= 16552
44CO 16553
cGxhbg== 16554
IGNvbGxhYm9yYXRpdmU= 16555
IGluc3RpbmN0 16556
IGZhcm1pbmc= 16557
YXVnZQ== 16558
RWR1 16559
IG1lbWJlcnNoaXA= 16560
IHNpbXVsdGFuZW91c2x5 16561
IGJha2U= 16562
IGvDpA== 16563
IGxlY3R1cmVz 16564
0YfQtdGB 16565
IHByZW5kcmU= 16566
IGNvbGxhcHM= 16567
IFNheWE= 16568
IEZ1dA== 16569
IHlvZw== 16570
IFJhdGhlcg== 16571
2LHZig== 16572
IGNhbXBz 16573
0L7Qu9C+0LQ= 16574
IHNpbXVsYXRpb24= 16575
IE1haw== 16576
TGF1Z2hz 16577
IGdyZXk= 16578
IHNlbnRlbmNlcw== 16579
eWVu 16580
IFVubGVzcw== 16581
SmU= 16582
IFNhdGFu 16583
INGC0LDQutC20LU= 16584
IE5B 16585
IGJyb24= 16586
ID9d 16587
IHNvdWxz 16588
IGxpZ2h0bmluZw== 16589
IGltYWdpbmVk 16590
IGN6eWxp 16591
cHNpbG9u 16592
ZXR0YQ== 16593
IGJlbGlldmluZw== 16594
IHN0cm9uZ2VzdA== 16595
IENPTg== 16596
IHF1ZWxxdWVz 16597
IGltbWlncmFudHM= 16598
IHdhbGxldA== 16599
6YCZ5piv 16600
IEplcnNleQ== 16601
IGltcGxpY2F0aW9ucw== 16602
IGZvcmI= 16603
44CP 16604
IHVuYmVsaWV2YWJsZQ== 16605
2KfYoQ== 16606
IG9wZXJhdGlvbmFs 16607
w7xz 16608
IEdN 16609
IOq3uOufsOuNsA== 16610
IGdyYWNpYXM= 16611
IGVudGVuZA== 16612
IFJlZ2FyZA== 16613
cm9i 16614
INGC0LXRhQ== 16615
6I8= 16616
IFJldm9sdXRpb24= 16617
IHdhYXI= 16618
IEJpeg== 16619
dGhlbGVzcw== 16620
IHNwb25zb3JlZA== 16621
cXVpZXI= 16622
IOydvOs= 16623
IHRlaw== 16624
IOuQoA== 16625
aWdrZWl0 16626
IEx1Y2s= 16627
IENlcnRhaW5seQ== 16628
IHRvbGw= 16629
INC90LjRh9C10LPQvg== 16630
IE1vbmV5 16631
INGB0YLQvtGA 16632
IERvdWJsZQ== 16633
IFdvbGY= 16634
IGNodW5r 16635
zqzOvQ== 16636
aXTDqXM= 16637
b25pbmc= 16638
TWFy 16639
IGdyYW5kZXM= 16640
IGNvbGxlY3Rpb25z 16641
IEV1cm9wYQ== 16642
INCw0YA= 16643
IOKAi+KAi+KAiw== 16644
IOq3uOufrOuptA== 16645
INC+0LHRig== 16646
IOOBqg== 16647
IOyLnOqwhA== 16648
IEN1c3RvbQ== 16649
IOyymA== 16650
0ZbQu9GM 16651
IGluZGl2aWR1YWxseQ== 16652
7Zc= 16653
IGRvemVu 16654
IG93ZQ== 16655
IFZpY3Rvcmlh 16656
5Y+v6IO9 16657
IGJlZXQ= 16658
dXJi 16659
IGFuYWxvZw== 16660
acOnw6Nv 16661
gpw= 16662
c29ldmVy 16663
IG1vZG8= 16664
IHN1YnNjcmliZWQ= 16665
7J6s 16666
IGVudGl0aWVz 16667
54mH 16668
IGNsb3NldA== 16669
IHJlc3BvbmRpbmc= 16670
IHByaW50ZXI= 16671
IFN0ZXBoYW4= 16672
IGJ5xYI= 16673
IERvbQ== 16674
IEZlcm4= 16675
IFBpZXI= 16676
IHdpxJlj 16677
IGhlbmNl 16678
IG1vZHVsZXM= 16679
44Os 16680
IOuUsQ== 16681
IERhbm55 16682
INGB0LXQsdC1 16683
IHZhZA== 16684
IOyXhA== 16685
IHNvdXM= 16686
IHNwaGVyZQ== 16687
Qlk= 16688
IFBlZA== 16689
aWduZWQ= 16690
IHdoZWF0 16691
IHVuZGVycw== 16692
IGV2b2x2ZQ== 16693
IGRlY2xhcg== 16694
IGxpZ2h0bHk= 16695
IGlkZW50aWZ5aW5n 16696
5oSP5oCd 16697
IGxlZ2VuZGFyeQ== 16698
IGdlbnVpbmU= 16699
IGdyaW5k 16700
IFVuZQ== 16701
Z2ViZW4= 16702
IGJpY3k= 16703
IGp1bXBz 16704
IHByb3ZpbmNl 16705
emnEmQ== 16706
INeQ16DXmQ== 16707
IGhvYw== 16708
INCx0Ls= 16709
IEdyYWQ= 16710
IHJldmVuZ2U= 16711
INin2YTYqg== 16712
b29o 16713
5ouc 16714
0LDRhtC40Lg= 16715
5bmz 16716
IGVsZWN0cm8= 16717
IOuQkA== 16718
44Gn44Gv 16719
IGZhbHM= 16720
cmllbA== 16721
b2tlcg== 16722
IEV4Y2VsbGVudA== 16723
IE1vcmdhbg== 16724
IGJyaWNr 16725
IHN1YnN0YW50aWFs 16726
IHBvbGx1dGlvbg== 16727
IFTDvHI= 16728
IEV2ZXQ= 16729
IGx1bmc= 16730
44GW 16731
15nXqQ== 16732
b21tZXM= 16733
IHJlYWxpemluZw== 16734
IGh1bWJsZQ== 16735
IExvY2s= 16736
IGJvZA== 16737
IOyWuA== 16738
IHBlZXJz 16739
dXp6 16740
IGVtYmVkZGVk 16741
IGNsYXJv 16742
IGFnZ3JlZw== 16743
IGVtcGxveWVycw== 16744
IFJhag== 16745
IOOBqA== 16746
IFlp 16747
IGpldQ== 16748
YXRlcnM= 16749
IHN0cmlrZXM= 16750
bm9z 16751
YXV0cmVz 16752
ZHI= 16753
b3BoZXI= 16754
IEFwcGFyZW50bHk= 16755
7ZiE 16756
IGluZmFudA== 16757
2KfYqA== 16758
0YLRiw== 16759
7Zs= 16760
2q8= 16761
IHJlZGVz 16762
YWNhxJ/EsW0= 16763
IERBVklE 16764
IENoaWNrZW4= 16765
IHBlcnNwZWN0aXZlcw== 16766
IHZpZXdlcg== 16767
IHNoYXI= 16768
INC/0YDQvtC40Lc= 16769
bGlndA== 16770
ZXJvcw== 16771
aXRhYmxl 16772
0LjQu9C+0YHRjA== 16773
IGRpZsOt 16774
tOuNsA== 16775
IHJldGlyZWQ= 16776
IHRoYXRz 16777
emVuaWU= 16778
YmVpdGVu 16779
IG15Y2tldA== 16780
IFJhYg== 16781
IGluZmxhbW0= 16782
7LCu 16783
IGR1bQ== 16784
IGRhZGR5 16785
5pyf 16786
IGltbWVycw== 16787
IHBsYXlsaXN0 16788
4K+G 16789
IHRyYXVt 16790
IHJlZnVzZQ== 16791
c3RlcA== 16792
4K6a 16793
Y3Vw 16794
IHBvcHM= 16795
cmltaW4= 16796
YXnEsW0= 16797
IGFsZA== 16798
IHVubmVjZXNz 16799
IGRhaA== 16800
IElyaXNo 16801
IGNvbXBy 16802
bGHFnw== 16803
VFA= 16804
IHRyYW5zbGF0ZWQ= 16805
U2M= 16806
Y2XEn2lt 16807
tJA= 16808
IGRyZWk= 16809
INC70Y7QtNC10Lk= 16810
IHF1aWVybw== 16811
IGhlbGU= 16812
emxpY2g= 16813
IGFwcGxlcw== 16814
IGRpc3RyaWN0cw== 16815
IGNyZWRpdHM= 16816
IGFzcA== 16817
IOuLqA== 16818
b3JhbA== 16819
5b2x 16820
IHN0ZXBwaW5n 16821
IFZh 16822
IGdhaW5z 16823
NjU= 16824
IG51ZXN0cmE= 16825
ZWRheQ== 16826
YXNzYWRvcg== 16827
IExpbmQ= 16828
IGNyb3Bz 16829
Y2llbmRv 16830
aWd1ZQ== 16831
IGJhbmE= 16832
QW0= 16833
IHBlbnQ= 16834
IGFkZGljdGlvbg== 16835
IHBhY2thZ2luZw== 16836
w6Rk 16837
qqg= 16838
IHBlcnF1w6g= 16839
IGNhbXBhaWducw== 16840
IHN0ZWVw 16841
IG5ldWU= 16842
IGVtYmFycmFzc2Vk 16843
IGRpc3RpbmN0aW9u 16844
aXR6ZXI= 16845
5ZGK 16846
IHJlZ2lzdHJhdGlvbg== 16847
IGxsYW0= 16848
IEFsbWlnaHR5 16849
bGllc3Q= 16850
IHV6 16851
bmFr 16852
57o= 16853
IHRlcmF6 16854
aWFtZW50ZQ== 16855
IHRyYW5zYWN0aW9ucw== 16856
IGPDtHQ= 16857
IHN3aXRjaGVk 16858
IGNvbWJv 16859
IHByYXllcnM= 16860
IGludGVybnNoaXA= 16861
IGFkZHJlc3Nlcw== 16862
IGNoYXJpdHk= 16863
IFdPTw== 16864
IGJhaXQ= 16865
6L+H 16866
IO+/vQ== 16867
IGZpY2E= 16868
IFR5bGVy 16869
YXJ1 16870
IGF0b21z 16871
IExldmVs 16872
INC/0L7RgtC+0Lw= 16873
IGZhbWU= 16874
dWxr 16875
IHRlYWNoZXM= 16876
IHJlYnVpbGQ= 16877
0LXQtNGM 16878
IEluZG9uZXNpYQ== 16879
dXNoaQ== 16880
IFNob3J0 16881
IGVuc3VyaW5n 16882
ZnM= 16883
ZWxl 16884
IG1hcmdpbmFs 16885
IGNvbmNsdWRl 16886
YW10 16887
IHZlcmlmeQ== 16888
IE1jRG9uYWxk 16889
IHNrYWw= 16890
IHJlY29uc3Q= 16891
IE1hbm4= 16892
IGJhc2VtZW50 16893
IHRyYW5zZm9ybWVk 16894
IG9jY2FzaW9uYWxseQ== 16895
em9uZQ== 16896
IERhbnM= 16897
INC60LDQutC+0Lk= 16898
IGRpYWdub3NlZA== 16899
IM+EzrE= 16900
IGNvbW1hbmRz 16901
IHByZXNpZGVudGlhbA== 16902
IGFiYg== 16903
IGJyYWNrZXQ= 16904
IExlbQ== 16905
w6VuZw== 16906
IGZhdm9yaXRlcw== 16907
IHJldm9s 16908
IO2KuQ== 16909
IGhhcmFzcw== 16910
6YU= 16911
IGNsZWFucw== 16912
c3TDpG5k 16913
IGtub2NrZWQ= 16914
IHBlb3BsZXM= 16915
IG11c2ljaWFucw== 16916
IG11dHVhbA== 16917
IENvbGQ= 16918
ODg= 16919
emVq 16920
YXRpZQ== 16921
IEhvbm9y 16922
IG9ic2Vzc2Vk 16923
IE1VU0lD 16924
IEJyZWFr 16925
w7puZw== 16926
IG1vZGlmeQ== 16927
IHPDtnlsZQ== 16928
INee15Q= 16929
IE9ubGluZQ== 16930
Zm8= 16931
IE1pbGxlcg== 16932
IGxpa2luZw== 16933
IGluaGFi 16934
IGdyYXRpdHVkZQ== 16935
IEpvdXJuYWw= 16936
YXJuZXNz 16937
Sm9obg== 16938
IEdpdA== 16939
5Ymb 16940
IHNpbmNlcmU= 16941
IFNjaQ== 16942
IEVsaQ== 16943
IHN5bWJvbHM= 16944
IG1hbnVhbGx5 16945
zrXPgg== 16946
INCy0ZbQtA== 16947
IEZhdA== 16948
IGxhYmVscw== 16949
IHNvcGhpc3RpY2F0ZWQ= 16950
dW1wcw== 16951
IHJlbGVhc2Vz 16952
IDQ3 16953
IE9N 16954
6rCA6w== 16955
IEJpZW4= 16956
IFJlZg== 16957
6KiY 16958
IFN0YQ== 16959
IEVnZw== 16960
IGluZGljYXRvcg== 16961
cHNvbg== 16962
IG5hc8SxbA== 16963
UmlnaHQ= 16964
IGNvbnZleQ== 16965
IGtub3Q= 16966
IGNvbm5lY3Rz 16967
dWxhcw== 16968
IHByZWNlZA== 16969
IGluZXF1YWxpdHk= 16970
YW1pZW50bw== 16971
IHJlcGx5 16972
T1k= 16973
IGRpc21pc3M= 16974
IOuQnA== 16975
54Sh 16976
INGF0L7RgNC+0YjQvg== 16977
IG3DqWQ= 16978
IHJhbmRvbWx5 16979
IE9udA== 16980
dWFyZA== 16981
IHB1bGxz 16982
INGC0LXQv9C10YDRjA== 16983
IE5lZWQ= 16984
IFNvZnQ= 16985
IHN0cmVuZ3Rocw== 16986
IGdvZWQ= 16987
dW1lbg== 16988
5q27 16989
IO2OuA== 16990
INC00L7QsQ== 16991
IGNsYXJpdHk= 16992
IEFp 16993
IGJhbGxvb24= 16994
IFBhbmQ= 16995
IOyVhOuL 16996
IHNoaW55 16997
IHNtYWxsZXN0 16998
b25pYQ== 16999
aGlsbA== 17000
b3Rpbmc= 17001
IGVpbmc= 17002
IG1lcmVseQ== 17003
IHNldXM= 17004
INC90LXQvw== 17005
IO2GtQ== 17006
IGd1aWRlcw== 17007
IHNwZWNpYWxpc3Q= 17008
IHN0ZWFr 17009
44KI44GG 17010
IG1pZ3JhdGlvbg== 17011
cXVlbGU= 17012
IHJ1aW5lZA== 17013
IHB1cHA= 17014
5aWz 17015
IGtlbmQ= 17016
YW5nYW4= 17017
IHBhbG0= 17018
IHVuZmFpcg== 17019
IHpt 17020
IERW 17021
Y2hlc3Rlcg== 17022
0LjRjg== 17023
IG9vaA== 17024
ZXJn 17025
QVRI 17026
sKk= 17027
5ZOq 17028
cmlzb24= 17029
IGludm9sdmluZw== 17030
IHBhcnRseQ== 17031
YW7Dp2Fpcw== 17032
IHZvdw== 17033
IHByb21pbmVudA== 17034
IGNyeXN0 17035
aWJh 17036
IGRlc2VydmVz 17037
IG92ZXJ0 17038
IHNlbnNpdA== 17039
IFdoZQ== 17040
IHRpZ2h0ZW4= 17041
IGludGltaWQ= 17042
IGFsaW1lbnQ= 17043
d2lsbA== 17044
IHN0cmVuZ3RoZW4= 17045
IFRhbg== 17046
5Y+I 17047
44GX44G+44GZ 17048
b25p 17049
IE11bg== 17050
IHByb3Bo 17051
IHJlaGVhcnM= 17052
IEtsZQ== 17053
IHZlY2Vz 17054
IHdvbmRlcmVk 17055
b2tp 17056
IHNlbnNlcw== 17057
tOyL 17058
xrDhu5s= 17059
IMiZaQ== 17060
IG11Y2hvcw== 17061
IHdhdGNoZXM= 17062
b3J0dW5hdGU= 17063
IEp1YW4= 17064
7J6W7JWE 17065
0YDQtQ== 17066
ZWk= 17067
aW9uZW4= 17068
IGV4cGVyaW1lbnRhbA== 17069
IGRhdWdodGVycw== 17070
4Lib 17071
IG1lbnRhbGx5 17072
YmVjY2E= 17073
YXdhcmU= 17074
7ISd 17075
IHdoYXRzb2V2ZXI= 17076
IGVuYWJsZXM= 17077
IExvdw== 17078
b2lk 17079
4LiK 17080
w7Nk 17081
2Lo= 17082
IGNvbnN0cnVjdGVk 17083
IExhZGllcw== 17084
IGFjY3VzZWQ= 17085
INCw0L0= 17086
RGFu 17087
IHNwYXdu 17088
IGNvbnRhaW5lcnM= 17089
IGFydGlzdGlj 17090
xLFw 17091
IGRpc2Ns 17092
IGF1dHJlcw== 17093
aW5hcw== 17094
IE5hdGlvbg== 17095
IG5hZw== 17096
YmVhbg== 17097
d2hl 17098
nOuPhA== 17099
IFNlb3Vs 17100
IO2PrA== 17101
IE5pY2g= 17102
IGNvbXBsZW1lbnQ= 17103
IGludGVydmVu 17104
IE1vZGVs 17105
IE9yYW5nZQ== 17106
bmFtb24= 17107
IGNhbGN1bGF0aW9u 17108
c2Vl 17109
IHVzdGVkZXM= 17110
IGxlYg== 17111
IGRvY3Q= 17112
0ZbQvQ== 17113
IGZvc3Rlcg== 17114
IGVsYXN0aWM= 17115
IEFoaA== 17116
IGFjZQ== 17117
IFBpbms= 17118
IEplZw== 17119
IGRlZXI= 17120
44GX44GE 17121
c2lz 17122
IGpha28= 17123
IEVtbWE= 17124
0YHRgtCy0LXQvdC90L4= 17125
IHBvcnRyYWl0 17126
IG1ha2Vy 17127
IGF1bWVudA== 17128
0YDQvtCx 17129
IGFpcnBsYW5l 17130
IHRyYW5zcGFyZW5jeQ== 17131
IGFkanVzdG1lbnQ= 17132
IENEQw== 17133
w6dvbg== 17134
IHVwbG9hZGVk 17135
INC00LXQudGB0YLQsg== 17136
INCz0L7RgtC+0LI= 17137
IGl0ZXI= 17138
IGN1cnNl 17139
w7Ru 17140
bWVyY2U= 17141
YXJhbg== 17142
IGxlYWs= 17143
57WQ 17144
IGFic2VuY2U= 17145
0YHQutC40Lk= 17146
IHJlYWRlcnM= 17147
YWxlcg== 17148
IGJlbmVhdGg= 17149
YW5nbw== 17150
aGV0aWM= 17151
IGZpbm5z 17152
IHBvb3A= 17153
IGR1cGxpYw== 17154
SGk= 17155
aWdz 17156
b2xvZ2ljYWxseQ== 17157
b3Bw 17158
IGRpemVy 17159
IEFsbGVu 17160
IGdsaQ== 17161
IGFjY2VsZXJhdGlvbg== 17162
IHZpdGFtaW4= 17163
44Ot 17164
dsOk 17165
IEFjY2Vzcw== 17166
4K6Z 17167
csOhcw== 17168
IGFwcHJlY2lhdGVk 17169
IG5haA== 17170
IHBvc3Rlcg== 17171
IHRhbGU= 17172
IGhpZ2hsaWdodGVk 17173
5paH 17174
xbxlbGk= 17175
IGJsb2NrY2hhaW4= 17176
IG1pY3Jvdw== 17177
IGNpbmVtYQ== 17178
IENoYW5n 17179
IFNlYXJjaA== 17180
dXN0ZXJz 17181
IFplcm8= 17182
IERpdmlzaW9u 17183
0YDQsNGB 17184
IHNjYXJl 17185
IGplbGx5 17186
IEFkbWluaXN0cmF0aW9u 17187
U08= 17188
IGxpbmVk 17189
IOqwhA== 17190
IGdlYmVu 17191
IHNvZGE= 17192
IHdpbm5lcnM= 17193
s7w= 17194
2ZI= 17195
IEFtYg== 17196
5ZWP6aGM 17197
5ZQ= 17198
IHBlZw== 17199
5bex 17200
NDM= 17201
IHJhdXM= 17202
IHJld2FyZHM= 17203
IGluY2x1cw== 17204
IGhpZ2h3YXk= 17205
IGhhaA== 17206
IG11bHRpcGxpZWQ= 17207
IHPhur0= 17208
IGRpc2NpcGxlcw== 17209
IG5pbmc= 17210
IGRyZXNzaW5n 17211
IGF0dHJpYnV0ZXM= 17212
IE1vc2M= 17213
IEdyZWVjZQ== 17214
IHNlaw== 17215
IExlYXJu 17216
IGp1cw== 17217
cmVuZHJl 17218
IHBlcnNvbm5l 17219
cGxldGU= 17220
IHBsYWNpbmc= 17221
IGx1ZWdv 17222
aWxsYW5jZQ== 17223
INC+0LHRiQ== 17224
IHByb3Zpc2lvbg== 17225
IGxpb24= 17226
dHJh 17227
Ym9hcmRz 17228
IGJlaGF2aW91cg== 17229
aGV5 17230
IHN1YnNjcmlwdGlvbg== 17231
IHByb3RhZ29u 17232
44Oj 17233
IHZhcmE= 17234
IMWfdQ== 17235
IGhhaGE= 17236
IHRlYXNwb29u 17237
5p8= 17238
YXZvaXI= 17239
IGNyeXB0bw== 17240
INGB0YLQsNGA 17241
IFN0b3Jl 17242
YWJz 17243
IFN0dWRlbnRz 17244
IGxhdW5k 17245
aW50bw== 17246
IGFwcHJvYWNoZWQ= 17247
sJw= 17248
0YPRjtGJ 17249
IExhYm9y 17250
b3Rlcw== 17251
aWF0cmlj 17252
IGdyb8Of 17253
dXRpdmU= 17254
INC40LQ= 17255
IEdpYg== 17256
IHBsYWNlbWVudA== 17257
IGRpZsOtY2ls 17258
IGZyb2c= 17259
INCy0YHQtdGF 17260
IEpy 17261
YXplZA== 17262
0YPRiQ== 17263
IOq8 17264
ZnJhbWU= 17265
0LDQtdGI0Yw= 17266
IGxvY2tkb3du 17267
5ZGz 17268
IG1lZGk= 17269
INeU157X 17270
0LXQvdC40Lk= 17271
ZW1hbGU= 17272
7KKF 17273
YXRlcmFs 17274
IGRpc3RhbnQ= 17275
IGJlYXJz 17276
IGpvdXJuYWxpc3Q= 17277
6Kej 17278
IE1hcnNoYWxs 17279
IElobmVu 17280
dWV0b290aA== 17281
YmFn 17282
IMSRw6M= 17283
IEhpZ2huZXNz 17284
IOywjQ== 17285
0LjQutCw 17286
IFd1 17287
IEZyYW4= 17288
IHBlbmc= 17289
IGZvbg== 17290
IGh5cG90aGVzaXM= 17291
INGA0YM= 17292
IGx5 17293
15o= 17294
7JuU 17295
IFJhZGlv 17296
4Lie 17297
RGF2 17298
IGVtYmFycmFzc2luZw== 17299
IOyeiOyWtA== 17300
IGNhc3Rpbmc= 17301
IGNhZ2U= 17302
IFBzeWNo 17303
IOydvOuLqA== 17304
IMW+ 17305
aW1i 17306
IGRpcmVjdG9ycw== 17307
U0g= 17308
IM+EzrfOvQ== 17309
4buBdQ== 17310
IGtvbnXFnw== 17311
IG9wdGlvbmFs 17312
cXVhcnRlcnM= 17313
aWtlcg== 17314
IFNhbnQ= 17315
IHZlcnNlcw== 17316
67aA 17317
IG9sYXI= 17318
IM+H 17319
44OV 17320
IM6zzrnOsQ== 17321
IEltbQ== 17322
IGNvbnRyb3ZlcnNpYWw= 17323
IGVyc3Rlbg== 17324
IHJlY2lw 17325
IENocmlzdGlhbml0eQ== 17326
IOq0nA== 17327
b3Jkb24= 17328
15XXqQ== 17329
IHNsYXNo 17330
IFBm 17331
0YPQtNGM 17332
15XXnQ== 17333
IFBlcnJ5 17334
IG1hbXk= 17335
IGJhY2tncm91bmRz 17336
IOCujuCuqQ== 17337
IHBlbmRhbnQ= 17338
IENvbHVtYmlh 17339
IGludmVyc2U= 17340
INGH0LXRgNC10Lc= 17341
IHN2 17342
IGRpZ2dpbmc= 17343
NDE= 17344
Y2hlbQ== 17345
IG5hdmlnYXRpb24= 17346
IFNoaW4= 17347
IEZyb250 17348
UEQ= 17349
IGJlYXJpbmc= 17350
IFdhc3Nlcg== 17351
IHdheA== 17352
IENIUklT 17353
Y2hpbmc= 17354
IHByZXNzZWQ= 17355
RWw= 17356
IERhbA== 17357
b25zaW4= 17358
IGJpbmRpbmc= 17359
0YHQutC+0Lk= 17360
cG9vbnM= 17361
IG1vY2s= 17362
YXJlc3Q= 17363
0LrRgNCw 17364
TU0= 17365
IGNvcnJ1cHQ= 17366
c3Rvcm0= 17367
IHJlZnJlcw== 17368
IENvYWNo 17369
bGzDpA== 17370
IFRISVM= 17371
IHBhcmFn 17372
IOyTsA== 17373
cG9vbA== 17374
IGJpbGxpb25z 17375
IOq5gA== 17376
Z3JvdXA= 17377
IHdlbGNvbWluZw== 17378
Y2VsbGVuY2U= 17379
IER1a2U= 17380
6ri0 17381
IHByaW1lcmE= 17382
7KC4 17383
IHBvbmQ= 17384
IHN0YXR1ZQ== 17385
IOq1rOs= 17386
IGhhdGNo 17387
IGluc3RydW1lbnRhbA== 17388
IHJlc2lkZW50aWFs 17389
7Luk 17390
IGFjY2VwdGluZw== 17391
b3NoaQ== 17392
ZGF0ZQ== 17393
IOyUqA== 17394
IHBsYW50ZWQ= 17395
IGpva2luZw== 17396
IOyEnA== 17397
IGhhdGVk 17398
INGA0LDRgdGB0Lo= 17399
IHNsZXB0 17400
IHBhY2thZ2Vz 17401
IGlzbGFuZHM= 17402
ZXNlbg== 17403
xJ/EsQ== 17404
IGRpYWdvbg== 17405
IE9zYw== 17406
IG1lc2g= 17407
IHNjYWxlcw== 17408
YXJpdHk= 17409
IERlZmVuc2U= 17410
44Gh44KH 17411
IExld2lz 17412
INGB0LXQs9C+0LTQvdGP 17413
IGZsaWVz 17414
dWluZWx5 17415
IENvbnNpZGVy 17416
IHN0YXJr 17417
aGV3 17418
IEFzw60= 17419
s7Tr 17420
IHByb3Bvc2U= 17421
IO2VmOuptA== 17422
b2Rv 17423
IE5vcm1hbGx5 17424
IGhlZWZ0 17425
IEhhcnJpcw== 17426
Z3Jv 17427
IEJsb29k 17428
YmFzZQ== 17429
IGlPUw== 17430
IHRvdWNoZXM= 17431
IGluc3Bpcg== 17432
INeT 17433
IGJpbmFyeQ== 17434
IOy2lA== 17435
IHNlcmlhbA== 17436
IGlvbg== 17437
IHVuZW1wbG95bWVudA== 17438
IG9kZHM= 17439
IEZhYg== 17440
IEZCSQ== 17441
QlJVTg== 17442
IHdlaWdodHM= 17443
zr3Ovw== 17444
YXRpbGU= 17445
IG51cnNlcw== 17446
IGludm9sdmVtZW50 17447
IO2UvA== 17448
IGdvdmVybmFuY2U= 17449
IOKCrA== 17450
0YDRg9C/ 17451
aWVycmE= 17452
7ZiV 17453
IEplcnJ5 17454
IGJlYXJk 17455
IHNhbHZhdGlvbg== 17456
IEFsb25n 17457
Z2VudGxl 17458
IEtp 17459
Ym9s 17460
IFBsYXQ= 17461
IGhhc2h0 17462
6L+R 17463
IHdhcmU= 17464
IHBhcnRpZQ== 17465
eWN6 17466
IGludHI= 17467
Rmlo 17468
bmVudA== 17469
IGNoZWF0 17470
aWxlbg== 17471
IOuv 17472
b3JpZQ== 17473
IGbDoWNpbA== 17474
ZXRyaWM= 17475
IGFmZmVjdGluZw== 17476
dW5jaWF0aW9u 17477
IGFmZmFpcnM= 17478
IGJlZQ== 17479
IHZpZXdpbmc= 17480
IG9yYW5n 17481
IExhbg== 17482
INCh0YI= 17483
5LiW 17484
IE1lcw== 17485
g4E= 17486
ZXJpZQ== 17487
IGVzcGE= 17488
IGludGVycHJl 17489
IHBvc3Nlc3M= 17490
IHB1cmVseQ== 17491
cml0bw== 17492
Zm91bmQ= 17493
YXNtYQ== 17494
7KCB7J24 17495
IGV4YW1pbmU= 17496
INGD0Lw= 17497
IGJlc2No 17498
IFRvbW9ycm93 17499
IEJsb2Nr 17500
IHZhcmlhbnQ= 17501
IHByZWZlcmVuY2U= 17502
IGNvYWNoZXM= 17503
IG1lZGljYXRpb25z 17504
IO2YhA== 17505
IGVtcGlyZQ== 17506
64Sk 17507
IElsbGlub2lz 17508
IGNyaXNweQ== 17509
IHRow6w= 17510
IGJlZXM= 17511
Nzc= 17512
IGdsb3c= 17513
6Lo= 17514
IFN0dWRpZXM= 17515
5ZCE 17516
IENoYWxsZW5nZQ== 17517
IHVubGlrZWx5 17518
0Kc= 17519
xLF5b3JzdW4= 17520
RElF 17521
IG1pbmltaXpl 17522
aXphcmQ= 17523
IMO6bg== 17524
IGVuY29udHJhcg== 17525
IEtpbGw= 17526
5bs= 17527
IHZhbmlsbGE= 17528
IEdyYW50 17529
IEdU 17530
c2Vh 17531
IHNvdWdodA== 17532
0LLQvtC0 17533
IG7DpG0= 17534
IEF1bnQ= 17535
T1dO 17536
IHB1bXBraW4= 17537
c3RlbGxlbg== 17538
IHJhZw== 17539
0LXQs9C00LA= 17540
IHN0b3J5dA== 17541
IGZvcnVt 17542
5qmf 17543
IGVzdGFiYQ== 17544
dWNoZQ== 17545
IGNvbmdyZXNz 17546
IFJleQ== 17547
IGRyYW1hdGljYWxseQ== 17548
IFNwb3J0 17549
IFllbGxvdw== 17550
IOqzhOyGjQ== 17551
IGRpc2d1c3Rpbmc= 17552
IFJlY2VudA== 17553
IGFjcXVpcmVk 17554
IGNhYmxlcw== 17555
55Sa 17556
ZGlu 17557
IHZpc3Rv 17558
IGNvbW11bmljYXRpbmc= 17559
0YHRgtCw0LLQu9GP 17560
0LXRgdGC0L4= 17561
44O744O744O7 17562
IHLDqWc= 17563
IHNvY2tz 17564
IHByb2Nlcw== 17565
YmVjYXVzZQ== 17566
IHV0dGVy 17567
IGNvbG9jYXI= 17568
IG5ld2VzdA== 17569
IGdyYW1t 17570
6KGo 17571
5LiN55+l6YGT 17572
IHNoaWZ0aW5n 17573
IGNhcnJpZXI= 17574
INGB0LrQvtGA 17575
IFNjaHc= 17576
IGV4ZWN1dGVk 17577
IG1haW50YWluZWQ= 17578
IM+G 17579
IE1vc2Vz 17580
IGRpc3Nl 17581
IGhvcnI= 17582
44Cc 17583
IHJhbGx5 17584
IGFsbGVt 17585
IEV2ZW50dWFsbHk= 17586
IGRpeW9y 17587
bHZhbmlh 17588
IHNjaG5lbGw= 17589
IOqzvA== 17590
IOunpA== 17591
IHN0cnVnZ2xlcw== 17592
bGF0ZQ== 17593
IGNsYXJpZnk= 17594
w6ltZW50 17595
IG11bHRpcGxpYw== 17596
0LjQsdC+ 17597
IGpvdXJu 17598
IGZyYWdy 17599
IHN1cnByaXNpbmdseQ== 17600
IGRlc3BlcmF0ZQ== 17601
NTI= 17602
IHN1bA== 17603
IFJlYWQ= 17604
IEZyaWVk 17605
IG1vbmQ= 17606
d29v 17607
IG9yZ2FuaXppbmc= 17608
44GX44KH44GG 17609
IFNvb24= 17610
INCy0L7Qv9GA0L7RgQ== 17611
IE51cg== 17612
INCX0LQ= 17613
IHNwaWRlcg== 17614
0LXRgdGP 17615
IHR1dG9yaWFscw== 17616
IG51dHJpZW50cw== 17617
b3Jlcg== 17618
IGNvZWZmaWNpZW50 17619
IGFycmFuZ2VtZW50 17620
IHByaWNpbmc= 17621
bmFu 17622
eXU= 17623
Qkw= 17624
IHRyaWJl 17625
IEhvd2FyZA== 17626
dW5rcw== 17627
IG5ld2Vy 17628
IHByb3Zpbg== 17629
IHByZWRpY3Rpb24= 17630
aG9z 17631
IG9sc3Vu 17632
IEFyb3VuZA== 17633
IHZpZXI= 17634
INGB0YLQvtGA0L7QvQ== 17635
IHZhbGxleQ== 17636
IEVsYQ== 17637
aWZp 17638
IGdhbGF4eQ== 17639
IHRyYW5xdQ== 17640
IGFkdmVycw== 17641
IFRlbXBsZQ== 17642
aWZmcw== 17643
aWdlbmNl 17644
6Ieq5bex 17645
IGvDtm5udGU= 17646
IMSRw7M= 17647
RGlk 17648
IHBob3RvZ3JhcGhz 17649
IEFXUw== 17650
0YbQuNGP 17651
IGd1YXJkcw== 17652
IGFwcG9pbnRlZA== 17653
IEdpbA== 17654
INC80L7QvA== 17655
IGNvZA== 17656
IFVubGlrZQ== 17657
IGV2ZW5seQ== 17658
aXNjb25zaW4= 17659
IGVzdG91 17660
IG1uaWU= 17661
IEV4ZWM= 17662
IE1W 17663
IEVpbmU= 17664
5L+h 17665
IFJvZ2Vy 17666
IEZhYw== 17667
IExpc3Q= 17668
IGZ1ZXI= 17669
0LDQtdGC0LU= 17670
b21lZA== 17671
IGF0dHJhY3Rpb24= 17672
6Imy 17673
IHRlcnJhaW4= 17674
IERyb3A= 17675
IGNvcnBvcmF0aW9ucw== 17676
IHNjaWVuY2Vz 17677
IHRocm9uZQ== 17678
44GE44Gf 17679
IGFq 17680
IFJvdA== 17681
54m5 17682
IHN1cHBvcnRlcnM= 17683
IEJlcmU= 17684
SGVyZQ== 17685
IGRpZmVyZW50ZXM= 17686
IHNpZ25pZmljYW5jZQ== 17687
z4POtw== 17688
5oiR6Ka65b6X 17689
IGNsYW1w 17690
IOuMgOs= 17691
IGZhYnVsb3Vz 17692
cmV6 17693
5oyB 17694
IGFzc3VtcHRpb25z 17695
dXRoZXI= 17696
d2lk 17697
cG90 17698
6L+O 17699
IHlhbg== 17700
dWxpbg== 17701
0YDRi9Cy 17702
IFNsb3c= 17703
IFBlbm5zeQ== 17704
IO2VtOyEnA== 17705
IG1laW8= 17706
IHdlYWx0aHk= 17707
IEVpZ2h0 17708
IHB1bHNl 17709
IGZyaWN0aW9u 17710
aWRpdHk= 17711
IEhvbGw= 17712
aXlvcnVt 17713
IHNvdW5kZWQ= 17714
IENhcnI= 17715
IGZvcms= 17716
4pg= 17717
IFBB 17718
IGNvbnNwaXI= 17719
IGNvZGluZw== 17720
cnQ= 17721
IFR5cA== 17722
IOyWkQ== 17723
INC/0L7Qsw== 17724
IG1pc2Vy 17725
INGB0LzQvtGC0YA= 17726
IFN3ZWRlbg== 17727
IG9sYXJhaw== 17728
IFpoYW5n 17729
IENoaQ== 17730
IFRpdGFu 17731
IHNjcmVlbmluZw== 17732
IFNwaWRlcg== 17733
IMWeaW1kaQ== 17734
IG9ic3RhY2xlcw== 17735
bGFyYQ== 17736
IGNoYWxsZW5nZWQ= 17737
cHNl 17738
VE9O 17739
4bul 17740
IFBp 17741
IGxhZ2k= 17742
aWV1cnM= 17743
IGh1cnRpbmc= 17744
IG5lZ2xlY3Q= 17745
IGdlbmVyYXRpbmc= 17746
IHlvdW5nZXN0 17747
IGF1ZGl0 17748
INGA0LXQtw== 17749
z4HOrA== 17750
IGRvbmF0ZQ== 17751
IFBERg== 17752
IHZpc2l0cw== 17753
IGNydWlzZQ== 17754
UFA= 17755
YXNlcg== 17756
IHdzcA== 17757
YmFja3M= 17758
aXZhbHM= 17759
44GG44KT 17760
IGRldmU= 17761
IHByb3BvcnQ= 17762
IGNhdGg= 17763
IEVmZmVjdA== 17764
IHdpbmRz 17765
IOyZlA== 17766
IGNoYXJ0cw== 17767
IHNhbWE= 17768
IGF1dG9tYXRpb24= 17769
INC/0L7QutCw 17770
IG9sYW4= 17771
IGJvYXRz 17772
IGNhZmU= 17773
IGRlbmllZA== 17774
IE1hbWE= 17775
IGJsb2NraW5n 17776
IFRob3I= 17777
IHBoZW5vbWVuYWw= 17778
IHN0YWtlaG9sZGVycw== 17779
IHVub3M= 17780
0YPQtdGC 17781
IEFicmFoYW0= 17782
44Gn44KC 17783
IGRldGVjdGlvbg== 17784
IGp1cmlz 17785
IHBvd2VyZWQ= 17786
emlhbA== 17787
IHdlbGZhcmU= 17788
IHVwZ3JhZA== 17789
IG1vxbxuYQ== 17790
IENhc2U= 17791
Y3VsYXI= 17792
lOydtA== 17793
44OB 17794
IEd1ZXNz 17795
IGN5Y2xlcw== 17796
5L6L 17797
57Wm 17798
cm9jaw== 17799
dW1p 17800
IGVsaXRl 17801
IHF1w6g= 17802
5aCx 17803
0YLQvtC8 17804
IHNob3Jl 17805
Z3VudGE= 17806
IGt1 17807
IGZhaXRoZnVs 17808
IEplcmVteQ== 17809
YWlk 17810
4Lc= 17811
dWdhbA== 17812
5bCN5ZWK 17813
IFZlbA== 17814
IHZyYWk= 17815
c3RlbGw= 17816
qLg= 17817
IGtvbA== 17818
6L0= 17819
IHF1YW50bw== 17820
INC30LDRgA== 17821
IDIwMDI= 17822
ZXN5 17823
IHJlc2VydmU= 17824
INC80L7QvNC10L3Rgg== 17825
IGRlcGxveWVk 17826
IGRlZmluaW5n 17827
IHNhdQ== 17828
IGdhYXQ= 17829
Iik= 17830
IHRyYW5zbWl0 17831
IHB1Ymxpc2hpbmc= 17832
IHJhbmtpbmc= 17833
IG9mZmVuc2U= 17834
IDQ2 17835
cGlu 17836
IFRha2luZw== 17837
IGVudGl0bGVk 17838
IGdlbnVpbmVseQ== 17839
IHZhcmlhdGlvbnM= 17840
IGZpbmRl 17841
IHRhdQ== 17842
IHVuZm9ydHVuYXRl 17843
IFJhaA== 17844
cG9ydHM= 17845
IGPF 17846
IG1vbmtleQ== 17847
IGJyYWM= 17848
d2Vp 17849
bHVuZw== 17850
IGFydGlm 17851
IHN5cnVw 17852
INCU0LDQsg== 17853
IGxpZnRlZA== 17854
IGNoZXo= 17855
IEFkdmVudA== 17856
IFN0b2Nr 17857
IGRvbA== 17858
0LzQtdC9 17859
0LjRiNGM 17860
IHlu 17861
Z2lv 17862
ZGV0 17863
IGRlc3Nl 17864
IGdyaQ== 17865
IENoYWlybWFu 17866
54U= 17867
IGN1ZW50YQ== 17868
YW5pbQ== 17869
IGNyYWI= 17870
IGVzY2Fs 17871
IHByZW1pw6hyZQ== 17872
IEdlZg== 17873
IGRpbmluZw== 17874
IHNldmVudGg= 17875
IGNoYXNpbmc= 17876
IFRvd2Vy 17877
IGJydXRhbA== 17878
IGZ1bmRhbWVudGFsbHk= 17879
44Go44GG 17880
0LvQtdC90LjRjw== 17881
c3RhZ2U= 17882
IGFjcXVpcw== 17883
IGN5bGluZGVy 17884
IGNvbW1hbmRlcg== 17885
bWVt 17886
IFVW 17887
aGFwcHk= 17888
IGVwc2lsb24= 17889
IGludml0YXRpb24= 17890
IGZhcm1lcg== 17891
Y2hhaXI= 17892
IGRlc3Rpbnk= 17893
IHNvdmVyZQ== 17894
IEhlYnJldw== 17895
IHNlcnZhbnQ= 17896
IGJldw== 17897
IGdhc3Q= 17898
dXRpZXM= 17899
IGFkbWluaXN0cmF0aXZl 17900
IENvbW1hbmQ= 17901
w6l0YQ== 17902
IG5pdHJvZ2Vu 17903
6re8 17904
IGFiaQ== 17905
IHZpbGxhaW4= 17906
IGJsYW5rZXQ= 17907
IFNlbmQ= 17908
IGJlYXRlbg== 17909
soQ= 17910
IHZvbHVudA== 17911
IHNjaG9sYXI= 17912
IEVtcGVyb3I= 17913
IDQz 17914
dmFibGU= 17915
IER1cw== 17916
IEdV 17917
IHRhcmdldGluZw== 17918
d3d3 17919
IGFtZW5kbWVudA== 17920
7IaM6w== 17921
IHRpbmc= 17922
IG5hc3R5 17923
IGdhdWdl 17924
INGA0L7QtA== 17925
IEhhbnM= 17926
WW91cg== 17927
zrHOvQ== 17928
IHByb2pldA== 17929
IEhhd2FpaQ== 17930
IHN1c3BpY2lvdXM= 17931
IHNjaHc= 17932
IHJlbW92YWw= 17933
IGludHJpZw== 17934
IE1V 17935
IHBvbnRv 17936
4KS+ 17937
INC+0LHRgNCw0Lc= 17938
IGd1ZXNzaW5n 17939
cGFjZQ== 17940
IG1vdGhlcnM= 17941
IG1pbGxpbWV0ZXI= 17942
0LvQtdC90LjQtQ== 17943
5rKh5pyJ 17944
IGF2YWlsYWJpbGl0eQ== 17945
aWN6 17946
5q2k 17947
IGZyYWN0 17948
IGJhc2Vz 17949
a20= 17950
IEJUUw== 17951
IEZpZWxk 17952
IGR6aWU= 17953
IHNlZ3VuZG8= 17954
IOuCmOuKlA== 17955
IGxlZ2l0aW1hdGU= 17956
aW1hcw== 17957
INCy0L0= 17958
IGNvcnJ1cHRpb24= 17959
IHNtYXNo 17960
IFZhbGVudA== 17961
IGFsaWduZWQ= 17962
IFBlbm5zeWx2YW5pYQ== 17963
IGdhYg== 17964
IEV1bg== 17965
ZW50aA== 17966
IE1vcm5pbmc= 17967
IGNhbmRsZQ== 17968
IGJhY2twYWNr 17969
IElzbGFtaWM= 17970
YcOnw7Vlcw== 17971
IGVuY3J5 17972
IG11c2hyb29tcw== 17973
7YyM 17974
ZGl0 17975
IHRyYW5zaXQ= 17976
IFdpc2NvbnNpbg== 17977
IHBhcnRpY2lwYXRlZA== 17978
IElscw== 17979
IHVuZm9sZA== 17980
toDr 17981
IHByb2ZpdHM= 17982
IHdhcm1pbmc= 17983
IEdhbmc= 17984
IG5ldHdvcmtpbmc= 17985
IG1lZ2E= 17986
IHRob3JvdWdobHk= 17987
bGVtZW50cw== 17988
IEht 17989
IGRlY2lkaW5n 17990
IGVtb3Rpb25hbGx5 17991
IGV4aGF1c3RlZA== 17992
INCf0L7Rgg== 17993
Y2lkbw== 17994
IEhUTUw= 17995
IGNvcHlyaWdodA== 17996
IG1lbG9keQ== 17997
eWlt 17998
IGFuZGVycw== 17999
b3Nob3A= 18000
IOuzvA== 18001
IGF0aGxldGU= 18002
IEdF 18003
IGZyZXF1ZW50 18004
IGRlc2lyZXM= 18005
IG5lZWRpbmc= 18006
IFl1bg== 18007
IHJpZmxl 18008
IGxvdmVy 18009
J1Q= 18010
IGRlbnNl 18011
IHTDo28= 18012
IG5vdGlmaWVk 18013
IGlkaQ== 18014
7Jet 18015
7YY= 18016
IGludGVyYWN0aW5n 18017
IHJhcHBvcnQ= 18018
0LXRgNC4 18019
c2tp 18020
IGJlc3Nlcg== 18021
IG1hbnVmYWN0dXJlcg== 18022
IEt5bGU= 18023
IGFjY291bnRhYmxl 18024
IFNhaw== 18025
IFBpbA== 18026
IERvbWlu 18027
IHByZXN1bQ== 18028
INCS0YHQtQ== 18029
IHZpbmVnYXI= 18030
IGd1YXJhbnRlZWQ= 18031
55yL5Yiw 18032
IGhhbmRsZWQ= 18033
6Z+z 18034
Y2F0 18035
IGNpdmlsaXphdGlvbg== 18036
IGFjY29tcA== 18037
IFZN 18038
w6ltb24= 18039
IGRlemU= 18040
IGdyYWRlcw== 18041
IHNvbGx0ZQ== 18042
IHN0YXJpbmc= 18043
15DXqg== 18044
YXJudA== 18045
IGhvcml6b24= 18046
IHRyYXZhaWw= 18047
aG91cg== 18048
56ys5LiA 18049
IEVE 18050
IERhaw== 18051
IG55 18052
IGNvbnZl 18053
IENoYW0= 18054
IGZpcm1z 18055
IExpdQ== 18056
INGB0YLRgNCw0L0= 18057
IGxpYmVydA== 18058
IGxlbnNlcw== 18059
IGludGFrZQ== 18060
INCy0YvQsQ== 18061
IG1lbnNlbg== 18062
aGVs 18063
IHByYWN0aXRpb24= 18064
IDM1MA== 18065
44Kz 18066
Rk8= 18067
IGJlZHM= 18068
IGFuY2VzdG9ycw== 18069
IOyXhOyyrQ== 18070
IGRpc3R1cmI= 18071
IExhc3RseQ== 18072
IFN1cHBvcnQ= 18073
4Li14LmJ 18074
IENvcm9uYQ== 18075
IGVudGh1c2k= 18076
INCy0L7Qt9C8 18077
IOyCrOuejOs= 18078
IDUy 18079
YmlyZA== 18080
IHJlZHVjZXM= 18081
IOyeiOydhA== 18082
IEdlbmU= 18083
6rWQ 18084
xJlw 18085
IMOcYmVy 18086
IGNvbmNlcm5pbmc= 18087
dXNlcg== 18088
IGNvbmNlbnRyYXRl 18089
IFdIQVQ= 18090
aXNob3A= 18091
b255bW91cw== 18092
bm9sZA== 18093
IHN1Z2dlc3Rpbmc= 18094
qbA= 18095
IEZpc2g= 18096
Li4uLi4uLi4= 18097
IHZlc3NlbA== 18098
IHRyYWJham8= 18099
44G1 18100
IE9jZWFu 18101
5aeQ 18102
eWc= 18103
IHRvd25z 18104
ZGVs 18105
IHRlcnJpZnlpbmc= 18106
IMOnYWzEscWf 18107
IHNpbm8= 18108
IGVhdHM= 18109
IGdleg== 18110
IGdlbWU= 18111
IOyZhA== 18112
IGNvbXBhcnQ= 18113
IGltcGxlbWVudGluZw== 18114
IFBvdHRlcg== 18115
IEdlcm1hbnM= 18116
IGfFgg== 18117
IHRlbm5pcw== 18118
IGNhcnBldA== 18119
YXVlcg== 18120
IFNhdWRp 18121
eWVvbmc= 18122
IGN1cnJ5 18123
IEZvcmVzdA== 18124
0YvQuw== 18125
IGZpZnRlZW4= 18126
IGJvbHRz 18127
IHtc 18128
rLQ= 18129
IHNldHRsZW1lbnQ= 18130
IGxhbmdl 18131
IGJhbQ== 18132
R2V0 18133
7ZWZ 18134
IHN3YXA= 18135
IEtoYW4= 18136
IGNvbW1lbmNl 18137
IHF1YXJhbnRpbmU= 18138
IHNjb3JlZA== 18139
55Y= 18140
IDE5NTA= 18141
IHRoaWNrZXI= 18142
IHPDu3I= 18143
5Y+j 18144
IExhcnJ5 18145
IGFsbGV6 18146
7Iuc64qU 18147
IGfDvA== 18148
IHNwZWN0YWN1bGFy 18149
Ly8= 18150
Ym90aA== 18151
IHN0YXRz 18152
5aaz 18153
IE5hbmN5 18154
IGJ1bnU= 18155
IGNydXN0 18156
IGFjdGl2YXRlZA== 18157
IOq3uOue 18158
b3V0aGU= 18159
IHBvcnRz 18160
IG5ldXJhbA== 18161
IGphdw== 18162
IG9ic2VydmF0aW9ucw== 18163
IHZvaXQ= 18164
YWJhbg== 18165
4bqjaQ== 18166
pqzrpbw= 18167
b21lcw== 18168
4K+L 18169
cXVp 18170
IGtpbmRuZXNz 18171
0JE= 18172
IDQx 18173
IG1vZGVyYXRl 18174
IGFuZ2Vscw== 18175
IFRhbWI= 18176
w6h0 18177
IGNobG9y 18178
IEJpbGx5 18179
7LKY6w== 18180
YWNvbg== 18181
IHNlbGVjdGluZw== 18182
IERlbHRh 18183
IG51bGw= 18184
ZGVubHk= 18185
IGNpdWQ= 18186
IHRlbmRlbmN5 18187
IGJyZWFrZG93bg== 18188
IG1pbnQ= 18189
0YTQvtGA0Lw= 18190
b3JwaA== 18191
IGRhd24= 18192
c3By 18193
IFdJTEw= 18194
w6RjaGxpY2g= 18195
IHB1cHB5 18196
NzAw 18197
IOCupA== 18198
IGZhaWxz 18199
IENvbmM= 18200
IHJlbGF0aXZlcw== 18201
IGludml0aW5n 18202
IGF1dG9ub20= 18203
IGNvbXBvc2Vk 18204
IHVuaXR5 18205
IGRlY2lz 18206
IGFjY2Vzc29yaWVz 18207
IENhc3M= 18208
IGJpc3Q= 18209
IFRpcA== 18210
7Ke4 18211
IHB1bnQ= 18212
IHLDoXA= 18213
6YCy 18214
QU5L 18215
44Ga 18216
ZXhpc3Q= 18217
IGNvbXBhdGlibGU= 18218
IG5lcg== 18219
INC10LzRgw== 18220
IGFwbGlj 18221
IGJhcHQ= 18222
IGZhaWxpbmc= 18223
IFRhbWFt 18224
IG9zY2lsbA== 18225
IGxldHp0ZW4= 18226
IHJlcGVhdGVkbHk= 18227
IGp1bmdsZQ== 18228
IFB1c2g= 18229
aGFp 18230
IM63 18231
IGRlYWRseQ== 18232
0Y/Qtg== 18233
d2nEhQ== 18234
IENvbW1vbg== 18235
IM6V 18236
IHNrYXRl 18237
VEM= 18238
IE1pbmk= 18239
IGhvYmJ5 18240
4bqnbg== 18241
IHJvdXRlcw== 18242
IGFtaWdvcw== 18243
IGNvbmp1bg== 18244
IHBhcnRuZXJzaGlwcw== 18245
IG5vdm8= 18246
IGF2ZXI= 18247
IHBvdXZleg== 18248
YnJpZGdl 18249
IHByZW9j 18250
aGlt 18251
IHR1cmI= 18252
IHNvYg== 18253
IFNuYXA= 18254
IOywuA== 18255
bWludXRl 18256
IHRyYWplY3Q= 18257
dWrEmQ== 18258
IGVhZ2Vy 18259
IHJlZ3VsYXRvcnk= 18260
IGJhbmtpbmc= 18261
Ymxpbmc= 18262
0YjRjA== 18263
YcW8 18264
IGJpemFycmU= 18265
aXRhdGVk 18266
ZGlyZQ== 18267
IHRocmVhdGVuZWQ= 18268
IHNoaW5pbmc= 18269
IG5lc3Nl 18270
IGNvcnBz 18271
INGB0YM= 18272
IHRlbGVz 18273
IHRlbXA= 18274
dGVt 18275
INC60LDQvQ== 18276
IGZldmVy 18277
TmV3 18278
IGhlYXZpZXI= 18279
IFNhaA== 18280
YnVk 18281
IG91dHJvcw== 18282
IOywvg== 18283
IOuqhQ== 18284
YXJyaW5n 18285
IOq0nOywrg== 18286
IE5hcA== 18287
IHNlbWlu 18288
IFRoYW4= 18289
aWZz 18290
IGRlc2Vu 18291
INGC0LDQutC+0LU= 18292
IGxvc2Vz 18293
IEJhbHQ= 18294
a29u 18295
INC90LDQv9GA 18296
IHZvaXM= 18297
IE1vc2Nvdw== 18298
IGNoYWlycw== 18299
aGlz 18300
IHJlZnVnZWVz 18301
a2c= 18302
IGtvbGU= 18303
jag= 18304
0LDRgdC40LHQvg== 18305
pr0= 18306
IFVuaXZlcnNl 18307
IERpcmVjdA== 18308
IGNoZWF0aW5n 18309
IENpbg== 18310
IHBhdHJp 18311
IGFkdmlzZQ== 18312
IE5ldGhlcg== 18313
IHByaW1laXJv 18314
IG1lbnRpb25pbmc= 18315
bnV0 18316
NTY= 18317
YXLEsQ== 18318
IHBldGl0ZQ== 18319
YmxlZA== 18320
IHBlbnNhcg== 18321
aWNpbw== 18322
SU5E 18323
IHZldGVyYW4= 18324
IGxhZGRlcg== 18325
IGNvbnNlcXVlbmNl 18326
0L7QttCw0Ls= 18327
IEJ1cm4= 18328
IHJ1Zw== 18329
IE1hZGU= 18330
IGdpdA== 18331
Ii4uLg== 18332
IGNvbXBldGl0b3Jz 18333
IHByemVk 18334
IGFwcGFyZW50 18335
IEFyZ2VudGluYQ== 18336
IFdvcmtpbmc= 18337
IGNvbGxhYm9yYXRl 18338
d29tYW4= 18339
IHJldGFpbg== 18340
IGxldXJz 18341
IGRhc2hib2FyZA== 18342
15nXkw== 18343
IEVhcmx5 18344
Qk0= 18345
INC10ZE= 18346
0L7Qu9C+0LM= 18347
IHNhdGlzZnlpbmc= 18348
IG9mdGVudGltZXM= 18349
IG1hcHBpbmc= 18350
w7xua8O8 18351
YXJ0aA== 18352
Zm9sZA== 18353
IGxhdW5jaGluZw== 18354
IGF1cmE= 18355
IHByZWNpc2lvbg== 18356
d29ya3M= 18357
R29k 18358
IHN0cmFw 18359
IEltcGVy 18360
IHJpdmVycw== 18361
IHw= 18362
IGN1ZXI= 18363
cmVnb24= 18364
IGFycml2YWw= 18365
0LrQsNGF 18366
IE1pYW1p 18367
0LDQvdGL 18368
IHN1cnZpdm9ycw== 18369
IFNlbmlvcg== 18370
RGF2aWQ= 18371
IGVzdGFkbw== 18372
IHNlY3RvcnM= 18373
IHBvcHBpbmc= 18374
IGNoaW0= 18375
YXnEsQ== 18376
IGt1bm5lbg== 18377
IGdhbGxlcnk= 18378
IHN1bmxpZ2h0 18379
ZXNlaGVu 18380
IHllbGxpbmc= 18381
IE1laW4= 18382
IFBob2VuaXg= 18383
IG1hbm8= 18384
IGhpc3Rvcmlh 18385
IG9jY3VycmluZw== 18386
5qy4 18387
7Lg= 18388
0LDQtNC4 18389
5b6F 18390
IGluc3RpdHV0aW9uYWw= 18391
IFR1dA== 18392
57I= 18393
IHNsYXZlcw== 18394
44Gp44GG 18395
IGZvcmdpdmVuZXNz 18396
IHR3aW4= 18397
IEh5dW4= 18398
0L3RjA== 18399
IEtvbW0= 18400
YW5kcmE= 18401
c2hvdA== 18402
c3PDpA== 18403
INGG0LU= 18404
YXR0YQ== 18405
IGV4cGVuc2U= 18406
IEdQVQ== 18407
IFBhc3Q= 18408
cmlibHk= 18409
IOutkOyVvA== 18410
INCz0L7QtNCw 18411
IHJlc3Bpcg== 18412
5p2x 18413
IFF1ZWVucw== 18414
aG9wcw== 18415
IHPDqXJpZQ== 18416
IHByZWY= 18417
IGNvbWVk 18418
IHBsdXQ= 18419
IE92ZXJhbGw= 18420
IOOBnQ== 18421
IGN1c2g= 18422
IHJpbmdpbmc= 18423
IGluY29ycmVjdA== 18424
INGB0YLRgA== 18425
IGdlb21ldHJ5 18426
IGFkdmVydGlz 18427
INCo 18428
IHJldmlld2Vk 18429
44GC44GC 18430
IGRvemVucw== 18431
IGRldGVybWluYXRpb24= 18432
IFBoaWxs 18433
IGNvbnRyaWJ1dGVk 18434
IENpdA== 18435
IHBhc3NlbmdlcnM= 18436
IGPDtHTDqQ== 18437
IHJldmVy 18438
IHRlY2hub2xvZ2ljYWw= 18439
IGFsbGVu 18440
IHJhaW5pbmc= 18441
YXZp 18442
IHNhbHR5 18443
IHR5cGluZw== 18444
INGC0LU= 18445
IHRpbHQ= 18446
IOy5mA== 18447
INC+0YA= 18448
INC/0YDRj9C8 18449
IHJvdQ== 18450
IGFyZW5h 18451
YXJhdA== 18452
5Yir 18453
SEhISA== 18454
IG1hbnVmYWN0dXJlcnM= 18455
IEVkd2FyZA== 18456
IHR1Y2s= 18457
IGJsb3dz 18458
aW5nbw== 18459
IE1hcmM= 18460
7JWE7ISc 18461
TWljaA== 18462
IENsZWFu 18463
6LQ= 18464
ZXN0bw== 18465
IFBhY2s= 18466
IHNoYWZ0 18467
QlJVTk8= 18468
IGF2ZW4= 18469
dXVy 18470
0YHQutC+0LvRjNC60L4= 18471
6rSA 18472
IGF1dG9tYXRlZA== 18473
IHZlbnR1cmU= 18474
IHN1cnZlaWxsYW5jZQ== 18475
IEdyb3c= 18476
IEVtZXI= 18477
INC00L7RgA== 18478
IGludmVzdG9y 18479
IFlvaw== 18480
IGxhdHRlcg== 18481
IE5J 18482
IGZ1bmN0aW9uaW5n 18483
IEhhbWlsdG9u 18484
IDUx 18485
IG11cmRlcmVk 18486
IGFuY2hvcg== 18487
IGN1Yw== 18488
IFNDUA== 18489
IE1hZGFt 18490
IGNvbnN0cmFpbnRz 18491
IGJhcm4= 18492
YW5rZW4= 18493
IOunjuydgA== 18494
IE1vdG9y 18495
IERvaW5n 18496
IGFtZW4= 18497
ZXR0cw== 18498
IGluc3RydWN0b3I= 18499
ZWd0 18500
YWtv 18501
IHBvc3R1cmU= 18502
aXZpYQ== 18503
IFBvbGlzaA== 18504
INC00LLQsA== 18505
IGNvbG9yZnVs 18506
IGVsYm93 18507
IHBhcmxl 18508
IHBhc3Nlcg== 18509
IGNvbmRlbQ== 18510
b3J0YWw= 18511
IGZlcnRpbA== 18512
2KfYrw== 18513
IENvbG9tYg== 18514
IGFsaWdubWVudA== 18515
IGFzdHJvbmF1dA== 18516
IE11dA== 18517
IHNhbG1vbg== 18518
IHN0cnVjdHVyZWQ= 18519
nteo 18520
IGNsaWNrcw== 18521
IG1pZWo= 18522
5pS/ 18523
44GE44KE 18524
IFJvdW5k 18525
IHJhaW5ib3c= 18526
IFZB 18527
44GU44GW 18528
7KeI 18529
b3R6 18530
LDwv 18531
IE5pY29sZQ== 18532
bGlzaGluZw== 18533
IHdoaWxzdA== 18534
IHJlcHVibGlj 18535
IHRhbWFt 18536
dmVydGVk 18537
IHJlY29nbml6aW5n 18538
INCz0LvQsNCy 18539
IGR1Yg== 18540
IEpvcw== 18541
ZmFsbHM= 18542
aWNoaQ== 18543
IGN6xJk= 18544
INCm 18545
IE1pdGNo 18546
Q1I= 18547
Y2xpY2s= 18548
44GE44Gm 18549
IHN0dW5uaW5n 18550
IEp1bGlh 18551
bWVycw== 18552
IFBvbHk= 18553
IGRlc3Nh 18554
IGludMOp 18555
IOqzoOs= 18556
IGRvxJ8= 18557
IGRpdmVy 18558
IHN0cmlraW5n 18559
YXBob3I= 18560
IGFwZW5hcw== 18561
b3VzZXM= 18562
IHRyYWdlZHk= 18563
IEZhbg== 18564
IFR1cmtpc2g= 18565
IHByb3BoZXQ= 18566
IGRpc3RhbmNpbmc= 18567
IEhlbQ== 18568
IGNhcnRvb24= 18569
S2U= 18570
YW50aW5n 18571
IENsYXJr 18572
578= 18573
IGRhdm9u 18574
IO2F 18575
IHl1bW15 18576
IGNvbXByb21pc2U= 18577
IHN0YXJ0dXA= 18578
cml0dA== 18579
IGNlcnRpZmllZA== 18580
IHBpbGxvdw== 18581
YmVyZQ== 18582
7KSA 18583
IHNlZ3Vpcg== 18584
IHN0YWRpdW0= 18585
YXRpdm8= 18586
IHNpbXBsZXI= 18587
s7g= 18588
IHZpc2E= 18589
IHBhdGh3YXk= 18590
IG51ZXZv 18591
IHJheQ== 18592
44OQ 18593
6Zw= 18594
w7bDnw== 18595
INC30LDQvQ== 18596
IGNlbGVicml0eQ== 18597
0LfQsA== 18598
IGVpbmVz 18599
IEdpdmVu 18600
IEFyYQ== 18601
IEpvYg== 18602
IHlhaw== 18603
IEFyYmVpdA== 18604
cmVzc2luZw== 18605
w6FuZA== 18606
IGdyYWJiZWQ= 18607
cGVuZA== 18608
IHNpbmU= 18609
aXJr 18610
INCe0YI= 18611
IEZsZQ== 18612
aWNoZW4= 18613
56Y= 18614
IE5laWw= 18615
6Jmf 18616
IHJlcGVhdGluZw== 18617
IGRyYXdpbmdz 18618
cmlzZQ== 18619
IGdsaXR0ZXI= 18620
Zml2ZQ== 18621
IHN1cnQ= 18622
IHNpY2hlcg== 18623
IGFkanVzdG1lbnRz 18624
IOmCow== 18625
aXBwaQ== 18626
Y2tl 18627
IHJlcHJlc2VudGF0aXZlcw== 18628
IG1pZHN0 18629
IHNwb2ls 18630
bWV5ZQ== 18631
IHRhZ3M= 18632
IHllcA== 18633
IFN0ZXBoYW5pZQ== 18634
IGdlcmU= 18635
IFJ1ZA== 18636
54s= 18637
IGdyb3M= 18638
IHF1ZXVl 18639
IGFjY29yZA== 18640
IG9yZ2FuaXNhdGlvbg== 18641
ZW5keQ== 18642
IFRleHQ= 18643
w7x5b3I= 18644
IMOt 18645
IGNvbmNsdXM= 18646
IOykgOs= 18647
IGFtcA== 18648
IExlc3M= 18649
IOuQmOuKlA== 18650
Y2Fubw== 18651
IFBpeA== 18652
YXBlZA== 18653
IGRhcmF1Zg== 18654
dW8= 18655
eW50aA== 18656
YWJlbA== 18657
IERvbmU= 18658
IGRpY2s= 18659
YXRob24= 18660
IGhpbGFy 18661
YWNjbw== 18662
IOyGjQ== 18663
IE9yZWdvbg== 18664
IFdlaWw= 18665
IG1hdGhlbWF0aWNz 18666
IGFsbQ== 18667
IHBpeGVscw== 18668
IGZyw6Vu 18669
0LHQvg== 18670
RkM= 18671
0L3Rjg== 18672
aGVpbQ== 18673
Z29z 18674
IEZvcmdldA== 18675
ZmVuZA== 18676
IFZvaWzDoA== 18677
IEdyZWV0 18678
IM6xz4XPhA== 18679
IHJlY3Vy 18680
5pS2 18681
NTE= 18682
IOyeiOqzoA== 18683
QXQ= 18684
IHlhcmRz 18685
0LjRgtC4 18686
IG9mZnNldA== 18687
cm9sbGluZw== 18688
INCf0L7RgQ== 18689
IGVubGlnaHQ= 18690
IFBhZA== 18691
bGltaXRlZA== 18692
0LjQu9GM0L3Qvg== 18693
IFNhcmE= 18694
INGB0LTQtdC70LDRgtGM 18695
bWFydA== 18696
IEp1bXA= 18697
IGFkb3JhYmxl 18698
b3JzZQ== 18699
Y2hlZXJpbmc= 18700
IGVtcGF0aHk= 18701
IFRvbmlnaHQ= 18702
b3Jw 18703
IEh1bnRlcg== 18704
UG9pbnQ= 18705
0LPQsA== 18706
IHBhc3Nlbmdlcg== 18707
IEtuaWdodA== 18708
IHNlZW1pbmdseQ== 18709
aHVo 18710
IHRoZWF0cmU= 18711
IHRvbWI= 18712
IGRlcHJlc3NlZA== 18713
IHN1bW1vbg== 18714
IHNhdGlzZmFjdGlvbg== 18715
ZG9vcnM= 18716
IEhvdXN0b24= 18717
0LDRjtGJ 18718
IFJpbw== 18719
0LPQu9GP 18720
IGFycmFuZ2Vk 18721
IGhhbmRsZXM= 18722
IHRyaWxsaW9u 18723
IG5pZ2h0bWFyZQ== 18724
IFF1YW5kbw== 18725
IG9sZQ== 18726
IEd1aWRl 18727
b29v 18728
IGJpbGU= 18729
IGVtcGV6 18730
IDcy 18731
Y3JpYmVk 18732
IHByb2dyZXNzaW9u 18733
IExpbnV4 18734
66as 18735
IOyymOydjA== 18736
IGZvc3NpbA== 18737
IHF1ZXJv 18738
7Iah 18739
YXRpdmE= 18740
IHB1eno= 18741
IFp1cw== 18742
44Kq 18743
IHRocmlsbGVk 18744
IENC 18745
IG1pbmVy 18746
0YDQsNGJ 18747
IFNBUg== 18748
IE5vcw== 18749
INCz0L7RgNC+0LQ= 18750
IGNhbWI= 18751
INGC0LA= 18752
IHJlc3VsdGVk 18753
IERpY2s= 18754
b3VuZw== 18755
IGNvbWljcw== 18756
IGFic29sdXQ= 18757
c3Rhbg== 18758
ZGltZW5zaW9uYWw= 18759
IHRlbnNl 18760
bXVz 18761
IEludGVsbA== 18762
INGN0YLRgw== 18763
IHBoYXNlcw== 18764
IHZvbHRh 18765
IHbDo28= 18766
Ym91bmQ= 18767
IEFuZGVyc29u 18768
IGN1cmlvc2l0eQ== 18769
IHBvbnQ= 18770
6YCZ6KOh 18771
IGRlbW9uc3RyYXRlZA== 18772
b2xpbmU= 18773
IFNwZWVk 18774
IG1hbWE= 18775
IHNob2NraW5n 18776
IGtpZWR5 18777
IGVhcnRocXVha2U= 18778
IGltcGxpZXM= 18779
IGVudGVycw== 18780
noA= 18781
IGVsZXZhdG9y 18782
IGRlbGlnaHRlZA== 18783
IE1pdHQ= 18784
IEJhc2Vk 18785
IERvbA== 18786
IGtlbg== 18787
IHdvcnJ5aW5n 18788
IGZpbGVk 18789
YWlsYW5k 18790
INC80LXRgg== 18791
IG1hc2M= 18792
IM6R 18793
IEp1bGll 18794
IGRpbWVuc2lvbmFs 18795
aHVtYW4= 18796
VG9r 18797
w78= 18798
IHVuc3Q= 18799
IHNldWxl 18800
IGVtYmFy 18801
IO2VqeuLiOuLpA== 18802
YWNpb24= 18803
IOyJ 18804
IOu2gOu2hA== 18805
IGhlYXRlZA== 18806
4oCm4oCm 18807
IiE= 18808
IHJlYWxpc2U= 18809
0LXRgtGL 18810
aWVuaWE= 18811
aWV6 18812
IGbDvGg= 18813
IEVzc2U= 18814
IHBz 18815
IGTDsw== 18816
YXN0ZXJz 18817
IG9ucw== 18818
UE0= 18819
IHJldHJv 18820
bWFrZXI= 18821
d2hlbg== 18822
IGVsbGE= 18823
IExpdmluZw== 18824
IExhbQ== 18825
IHRyb25n 18826
IGFwcHJvdmU= 18827
IM64zrE= 18828
IHN1bmc= 18829
0LXQvdC40Y4= 18830
IFJlbW92ZQ== 18831
w6huZQ== 18832
aXJlbg== 18833
IHN0cmFuZ2Vy 18834
0LjQvdGL 18835
IHbDpg== 18836
YWZ0ZXI= 18837
b3R0bw== 18838
lOuhnA== 18839
IEFob3Jh 18840
bWlsbA== 18841
SVNI 18842
IGdyYWR1YXRpbmc= 18843
a3Rl 18844
IHJlbm92 18845
IHByb2Nlc3NlZA== 18846
a2V5cw== 18847
0LXQutC+ 18848
IGVucmljaA== 18849
IMWfZWs= 18850
IGluc2Vj 18851
IE5hbg== 18852
Y2FrZXM= 18853
IGlsbHVzaW9u 18854
mOulvA== 18855
IGFpcmw= 18856
aW1z 18857
IGFudGVu 18858
4buvbmc= 18859
c24= 18860
IHByZWNpc2E= 18861
6riw7J6Q 18862
INin2YTYuQ== 18863
IGZvcmVtb3N0 18864
IHBhcmFncmFwaA== 18865
YXZhaXM= 18866
INCy0L7RgQ== 18867
IG1hbnM= 18868
w61maWM= 18869
Ym90 18870
INi52YY= 18871
IGJyb3Ro 18872
IGFsdGVybmF0ZQ== 18873
IENoYXB0ZXI= 18874
IHZlY3RvcnM= 18875
ZXNhcg== 18876
IGluZGljYXRpb24= 18877
IE5laW4= 18878
toE= 18879
IGplYW5z 18880
WUU= 18881
Y29uZA== 18882
IHVuaXRlZA== 18883
YWJp 18884
IFNlcmdl 18885
IHBhcnRpYWxseQ== 18886
IG1hY3Jv 18887
5omN 18888
5by1 18889
IGV0aGljYWw= 18890
cnVpdA== 18891
IHNoaWZ0ZWQ= 18892
IGNhYmU= 18893
IG1hdGhlbWF0aWNhbA== 18894
IHJ1ZGU= 18895
15nXldeq 18896
IE1lcmM= 18897
IGdhbnpl 18898
aWNpb24= 18899
IHVuY29uc2Npb3Vz 18900
IGJ1cm50 18901
INGA0LXQsQ== 18902
7Yq46w== 18903
IGNoYXJt 18904
YW5kYWw= 18905
7LKc 18906
b3RoeQ== 18907
IEhhZGk= 18908
IGFwcHJlY2lhdGlvbg== 18909
RU5E 18910
IHLDqWFs 18911
toTrk6Q= 18912
IE5hZw== 18913
oKTqs6A= 18914
IExhdXJlbg== 18915
IHbhu5tp 18916
IEJyaWRnZQ== 18917
IFVtbQ== 18918
IFdlZw== 18919
IGNoYXF1ZQ== 18920
IFNvcGg= 18921
IGdkemll 18922
7ZGc 18923
IHN0ZXI= 18924
IEJsYQ== 18925
IHJlZmxlY3Rz 18926
IGJlbmNobWFyaw== 18927
0LLQsNGC 18928
YW1pbmU= 18929
44Gh44KD 18930
IGFuaA== 18931
IGNvbnRpbmVudA== 18932
IEZEQQ== 18933
7KGw 18934
IMOqdGVz 18935
15nXkA== 18936
5byA 18937
IGJsb29keQ== 18938
IE5pbmU= 18939
aWVsdA== 18940
ZW1hbmQ= 18941
IOuztOqzoA== 18942
IHRpZGFr 18943
IFNjaWVudA== 18944
cGxleA== 18945
b3N0ZW4= 18946
IGFuaW1hdGVk 18947
YXNzYQ== 18948
IGRlcml2ZWQ= 18949
INC40YHRgtC+0YA= 18950
IE1pZw== 18951
7IWY 18952
IHJvcw== 18953
cGx1cw== 18954
b3NhdXI= 18955
IF4= 18956
IGludGVuc2l2ZQ== 18957
IGdsb2JhbGx5 18958
IGRpZmVyZW4= 18959
7J206rOg 18960
5L2g55qE 18961
xIVk 18962
IGTDqXM= 18963
IHByZXNlbnRhdGlvbnM= 18964
IENybw== 18965
IGVzc2Vz 18966
IEJldHdlZW4= 18967
UGE= 18968
IG5hdw== 18969
4Lit4LiH 18970
IGJyZWVk 18971
aWNodGU= 18972
INCe0L3QuA== 18973
IEJ1aWxkaW5n 18974
IGNvbmZvcm0= 18975
TU8= 18976
INCW 18977
IEtpZA== 18978
bmFz 18979
IER1ZQ== 18980
csOpcw== 18981
IGRpb3g= 18982
IEJpbg== 18983
IHRheGk= 18984
IHNhcA== 18985
IEh1Yg== 18986
54K65LuA6bq8 18987
IGNlbnRlcmVk 18988
IHN1cmdl 18989
IGF2b25z 18990
IGxlYXJudA== 18991
IFlhbQ== 18992
IERpZXNl 18993
0L3QuNC60Lg= 18994
IEJlaWo= 18995
V2lsbA== 18996
IGF0dGVtcHRlZA== 18997
IGdyaWVm 18998
w7Nq 18999
IGtpZG5leQ== 19000
IG9wcG9uZW50cw== 19001
5pu0 19002
IG5vbWU= 19003
NTc= 19004
0Y/RgtC90L4= 19005
IG1pZG5pZ2h0 19006
QW5ub3VuY2Vy 19007
YWNpdHk= 19008
b25lZA== 19009
IHB1ZWRlcw== 19010
IHByb2JsZW1hdGlj 19011
IGNvcHM= 19012
IFBldGU= 19013
cmludA== 19014
dW50ZWQ= 19015
IGJpcA== 19016
5qI= 19017
IMOA 19018
IGNlbnM= 19019
YXRpdmVseQ== 19020
IOS4jQ== 19021
IHVyZ2VudA== 19022
IHN0cnVnZ2xlZA== 19023
YWNodXM= 19024
IG1pY3Jvd2F2ZQ== 19025
IFNpZGU= 19026
IERlbm4= 19027
INGP0LI= 19028
IHVyZ2U= 19029
IGZvcmNpbmc= 19030
d2FuZw== 19031
INC60L7RgtC+0YDQsNGP 19032
IG1hbW0= 19033
IPCfjg== 19034
IHRyaWJlcw== 19035
IFNoYWRvdw== 19036
IFNhbmc= 19037
IEhpdGxlcg== 19038
IGx1bg== 19039
IHNjZW50 19040
7KeR 19041
IG92ZXJ3aGVsbWVk 19042
IGJvbWJz 19043
IGNyaW1pbg== 19044
IGNvbnNvbGlk 19045
IG1vbGVjdWxhcg== 19046
15XXpw== 19047
bm9y 19048
IHBlcmNlaXZlZA== 19049
IHbDqQ== 19050
IGFsdG9nZXRoZXI= 19051
IG9ydGg= 19052
IHZlbQ== 19053
IHp3YXI= 19054
aXpv 19055
xas= 19056
IG1lbHRlZA== 19057
b3JkZW4= 19058
IENoYXJsb3R0ZQ== 19059
IEV4Y2Vs 19060
YXJ0YQ== 19061
7Jyg 19062
IEdldw== 19063
IHJvbWFuY2U= 19064
ZXJlbW9z 19065
IGNvbG9uaWFs 19066
IHRyYWRpdGlvbmFsbHk= 19067
IHF1YW4= 19068
aG9v 19069
IGNoYW1waW9uc2hpcA== 19070
IGFyYml0cg== 19071
7IWU 19072
INC80LjQvQ== 19073
IHNlbGZpc2g= 19074
IGJsZXc= 19075
cnlpbmc= 19076
IG9wZXJhdG9ycw== 19077
IGp1cmlzZA== 19078
j4U= 19079
dWl0aW9u 19080
IEVD 19081
IEFueWJvZHk= 19082
dmF0ZQ== 19083
aWV0aWVz 19084
IGFuYWx5c3Q= 19085
tOyXkA== 19086
INCy0YHQtdCz0LTQsA== 19087
w6dlaw== 19088
IEt1bg== 19089
IGFnaW5n 19090
1aE= 19091
0YDQsNGE 19092
IE1vbWVudA== 19093
IEh1YQ== 19094
6IM= 19095
dGhlbg== 19096
0LXQu9Cw 19097
ZXN0b25l 19098
IGVuZGU= 19099
IGF3YXJkZWQ= 19100
IG7DpGNoc3Rlbg== 19101
IFNwb3Q= 19102
IE5lZw== 19103
IGZhaXJ5 19104
5Luj 19105
IENvdmVy 19106
IGRlcG9zaXQ= 19107
IHN0cmVzc2Z1bA== 19108
IGp1bms= 19109
IG1ldGFib2w= 19110
SmE= 19111
IOq3gA== 19112
IHVuZGVyZ3JhZHVhdGU= 19113
IGNhbmNlbGw= 19114
IGNvbnNlbnN1cw== 19115
IG9zbw== 19116
6YeR 19117
4bq3 19118
xJ9lcg== 19119
cmFkYQ== 19120
IFBhbGFjZQ== 19121
IHBlZGFs 19122
IGV4YWdnZXI= 19123
IGJlaGF2aW9yYWw= 19124
cGxheWVy 19125
bGxlcw== 19126
IGNvbm5lY3Rvcg== 19127
IHNrZXB0 19128
jZTrnbzqs6A= 19129
IG1pdHQ= 19130
IEhhaGE= 19131
IHBlcXVl 19132
IEdvdHQ= 19133
ZmFuZw== 19134
4LA= 19135
am9z 19136
IGtpY2tpbmc= 19137
IG1vdW50ZWQ= 19138
IHJlcGxhY2luZw== 19139
dm9z 19140
IHF1aWV0bHk= 19141
IG1pbGl0 19142
IG93bnM= 19143
IG5pdmVhdQ== 19144
IGF1cg== 19145
IEJ1eQ== 19146
IHByZWRpY3RlZA== 19147
IGNvd3M= 19148
IHBvbmVy 19149
IERyaQ== 19150
IHJlbWFya3M= 19151
IHJlcG9ydGVy 19152
IGFya2FkYcWf 19153
0LXRgdGC0Lg= 19154
IHNhdmVz 19155
IMOnb2M= 19156
IG1ldGFwaG9y 19157
IEtlbA== 19158
c3RhdGlvbg== 19159
c2VtYmx5 19160
IGFkdmlzb3I= 19161
IHdvcmtzaG9wcw== 19162
IGFjY291bnRpbmc= 19163
IHRvaw== 19164
bmllcg== 19165
aW5uZXI= 19166
IGJ1cmFkYQ== 19167
IEJC 19168
IE9seW1waWM= 19169
IFByYWN0 19170
Q2hyaXN0 19171
INGB0Y4= 19172
IGthcw== 19173
IHZpZXdlZA== 19174
IG1hcmtlcnM= 19175
IGZvdG8= 19176
Z2V0aWM= 19177
IEx1Y2Fz 19178
IHBhZHM= 19179
IEpvaA== 19180
IENEVQ== 19181
YWZmZW4= 19182
YXJlbQ== 19183
IEJlY2s= 19184
IEdvc2g= 19185
c2hpdA== 19186
44GM44Go44GG 19187
IE1hdGVy 19188
YWJ1bGFyeQ== 19189
IFJvb20= 19190
bGxlbg== 19191
IEZvbGxvd2luZw== 19192
IGRvaXQ= 19193
YmFsbHM= 19194
aXhh 19195
IGdyb3VuZHM= 19196
IOyeiOuKlOuNsA== 19197
TFM= 19198
IHdpbGRsaWZl 19199
IFNRTA== 19200
IHNoaWZ0cw== 19201
5LiA6bue 19202
Qm9vaw== 19203
IGhvc3RlZA== 19204
bGxvcg== 19205
IHNuYXBz 19206
IGJlc29pbg== 19207
INep15Q= 19208
IHBlYW51dA== 19209
w6RmdA== 19210
uaA= 19211
xZts 19212
QXVkaWVuY2U= 19213
IEJhcmJhcmE= 19214
IGFkb3B0aW9u 19215
IHdvbGY= 19216
INC+0YHQvdC+0LI= 19217
YXJkYQ== 19218
IGV4cG9zZQ== 19219
IOym 19220
amFz 19221
xJM= 19222
IGNvdW50bGVzcw== 19223
IOyngQ== 19224
aGVhbHRo 19225
dWVudA== 19226
aXNv 19227
b3Rpb24= 19228
IGh1bmdlcg== 19229
IG1vaXM= 19230
b2Zmcw== 19231
IGNsYWltaW5n 19232
IM6a 19233
IEJlbGc= 19234
INC90LDQuQ== 19235
6riw64+E 19236
IHVucHJl 19237
IGdlZA== 19238
IElv 19239
INC/0L7RgdC80L7RgtGA 19240
IGNvxZs= 19241
IE5hcnJhdG9y 19242
IMOHb2s= 19243
7Zmp 19244
4Lit4Lii 19245
Y2lwbA== 19246
IHRpbWVy 19247
IGRlZmlj 19248
YXZpbg== 19249
IGNhdGVnb3I= 19250
IHRocm93cw== 19251
IOuCnA== 19252
INC/0L7RgdC70LXQtA== 19253
IFRoYWk= 19254
IG1hc2N1bA== 19255
IGJla29tbWVu 19256
IGludGVybmF0aW9u 19257
dWxzZQ== 19258
IGF5ZQ== 19259
IHBvaQ== 19260
IHBpeGVs 19261
Q2hyaXM= 19262
IHN0b3Zl 19263
zr/OuQ== 19264
IGdlbmVyYXRvcg== 19265
IOy7rOs= 19266
IGFjYWRlbQ== 19267
IHByYWN0aWNlZA== 19268
IGFxdWVzdA== 19269
IGNvbnRyaWJ1dGluZw== 19270
IEln 19271
IOG7nw== 19272
IGNvbnRhaW5pbmc= 19273
IHdyZXN0bGluZw== 19274
INGH0LXQs9C+ 19275
aGF1cHQ= 19276
IGVzc2Fz 19277
dmVsb3Bl 19278
IGV4Y2VwdGlvbmFs 19279
WVU= 19280
IEFwcGxhdXNl 19281
cmljYW5l 19282
IGNvbnZlbmllbmNl 19283
INC00LXQu9Cw0YLRjA== 19284
0LjQu9C40YHRjA== 19285
IEVudmlyb24= 19286
ODU= 19287
IGPDog== 19288
IOyViOuFle2VmOyEuOyalA== 19289
IE1P 19290
IFBvcGU= 19291
IHNhaA== 19292
b2Jp 19293
IG1hc3RlcnM= 19294
YWluZXM= 19295
IGJsZXNzaW5ncw== 19296
IG9iZXk= 19297
IGZsdXg= 19298
IGJyb3c= 19299
IOyLpA== 19300
IHBvcHVsYXJpdHk= 19301
IExhbWI= 19302
emV1Zw== 19303
7JmU 19304
j4TroZ0= 19305
aXR1YXRpb24= 19306
IGFjY29tcGFu 19307
IGRpYWxvZw== 19308
IEphbWll 19309
5Yqg5rK5 19310
IHNld2luZw== 19311
IGJsZWVkaW5n 19312
IGJhaWw= 19313
IHRocmVhZHM= 19314
b2RnZQ== 19315
IFNoYW5n 19316
IGRlcGxveW1lbnQ= 19317
Y2hlZA== 19318
IHNhdGlzZnk= 19319
IGxheg== 19320
IG1pc3NpbGU= 19321
IExpbmtlZA== 19322
IG1ha2Vycw== 19323
Y2l1bQ== 19324
ZnJl 19325
IOuovA== 19326
IOustOs= 19327
IEVkZ2U= 19328
IHNvY2lldGllcw== 19329
IGFndWE= 19330
IHN5bmNocm9u 19331
oaA= 19332
dW5mdA== 19333
IHVubQ== 19334
IHRyaWFuZw== 19335
IGluanVzdA== 19336
dG9w 19337
IG9yYWw= 19338
a29y 19339
IO2VqA== 19340
bGRpZ3Q= 19341
Y2XEnw== 19342
cXVldA== 19343
IExlbw== 19344
IHNhdm9pcg== 19345
IGVhc3Rlcm4= 19346
aWV1 19347
IGV4cGVk 19348
INCh0L8= 19349
IHVubmVjZXNzYXJ5 19350
IFBlcmZvcm0= 19351
IE1pbmc= 19352
INGA0LDQsg== 19353
IGludGVudGlvbnM= 19354
IGNvbXByZXNzaW9u 19355
IFNhYw== 19356
zr/Ouw== 19357
YXJzb24= 19358
IHRyb3V2ZQ== 19359
IE11aGFtbWFk 19360
INCy0YvRgQ== 19361
IGZpbml0ZQ== 19362
INC90LDRhdC+0LQ= 19363
dWdh 19364
0YDQsNC30YM= 19365
IGNlbGVicmF0ZWQ= 19366
IGNvbmZlc3M= 19367
IHNxdWFyZXM= 19368
IEdvcmRvbg== 19369
IOuCmOyY 19370
IHN5bmRyb21l 19371
IGNvbXBsZXRpb24= 19372
IGJhY2tpbmc= 19373
IGRhcmY= 19374
IFF1cmFu 19375
IGludGVybWVkaWF0ZQ== 19376
IGtlcg== 19377
IGTDvA== 19378
aGVzaXZl 19379
IGFjY291bnRhYmlsaXR5 19380
IFJlYmVjY2E= 19381
6JGX 19382
IFNsZWVw 19383
IGRpZmbDqXJlbnQ= 19384
b2xz 19385
IFJpY2U= 19386
IOuzuA== 19387
IGFudGliaW90 19388
z4TOrA== 19389
cno= 19390
YW1ibGluZw== 19391
IHNlbnNpdGl2aXR5 19392
IGNocm9u 19393
YWxsYXM= 19394
NjQ= 19395
IGZsZWV0 19396
IG9wdGltaXN0aWM= 19397
0YHQutC+0LPQvg== 19398
IGphZGk= 19399
YWlsbGV1cnM= 19400
IEVub3VnaA== 19401
IHNlbmlu 19402
IHBhY2tz 19403
Ym4= 19404
IEFyZWE= 19405
IFRybw== 19406
qOumrA== 19407
0LDRlA== 19408
IFRob20= 19409
IGhhcm1vbnk= 19410
0L3QuNC60LA= 19411
IHNvbWVkYXk= 19412
SVNF 19413
IEJyb2Fkd2F5 19414
bGFyZXM= 19415
ZXJuZXNz 19416
4LmE4Lih 19417
IFRlbm4= 19418
IE5BVE8= 19419
44KK44G+44GZ 19420
IG1pbnV0b3M= 19421
IEthbnNhcw== 19422
IE1vbmc= 19423
IGNvbXB0ZQ== 19424
5Zub 19425
iqQ= 19426
IOyXrQ== 19427
IHN1cGVyaGVybw== 19428
IEdhcmRlbg== 19429
IE1vcw== 19430
IGF0dGFjaG1lbnQ= 19431
IGJ1c3Q= 19432
4K+K 19433
IFRoYWlsYW5k 19434
c3RhdA== 19435
IHNwaWNl 19436
IExlYg== 19437
IGxlYXA= 19438
emVjaA== 19439
R0w= 19440
IHZlcmw= 19441
IGZpeGluZw== 19442
IOuztOuptA== 19443
IHBvcm4= 19444
IGLDvHk= 19445
INmF2Kc= 19446
IFZpcnQ= 19447
IFRvbW15 19448
IGNhcmdv 19449
IE9saGE= 19450
IHJva3U= 19451
2YPZhg== 19452
IGJha2Vk 19453
IHRhY3RpY3M= 19454
IG1hcmtldHBsYWNl 19455
IGt0w7NyYQ== 19456
YXJsbw== 19457
IHN3aXRjaGVz 19458
IGNhY2hl 19459
IEhS 19460
IEdhbg== 19461
IEdQUw== 19462
IGR1YXM= 19463
aGVyZXM= 19464
0LXRgNGI 19465
dHJhY2s= 19466
IGx1bmdz 19467
U3RhdGlvbg== 19468
aWdnbGVz 19469
IGNhbXBpbmc= 19470
5ZOH 19471
IGNvbXBsZXRpbmc= 19472
YW1hcw== 19473
IGN5Y2w= 19474
IHByb3RvdHlwZQ== 19475
IEp1ZGdl 19476
b3R5cGVz 19477
IGluZmVjdGlvbnM= 19478
oKTr 19479
0LXRgNCz 19480
b2Jh 19481
IEJvZA== 19482
IFNlY29uZGx5 19483
IGFwb3N0 19484
IHNvZ2Fy 19485
IHJlYXNz 19486
aWVr 19487
5pa8 19488
IGFzaGFtZWQ= 19489
IGN1cnZlcw== 19490
INCy0LDQtg== 19491
IGVuc2VtYmxl 19492
YXR1cg== 19493
IHBob3RvZ3JhcGhlcg== 19494
IGVpZ2h0aA== 19495
IHdhc3RlZA== 19496
566X 19497
IGRhbXA= 19498
INC80LDQuw== 19499
YXJlbmE= 19500
IGludGVybmFsbHk= 19501
IGhlZWxz 19502
IFNhbHQ= 19503
IGJsaXI= 19504
iOuCmA== 19505
IGNvbnRyYXJ5 19506
IHByaW1h 19507
IG9zcw== 19508
IHJhYmJpdA== 19509
IGF1dG9y 19510
IGJyb2FkbHk= 19511
w61zdA== 19512
IGJhY2tz 19513
7ZSE 19514
ZXRv 19515
IGp1cnk= 19516
6LE= 19517
IHByb3N0dQ== 19518
IGJhcmE= 19519
IHBhcmxpYW1lbnQ= 19520
b3JpZW50 19521
0LjQu9Cw0YHRjA== 19522
IGluZGlyZWN0 19523
w6Ft 19524
IMOlcg== 19525
IHRyYWl0cw== 19526
IGTDrWFz 19527
2YTZhQ== 19528
IENU 19529
YWx5c3Q= 19530
IGxpdmVzdA== 19531
IGtvcw== 19532
TWF5 19533
IEppbmc= 19534
IGpvdXJuYWxpc3Rz 19535
0YfQuNC6 19536
YXJtcw== 19537
IOqwkOyCrA== 19538
INC40LzQtQ== 19539
IMOpZ2Fs 19540
IE5ld3Rvbg== 19541
IHJlY292ZXJlZA== 19542
IGJyYXVjaGVu 19543
IEJyb24= 19544
0LDQvdC+ 19545
IHBhbGU= 19546
cHJpc2Vz 19547
IGhvcmFz 19548
Y2h0cw== 19549
6YCa 19550
w7/Dvw== 19551
YWtlcnM= 19552
IEFsYXNrYQ== 19553
emllag== 19554
IHNjb29w 19555
7J206rCA 19556
44GV44GE 19557
Y29y 19558
w6lsw6k= 19559
IHN1cmc= 19560
IHZpZW5l 19561
IEtyaXN0 19562
NTQ= 19563
IGJhbm5lZA== 19564
IHNtb290aGx5 19565
IHRyZWF0cw== 19566
IHByb25vdW5jZQ== 19567
IGZsdXNo 19568
IGNhbWJp 19569
IG11c2ljaWFu 19570
IEFzaGxleQ== 19571
IFNQRA== 19572
IEJvYmJ5 19573
IGdsb3Nz 19574
cmVzcGVjdA== 19575
IHJldmlld2luZw== 19576
IGdlbmVyaWM= 19577
xrDhu5tj 19578
YXRzw6RjaGxpY2g= 19579
IGhlYWx0aGllcg== 19580
dWJlcnM= 19581
INC00LDQvQ== 19582
IE1lZGljYXJl 19583
NTM= 19584
IGNvbXBsYWludHM= 19585
amFj 19586
IGFncmljdWx0dXJhbA== 19587
U3Bl 19588
IEpvbmc= 19589
IGRpb3hpZGU= 19590
6rKo 19591
ZWxpams= 19592
IFNoaXQ= 19593
YWludHM= 19594
IElhbg== 19595
IFNpbXBseQ== 19596
IFN0cmU= 19597
5pyL 19598
IEdEUA== 19599
NTk= 19600
YXN6 19601
IEthdGll 19602
INCx0YA= 19603
IHBlZWs= 19604
b3d5Y2g= 19605
IHJlc29ydA== 19606
IHJlc2lkZW5jZQ== 19607
IHNwaWNlcw== 19608
Y2nDsw== 19609
IGplZGVy 19610
IGVtbw== 19611
YXJpdW0= 19612
IHB1ZmY= 19613
66eJ 19614
0YPQu9GM0YI= 19615
IG1ldGE= 19616
IOyghOs= 19617
IG9wdGltaXphdGlvbg== 19618
Z2FuZw== 19619
IO2VhA== 19620
IGVmZmljaWVudGx5 19621
IHZpc3VhbGx5 19622
IGZyb3N0 19623
IEFydGh1cg== 19624
IMW8 19625
IGFjaGlldmluZw== 19626
IHJvdGF0aW5n 19627
IGxpbmluZw== 19628
IG9jY3VwaWVk 19629
5byf 19630
bWVudGF0aW9u 19631
IHN0cmV0Y2hpbmc= 19632
IHN0YWxs 19633
b3N0aWM= 19634
IFNldmVy 19635
IGdsdWM= 19636
IHLDs8W8 19637
IG91dHJlYWNo 19638
c3RyYQ== 19639
aWtlbg== 19640
IOyWmOq4sA== 19641
IEpvaW4= 19642
IGltcGU= 19643
IGNvbXBlbnNhdGlvbg== 19644
IFRhdA== 19645
IENhcmxvcw== 19646
w7xocnQ= 19647
IEZyYW5jaXM= 19648
Y2pp 19649
eWVhaA== 19650
IG1lbWJyYW5l 19651
IGV4aGFsZQ== 19652
IHJlbGk= 19653
IE9S 19654
IHJlZnJpZ2VyYXRvcg== 19655
IFZlbmV6 19656
TGlrZQ== 19657
IHJhaXNlcw== 19658
b3R0bGU= 19659
YXR1cmE= 19660
IHJ1bGVy 19661
IHdlZXI= 19662
IGd1aWRlZA== 19663
IE1hZ24= 19664
IENvcnBvcg== 19665
jZQ= 19666
IGF0dHJpYnV0ZQ== 19667
IFdvYWg= 19668
IGFycm93cw== 19669
IGF3YWl0 19670
IFByaW0= 19671
IGRpZ25pdHk= 19672
IE9udGFyaW8= 19673
aXNjaGVy 19674
IOyLnQ== 19675
aW1lbg== 19676
b3V2ZXI= 19677
QVNT 19678
4buHbg== 19679
b3B5 19680
YWNodXNldHRz 19681
IGVsZGVybHk= 19682
5Y6f 19683
RkE= 19684
IERhaWx5 19685
c2hpbmU= 19686
IDU2 19687
6KI= 19688
aWVybm8= 19689
IHNraWxsZWQ= 19690
IGdyb8OfZQ== 19691
IE9haw== 19692
56ys5LqM 19693
aWdnbGU= 19694
0LXQu9C10Lk= 19695
IGJpcmF6 19696
IGFyZ3Vpbmc= 19697
INC/0L7RjdGC0L7QvNGD 19698
IGRyaWZ0 19699
IGhhcm5lc3M= 19700
IGRlaXhhcg== 19701
YXV0cmU= 19702
IFNlZWluZw== 19703
IGNhcGl0YWxpc20= 19704
IEVsZA== 19705
emlvbmU= 19706
IEJleW9uZA== 19707
IHBlcmZlY3Rpb24= 19708
IGhvZQ== 19709
IGRlY2xhcmU= 19710
0LDQu9Cw0YHRjA== 19711
IHBva2U= 19712
INeh 19713
IGZpZ2h0ZXJz 19714
6rKg64uk 19715
0L7RgNC+0LI= 19716
IGFjY29yZGluZ2x5 19717
IElzYQ== 19718
IG9wdGltaXpl 19719
IE1pbmlzdHJ5 19720
IHNhZ2U= 19721
7Iuc66m0 19722
IGJlbmk= 19723
IGRvbmF0aW9u 19724
IGNsZWFyZWQ= 19725
IEx1Y2tpbHk= 19726
IGhhcm1mdWw= 19727
tey7pA== 19728
IGNlbWVudA== 19729
0L/QuNGB 19730
IGRlZGk= 19731
IENyYWln 19732
IGRlbW9ucw== 19733
IGN1c3RvbWl6ZQ== 19734
IGlnbm9yZWQ= 19735
IFRpYW4= 19736
IGhvcGVk 19737
IEJ1cmVhdQ== 19738
IHJp 19739
IFlhaA== 19740
IHNvY2tldA== 19741
IGZlYXR1cmluZw== 19742
IHBhcmY= 19743
IFRF 19744
IFRlYWNoZXI= 19745
IGNhdGFsb2c= 19746
6rCA7KeA6rOg 19747
IFNlaXRl 19748
IGNvbmU= 19749
IFBhbGVzdGlu 19750
IGdld29vbg== 19751
IGdhaW5pbmc= 19752
INii 19753
IGNhdGFzdA== 19754
IG5laWdoYm91cg== 19755
SVNU 19756
IHN0ZWFsaW5n 19757
IHRyb2lz 19758
IGludGVuZA== 19759
IFNob290 19760
IHBpb25l 19761
IEludGVs 19762
IExJTg== 19763
IGJyaWdodGVy 19764
IFllc3RlcmRheQ== 19765
IHNvdw== 19766
c2lu 19767
b2Rz 19768
IGV0aGljcw== 19769
IGludGVydmlld2Vk 19770
cmVsbA== 19771
IHJlZnJlc2hpbmc= 19772
c8Ol 19773
IGFic3VyZA== 19774
IHBob3NwaA== 19775
Zmls 19776
IHN0ZWhlbg== 19777
dmFscw== 19778
IGNhcmVk 19779
5oiW 19780
IGRlbGw= 19781
Ym9uZQ== 19782
IGhvY2g= 19783
IHB1cA== 19784
IGlv 19785
IGFjb250ZWNl 19786
ZWxsZXM= 19787
IFNwbA== 19788
aWdp 19789
IHTDpG4= 19790
IGVsZXBoYW50 19791
IGdhdGVz 19792
IHNsaWNlcw== 19793
IHByYW5r 19794
b2tyYXQ= 19795
IGhpbGFyaW91cw== 19796
IFNpZA== 19797
IOuSpA== 19798
IGVzc2VyZQ== 19799
IHRlbGVwaG9uZQ== 19800
aW5hbGx5 19801
cmF0b3I= 19802
IGhlbGljb3B0ZXI= 19803
IGnFn3Rl 19804
IGdpZA== 19805
IHRvdXJpc3Q= 19806
IGNvbmZsaWN0cw== 19807
0LDRgtCw 19808
IHTDqQ== 19809
IGFzc2VydA== 19810
IGxhdW5kcnk= 19811
IEJvbQ== 19812
IHNwZWNpYWxpemVk 19813
IE1vZGVybg== 19814
b2dyYWY= 19815
IGFubw== 19816
IHJldHJpZQ== 19817
IFB1dGlu 19818
IEhBUg== 19819
INC80LDRiA== 19820
IM6xz4DPjA== 19821
IHR1dHRp 19822
INCy0YLQvtGA 19823
7Ja1 19824
IEJ1bA== 19825
64uk66m0 19826
xYJl 19827
5pyL5Y+L 19828
YXJpbg== 19829
IHRoZXJhcGlzdA== 19830
IGfDpXI= 19831
IEN6eQ== 19832
cHBl 19833
bWly 19834
IFRlcm0= 19835
IEJlYXI= 19836
bGFjZQ== 19837
IE1vcmVvdmVy 19838
IERpc2M= 19839
IO2DgA== 19840
IHRpdGxlZA== 19841
IHN0cmlwcw== 19842
IEZhaHI= 19843
IFJpbmc= 19844
cmFuZG8= 19845
YWZh 19846
6Lqr 19847
IHNob3J0cw== 19848
IHRydW5r 19849
IHNlbnRpZG8= 19850
z4nOvQ== 19851
IGFjcmVz 19852
IG92ZXJk 19853
IE9seW1waWNz 19854
5Y+r 19855
IE1lcmNp 19856
IOuCmOyYpA== 19857
IGdlcm0= 19858
YW1tZWQ= 19859
IHByZWd1bnQ= 19860
IE51dA== 19861
IDwv 19862
IHRyYXZlbHM= 19863
IHZvY2FidWxhcnk= 19864
ZXRlbg== 19865
b2Rlcg== 19866
IGNvbnN1bWluZw== 19867
d3JpdGluZw== 19868
6LaF 19869
IGFwcGVhcmluZw== 19870
IGFkanVzdGVk 19871
c2Vt 19872
IGZyZW50ZQ== 19873
IG1heGltaXpl 19874
IHp3aXNjaGVu 19875
IHphbQ== 19876
Y29uc2Npb3Vz 19877
emVr 19878
6LCi6LCi 19879
aGFv 19880
7LKY65+8 19881
IEVwaXNvZGU= 19882
IHZpc2liaWxpdHk= 19883
IG1pam4= 19884
IHZpZWxlbg== 19885
IEJyb3RoZXJz 19886
15nXkQ== 19887
IHbDpGxkaWd0 19888
IGNydXNoZWQ= 19889
dWZlbg== 19890
5L2g5YCR 19891
YWN0aWM= 19892
IEJlZA== 19893
IEZB 19894
aXNzaXBwaQ== 19895
IHJlbW90 19896
IHBldHM= 19897
IHRodW5kZXI= 19898
IE1hbQ== 19899
7JW17Luk 19900
cGFyZW50cw== 19901
IGLEsQ== 19902
IHN1cnRvdXQ= 19903
IHNlZ21lbnRz 19904
IG5laG1lbg== 19905
IHV0aWxpeg== 19906
IFJ1Ynk= 19907
IHLhu5Np 19908
IGhhcHBpbHk= 19909
IGJ1c2g= 19910
dWx0YW4= 19911
546p 19912
2Lg= 19913
IEhpbA== 19914
IGxhd24= 19915
IGV5ZWJyb3dz 19916
bWV6 19917
IFN5ZA== 19918
cmVw 19919
aW5m 19920
6aCt 19921
IG92ZXJoZWFk 19922
Y3puaWU= 19923
IG94aWQ= 19924
IFdvbA== 19925
IGRlc3Ryb3lpbmc= 19926
IEFkZGl0aW9uYWxseQ== 19927
dW1ibGVk 19928
ZGVw 19929
IGRlcG9z 19930
IGNvbW1vZA== 19931
IGNha2Vz 19932
IHRhbGVudHM= 19933
IHBvdXJxdW9p 19934
IGNvbnRlbXBs 19935
bmVscw== 19936
0L7RiQ== 19937
IEFyYWJpYw== 19938
IE1hcnlsYW5k 19939
546L 19940
b3dv 19941
IFBsYQ== 19942
xJ9sdW0= 19943
IHByb3BoZQ== 19944
IFJlcHJlc2VudA== 19945
b3BvbA== 19946
YWNjb3Jk 19947
IE1lYW5pbmc= 19948
IGpvaW50cw== 19949
IGJyYWtlcw== 19950
Y2t0 19951
IDE5OTk= 19952
IHB1YmxpY2F0aW9u 19953
IFJldmlldw== 19954
0L7QudC0 19955
IG5pY2hl 19956
IHNpZ25pZmljYQ== 19957
IGRlYnI= 19958
IG92ZXJsYXA= 19959
IGRlbWFuZGluZw== 19960
IFPDsw== 19961
IHN1YnNlcXVlbnQ= 19962
IHF1b3Rlcw== 19963
IEN1cnJlbnRseQ== 19964
IHByZXZlbnRpbmc= 19965
IDEzMA== 19966
IENlbA== 19967
b25u 19968
d25pZcW8 19969
7JW9 19970
INC60LDQutC40LU= 19971
QUNI 19972
IGd1bQ== 19973
IElzcmFlbGk= 19974
7Jy864uI6rmM 19975
5ag= 19976
cnVrdA== 19977
IGNsYXBwaW5n 19978
IE1hc3NhY2h1c2V0dHM= 19979
IHJlc2lsaWVuY2U= 19980
IHN1YnNjcmliaW5n 19981
IGpld2Vscnk= 19982
Z2VicmE= 19983
IGNvcnJlY3Rpb24= 19984
Ym9v 19985
2KY= 19986
bGlv 19987
c2Ft 19988
IGVudmVsb3Bl 19989
a2Fs 19990
IEZhcm0= 19991
IGNhdHRsZQ== 19992
IGJyYXM= 19993
IHJlcGVudA== 19994
IHRvbmVz 19995
b3Npb24= 19996
cGVjdGlvbg== 19997
IGRlbmVu 19998
yJtp 19999
IE1hcmc= 20000
IGFjcXVpcmU= 20001
aWJsaW5ncw== 20002
IGFzcGly 20003
IHNpemVk 20004
IGFsYw== 20005
IHZpYnJhdGlvbg== 20006
dGls 20007
ZW1pbg== 20008
IGNvcnJlbGF0aW9u 20009
IHNpbmd1bGFy 20010
INC/0L7Rj9Cy 20011
cmVr 20012
IGNoYXB0ZXJz 20013
bWJyZQ== 20014
IGF1ZGl0aW9u 20015
w6dhcw== 20016
IHZhbXA= 20017
IHRlcw== 20018
INGA0LDQt9Cy 20019
IHJlc3BlY3RlZA== 20020
Y2lu 20021
IGZ1Y2tpbg== 20022
IMO8YmVyaGF1cHQ= 20023
INC/0L7QsQ== 20024
IGFsaWtl 20025
tog= 20026
cm9iaQ== 20027
w650 20028
IFRvdWNo 20029
YW56YQ== 20030
IGZpcm1seQ== 20031
IEdyZWV0aW5ncw== 20032
c2NhbGU= 20033
ZGFk 20034
0LDQutGC0Lg= 20035
IGJhY2t5YXJk 20036
0L7QttC0 20037
R3I= 20038
IFNURQ== 20039
0L7RgNGC 20040
IGjDpHR0ZQ== 20041
IEZpcnN0bHk= 20042
IE9mdGVu 20043
YXN1cmVz 20044
IGRyYXdz 20045
cmVkaXQ= 20046
QVRF 20047
UGU= 20048
Q1A= 20049
IGNvbXBlbGxpbmc= 20050
IHN1YnNpZA== 20051
IG5laWdoYm9yaG9vZHM= 20052
IGRpcGxvbQ== 20053
IGVudGVuZGVy 20054
cGVyaW5n 20055
YXVn 20056
Y2hhdA== 20057
0J3Rgw== 20058
IERvbGw= 20059
IOygkA== 20060
IGhvc2U= 20061
bmFy 20062
IHJld2FyZGluZw== 20063
IFNvbGQ= 20064
IHRha2k= 20065
IGJsYWRlcw== 20066
IEthdGg= 20067
IGpvZ28= 20068
IHNlbnNhdGlvbg== 20069
dWFuYQ== 20070
cGVs 20071
IFJlY2VudGx5 20072
IHBvbHltZXI= 20073
IFVQ 20074
LS0t 20075
IGhvdmVy 20076
IHJ1bGVk 20077
5rW3 20078
INeU15DX 20079
IGFmZmVjdGlvbg== 20080
IMSR4buD 20081
IGJyZWU= 20082
56eB 20083
IExheQ== 20084
IFlvbmc= 20085
IHJlY2VpdmVy 20086
nOulvA== 20087
IGRpc3Nv 20088
IFFpbmc= 20089
IMOpdg== 20090
IG3DunNpY2E= 20091
IGFlc3RoZXRpYw== 20092
IEJyZWF0 20093
IFRB 20094
IGFjY3VyYXRlbHk= 20095
P+KAiw== 20096
IHdhZ2Vz 20097
cmF3ZMSZ 20098
IHN3YWxsb3c= 20099
IGNvbXBsYWludA== 20100
IGxpZWQ= 20101
YmVjdWU= 20102
IHJlbGF4aW5n 20103
IFBva8OpbW9u 20104
IHRlY24= 20105
YmFuZw== 20106
s7Ts 20107
IHF1aWVu 20108
0L3QvtC80YM= 20109
IGhhYml0YXQ= 20110
Li4uLi4u 20111
YWJsaW5n 20112
INGC0LDQutC40LU= 20113
IGJlc29uZA== 20114
IGVtcGxveWVk 20115
IGFycml2ZXM= 20116
IHZlc3NlbHM= 20117
IEF4 20118
IGRpc3BsYXlz 20119
MTUw 20120
b2xvZ2ll 20121
IOyXkA== 20122
IGNsbw== 20123
INC00L7Qsg== 20124
INCe0LQ= 20125
IHZ1ZWw= 20126
6Iqx 20127
d2VuZA== 20128
IHNsaXBw 20129
dXJw 20130
IExvdA== 20131
IGJ1bGxldHM= 20132
IHJhZ2U= 20133
IHNraXJ0 20134
aWVudGVz 20135
IG5o4buvbmc= 20136
IE5hdHVyYWw= 20137
IGhpbmQ= 20138
IHdvcmtsb2Fk 20139
bXU= 20140
7YOc 20141
IHN1bnNldA== 20142
0LLQvtC7 20143
cGl0 20144
5Y2B 20145
IEFTSA== 20146
IOu2hOuTpA== 20147
IGRvd25zdGFpcnM= 20148
6a0= 20149
IGNvdW50ZWQ= 20150
IG5heg== 20151
15XXpA== 20152
IFBoaWxpcHBpbmVz 20153
IDExMA== 20154
IFBhcmtlcg== 20155
IGdpdHU= 20156
IGludGVyZXM= 20157
IHVtYnJl 20158
IE5hdHVyZQ== 20159
IGplcg== 20160
ZW5vcw== 20161
IHBhbmVsaXN0cw== 20162
IGNvYXRpbmc= 20163
IGNoZXJyeQ== 20164
IFBlbnQ= 20165
IE1pc3Q= 20166
cmVnYXRpb24= 20167
IHZpbmQ= 20168
IENvcnBz 20169
IE1pc3Npb24= 20170
IG5vYmxl 20171
IGZvbmN0aW9u 20172
IHdhcnJpb3I= 20173
IHByb3Rlc3Rz 20174
b3VyaQ== 20175
IGNvbnN0aXR1dGlvbmFs 20176
xYJhbQ== 20177
IGVtZXJnZWQ= 20178
IGR5ZQ== 20179
IFRyeWluZw== 20180
aWdt 20181
5LiA5Liq 20182
w6lxdQ== 20183
TE8= 20184
IFZlcm0= 20185
ZXJ2aW5n 20186
IFRJTQ== 20187
IENp 20188
IGZyZWV6ZXI= 20189
IGdydXBv 20190
IFNwb3J0cw== 20191
INC/0YDQvtCz 20192
INmE2Kc= 20193
b3RoZXJhcA== 20194
aWZmYW55 20195
Ymlhbg== 20196
IHJhbmtlZA== 20197
IHByb3Bvc2Fscw== 20198
IMSRw6J5 20199
IGZyZWV6aW5n 20200
IGluc2VjdHM= 20201
dmls 20202
IGNvbXBvc3Q= 20203
546w 20204
IHNlbWFuYQ== 20205
IGRpc3Rpbmd1aXNo 20206
IGZhY2lsaXRhdGU= 20207
IHBsdXNpZXVycw== 20208
IHZlcmc= 20209
IGFsZ3Vucw== 20210
IFRpa1Rvaw== 20211
IEV4cHJlc3M= 20212
0LzQtdC90YI= 20213
U1U= 20214
IGludGltYXRl 20215
IEF1dGhvcg== 20216
IHdpdG5lc3Nlcw== 20217
IGthbGF1 20218
IGFyZ3VlZA== 20219
IGF2b2lkaW5n 20220
Y3RpdmU= 20221
IHB1cnN1aW5n 20222
IHN5bGw= 20223
w6F2ZWw= 20224
IEF0bGFudGE= 20225
IFV0YWg= 20226
IFRpbGw= 20227
IGVyZg== 20228
IDIwMjI= 20229
w6R0ZXI= 20230
IGZ1bmVyYWw= 20231
IEZsYXNo 20232
IEF0bGFudGlj 20233
IGdlbGU= 20234
7KaI 20235
IG1vcnRnYWdl 20236
IOuEmA== 20237
bGljaHQ= 20238
IGFtYml0aW91cw== 20239
IEJlaWppbmc= 20240
IGRpdmluZw== 20241
IHVuYm94 20242
aWxsYXM= 20243
IG90cmFz 20244
IGV2YWM= 20245
IG1hcmluZQ== 20246
INGB0L7Qt9C0 20247
IENyZWF0ZQ== 20248
IGdq 20249
IGZyZXF1ZW5jaWVz 20250
aW5ndG9u 20251
IFJvbWFucw== 20252
IGFpbWluZw== 20253
IEJ1ZmY= 20254
IGVtcGVyb3I= 20255
IE1vaQ== 20256
IHByb21pc2luZw== 20257
44Gc 20258
IGFsZ3VtYQ== 20259
IHBhc2E= 20260
IGRpc29yZGVycw== 20261
U0k= 20262
IHN1Y2NlZWRlZA== 20263
IGN1ZXJwbw== 20264
IHNvZGl1bQ== 20265
IHN0dWI= 20266
aGVpcm8= 20267
IGRlbGF5ZWQ= 20268
ZXRlcmE= 20269
dHc= 20270
IHN5bmM= 20271
aGQ= 20272
IHRvdXJpc3Rz 20273
IHN5c3Q= 20274
IG3DqXQ= 20275
IHF1YWxpZnk= 20276
IE90aGVycw== 20277
bGxlcnM= 20278
0LDRgtC10LvRjNC90L4= 20279
INCe0L3QsA== 20280
IHBlcmNlaXZl 20281
IOqygA== 20282
IOqwgOyepQ== 20283
INC40YHQug== 20284
IE1hdHRlcg== 20285
IEJsdWV0b290aA== 20286
IHBlYXJs 20287
IGFyaXNl 20288
IG1vbnVtZW50 20289
INC40LzQtdC90L3Qvg== 20290
YWdp 20291
2YTZig== 20292
IHJobw== 20293
IHNtYXJ0ZXI= 20294
IGNvbmo= 20295
0L7QutCw 20296
IGtlZW4= 20297
IFRyZWF0 20298
0LrQu9GO0Yc= 20299
IHBhY2tldA== 20300
ZWxzaXVz 20301
IEFsYWI= 20302
0LjQvdC4 20303
IHBzaQ== 20304
IGVuam95YWJsZQ== 20305
IEVsbGVu 20306
INCy0Lw= 20307
IGVsaW1pbmF0ZWQ= 20308
IFJvdw== 20309
IHpvbWJpZQ== 20310
IEt1 20311
IHBocmFzZXM= 20312
IGdyZW4= 20313
dXRlcg== 20314
IGRpcmVrdA== 20315
15Y= 20316
ZW5lbg== 20317
dXNh 20318
INGB0LvQvtCy 20319
xLA= 20320
IEdo 20321
IGNvcnJpZA== 20322
IHF1ZWVy 20323
IExpbmRh 20324
IG9uYQ== 20325
IG9ibGlnYXRpb24= 20326
ZGFy 20327
INi1 20328
ZW1tZW50 20329
YWNpZXM= 20330
IHNjcmV3ZWQ= 20331
IG5haw== 20332
IGF5dWQ= 20333
5LiU 20334
w6Fy 20335
bGV6 20336
IGRyb3du 20337
IE1lZGljaW5l 20338
IGxhYnM= 20339
IGp1c3F1 20340
IEdvbm5h 20341
IHRlcnJvcmlzdA== 20342
cXVlc3Q= 20343
IGZhcnRoZXI= 20344
IHJlcGxpZWQ= 20345
IFNX 20346
IE1pc3Npc3NpcHBp 20347
aXNobmE= 20348
IGhvbGRlcg== 20349
IHJlaWdu 20350
IGFjY2VwdGFuY2U= 20351
IHVs 20352
tow= 20353
IEhvdGVs 20354
IENvb3Blcg== 20355
dGFu 20356
IEdyYWI= 20357
IHZhcG9y 20358
IGFjdGVk 20359
IEthbmc= 20360
ZmFu 20361
IOydtOyDgQ== 20362
55Sa6bq8 20363
dXRldA== 20364
IHdvcmR0 20365
IGZhcm1z 20366
ZGF0 20367
IGNvdXBsZXM= 20368
IGJlYWRz 20369
aWVudG9z 20370
VGhlbg== 20371
5L+C 20372
b3NpdHk= 20373
IFN0YW5mb3Jk 20374
Li0= 20375
V2FpdA== 20376
IGRhdGFz 20377
b2lyZQ== 20378
IGhhc2h0YWc= 20379
aW1tZQ== 20380
IGVuY291bnRlcmVk 20381
IHNob3V0aW5n 20382
IHJlc2lzdGFudA== 20383
IFNldW5n 20384
IHRyYWdpYw== 20385
IERyYXc= 20386
LCw= 20387
IHNob3djYXNl 20388
IEFG 20389
IFN0cmk= 20390
IGJhY2tlZA== 20391
INGD0LM= 20392
INCx0YPQtNGD0YI= 20393
IENvbGU= 20394
ZXVycw== 20395
KD8p 20396
IGVzY2FwZWQ= 20397
QVNU 20398
IEFzc2VtYmx5 20399
IHN0aWNrZXI= 20400
IG1pZXV4 20401
IGVudGVydGFpbmluZw== 20402
IERPTg== 20403
IEFtZW5k 20404
IEthcmw= 20405
IGluaGli 20406
c3N0 20407
aWVn 20408
fn5+ 20409
IGhvb2tlZA== 20410
IGxpdGVyYWw= 20411
IHN1bm55 20412
c3RlcHM= 20413
IOuwnOs= 20414
IE1hcmluZQ== 20415
IHN1ZQ== 20416
IHByaXNvbmVycw== 20417
IEVi 20418
NTg= 20419
IGRydW1z 20420
IGd1aWx0 20421
YWxn 20422
IGhhcHBpZXI= 20423
IENN 20424
IOyVhOuLiOyVvA== 20425
INCf0LXRgA== 20426
0YPQu9GP 20427
IGtleXdvcmQ= 20428
IFBhcmNl 20429
IEZvcmVpZ24= 20430
IEFtYW5kYQ== 20431
56We 20432
IOuqqQ== 20433
cGxlc3M= 20434
iKw= 20435
w7Ntbw== 20436
IHF1YWxxdWVy 20437
7J2065286rOg 20438
IGNvbnNwaXJhY3k= 20439
IHN0cmF3YmVycnk= 20440
IGhhdHRlbg== 20441
RXM= 20442
IHNwb3M= 20443
IHZpbGxhZ2Vz 20444
IGxldg== 20445
INGB0YDQtdC0 20446
IHdha2luZw== 20447
IGNhbGN1bGF0aW9ucw== 20448
INmF2Lk= 20449
IHBvdXJpbmc= 20450
IGxlYmlo 20451
IHBvbGlzaA== 20452
IFRvdXQ= 20453
IGZ1bmt0aW9u 20454
0LzQvg== 20455
IFRp 20456
IHdhc3Rpbmc= 20457
aXN0aWNhbGx5 20458
IG1hbmlwdWxhdGU= 20459
IHNpbXBsaWZ5 20460
IHRlYW1tYXRlcw== 20461
INCx0L4= 20462
IGNvbnRhbQ== 20463
IFF1aXRl 20464
IGt1cno= 20465
IENhbmQ= 20466
dHlwZQ== 20467
b3V0aGVhc3Q= 20468
IGZpbmFuY2lhbGx5 20469
0L7Qu9C9 20470
ZWxzb24= 20471
IGZvcmVoZWFk 20472
dWFnZQ== 20473
bmF1ZGlibGU= 20474
IEJlaGluZA== 20475
IG5lZ290aWF0aW9ucw== 20476
IOuniOydjA== 20477
IGFsdGVybmF0aXZlcw== 20478
cmFuaw== 20479
aG9sZGVy 20480
5oeJ 20481
IGhlYWxlZA== 20482
0YLQvtGH 20483
IFNwZWM= 20484
5Lu2 20485
5LuW5YCR 20486
IGV4aGliaXQ= 20487
IHNoYWxsb3c= 20488
IGdvYg== 20489
IOuc 20490
IGZydXN0cmF0aW9u 20491
w61v 20492
IG1lbHRpbmc= 20493
IFN0b3Jt 20494
IHBhdGVudA== 20495
IEJhcmNlbA== 20496
IHBlZGVzdA== 20497
2YjZhQ== 20498
IHRhaQ== 20499
IE1vZGU= 20500
IHdpbA== 20501
IOuqqOultA== 20502
IMOpZ2FsZW1lbnQ= 20503
6YKj6bq8 20504
INeQ15c= 20505
YXlhbg== 20506
IGFtYXplZA== 20507
7KeA64qU 20508
IGhhY2llbmRv 20509
IOydtOyVvA== 20510
zrvOsQ== 20511
4LiC 20512
0LXRgtCw 20513
IGV4YW1z 20514
IHRyYXZlbGxpbmc= 20515
UHJlc3M= 20516
0LjRgNGD 20517
IGJhc2VsaW5l 20518
IGJ1c2Vz 20519
IHJlaW5mb3I= 20520
dmVuYW50 20521
IFRydXRo 20522
nb0= 20523
b2Jl 20524
IHllbGw= 20525
IHNhdXNhZ2U= 20526
VEY= 20527
IEV2aWw= 20528
IG1laW5lcg== 20529
15nXpw== 20530
IGhvcGVmdWw= 20531
IHLDs3duaWXFvA== 20532
IFBlcsOy 20533
dHdv 20534
bmRlcg== 20535
INC80LjRgA== 20536
IGNvbnNjaWVuY2U= 20537
IFdhcnJlbg== 20538
aWNreQ== 20539
IGFpbWVk 20540
IGfDtnJh 20541
WFQ= 20542
IHB5cmFt 20543
UmVk 20544
6Zu7 20545
YXR1 20546
IEVzdGE= 20547
IGVhcm5pbmdz 20548
IGhhdHM= 20549
IFN0YWR0 20550
aWNrZXQ= 20551
cG9pbnRz 20552
aW5hbmRlcg== 20553
IG1vdG9yY3ljbGU= 20554
IOuPjA== 20555
IO2VtOyVvA== 20556
a29t 20557
IERpbmc= 20558
5pI= 20559
IHJlY3Vycw== 20560
IGVzdGltYXRlcw== 20561
IGRlcm5p 20562
IHZlcnNjaA== 20563
44Gd44Gu 20564
IE1JQw== 20565
0LjQstCw0YLRjA== 20566
INC/0YDQvtGI 20567
IGRvc3Q= 20568
INCy0YHRgtGA 20569
IHdpZWw= 20570
IHNpYmxpbmdz 20571
INC00LXQsg== 20572
IGVhcmxpZXN0 20573
IGZhdGlndWU= 20574
IG5oaQ== 20575
IGd1c3Rh 20576
IGJvbm5l 20577
5pyA5b6M 20578
ZnJvbQ== 20579
IEplbm55 20580
IHN1cHBvc2VkbHk= 20581
aW50YWdl 20582
IGNvdW50aWVz 20583
IHVucmU= 20584
IHBsYW50aW5n 20585
IEdyYWM= 20586
IEdlbmVzaXM= 20587
IEFscGhh 20588
eXN6 20589
IHRpbGU= 20590
IOqyveyasA== 20591
INeZ16k= 20592
cXVlbA== 20593
IGRpc3RyaWJ1dGU= 20594
ZGVm 20595
w6lyYWw= 20596
IGNsdXRjaA== 20597
YWRlbHBo 20598
IFBsYXlTdGF0aW9u 20599
hLg= 20600
IHNq 20601
YnJlYWtpbmc= 20602
IOuQmOs= 20603
IEN1YmE= 20604
IFJ1c3NpYW5z 20605
IE1BUks= 20606
IHBlcnNl 20607
IHJlc3RyaWN0ZWQ= 20608
aWdlcw== 20609
IFRyYXZlbA== 20610
IGVsZWN0cm9uaWNz 20611
IHF1YXJ0ZXJz 20612
IEtlaXRo 20613
c2l6ZWQ= 20614
IGRlYWRsaW5l 20615
YXJlbnRo 20616
IHbDrWRlb3M= 20617
IHByb3RvY29scw== 20618
YW1tZW50 20619
IFRyYWluaW5n 20620
IMOi 20621
IHNlcXVlbA== 20622
0L3QsNC6 20623
IGtlaW5lbg== 20624
IG1hdHRyZXNz 20625
bHVkaW5n 20626
IGNsYXNzaWZpZWQ= 20627
IHJlYWN0b3I= 20628
IEtvbnQ= 20629
IHBhc3Nhcg== 20630
IGhvbm91cg== 20631
b3JpZw== 20632
SU5B 20633
IE5hdGhhbg== 20634
0LLQsA== 20635
INGB0LrQsNC30LDRgtGM 20636
dMSxcg== 20637
IGV4Y2x1c2l2ZWx5 20638
IHNoYWRlcw== 20639
INC/0YDQvtGG 20640
IG9jY2FzaW9ucw== 20641
aWph 20642
55qE5pmC5YCZ 20643
5Y6y 20644
5oWi 20645
Zmln 20646
IHR1cw== 20647
IHJlbWVt 20648
IENocmlzdG9waGVy 20649
IHNsaW1l 20650
IGFsZ3VuYQ== 20651
IEZvcnR1bmF0ZWx5 20652
IGxvcnM= 20653
dm9sbA== 20654
YXZlcg== 20655
IG91dGxldA== 20656
IExpbmtlZElu 20657
IEV4ZWN1dGl2ZQ== 20658
IG9yZ2Fucw== 20659
IEJlZ2lu 20660
IO2ZlA== 20661
IHRyYW5zcGxhbnQ= 20662
cmFnZW4= 20663
Vk8= 20664
IEbDtnI= 20665
INio2KfZhA== 20666
IEFuZHJl 20667
aXNpbmU= 20668
IGxhc3Rz 20669
IGhpc3TDs3JpYQ== 20670
IGx1eg== 20671
IGNvbGxhcg== 20672
IGtpZG5h 20673
IG9wdGljYWw= 20674
aW92 20675
IHRvYg== 20676
IGV4dGVyaW9y 20677
IG1ldHJpYw== 20678
aWV1cg== 20679
IHRyb2xs 20680
INGA0L7Qtw== 20681
5pif 20682
IHTDtA== 20683
IOyYiOyB 20684
IEdlc2V0eg== 20685
INC10LQ= 20686
IGRlbm9taW5hdG9y 20687
7LM= 20688
IGxldHQ= 20689
5YWJ 20690
IGdyw7bDnw== 20691
6aGY 20692
IEx1dGhlcg== 20693
IHJlc3Rl 20694
IHJlc2VtYg== 20695
IHBlcm1ldA== 20696
a3Np 20697
IGZpc2hlcg== 20698
44Gf44GE 20699
IFZvbg== 20700
7ZS8 20701
IM+Dz4TOvw== 20702
IGxvY2tz 20703
IHNob290cw== 20704
IGthbXU= 20705
IEtlcg== 20706
IE9icw== 20707
552A 20708
IGJpbGk= 20709
IOuwsQ== 20710
IHRvcnR1cmU= 20711
YXNzeQ== 20712
INC40LM= 20713
IGxhc3Rpbmc= 20714
5aW955qE 20715
IHRpZW5lcw== 20716
IHJlY2VpdmVz 20717
IE9zY2Fy 20718
IHJlbWVtYmVyaW5n 20719
IHByb2JsZW1hcw== 20720
IGlh 20721
5Zib 20722
IG1lbW9yYWJsZQ== 20723
IGpvdXJz 20724
IGZhw6dvbg== 20725
YW1pYw== 20726
IOu0pA== 20727
YXRpcXVl 20728
IOutlOqwgA== 20729
IHppcA== 20730
aGFsdA== 20731
IPCfmA== 20732
IGZyaWVz 20733
IGZpbmRlbg== 20734
Z3Jh 20735
0YDRg9C0 20736
aW1wb3J0 20737
IOuLrOs= 20738
IGlraQ== 20739
IGNvbXBsYWluaW5n 20740
IGZhemVuZG8= 20741
IGdvb2dsZQ== 20742
IHRhYnM= 20743
IOuTpOyWtOw= 20744
44Km 20745
dWdv 20746
aWVydG8= 20747
YXVmZW4= 20748
IOuovOyggA== 20749
IHNrdWxsZQ== 20750
IHN1aXY= 20751
IHNweQ== 20752
IEthaQ== 20753
6YKj5YCL 20754
IG1hcnRpYWw= 20755
IG9uZGVy 20756
6Kqw 20757
YXRpbGl0eQ== 20758
IGlyZ2VuZHdpZQ== 20759
IGNsYXA= 20760
aW50ZWxs 20761
IGluc3RhbGxpbmc= 20762
IHVuaXF1 20763
IENlbnRyZQ== 20764
YXN0cw== 20765
dWFy 20766
IHJldmlz 20767
IHRocmVhdGVuaW5n 20768
cmFpcw== 20769
IGN1aWQ= 20770
c2th 20771
IHJlc29sdmVk 20772
IHJpZGVz 20773
IGZhaWx1cmVz 20774
IHNlbWI= 20775
IG1hbGVz 20776
VUZG 20777
5b6I5aSa 20778
IHRyw6pz 20779
YXBwZWQ= 20780
IG5ld3NwYXBlcnM= 20781
cmlldA== 20782
IGFwcGxhdWRz 20783
0JM= 20784
IOOBrw== 20785
IE5D 20786
5Y2D 20787
5pmC6ZaT 20788
IGhldGVy 20789
IGhhemFyZA== 20790
IHJ5 20791
IHN0cmljdGx5 20792
IDU0 20793
IOuTpOyWtOqwgA== 20794
IHNwb250 20795
IHRhdHPDpGNobGljaA== 20796
IOunkOyU 20797
bGF1Yg== 20798
IGFic29yYmVk 20799
YWNhxJ/EsXo= 20800
IG9udQ== 20801
INCQ0L0= 20802
IGV4cGxpY2l0bHk= 20803
IOyerA== 20804
IEZ1dHVyZQ== 20805
YWNodGVu 20806
w6Bv 20807
eW9u 20808
IHNlcmlh 20809
IEhlcnJlbg== 20810
Y2Vq 20811
IEFsYmVydA== 20812
7J2064qU 20813
ZWN0b3I= 20814
IHBhY2tpbmc= 20815
IHZpcnR1ZQ== 20816
IHZlbmly 20817
REQ= 20818
IHlheg== 20819
IGxvZ3M= 20820
IFBob3Rvc2hvcA== 20821
IHNpZA== 20822
bGluZ3M= 20823
IHJlbW90ZWx5 20824
IERpZmZlcmVudA== 20825
IG9wZXJhdGVk 20826
bGlnaHRz 20827
IGRpc2NyaW1pbg== 20828
aXN0YW5jZQ== 20829
IEdSRQ== 20830
IHBsYWM= 20831
IHNoaXJ0cw== 20832
IGp1c3RpZnk= 20833
IHRyYWJhbGhv 20834
dXRpbA== 20835
dm9j 20836
IHF1YXJ0 20837
IM6k 20838
U0M= 20839
IFNS 20840
IC0i 20841
IGhlc2l0YXRl 20842
IHBhaw== 20843
6Iez 20844
Z3Vh 20845
Sm8= 20846
IHNvdXZlbnQ= 20847
IEFuZ2VsYQ== 20848
ZXNzZWU= 20849
YWRlbHBoaWE= 20850
YXJrcw== 20851
IHdlZWQ= 20852
IGthbm5zdA== 20853
5YKZ 20854
IOq3uOufrOuLiOq5jA== 20855
IHBsdXTDtHQ= 20856
IENvbW1hbmRlcg== 20857
IHN1bW1hcml6ZQ== 20858
4K+A 20859
IDk4 20860
44GH 20861
IGRldmVsb3BtZW50cw== 20862
IENvc3Q= 20863
IHRoZW9yZXRpY2Fs 20864
IG9yZQ== 20865
IG1ldGFsbA== 20866
zr/Phc69 20867
ZmFocg== 20868
0JrQkA== 20869
IGNodWNr 20870
IGFkYXB0ZWQ= 20871
IE9rbGFo 20872
IE5ldGhlcmxhbmRz 20873
IHBvZXQ= 20874
c3Rv 20875
a2F0 20876
IHdlYXJz 20877
568= 20878
IOyWtOuUlA== 20879
IEVzdG8= 20880
IGxhdWdoZWQ= 20881
IGRvbm5lcg== 20882
IOuNsA== 20883
IOybkOs= 20884
b2N1cg== 20885
IEtpY2s= 20886
IERldHJvaXQ= 20887
IGJpY3ljbGU= 20888
IGxhY2tpbmc= 20889
cGhhYmV0 20890
IEtlbmQ= 20891
QXNz 20892
IHJldmVhbHM= 20893
IM6g 20894
IE5vYWg= 20895
pqzripQ= 20896
IHNlbGxz 20897
IEFsYWJhbWE= 20898
IHRlcnJpZmlj 20899
IEVsZW1lbnQ= 20900
IO2G 20901
IHR1cmJv 20902
IEhvbQ== 20903
IHRoZW9yZW0= 20904
IGFkdmVudHVyZXM= 20905
IHB1cmNoYXNpbmc= 20906
IFTDoQ== 20907
INC80LDRgg== 20908
IHZlbW9z 20909
IGR1dGllcw== 20910
IHdlbmln 20911
IGJvb3Ro 20912
IGVudHJhcg== 20913
VkE= 20914
IGdlYXJz 20915
IEphZQ== 20916
w6hu 20917
IGNhbGNpdW0= 20918
IFJvYmVydHM= 20919
INC/0YDQvtCx0LvQtdC8 20920
IHJpYmJvbg== 20921
INC90LDQt9GL0LI= 20922
IGxhdg== 20923
IGludGVydmVudGlvbnM= 20924
IFVsdHJh 20925
IG5hbWVseQ== 20926
IGFkZXF1YXRl 20927
IHJlY2Fw 20928
IGRvY2s= 20929
ZnRpbmc= 20930
IHZvaQ== 20931
IGNvbnN1bHRhdGlvbg== 20932
INGB0LXQvA== 20933
IHBvZGVt 20934
IHBvc3Nlc3Npb24= 20935
IGNsdWVz 20936
IFJ1c3NlbGw= 20937
IHJlbmV3YWJsZQ== 20938
5Y6y5a6z 20939
INGD0Lc= 20940
aW5mb3JtYXRpb24= 20941
aWdnZXJz 20942
V2l0aA== 20943
d25v 20944
IGVsYWJvcmF0ZQ== 20945
Y3RvcmFs 20946
IERvdw== 20947
IHJhbWVu 20948
5o+Q 20949
4buV 20950
IGVyc3Rl 20951
IFplbA== 20952
44OX 20953
IHF1YXNp 20954
INC90LDQug== 20955
56eS 20956
IFN0YXJz 20957
IHRyaWJhbA== 20958
IHNlYXRlZA== 20959
IHdvbA== 20960
IGNob2w= 20961
w6Rtw6Q= 20962
IG91dGJyZWFr 20963
IGNyZXM= 20964
IHVuc2VyZXI= 20965
IO2RnA== 20966
IHVuZGVyd2F0ZXI= 20967
IGFzc3VyZQ== 20968
T09E 20969
IG5hcHJhd2TEmQ== 20970
IGVzdGFibGlzaG1lbnQ= 20971
IGluY29u 20972
IGRpZmVyZW50ZQ== 20973
IGV4Y3Vz 20974
IERpbQ== 20975
0L7RhQ== 20976
IExpbmc= 20977
cm9sb2c= 20978
IOOBvg== 20979
IG91dGRvb3Jz 20980
bmFq 20981
IGVwaWRlbWlj 20982
IHVudGVycw== 20983
IDMwMDA= 20984
IEdhYnJpZWw= 20985
IOyXhuuKlA== 20986
IGVuY2w= 20987
IE9kZXI= 20988
IEZvb3Q= 20989
cGFz 20990
IFp1aw== 20991
5ZOh 20992
IHdvcmtmbG93 20993
IHVucA== 20994
IGFsbGlhbmNl 20995
ZW5zY2hhZnQ= 20996
IHlvZ3VydA== 20997
0LjQvdC1 20998
IGVydQ== 20999
IGZpeg== 21000
5LqU 21001
IGHFnw== 21002
IGFwcmVuZA== 21003
IGN1YWxxdWllcg== 21004
IGNhcnJvdHM= 21005
xLFuxLFu 21006
YWZvb2Q= 21007
IGZsb29ycw== 21008
IGtleXdvcmRz 21009
IHNwb3R0ZWQ= 21010
IGRyYW5r 21011
IHBhcmFz 21012
IMO6bHRpbW8= 21013
IGhhYmxhcg== 21014
IHByb3NlY3V0 21015
7JeQ64+E 21016
6ZaL5aeL 21017
IMOpcA== 21018
IHN0aWNrZXJz 21019
IHB1c2hlcw== 21020
a2g= 21021
IHJlc3RhcnQ= 21022
IFRodW5kZXI= 21023
4budaQ== 21024
IG11aXRh 21025
IGZveA== 21026
YXJkZcWf 21027
IFphY2g= 21028
IE1pbmVjcmFmdA== 21029
57g= 21030
ID09PT0= 21031
IGfDtnJl 21032
IHN0YW5jZQ== 21033
aWd1bmc= 21034
2Y7ZkQ== 21035
a8Ok 21036
IHRlYWNoaW5ncw== 21037
6YY= 21038
IGRlY2F5 21039
IHJpYw== 21040
b21lbmE= 21041
INCy0YHQtdC8 21042
Y2h0ZW4= 21043
IFZlcnQ= 21044
IO2VnOq1rQ== 21045
rLTr 21046
IGNvYw== 21047
Oik= 21048
a2VpdGVu 21049
IEJB 21050
ZXRoZWxlc3M= 21051
IGhlYWRxdWFydGVycw== 21052
IHNwaWtl 21053
IEJhc2U= 21054
IDEwMQ== 21055
IGNvb3JkaW5hdGVz 21056
IHRhcmQ= 21057
IGJvaWxlZA== 21058
IE1vbnN0ZXI= 21059
IG5vdGVib29r 21060
IOq0gA== 21061
IFdha2U= 21062
IFNldHRpbmc= 21063
7J207Jc= 21064
IFN5ZG5leQ== 21065
IEZpbm4= 21066
IGxvYmJ5 21067
5b6e 21068
IHNlbmlvcnM= 21069
0L3QuNGF 21070
YXZhbg== 21071
IEpF 21072
IHRyYWZm 21073
dGhpbms= 21074
IHNsYXA= 21075
IENhc3RsZQ== 21076
qW5n 21077
IGFsZ3Vub3M= 21078
IFBlcnNvbmFsbHk= 21079
IE1hbGU= 21080
7Yuw 21081
IEdlbmVyYWxseQ== 21082
IFBlbA== 21083
IGRpYXM= 21084
IGV2b2x2aW5n 21085
aXRvbA== 21086
0LLQvtGA 21087
IHBsZWlu 21088
IGZsaWdodHM= 21089
IGVsZXZlbg== 21090
b3dlag== 21091
4buRbmc= 21092
IGFrdQ== 21093
IGdsYW5jZQ== 21094
IGNvbm5lY3Rpdml0eQ== 21095
IGJhbGQ= 21096
0YvRhw== 21097
IGludGVzdA== 21098
w6Fn 21099
IEdSw5w= 21100
aWJsaWNhbA== 21101
IFBhcGE= 21102
IHBpdHk= 21103
IGZhaW50 21104
IHd1cmRlbg== 21105
IGxlZ2FsbHk= 21106
IHByZXk= 21107
IFNjaWVuY2Vz 21108
INC/0YDQvtGB 21109
IHRyYWluZXI= 21110
IHByb2Jsw6htZQ== 21111
IGtpbG8= 21112
0LrQvtCz0L4= 21113
IGJyaWRnZXM= 21114
ODk= 21115
IGxhc3RlZA== 21116
IGVsZWdhbnQ= 21117
Ym93cw== 21118
IHBhbGFi 21119
IGRpcmVjdG9yeQ== 21120
5LiN5pyD 21121
IGJ1bGI= 21122
cGVvcGxl 21123
SVg= 21124
IGdlYg== 21125
IDY2 21126
IFRlbm5lc3NlZQ== 21127
YWhsZW4= 21128
aWV2YWw= 21129
IGNhdXQ= 21130
IERhbWVu 21131
cGxv 21132
aWFuZQ== 21133
0LDQu9C1 21134
YXR0YW4= 21135
INin2YTYsw== 21136
IHJpc2t5 21137
IHNsZWV2ZQ== 21138
IGluY2lkZW50cw== 21139
IOuwlQ== 21140
Q28= 21141
IGFwcGxpY2FibGU= 21142
IGltcGVyaWFs 21143
IFBoaWxpcA== 21144
IFllYQ== 21145
0LXRgNC+ 21146
INC/0L7QutCw0Lc= 21147
w7xuZQ== 21148
7JiA 21149
SHVi 21150
dG9y 21151
IHNpZ3U= 21152
Y2VuZA== 21153
IHBvbGl0aWNhbGx5 21154
IOyCtA== 21155
IHBhcnM= 21156
IG91dg== 21157
IHByaW1laXJh 21158
IFNoYWg= 21159
IHNhdHVy 21160
IGNvbWJ1c3Q= 21161
IHByb21vdGVk 21162
7KO86w== 21163
5oCV 21164
IHRlbXBsYXRlcw== 21165
IOuLrA== 21166
IGhhdWw= 21167
INGC0LXRgA== 21168
IHNsaWRpbmc= 21169
Y2VkZW50ZWQ= 21170
IOOBrg== 21171
Y2hpbGRyZW4= 21172
TVI= 21173
IFdlaQ== 21174
IGLDtnI= 21175
5pep 21176
IHByw7N4aW1v 21177
YXLDrWE= 21178
IHNhbXBsaW5n 21179
0LXQu9C10L0= 21180
ZXNp 21181
IERhbmllbGxl 21182
IE9rbGFob21h 21183
6IU= 21184
55WM 21185
0LXRgdC/ 21186
IERWRA== 21187
INCy0YvQvw== 21188
cm91cw== 21189
Y29ucw== 21190
IGVuaGFuY2Vk 21191
6Zuj 21192
IHBhc3Rvcg== 21193
IFN1ZGRlbmx5 21194
6K6T 21195
ZmFy 21196
UEVS 21197
IE5n 21198
MTAwMA== 21199
IGNoZXc= 21200
IHJ1bW9ycw== 21201
IEFuYQ== 21202
IGFubsOpZXM= 21203
INGD0YHRgg== 21204
IFBoaWxhZGVscGhpYQ== 21205
5Zev 21206
0LXQttC00YM= 21207
IGVmZmVjdGl2ZW5lc3M= 21208
6L+Z5qC3 21209
w6l0w6k= 21210
IGRpbmc= 21211
IHJlbGlnaW9ucw== 21212
IGFnZWQ= 21213
emllxIc= 21214
IFJpYw== 21215
IEthcA== 21216
IFBhZ2U= 21217
IHPDvA== 21218
IG7DpG1saWNo 21219
IG1hbmtpbmQ= 21220
IHJlc3Rpbmc= 21221
IGluZmx1ZW5jZXM= 21222
IFNjaHVs 21223
INC90LXQsg== 21224
IG1hbmE= 21225
IGNvbnN1bWVk 21226
IFBvbQ== 21227
576O5ZyL 21228
IGNvbnNlZ3Vpcg== 21229
IFRoYW5rc2dpdmluZw== 21230
IEhpbmR1 21231
bGFpcw== 21232
IHRocml2ZQ== 21233
IGNvbnRvdXI= 21234
0LDRhtC40Y8= 21235
IGZhbGFuZG8= 21236
IErDoQ== 21237
emFu 21238
0LjRgtGD 21239
aXBoZXI= 21240
amFtaW4= 21241
IEhhbGxv 21242
IDE2MA== 21243
INC+0YHQvtCx 21244
IG1ldGU= 21245
IOyVjOs= 21246
IEJhcmNlbG9uYQ== 21247
bGV0dGVy 21248
INCd0LXRgg== 21249
5Zk= 21250
IGFkZW3DoXM= 21251
IGNvb3JkaW5hdGlvbg== 21252
dW50cw== 21253
IHNsb3A= 21254
INC/0YDQuNC0 21255
7KeA66eJ 21256
IHF1ZXN0aW9uaW5n 21257
IGRpZXNlbA== 21258
IGRlag== 21259
IGFmZmlybQ== 21260
jZTrnbzqs6DsmpQ= 21261
aWVubmU= 21262
IGNyYW5r 21263
IHByZWRpY3Rpb25z 21264
IHBoeXNp 21265
Y2hzZWw= 21266
IGNvbWJpbmF0aW9ucw== 21267
IGV4Y2VsbGVuY2U= 21268
6YCZ6bq8 21269
4bud 21270
d2lkdGg= 21271
d2VlZA== 21272
hOulvA== 21273
hOuniA== 21274
IGFsdG8= 21275
IGRhaXJ5 21276
IE5vcm1hbA== 21277
cHBlbg== 21278
IG9iZW4= 21279
IGRldmFzdGF0aW5n 21280
IHBveg== 21281
IEh1cw== 21282
bWF6 21283
IHdhcm5lZA== 21284
IGRlbms= 21285
IEF1c3M= 21286
IHRyYWRlcw== 21287
aGVsbA== 21288
IHByaW1lcm8= 21289
IG1pYQ== 21290
0LLQsNGA 21291
2KjZig== 21292
IGtpY2tz 21293
IGHEnw== 21294
IE3DvA== 21295
IGx1Yw== 21296
0LXQvdC40LXQvA== 21297
IFN0YW5kYXJk 21298
cmljZQ== 21299
IEN1Yg== 21300
IGdvdQ== 21301
IEpvw6Nv 21302
0YPRgdC6 21303
IGVucXU= 21304
o4w= 21305
Z2V3 21306
IO2BsA== 21307
b3dhbmlh 21308
aWFuaQ== 21309
IGZha3Q= 21310
0Y/QvdC4 21311
IGJlZg== 21312
IHRodW1ibmE= 21313
IGNldXg= 21314
5q2h6L+O 21315
YXBwbGU= 21316
TkVO 21317
IGdhZA== 21318
YXBvbg== 21319
IEZhbnRhc3RpYw== 21320
IGNvbmNlbnRyYXRlZA== 21321
Z2lybA== 21322
bGVuZQ== 21323
INCU0LvRjw== 21324
IMOpdGE= 21325
YWFu 21326
IG91dHRh 21327
IG5hcmM= 21328
IEJvZHk= 21329
YnJ1c2g= 21330
IGxlZ2lzbGF0aXZl 21331
IE1lZ2Fu 21332
IG1pc3Rha2Vu 21333
IE1pc3NvdXJp 21334
IGxhYmVsZWQ= 21335
0LvRj9C10YLRgdGP 21336
IHJlYWxpc2Vk 21337
eW9yc3Vu 21338
44GC44KK44GM44Go44GG 21339
IFNhZmV0eQ== 21340
IGFjY2VsZXJhdGU= 21341
IHNhbmN0aW9ucw== 21342
IHBlZQ== 21343
IGp1ZWdv 21344
IHBlcHBlcnM= 21345
IHdhbA== 21346
6riJ 21347
ZWxsb3c= 21348
INC20LXQvQ== 21349
IGNpbmNv 21350
INGB0LjRgdGC 21351
Y292ZXJ5 21352
IGdyYW0= 21353
IMOpcG8= 21354
IEJNVw== 21355
aXZvbA== 21356
IENoZW0= 21357
55qE6Kmx 21358
dXNlbWVudA== 21359
IFN1cHBvc2U= 21360
IOqwgOyngOqzoA== 21361
IG1pbGxlbm4= 21362
IFR1bg== 21363
IG1lZGFs 21364
IGhhY2lh 21365
IHN0aW11bHVz 21366
IGJyaWdodG5lc3M= 21367
YWllbnQ= 21368
IEhhbmRz 21369
aW5ldA== 21370
IGNvYWxpdGlvbg== 21371
5a24 21372
IHJpc2Vz 21373
cmluYQ== 21374
IHNjb290 21375
IOOBpw== 21376
IGRlZmVuZGluZw== 21377
IGludmVycw== 21378
IGhpbGxz 21379
IGZ1bGZpbGxlZA== 21380
5Yiw5LqG 21381
bGxpZQ== 21382
IGFkb2xlcw== 21383
IENoYXNl 21384
5Zac5q2h 21385
IEpK 21386
IG5ldWVu 21387
IFRydQ== 21388
IGluaGVyaXQ= 21389
IHNpeHR5 21390
IEV4cA== 21391
IENsYXk= 21392
0L7RgdC+0LE= 21393
YXJuYQ== 21394
IEltcGVyaWFs 21395
INGN0YLQsA== 21396
IHNvY2lhbGx5 21397
YXR5 21398
b2R5bmFt 21399
IHJpYnM= 21400
b21pYw== 21401
IFRvbA== 21402
0L7Qu9C2 21403
IDE5OTg= 21404
IGZyYW0= 21405
IHJhbmtz 21406
INCx0YPQtNGD 21407
IENvbG9u 21408
SHo= 21409
IGFjY29tbW9kYXRl 21410
IGV4cGxvZGU= 21411
7YSw6w== 21412
SEFFTA== 21413
IEhhcnQ= 21414
INC20LjQt9C90Lg= 21415
5qE= 21416
IGRlbGljYXRl 21417
oNeX 21418
IHRvZnU= 21419
IGFjaGlldmVtZW50cw== 21420
IFNvcg== 21421
IGFncmVlbWVudHM= 21422
IDU3 21423
IHRhbXA= 21424
IGZyYW7Dp2Fpcw== 21425
IGhlcmJz 21426
Y29ybg== 21427
IGtvbms= 21428
QU5B 21429
IFFp 21430
IHByw7Nw 21431
IHRpZ2Vy 21432
IOuRmA== 21433
xINt 21434
IGFwcHJlbnQ= 21435
YWhhbg== 21436
IHJ1bGluZw== 21437
IHRzcA== 21438
IHR3aXR0ZXI= 21439
IHRlZW5hZ2Vy 21440
YnVz 21441
IO2S 21442
IEFtZW5kbWVudA== 21443
IHRhcHBpbmc= 21444
IEFQSXM= 21445
5aC0 21446
IG1hdGNoZWQ= 21447
66m0 21448
V0E= 21449
IEJlYXV0eQ== 21450
IGluZXZpdGFibGU= 21451
IGdhc2Vz 21452
INm+ 21453
aGlnaA== 21454
IE9wdA== 21455
IHByZWRvbWlu 21456
z4HPjA== 21457
IHR1YmVz 21458
IOyVoA== 21459
IEFh 21460
IOaciQ== 21461
b21ldG93bg== 21462
IElN 21463
IGRlc2Fy 21464
w6RyZW4= 21465
INC80LDRgQ== 21466
IE3DtmdsaWNo 21467
IHJlbnRhbA== 21468
IO2VqOq7mA== 21469
IERpYW5h 21470
IGF1dGlzbQ== 21471
IFB1ZXJ0bw== 21472
xLFsZA== 21473
IGZhbGFu 21474
IGRyZWFtaW5n 21475
IGd1dGU= 21476
INC60LDQvA== 21477
IHdyZWNr 21478
IHN0b3J5dGVsbGluZw== 21479
IExlZ2VuZA== 21480
IFVrcmFpbg== 21481
INC/0YDQvtC40YE= 21482
IFNL 21483
IO2WiQ== 21484
IMWbd2k= 21485
IEJlbGlldmU= 21486
IG1vc3RyYXI= 21487
IFRvZGQ= 21488
IE5pZ2Vy 21489
aWN0aW5n 21490
aGFyZA== 21491
Oi8v 21492
aXJhYmxl 21493
aWdhdGlvbg== 21494
IE1lbWJlcnM= 21495
IOygnO2SiA== 21496
IGRpc2NvdXI= 21497
n70= 21498
cmlrYQ== 21499
IERO 21500
IEZpZg== 21501
IENhcGl0YWw= 21502
0YDQvtC8 21503
IFNhbnM= 21504
eXVu 21505
IHBpbG90cw== 21506
IHRyYXQ= 21507
IG55dA== 21508
IOuvvA== 21509
IGV4cG9uZW50aWFs 21510
IGVtZXJnZQ== 21511
IHRyYWplY3Rvcnk= 21512
INC/0L7Rh9C10LzRgw== 21513
IHNlYWxlZA== 21514
YXR0aQ== 21515
IHdpZGVz 21516
INC+0LPRgA== 21517
aWFuY2Vz 21518
IHdpdG5lc3NlZA== 21519
T3I= 21520
b3Np 21521
IEpvZWw= 21522
b25hbA== 21523
6IG9 21524
IEludGU= 21525
Y2VkZXM= 21526
IEdvdHRh 21527
YW5pdW0= 21528
IGZlbWFsZXM= 21529
IExlYmVucw== 21530
IG1vaXN0dXI= 21531
IFNpbXBsZQ== 21532
IERvY2g= 21533
YXLDoQ== 21534
IGdlc2VoZW4= 21535
VVNU 21536
xqFp 21537
IGNsYXNzaWZpY2F0aW9u 21538
IGRpYWdvbmFs 21539
IHBlcm1ldHQ= 21540
Y29tcA== 21541
INin2YTYrQ== 21542
IE1hbGF5cw== 21543
IGdlaMO2cnQ= 21544
IHBvcHBlZA== 21545
IGNvbnRhY3RlZA== 21546
INeb15w= 21547
IDE0MA== 21548
IGFkYXB0YXRpb24= 21549
IG1hbnVz 21550
IHR1cmtleQ== 21551
IHByZWFjaA== 21552
YnJpZ2h0 21553
IGRvd25z 21554
IHVucHJlY2VkZW50ZWQ= 21555
IG1pZ2h0eQ== 21556
IGNhdGVy 21557
aXR0aQ== 21558
Z3M= 21559
IERlcHV0eQ== 21560
d3JpdGU= 21561
IEJsZXNz 21562
w6Fj 21563
IHN1bW1pdA== 21564
IOuPvOyalA== 21565
IHRob3VnaHRmdWw= 21566
IHNocmVk 21567
c2luZ2luZw== 21568
INC70YPRh9GI0LU= 21569
IHllbg== 21570
IHZpYnJhbnQ= 21571
IFdhbHRlcg== 21572
IGhvc3Rz 21573
IGFtYnVs 21574
IGludmFzaW9u 21575
b2dhbg== 21576
IHJlYXNvbmluZw== 21577
IHN1Y2M= 21578
0LvQtdC60YI= 21579
IGZhbGE= 21580
IGtpbmdz 21581
IGdvaW4= 21582
IGNhbGli 21583
IEdSw5xORU4= 21584
b3Rlcg== 21585
IGVpbno= 21586
IGluc3VsaW4= 21587
iqg= 21588
IHNjYWxpbmc= 21589
IENvcm4= 21590
aHlk 21591
IG1hdHRl 21592
UEw= 21593
IGFsaWVucw== 21594
IFNlZw== 21595
6K+d 21596
ZXN0aQ== 21597
YXN0aWNz 21598
IHdhcm1lcg== 21599
IGluZ2Vu 21600
IE1M 21601
IHJvZGU= 21602
IEV5ZQ== 21603
YmVpdHM= 21604
IEJhcm4= 21605
wrss 21606
IENodWNr 21607
IHByb2ZpdGFibGU= 21608
dWd1ZXNl 21609
IEFyYWJpYQ== 21610
IGNvY28= 21611
IHB1ZWRv 21612
IGluZmxhbW1hdGlvbg== 21613
Y2xpcA== 21614
IHRhYmxlc3Bvb25z 21615
IOygkQ== 21616
IFN3ZWQ= 21617
IGFuYXQ= 21618
7Iig 21619
IGFycmli 21620
IGRhbmNlcg== 21621
IENhcnRlcg== 21622
IG1hZ25pZmlj 21623
c3RvcmU= 21624
6YG4 21625
IGZhZGU= 21626
IGFjY29tcGFueQ== 21627
IHdhaHI= 21628
IHllYXN0 21629
IG1pbmVyYWw= 21630
IGxlZ2lzbGF0dXJl 21631
5L2P 21632
aXJvcw== 21633
IGNyb3dkZWQ= 21634
0YDQsNGI 21635
b2NhZG8= 21636
7Ja07JW8 21637
IO2bhA== 21638
IEJhcnJ5 21639
bWFzdGVy 21640
IG5pY2tuYW1l 21641
ICIuLi4= 21642
IFJz 21643
IE1vb3Jl 21644
IHZlbnVl 21645
INCx0YM= 21646
44Oh 21647
bGlob29k 21648
IEFnZW5jeQ== 21649
0LvQvtCy 21650
IGthaA== 21651
IOyGjOumrA== 21652
IG1hcnNo 21653
IGluY29ycG9yYXRlZA== 21654
YW50d29ydA== 21655
IGtpbWNoaQ== 21656
IHdvbw== 21657
IGRpc3RyYWN0ZWQ= 21658
ZXJpZXM= 21659
IGluZm9ybWFjacOzbg== 21660
IENob29zZQ== 21661
IEphZGk= 21662
IGFuYWxvZ3k= 21663
c2F5 21664
dWZmbGU= 21665
Ym9r 21666
IGFjaWRz 21667
IGFjcXVpc2l0aW9u 21668
IHZhcmlhbnRz 21669
6LW35L6G 21670
IHBhc3NpZXJ0 21671
7J2064KY 21672
cnVjdGl2ZQ== 21673
YnJpZw== 21674
IOOAjA== 21675
ZXBoZXI= 21676
IHBI 21677
dXRsaWNo 21678
5beu 21679
IHJlbGll 21680
dWl0ZQ== 21681
IHJlY2VwdGlvbg== 21682
IGNvaA== 21683
IFByZXA= 21684
IGFudGljaXBhdGU= 21685
5oCn 21686
a2Vl 21687
IGRlc2lnbmF0ZWQ= 21688
0Y/RgtC4 21689
IEtvcg== 21690
IEFuaW0= 21691
w7xobA== 21692
IFdoaXQ= 21693
IHVuY292ZXI= 21694
IE1heWE= 21695
INGC0L7Qs9C00LA= 21696
sJU= 21697
dXRlbmFudA== 21698
IOyWvOs= 21699
IGZvcmVzdHM= 21700
IG1lbWU= 21701
IGRpc3Rpbmd1aXNoZWQ= 21702
IE1hcng= 21703
IExpb24= 21704
IHNlcnZhbnRz 21705
IERpYW0= 21706
55W254S2 21707
IFBvbGljeQ== 21708
jbw= 21709
IHRyaWdnZXJlZA== 21710
YWJpbGly 21711
IOydkQ== 21712
IG5lZ290aWF0ZQ== 21713
IGZleg== 21714
IGVydw== 21715
IHZhcmllcw== 21716
IGplbWFuZA== 21717
IGRpc2NoYXJnZQ== 21718
0YHRj9GH 21719
IFBBUg== 21720
IEFmZmFpcnM= 21721
IHZvdGVy 21722
IGF0ZW4= 21723
IGNyb2lz 21724
b2JpbA== 21725
IE9vcHM= 21726
IEFyYw== 21727
IEhlYXRoZXI= 21728
YW5rYQ== 21729
IHNpbXBsZXM= 21730
zr/OvQ== 21731
Ij4= 21732
IGNob3Jkcw== 21733
IFNhbmRlcnM= 21734
IOu2hOs= 21735
QmVu 21736
IGRhcsO8YmVy 21737
aWxpYW5z 21738
IG9yZGVyaW5n 21739
IE1hbmg= 21740
IGtpbG9ncmFt 21741
IGthcsWf 21742
IGdyYXNw 21743
IGdob3N0cw== 21744
YWxlbg== 21745
IEplZGk= 21746
INCx0LvQuA== 21747
IGRvd25sb2FkZWQ= 21748
IGNvbmR1Y3Rpbmc= 21749
IEhhaw== 21750
IHJlc2VhcmNoZXI= 21751
aWxhbg== 21752
Z29vZA== 21753
IEhhbm5haA== 21754
IGTDvMWfw7xu 21755
IE1lc3NpYWg= 21756
dWl0eQ== 21757
aW9uYQ== 21758
IHByb2JhYmxl 21759
IFlF 21760
IGluZGVwZW5kZW50bHk= 21761
IGJ1ZmZlcg== 21762
YnVybg== 21763
b3VyZA== 21764
IE1jSw== 21765
IGxpbmd1 21766
dWplbXk= 21767
0LXRgNGC 21768
IGludHVpdGl2ZQ== 21769
IGNyYWNrcw== 21770
YXBwcm9wcmk= 21771
bnR5 21772
IGdlZW4= 21773
IGxlbmQ= 21774
IGNlcnRpZmljYXRpb24= 21775
SURT 21776
dW50ZXI= 21777
cGVlcw== 21778
IHRydW1w 21779
IGJhbmtydXB0 21780
IGZlYXM= 21781
6Jc= 21782
IGR1xbw= 21783
5riF 21784
IHZpcnVzZXM= 21785
IDU4 21786
Z29k 21787
INC20LXQuw== 21788
IHN0YWxr 21789
SW5k 21790
YWNoaQ== 21791
IENG 21792
IENvbmQ= 21793
IHNhbmN0 21794
IGNvbnRlbg== 21795
IGZyZWVk 21796
IFJU 21797
IG1lbnRvcnM= 21798
7KGx 21799
IHBvcnRhYmxl 21800
IFBhdWxv 21801
cmFuZQ== 21802
SEFIQQ== 21803
IFNlY3Rpb24= 21804
54Y= 21805
aHl1bg== 21806
IM6tz4c= 21807
IFB1Yg== 21808
IEluZGVwZW5k 21809
IGNvbXBvdW5kcw== 21810
INGB0Ys= 21811
IG1lc3NhZ2luZw== 21812
IGRlZGljYXRpb24= 21813
IG5vdGljaW5n 21814
IGRldm90ZWQ= 21815
0Y7RgtGB0Y8= 21816
IHNuYWtlcw== 21817
IGJhdHRsZWZpZWxk 21818
cGVycw== 21819
IGRlbGE= 21820
OTI= 21821
IGhhaQ== 21822
aWxsw6Q= 21823
w6lyZXI= 21824
ZXZlcnk= 21825
IHJlc3BvbnNpdmU= 21826
15nXlQ== 21827
b3Bm 21828
6Yk= 21829
irg= 21830
QmVjYXVzZQ== 21831
IHRvdXJpc20= 21832
IOq3uOqyjA== 21833
15XXpg== 21834
IGNhbnM= 21835
c3TDvHQ= 21836
IGRvbm5l 21837
IERpb3M= 21838
IFViZXI= 21839
YWN0b3J5 21840
IG9yaWVudGVk 21841
IEhlcm0= 21842
IHBhdHJvbg== 21843
dXJm 21844
YmVp 21845
IHByb2dyYW1h 21846
IE9oaA== 21847
Z2VuZXI= 21848
IGZpc3Q= 21849
IFdlbmR5 21850
IGFuZGE= 21851
IGd1ZXNzZWQ= 21852
IGZyZWFr 21853
5Lit5ZyL 21854
IEtpbmdz 21855
Y2hvb2w= 21856
IG9mZmxpbmU= 21857
IEluZGlhbmE= 21858
IEFsbGlhbmNl 21859
IDUz 21860
IHBhcnRpY3Vs 21861
IEZvY3Vz 21862
IGluaGFiaXQ= 21863
IOqwmeydgOuNsA== 21864
IE1jRw== 21865
b3dza2k= 21866
IOydtOqxtA== 21867
IHBhxYRzdA== 21868
0L7QvdC4 21869
aXR0YQ== 21870
IGNvbmZpcm1hdGlvbg== 21871
IEJyb29rbHlu 21872
IG5vb2RsZQ== 21873
ZnVuZA== 21874
aXR1ZA== 21875
IGdyYW5kcGFyZW50cw== 21876
IGJhcmJlY3Vl 21877
zrXOuc+C 21878
IOE= 21879
IGJhbGxvdA== 21880
IFZldGVy 21881
IHBpcGVz 21882
aWdpb3Vz 21883
IEdyYXBo 21884
ZXN0ZWQ= 21885
IOu4jOs= 21886
IEtF 21887
44Gh44KH44Gj44Go 21888
IGVpbnM= 21889
IGhhdHJlZA== 21890
44GR44Gp 21891
IGRhbmc= 21892
ZWVlZQ== 21893
IGFyY2hhZQ== 21894
IEplc3Nl 21895
IGRldGVjdGVk 21896
IHNlbmk= 21897
YnVyZ2g= 21898
IGRpc3BsYWNlbWVudA== 21899
IGRvcA== 21900
IGNvbmRpdGlvbmluZw== 21901
INC90LXRgdC60L7Qu9GM0LrQvg== 21902
IGRpc3R1cmJpbmc= 21903
UEg= 21904
IHRoaW5uZXI= 21905
IHdvdW5kZWQ= 21906
IEN1YW5kbw== 21907
IGN1c2hpb24= 21908
IHdoaXRlcw== 21909
IHByZWZlcmVuY2Vz 21910
IOykgOu5hA== 21911
IGthxbw= 21912
IEdhdGU= 21913
IFBhdGg= 21914
ZGxlcw== 21915
4LiE4Lij 21916
aW1vcmU= 21917
IOuztOyXrA== 21918
IGRpc2NpcGxpbmVz 21919
4buP 21920
IG1lc21h 21921
IOyDiOs= 21922
IOyLrA== 21923
IGdpbmc= 21924
IHVtYnJlbGxh 21925
SUdIVA== 21926
IHBlbnNpb24= 21927
IGNvbWJpbmluZw== 21928
U1M= 21929
IHJlY3RhbmdsZQ== 21930
4buHdA== 21931
IHByb3hpbQ== 21932
IENvdw== 21933
uIw= 21934
IGludGVudGlvbmFs 21935
5pWZ 21936
IGRlY2lk 21937
INGB0LrQsNC2 21938
IFVtYQ== 21939
aWFzbQ== 21940
YnV6 21941
IGRlYnJpcw== 21942
IGNhc3M= 21943
IFByb3A= 21944
aXNrYQ== 21945
66Cl 21946
ZXN0ZXJvbA== 21947
dXNzaWFu 21948
7J20656R 21949
IHVubGltaXRlZA== 21950
IGFkbWlyZQ== 21951
IHRpZ2h0bHk= 21952
IGdlbm9tZQ== 21953
IEp1bmlvcg== 21954
dmVuaXI= 21955
Z3Vz 21956
IGPEgw== 21957
IFZsYWQ= 21958
IO2C 21959
IHJlbGF0aXY= 21960
aW5jaQ== 21961
IGF1bnF1ZQ== 21962
IEJveXM= 21963
0YbQuNC+0L0= 21964
IFN3aXNz 21965
IHBoeXNpY2lhbnM= 21966
IO2PiQ== 21967
IFBFVA== 21968
IHdvdW5kcw== 21969
YWJvdXQ= 21970
w6Bp 21971
b256 21972
dXJpdGllcw== 21973
INGD0LLQuNC0 21974
5bem 21975
IG1lbnRhbGl0eQ== 21976
IHZhcmlhbmNl 21977
IHNlZ3VuZGE= 21978
IHZvbGNhbm8= 21979
YWxpZQ== 21980
4KWH 21981
IHRpbGVz 21982
IFRlcnJ5 21983
INin2YTZhNmH 21984
IGNhbm9u 21985
IHNjYXR0ZXJlZA== 21986
cHRvbg== 21987
IGRlZmluaXRpb25z 21988
IGFsZ2VicmE= 21989
b3Rlbg== 21990
YWJsbw== 21991
aWp1YW5h 21992
IHdyYXBwaW5n 21993
IHNlc2FtZQ== 21994
INC90LDRh9C40L3QsA== 21995
IEFsZg== 21996
INCg0L7RgdGB 21997
b3Jubw== 21998
IGFua2xl 21999
IHNwZWNpYWx0eQ== 22000
IGF0dGVtcHRpbmc= 22001
aWxpYXRpb24= 22002
IDE5MjA= 22003
IHBoZW5vbWVuYQ== 22004
IFByb2R1Y3Q= 22005
IEJ1Y2s= 22006
IEF3dw== 22007
c2Vlbg== 22008
IHZvaWQ= 22009
IEZyYW5rbGlu 22010
IGFkdm9jYWN5 22011
IFNlcA== 22012
IGNvb2xlc3Q= 22013
INGB0YDQsNC30YM= 22014
IFF1YW5k 22015
IDkwMA== 22016
IFRyYWQ= 22017
ZGllcw== 22018
IGhhc2g= 22019
5oiR5bCx 22020
5Lmf5piv 22021
IHBvdHM= 22022
IHNhZGx5 22023
IHZpYWJsZQ== 22024
IFRpZ2Vy 22025
IE9ORQ== 22026
IG5ldXJvbnM= 22027
b3dhbmll 22028
xJc= 22029
IFNoYXI= 22030
IExhbmRlcw== 22031
IGNvbmZlcmVuY2Vz 22032
6Kmy 22033
IGNyZWRlbnRpYWw= 22034
IGxpbWU= 22035
aW5lZQ== 22036
eGl0 22037
cGF5 22038
IGluY29ucw== 22039
ID4+Og== 22040
6KqN 22041
IO2emOs= 22042
IGxlc3Nlcg== 22043
IHNwaWxs 22044
IHByZW1pc2U= 22045
IDM2NQ== 22046
IEhvc3Q= 22047
IHRvbWFy 22048
15DXnA== 22049
67KI 22050
IFdoYXRz 22051
IGxpZ2h0d2VpZ2h0 22052
IE1hcA== 22053
Zmlh 22054
ZWxsc2NoYWZ0 22055
IHZlbmRvcnM= 22056
dWVzdG8= 22057
IE1pc3Rlcg== 22058
INCf0YDQuA== 22059
5Y+z 22060
aG1h 22061
IGludGVudGlvbmFsbHk= 22062
IFRhbmc= 22063
6Zeu 22064
IGlkZW50aWZpY2F0aW9u 22065
IGV0Y2V0ZXJh 22066
IE5lZQ== 22067
INGC0YDQuA== 22068
6re4 22069
IGNyeXB0b2N1cg== 22070
IGluaGFsZQ== 22071
IGFkZGljdA== 22072
5ZCE5L2N 22073
IG1hdQ== 22074
INGC0LDQutCw0Y8= 22075
IOuyhA== 22076
IGNvbXByYXI= 22077
aWVkemllxIc= 22078
INC+0YLQvdC+ 22079
IGJlZ2lubmVy 22080
INC80YPQtg== 22081
IG9ic2M= 22082
IGxpbWl0aW5n 22083
YXNjdWxhcg== 22084
IGluc3BlY3Rpb24= 22085
YWNp 22086
IHJlam8= 22087
TXVz 22088
IHphdGVu 22089
IHN6Y3o= 22090
IE1hZHJpZA== 22091
IHZhcmlldGllcw== 22092
IGVzdMOg 22093
IFNoYWtlcw== 22094
IGtpdHM= 22095
IGFkbWluaXN0ZXI= 22096
IGxhdmE= 22097
IGfDpQ== 22098
6Kmm 22099
16rXmQ== 22100
IFdheW5l 22101
IGluc3RhZ3JhbQ== 22102
IHJhdGVk 22103
cGFwZXI= 22104
IGJpbGQ= 22105
IHByZXRlbmRpbmc= 22106
IG9ic2VydmluZw== 22107
INGB0LDQvNC+0Lw= 22108
IHRyb3I= 22109
IG9yZ2FuaXNtcw== 22110
IGZhbHRh 22111
IGhvbWV0b3du 22112
57E= 22113
IO2L 22114
IGNoZWc= 22115
IOyh 22116
IGNvbW1h 22117
aXPDqQ== 22118
IGxpa2VsaWhvb2Q= 22119
YXZvcmVk 22120
IGdlbGRp 22121
0L3QuNC60L7Qsg== 22122
IG1lZGlv 22123
IGpha2ll 22124
IEp1cA== 22125
IGdyZWVuaG91c2U= 22126
IHNwaXQ= 22127
0LrQvtC1 22128
INC60LDQtg== 22129
IEdyYW0= 22130
IENvbmZlcmVuY2U= 22131
IGRlZmljaXQ= 22132
c8Sxbg== 22133
aW5zZQ== 22134
dcSf 22135
IHJpY2h0 22136
IGNvaW5jaWRlbmNl 22137
5Y+N 22138
IGV1cm9w 22139
IGJ1dHRlcmZseQ== 22140
cHJlYWQ= 22141
IOyWvA== 22142
6IC2 22143
IHdhdmVs 22144
IEluZmlu 22145
IFBsYW5ldA== 22146
IHNlbGZpZQ== 22147
aWVudHJhcw== 22148
IGFycm9n 22149
b3Nlcg== 22150
aWRhbA== 22151
oNeX16DXlQ== 22152
w7x0w7xu 22153
IGZyZXNobWFu 22154
IE1hY2hpbmU= 22155
z4PPhA== 22156
IERpYQ== 22157
7J2064uk 22158
44GT44GG 22159
bmVh 22160
IGxpc3Rpbmc= 22161
IGNvbmZpZ3VyZQ== 22162
dXRvcg== 22163
VXA= 22164
dHNjaGFmdA== 22165
cmnDqHJl 22166
IHVwd2FyZHM= 22167
INGF0L7Rh9GD 22168
IHN3ZWVw 22169
QnI= 22170
IGV4cHJlc3Npbmc= 22171
IHVuaGFwcHk= 22172
IG1hbmRhdG9yeQ== 22173
Z2VuZGVy 22174
IEHDrQ== 22175
IGluZGljYXRvcnM= 22176
IG9pbHM= 22177
bm90ZQ== 22178
IHNlZ3Vy 22179
0L7QttC10YI= 22180
eW5hc3R5 22181
IGRpc3RhbmNlcw== 22182
IG1lcmdl 22183
QkVSVA== 22184
IHN1cnJlbmRlcg== 22185
IGJ1YXQ= 22186
IEF3YXJkcw== 22187
IHNlw7Fvcg== 22188
b2RveA== 22189
IGZsYXZvdXI= 22190
IGFiZG9t 22191
IGNvbmZpZ3Vy 22192
ODY= 22193
IERJWQ== 22194
IHJpZ2lk 22195
sJg= 22196
IGNvcnBvcmF0aW9u 22197
IGdyb29t 22198
amF3 22199
IE5lYXI= 22200
0LjQu9C+ 22201
IG9wZXJh 22202
IElubm92 22203
0LjRgNCw 22204
k7E= 22205
IHNwZWNpZmllZA== 22206
IGNvc20= 22207
IEZyZWVkb20= 22208
IGNsb3du 22209
IE5lbQ== 22210
INCy0L7Quw== 22211
0ZHQvQ== 22212
IGNoYXJnZXI= 22213
4LmB4Lil 22214
IGluZmx1ZW50aWFs 22215
w6RzaWRlbnQ= 22216
6aQ= 22217
IOyEoOs= 22218
IHZvbHVtZXM= 22219
5pA= 22220
IG91dHJhcw== 22221
IFR3aXRjaA== 22222
IGZvdW5kaW5n 22223
IGF3aGlsZQ== 22224
IGNvaWw= 22225
6rCZ 22226
IGPhuqM= 22227
IFRocm93 22228
IEhlbmNl 22229
b21tdA== 22230
IEJlbmphbWlu 22231
0LPQu9GP0LQ= 22232
VGltZQ== 22233
b2JpYw== 22234
IG1vdXI= 22235
IGRyZWFk 22236
IEzDoA== 22237
IENoaWxl 22238
IHByZXZhbA== 22239
IHZhaW4= 22240
IGFydMSxaw== 22241
IHByZXNlcnZlZA== 22242
INC+0YLQtA== 22243
IHdhcmVob3VzZQ== 22244
IGJlc3Rl 22245
IFNldmVyYWw= 22246
IFNpdHVhdGlvbg== 22247
IGNhcmRib2FyZA== 22248
VG9k 22249
ZXJuYQ== 22250
IGdhcmFudA== 22251
IGdlc3R1cmU= 22252
IGhlbg== 22253
IHNwZWxsaW5n 22254
b3NleHVhbA== 22255
IGFubmU= 22256
IG1pY2U= 22257
IE1laW5l 22258
Y2FyZA== 22259
IHJlYmVsbA== 22260
IGNlcnRv 22261
IOycoOs= 22262
IHZlcnNjaGllZA== 22263
IEJvcw== 22264
IGludmVudGlvbg== 22265
IHRyemU= 22266
IG1hbmnDqHJl 22267
IENoYWQ= 22268
IHNwcmU= 22269
IG9yZ2FuaXNhdGlvbnM= 22270
IHBvb3JseQ== 22271
IGFudGVyaW9y 22272
IHN0YWly 22273
0LrRgA== 22274
IGF0b21pYw== 22275
IHN5bXBhdGg= 22276
IGNvbnRpbnVhbGx5 22277
IGtsZWluZQ== 22278
w6h0ZQ== 22279
0LjRiQ== 22280
zr/Pgg== 22281
cGV1dA== 22282
IHJlcG9zaXQ= 22283
IGVudHJh 22284
RW0= 22285
IGZpbmFuY2luZw== 22286
INC80L3QvtCz 22287
IHRoZXNpcw== 22288
IENvbXB1dGVy 22289
ZWF1 22290
IFRyZWU= 22291
IGJyaWRl 22292
b25zaWV1cg== 22293
c2hpcmU= 22294
d2lj 22295
REU= 22296
IOyImOs= 22297
IGFjb20= 22298
IFBP 22299
ZXJzY2g= 22300
INC/0L7QvNC+0Yk= 22301
IEFybWVu 22302
IOyjvQ== 22303
IHpvcg== 22304
IHByaW50cw== 22305
IERhc3M= 22306
5riv 22307
IGR1cmFibGU= 22308
IFRyYW5zcG9ydA== 22309
7J6Q6rCA 22310
INC70LXQsw== 22311
IGTDqXQ= 22312
w7RsZQ== 22313
YW1vdXM= 22314
WU4= 22315
IGNsaWZm 22316
IGdyYW1tYXI= 22317
INCf0L7RjdGC0L7QvNGD 22318
IGzDoG0= 22319
ZXNjaA== 22320
IG1pc2VyYWJsZQ== 22321
IHZvbHRz 22322
IENhZA== 22323
dWthbg== 22324
0YLQuNCy 22325
cnVzdA== 22326
IOyYrOudvA== 22327
IHZlcms= 22328
IGNoaWNrZW5z 22329
IFlvbw== 22330
IG91dGZpdHM= 22331
Y29kZQ== 22332
IGhpZXJhcmNoeQ== 22333
bmV0ZXM= 22334
IGNvdW50ZXJwYXJ0 22335
IHTDtGk= 22336
IHRlZA== 22337
IEJhcnQ= 22338
IOudvA== 22339
IEdlbmF1 22340
IGluY29taW5n 22341
IEFCQw== 22342
cmlxdWU= 22343
INC+0YLQvw== 22344
cXVhbA== 22345
IGluY2VudGl2ZQ== 22346
IGlocmVu 22347
16DXmQ== 22348
bG9l 22349
IDE5MzA= 22350
IGJhcmc= 22351
IGRpY3Rpb24= 22352
IMO2bmNl 22353
SU5T 22354
IHJlaA== 22355
aXNpYWo= 22356
bW91dGg= 22357
IHNjb3Jpbmc= 22358
bMSxaw== 22359
IOyVhOyjvA== 22360
T1JJQQ== 22361
IEVzdGFkb3M= 22362
IGNvbXBhbmlvbg== 22363
IGFzc2VtYmxl 22364
IHB1bmlzaGVk 22365
IGl0YWw= 22366
IHByZXZlbnRz 22367
aXN0ZXM= 22368
IEtlbnR1Y2t5 22369
IGxvY2F0ZQ== 22370
IGZhc3Rpbmc= 22371
44Go5oCd 22372
g4A= 22373
IFNlYg== 22374
IENyb3du 22375
b3BpYQ== 22376
IHdoaXA= 22377
dXN6 22378
0LrQsNC80Lg= 22379
IGRhdGFiYXNlcw== 22380
5a2X 22381
IHByb3NlYw== 22382
IDE5OTc= 22383
IOyCtOynnQ== 22384
IFNvbGFy 22385
IFB1ZXM= 22386
IFplbg== 22387
b2xsbw== 22388
IEd1cnU= 22389
IHNxdWVleg== 22390
INCX0LA= 22391
IMSN 22392
Y2VwdGlvbnM= 22393
Y2Nh 22394
aXphYmxl 22395
bWFuZA== 22396
IGJyZWFrdGhyb3VnaA== 22397
IHRhYmxlc3Bvb24= 22398
IFNFQw== 22399
aWto 22400
IFPDo28= 22401
INC/0LvQvg== 22402
YW1lbg== 22403
IHByYWM= 22404
IGRhcmxpbmc= 22405
IHRhbGxlcg== 22406
IHJlbmRlcmluZw== 22407
IOyasOumrOqwgA== 22408
IM+EzrfPgg== 22409
IG3Dow== 22410
IGVzb3M= 22411
dWVyZG8= 22412
INGB0YfQuNGC 22413
YWxsZXI= 22414
7JeI7Ja07JqU 22415
IG1pbGxvbmVz 22416
bGVyaW4= 22417
IHBlZ2Fy 22418
b25uZQ== 22419
IGVucm9sbG1lbnQ= 22420
IGxpZWd0 22421
IGJvYQ== 22422
d2nEmQ== 22423
YnNw 22424
IGN5Y2xpbmc= 22425
IEJlcm5pZQ== 22426
IDE5ODk= 22427
INC00LDQu9GM 22428
IERha290YQ== 22429
INGB0LLRj9C3 22430
IENQ 22431
IHN0YXJl 22432
7YKk 22433
IHByb3NwZXJpdHk= 22434
IGFycmFuZ2VtZW50cw== 22435
IGFycml2aW5n 22436
bcOk 22437
IGtheWFr 22438
aXB0 22439
IHBhcmRvbg== 22440
IHJlbGF0 22441
IHZlcnN0ZQ== 22442
IEZpZw== 22443
IGZvaWw= 22444
IFRhbGtpbmc= 22445
cGVhcmU= 22446
IG5vaQ== 22447
INC/0YDQuNGI 22448
IGhvY2tleQ== 22449
IGFkbw== 22450
IE9VVA== 22451
Njc= 22452
IGhvcm1vbmVz 22453
IEF2ZW51ZQ== 22454
IFN1cGVybWFu 22455
IHByZXNjcmlwdGlvbg== 22456
dWJlcm5ldGVz 22457
Q0w= 22458
b3RpdmU= 22459
TklT 22460
aWVuZW4= 22461
IHNhZG5lc3M= 22462
IFZpdA== 22463
VHk= 22464
IHN0YXJ0ZXI= 22465
IGJlZGU= 22466
IGZvdW5kYXRpb25z 22467
IHNvcmU= 22468
5bqX 22469
0YnQtdGB0YLQsg== 22470
7Jqw6w== 22471
INGH0YPQsg== 22472
bGluaw== 22473
IG1hbmV1 22474
d29ya2luZw== 22475
w6Bu 22476
IEF0dGFjaw== 22477
IENhcnQ= 22478
dmVpcw== 22479
IFJlc3A= 22480
ZW5zaW5n 22481
IOyii+yVhOyalA== 22482
IGVzY3VjaA== 22483
IFJOQQ== 22484
grQ= 22485
IGFkb3A= 22486
IGJlbmRpbmc= 22487
2LnYrw== 22488
IG1hbmFnZXM= 22489
dXNw 22490
IHRhcnQ= 22491
IHJvdXRlcg== 22492
Qm8= 22493
IGVzdGFibGlzaGluZw== 22494
IGJhbGFuY2luZw== 22495
IGF0aGxldGlj 22496
IFNsbw== 22497
IGZpbGxz 22498
INC90LDQsQ== 22499
INC00LDQuw== 22500
IHBvc3Nv 22501
IFZpZWxlbg== 22502
IGNyaXRpY3M= 22503
IGxhd3N1aXQ= 22504
IElzYWFj 22505
INGE0LjQu9GM0Lw= 22506
IHRyYXM= 22507
IHByYXc= 22508
IENyYXp5 22509
IG5ldQ== 22510
IGt1bGw= 22511
IHR1bW9y 22512
IEFQUA== 22513
Z2F0ZQ== 22514
IEFSRQ== 22515
OTg= 22516
IFN0ZWFt 22517
IGZ1Y2tlZA== 22518
bGFnZQ== 22519
IOKZrA== 22520
IE1E 22521
Znk= 22522
IHNoZWxscw== 22523
IFNlZW1z 22524
aXplcnM= 22525
IHJhbmdlcw== 22526
IEFudG9uaW8= 22527
QVRJT04= 22528
IEJhYmE= 22529
IOyDiQ== 22530
a3Vu 22531
IHByYXllZA== 22532
0YDRjw== 22533
INC/0YDQvtGC0LjQsg== 22534
IHNlYXM= 22535
YnVyeQ== 22536
INeU16k= 22537
IHRyYWl0 22538
IERlcGVuZGluZw== 22539
IGRyZQ== 22540
IGvDtm5udA== 22541
0YbRgw== 22542
IGxpcHN0aWNr 22543
ZWV6 22544
INC/0YDQuNC80LXRgA== 22545
IGFzc2lnbm1lbnRz 22546
Qm9i 22547
IG1ldGFscw== 22548
IHNwZWNpYWxseQ== 22549
5bCN5LiN5bCN 22550
IOyYiOs= 22551
IMWh 22552
IHZpc3Rh 22553
IM6s 22554
IHR3aW5z 22555
IG5vdGFibGU= 22556
IFNhdQ== 22557
IGTDqXZlbG9w 22558
IMOnZWs= 22559
IHBvbHlub20= 22560
YXZhbQ== 22561
IHRhbWLDqQ== 22562
0L7QvdC+0Lw= 22563
IHBsYXNtYQ== 22564
IGVmZWN0 22565
IGzDpG5n 22566
IGNhc2k= 22567
0YHQsA== 22568
xLFtxLE= 22569
44GZ44KL 22570
k6TsnYA= 22571
IGxhYm91cg== 22572
b3NzZW4= 22573
IFB1bg== 22574
cmlm 22575
IGRvc2Vz 22576
IG9wZXJhdGVz 22577
0LjQu9C70Lg= 22578
IGphYXI= 22579
c3Rhdw== 22580
IOyCrOuekQ== 22581
IGF0bQ== 22582
IHByb3RlY3Rz 22583
IGltcGVk 22584
SE8= 22585
IGNpbWE= 22586
IHRvY2g= 22587
YWJpcw== 22588
IHNlbmRv 22589
bGF1cw== 22590
IGN1cmw= 22591
IE51bQ== 22592
IHNwb25zb3Jz 22593
IGTDqWJ1dA== 22594
IEFsZXhh 22595
IELDvHI= 22596
IEFtZXI= 22597
IGNvcGU= 22598
INC40LfQsg== 22599
amFs 22600
IDE5OTU= 22601
YXBhdA== 22602
cmVzc2U= 22603
IFByaXpl 22604
IENsYWlyZQ== 22605
IEJyYW5kb24= 22606
IHdzenlzdGtv 22607
IHZhbHVlZA== 22608
4LiZ4Liw 22609
IHNlY3Q= 22610
IHNlY3JldGx5 22611
IGRpYW1vbmRz 22612
IEV2YW4= 22613
IFJQRw== 22614
44Gr44Gq 22615
iOuPhA== 22616
IFVuaXZlcnNhbA== 22617
IGRvdWJ0cw== 22618
IFBpbg== 22619
d2nEhXo= 22620
mqk= 22621
IGFsYm8= 22622
IGJyYXVjaHQ= 22623
QVVM 22624
IE1vYmlsZQ== 22625
Z3JhZGVz 22626
IHNjaGVt 22627
d2h5 22628
IE5pY2h0 22629
cGk= 22630
Z2xl 22631
IGNob3J1cw== 22632
IGdseQ== 22633
IHJlaW5mb3JjZQ== 22634
IG11ZmY= 22635
IFNoZW4= 22636
IEhvbGE= 22637
0YPQsw== 22638
dmlkZW1tZW50 22639
dmlhbA== 22640
YWNpb3Vz 22641
bGFpbWVk 22642
IFJpY28= 22643
IHZlZ2c= 22644
IGlsbHVzdHJhdGlvbg== 22645
IEJ1dHRlcg== 22646
b3dhZA== 22647
IGV1eA== 22648
IGVuZmFudHM= 22649
IExlYWRlcg== 22650
IFZpbGxhZ2U= 22651
ZXRpY2FsbHk= 22652
2YbZig== 22653
IHN0ZXc= 22654
IHN1cnByaXNlcw== 22655
IGN1ZQ== 22656
IEdyYW5kbWE= 22657
IENlbHNpdXM= 22658
IFJpY2h0 22659
ZW5j 22660
IHBldGl0aW9u 22661
IGhlcmI= 22662
IHdpY2tlZA== 22663
IHNjaGxl 22664
b2NhbHk= 22665
IHRyYW5zZg== 22666
IHRva2Vucw== 22667
IEdyYXk= 22668
IEJCQw== 22669
SUs= 22670
IDE1MDA= 22671
em4= 22672
IE5ldg== 22673
IGtveQ== 22674
IHphcg== 22675
IGJ1bGxzaGl0 22676
IENvbG9tYmlh 22677
dWxhdGl2ZQ== 22678
IHdpZGVzcHJlYWQ= 22679
eWVjdA== 22680
a2l0 22681
IGVtcHJlc2E= 22682
IG5vdXI= 22683
IGJ1cm5z 22684
YXRpbg== 22685
YWlyZWQ= 22686
IHJldm9sdXRpb25hcnk= 22687
INCz0L7QtNGD 22688
IExvZ2Fu 22689
IDE5OTY= 22690
IEdyYWhhbQ== 22691
cmVi 22692
IE5IUw== 22693
5pyb 22694
IGNvc3R1bWVz 22695
IG5hd2V0 22696
IGxvdmVycw== 22697
IEx1Y3k= 22698
IEluZGlnZW5vdXM= 22699
7ZWY6riw 22700
IGltbXVuaXR5 22701
pbTr 22702
dWl0bw== 22703
IGV4Y2Vzc2l2ZQ== 22704
IGRvbmF0aW9ucw== 22705
INeU16g= 22706
IOyyqw== 22707
6YmE 22708
IGRyeWluZw== 22709
bWVsb24= 22710
IHN1cnZleXM= 22711
IOustOyKqA== 22712
6aKo 22713
YWFh 22714
IHByb2Jl 22715
YW5jaWFs 22716
IGxvdWRlcg== 22717
IGhvdGVscw== 22718
w7zEnw== 22719
YWduZXI= 22720
IG9yaWdpbnM= 22721
IOuniOyngOuniQ== 22722
ICoq 22723
IHN0cmFuZ2Vycw== 22724
IEhhdXM= 22725
Y29tZWQ= 22726
IGFudGhyb3A= 22727
IHVzbw== 22728
IOyVhOyngQ== 22729
IFl1YW4= 22730
IO2VhOyalA== 22731
cGxlcg== 22732
cmVzc2l2ZQ== 22733
IHNwcmF3 22734
IFN0ZXc= 22735
IDE5OTQ= 22736
IGVsZGVycw== 22737
IG1laW5lbg== 22738
IGp1bnQ= 22739
IGFjb3VzdA== 22740
IFdvaG4= 22741
IGJhbmFuYXM= 22742
IHByb2plY3Rpb24= 22743
IFN0aWNr 22744
bGVndA== 22745
c3BlZWQ= 22746
IGPFqW5n 22747
IFdvcnQ= 22748
IEJhbHRpbW9yZQ== 22749
INGG0LXQuw== 22750
IGR1bm5v 22751
5by3 22752
Pyw= 22753
44OJ44Oz 22754
IExvY2Fs 22755
b3N0bw== 22756
0K0= 22757
0L7QtNCw 22758
IFBvcnR1Z3Vlc2U= 22759
IHRoZWlycw== 22760
IGTDqW0= 22761
5Y+m 22762
IGRyYXVm 22763
IEJ1ZGRoaXN0 22764
ZXJ0YQ== 22765
R2U= 22766
IGNhcnJvdA== 22767
IFdvbmRlcmZ1bA== 22768
IHNvYWs= 22769
IGNoYWlybWFu 22770
Z2dp 22771
SUNB 22772
ZnJpZWQ= 22773
IGZsaWNr 22774
IFRocm91Z2hvdXQ= 22775
IOyasOs= 22776
IGNvdWdo 22777
IGZsdWZmeQ== 22778
c2Nob29s 22779
IHJpcHBlZA== 22780
LS0tLS0tLS0= 22781
IFp1a3VuZnQ= 22782
INC90LXQsQ== 22783
IHN0bw== 22784
IEJP 22785
cGVudA== 22786
IExhd3JlbmNl 22787
z4nPgg== 22788
c3RpY2tz 22789
IEVpbnM= 22790
INGA0Ys= 22791
IFN0cm9uZw== 22792
IGNhcmFtZWw= 22793
IHNwaXRl 22794
YXphcg== 22795
6YO95piv 22796
IGNyaXRpY2FsbHk= 22797
IG9icmE= 22798
b3dpdHo= 22799
IFpvbmU= 22800
INGA0LXQug== 22801
IHN1Zw== 22802
YXJkZWQ= 22803
IGfDrA== 22804
ZmZlbnRsaWNo 22805
YW5jaGU= 22806
2J8= 22807
YXN0aWNhbGx5 22808
7J286w== 22809
0LvQsNCy 22810
IHNpbXBsZXN0 22811
IEZyaWVuZA== 22812
IHF1ZWxsbw== 22813
IGFtYml0aW9u 22814
IGFiYmlhbW8= 22815
5bqV 22816
INGE0L7RgNC8 22817
IEVzc2E= 22818
IGVkdWNhdG9ycw== 22819
IHN0YXRpc3RpY2Fs 22820
6YCZ6YKK 22821
IGNoYW5nZXI= 22822
IGF0YXU= 22823
w6l0YWlz 22824
IFNoYWtlc3BlYXJl 22825
65CY 22826
IHRyaWdnZXJz 22827
IHJlYWxpeg== 22828
IGNlbHVp 22829
d2hlZWw= 22830
IGxveWFsdHk= 22831
IHNjcmVhbXM= 22832
a2Vocg== 22833
IE1lZ2E= 22834
ZWFzdA== 22835
IHRvcHM= 22836
IFRvdGFsbHk= 22837
b3VudGFpbg== 22838
bG9yZA== 22839
IHZpb2xhdGlvbg== 22840
IEdB 22841
IG5pY2Vy 22842
IEZyZXNo 22843
IE1lbGlzc2E= 22844
ZnVuY3Rpb24= 22845
IHJhcGU= 22846
IGV4Y2VwdGlvbnM= 22847
IHNpbGljb24= 22848
IGxpYmVydHk= 22849
IGhvdXNlaG9sZHM= 22850
44GN44G+44GZ 22851
IENB 22852
INCe0LE= 22853
IGxpYg== 22854
now= 22855
Y2lmaWM= 22856
IHRyb3BpY2Fs 22857
IGludmVzdGlnYXRpbmc= 22858
SEQ= 22859
IGFkYXB0ZXI= 22860
IFBpdHQ= 22861
YW5jaWE= 22862
IFNoZWxs 22863
ZnJpZW5kbHk= 22864
IGNvbmNsdXNpb25z 22865
IHR1cnRsZQ== 22866
IGRlY29tcA== 22867
IGFuaW1hdGlvbnM= 22868
INGB0LXQug== 22869
aW5zaQ== 22870
IHJldGVudGlvbg== 22871
a2ll 22872
IGluamVjdGlvbg== 22873
IE1hZGlzb24= 22874
7LCw 22875
IHZpZW50 22876
IHZhcmllZA== 22877
IHZpb2xpbg== 22878
IEJpbA== 22879
IGx1Y2tpbHk= 22880
IGh0dA== 22881
bMOk 22882
IHJhbmNo 22883
55yL55yL 22884
IHPDs2xv 22885
7JWF 22886
IERlcmVr 22887
IFNjcmlwdHVyZQ== 22888
0L7RgNCw 22889
IGNsYXNzcm9vbXM= 22890
YXZpbA== 22891
Zm9ybWVk 22892
IGJlZm9yZWhhbmQ= 22893
IEdlbQ== 22894
cHJlY2g= 22895
IGxpbg== 22896
IGdyZWVucw== 22897
0YbQtdCy 22898
IE1lcmNlZGVz 22899
IGRyb3VnaHQ= 22900
Z2FzcHM= 22901
IGFib3J0aW9u 22902
IHRlcnJpYmx5 22903
IHNwb3PDs2I= 22904
IHNlY3VyZWQ= 22905
IGF0csOhcw== 22906
IHdhdmVsZW5ndGg= 22907
IGdyYWlucw== 22908
ZWN0aXZl 22909
IHNwYWNlY3JhZnQ= 22910
IHRvdXJz 22911
IHByb2Zlcw== 22912
IHN1cmdlb24= 22913
IFBpZQ== 22914
IGlkZWFsbHk= 22915
YXJuZXI= 22916
VVA= 22917
b3BhcmQ= 22918
c2Nl 22919
IGltbWVuc2U= 22920
IE9ydA== 22921
cm9sbGVy 22922
IERhbGxhcw== 22923
IE5pY2hvbGFz 22924
IHN1bGY= 22925
IFRveW90YQ== 22926
IHF1YW50aXRpZXM= 22927
Y2VhbnM= 22928
IGN1aQ== 22929
YW7Dp2E= 22930
IENBTg== 22931
aXR6ZXJsYW5k 22932
5YS/ 22933
IHpvdQ== 22934
IEN5YmVy 22935
bGVnZW4= 22936
IEluaXQ= 22937
ZWR1 22938
IGFwZXJ0 22939
IGFkamFj 22940
b3V2 22941
6ICM5LiU 22942
cnM= 22943
IGNhYmJhZ2U= 22944
IHdoZWVsY2hhaXI= 22945
aW55bA== 22946
IER5bmFt 22947
IOyVhOuLiOudvA== 22948
IGxpbmc= 22949
aGw= 22950
INC80L7Qs9GD 22951
IGNyaXNw 22952
IG1pag== 22953
IGR1Zw== 22954
bmlu 22955
IGJsb3Nz 22956
IGJlbG9uZ2luZw== 22957
IGxvdWRseQ== 22958
IG1pbmVyYWxz 22959
IGNvbmNsdWRlZA== 22960
IHNlYXJjaGVk 22961
OTY= 22962
IE1lZXQ= 22963
IFNFTw== 22964
INCh0Lo= 22965
IEhvYg== 22966
b3R0YQ== 22967
IHByb3BhZ2FuZGE= 22968
IGNpbm5hbW9u 22969
IGh1bnRlcg== 22970
IGdlbWVpbnM= 22971
IHNjdWxwdHVyZQ== 22972
dWxzaW9u 22973
IHbDpGw= 22974
IG1hZ2F6aW5lcw== 22975
IGNvbnRyb3ZlcnN5 22976
5LiA5qij 22977
IHNlcXVlbmNlcw== 22978
44GE44KL 22979
IO2ajA== 22980
IGRlbGV0ZWQ= 22981
5L2/ 22982
kOuPhA== 22983
IHZhcnlpbmc= 22984
44OG 22985
IG1vdW50aW5n 22986
IGFmZmFpcg== 22987
IHBhdGh3YXlz 22988
5qY= 22989
IGRpZ28= 22990
5Lqu 22991
INC00L7Qug== 22992
QWxleA== 22993
IHRvYmFjY28= 22994
IENW 22995
IGJvdGhlcmVk 22996
IGFtYmllbnQ= 22997
aW5reQ== 22998
IFNM 22999
IGhhdGVz 23000
IGplxbxlbGk= 23001
IGNvbmdyZWc= 23002
IGVsYXM= 23003
IGRldXRz 23004
IFN0dWRpb3M= 23005
Y2jEmQ== 23006
IGRvY3VtZW50ZWQ= 23007
IENydXo= 23008
IExlbg== 23009
IERvdWdsYXM= 23010
IFBvcnR1Z2Fs 23011
ZW50aQ== 23012
IHNwb3VzZQ== 23013
IGFuYWx5cw== 23014
YXZpYQ== 23015
IGVkaXRlZA== 23016
IGzhuqFp 23017
YnVpbHQ= 23018
IHZpbGxl 23019
YWRvcmE= 23020
IGJyYWNlbGV0 23021
IHN1c2hp 23022
IHBt 23023
IHRyYWlscw== 23024
IGx1Zw== 23025
IMO2dmVy 23026
IHNvcnJvdw== 23027
IGNvbG9ueQ== 23028
YWRveA== 23029
IHNlcmll 23030
YW55YWs= 23031
INi3 23032
IEd1bGY= 23033
5piv5LiN5piv 23034
IFBW 23035
IFNhbXVlbA== 23036
IEtpdA== 23037
IFJhbA== 23038
b250aW4= 23039
ZXhwbA== 23040
IGVudHJpZXM= 23041
IGFjdGl2aXN0cw== 23042
UHM= 23043
IHNhbnQ= 23044
INGC0L7Rhw== 23045
IEJydW5v 23046
a2VsZXk= 23047
IHR1dHRv 23048
6ZQ= 23049
IHZpbnRhZ2U= 23050
IHRlcnJpZmllZA== 23051
INC/0L7RhQ== 23052
dXNpdmU= 23053
b3dlcnM= 23054
0LDQudGC 23055
64+Z 23056
IHR3aXN0ZWQ= 23057
IFRob3VnaHQ= 23058
IHRhaA== 23059
IHNocmluaw== 23060
IHNoZWVy 23061
bGl0 23062
IGRhbGFt 23063
IGRpYg== 23064
IHZhcmQ= 23065
b3dhbmU= 23066
IGRvYnI= 23067
IFJlbmE= 23068
INGB0LLQvtGO 23069
IHBhw61zZXM= 23070
IEVyYQ== 23071
44Gu44Gn 23072
IEJVVA== 23073
c2lnaHM= 23074
IOq3uOqxsA== 23075
IGdyb8OfZW4= 23076
IOu5qOumrA== 23077
IG5lcnZlcw== 23078
IGNvbnN0aXQ= 23079
IHByZW9jdXA= 23080
IEdheQ== 23081
IFh1 23082
a2VlcGVy 23083
aGV1cmU= 23084
Li4p 23085
IENhbG0= 23086
IFVuaWRvcw== 23087
IOydtOqygw== 23088
IEFxdWk= 23089
IOygnOydvA== 23090
ZMSxcg== 23091
7KaY 23092
eW91cg== 23093
INGN0YLQuNC8 23094
MjAyMA== 23095
IHJ1bmQ= 23096
IEhP 23097
IENhdGhlcmluZQ== 23098
aWVsaQ== 23099
IGZ1c2lvbg== 23100
IGlkZW9sb2d5 23101
IGZvcmFt 23102
c2hhcGVk 23103
IO2bhOs= 23104
IHd0 23105
IHJldHI= 23106
IHByw6lj 23107
IOqwkQ== 23108
IG9wZW5seQ== 23109
dml0eQ== 23110
6rWs7JqU 23111
IG9ic3RhY2xl 23112
IGJvbw== 23113
IHNlaW5lcg== 23114
aWNvcm4= 23115
IGVpZ2VubGlqaw== 23116
IGhlYWRlcg== 23117
YXJlbW9z 23118
IHNvZnRlcg== 23119
INCf0L7QtA== 23120
IHByZWp1ZA== 23121
IGRlZmluZXM= 23122
aWVydGU= 23123
IGJsZW5kaW5n 23124
IGJlbGlldmVycw== 23125
IFdvY2hlbg== 23126
INC90LjQutCw0Lo= 23127
INCa0L7Qs9C00LA= 23128
IFR5cGljYWxseQ== 23129
IO2BrA== 23130
566h 23131
Y2lvcw== 23132
IG1pc3NpbGVz 23133
IHNwb25nZQ== 23134
IEtpdGNoZW4= 23135
IHRyZW4= 23136
bmluZ2Vu 23137
IHNjcmFw 23138
IHNlcmFpdA== 23139
tOyg 23140
57k= 23141
IOuwmOs= 23142
IHJlc3RvcmVk 23143
IHByenlrxYJhZA== 23144
IEt1YmVybmV0ZXM= 23145
IHNhaXQ= 23146
IHV3 23147
IGVuYWJsaW5n 23148
IHRyYXZlcnM= 23149
YW1wcw== 23150
5Y+X 23151
IE9NRw== 23152
ZW5zb3I= 23153
IHpvc3Rh 23154
IHByb25vdW5jZWQ= 23155
QW5n 23156
bm9ybWFs 23157
IGVjb25vbWllcw== 23158
dGlu 23159
IENoYW1waW9u 23160
aXplbg== 23161
IGFyYmVpdGVu 23162
IEdvc3BlbA== 23163
IFp1 23164
bmdh 23165
IGxpdGVyYWN5 23166
IE1hbnM= 23167
IGNpcmN1bGF0aW9u 23168
IGFkYXA= 23169
IFRvdGFs 23170
IG1lcmVrYQ== 23171
IG9sYWNhaw== 23172
0YHRgtCw0YLQuA== 23173
SmFjaw== 23174
IG11bmQ= 23175
IHRoaWVm 23176
Ymllcw== 23177
IOqygQ== 23178
YXF1ZQ== 23179
INqp24w= 23180
IFNjYXI= 23181
5bI= 23182
IGFib2w= 23183
IGRldm90ZQ== 23184
IDAx 23185
IHNpdHRlbg== 23186
IFZpc3VhbA== 23187
d2Vlaw== 23188
c29tZQ== 23189
aW5ndA== 23190
IGpvdXJuYWxpc20= 23191
IEhpcg== 23192
IEJhY2hlbG9y 23193
aW5lcnk= 23194
w5xORA== 23195
44Of 23196
57uZ 23197
IGNvbG9yaW5n 23198
IENyaXN0 23199
IGNlbGVicml0aWVz 23200
INGH0LjRgQ== 23201
IENyaXQ= 23202
IGRpZmZlcmVudGlhdGU= 23203
INCc0L3QtQ== 23204
ZWxpbQ== 23205
IHNlYWZvb2Q= 23206
IGFsZ3VtYXM= 23207
b3RoZXJhcHk= 23208
5oiw 23209
IGdsYXVi 23210
IGFyYml0cmFyeQ== 23211
Z2Vucw== 23212
INCx0YPQtNC10Lw= 23213
IHRhdg== 23214
IGNyZWFteQ== 23215
IENvdW50cnk= 23216
YcOx 23217
0LzQtdGC 23218
IGhpbnRlcg== 23219
IG1pc20= 23220
IGlsbHVzdHJhdGU= 23221
w5xORE5JUw== 23222
IGRlY3JlYXNpbmc= 23223
IHdlbmlnZXI= 23224
QUtJ 23225
aXhvbg== 23226
INC90LXQuQ== 23227
IGZhdHRv 23228
IG5lcmQ= 23229
56A= 23230
IGJpdHRl 23231
UGVy 23232
IHRhbmU= 23233
IGfDtno= 23234
IGZvcnRl 23235
IEV5 23236
INC90LDQstC10YA= 23237
6KKr 23238
IFdvcmRQcmVzcw== 23239
IE1pcw== 23240
xa8= 23241
esOkaA== 23242
IGludMOpcmVzcw== 23243
b3NhdXJz 23244
IEZhbGxz 23245
IG5lc3Nh 23246
OTc= 23247
IG11c2V1bXM= 23248
IGNvcnJlc3BvbmRz 23249
IHNpbmdz 23250
Zm91cg== 23251
IGVkZXI= 23252
IENvbW11bmlzdA== 23253
b2E= 23254
bmVr 23255
IFdITw== 23256
IGNvcnBv 23257
IG1lc3Npbmc= 23258
z4TOsc65 23259
IGJydXNoZXM= 23260
IGJpc2M= 23261
IEFyYmVpdHM= 23262
IFRheA== 23263
IHNlbGU= 23264
IGZsYWdz 23265
b3VwZQ== 23266
IGFudGljaXBhdGVk 23267
44OR 23268
IE5hZA== 23269
IHBvdXJlZA== 23270
IG1s 23271
IGxsYW1h 23272
IHZpc3VhbGl6ZQ== 23273
IGxpc3RlbmVycw== 23274
2YTZgw== 23275
YWx0ZW4= 23276
TWljaGFlbA== 23277
IGNvc8Os 23278
1aHV 23279
b3B1cw== 23280
IO2VtOyjvA== 23281
IGhpa2U= 23282
IEF0dG9ybmV5 23283
IEhpbGxhcnk= 23284
dWRlZA== 23285
IO2VmOyngOunjA== 23286
IGRvdmU= 23287
IHN0b3Jtcw== 23288
0LDQutGB 23289
IGRvY3RyaW5l 23290
IGhleA== 23291
aWtz 23292
bm/Fm8SH 23293
IHNjcmlwdHM= 23294
IM60zrXOvQ== 23295
INGN0YLQuNGF 23296
INCG 23297
YWJlcg== 23298
IFZhcw== 23299
IGNlbnRpbWV0ZXJz 23300
157XlA== 23301
0L3QuNCx 23302
IHJpZGVycw== 23303
IFRyaWI= 23304
5YyF 23305
IHRha8W8ZQ== 23306
IG5vdW4= 23307
IGljb25z 23308
IHNvbGVseQ== 23309
bWluZGVk 23310
IGRpc3Bvbg== 23311
IFN3aXR6ZXJsYW5k 23312
IGNsdXN0ZXJz 23313
IHF1ZWRh 23314
YWlsaW5n 23315
IG1hbmdh 23316
IDY4 23317
hIg= 23318
IHRldA== 23319
Z2lucw== 23320
aGF1cw== 23321
56m6 23322
5bel 23323
IE9Q 23324
b3RlZA== 23325
IG5vdXZlYXU= 23326
QUxMWQ== 23327
2YjYrw== 23328
w7Ju 23329
IG1vcnRhbGl0eQ== 23330
IEdpdEh1Yg== 23331
ZHJvcA== 23332
IGRpc2d1 23333
IHJlY29t 23334
IGxvY2Fscw== 23335
IGhvbWVtYWRl 23336
YW1iYQ== 23337
IHByb251bmNpYXRpb24= 23338
IGFscGhhYmV0 23339
0LDQvdGM 23340
b3dhbnk= 23341
aXJhcw== 23342
aWRlbmN5 23343
T01F 23344
INGA0LDRgdGB 23345
YXJhaw== 23346
dmlhbWVudGU= 23347
IG5vbnByb2ZpdA== 23348
IFlvdVR1YmVy 23349
IHBhcmVudGg= 23350
IEJvbw== 23351
dmF0 23352
IFN0aXI= 23353
IHByZWNpcA== 23354
IGFudHM= 23355
IGFsbHk= 23356
IE1hb3Jp 23357
IOuMgO2VnA== 23358
5Y+v5piv 23359
b2dlbmU= 23360
IExhYm91cg== 23361
YXJldHRl 23362
IHJlY3ljbGluZw== 23363
ZW5zYQ== 23364
IHB1cnN1aXQ= 23365
IHNhaw== 23366
INCX0LTQtdGB0Yw= 23367
IHRvbGVyYW5jZQ== 23368
IHNhYXQ= 23369
IGNsaWNrZWQ= 23370
4pml 23371
IGZhY2Vib29r 23372
IEludG8= 23373
IGluY2VudGl2ZXM= 23374
6riw64qU 23375
IERlbm5pcw== 23376
IFdpaw== 23377
Z2VzY2g= 23378
4LmA4Lib 23379
IM+AzrE= 23380
IFdob28= 23381
IHJvdW5kZWQ= 23382
IGRvcGU= 23383
IGNhcHR1cmluZw== 23384
IFdhcnJp 23385
IGNpdmlsaWFu 23386
IGNoYXJtaW5n 23387
IGVzYXM= 23388
IHN1c3RhaW5lZA== 23389
IGxlYW5pbmc= 23390
IGFidW5kYW5jZQ== 23391
w61saWE= 23392
0LDQu9GM0L3Ri9C5 23393
IHBo4bqjaQ== 23394
YWNqYQ== 23395
IOqwmeyVhA== 23396
YWN0aXY= 23397
4Liy4Lii 23398
IDk3 23399
INC80L7QuQ== 23400
Y3Jv 23401
IEphY2tpZQ== 23402
aXR0ZWVz 23403
YnJhY2h0 23404
dWxlbnQ= 23405
IOygnOs= 23406
IHBsdWdpbg== 23407
dmFudGFnZQ== 23408
cGFydHk= 23409
IHN1YXM= 23410
IGFudGU= 23411
0YPQuw== 23412
0J3QkA== 23413
5oKo 23414
IM+Dz4U= 23415
IG1ldGg= 23416
IGVudGh1c2lhc20= 23417
0Y/RgtGB0Y8= 23418
7ZmU6w== 23419
IHN5bnRoZXRpYw== 23420
IHNlYXNvbmluZw== 23421
IExvc3Q= 23422
b25vbXk= 23423
IFNwYXJr 23424
IGJ1cmU= 23425
IGFzc3VyZWQ= 23426
IGltYWdpbg== 23427
IGNhcnJv 23428
U2hh 23429
xIV0 23430
0L3Rg9GC0Yw= 23431
w6F0aWNh 23432
VFk= 23433
IGtlcm4= 23434
IEJyYXppbGlhbg== 23435
w7A= 23436
IHN1c3BlbmRlZA== 23437
IENhcmli 23438
IGJpemlt 23439
IE9saXZlcg== 23440
44G2 23441
VG9t 23442
INC/0LvQsNC9 23443
IG5vcGU= 23444
b21ldGhpbmc= 23445
IGJlaWRlbg== 23446
0YbQtdC9 23447
IGZsdWN0 23448
IM68zr/PhQ== 23449
IGZhdGhlcnM= 23450
IEJsYWtl 23451
IHVwd2FyZA== 23452
IERhc2g= 23453
IExpbA== 23454
IOyImOuPhA== 23455
IHJldmVsYXRpb24= 23456
IGVsZXZhdGVk 23457
IEppYW5n 23458
TEVE 23459
IFRob21wc29u 23460
INC80L7Qs9GD0YI= 23461
0YHRgtGA0YM= 23462
aWZpZXJz 23463
IGNvbWViYWNr 23464
IGJ1eWVycw== 23465
6rKw 23466
IFNhbGVz 23467
0LjRh9C1 23468
Y2lvbmVz 23469
IHdoaXN0bGU= 23470
IGR1bGw= 23471
TEVY 23472
IO2VmOqyoOyKteuLiOuLpA== 23473
IGNyaW1pbmFscw== 23474
IGRlc2NlbnQ= 23475
aXBwbGU= 23476
bWFzxLE= 23477
IGZvb2xpc2g= 23478
INC00YPQvNCw0Y4= 23479
dGFy 23480
IG1hbmdv 23481
IGNob3Jlb2dyYXBoeQ== 23482
TWF0dA== 23483
IHRlcnJpdG9y 23484
IGFjYWJh 23485
IEVpbnN0ZWlu 23486
IElCTQ== 23487
IE1ldGFs 23488
IENyeXN0YWw= 23489
IHJhaA== 23490
IGZvdWw= 23491
IElzbGFuZHM= 23492
IGludGFjdA== 23493
IFJhaWw= 23494
Ljo= 23495
IGFjw6E= 23496
INC/0YDQvtC/ 23497
0LXRgNC1 23498
IFdyaXRl 23499
aGVoZQ== 23500
IEZP 23501
IM+Dz4TOtw== 23502
IGRvaW4= 23503
aGVsZA== 23504
IGFwcHJvcHJpYXRlbHk= 23505
IGRlbGliZXJhdGVseQ== 23506
IGFyY2hpdmU= 23507
IGdpdmVhd2F5 23508
44GT44GT 23509
IGZpbmFsZQ== 23510
0LvQsNGB 23511
0LXQvdC+ 23512
xqFu 23513
5qOS 23514
b2dv 23515
54mp 23516
IEF1ZGllbmNl 23517
44Wg 23518
IHN1YnVy 23519
IGhlYWRhY2hl 23520
0LDQvdC90Y8= 23521
IFdpdGNo 23522
IFN3ZWRpc2g= 23523
IEJJ 23524
IGVyYXNl 23525
IGtoaQ== 23526
IGNvbW1lbnRhcnk= 23527
IFN1bHRhbg== 23528
7YOd 23529
IExlYmFu 23530
IOuztOyL 23531
IFBhbQ== 23532
cGVrdA== 23533
bW9udGg= 23534
IGdyb3VuZGVk 23535
6r4= 23536
IMWfZWtpbGRl 23537
MjUw 23538
IFNDSA== 23539
aW9zbw== 23540
IGluYXVn 23541
aGVpbWVy 23542
IHJlZmxlY3Rpbmc= 23543
IFJ1dGg= 23544
IE9pbA== 23545
IHRyb3V2ZXI= 23546
dWVw 23547
Li5d 23548
IOyeiOs= 23549
IG9saGE= 23550
IHJlYXNvbmFibHk= 23551
IGdsaXRjaA== 23552
VUI= 23553
IEdyYW4= 23554
IGFkYWxhaA== 23555
IGxlbnQ= 23556
2LHYpw== 23557
IHRyYWN0aW9u 23558
IGFkanVzdGluZw== 23559
tKQ= 23560
0L3QuNCx0YPQtNGM 23561
INC00L7Qvw== 23562
IHN0cmV0Y2hlZA== 23563
IG9ydA== 23564
IGNvc2luZQ== 23565
dmlvbA== 23566
IOyF 23567
Y2ly 23568
IGJhc3RhcmQ= 23569
5LiH 23570
INGF0L7QtA== 23571
IHF1aWVy 23572
IHByZXNzdXJlcw== 23573
IEFuaA== 23574
5bm+ 23575
IGVsbGVz 23576
INC00YDRg9C3 23577
INC80L7QttC10YLQtQ== 23578
IGNo4bs= 23579
IE3DqQ== 23580
w7Zr 23581
4bqndQ== 23582
7KCI 23583
emlu 23584
IGNhdXRpb24= 23585
aWJhbg== 23586
IGp1ZGdpbmc= 23587
0YPRjtGC 23588
IGJhag== 23589
INCh0LXQudGH0LDRgQ== 23590
IFBvb3I= 23591
IE5hemk= 23592
IHVwYmVhdA== 23593
eWFuZw== 23594
IHdlZWtlbmRz 23595
IEVzc2VudGlhbGx5 23596
IG9sdXlvcg== 23597
IHNwYXRpYWw= 23598
YWNrZXI= 23599
IHNlbGxlcg== 23600
INeQ15XXqg== 23601
kdec 23602
IHZpdmlk 23603
IEJvbmQ= 23604
6raM 23605
aXNrdA== 23606
44K1 23607
IGdvYXQ= 23608
ZHJpdmVy 23609
IG11Zw== 23610
aWN0aW9uYWw= 23611
IGFsbHQ= 23612
IEluaXRp 23613
IFJhbmQ= 23614
IGZpbmlzaGVz 23615
IOqwiA== 23616
IHZpdGFt 23617
IHRlZW5hZ2Vycw== 23618
IE1vcnJpcw== 23619
7KSE 23620
IE9yaQ== 23621
aXlh 23622
IG15w7Zz 23623
U3RlcA== 23624
IEtyZQ== 23625
6L6m 23626
IGRpbm9zYXVy 23627
IOuqhw== 23628
YWZmZQ== 23629
IOuQqeuLiOuLpA== 23630
IHplZw== 23631
5YiH 23632
IE1hbmhhdHRhbg== 23633
IHN1amV0 23634
dWVsbGU= 23635
c3RvZmY= 23636
IGTDvHI= 23637
IHN1Ym1hcg== 23638
ZXNlcw== 23639
IGFxdWVsZQ== 23640
IG5vdQ== 23641
IEZhaXRo 23642
dHo= 23643
INGC0L7QvNGD 23644
YWNldXQ= 23645
bGllcnM= 23646
IGJhbmR3aWR0aA== 23647
xrDhu50= 23648
IHJlc3BlY3RpdmU= 23649
IEF2ZQ== 23650
IHNwcmVhZHNoZQ== 23651
IFNlbnQ= 23652
aWNhbWVudGU= 23653
IGluZnJh 23654
IGxlYXJuZXJz 23655
IOCuiQ== 23656
YWlhaA== 23657
cmVuYWw= 23658
IG11c3RhcmQ= 23659
IGhhYnQ= 23660
54M= 23661
IFF1w6k= 23662
IGFuYWx5emluZw== 23663
5q+P 23664
IHNvbGlj 23665
INeU15XXkA== 23666
IGNhdXNh 23667
IHdlbGNvbWVk 23668
IFN1Y2Nlc3M= 23669
IGZhY2lsZQ== 23670
INCf0L7RgtC+0LzRgw== 23671
c2NoZWlu 23672
IGZldGNo 23673
IHN0cmF0 23674
INGB0YLQvtC40YI= 23675
7JeQ7ISc64qU 23676
INGB0L/QvtGB0L7QsQ== 23677
bWFt 23678
IHNlcsOtYQ== 23679
bmFtZW50cw== 23680
d3JpdGVy 23681
IGNvbnN1bHRpbmc= 23682
7ZiA 23683
IEJlcmtlbGV5 23684
ZXU= 23685
YXNpdmU= 23686
VVU= 23687
IEFuYWx5dA== 23688
IHN1Ym1pc3Npb24= 23689
IG1hZ25pZmljZW50 23690
ZW56YQ== 23691
IGVjb24= 23692
IHByb2ZpbGVz 23693
IGluY2Fy 23694
QWI= 23695
IE51bg== 23696
IGhpYw== 23697
c2NyZWFtaW5n 23698
IHJlc2lsaWVudA== 23699
5Yip 23700
Z3J1bmQ= 23701
IGNvbmN1cg== 23702
IGJlcmVpdHM= 23703
TEQ= 23704
IG51cnQ= 23705
7Ik= 23706
IGZlYXN0 23707
IGVuY3VlbnQ= 23708
IE1pY2hlbA== 23709
IHN1cHJlbQ== 23710
Il0= 23711
IGZlZWRz 23712
IEtvbGxlZ2Vu 23713
aXNzZXI= 23714
IEZlbmc= 23715
IFdlbg== 23716
bXVu 23717
IHRlbsOtYQ== 23718
IFdyZXN0 23719
IOyYpOuKmOydgA== 23720
IHN0ZWFk 23721
IHJlc3RvcmF0aW9u 23722
IGRvbmF0ZWQ= 23723
IGRlbHM= 23724
IGNlbnN1cw== 23725
IGRlc3BlcmF0ZWx5 23726
d29ydGh5 23727
SEU= 23728
IFNwYQ== 23729
IEJyeWFu 23730
IGhq 23731
IFJhdw== 23732
7JWE6w== 23733
IENhbWVyYQ== 23734
IHppZW4= 23735
IHN0eWw= 23736
IFRX 23737
IENoZWVzZQ== 23738
Ym9ybmU= 23739
IG9ibA== 23740
IEFscmVhZHk= 23741
IHVuc3RhYmxl 23742
IGZsYW1lcw== 23743
cG9zdA== 23744
SGE= 23745
cm9tYWdu 23746
IOyXhOuniA== 23747
ZGVzdA== 23748
IGtvbGVq 23749
IHRlbXBvcmFyaWx5 23750
IGRldGVybWluaW5n 23751
IEdsYXNz 23752
0YDQvtC9 23753
b2xhbg== 23754
IGRvbWluYXRlZA== 23755
5YyW 23756
X19fXw== 23757
INmH2LDYpw== 23758
IERhbmE= 23759
IGRpbmhlaXJv 23760
YXF1 23761
66+8 23762
IMOgcw== 23763
IEpvZXk= 23764
IEdyaWZm 23765
IGF0dGFpbg== 23766
IHRyYW5zaXRpb25z 23767
IExpdGVyYWxseQ== 23768
0LXQvdC0 23769
IEhhdmVu 23770
IGdyYWJiaW5n 23771
IGNyeXN0YWxz 23772
IEZvdXJ0aA== 23773
IGNhbmRsZXM= 23774
INGB0LvRg9GH0LA= 23775
cmljbw== 23776
IDUwMDA= 23777
ZXR0bw== 23778
IHVuZG8= 23779
IGt0bw== 23780
IGRpdmVydA== 23781
IGNoaXI= 23782
IHBlcnNlYw== 23783
IGhpa2luZw== 23784
IGFubm91bmNlbWVudHM= 23785
55Sx 23786
0LfRiw== 23787
IGF1Yw== 23788
IHN5c3RlbWlj 23789
IFJN 23790
z4POsQ== 23791
INCU0LY= 23792
IHlhcg== 23793
IFdhcmQ= 23794
IHBpc3NlZA== 23795
IGNhcm4= 23796
IGF1dG9ub21vdXM= 23797
44WO44WO 23798
c292ZXI= 23799
5rKS6Yyv 23800
5b6I5aW9 23801
IHJlZmxleA== 23802
IGdhcmRlbnM= 23803
IGRhdGVk 23804
7LE= 23805
YW1pxJk= 23806
IGNvbnRpbnVpdHk= 23807
IGNpdGl6ZW5zaGlw 23808
IHNjaHdlcg== 23809
IHphaw== 23810
dGFibGU= 23811
INGB0Yc= 23812
6KeB 23813
IM+DzrU= 23814
IGdlbmVyYXRlcw== 23815
6rWs64KY 23816
w7Zo 23817
w7Nt 23818
YWxhbQ== 23819
IEpVRFk= 23820
IEJ1Zw== 23821
IOOBpg== 23822
IGRyb25lcw== 23823
IMOhZ3Vh 23824
YWNha3M= 23825
5po= 23826
INCa0L7QvQ== 23827
15bXlA== 23828
IHN0cml2ZQ== 23829
IEFsdGVybg== 23830
IG5lYXJlc3Q= 23831
IHByb3llY3Q= 23832
dGVyYQ== 23833
IEFTSExFWQ== 23834
IHdvcm0= 23835
IHJlcGxheQ== 23836
IHRhcmE= 23837
IEluZGlhbnM= 23838
44Kw 23839
aWNhaWQ= 23840
IOyInA== 23841
IGFwcGVhbGluZw== 23842
IFdlcw== 23843
IG1lbnRpb25z 23844
INC00LXQu9C1 23845
IGt3 23846
IGZyYWdpbGU= 23847
aXN6 23848
a8Ozdw== 23849
aGFuZw== 23850
Y29sb3I= 23851
IHByZXNpZGVudGU= 23852
ODc= 23853
0LXRhA== 23854
54i4 23855
INC00L7QsdCw0LI= 23856
IE5lbHNvbg== 23857
w6FmaWM= 23858
IE1JQ0hBRUw= 23859
IG1lY2hhbmlj 23860
IG1ldHJlcw== 23861
IG9jenl3acWbY2ll 23862
IENpbmQ= 23863
IG9nc8Ol 23864
IGxhbmRzY2E= 23865
QUNF 23866
IGhlYWRsaW5lcw== 23867
IGNhdGFseXN0 23868
IENhdGNo 23869
aW5rbGVz 23870
IHBpbGxz 23871
b3Jkbw== 23872
IGltbWlncmFudA== 23873
IGV4YW1pbmF0aW9u 23874
IGFjY2lkZW50cw== 23875
esSFZA== 23876
IHF1aWVyZQ== 23877
IG5lbGxh 23878
IDY3 23879
IHBhc3Nh 23880
IHN1cGVyZmlj 23881
aXN0b3I= 23882
IG5vdg== 23883
64u1 23884
IG1hbmRhdGU= 23885
aXNvbnM= 23886
IFZpcnR1YWw= 23887
IHNlbGJlcg== 23888
IGNvdW5zZWxpbmc= 23889
IE5CQQ== 23890
IHNlcHQ= 23891
IGJlbGlldmVy 23892
IG1hcnZlbA== 23893
IEludGVncg== 23894
INC80ZY= 23895
IG9ycGg= 23896
IGJhY2t3YXJk 23897
IEdlbmVyYXRpb24= 23898
IFBpY3Q= 23899
INGC0L7Rgg== 23900
IHRhcGk= 23901
cHJvY2hlbg== 23902
IGhhbGx3YXk= 23903
aHRl 23904
INuB25I= 23905
IFp1bQ== 23906
6ICB5bir 23907
YWNobWVudA== 23908
aXF1ZXI= 23909
Zm9sZw== 23910
IEVkZGll 23911
IEtpbA== 23912
IHdlbGxuZXNz 23913
c3RvY2s= 23914
6LyD 23915
IGthw6c= 23916
IHRlcnJvcmlzbQ== 23917
IHBvaW50ZXI= 23918
T2Y= 23919
aGVyaWM= 23920
IFVsdGltYXRlbHk= 23921
IG1lc2Vz 23922
IFRyYWRl 23923
IHBpbnQ= 23924
IHR1aXRpb24= 23925
IGRpc2FncmU= 23926
IOqyjOyehA== 23927
IG1hbnVzY3JpcHQ= 23928
IHJvb21t 23929
IG91dHB1dHM= 23930
0LXRhtC4 23931
IHJpZXM= 23932
IHNhbHVk 23933
b3R6ZGVt 23934
IG1hc3Nlcw== 23935
IGJ5xYJh 23936
IGNsZWFyaW5n 23937
IGRpc2NvdXJzZQ== 23938
YXRzb24= 23939
IGZvbGRlZA== 23940
IEphcg== 23941
2YTZiQ== 23942
OTAw 23943
INGD0YHQvw== 23944
IHByb3BoZWN5 23945
IGludGVyZmVyZQ== 23946
0LjRhdC+0LQ= 23947
4LmM 23948
IHRocmk= 23949
INee16k= 23950
IGxhesSxbQ== 23951
IDE5OTI= 23952
IGZ1dHVybw== 23953
IGxvY2tpbmc= 23954
IGVtYmFyZ28= 23955
IE5laXRoZXI= 23956
aXZhbWVudGU= 23957
IG3DpXN0ZQ== 23958
IG1paw== 23959
IGNvbGxlY3Rvcg== 23960
0LXQutC+0YLQvtGA 23961
IEdhbmQ= 23962
IHNlbnRpcg== 23963
IE1pZ2h0 23964
5aGU 23965
IGdhbnplbg== 23966
VUM= 23967
IHJlbGF0aW5n 23968
U0Q= 23969
IG1vc3F1aXRv 23970
R1I= 23971
IGhvbGxvdw== 23972
4piF 23973
IFdhbGtlcg== 23974
IGFmZmlsaWF0ZQ== 23975
IGR1cGxpY2F0ZQ== 23976
0L3QtdC8 23977
IGdyYXBl 23978
IE9yZ2FuaXphdGlvbg== 23979
IHN5bnQ= 23980
Sm9l 23981
IGdlZw== 23982
IHJldmVhbGluZw== 23983
IEV0aGFu 23984
b3V0ZXI= 23985
IHlheQ== 23986
6auU 23987
0LvQsNGA 23988
IHJlcG9ydGVkbHk= 23989
IGlocmVy 23990
IHJlY29nbmlzZQ== 23991
IGJ1bXBlcg== 23992
IFJhbmR5 23993
IFZlbnVz 23994
dGxlcw== 23995
IGFwcGV0aXRl 23996
IGdsdWNvc2U= 23997
IGNob2R6aQ== 23998
IEZ1cnRoZXJtb3Jl 23999
dGly 24000
IGNvbnRh 24001
IGludHVpdGlvbg== 24002
IGFsdGl0dWRl 24003
IGNodW5rcw== 24004
IEpvc2h1YQ== 24005
xLHEn8SxbQ== 24006
cnlsaWM= 24007
bGVhbnM= 24008
IO2UvOs= 24009
TEw= 24010
UXVl 24011
IGdvcg== 24012
INC30L3QsNGH0LjRgg== 24013
IHBvZW1z 24014
IGV4Y2Vs 24015
IGV4cGxvcmVk 24016
IHBvcHVs 24017
IGluY2x1c28= 24018
c3TDpA== 24019
IEdhdmlu 24020
YWxsaW5n 24021
IM+Ezr/OvQ== 24022
6ak= 24023
YXJiZWl0 24024
IEdhcw== 24025
IGdsb3Jpb3Vz 24026
cmllYmVu 24027
IHNwYW0= 24028
IGluZG9vcg== 24029
IHRocnVzdA== 24030
IEFsZA== 24031
IFByaW9y 24032
IG9uYm9hcmQ= 24033
44Gg44GV44GE 24034
b2Nh 24035
QVNI 24036
o6A= 24037
IENocmlzdGluZQ== 24038
IGRyYXdlcg== 24039
IG5vb24= 24040
IOyemOs= 24041
IHBlcm1hbmVudGx5 24042
5rex 24043
INC90LDQv9GA0LjQvNC10YA= 24044
IHBvZGNhc3Rz 24045
ZXJhcGV1dA== 24046
cHJpdA== 24047
IHN0YWlubGVzcw== 24048
INqp25I= 24049
IGZhbWlsaWE= 24050
INGA0LDQt9GA 24051
dW50bw== 24052
INGB0YLQvtC7 24053
IGjDpA== 24054
IEhhaQ== 24055
IFBC 24056
aXpvbg== 24057
IGtvbm50ZQ== 24058
IGLDvHnDvGs= 24059
IHV0aWxpemFy 24060
2oY= 24061
IGFxdWVzdGE= 24062
IG1peGVy 24063
dWRlbnQ= 24064
0LvQtdC60YE= 24065
xYJ1 24066
INGB0LjRgdGC0LXQvA== 24067
INC90L7RgNC8 24068
IGZhdGFs 24069
IGNvbnNpZGVyYXRpb25z 24070
IHZhbGlkYXRpb24= 24071
IG9saQ== 24072
IGthcmRlxZ8= 24073
IEdMT1JJQQ== 24074
IHBhbGw= 24075
0LXRgdGC0LU= 24076
IHJlY3Rhbmc= 24077
IG1lZGlldmFs 24078
YWxsYWhp 24079
YXN0aQ== 24080
IFN5cmlhbg== 24081
IHNoZWFy 24082
IGRlYnVn 24083
IE1haQ== 24084
IGtub2NraW5n 24085
IExleA== 24086
YXJkYW4= 24087
cm92 24088
IG1lbW9yaWFs 24089
5rCj 24090
b29reQ== 24091
IHN0dWZmZWQ= 24092
IHBhc3PDqQ== 24093
IHdpZw== 24094
gqA= 24095
IHByw7N4aW1h 24096
IDE5OTE= 24097
INC80LXQttC00YM= 24098
IG51ZXN0cm9z 24099
IEJlYXN0 24100
IHNtbw== 24101
YXRjaGVk 24102
b2xvZ2lh 24103
INC80L7QtA== 24104
IGdlZQ== 24105
IGNvbmNlcHR1YWw= 24106
IMO0 24107
IGRlY3JlYXNlcw== 24108
IHF1ZXJpZXM= 24109
0L7Qu9GM0Yg= 24110
IEFwYXJ0 24111
IGV4ZW1wbA== 24112
5bGx 24113
IGZsZWQ= 24114
IE9GRg== 24115
Z2dhaw== 24116
IGJlYWQ= 24117
aGly 24118
bGllcw== 24119
IENsZWFybHk= 24120
xLFsYXI= 24121
IGNoZXNz 24122
IHdoaWNoZXZlcg== 24123
IDk2 24124
4bqx 24125
IHJlc3BlY3Rz 24126
INC80L7RgA== 24127
IG9yZ2FuaXNt 24128
IGdyYW5kcGE= 24129
IFZpZQ== 24130
6Lef5L2g 24131
IGZsb29kaW5n 24132
IHVwZ3JhZGVk 24133
0ZHRgA== 24134
IGNoZWVrcw== 24135
IGNvbnF1ZXI= 24136
IHN0dWJib3Ju 24137
IHB1enpsZXM= 24138
IGF1Y3Rpb24= 24139
IHJlbHlpbmc= 24140
IFBST0Y= 24141
IEVzcGVy 24142
INCc0KM= 24143
IGh5cGU= 24144
IHBvc3NpYmls 24145
IGltcHJpc29u 24146
IEVybg== 24147
7JeI7Iq164uI64uk 24148
IGVudmll 24149
IHJlc3VycmVjdGlvbg== 24150
5LiN6KGM 24151
IHNwZXI= 24152
IFZlbmV6dWVsYQ== 24153
c29t 24154
IOyeoOq5 24155
IG5vdXZlbGxl 24156
IGNsb3Nlcw== 24157
IDE5NDA= 24158
IHF1YQ== 24159
IEphcmVk 24160
IFBpcg== 24161
IGluZGU= 24162
IHNjcnVi 24163
dWt1 24164
IHJlcXVpcmluZw== 24165
INCy0LDQvNC4 24166
IGNvbnNpZGVyYWJsZQ== 24167
5ZCb 24168
aWxpYQ== 24169
IGlubmU= 24170
IG1laW5lbQ== 24171
IGhhcmRzaGlw 24172
IHRyYXBz 24173
cm9j 24174
IOyEpOs= 24175
IHJlc2VhcmNoaW5n 24176
IE1hcmdhcmV0 24177
IHBlbm55 24178
IGLEsXJhaw== 24179
0ZHQuw== 24180
IHdvb2w= 24181
IHJoZXQ= 24182
IGZsYXR0ZW4= 24183
54c= 24184
4LmA4Lij 24185
IHBpZWQ= 24186
IENoYXA= 24187
IHVuZGVybQ== 24188
IGZyZXQ= 24189
IGNyYXNoZWQ= 24190
IEZyYXVlbg== 24191
2LDZhw== 24192
aXZhbg== 24193
IGxpdGVyYXJ5 24194
bGF0ZWdv 24195
IHNww6R0ZXI= 24196
IHNpbWlsYXJpdGllcw== 24197
4oY= 24198
IENvcm9u 24199
IENyZWVr 24200
IGJvc3Nlcw== 24201
IGFjY29tcGFuaWVk 24202
IGRlYmF0ZXM= 24203
IGFzc2VtYmxlZA== 24204
IMOB 24205
IFZhaQ== 24206
IHRyYWN0 24207
IHNpbXBsZW1lbnQ= 24208
IEFyaW4= 24209
IHZ1bG5lcmFiaWxpdHk= 24210
IGhvcm1vbmU= 24211
SUVM 24212
T09L 24213
IHJlbGF5 24214
IEFuZHJlYQ== 24215
cmls 24216
IG5lY2Vzc2l0eQ== 24217
YWNldXRpY2Fs 24218
0Y7RiQ== 24219
b3VzaW5n 24220
bmFobWVu 24221
IGZvb3RwcmludA== 24222
bWFw 24223
IFRpZXI= 24224
YW5ueWE= 24225
aW50ZW5k 24226
5Zau 24227
5aI= 24228
IGRlY29yYXRl 24229
IHpvbWJpZXM= 24230
IEh5ZA== 24231
IFN1eg== 24232
IGNhbXB1c2Vz 24233
IEVtYg== 24234
IHRocm90dGxl 24235
IGFkbWlu 24236
IG9wb3J0dW4= 24237
IG1pcnJvcnM= 24238
IGlkZW50aXRpZXM= 24239
IENsaW4= 24240
IOu5hOs= 24241
4bmj 24242
IE90dA== 24243
IGJsdWVz 24244
IGltcHJlc3Npb25z 24245
LSw= 24246
IHZhZ3Vl 24247
YWZl 24248
IGluZmVyaW9y 24249
ZXJhbGQ= 24250
IG1lZGljaW5lcw== 24251
IHByZWd1bnRh 24252
b3NlbHk= 24253
IHTDqWzDqQ== 24254
IE1vbnRo 24255
IExlYWRlcnM= 24256
IEVneXB0aWFu 24257
IHJhdGlvbg== 24258
a2Vycw== 24259
aGVpdHM= 24260
IHJlY2h0 24261
UGxheQ== 24262
IGVn 24263
IHBvbGxz 24264
IFdPT0RS 24265
IHNsb3Rz 24266
amFt 24267
Qm90aA== 24268
IFJhdA== 24269
0YDQsNC2 24270
IEJyaWdodA== 24271
5LiA5a6a 24272
4buRaQ== 24273
dXJpb3Vz 24274
IHNpbmdlcnM= 24275
IGxvZ2lu 24276
IHTDqm0= 24277
bGF0aW9u 24278
IE11bQ== 24279
xrDhu51uZw== 24280
IEVkaXRvcg== 24281
5ZCR 24282
IGlubm92YXRpb25z 24283
aGF2ZQ== 24284
IFNlaw== 24285
IHdlYWtlcg== 24286
IEdvYg== 24287
QWZ0ZXI= 24288
tOyngA== 24289
IOusuOygnA== 24290
44O844O8 24291
IGRpc2FkdmFudGFnZQ== 24292
56K6 24293
IGdhemU= 24294
IE1hY2s= 24295
z4HOrw== 24296
IEtpc3M= 24297
IEhvbG8= 24298
IEJpcnRo 24299
aXpp 24300
YmFi 24301
5L+d 24302
7Iuc6rOg 24303
0LTQtdGA0LY= 24304
IHNxdWF0 24305
0LrRg9GB 24306
dW5p 24307
IENvbW1l 24308
IFdPT0RSVUZG 24309
IENoYW1waW9uc2hpcA== 24310
IHdlbGNoZQ== 24311
IFlvdXRo 24312
emVt 24313
IG9kcG93 24314
IHBlcnNpc3RlbnQ= 24315
cnV0 24316
7JSp 24317
7Zal 24318
bGFpcg== 24319
aWt1 24320
IHZlbmRvcg== 24321
IGNow7puZw== 24322
IGZpbmFuY2k= 24323
IG92ZXJseQ== 24324
w6J1 24325
IGdsdXRlbg== 24326
IDE4MDA= 24327
IGRpdmlzaW9ucw== 24328
IGNpdWRhZA== 24329
IG9iZWQ= 24330
IHdhcnVt 24331
IGVoZXI= 24332
IGVsaW0= 24333
INCS0L4= 24334
IHBldXZlbnQ= 24335
IFdhbm5h 24336
IGF0dGVuZGFuY2U= 24337
IGFzc2Vzc21lbnRz 24338
IEJvZw== 24339
IGltYWdlcnk= 24340
IGNvbGxlY3RpdmVseQ== 24341
IGluZm9ybWFs 24342
IFNjaHdl 24343
IGRldXRsaWNo 24344
IENoZWw= 24345
IFBF 24346
b3dlZA== 24347
IGJhbm5lcg== 24348
IHNoZWx2ZXM= 24349
IFJldHVybg== 24350
5ou/ 24351
TEFVR0hT 24352
IGNvbmdyYXR1bGF0ZQ== 24353
IE5vcndheQ== 24354
IGR3ZWxs 24355
IENhcmliYmVhbg== 24356
IG5vcm1z 24357
IEFuaW1hbA== 24358
IFZhbGVudGluZQ== 24359
IGV4dGVuZGluZw== 24360
IFZvdQ== 24361
b3Jy 24362
IENoZW5n 24363
wqE= 24364
INC00L7RgNC+0LM= 24365
IHZlZw== 24366
IGjDpQ== 24367
IFhpbg== 24368
IOy5tOs= 24369
ZW1ldA== 24370
IGh5cG90aA== 24371
IGludGVyZXNzYW50ZQ== 24372
cmljZXM= 24373
SVo= 24374
IFVTRA== 24375
IHJ1bm5lcg== 24376
IEJhZw== 24377
IOq9 24378
IGNvbWXDp2Fy 24379
IHBpZ3M= 24380
IHdlYWtuZXNzZXM= 24381
UGg= 24382
IFZpb2w= 24383
5LiN55So 24384
IGRyYWdnaW5n 24385
IEFxdcOt 24386
IENTUw== 24387
IG1pbGxpbWV0ZXJz 24388
IGVzdMOhcw== 24389
IGFjdXRl 24390
IGRlamFy 24391
acSf 24392
b2JyYQ== 24393
TG92ZQ== 24394
IHNpbGs= 24395
KioqKg== 24396
IGpvaW5z 24397
IHByb2w= 24398
IOqwkOyCrO2VqeuLiOuLpA== 24399
5pSv 24400
2K3Yrw== 24401
YWdoZXR0aQ== 24402
w6RubmVy 24403
IHN0cmFuZw== 24404
IGRvdWJsZWQ= 24405
IGRlc2NyaXB0aW9ucw== 24406
IHN0ZWxsZW4= 24407
IHBhcnRp 24408
56uL 24409
soTr 24410
IMO2xJ8= 24411
aWdoaW5n 24412
IGFuZ3VsYXI= 24413
IG5hdHV1cg== 24414
IFNoZWw= 24415
xrDGoQ== 24416
IHJheXM= 24417
IHNlcGVy 24418
c3RhcnQ= 24419
dmlzZWQ= 24420
IHJ1c2hlZA== 24421
IGludGVybmF0aW9uYWxseQ== 24422
IG5pdmVs 24423
IGJveGluZw== 24424
ZmFsbGVu 24425
4buRYw== 24426
IHNlaW5lbg== 24427
cGxpY2l0eQ== 24428
IGNhcmJvaA== 24429
IFRyYXZpcw== 24430
dXNv 24431
IFBoYXNl 24432
IGFjdGl2YXRpb24= 24433
IG9waW8= 24434
t6g= 24435
IGRlY3JlYXNlZA== 24436
Q2Fy 24437
IGJ1bmRsZQ== 24438
IGV4cGVuZA== 24439
b3JtYWw= 24440
IGFkamFjZW50 24441
IG1lZQ== 24442
INC+0YDQsw== 24443
IHRyYW5zY3JpcHQ= 24444
IExhbmd1YWdl 24445
R1M= 24446
6KeJ 24447
IHNldWw= 24448
w6BuaA== 24449
IG55YQ== 24450
bmluZ3M= 24451
IOyLnOs= 24452
IOuUsOudvA== 24453
IEFncg== 24454
w61k 24455
55WZ 24456
IGFieQ== 24457
IE5lbw== 24458
xLF5b3J1eg== 24459
IFRoaW5raW5n 24460
YWltZQ== 24461
IHZpdGU= 24462
IHRyYXbDqXM= 24463
INeR16I= 24464
INC80LXQtA== 24465
T3Vy 24466
aG9vdA== 24467
IGxpbmVy 24468
IFBpenph 24469
IGh5Zw== 24470
ZmxpZXM= 24471
IENvbnRpbnVl 24472
IGRlbnRhbA== 24473
IFRpYg== 24474
IHJlZ3VsYXRl 24475
bGllw58= 24476
QUxL 24477
IFRhZQ== 24478
6ri4 24479
IEJyZXhpdA== 24480
IEd1dA== 24481
IG9jY3VwYXRpb24= 24482
IHpyb2Jp 24483
w6Jt 24484
IHdoaXNr 24485
5LiW55WM 24486
IGthbnNrZQ== 24487
b21vbg== 24488
cm9iZQ== 24489
IHdhcmZhcmU= 24490
IHRo4buD 24491
IGpha2k= 24492
IHN0cm9rZXM= 24493
IHBlYXM= 24494
IERhbWl0 24495
SEFO 24496
IGludGVyZmVyZW5jZQ== 24497
INC80LjQvdGD0YI= 24498
TkVS 24499
b3V0aW5n 24500
IHRleHR1cmVz 24501
n4k= 24502
b3dp 24503
IO2VmQ== 24504
IGRlbnM= 24505
IHByb3RhZ29uaXN0 24506
w6Rubg== 24507
IGdvZGRlc3M= 24508
IHdvbGx0ZQ== 24509
aWpv 24510
IFdvY2hl 24511
IFZQTg== 24512
c3Rvcnk= 24513
IGtpbmRlcmc= 24514
IGZ1bm5lbA== 24515
IGRpc3RyZXNz 24516
0L3QvtGB0YLRjNGO 24517
IG5vaXN5 24518
INC/0YDQvtC00L7Qu9C2 24519
IGRhcmFu 24520
IGVuenltZQ== 24521
0LvQvtC2 24522
IG11dGU= 24523
IGR3YXI= 24524
INin2LM= 24525
IGtvbXBs 24526
IG1lcml0 24527
IGZvc3Nl 24528
IERyaW5r 24529
IGZvcmE= 24530
IHdvaGw= 24531
IGJyZWV6ZQ== 24532
IHNhbml0 24533
IGRyaW4= 24534
IOydtOqxsOuKlA== 24535
IDYy 24536
IOywqOs= 24537
YWJ5dGVz 24538
IGRlZWRz 24539
INC5 24540
acOobWU= 24541
aWdnbGluZw== 24542
ICIn 24543
INGH0LDRgdGC0Yw= 24544
IEFuc3dlcg== 24545
IGV2YW5nZWw= 24546
IDEwODA= 24547
IFZpc2l0 24548
aWNpZW50 24549
IHJlbGlhYmlsaXR5 24550
0Y7RgdGM 24551
IEVhcmxpZXI= 24552
IGZpZA== 24553
562J5LiA5LiL 24554
IHNsZWV2ZXM= 24555
aXlvcnN1bg== 24556
IGJpYg== 24557
IEFjY291bnQ= 24558
0Y/Qu9C4 24559
Y2lwbGluYXJ5 24560
emFz 24561
INCx0LXRgA== 24562
IG5lY2tsYWNl 24563
IGJsZW5kZXI= 24564
IFBoaWxsaXBz 24565
ZXRp 24566
IEp1cGl0ZXI= 24567
IHByb3ZvYw== 24568
IFllYXJz 24569
ZW50cmU= 24570
YWNpbw== 24571
IGvDvA== 24572
IGFudGVubmE= 24573
IG5vdmVscw== 24574
IGZhcnQ= 24575
IFN1Z2Fy 24576
IEp1ZHk= 24577
IGNvbGxhcHNlZA== 24578
57A= 24579
cml0aXM= 24580
IOyDge2ZqQ== 24581
0JfQqw== 24582
IFZlcmY= 24583
cmFuZWFu 24584
ZXJldW0= 24585
IFRhcmdldA== 24586
IDg4 24587
INCY0Lc= 24588
aWRlbw== 24589
IHJlZ3Jlc3Npb24= 24590
7Lac 24591
IG3Ds3dp 24592
IHN0dWRpb3M= 24593
aWVucw== 24594
aXBo 24595
IGZyeWluZw== 24596
IGZhc2NpbmF0ZWQ= 24597
IFdhaA== 24598
YnVja3M= 24599
bWF5YQ== 24600
IFNhdHVybg== 24601
IE1vbW15 24602
IHJhdGluZ3M= 24603
IGF1dHVtbg== 24604
xrDGoW5n 24605
IGxvc2Vy 24606
IGNlbnRybw== 24607
w6lyaWV1cg== 24608
IEZvbGQ= 24609
IHN1cGVydmlzb3I= 24610
IE5vYmVs 24611
IHVuZGVyZXN0 24612
b2JpYQ== 24613
INCy0YHRjw== 24614
IHZlcnc= 24615
IGZ1ZWxz 24616
IGFydGlmYWN0cw== 24617
IOu2mQ== 24618
IEF1dG9t 24619
55qE5piv 24620
25Q= 24621
15XXoQ== 24622
IGlobmVu 24623
IDU5 24624
b3VuZGluZw== 24625
0LXRgNGL 24626
aW5hcnM= 24627
Y2hhbnQ= 24628
IGFkZGljdGVk 24629
IGV4cGxvc2l2ZQ== 24630
IGRpc3BlcnM= 24631
4paI 24632
YXhpcw== 24633
QVJZ 24634
IGx1bQ== 24635
INGD0YHQuw== 24636
INiM 24637
IHJ1cGVlcw== 24638
IFBlYXJs 24639
Y2FtcA== 24640
dHY= 24641
b3lh 24642
IGNvbmNsdWRlcw== 24643
IGNvbGxpc2lvbg== 24644
IGJ1eWVy 24645
IHBsYXlncm91bmQ= 24646
IHNwcmluZ3M= 24647
IGZlbWluaW5l 24648
IFJhcw== 24649
IGluY2FyY2Vy 24650
7ZeY 24651
IGRpYWxlY3Q= 24652
IGNsb3N1cmU= 24653
IGNoYXR0aW5n 24654
IGJhYmU= 24655
IHNwb3RsaWdodA== 24656
IG5vdGF0aW9u 24657
6Lev 24658
U3Rhcg== 24659
acOjbw== 24660
IHTDqnRl 24661
IHRpZGU= 24662
IGp1bnRv 24663
IHNlbmF0b3I= 24664
0KU= 24665
IGV4Y3VzZXM= 24666
IGJsaW5r 24667
IGFkbWlzc2lvbg== 24668
IExpbHk= 24669
0YvQvNC4 24670
IGFtaWdv 24671
IGx1c3Q= 24672
64us 24673
IGFtaW5v 24674
5LqL5oOF 24675
IGNvbnN1bHRhbnQ= 24676
IEVsZWN0cmlj 24677
IOuFuOuemA== 24678
dWphaA== 24679
IHNob290ZXI= 24680
aWNodGVu 24681
IFVrcmFpbmlhbg== 24682
IGFpbXM= 24683
IEVudGVydGFpbg== 24684
IG1pcmFjbGVz 24685
6K2w 24686
IHplaWdlbg== 24687
IGxhbQ== 24688
IHJlc3M= 24689
IEppbGw= 24690
eWxhbg== 24691
IHJvb2s= 24692
IGhheWE= 24693
IHBhc3Nwb3J0 24694
YWRhdGE= 24695
IGp1aWN5 24696
Y29uZg== 24697
0LvQtdC5 24698
IFN6 24699
IGludGVyY2VwdA== 24700
44GC44KK44GM44Go44GG44GU44GW 24701
IFRlYW1z 24702
IG1ha2Vu 24703
aXJyZWw= 24704
IExJS0U= 24705
4bqteQ== 24706
6rWw 24707
IHNob3J0YWdl 24708
IHBhcmFkaWdt 24709
IHBhcGVs 24710
IGFzdGVybw== 24711
44G+44Gf 24712
IHNvbGxlbg== 24713
IE1pY2tleQ== 24714
IE9ybGVhbnM= 24715
IGNob2xlc3Rlcm9s 24716
IGdvb3Nl 24717
0YbQuNGO 24718
44GC44KL 24719
IEZM 24720
INCz0L7Qu9C+0LI= 24721
IHRyaWJ1dGU= 24722
IEdhbQ== 24723
IMOpdmlkZW1tZW50 24724
0Y/RhQ== 24725
5a6e 24726
55Sw 24727
IGluYXBwcm9wcmk= 24728
dWhhbg== 24729
IG9yZ2FuaXphdGlvbmFs 24730
YWlsZWQ= 24731
IGVuZHVyZQ== 24732
IDc2 24733
IHNob3RndW4= 24734
IGxpdnJl 24735
IHN1aXRlZA== 24736
IHdhcm10aA== 24737
IFNJTQ== 24738
IGVudmlzaW9u 24739
IGRlZ3JhZA== 24740
w65uZQ== 24741
TGF1Z2hpbmc= 24742
IFdob2V2ZXI= 24743
IEJ1ZGRoaXNt 24744
IHNwcmlua2xl 24745
Y2XEn2l6 24746
IHJ1aW5z 24747
IHN0YXJjaA== 24748
IEhlcno= 24749
IGluanVzdGljZQ== 24750
IGh1bWlkaXR5 24751
0L7QttCw0LvRg9C5 24752
IE9iamVjdA== 24753
IElnbg== 24754
IEV4YW0= 24755
aWdlcnM= 24756
IHRob3U= 24757
IFNveQ== 24758
aXZhcw== 24759
IHBvbGVz 24760
bWF0aA== 24761
INCy0L3QuNC8 24762
SU5HSU5H 24763
ZWRyYWw= 24764
IGV4cGxvcg== 24765
IHJvYXN0ZWQ= 24766
IGNyYXds 24767
IGNvZmY= 24768
IGFub20= 24769
IHdpag== 24770
IGltcHJvdmVz 24771
IHRyZWF0eQ== 24772
IGRpc2NvdmVyaW5n 24773
IHN0YXR1dGU= 24774
IG1lcmNhZG8= 24775
INGB0LjQuw== 24776
IGludGVs 24777
IENoYW5jZWxsb3I= 24778
IE1lZGljYWlk 24779
dWdp 24780
IHZlcmJhbA== 24781
IGTDtm4= 24782
IHNjcmlwdHVyZQ== 24783
IGl0ZXJhdGlvbg== 24784
ZWtz 24785
IE94Zm9yZA== 24786
IHfDpGg= 24787
IFZhZA== 24788
IEFL 24789
IOyVhOydtOs= 24790
IGlldHM= 24791
IG5lZWRsZXM= 24792
2YPZhQ== 24793
IHBhc2Fkbw== 24794
IGFsYnVtcw== 24795
IHllYQ== 24796
ZXR6ZW4= 24797
hOuPhA== 24798
IGRldGVybWluZXM= 24799
IHRoZWU= 24800
IFBsYXlpbmc= 24801
w6RydA== 24802
INem 24803
Y2xlZA== 24804
IGRvd253YXJk 24805
YWxvbmU= 24806
IHNvbHU= 24807
IHBhcnRpdGlvbg== 24808
IHd6 24809
ZGQ= 24810
IHBlc3NvYWw= 24811
5aq9 24812
IGZhY3Rvcmllcw== 24813
IGJsZWlidA== 24814
4Lih4Liy 24815
YWxzYQ== 24816
IE5GTA== 24817
IGZ1ZXJh 24818
IHJlc2VydmVk 24819
IEVhcm4= 24820
IGhlbHQ= 24821
IHNob3J0Y3V0 24822
IGNvbnZpbmNpbmc= 24823
c3BhY2U= 24824
IGVuZm9yY2U= 24825
IGNvcmVz 24826
IGVmdGVy 24827
IHJlY2Vzc2lvbg== 24828
eGljbw== 24829
IHByb3Bvc2l0aW9u 24830
YXJpYW5z 24831
cm9wb2w= 24832
IOuqsOs= 24833
IM6c 24834
IOyalOymmA== 24835
IGFjdGl2aXN0 24836
IGNvbnZpY3Rpb24= 24837
IHphYg== 24838
IGNhbmNlbGVk 24839
0YLQvtGH0L3Qvg== 24840
IM6u 24841
6YCZ5qij5a2Q 24842
bml0ZQ== 24843
IGZ1bmRyYQ== 24844
YnV6emVy 24845
0LXQu9C+ 24846
aWNhdGlvbnM= 24847
IHpvbmE= 24848
IHRlZW5z 24849
IG1ldGhvZG9sb2d5 24850
IOykkeyalA== 24851
dGhhbg== 24852
IFVs 24853
IEdyZXk= 24854
IGhvZw== 24855
SU5L 24856
IFN1bmc= 24857
IENsYXVk 24858
IENOTg== 24859
IGRlbGl2ZXJz 24860
YWxpbg== 24861
IEFkb2Jl 24862
b3RoZQ== 24863
IERlc3dlZ2Vu 24864
4Liz 24865
IHdlcmRl 24866
IGdyZWFzZQ== 24867
IHVwZ3JhZGVz 24868
IEZpbmxhbmQ= 24869
YWNjZXB0 24870
IGludGVycm9n 24871
YmVl 24872
IOOBqw== 24873
IHByZWRl 24874
IE5lcA== 24875
IENhbWJyaWRnZQ== 24876
IGdyYXBocw== 24877
IGhhdW50ZWQ= 24878
0YHQtdC8 24879
5qc= 24880
5YWL 24881
U29tZQ== 24882
IE1hbGw= 24883
IHJlaGVhcnNhbA== 24884
IFVyYmFu 24885
IExhZw== 24886
IG5pbQ== 24887
6rCV 24888
IHBvc2l0aW9uZWQ= 24889
IGF2b2lkZWQ= 24890
RU1B 24891
IGxsZWdhcg== 24892
IHLDoXBpZG8= 24893
IGdvdXZlcm4= 24894
IGhpbmc= 24895
IGRlYWxlcg== 24896
IHJlZm9ybXM= 24897
IGZhdHR5 24898
0LrQvtC7 24899
IEFjZQ== 24900
IG5lcA== 24901
IOyyrQ== 24902
IGNvbXB1dGF0aW9u 24903
IFN0cmVhbQ== 24904
Ym91cm5l 24905
dHVy 24906
UG9y 24907
IHNsZWVweQ== 24908
IGJhbmdldA== 24909
44GC44Gu 24910
IHdlaWdocw== 24911
IGJsZWliZW4= 24912
IEdyZW4= 24913
IHVuaW9ucw== 24914
IOq1kA== 24915
IGFwcmVuZGVy 24916
dWl0YXI= 24917
IEplc3Q= 24918
dW1pbmc= 24919
IFBsYXllcg== 24920
IEV4dHJlbQ== 24921
IGludGVnZXI= 24922
0LDRh9C1 24923
IGNvbmNlcnRz 24924
15XXmw== 24925
IHRyb2NoxJk= 24926
IFJlcGU= 24927
6YeN6KaB 24928
4LmC 24929
xbxlbg== 24930
IHNvdW5kaW5n 24931
IGFub255bW91cw== 24932
IGV4Y2E= 24933
IElyYW5pYW4= 24934
IGVuZXJnZXRpYw== 24935
IHdpdmVz 24936
INGG0LLQtdGC 24937
IGFpcw== 24938
44GL44Gq 24939
IHN1ZGFo 24940
IHVuZGVyd2Vhcg== 24941
IGNydW5jaHk= 24942
IFBhaW4= 24943
IGdlcsOnZWs= 24944
cmVkaWN0 24945
IG1pc21h 24946
0ZbRgg== 24947
IHN1cnZpdmluZw== 24948
zq3Pgg== 24949
IHBhcnRpY2lwYW50 24950
IEhlc3Nlbg== 24951
w6FyaWFz 24952
IHN1YndheQ== 24953
aXN0w6Q= 24954
IGNvcmFs 24955
IG1hcmlqdWFuYQ== 24956
IE1lbW9yaWFs 24957
0YjQuNC5 24958
cml6 24959
IHNhdGVsbGl0ZXM= 24960
IGxlYXNl 24961
IENhbWVyb24= 24962
dW1waA== 24963
IGNsYXNzbWF0ZXM= 24964
w6Row6Ru 24965
0YHRgtCy0LU= 24966
IGh1ZQ== 24967
k6TsnYQ= 24968
IHByb3BvcnRpb25hbA== 24969
IG5vc3M= 24970
IGxhcHM= 24971
csOl 24972
IGJpdGNvaW4= 24973
0JfQq9Ca0JA= 24974
IOy2qQ== 24975
INmE2YQ= 24976
IE1vcnQ= 24977
IEVzcA== 24978
YXJub3M= 24979
INGB0LrQsNC30LDQuw== 24980
IMOkbmQ= 24981
5YWE 24982
15nXmded 24983
IEdlYg== 24984
Z2VoZW4= 24985
SW5hdWRpYmxl 24986
Ym9yb3VnaA== 24987
0YTRhA== 24988
IGZlbGxvd3NoaXA= 24989
IFBhcGVy 24990
IGN1cnZlZA== 24991
IEdFT1I= 24992
IGNhbGN1bGF0b3I= 24993
IENhdGFs 24994
IHbDoG8= 24995
IGJ5cGFzcw== 24996
0LvQtdGC 24997
4LM= 24998
dHJhbnM= 24999
cmVuY2llcw== 25000
7KGM 25001
aWdlbnQ= 25002
IHRhc3RlZA== 25003
IG9jZWFucw== 25004
dWZ0 25005
ZXJ2aWNl 25006
INCc0KPQl9Cr0JrQkA== 25007
IENsYXNzaWM= 25008
IHJlc3BlY3RpdmVseQ== 25009
fik= 25010
w650cmU= 25011
IE5hc2g= 25012
IHppdA== 25013
IOybgw== 25014
IOuGkg== 25015
cXVvdGU= 25016
IFVucw== 25017
IHRhYw== 25018
IHByb3Zlcw== 25019
IFBvcnRsYW5k 25020
Ymx5 25021
IGVyZQ== 25022
7LaU 25023
IMOpcG9jYQ== 25024
INGC0YvRgdGP0Yc= 25025
NzY= 25026
IGhhZGU= 25027
IEZybw== 25028
IHBvbMOtdGljYQ== 25029
dGFn 25030
IO2VrQ== 25031
IHNjaMO2 25032
YXJldHQ= 25033
IHByb3Zpc2lvbnM= 25034
IG1vdG9ycw== 25035
IGltYWdpbmc= 25036
IGRvaw== 25037
dWxvdXNseQ== 25038
IG1laWxsZQ== 25039
546w5Zyo 25040
65A= 25041
IElTTw== 25042
IFNURU0= 25043
IEJvd2w= 25044
IHRvd2Vycw== 25045
IEVl 25046
IFBlcmZvcm1hbmNl 25047
IGxvaW4= 25048
Y3Vzc2lvbg== 25049
IGNvYXN0YWw= 25050
aWFsZQ== 25051
Y29tcGFzcw== 25052
IHNwZWxscw== 25053
IGRpc2FwcG9pbnRpbmc= 25054
IOuyiOynuA== 25055
RUVS 25056
IHZlcnNhdGlsZQ== 25057
YXN1cnk= 25058
IGVuZmlu 25059
IGRvd25zaWRl 25060
IGd1aWRpbmc= 25061
INin2YTZgg== 25062
IG5pbmV0eQ== 25063
Y2hhcmdlZA== 25064
IEZhbnM= 25065
IHBoaWxvc29waGljYWw= 25066
IGdhcm4= 25067
IG3DpW5nYQ== 25068
IHdpbGxpbmduZXNz 25069
IHBvcnRpb25z 25070
YWJlbg== 25071
IO8= 25072
wr8= 25073
cmF1bA== 25074
IHNwcmludA== 25075
aWZlbg== 25076
xLF5bGE= 25077
INC60YPQvw== 25078
44GP44Gg44GV44GE 25079
IGVuc3VpdGU= 25080
IENhcGl0b2w= 25081
IDYz 25082
INCz0L7QstC+0YDQuNGC 25083
IGFwcG9pbnRtZW50cw== 25084
5om+ 25085
b21pYXN0 25086
IGNhcmVn 25087
IHB1Ymxpc2hlcg== 25088
IGhlcmF1cw== 25089
IM61zq8= 25090
IFZT 25091
44Gd44GX44Gm 25092
5Lit5YWx 25093
IHNhY3JpZmljZXM= 25094
dGhpcmQ= 25095
IGh1bWFuaXRhcmlhbg== 25096
IOuCtOw= 25097
aW1vbg== 25098
IGluZXF1 25099
IHpvYg== 25100
IGNvbWZvcnRhYmx5 25101
IERpbmdl 25102
IGNhbmNlbGxlZA== 25103
IFBTQUtJ 25104
IFJvYmluc29u 25105
IGZpbnM= 25106
KT8= 25107
IEhpc3Rvcg== 25108
INGH0LXQu9C+0LLQtdC60LA= 25109
IHRic3A= 25110
dGV4dA== 25111
a2lt 25112
IHVwZGF0aW5n 25113
IGdlbGQ= 25114
ZmVsZA== 25115
j7w= 25116
IG3DpA== 25117
IGNhZsOp 25118
1oA= 25119
IFNyaQ== 25120
IFJlZ2lvbg== 25121
IEhhaGFoYQ== 25122
IGZpbmFuY2Vz 25123
INin2YTYtA== 25124
IGJ1bms= 25125
cnVr 25126
aGFmdA== 25127
IGxhdGVyYWw= 25128
IGV4dGVuc2lvbnM= 25129
IOyVhOydtA== 25130
IGRlZmluaXRl 25131
IFpoYW8= 25132
IEx1aXM= 25133
c3R5 25134
IGNhc29z 25135
IEtsaW0= 25136
IDE5OTM= 25137
IHJlYWxpemF0aW9u 25138
IGhpc3Rvcmlhbg== 25139
IGNyYWNrZWQ= 25140
64K0 25141
IHN5c3TDqG1l 25142
IENJQQ== 25143
INGC0LLQvg== 25144
b3NwaGVyaWM= 25145
IGZsZWU= 25146
IHLhuqV0 25147
IFJlZ2FyZGxlc3M= 25148
IHJlbHVjdA== 25149
IHRpbWVseQ== 25150
IEp1bGlhbg== 25151
R00= 25152
6ZI= 25153
YWR1cmE= 25154
6aOf 25155
IGRyZXNzZXM= 25156
54Gj 25157
IOuUlA== 25158
IG5vbWluYXRlZA== 25159
IGFkdm9jYXRlcw== 25160
eW1waA== 25161
IHJlY29yZGluZ3M= 25162
IGRldmlhdGlvbg== 25163
IHByaW9yaXRpemU= 25164
IHNwaXJhbA== 25165
IFlPVVI= 25166
IHRyYW5zcG9zZQ== 25167
YW1wb28= 25168
IOybkOuemA== 25169
IFZpc2lvbg== 25170
IHBvbGl0ZQ== 25171
IGhhbWI= 25172
IFBhdGllbnQ= 25173
5q+U6LyD 25174
7YGs6w== 25175
IHNpYQ== 25176
IOqzsw== 25177
IMW+ZQ== 25178
6KeA 25179
IHN1cGVybWFya2V0 25180
67k= 25181
IFNpZXJyYQ== 25182
IGdyaWxsZWQ= 25183
IFVwb24= 25184
IGFic2VudA== 25185
IG1lYw== 25186
IEFwb2xsbw== 25187
IHB1bms= 25188
IFBhxYRzdA== 25189
INGB0LLQvtC5 25190
IOqxsOq4sA== 25191
R2lybA== 25192
IHNraW5ueQ== 25193
IFByZW1pZXI= 25194
IHRlcnJpdG9yaWVz 25195
IGxpYWJpbGl0eQ== 25196
IGplcms= 25197
cmF0aWM= 25198
IGRhbmNlcnM= 25199
INGD0YDQvtCy 25200
IOq0gOs= 25201
b25seQ== 25202
IFN0dQ== 25203
IHNrZWxldG9u 25204
IOutkOs= 25205
INC30LDQutC+0L0= 25206
xLFrdA== 25207
IE1JS0U= 25208
IGzDtg== 25209
bWll 25210
IHJlaXRlcg== 25211
44GT44KM44Gv 25212
IEtvbGxlZw== 25213
IEFkYW1z 25214
bGljaGVy 25215
IMOnb2N1aw== 25216
0Y/Qsw== 25217
IGJsdXNo 25218
IHN1bnNoaW5l 25219
IGV6 25220
IERldmls 25221
IOq4uA== 25222
IOOBig== 25223
YWRk 25224
IGxpY2Vuc2Vk 25225
IHZpbnls 25226
IEN6ZWNo 25227
aW1hZw== 25228
IGNyYWNraW5n 25229
IOy6 25230
IHVkYWg= 25231
IHNvbW1lcw== 25232
IOyWvOq1 25233
d2HEhw== 25234
IGZyZXM= 25235
5ZG9 25236
IFdhbG1hcnQ= 25237
INCi0LXQv9C10YDRjA== 25238
YXRpc2Y= 25239
Q0k= 25240
bGFuZw== 25241
IGRpZmZ1c2lvbg== 25242
55S3 25243
IHNvbW9z 25244
IE1ha2Vz 25245
5oiR5oOz 25246
IFJpY2t5 25247
IG11Y2hh 25248
7ZWo 25249
IGhvcnNlcG93ZXI= 25250
YXNpYQ== 25251
IGZpYmVycw== 25252
IGVybQ== 25253
0YHQutC40LU= 25254
IGplc3Rl 25255
IGZpcmVmaWdodA== 25256
IGN1aXNpbmU= 25257
IGJlc29uZGVycw== 25258
ZGln 25259
IOyihQ== 25260
INGD0LY= 25261
IHRyYWNpbmc= 25262
IGNlcnRhaW5z 25263
IEFwcGx5 25264
0YvQstCw0YLRjA== 25265
54w= 25266
IGJydQ== 25267
IFlFUw== 25268
IEJhaQ== 25269
IERpdA== 25270
IEJpcw== 25271
IHVubGU= 25272
0YHRgtCw0YLQvtGH0L3Qvg== 25273
IEF3YWs= 25274
Li4i 25275
IDEyNQ== 25276
IHJvb3RlZA== 25277
IGNhdXRpb3Vz 25278
Y29uc3Q= 25279
IG9yY2hlc3RyYQ== 25280
55y8 25281
INCy0L3Rg9GC 25282
IHF1ZWxxdQ== 25283
INC+0YLQstC10YI= 25284
IE1ldGhvZA== 25285
7Lmc 25286
IM68zrHPgg== 25287
bMO8 25288
IOyVhOq5jA== 25289
IG5hbWluZw== 25290
Q2hhcg== 25291
IFNpY2hlcg== 25292
IHByaXZpbGVnZWQ= 25293
IEZseQ== 25294
IOOBiw== 25295
4bqtdA== 25296
IGFkdmFuY2Vz 25297
IFplbGRh 25298
IGFuZHJh 25299
IGdyaW5kaW5n 25300
IEVkaXRpb24= 25301
cGY= 25302
IHdhcnJpb3Jz 25303
IGhlZGdl 25304
IHVuc2VyZW4= 25305
INGB0Y7QtNCw 25306
ZWxpbmVzcw== 25307
IHBlcnNvbmFsaXRpZXM= 25308
IGbDtg== 25309
J00= 25310
INGC0L7Rh9C90L4= 25311
IHNoaXBwZWQ= 25312
IG1ldGVvcg== 25313
IHN1cnJvdW5kaW5ncw== 25314
IEZpbGw= 25315
dWVzdGE= 25316
IFBlcnNvbmFs 25317
IEFsbGU= 25318
T1JU 25319
5LmF 25320
IFNjaGU= 25321
Vkk= 25322
IGNvbXBhcmFibGU= 25323
ZGFtbg== 25324
IGRpdGNo 25325
WUFO 25326
aXNtdXM= 25327
IHBpY2t1cA== 25328
IGRhaw== 25329
IEVQ 25330
YmVzdA== 25331
IFN1ZQ== 25332
w6RsbHQ= 25333
IHBvcGNvcm4= 25334
IGZvbGRpbmc= 25335
aG9tZQ== 25336
0LjQstCw0LXRgg== 25337
5bey57aT 25338
IGFubm90 25339
Y2h1Y2s= 25340
IGZpZXJjZQ== 25341
IGRhbWFnaW5n 25342
IGZsb3A= 25343
IHBhc2Fy 25344
IHJlZWY= 25345
INGB0LLQvtC10Lk= 25346
IHpvbw== 25347
b3ZlcnM= 25348
amV0cw== 25349
IHByw6hz 25350
IFNpbGljb24= 25351
dGVvaw== 25352
IFNldGg= 25353
YXRhbWVudGU= 25354
IHRyYW5zbWl0dGVk 25355
IHJlcGxpY2F0ZQ== 25356
IHNsaW0= 25357
IENyZWFt 25358
5oSf44GY 25359
IHNpZGV3YWxr 25360
7IiY6w== 25361
INC20LjQt9C90Yw= 25362
IE1vbmljYQ== 25363
5L6G5LqG 25364
IGNvcGllZA== 25365
IFRlcnJh 25366
aXN0ZW50 25367
57O7 25368
INC+0L3Qvg== 25369
IHdoYWxl 25370
IFdJVEg= 25371
0LvRg9GI 25372
5b2x54mH 25373
IEVlbg== 25374
INGB0LLQvtC4 25375
IG9yZGlu 25376
IHBsdXJhbA== 25377
IHNwb2tlcw== 25378
IGRpc3B1dGU= 25379
IHNlbnNpYmxl 25380
IHByZWFjaGluZw== 25381
IGt0w7Nyenk= 25382
cHRlZA== 25383
YXZpZXI= 25384
IHBpc3RvbA== 25385
IFRhcGk= 25386
IMWC 25387
ZmZmZg== 25388
IGFjcnlsaWM= 25389
IGlnbm9yYW5jZQ== 25390
IFppZWw= 25391
cmFucw== 25392
IHdlbGRpbmc= 25393
bWlk 25394
5oiR5LiN 25395
INC30LDQvdC40Lw= 25396
IGxhbmVz 25397
IG1pbmVz 25398
IG1vbXM= 25399
15XXlw== 25400
IENoYW1iZXI= 25401
dGllcg== 25402
IG1vZGVzdA== 25403
IOyXrOq4sOyEnA== 25404
IHVuYXM= 25405
IHdyZW5jaA== 25406
aGFuZGVk 25407
IHNhdHVyYXRlZA== 25408
IEZhbmc= 25409
IENvbW1pc3Npb25lcg== 25410
4KSw 25411
INeW 25412
IExvdWlzaWFuYQ== 25413
IE1hc2s= 25414
IGN1YmVz 25415
7JSo 25416
IHZpZMOpb3M= 25417
IG7DpWdvbg== 25418
IHJpZGVy 25419
IOy2nA== 25420
IHPDs24= 25421
IExhdGlubw== 25422
YmFuaw== 25423
7ZW07KO8 25424
IEJyZW5k 25425
IHNleHVhbGl0eQ== 25426
Li4uLA== 25427
IGZvcmdldHRpbmc= 25428
INuM 25429
IEF2ZW5nZXJz 25430
IEJvbmpvdXI= 25431
Y2Vzc29y 25432
0LrRgNCw0Zc= 25433
Y2VuY2U= 25434
IGdlb2dyYXBo 25435
Y3Vsbw== 25436
0L7RgdGC0Yw= 25437
IHN3ZWF0aW5n 25438
7YOA 25439
IHN5bW1ldHJ5 25440
dHPDpQ== 25441
IGphbg== 25442
IEZlcnI= 25443
6aaW 25444
IGFtYmFzc2Fkb3I= 25445
emnEmWs= 25446
IG11c3Vu 25447
INGD0YI= 25448
IExH 25449
aXNzZW50 25450
Y29tbXVu 25451
IGNvdXJz 25452
IGRldmVsb3Bz 25453
IGJyb256ZQ== 25454
IHN1YnN0YW5jZXM= 25455
ZHJpdmVu 25456
7KO87IS47JqU 25457
IGFvcw== 25458
5YSE 25459
IFBST0ZFU1M= 25460
aGFsZg== 25461
IHNvcnRlZA== 25462
IEJvbWI= 25463
0LvQsNCz 25464
IE1hbGF5c2lh 25465
IENocmlzdGluYQ== 25466
IHRlYW1tYXRl 25467
6IGe 25468
RlQ= 25469
IGvEsQ== 25470
aGVhcnRlZA== 25471
Kys= 25472
b2dlbmlj 25473
IGJlbGxz 25474
IE91YWlz 25475
IHNwZWNpYWxpc3Rz 25476
0LHRiw== 25477
ZGVwdGg= 25478
bGFzc2Vz 25479
Z2llcw== 25480
IENvZmZlZQ== 25481
IG1hcmtpbmc= 25482
IGZvbGw= 25483
dWxp 25484
IGFkaGVzaXZl 25485
IEJvdA== 25486
IFB1bmt0 25487
ZXll 25488
IEJ1Yg== 25489
ZWxvbmc= 25490
5Yi2 25491
INC/0YDQuNC6 25492
IGRvbm9y 25493
ODQ= 25494
IGVuZm9y 25495
IGNhdGNoZXM= 25496
IGJyaWNrcw== 25497
IGtuaXR0aW5n 25498
IEtub3dpbmc= 25499
b2tz 25500
SFk= 25501
cmlkZQ== 25502
IEZhbnRhc3k= 25503
aW1hbg== 25504
IHBzZQ== 25505
IOyYqA== 25506
INCy0LQ= 25507
IHJlc3RyYQ== 25508
IGV2YWx1YXRlZA== 25509
0YDQtdCy 25510
IGZvcnR1bmF0ZWx5 25511
IGNoZWdhcg== 25512
2LHYqA== 25513
IGRvbWFpbnM= 25514
aWJp 25515
YXJyeQ== 25516
IHNodXR0ZXI= 25517
IGZpY291 25518
TWlrZQ== 25519
IGluY2x1 25520
IGRvbm9ycw== 25521
IGFwbA== 25522
IExvd2Vy 25523
IGltcG9ydGVk 25524
IGFjYWRlbXk= 25525
IGZpbmFscw== 25526
IGRpc2FwcGVhcnM= 25527
2YrYpw== 25528
IGFkbWluaXN0cmF0b3I= 25529
anM= 25530
IGN1dHRlcg== 25531
IHJhbmdpbmc= 25532
w7ZycGVy 25533
IGNvbnN0cmFpbnQ= 25534
IFRhYmxl 25535
IFNoYW4= 25536
dmlj 25537
IEZpeA== 25538
IFN3aWZ0 25539
b3VuY2Vz 25540
IFdhcnVt 25541
IGxldHR1Y2U= 25542
YXBwZWxsZQ== 25543
IHNoYXZl 25544
IGLDoXM= 25545
IDc3 25546
IE9vbw== 25547
YW8= 25548
IE1jTQ== 25549
IERyZXc= 25550
IGx1bXA= 25551
IGxhc2hlcw== 25552
c2NoZWlubGljaA== 25553
UmVw 25554
aW5pcw== 25555
IENldHRl 25556
IGNvbXBvc2l0ZQ== 25557
ZW1ldGVyeQ== 25558
IHNvcnRl 25559
IEZpbmFuY2lhbA== 25560
0L7QvdC1 25561
cm9uZXM= 25562
IFZveQ== 25563
IHTDqWM= 25564
oLk= 25565
IE5pbmph 25566
IENvcmlu 25567
0LXQvdC90Y8= 25568
7J207JeI 25569
IG5pY2g= 25570
IGRldGVjdGl2ZQ== 25571
4oCmIg== 25572
z4POtQ== 25573
nbzrj4Q= 25574
IOuzgA== 25575
IOu4lOs= 25576
IHByb3Bl 25577
IFdyaWdodA== 25578
INeU16o= 25579
IFNoaQ== 25580
IOOBnw== 25581
IGludmVzdGlnYXRpb25z 25582
6YKE5piv 25583
IFBvd2VyUG9pbnQ= 25584
IENodQ== 25585
IOyYpO0= 25586
IOyZhOyghA== 25587
IEZyYWdlbg== 25588
dW5uaW5n 25589
IHBvdXJyYWl0 25590
IHRleHRib29r 25591
0LzRiw== 25592
IGZhaHJlbg== 25593
INGC0L7RgA== 25594
IGxha2Vz 25595
w7xuZGU= 25596
SW50 25597
IE1ldHJv 25598
IG1hbnNpb24= 25599
INCw0LE= 25600
IFpob3U= 25601
IGNvcnJpZG9y 25602
IGVzY29s 25603
IGluZGljYXRpbmc= 25604
aWHFgmE= 25605
IG1vbW15 25606
IGFyY2hpdmVz 25607
IGZvdW5kZXJz 25608
ZW5naW5l 25609
IERpZXU= 25610
IHNpY2tuZXNz 25611
IOuztOuLiOq5jA== 25612
IGFyYg== 25613
IG5lZA== 25614
IENob3A= 25615
IGNvdmlk 25616
IHNsYW0= 25617
IHB1YmxpY2F0aW9ucw== 25618
REM= 25619
IHNwZW5kcw== 25620
5r4= 25621
IHJlZnVnZWU= 25622
IGRpbGU= 25623
INeQ15Y= 25624
aWZpY2Fy 25625
IFNhY2g= 25626
R3U= 25627
IHJlbG9hZA== 25628
Pz8/Pw== 25629
IGplxZtsaQ== 25630
INGB0L7RgdGC0L4= 25631
IHNpbXBsaWNpdHk= 25632
IGJ1bGx5aW5n 25633
INC80L7Quw== 25634
IHJlYWxpZGFk 25635
IHVuY2xlYXI= 25636
YXBwYQ== 25637
bGV2YW50 25638
IElTSVM= 25639
IFdhdHNvbg== 25640
IGRlaW4= 25641
IE1pY3Jv 25642
7ZWc6w== 25643
w7xn 25644
IGRldmFt 25645
IHR3ZWV0ZWQ= 25646
5bCO 25647
IHVuZGVyc3RhbmRhYmxl 25648
YXRhbg== 25649
IHZlcnNh 25650
IHByZWNh 25651
IHbhu4E= 25652
IENvcHk= 25653
IE9yYWNsZQ== 25654
IG1pbmRmdWxuZXNz 25655
IGRpc2NyZXQ= 25656
ZXJuZW4= 25657
IFBsZQ== 25658
SGF2ZQ== 25659
IGlzb2xhdGU= 25660
IGRldQ== 25661
IHNldmVudHk= 25662
IEhpbGxz 25663
IGFyY2FkZQ== 25664
INGB0L/QtdGG0Lg= 25665
IHNpZ3VpZW50ZQ== 25666
IELDnE5ETklT 25667
bGlnYQ== 25668
INCy0YHRgtGA0LXRhw== 25669
w7Rt 25670
IHR3ZWV0cw== 25671
IHNjaGF1ZW4= 25672
IGNyaXRpcXVl 25673
IPCfjrU= 25674
IHN0YXR0 25675
INGB0LDQvNC+0LU= 25676
w6JuY2lh 25677
IHN1cGVybmF0dXJhbA== 25678
IHBsdWdnZWQ= 25679
Rmw= 25680
eW7EsQ== 25681
IFRhbWJpw6lu 25682
IGVuY291cmFnZW1lbnQ= 25683
IFNlcnZlcg== 25684
64Kc 25685
dXBh 25686
IGFzdG9u 25687
IGhlYXJz 25688
0YDQsNGF 25689
IHNjaGU= 25690
IHJhdHM= 25691
IHJlY3VwZXI= 25692
IHVudGVu 25693
IEZpZ2h0aW5n 25694
IGFjYWRlbWljcw== 25695
56S6 25696
IFPDvA== 25697
0YHQutC40YU= 25698
IHBhaXJlZA== 25699
gOydhA== 25700
IMOhcmVh 25701
IHN3ZWV0bmVzcw== 25702
5Y+K 25703
IGRlZmVy 25704
IG11aXRhcw== 25705
IEF1ZGlv 25706
IGxvY2tlcg== 25707
2YrYrw== 25708
INGB0YLQsNCy 25709
IGJ1ZW5h 25710
QU5T 25711
IGRldGVjdG9y 25712
YXZv 25713
YmVr 25714
IM6xzr0= 25715
7Y64 25716
IGRyYWdnZWQ= 25717
INC00L7Qu9C20LXQvQ== 25718
w5Y= 25719
2LHYqQ== 25720
7J207KeA 25721
IGNlbGxl 25722
Y2tpbmc= 25723
INin2YTYrA== 25724
IENhbnZhcw== 25725
IGVzcGHDsQ== 25726
IGdsaW1w 25727
IHNwcmVhZHM= 25728
b25nbw== 25729
IE1hc29u 25730
IEluZw== 25731
IOqwgOuKpQ== 25732
z4TOuc66 25733
IHNlY3VsYXI= 25734
IGJhdGVy 25735
IGlucXVpcnk= 25736
IGVuZXJnaWVz 25737
IG1hbnVmYWN0dXJlZA== 25738
IHZlZ2V0YXJpYW4= 25739
IHBpbmVhcHBsZQ== 25740
0Y/RgtCw 25741
IHByYWN0aXRpb25lcnM= 25742
MjAwMA== 25743
IO2VtOyalA== 25744
IOyXrOufrOu2hOuTpA== 25745
IOu2iOs= 25746
IEplZmZlcnNvbg== 25747
IEpvYW4= 25748
IHRyYW0= 25749
5a65 25750
Y2htYWw= 25751
IEhhaXQ= 25752
4bmH 25753
IHVucmVhbA== 25754
IHN5bWJvbGlj 25755
IHN0ZWFsdGg= 25756
IHNwbGFzaA== 25757
IEVudGVydGFpbm1lbnQ= 25758
IG1ldGFsbGlj 25759
PyIu 25760
6LaK 25761
YXJvdW5k 25762
IGRlc3BhaXI= 25763
IE5ldmFkYQ== 25764
IEZpbmFuY2U= 25765
IGtyaWU= 25766
IEx1eA== 25767
IFNtYXNo 25768
a2VlcGluZw== 25769
INC30LDQsw== 25770
IG5hcmNpc3M= 25771
IGR6aXNpYWo= 25772
IHRvbGVyYXRl 25773
b2FyZA== 25774
IGxpbmtpbmc= 25775
IEVjb25vbWlj 25776
IOy8 25777
IG1vcnBo 25778
IE5haw== 25779
IEJha2Vy 25780
YXRvbg== 25781
cmluZ3M= 25782
IFBlbmc= 25783
IEFpcnBvcnQ= 25784
44GL44Gj44Gf 25785
7ZWY64uk 25786
p4E= 25787
cHJpbnRz 25788
IGhhZGk= 25789
IGVtcGly 25790
IExpdmVz 25791
YW5uZXJz 25792
INC90LjQvA== 25793
IFBST0ZFU1NPUg== 25794
IHBvc2l0aXZlbHk= 25795
YW50b20= 25796
IGJhZGdl 25797
a2VsdA== 25798
IGludGVyZmVy 25799
IGZ1bGZpbGxpbmc= 25800
IHZpc3VhbGl6YXRpb24= 25801
6Zec5L+C 25802
IFByaWNl 25803
77+977+9 25804
IHNjZW5lcnk= 25805
IHByb25l 25806
IHdpemFyZA== 25807
IGJhbnlhaw== 25808
dmVyYg== 25809
c2t5 25810
IHdpc2hlZA== 25811
IHJhaWx3YXk= 25812
IMO8emVy 25813
IGFsZ3VpZW4= 25814
IEFX 25815
INC60L7Qu9C40YfQtQ== 25816
IHJlYWN0aW5n 25817
IEJ1Y2g= 25818
4Li2 25819
IGFudGg= 25820
IHNpaA== 25821
IGh1c3Q= 25822
IFNjcmVlbg== 25823
aWxhbnQ= 25824
YWhv 25825
IGZyYWdyYW5jZQ== 25826
IGVsZXZhdGlvbg== 25827
IE1lZGl0ZXI= 25828
IOu/ 25829
IMOpcXU= 25830
IHdyYXBz 25831
IGluZXJ0 25832
IHJlY3JlYXRl 25833
0LvQsNGC 25834
IGJvbGVo 25835
IGhhcmFzc21lbnQ= 25836
dW5reQ== 25837
IGdsaW1wc2U= 25838
cmVnaWVydW5n 25839
IGZ1dHVy 25840
IHJlcG9zaXRvcnk= 25841
IGVuZ3Jh 25842
IHRyYWZmaWNraW5n 25843
YXNzaXM= 25844
IFRyZWs= 25845
IOuyjA== 25846
IOuniOs= 25847
IEthYg== 25848
YW5pdQ== 25849
Z2l2ZQ== 25850
IGRpbm9zYXVycw== 25851
IGZlYXRoZXI= 25852
IGF0dGl0dWRlcw== 25853
IHBsdW0= 25854
IFJT 25855
IEFuZmFuZw== 25856
aWxsZXJ5 25857
IOyKpA== 25858
TVk= 25859
IHRyemViYQ== 25860
IHNraWVz 25861
IEFq 25862
dXJhYmxl 25863
Q1U= 25864
IFNoYW5l 25865
IGRlcGFydHVyZQ== 25866
IFRPTg== 25867
aWV0ZW4= 25868
cmF0cw== 25869
5rCX 25870
aXN1 25871
IGJvcmQ= 25872
IGludGVyZXN0aW5nbHk= 25873
55m7 25874
b3VnaGluZw== 25875
IHJ1c2hpbmc= 25876
IHZvbGF0aWxpdHk= 25877
IHB5dA== 25878
IGZvcm1hdHM= 25879
INC30LDRgg== 25880
IOq8rQ== 25881
IHdoYXRub3Q= 25882
IGNvbXBvcnQ= 25883
c3c= 25884
b3JlYW4= 25885
IFJlbGF4 25886
IGNsYW4= 25887
IEFI 25888
IHBldw== 25889
IGRpY3Rpb25hcnk= 25890
VGFrZQ== 25891
c2hpcnRz 25892
IEh1Z2g= 25893
INi52YTZig== 25894
IFBpYw== 25895
IGVucm9sbGVk 25896
IGplZG5haw== 25897
IG9mZmVyaW5ncw== 25898
IGNvcmF6 25899
TGlmZQ== 25900
ICEhIQ== 25901
IGNsZXI= 25902
IFZpZGVvcw== 25903
IFJvZHJpZw== 25904
IElkZW50 25905
IFBvcw== 25906
IFN0YWdl 25907
IFJhY2U= 25908
IGVuYWN0 25909
44GE44G+44GX44Gf 25910
IEd5 25911
IEhpc3Bhbg== 25912
IGRlZmVuY2U= 25913
IENhbXBiZWxs 25914
bWF0aWM= 25915
IHJlbGV2 25916
IHBlYWNo 25917
hLjsmpQ= 25918
IHBhcmFkaXNl 25919
IGNlcmVtb24= 25920
IGFubm95ZWQ= 25921
5oyH 25922
bGF4 25923
IGV4cGxvaXQ= 25924
IGNsYXVzZQ== 25925
ZWtlcg== 25926
IEJsb29t 25927
bmFudA== 25928
YXRldXJz 25929
IGhlaWdodHM= 25930
RXZlbg== 25931
0YHQvtC9 25932
IG91dHJhZ2U= 25933
IFZpZXRuYW1lc2U= 25934
44Gv44Gv 25935
VFI= 25936
IGVlcg== 25937
IGNhbm5vbg== 25938
IENvbWI= 25939
kOunjA== 25940
6LuK 25941
IOqyg+uPhA== 25942
IGFjY29tcGxpc2htZW50cw== 25943
IEFuYWx5dGljcw== 25944
IHNoYXBpbmc= 25945
cmVpYmVu 25946
IGJhY2hlbG9y 25947
IGZpbmdlcnQ= 25948
YWNrZWQ= 25949
IHB5cmFtaWQ= 25950
IFN0ZXdhcnQ= 25951
w6FzdA== 25952
IHN1cnZpdm9y 25953
IGR1Y3Q= 25954
IGRlYWxlcnM= 25955
5rS7 25956
2LnZhQ== 25957
0LvQuNC9 25958
IGVkZQ== 25959
15XXog== 25960
INmD2KfZhg== 25961
IM+Ezrk= 25962
IGNob29zZXM= 25963
IE93bg== 25964
0LPQvtGC0L7Qsg== 25965
aGlyZQ== 25966
0LDQu9GM0L3Ri9C1 25967
INCb0Y4= 25968
INC+0YHRgtCw0LI= 25969
dGVjaA== 25970
IGRyb2l0 25971
IHN1YmplY3RpdmU= 25972
ZW5lcw== 25973
IGRpdmlz 25974
YXZleg== 25975
IG1hbmV1dmVy 25976
4LmE4LiU 25977
YWRlY2U= 25978
IEVucw== 25979
YWNpYWw= 25980
IFByb3RlY3Rpb24= 25981
lrQ= 25982
IGZvcm1hbGx5 25983
IHd5ZA== 25984
aW5ndcOpbQ== 25985
IHppZW0= 25986
IHJlY3J1aXRpbmc= 25987
15nXmg== 25988
bmVt 25989
IGZvcmJpZGRlbg== 25990
IEJhcHQ= 25991
15DXoNeZ 25992
IHN1YnNldA== 25993
IE1hZ2F6 25994
bmVtZW50 25995
IGFxdWVsYQ== 25996
cmFnb24= 25997
IGNvbW1pdHRlZXM= 25998
IMOpdGFpZW50 25999
dWRp 26000
IERhd24= 26001
IGJvcmU= 26002
IGNvbXBvc2Vy 26003
IHdpxJljZWo= 26004
YW5nYQ== 26005
IGRpc2xpa2U= 26006
IERheXM= 26007
5Z+6 26008
IHBhcmFs 26009
IG1pZW50cmFz 26010
IGhlYXZlbnM= 26011
44GS 26012
aGVpZA== 26013
IHRyYWRlcnM= 26014
b25jZQ== 26015
IG1hc2NhcmE= 26016
IM+Az4HOvw== 26017
IHdoaXNwZXI= 26018
IE11c2s= 26019
6ZuG 26020
IEZhbWlsaWU= 26021
QWxsYWg= 26022
IE9saXZpYQ== 26023
IFByb3M= 26024
IG9saWth 26025
aWxpbQ== 26026
IHLDqXBvbmQ= 26027
IFBldGVycw== 26028
IOW+iA== 26029
IGJpdGVz 26030
IHZpYw== 26031
IE5Z 26032
ZW1wdGlvbg== 26033
IDQ1MA== 26034
IHZpc3VhbHM= 26035
IGxpZXU= 26036
w7xja2Vu 26037
IFN0ZWVs 26038
IEdQ 26039
d2FpdA== 26040
IG5vdGljZWFibGU= 26041
dWNoYQ== 26042
IHJlaGFiaWw= 26043
IHJlamVjdGlvbg== 26044
INGB0LvQtdC00YPRjtGJ 26045
IHNsaWRlcg== 26046
IHJlZ2FyZGVk 26047
IGdyYXZpdA== 26048
IFJlc2VydmU= 26049
Y291bnQ= 26050
IGJyZWVkaW5n 26051
IGxvbmdl 26052
YWxlYg== 26053
IGtuaWdodA== 26054
INCy0L7QuQ== 26055
IHByw6lzZW50 26056
gpjsmpQ= 26057
IFNwZWNpZmljYWxseQ== 26058
IHBvc2Vz 26059
IHZldXJl 26060
b2theQ== 26061
ZW1hcw== 26062
IOOBp+OBmQ== 26063
IG1hasSF 26064
IHdlYmluYXJz 26065
IGNhbm5hYmlz 26066
IGRhbWFscw== 26067
IE5vcnRod2VzdA== 26068
IHBhZGE= 26069
IGNyb3dkcw== 26070
IGZ1dHVyZXM= 26071
IMOkbg== 26072
IGNpdmlsaWFucw== 26073
IFNhY2hlbg== 26074
5o0= 26075
IHRyYWNlcw== 26076
IOuoueqzoA== 26077
UVU= 26078
6aGY44GE 26079
IElG 26080
YW7EsW4= 26081
7IK0 26082
IGJpYmxpY2Fs 26083
IFZlZA== 26084
IHN0b3Jpbmc= 26085
0YDQsNCy0LvRjw== 26086
5oeJ6Kmy 26087
IG5hc3Q= 26088
IGTDtg== 26089
0YDQvtC/ 26090
ZWxpYQ== 26091
IHNpZGV3YXlz 26092
IFVuZGVyc3RhbmQ= 26093
IFF1cg== 26094
IHBlcnBlbmQ= 26095
IE1pbGxpb25lbg== 26096
IHdhdGVybWVsb24= 26097
IERpdmluZQ== 26098
dWx0dXI= 26099
YWJvcmQ= 26100
IHN1Y2Nlc3Nlcw== 26101
IGhvbWJyZQ== 26102
IGNhcnA= 26103
IHN1c2NlcHQ= 26104
dW5na2lu 26105
IGtpag== 26106
dWx1cw== 26107
2KfYrA== 26108
IG5vdGNo 26109
IHBvbHlub21pYWw= 26110
5bmy 26111
5ak= 26112
IMO6bmljbw== 26113
IHRlbGVzY29wZQ== 26114
IHBvbGl0aXF1ZQ== 26115
a2llbQ== 26116
IM6tzr3OsQ== 26117
IGFnZ3JlZ2F0ZQ== 26118
IEdlb2Zm 26119
IHRyaWw= 26120
IEdSQQ== 26121
IHN1YnNjcmliZXI= 26122
aW1ldA== 26123
INC00L7Qu9C70LDRgA== 26124
b3Bpbmc= 26125
IHRoZXJhcGV1dA== 26126
IENhbmNlcg== 26127
IHBhcmFkZQ== 26128
IGlycmln 26129
4pmq4pmq 26130
IGNsZWFyZXI= 26131
IGJvZw== 26132
IE1hdXI= 26133
4Liy4LiH 26134
IFNoYW5naGFp 26135
YWNodGU= 26136
IEtvbA== 26137
ZWx1amFo 26138
IGhhdg== 26139
IENyaW1l 26140
c2Vr 26141
IOuhnA== 26142
aWVubmE= 26143
IEdvcg== 26144
6Js= 26145
INC/0L7RgtGA 26146
INC60LDQttC10YLRgdGP 26147
IExpZnQ= 26148
IFNvcnQ= 26149
IFBzYWw= 26150
IHBpbmc= 26151
k50= 26152
cGhpcw== 26153
IEZVQ0s= 26154
IFN5bg== 26155
IGJhbWJvbw== 26156
rOyYgQ== 26157
Y3V0cw== 26158
IG1tbQ== 26159
IGZ1bmt0aW9uaWVydA== 26160
IF8= 26161
w61jaW8= 26162
U3RvcA== 26163
IGltYWdpbmFyeQ== 26164
IG5vdGFtbWVudA== 26165
IEluaXRpYXRpdmU= 26166
44Ol 26167
IEt1cnQ= 26168
IGxvb3Nlbg== 26169
IGJ1c2Nhcg== 26170
54Gr 26171
IHplbGY= 26172
IHByb3Bz 26173
5ZuJ 26174
IG1vZXRlbg== 26175
IG1pbGxp 26176
IGhhbGxz 26177
IE1hdGNo 26178
IGJyYWNrZXRz 26179
IENvdQ== 26180
5qaC 26181
INCc0LDRgA== 26182
SVNB 26183
IGNpZ2FyZXR0ZQ== 26184
IGNvbXBldGl0aW9ucw== 26185
IE1JTg== 26186
IGJlaMO2 26187
dm9vcg== 26188
IHVzdA== 26189
IFpp 26190
IE9jYw== 26191
dWxhdGVz 26192
IGJhbGxvb25z 26193
IHByb250bw== 26194
IE1peQ== 26195
IEZpbGU= 26196
INC60LvQsNGB0YE= 26197
0L3Rg9C7 26198
IGNlcmVhbA== 26199
IGluY3JlbWVudA== 26200
IHJlZmluZWQ= 26201
5Y+m5aSW 26202
cHJpc2luZw== 26203
IFJG 26204
IHJlc3BlY3RmdWw= 26205
IGxvb3Q= 26206
YXNrZXQ= 26207
IGRlaXhh 26208
aW5nbGU= 26209
IGZ1bmNpb25h 26210
IFJldmVs 26211
IHNvYmVy 26212
IHBlcmZvcm1z 26213
IEdlbnRsZQ== 26214
44Ko 26215
IHJlY2lwaWVudA== 26216
IEhhdXNl 26217
IOuD 26218
RnJvbQ== 26219
IG1pbmlzdGVycw== 26220
IHBhcmFkb3g= 26221
5bCx5piv6Kqq 26222
IHRhc3Rpbmc= 26223
INeU15c= 26224
IHJldXNl 26225
IExhbmU= 26226
INGB0L7QstC10YDRiA== 26227
IHJlbWVtYmVycw== 26228
IGZlbWluaXN0 26229
IGNvbW1pdG1lbnRz 26230
IHByb2plY3RlZA== 26231
IGdheg== 26232
aXlvcnV6 26233
IG9ibGlnYXRpb25z 26234
Um8= 26235
emFy 26236
IGNodw== 26237
IEpBTQ== 26238
IGLEmWTEhQ== 26239
YXNwYmVycnk= 26240
INC80LXRgdGC0L4= 26241
67KV 26242
IHJlZ3VsYXRlZA== 26243
IHdpY2h0 26244
IFRyZXZvcg== 26245
IHNlY29uZGx5 26246
IElocmU= 26247
ZWxzaA== 26248
IHJlcG9ydGVycw== 26249
0YLQvtGA0LA= 26250
b3lv 26251
R0k= 26252
IGludGVyY29ubmVjdA== 26253
6ZCY 26254
T1NI 26255
5q2y 26256
IGJyYXNz 26257
IGlnbm9yaW5n 26258
5LuK5pel 26259
aW5mZWN0 26260
IHByb2pla3Q= 26261
b3JldA== 26262
z4TOsc69 26263
INGC0LjQvw== 26264
IG11dHRh 26265
IHVuYm94aW5n 26266
hLA= 26267
5aGK 26268
IGFkdmlzZWQ= 26269
IERlbnZlcg== 26270
IHNldmVyZWx5 26271
IE1obQ== 26272
IGZsaXBwZWQ= 26273
IHBpZW4= 26274
IGtvbW11bg== 26275
IEZSRQ== 26276
IOCuh+CusA== 26277
YWludGVk 26278
IGtuaXZlcw== 26279
IGhhYmw= 26280
IGdld29yZGVu 26281
YXJldHRlcw== 26282
Q1M= 26283
INC80LDQu9C10L3RjA== 26284
IGdhbGF4 26285
IG5pbmV0ZQ== 26286
6rGw64KY 26287
IHNpcw== 26288
IGFkdmlzb3J5 26289
IGRyaWxsaW5n 26290
IFdvdWxkbg== 26291
w7xuZg== 26292
Z2VzdGVsbHQ= 26293
IEhlbGVu 26294
INee15A= 26295
YXBvbGlz 26296
IHJ6ZWN6eQ== 26297
IHRlcnJh 26298
IGhlcA== 26299
IGFsZ8O6bg== 26300
aWtr 26301
IGFzdHJvbm9t 26302
IFN0YXJidWNrcw== 26303
a8SF 26304
IHBhdHJvbA== 26305
IOy9lA== 26306
IGdvbg== 26307
IOOAkA== 26308
IHNvbnN0 26309
IGVuY291bnRlcnM= 26310
IHJldHJvdQ== 26311
IHNoYXJrcw== 26312
IGRvcg== 26313
IFJldmVy 26314
IGV2YXBvcg== 26315
IHJlc2Vydm9pcg== 26316
IGFsbGVnZWQ= 26317
dWxlcg== 26318
IHZlcm0= 26319
IGNvbW1lcmNl 26320
IGZpdHRlZA== 26321
Z2Vt 26322
IHRhY3RpY2Fs 26323
IGxpdGg= 26324
6YmE5aGU 26325
aGFk 26326
6K6K 26327
IGNhcmJvaHlk 26328
IGxlbmd0aHM= 26329
zrnOvw== 26330
IGRlbW9ncmFwaGlj 26331
Um9i 26332
IFNraW4= 26333
Y2NvbGk= 26334
IHNpbXBsaWZpZWQ= 26335
IHJlYWRpbHk= 26336
IEN1bQ== 26337
YWRlc2g= 26338
IETDpQ== 26339
dXNzdA== 26340
aWduZQ== 26341
ZXRvbg== 26342
IG1lbm9y 26343
cWk= 26344
T09N 26345
4Lit4LiZ 26346
IHBzeWNoaWF0 26347
IGVpZ2h0eQ== 26348
INC80LjQu9C70Lg= 26349
IFRvYg== 26350
ZWRv 26351
57ay 26352
IMSR4bq/bg== 26353
IGNpcmN1aXRz 26354
IExBVUdI 26355
aWNpc20= 26356
ZW1vcg== 26357
IHJlZ2VuZXI= 26358
ZWdyZWU= 26359
IGJ1cmVhdWM= 26360
IEFsYmVy 26361
5LmL5b6M 26362
IFdvcg== 26363
5aSr 26364
IHJlc2lu 26365
IGJ5xYJ5 26366
IElH 26367
4K+NLA== 26368
IDc4 26369
IHdlZWRz 26370
IE15dGg= 26371
OTM= 26372
5r8= 26373
IOuCmOyZlA== 26374
w6l2 26375
4b0= 26376
w7ZyZW4= 26377
w6dhcg== 26378
IFBBVUw= 26379
IGRpc2FkdmFudA== 26380
IHBvc2l0aW9uaW5n 26381
IGNvY2t0YWls 26382
IGFncmVlcw== 26383
bm4= 26384
IFNhbGx5 26385
TXM= 26386
IGluaGVyZW50 26387
IG1vbmV0YXJ5 26388
IG5hdHVy 26389
IE5o 26390
IEltcG9ydA== 26391
IGxlYmVu 26392
IHdp 26393
dXNzeQ== 26394
IG9iZXM= 26395
IHdhbmRlcmluZw== 26396
IOyLoOs= 26397
xIVkYQ== 26398
ZXRjaHVw 26399
IGRpc3Bvc2Fs 26400
IEpB 26401
IENlcg== 26402
emlsbGE= 26403
IHZpcmdpbg== 26404
IFNsaWRl 26405
YW5kZWw= 26406
IHJpZ2h0ZW91c25lc3M= 26407
IM6j 26408
IGlkZWlh 26409
5L2g5aW9 26410
0LjRgNC+0LLQsNGC0Yw= 26411
16jXkA== 26412
Q29tbWVudA== 26413
IHByZWxpbQ== 26414
IFZhbGU= 26415
IOyngOuCnA== 26416
IFZhbmM= 26417
T01BTg== 26418
INC/0ZbQtA== 26419
IHl1bQ== 26420
c3RyZQ== 26421
Y2Vt 26422
IHBvY3o= 26423
IGZyYWdtZW50 26424
INGB0LvRg9GH0LDQtQ== 26425
IHVuZGVyZ28= 26426
IEhhbms= 26427
Y2Vrcw== 26428
IEZQUw== 26429
IG9jdXI= 26430
IGRldGVyaW9y 26431
5rOo 26432
IGVtcHJlc2Fz 26433
UGF1bA== 26434
ICkpKQ== 26435
INCy0YDQtdC80LXQvdC4 26436
IHNjb2xk 26437
15nXog== 26438
IHN1c3BlY3RlZA== 26439
IGFjY2Vzc2luZw== 26440
IHN1YnN0aXQ= 26441
IGhpc3RvcmlhbnM= 26442
5Lu7 26443
INC00LXQu9C+ 26444
IHNvY2llZA== 26445
cm9uZQ== 26446
IHJlZGVu 26447
IGV4dGVuZHM= 26448
ZXBoZXJk 26449
IGJhbGNvbg== 26450
5LiN6LW3 26451
IFNvbG8= 26452
IHBvbGl0aWNpYW4= 26453
0L7Qu9GM0L3Qvg== 26454
IGlyZ2VuZHc= 26455
IHRyYXVtYXRpYw== 26456
IHJhcHBlcg== 26457
IFJPQkVSVA== 26458
UmVhbGx5 26459
5oGv 26460
IGxpbmV1cA== 26461
QVNF 26462
IGNvbnRyYWN0b3I= 26463
IENvcnBvcmF0aW9u 26464
Z29y 26465
IFRvZG8= 26466
0YHRgtGA0L7QuQ== 26467
RkJF 26468
IG5ld3NsZXR0ZXI= 26469
IGtvxYQ= 26470
YWx0aWVz 26471
INC/0YDQuNGH 26472
IEhlYXZ5 26473
IHN3b3Jkcw== 26474
IG1hbmlwdWxhdGlvbg== 26475
IGZ1bms= 26476
IHbDpXI= 26477
IFRhbGliYW4= 26478
IOuwpQ== 26479
IGFjbmU= 26480
w7xyw7w= 26481
IGRlc3dlZ2Vu 26482
IER1c3Q= 26483
IHNpbGlj 26484
IGhvb2tz 26485
IGJsaWo= 26486
IHBldGl0cw== 26487
IGZpbG1l 26488
IEJlcmVpY2g= 26489
IFNhaWQ= 26490
IGltcG9zZWQ= 26491
IGRpYXJ5 26492
INCz0L7RgA== 26493
IEdhdGVz 26494
IGFsdGE= 26495
5biM 26496
IGNoY2lh 26497
cGxlYXNhbnQ= 26498
IOuwnQ== 26499
IG1vxbxlbXk= 26500
IEF1c3RyaWE= 26501
IGJyb2tlcg== 26502
IHN1Y2tlZA== 26503
6ICD 26504
IGNvbXBhcnRtZW50 26505
IGNsb25l 26506
INeU16I= 26507
IERhbmtl 26508
IG5vY2htYWw= 26509
0LXQt9C0 26510
IGFkcmVuYWw= 26511
IGtsZWluZW4= 26512
44G+44GX44KH44GG 26513
IHN1YnNlcXVlbnRseQ== 26514
IGRlY2VudHJhbA== 26515
IGdlbmV0aWNz 26516
IOq0kQ== 26517
IG1vbml0b3Jz 26518
IEFwcGxpYw== 26519
IFJlcG9ydGVy 26520
d2VydA== 26521
IHdpZW0= 26522
IE1vdmVtZW50 26523
IGludGVydmlld2luZw== 26524
IGhhaXJz 26525
IHB1w7I= 26526
IENoZWxzZWE= 26527
IGNvaGVy 26528
IGNvdA== 26529
IHphcw== 26530
IHBhdGNoZXM= 26531
IGxhaA== 26532
0YPQvdC6 26533
IFJlYWdhbg== 26534
IE1hcmNv 26535
Y2l0eQ== 26536
IGRlZmVuZGVy 26537
IGRlY29yYXRpb24= 26538
aWpp 26539
IGxpdHRlcg== 26540
0Kg= 26541
IGplZ28= 26542
UkVX 26543
IFBpaw== 26544
IEhlZQ== 26545
IEl2 26546
INC40LTQtQ== 26547
IFRoZWF0ZXI= 26548
INGH0LDRgdGC0L4= 26549
IHN3ZWF0ZXI= 26550
IGhpZ2hsaWdodGluZw== 26551
IGFpbnNp 26552
IGRpcGxvbWF0aWM= 26553
IE5ldmVydGhlbGVzcw== 26554
5bM= 26555
QVNPTg== 26556
IHDDumJsaWNv 26557
IGZlcm0= 26558
cmVhdGVk 26559
Y29k 26560
IOusvOs= 26561
IG1pc3Rlcg== 26562
IFZhbmNvdXZlcg== 26563
IHJlY29nbml6ZXM= 26564
ZWNk 26565
IGNvbXBsaWNhdGlvbnM= 26566
ZW5jaWFs 26567
44GX44GP 26568
IOqwgOyngA== 26569
IFVsdGltYXRl 26570
IHZhaWc= 26571
IE1lcnJ5 26572
15XXkg== 26573
IE1hcmN1cw== 26574
57i9 26575
b3dlZ28= 26576
IG1lbnRl 26577
U20= 26578
IGFqYQ== 26579
IFRhbw== 26580
IGp1ZGljaWFs 26581
IGVudHJlcHJlbmV1cnNoaXA= 26582
INC90LXQvNC90L7Qs9C+ 26583
IHBpcw== 26584
IGVyZw== 26585
IGNocmlzdA== 26586
IEN1cnQ= 26587
INGA0LDRgdC/ 26588
zrvOtQ== 26589
ZW5zY2g= 26590
w61yZQ== 26591
IGZvY2Fs 26592
IERpYW1vbmQ= 26593
YXbDrWE= 26594
IGhhbm5v 26595
IFNxdWFk 26596
IGFzc29jaWF0aW9ucw== 26597
IENyZWF0aXZl 26598
IG1lc3Nlbmdlcg== 26599
IGJlZ2dpbmc= 26600
IGRlY2ltYWw= 26601
IGTEscWf 26602
IG1ldGFkYXRh 26603
c2Vscw== 26604
IMSwxZ8= 26605
4buvYQ== 26606
IGRpZmZpY2lsZQ== 26607
ZMSx 26608
IHNsYXVnaHRlcg== 26609
IFZlcmc= 26610
INeS150= 26611
57Ch 26612
5oyJ 26613
IFRlYQ== 26614
YXNzZXM= 26615
T2s= 26616
IHN5bnRoZXM= 26617
b3RpYXRpb24= 26618
IHBhaW50ZXI= 26619
IGVsYm93cw== 26620
IGFyY2hpdGVjdHVyYWw= 26621
INGA0LDQtA== 26622
IGdsb3I= 26623
aW1hZ2U= 26624
YW1wYQ== 26625
Y3VsaWFy 26626
oKg= 26627
IHRldmU= 26628
IFN0ZWxsZQ== 26629
IEJhbQ== 26630
IOy0iA== 26631
YXNpcw== 26632
aXBlZGlh 26633
IEdJ 26634
IEFjdGl2ZQ== 26635
54S25ZCO 26636
YXpp 26637
44KM44Gm 26638
IEx1Y2t5 26639
7ZWp 26640
INC/0YDQuNGF0L7QtA== 26641
IHJ1bndheQ== 26642
IGF1dGhlbnRpY2F0aW9u 26643
IHBvc2libGU= 26644
IHN1cHBsZW1lbnRz 26645
IHN1cmdpY2Fs 26646
R2Vu 26647
IGZlYXNpYmxl 26648
RE8= 26649
IG91dGxvb2s= 26650
IGludGVydmFscw== 26651
IGFuZWNk 26652
w6BuZw== 26653
IHN0cmFwcw== 26654
IFNodQ== 26655
dWRk 26656
aXNzZW5zY2hhZnQ= 26657
IHBvcnRl 26658
IGNvbW1pdHRpbmc= 26659
IGFsbGV5 26660
IGNvdmVuYW50 26661
IFBlZHJv 26662
bGVzc25lc3M= 26663
IFNvbGlk 26664
IE1vbGx5 26665
INC90LXQutC+0YLQvtGA 26666
IGNvb3BlcmF0ZQ== 26667
5YyX 26668
b2xsZW4= 26669
IHR1bmE= 26670
IGtpbmRlcmdhcnRlbg== 26671
IFNpeg== 26672
IGR1xbxv 26673
IE1CQQ== 26674
IEdFT1JHRQ== 26675
IEZpc2hlcg== 26676
5b+Y 26677
IENhZXNhcg== 26678
INC60YDQsNGB0LjQsg== 26679
IERlbGhp 26680
enlt 26681
IGV4cGxpY2Fy 26682
6rCA7KeA 26683
dW5z 26684
Z3Jvdw== 26685
INC/0YDQuNGB 26686
IDg2 26687
IHN0YXRpbmc= 26688
IG1hc3Nh 26689
Y2h0ZXI= 26690
IOy7rOufrA== 26691
IGRlcHV0eQ== 26692
U00= 26693
bm9j 26694
IGdlb2dyYXBoeQ== 26695
IEVudGVycHJpc2U= 26696
IENhbnQ= 26697
w7Z6 26698
IHVucGFjaw== 26699
IO2ZlOs= 26700
IHNlYXJjaGVz 26701
IHByZXNpZGVuY3k= 26702
IHRyaXZpYWw= 26703
IHBpZ2U= 26704
b3VidA== 26705
44Ka 26706
7LyA7J20 26707
IGJ1ZGdldHM= 26708
IHVi 26709
IHBuZQ== 26710
IFlhbGU= 26711
IMWfw7Z5bGU= 26712
cmVndWxhcg== 26713
IGltcGVyZmVjdA== 26714
QVJB 26715
IGZhbcOtbGlh 26716
dXJt 26717
IEFkdmVudHVyZQ== 26718
44OK 26719
Y2lz 26720
ZW1hcms= 26721
IG5lZ28= 26722
IGluYXBwcm9wcmlhdGU= 26723
INC/0YDQuNC3 26724
INGA0L7Quw== 26725
IGRyZWFtZWQ= 26726
QnJ5 26727
IHNodXR0bGU= 26728
IHBpbGxhcnM= 26729
IGJpaw== 26730
aW51bQ== 26731
INGD0YE= 26732
IE5lYnI= 26733
IHBlcnBlbmRpY3VsYXI= 26734
IGJvb2tlZA== 26735
YmVyeQ== 26736
IHZpa3Q= 26737
YmVhcg== 26738
ZXN1cw== 26739
INCy0L7Qt9C80L7QttC90L4= 26740
qLk= 26741
IHByZXN1bWFibHk= 26742
IE1lbXBoaXM= 26743
IGFtYnVsYW5jZQ== 26744
15XXnteo 26745
IHRodW1ibmFpbA== 26746
IG1vZGlmaWNhdGlvbg== 26747
6YeP 26748
IGludGVycHJldGVk 26749
IHByb21v 26750
IM66zqw= 26751
IM61z4A= 26752
IGFjb3VzdGlj 26753
IERC 26754
5ZOO 26755
IG5vbmV0aGVsZXNz 26756
b3VsZQ== 26757
IHBlcXU= 26758
IGtub2I= 26759
44Kj 26760
IOuPjOyVhA== 26761
IHB1cmNoYXNlcw== 26762
IMOHw7xua8O8 26763
IGRpdmlkaW5n 26764
cGVyZm9ybQ== 26765
cmFjdGlvbg== 26766
aGVhbHRoeQ== 26767
IFRpdGxl 26768
IHVr 26769
IGNlcmNh 26770
IGFyZ3VhYmx5 26771
IGZhbGU= 26772
67O1 26773
IGdhbWVycw== 26774
IHV0aWxpemluZw== 26775
IG9mZmVuZGVk 26776
IHRhdmE= 26777
YWzEsQ== 26778
IG1lZGlhbg== 26779
IGluZmVjdGlvdXM= 26780
IEFubmll 26781
IHNtYXJ0cGhvbmVz 26782
IHBhcm9sZQ== 26783
5Zad 26784
IEVwaWM= 26785
enph 26786
IHVuaWZpZWQ= 26787
IOq3uOuVjA== 26788
IGN1cnRhaW4= 26789
IMSD 26790
IHNleHVhbGx5 26791
IHVuc2VyZW0= 26792
IENvbnZlbnRpb24= 26793
IGFsbGVnZWRseQ== 26794
WWE= 26795
IEhvbw== 26796
ZW5tZW50 26797
5oCq 26798
7ZuE 26799
IGdpZ2FudGlj 26800
IG5vdGluZw== 26801
IHJlYm8= 26802
IEphbWE= 26803
IEFseg== 26804
IGJvcnJvd2Vk 26805
7Lmo 26806
IHBlcmlwaGVy 26807
0L7RgtCw 26808
IEdC 26809
IEdlYXI= 26810
IGVjb25vbWljYWxseQ== 26811
IHRlbGVmb24= 26812
IHF1ZXJlbW9z 26813
INC00LDQu9GM0YjQtQ== 26814
IHJhcw== 26815
IFRlYWNo 26816
aWNpb3M= 26817
YXRvcw== 26818
IHBsZWRnZQ== 26819
YmF1 26820
IEhpbXNlbGY= 26821
TGluaw== 26822
IGVzcGVybw== 26823
IGNocm9tb3M= 26824
IFBFUg== 26825
IGVybGU= 26826
IHBvZGl1bQ== 26827
w6dvcw== 26828
IG5pZXU= 26829
IGZlbg== 26830
IEdPRA== 26831
IENob2NvbGF0ZQ== 26832
d2Vyaw== 26833
IHThu6s= 26834
IHN1cHByZXNz 26835
zrvOtw== 26836
IDI0MA== 26837
IHNpdMOk 26838
IGhvbmVzdHk= 26839
IEJpbw== 26840
IEJhcmQ= 26841
INC+0LHRidC10Lw= 26842
INC80YPQtw== 26843
IG1hcmJsZQ== 26844
INGG0LXQvdGC 26845
IHByb2N1cmU= 26846
IHJvdG9y 26847
YmVybg== 26848
IHR1aA== 26849
IGhlYWRzZXQ= 26850
YXRlbQ== 26851
IHdhcnJhbnR5 26852
4K60 26853
IGZpbGluZw== 26854
zrnOrA== 26855
IGNvbXByZW5kcmU= 26856
IGltcHVsc2U= 26857
IHNhbHY= 26858
d3JpdHRlbg== 26859
IGluc3RpdHV0ZQ== 26860
S2lt 26861
IExHQlRR 26862
ZmljaWVudGU= 26863
SGlz 26864
IM6xz4XPhM+M 26865
IHRlZW5hZ2U= 26866
b3J1cw== 26867
INGA0LDQt9Cx 26868
U2Vl 26869
IENvbnNlcnY= 26870
4buBbg== 26871
ZnVsbmVzcw== 26872
IHN0cmF3YmVycmllcw== 26873
IEFidQ== 26874
0LjQvtC9 26875
IG9sbGE= 26876
Tk9JU0U= 26877
IEVtcGxveQ== 26878
IHdpcGVk 26879
dXJnZXI= 26880
IG1vZGlmaWNhdGlvbnM= 26881
IO2VmOyngA== 26882
IGZvb3RzdGVwcw== 26883
IGhvbm9ycw== 26884
IGFkdWw= 26885
IGZsaXBwaW5n 26886
IEhV 26887
Wlk= 26888
IGludGVncmF0aW5n 26889
2KjYsQ== 26890
dWxsYQ== 26891
IG5hdHV1cmxpams= 26892
IO2XiA== 26893
IEV0aGVyZXVt 26894
2YrZhA== 26895
d2Vk 26896
IHBlYWtz 26897
IEtlcw== 26898
IGJsb29t 26899
IGNyYXNoaW5n 26900
IDkxMQ== 26901
INC+0YLQu9C40Yc= 26902
IGNvbnRyb2xsZXJz 26903
IERvZA== 26904
INCy0LzQtdGB0YLQtQ== 26905
IHNvcnRpcg== 26906
5aWH 26907
IFN0cmFpZ2h0 26908
IEdyYWNpYXM= 26909
IGdyb292ZQ== 26910
IHRvZ2c= 26911
IOyLtuydgA== 26912
w6lybw== 26913
IG91dHdhcmQ= 26914
IFdB 26915
IFJvY2t5 26916
IHNjYW0= 26917
IGhheWF0 26918
aWdudHk= 26919
4oQ= 26920
cGxpbmdz 26921
IGFudGliaW90aWNz 26922
IOS4gA== 26923
IG5ldmVydGhlbGVzcw== 26924
amFuZw== 26925
Y29tbWVyY2U= 26926
IHNwb2lsZXI= 26927
IGdsb3Zl 26928
IGNoYXR0ZXI= 26929
IEJZ 26930
fj8= 26931
IO2YuA== 26932
IGRlbW9s 26933
d2VjaHNlbA== 26934
aW1pcg== 26935
IHJhaWQ= 26936
0LXRgNGF 26937
7J6Q6riw 26938
ZW5m 26939
IGNvbW1lbnRlZA== 26940
IG9wdGltaXplZA== 26941
IGNvbnZpY3RlZA== 26942
IGJhdHM= 26943
IFNC 26944
IEF1cg== 26945
IFRvbmc= 26946
IGltcGxpY2l0 26947
IEphbmV0 26948
IHJlYWc= 26949
44Gy 26950
IEFkdmFuY2Vk 26951
IGltcG9zZQ== 26952
16nXlA== 26953
IHNjaGVtZXM= 26954
b3VnaGVy 26955
YWJvbGlj 26956
IOqxsOyjoA== 26957
IHNsb3dpbmc= 26958
IHd0ZWR5 26959
IGRlc3RydWN0aXZl 26960
INC+0L/RgNC10LQ= 26961
IGxhbmRtYXJr 26962
IOuPiA== 26963
IFdhbGtpbmc= 26964
4bq5 26965
IHRpamQ= 26966
IEtO 26967
IFF1YW50 26968
7Jik6w== 26969
INC60YDRgw== 26970
IHBlcmRlcg== 26971
IG5vdmU= 26972
w6RuZGU= 26973
IOOBlw== 26974
Ymlh 26975
IGN1c3RvZHk= 26976
IGJpb2Q= 26977
5p2x6KW/ 26978
IGRpcmVjdGluZw== 26979
Li4u4oCL 26980
IHJlbG9j 26981
IGRlbWFuZGU= 26982
44KT44Gg 26983
IG/En2x1bQ== 26984
INC+0LTQvdCw 26985
IE1pbGs= 26986
5Y+3 26987
IEtyYQ== 26988
IEhvbmRh 26989
IHB1ZQ== 26990
IGVsZWt0 26991
IGJlZ2lubmVycw== 26992
IHNwZWFy 26993
w61uaA== 26994
IEx1ZnQ= 26995
IG5pZw== 26996
IFNjaG9vbHM= 26997
IGZvcnVtcw== 26998
IFFpbg== 26999
cHBv 27000
IHphZw== 27001
INCu 27002
IHRvb3RocA== 27003
IFN0eWxl 27004
7LSI 27005
IHB1bmN0 27006
IHJlcHM= 27007
IEFseQ== 27008
IGFtZW5kbWVudHM= 27009
IMO2eg== 27010
IGRpZ2l0cw== 27011
dXJhaQ== 27012
IGNoYW90aWM= 27013
IE1hc3RlcnM= 27014
ZW9u 27015
IENhc2g= 27016
IEN1eg== 27017
IGJlZGV1dGV0 27018
IHNjYW5uaW5n 27019
INC20LQ= 27020
0L3QtdGC 27021
IGNlcnRhaW50eQ== 27022
amVr 27023
IGRpam8= 27024
IENsaW1hdGU= 27025
IHJpbnNl 27026
IGtyaWo= 27027
dmVsYW5k 27028
IHNvdW5kdHJhY2s= 27029
IFNhZmU= 27030
IE5vdmE= 27031
OTQ= 27032
IGF0aGU= 27033
IFZlcmI= 27034
b2xlcg== 27035
7J207KOg 27036
IHZpbg== 27037
IHJlc3BpcmF0b3J5 27038
IFN0dWR5 27039
IENBTQ== 27040
IGF2b2NhZG8= 27041
IFpoZW4= 27042
IGxhdGVuY3k= 27043
IGZlYXRoZXJz 27044
IGNvbnRhcg== 27045
INCy0LXRiQ== 27046
IGZhcms= 27047
IGJsZW5kZWQ= 27048
IGV4cGxvZGVk 27049
IFhY 27050
IEJlbmlt 27051
IGFsZ3XDqW0= 27052
aXN0b2lyZQ== 27053
IGNvbmZpZGVudGlhbA== 27054
IG1hc3Q= 27055
IOy/ 27056
Z2Vo 27057
IGRpc3Jlc3BlY3Q= 27058
IFN5c3RlbXM= 27059
xrBh 27060
RWQ= 27061
IHd5cw== 27062
IGV4b3RpYw== 27063
IGdsb3dpbmc= 27064
w7luZw== 27065
b3VuZ2U= 27066
6IQ= 27067
0LDQvdC40Lc= 27068
IHBhbGF2 27069
IFN3b3Jk 27070
IGdpbQ== 27071
IENyb3c= 27072
IHBvdGVudA== 27073
YmlzaA== 27074
IGFidXNlZA== 27075
IEplZA== 27076
IGdhbWJsaW5n 27077
IFNwZWN0 27078
IGludmVzdGlnYXRvcnM= 27079
5pma 27080
IHJhdHQ= 27081
IGRvYg== 27082
IERFUw== 27083
aG9n 27084
INC+0YLQutGA0Ys= 27085
7YyF 27086
INC00LXQvdGM0LPQuA== 27087
IO2YuQ== 27088
IOuouOumrA== 27089
IHNhdHVyYXRpb24= 27090
IGluaGVyaXRlZA== 27091
IElubm92YXRpb24= 27092
7JeI642Y 27093
IHRhbmdpYmxl 27094
IGRlcHJp 27095
aGVk 27096
INC/0L7QvNC+0LM= 27097
IHNsaWNlZA== 27098
4KWN 27099
IHRo4bq/ 27100
xaU= 27101
Njg= 27102
IGNvcm9uYQ== 27103
IGdpZnRlZA== 27104
IHNvaXI= 27105
IGh1bWlsaXR5 27106
IOydtOqxuA== 27107
IGZsYXdz 27108
INC/0YDQsNC60YLQuA== 27109
IGthbGQ= 27110
d2HFvA== 27111
eXc= 27112
44KT44Gn44GZ 27113
aXJ0ZWVu 27114
IGNyb2NoZXRz 27115
pqzqsIA= 27116
IOyghOyXkA== 27117
IGRlc2U= 27118
5qWt 27119
INC80LDQsw== 27120
IGR6aWHFgg== 27121
IGzDqWc= 27122
Y2hhbmdpbmc= 27123
IGxsZXY= 27124
xYRzaw== 27125
55S7 27126
IDE5ODQ= 27127
b3Jucw== 27128
IFdlbHNo 27129
IHBoYXJtYWNldXRpY2Fs 27130
IHB1bXBpbmc= 27131
IFNoYXc= 27132
cHVuaw== 27133
IHZhdWx0 27134
IGtpbmV0aWM= 27135
IGh1cnJpY2FuZQ== 27136
IEluY2x1ZGluZw== 27137
4bupYw== 27138
IEdyYW5kcGE= 27139
YW5zaGlw 27140
6aaZ5riv 27141
INCy0YvRhdC+0LQ= 27142
0L3QvtC2 27143
nKA= 27144
dXR0YQ== 27145
IOqygeuLiOuLpA== 27146
IGJheg== 27147
INC/0L7RiA== 27148
IHBlY3VsaWFy 27149
ennEhw== 27150
IEVsbGll 27151
IGxlYXJucw== 27152
IEtyaXNobmE= 27153
IGNvbnNlY3V0 27154
IGVtcGF0aA== 27155
IERpbg== 27156
IHRyYWRlZA== 27157
IEJvcmlz 27158
dWdnYWdl 27159
b2xsYQ== 27160
INC90LDQt9Cy 27161
IGV0ZXJuaXR5 27162
INCy0L8= 27163
w6htZXM= 27164
IGdyYXBw 27165
YsOp 27166
INC/0YDQtdC00YHRgtCw0LI= 27167
IEZD 27168
jeuLiOuLpA== 27169
ZXZlbg== 27170
IE5lYnJhc2th 27171
b3J0dW5l 27172
IGthcmVuYQ== 27173
IEFnZW50 27174
IHN0aW5n 27175
IFBJ 27176
IG11bmljaXBhbA== 27177
cG93ZXJlZA== 27178
IGNvbnNlZ3Vl 27179
IE1hbmNoZXN0ZXI= 27180
IHJhaW55 27181
IGJsaQ== 27182
IGtvc3Q= 27183
IGhhbHRlbg== 27184
IEFoaGg= 27185
aW5zdWxh 27186
ZXJ0aW5n 27187
INin2YTZgQ== 27188
IHJlbGFjaW9u 27189
IGtvbWVu 27190
IGRvbWU= 27191
IHByaWVzdHM= 27192
IEludHJvZHU= 27193
cm9waGU= 27194
c2hvcmU= 27195
dmVsdA== 27196
Y2xpcHNl 27197
INGA0YPRgQ== 27198
15nXoQ== 27199
IHNhYmVtb3M= 27200
IEhvbGxhbmQ= 27201
b2dp 27202
YW5raQ== 27203
IE1hdHM= 27204
IHNtb2tlZA== 27205
dWxsaWU= 27206
IGV1cm9wZQ== 27207
INC00LXQudGB0YLQstC40YLQtdC70YzQvdC+ 27208
IGJhcmR6aWVq 27209
IHRyYW5zZm9ybWluZw== 27210
IEV6 27211
b3BhdGg= 27212
IOyWuOuLiA== 27213
INGB0YLQsNC9 27214
4bqxbmc= 27215
4Lix4LmJ 27216
IE91Y2g= 27217
IGNsZWFyYW5jZQ== 27218
dXN0YWlu 27219
IHNvbGlkYXJpdHk= 27220
IHByb3Zpbmc= 27221
INCY0L0= 27222
INGB0Yo= 27223
IHByb2xvbmc= 27224
0LDQtNC90L4= 27225
IHNvcw== 27226
IERlYWw= 27227
IDE3MA== 27228
bW9ucw== 27229
INC30LXQvA== 27230
IGxvZ2dlZA== 27231
IGxpZmVsb25n 27232
IHNlbnNvcnk= 27233
IGJlaG9sZA== 27234
IEZBUg== 27235
w6h0ZW1lbnQ= 27236
IEZlZGVyYXRpb24= 27237
IGRvZGdl 27238
IFNoaXI= 27239
IGRyYWdvbnM= 27240
IEFyY3RpYw== 27241
xIXFvA== 27242
xY0= 27243
wro= 27244
IGRlbmtl 27245
IHBvZHLDrWE= 27246
Y29sZQ== 27247
0YPQu9GM0YLQsNGC 27248
IHN5c3RlbWF0aWM= 27249
0LDQvNCw 27250
Y2hvcw== 27251
IGNsaW5pY3M= 27252
IEJT 27253
IHRhbGVz 27254
dXNpb25z 27255
IO2IrA== 27256
IHByZXNlcnZhdGlvbg== 27257
IGxvcmU= 27258
IFByb3Rlc3Q= 27259
4bub 27260
5biC 27261
IGFja25vd2xlZGdlZA== 27262
IElzYWlhaA== 27263
IOuVjOuKlA== 27264
INeY 27265
IGNvbXBldGl0b3I= 27266
IGFkdmFuY2luZw== 27267
emlw 27268
IHRlbnRo 27269
IExhdXJl 27270
IGhpbnRz 27271
IGV4ZXJjaXNpbmc= 27272
npzr 27273
IEludGVsbGlnZW5jZQ== 27274
dWF0ZWQ= 27275
T1VU 27276
b3BlZA== 27277
IGF1dG9ub215 27278
IGJyYW5kaW5n 27279
IE1lZGl0ZXJyYW5lYW4= 27280
0ZbQug== 27281
IHNjcmV3ZHJpdmVy 27282
IHN1cHJl 27283
IHN0YXA= 27284
IGp1cmlzZGljdGlvbg== 27285
IFNldHRpbmdz 27286
IGZvcmVmcm9udA== 27287
IEZlbWFsZQ== 27288
Y29tZm9ydA== 27289
IG11bHRpcGxpY2F0aW9u 27290
IE11cnJheQ== 27291
IGJvYg== 27292
IFRhcw== 27293
IHRhaHU= 27294
IG9udW4= 27295
ZXR0ZXI= 27296
IHByb3BoZXRz 27297
bGFn 27298
IHJldmVudWVz 27299
IHByw6E= 27300
IHVwbG9hZGluZw== 27301
IG1hY2hpbmVyeQ== 27302
YXNjYWw= 27303
IEVzdMOh 27304
IEdvdGg= 27305
IEJhbGQ= 27306
IFNhdw== 27307
IHN0cmlwZXM= 27308
7KCR 27309
IHBvd2lu 27310
5pel5pys 27311
IGhvc3RpbGU= 27312
IGRhcnVt 27313
IHByZXZlbnRlZA== 27314
0L7QttCw0LvRg9C50YHRgtCw 27315
IGFsZ3VuYXM= 27316
IGhvcGVsZXNz 27317
IHpuYWo= 27318
IHJlYWRpbmdz 27319
IGNyYXZpbmc= 27320
dGF0 27321
IFBpZw== 27322
IGxpYXI= 27323
54ix 27324
IG11bHRpcGxheWVy 27325
IGRhbGU= 27326
IENvdXJzZQ== 27327
7YG8 27328
IEtpdGE= 27329
IGN1c3RvbXM= 27330
IHJlc3BvbmRz 27331
ZW5kcmE= 27332
6KaW 27333
IG1ldHJv 27334
0YHQvtC7 27335
IG1pdGlnYXRl 27336
IG9wcHJlc3Npb24= 27337
IOaIkeWAkQ== 27338
cXVpbmhv 27339
IGFtbW8= 27340
IGVuZmVy 27341
IHBvbnk= 27342
IG91bmNlcw== 27343
sJQ= 27344
IOyImOqwgA== 27345
IGRpY2hv 27346
IERlYg== 27347
IHdvbmRlcnM= 27348
IFJvb3Nl 27349
IHByaXplcw== 27350
IEFMRVg= 27351
IHRoYW5rZnVsbHk= 27352
IHRpc3N1ZXM= 27353
INGA0LDQstC90L4= 27354
IEx1bmE= 27355
aW50ZWxsaWdpYmxl 27356
IOyZuA== 27357
6rCR 27358
IEhlYXQ= 27359
INGB0LjQtA== 27360
IFF1aQ== 27361
IGlvbnM= 27362
IGFjY29tbW9kYXRpb24= 27363
5L6/ 27364
IEthcnQ= 27365
aWVuc3Q= 27366
IHRhcmRl 27367
IHNvYWtlZA== 27368
IENhc2V5 27369
IOy0nQ== 27370
INGA0YPQsQ== 27371
IGRpZmZlcmVudGk= 27372
IGxlZnRvdmVy 27373
IGV4Y2hhbmdlcw== 27374
c2Vjb25k 27375
IGZpcnN0bHk= 27376
IGJ1aWxkZXI= 27377
cmllbg== 27378
IGR3 27379
IGJvdW5jaW5n 27380
Pzwv 27381
IOuMgO2VtOyEnA== 27382
INGB0LU= 27383
IE1pbGVz 27384
aWVuaWU= 27385
INC/0L7QtNC/0LjRgQ== 27386
IOustA== 27387
IGFyaXNlcw== 27388
IHN1YmNvbnNjaW91cw== 27389
IFNhbmR5 27390
IGxvdHRlcnk= 27391
4oCR 27392
YW1pbGlhcg== 27393
IGNvb3JkaW5hdG9y 27394
6Iw= 27395
IGV4dHJhb3JkaW4= 27396
IFJvbmFsZA== 27397
IE1PTg== 27398
Z3JlZW4= 27399
IG1hbnVmYWN0dXJl 27400
IFJlY29yZA== 27401
IE1hcmtldGluZw== 27402
0LjRhg== 27403
IGNyZWRlbnRpYWxz 27404
IHVwcmlnaHQ= 27405
IEhlcml0YWdl 27406
IGfDtnJk 27407
5pyN 27408
ZXhwZW5zaXZl 27409
4bqtbg== 27410
IOyxhA== 27411
IG91dGxpbmVk 27412
IE9vb2g= 27413
b3JpZW50ZWQ= 27414
IHdpcmVk 27415
IG91dGxldHM= 27416
IGh1Z2VseQ== 27417
IO2WiOuKlOuNsA== 27418
0LDRgNGC 27419
IGxvZ2lzdGljcw== 27420
IHNlYXNvbmFs 27421
IGRlYmU= 27422
IHRoZW9y 27423
IHBpcmF0ZQ== 27424
YXBweQ== 27425
IGtub3Rz 27426
IGZlbW1l 27427
IFNvZnR3YXJl 27428
Z2VuZGU= 27429
0YLQsNC60Lg= 27430
IHRlbXBsZXM= 27431
IGxpbWl0YXRpb24= 27432
IGFtcGxpdHVkZQ== 27433
IGhhY2Vu 27434
IGF1ZGk= 27435
IOuWqA== 27436
IFdhaGw= 27437
IG5paA== 27438
IGFtcGxpZmllcg== 27439
YXJpdXM= 27440
aXphZG8= 27441
YWNoYQ== 27442
IGt1bGxhbg== 27443
IFR3aW4= 27444
IEZvcmNlcw== 27445
IGFicmly 27446
IEVQQQ== 27447
IEFoYQ== 27448
IOq3uOuemOuPhA== 27449
IGJpb20= 27450
INCi0LDQvA== 27451
IHNhaWxpbmc= 27452
IEpva2Vy 27453
Rmlyc3Q= 27454
6L+Z5piv 27455
fl0= 27456
b3JzY2g= 27457
IHbDpnJl 27458
IGJlZXRqZQ== 27459
IFNwYcOf 27460
cG9saXQ= 27461
IHR1cmJ1bA== 27462
IOyggO2drOqwgA== 27463
IGNpYw== 27464
IERyYWtl 27465
IEJSSQ== 27466
aXphw6fDo28= 27467
IOyeiOuLpA== 27468
IEx5bm4= 27469
IHRyYW5zZ2VuZGVy 27470
IHJlc2lnbg== 27471
IGNoYXJ0ZXI= 27472
IEpI 27473
IEhvbG1lcw== 27474
IExpcA== 27475
ZGFz 27476
IHBlZGlhdHJpYw== 27477
IG1lbW9yaXpl 27478
IGV2YWx1YXRpbmc= 27479
IPCfkA== 27480
Y2Fr 27481
IGNvbmp1bmN0aW9u 27482
IHJlc2VydmVz 27483
IHNoYW1wb28= 27484
IGp1ZGdlZA== 27485
IHdpZHo= 27486
VklO 27487
IGFib2FyZA== 27488
YXJpcw== 27489
IFJvaA== 27490
IGNvb2xlZA== 27491
0YHRgtC1 27492
Y2Vw 27493
cm9zdA== 27494
aG90cw== 27495
IE1lbGJvdXJuZQ== 27496
0L7Rh9GM 27497
IHZlbnRpbA== 27498
0LjQvdC+0LI= 27499
IG1vdGlvbnM= 27500
7JeI64qU642w 27501
0LzQtdGA0LjQug== 27502
IENoYXQ= 27503
IGdvdXZlcm5lbWVudA== 27504
5LiA5qyh 27505
IEtpdm9s 27506
IEtpdm9sb3dpdHo= 27507
IG7Ds2k= 27508
INC60YPQtNCw 27509
IGh5ZHJhdWw= 27510
IEJlcmc= 27511
eWx1bQ== 27512
IFByw6RzaWRlbnQ= 27513
cm9weQ== 27514
IHNlbWlj 27515
0Y/QtdGC 27516
IENhcGU= 27517
IGNhbmU= 27518
IGJyaW5nZW4= 27519
IHdpcmluZw== 27520
dW55YQ== 27521
IHJlcGF5 27522
qqk= 27523
IHdvbnQ= 27524
w6FudA== 27525
IGdvdmVy 27526
IExpYmVydHk= 27527
IGVsZWN0cm9tYWdu 27528
IFNpbmdo 27529
INCz0YDRg9C/ 27530
0LPQvtCy 27531
iOustOs= 27532
IFJ1bGU= 27533
IHVuZGVyd2F5 27534
IEZyZWRlcg== 27535
IHR1cmJpbmU= 27536
aXNoaQ== 27537
IGbDrXM= 27538
IEN1bHR1cmU= 27539
YWNyZQ== 27540
IHdhbmRlcg== 27541
IGd1ZXJyYQ== 27542
IHPDtnk= 27543
IEp1cg== 27544
YXdheXM= 27545
IHNjaHdpZXI= 27546
Z3VhcmQ= 27547
IEFiZA== 27548
dWN0aW9u 27549
IGFya2FkYcWfbGFy 27550
IEhhbWI= 27551
Py4= 27552
c2l6ZQ== 27553
IE9ydGg= 27554
IHN3YXk= 27555
IM6U 27556
IGFic29ycHRpb24= 27557
aW5lZXM= 27558
IHBhdHJvbnM= 27559
IGJlYWNoZXM= 27560
R0c= 27561
IGNvbnRhbWlu 27562
aW50ZW5kZW50 27563
INC90YDQsNCy 27564
INC00LXRgNC2 27565
IHF1aWx0 27566
IGV2b2x1dGlvbmFyeQ== 27567
7J206528 27568
YXppb25p 27569
IGVya2w= 27570
IEJ1dGxlcg== 27571
IGRvbw== 27572
IG5lZ290aWF0aW9u 27573
ZW5kdW0= 27574
IHRlcm1pbm9sb2d5 27575
IGt1bA== 27576
IFVudGVybmVobWVu 27577
w6lyaWM= 27578
eGk= 27579
YmFk 27580
INC00L7Qu9C20L3Riw== 27581
IE1pdGNoZWxs 27582
dGhyZWU= 27583
5byP 27584
IHN1YnN0cmF0ZQ== 27585
IEluaGFsZQ== 27586
IEFncmlj 27587
dW5nZQ== 27588
INC30YA= 27589
IGFkdmVyc2U= 27590
IOyggOuPhA== 27591
IHBpbGxhcg== 27592
IE1pbnV0ZW4= 27593
IE1hdGU= 27594
IFBsYXR6 27595
IGhlbHBsZXNz 27596
IGFsYXI= 27597
IGZyZW5jaA== 27598
IGFsbG9jYXRpb24= 27599
IHN0ZW1z 27600
IG1hcmF0aG9u 27601
IEhBUkY= 27602
aXphY2nDs24= 27603
SmVzcw== 27604
INC30L3QsNGH 27605
IGRlY2xhcmF0aW9u 27606
RUVSSU5H 27607
c3RlcmRhbQ== 27608
YXNzaXVt 27609
IHNlaXo= 27610
IHByZXNpZGVudHM= 27611
dGFrZQ== 27612
IHdpbGRlcm5lc3M= 27613
IGNvc21pYw== 27614
IOuqqOuRkA== 27615
c3Rybw== 27616
IHBvd2llZHo= 27617
IE1hZ2F6aW5l 27618
IFZJ 27619
INC00LXRgA== 27620
IHfDvHJkZW4= 27621
IHRhYmxldHM= 27622
IHBpZXJ3cw== 27623
IG1vcnRhbA== 27624
IHN1cHBsaWVk 27625
IE7Ds3M= 27626
IFByb3Blcg== 27627
INC60LDQttC00YvQuQ== 27628
b2zDs2c= 27629
67Cp 27630
IG1pc2Nvbg== 27631
IHByb3hpbWl0eQ== 27632
IEFsbGVz 27633
INCz0LvQsNC3 27634
IGxhbWU= 27635
IHZpYmVz 27636
IGRlZW1lZA== 27637
IHVyaW5l 27638
IHJlbWluZGluZw== 27639
IGNpcmN1bXN0YW5jZQ== 27640
65Ok7J20 27641
IGxhcHRvcHM= 27642
wrI= 27643
7ZW07JW8 27644
IE9tZWdh 27645
44Gq44KT44GL 27646
Tlk= 27647
IHB1bXBz 27648
IHJhaWxz 27649
IHN1cnBhc3M= 27650
IEJyb3M= 27651
IG5hdGlvbmFsbHk= 27652
IGdld2VzZW4= 27653
5Lqr 27654
s7Tri6Q= 27655
b3NoaW5n 27656
6rCI 27657
56S+ 27658
IGNyaWFu 27659
IOyCrOuejOydtA== 27660
Y2F1c3Q= 27661
5pW0 27662
0YbQuNC/ 27663
IE9iZXI= 27664
IERBWQ== 27665
IENhbm9u 27666
enVuZw== 27667
IOqwlg== 27668
INCw0LLRgtC+0Lw= 27669
IGRpdm9yY2Vk 27670
15nXpA== 27671
z4HOtQ== 27672
Y2VsYW5k 27673
Y2llcg== 27674
0YDQtdC3 27675
VG9kYXk= 27676
IG9yYml0YWw= 27677
IHN0cmV0 27678
IHNhdHU= 27679
IO2BrOs= 27680
em9z 27681
IFNjbw== 27682
zrzOrQ== 27683
IEd1YXJkaWFu 27684
aW50ZXJlc3Q= 27685
IFZFUg== 27686
w7xuZGVu 27687
INGF0L7RgtC10Ls= 27688
dGl0 27689
Qnk= 27690
IGFubGF0 27691
U2hvdw== 27692
IG9pbHk= 27693
56+A 27694
IGxlZ2VuZHM= 27695
IHNwZWN1bGF0aW9u 27696
IFdpc2g= 27697
IG1vbms= 27698
R0FO 27699
IGjhu40= 27700
IGRhbmdlcnM= 27701
IEJlbmU= 27702
aXF1ZW1lbnQ= 27703
IOuCmOyZgA== 27704
INCw0LQ= 27705
IGRpc2NyZXRl 27706
w4c= 27707
IGNvbmRpdGlvbmFs 27708
IEdpbGw= 27709
dWF0ZXM= 27710
INGB0L7QstGB0LXQvA== 27711
IHNjcmVlbnNob3Q= 27712
Y2Fkbw== 27713
IOuqqOuToA== 27714
IGZpbmdlcnRpcHM= 27715
IE1BQw== 27716
IGR1ZGVz 27717
Y29zdA== 27718
IGJ1bXBz 27719
b25kbw== 27720
IGRhdG9z 27721
IGJlZXBz 27722
IFByb24= 27723
IEtoYWw= 27724
emVnbw== 27725
IEFiYnk= 27726
VWg= 27727
WW8= 27728
IFRlbA== 27729
IM68zq0= 27730
S0k= 27731
IHN0cmVzc2Vz 27732
IHNwcmVhZHNoZWV0 27733
IE5PVw== 27734
REI= 27735
IGxpYmVyYXRpb24= 27736
IHByZWRpY3RhYmxl 27737
IFF1ZXN0aW9ucw== 27738
IHNwYWNpbmc= 27739
IGluaGFiaXRhbnRz 27740
IHp3acSFeg== 27741
57Gz 27742
IFNBUA== 27743
IGx1Z2dhZ2U= 27744
IGhpcHA= 27745
6JY= 27746
IHRhbmdlbnQ= 27747
IHbDpQ== 27748
0LDQu9GM0L3QvtC5 27749
c2VoZW4= 27750
IHByb2Nlc3NvcnM= 27751
IGZpbmRldA== 27752
IGNhcnRyaWRnZQ== 27753
IGFkbWluaXN0cmF0b3Jz 27754
IOyWtOya 27755
IHN1cHJlbWU= 27756
IEFudGk= 27757
IO2UhOuhnA== 27758
IGluZm9ybWF0aXZl 27759
IGtvbXQ= 27760
5oiR5Lmf 27761
15nXmA== 27762
QXNzaXN0YW50 27763
IGxpc3Rh 27764
w7ZsbA== 27765
IGRpc3RpbmN0aXZl 27766
IEh1ZA== 27767
IHNhbG9u 27768
5LiL5L6G 27769
bcOqbWU= 27770
IE1vdGlvbg== 27771
IHNldWxlbWVudA== 27772
IE1lbnNjaA== 27773
IHB1bXBlZA== 27774
w7xoZXI= 27775
aWJv 27776
IHdhxbw= 27777
IHF1YW50aXRhdGl2ZQ== 27778
2b4= 27779
IOuqqOyKtQ== 27780
IHBvdWNo 27781
IFRoZWF0cmU= 27782
YWhp 27783
IHNwaW5hY2g= 27784
IHJlYWxpdGllcw== 27785
IGxleQ== 27786
IE1hcnRoYQ== 27787
IHJlY2hlcg== 27788
ZWNoZXM= 27789
IHBlcmlvZGlj 27790
b2NpZGU= 27791
IEluY3JlZA== 27792
IHRo4bqleQ== 27793
b3Rvbg== 27794
IEVzbw== 27795
IGfDqW7DqXJhbA== 27796
aWxpZ2h0 27797
IGltYWdpbmluZw== 27798
aGVh 27799
ZXRpY2Fs 27800
4but 27801
IERlbW9rcmF0 27802
IGVuam8= 27803
IGFkanVzdGFibGU= 27804
IHJhaW5z 27805
aWV3YcW8 27806
IGp1c3RlbWVudA== 27807
IGp1c3RpZmllZA== 27808
IFNoYWtl 27809
dml2 27810
7IKs66W8 27811
IG1ldHQ= 27812
IEVudmlyb25tZW50YWw= 27813
IHNvbGFtZW50ZQ== 27814
IGludGVyc2VjdA== 27815
IDE5ODg= 27816
IHNpbXVsYXRl 27817
SkE= 27818
INC30LDRgQ== 27819
IGNvbnRpbmc= 27820
IFRlaw== 27821
IHRvcmNo 27822
INC00YDRg9Cz0L7QuQ== 27823
IGluc2NyZQ== 27824
IG1vZGVsbw== 27825
IEdlZw== 27826
IERlbW9jcmF0 27827
0LrQsg== 27828
IEJ1ZGR5 27829
IHJlZHVuZA== 27830
IGNyYWZ0cw== 27831
IEhpag== 27832
IGp1ZQ== 27833
IEtpcms= 27834
IGthYg== 27835
4buj 27836
IGFlc3RoZXQ= 27837
IEpPTg== 27838
IHN1cGVyY29t 27839
INGB0LjRgtGD 27840
IM+Mz4TOuQ== 27841
2YXZhg== 27842
IEVWRVI= 27843
7JWY7Ja0 27844
b2l0 27845
IENsZXZlbGFuZA== 27846
IHNpeHRlZW4= 27847
IHdhdGVyZmFsbA== 27848
77g= 27849
aW5mbA== 27850
IGNvdW5zZWxvcg== 27851
IFB1bms= 27852
IHNwcmVjaGVu 27853
5rWB 27854
ZXhj 27855
IFNraWxscw== 27856
cm96 27857
YWRhbWVudGU= 27858
IHBhbmNha2Vz 27859
6riw66Gc 27860
IHBsYW5r 27861
IHNvdmVyZWlnbnR5 27862
IGZ1aQ== 27863
INC90LXQvtCx 27864
IFdpaQ== 27865
IFNjaG9s 27866
4oCO 27867
IFNwZWFr 27868
6Iux 27869
Y2lsaWF0aW9u 27870
IHRoaWdo 27871
IOqxsOydmA== 27872
IGpvdA== 27873
IOy0rOyYgQ== 27874
INmF24zaug== 27875
IENDUA== 27876
INC/0L7RgdGC 27877
IG9ic2VydmVy 27878
w6Fi 27879
IHN0aWdtYQ== 27880
IHByb3ByaWV0 27881
IGNpZGFkZQ== 27882
IGJhxZ9rYQ== 27883
2LnYqQ== 27884
a3Jl 27885
IHBvd2llZHppZcSH 27886
IGNlYXNl 27887
IHNraW5z 27888
IHZlZ2dpZXM= 27889
IG9wcG9zaW5n 27890
b3BvbHk= 27891
IEp1Zw== 27892
IFlvb24= 27893
IFVuaXQ= 27894
IDE5ODY= 27895
IGtvbnM= 27896
IGRpYWdub3N0aWM= 27897
IGVtcG93ZXJlZA== 27898
IHRobw== 27899
IGNlbg== 27900
w6lyYXRpb24= 27901
INGX 27902
IHBoeXNpYw== 27903
IFByYWN0aWNl 27904
5bed 27905
IFNvdXRoZWFzdA== 27906
IEVzcGE= 27907
6K+3 27908
IEdlb3I= 27909
cm9wb3J0aW9u 27910
IHNwZWNz 27911
IGFkYXB0aXZl 27912
IFVuaXR5 27913
IFdvcmtz 27914
dWdlbg== 27915
IE1vbnRhbmE= 27916
VGhhbmtz 27917
IHdoaXBwZWQ= 27918
IGR1bmdlb24= 27919
IHZpdGFtaW5z 27920
U1A= 27921
IHNjYW5kYWw= 27922
IGRpbmVybw== 27923
b3Zh 27924
IGVtYnJv 27925
IEVhZ2xl 27926
IHRoZW9sb2d5 27927
IFZhbmVzc2E= 27928
IEFJRFM= 27929
65Cc 27930
IGZyZWVs 27931
IEFsemhlaW1lcg== 27932
IMWa 27933
SGVy 27934
IHRvcm5hZG8= 27935
YWdlbnM= 27936
IOyeiOyWtOyEnA== 27937
IFRyYW5zZm9ybQ== 27938
IHByb2Nlc3Nv 27939
IG1pbGxpc2U= 27940
IHByb2Zlc3Npb25hbGx5 27941
IG1lbWI= 27942
b2NhdGlvbg== 27943
IHN0eWxpbmc= 27944
INC+0LHRj9C3 27945
IE9wZXJhdGlvbg== 27946
IHd5Z2w= 27947
IFJhbg== 27948
IOeahA== 27949
IEtpbg== 27950
4buxYw== 27951
IEJBUg== 27952
IHBhcGVyd29yaw== 27953
IHR1bGU= 27954
IHF1ZXJpYQ== 27955
IGNvbXBseQ== 27956
IEhhaXI= 27957
15nXmw== 27958
INC/0YDQvtGB0YI= 27959
IG11dGF0aW9u 27960
IHJlcHLDqXM= 27961
IG9jdG9wdXM= 27962
IGltcG9ydGFudGVz 27963
IGRlc2VydmVk 27964
ZXRy 27965
IGRpc2FzdGVycw== 27966
bMSxbmRh 27967
aXF1w6k= 27968
IERlc2hhbGI= 27969
c29v 27970
b3NzaXA= 27971
IHJlbGlldmVk 27972
IENvbGxpbnM= 27973
IHdhdGVycHJvb2Y= 27974
IFl1aw== 27975
IGNvcHlpbmc= 27976
IGLDvHTDvG4= 27977
IEhldXRl 27978
IEVudHJl 27979
IHJlc2lkdWFs 27980
IGNvbG9uaWVz 27981
IMOpbm9ybQ== 27982
IEVyaW4= 27983
IHN0YW4= 27984
IHRyZW1lbmRvdXNseQ== 27985
IGNhcHR1cmVz 27986
IFNhaQ== 27987
w6JjZQ== 27988
IG1pYcWC 27989
IDg3 27990
IGxvZ2dpbmc= 27991
IGluc2VydGVk 27992
IGluaGVyZW50bHk= 27993
7J2R 27994
bGF2ZQ== 27995
0L3QuNGH 27996
IGZlbW1lcw== 27997
IGTDqXA= 27998
dWtz 27999
YWNpYQ== 28000
IFdhZGU= 28001
IGppag== 28002
IFZpbmNlbnQ= 28003
IEljZWxhbmQ= 28004
aGVt 28005
IGFwb2xvZ3k= 28006
IFBlZw== 28007
IGdsdWVk 28008
IGNvbXBhbmlvbnM= 28009
IExpdmVy 28010
IGNyaXRpY2l6ZWQ= 28011
bGVhZGluZw== 28012
IHPDpGdh 28013
5ryC 28014
IHNxdWlk 28015
IG5hcnJhdGl2ZXM= 28016
IHRha2E= 28017
bmV6 28018
d2VpdA== 28019
IHRyaXBvZA== 28020
IGV4cGxpYw== 28021
IHNwaW5hbA== 28022
IGFwcHJveGltYXRpb24= 28023
IHBhZ2Fy 28024
IENhbHZpbg== 28025
INCy0LXQtNGM 28026
IGxhYw== 28027
IHByb2FjdGl2ZQ== 28028
IFRyYWlu 28029
b3Jm 28030
IHN0ZW4= 28031
IGdyYXBlcw== 28032
IG1ldXM= 28033
IGF1dG9tYXQ= 28034
IGJpYXNlZA== 28035
IGNoYcOubmU= 28036
Y29hbA== 28037
IHJlbmNvbnQ= 28038
IEt1bQ== 28039
IGZlc3RpdmFscw== 28040
IHN0YXJ0dXBz 28041
IGFrYQ== 28042
44G5 28043
IGN5bGluZA== 28044
c25h 28045
Q1JJ 28046
IHJlc3VsdGFkbw== 28047
IG1pbGVzdG9uZQ== 28048
IM+F 28049
IHRlbGVwb3J0 28050
enljaA== 28051
NjI= 28052
5YWz 28053
IEZlYXI= 28054
IG51Y2xldXM= 28055
IHNoaW5lcw== 28056
aG92 28057
IFBhcnRuZXJz 28058
IEthcw== 28059
IG5hZGll 28060
IGFsZXJ0cw== 28061
IEJJTEw= 28062
c3Ryb25n 28063
IE5hdGU= 28064
IERlbm1hcms= 28065
IENhdg== 28066
T1NU 28067
aMOkbHQ= 28068
IOyVhOuLjA== 28069
YW55b24= 28070
IGVuY291cmFnZXM= 28071
INC/0L7RgdGC0LDQsg== 28072
IEh1YW5n 28073
44GK6aGY44GE 28074
U1RB 28075
IHBhaW50cw== 28076
44GZ44GU 28077
IHNjaGVkdWxlcw== 28078
IGNoZWF0ZWQ= 28079
IGFwcHJveA== 28080
IO+3 28081
IMK7Lg== 28082
IHNtaWxlcw== 28083
aXN1cmU= 28084
IG5lcmVk 28085
YXJkZW4= 28086
IGN1cnQ= 28087
IOuM 28088
IFJvdGg= 28089
IHB1aXNxdWU= 28090
IEdFVA== 28091
IFZlZ2V0 28092
IHByb2R1eg== 28093
IEJlbGdpdW0= 28094
IENhbXB1cw== 28095
16jXmded 28096
aWN1dA== 28097
INGB0L3QuNC8 28098
IHLDqXVzcw== 28099
IHNsaXBwZXJ5 28100
IEV3 28101
xbM= 28102
IExlZ2VuZHM= 28103
IFRpZmZhbnk= 28104
0LDQu9C40Lc= 28105
INC/0LXRgNC10LI= 28106
INC+0LPRgNC+0Lw= 28107
IGNyb3M= 28108
IENF 28109
QnU= 28110
IGVuc3VyZXM= 28111
IGdyYW5kY2hpbGRyZW4= 28112
IGFjdWVyZG8= 28113
IHByaXNvbmVy 28114
IHRoaXJzdHk= 28115
YmFuZQ== 28116
IOu5oA== 28117
IMO6bHRpbWE= 28118
IExhdW5jaA== 28119
bml0eQ== 28120
IGNvbWJ1c3Rpb24= 28121
IHVuaWNvcm4= 28122
IGZhbWlsbGU= 28123
IGxvd2VyaW5n 28124
IFlpbmc= 28125
YnVpbGRpbmc= 28126
IGR1bw== 28127
IE3DqXhpY28= 28128
YXN0aWFu 28129
IOuoueydhA== 28130
IFJhbHBo 28131
IHJld3JpdGU= 28132
IGdsYW0= 28133
aWZpcXVl 28134
RXI= 28135
IFJ1bm5pbmc= 28136
0L7QvdC+0LI= 28137
IG1lYW5pbmdz 28138
IGNoZXd5 28139
IExlc2xpZQ== 28140
IGZpbmVzdA== 28141
IGhhaGFoYQ== 28142
IFNURVA= 28143
IGxvbmVsaW5lc3M= 28144
cmlhbnM= 28145
IHF1ZXN0aW9uZWQ= 28146
IGVzcXVl 28147
IHNpbmtpbmc= 28148
IHBlc28= 28149
IFdyb25n 28150
YXNtaW5l 28151
IGRlZmluaXRpdmU= 28152
IGJ1eXM= 28153
IGNydWM= 28154
Y29vbA== 28155
IOugiA== 28156
IHDDsw== 28157
IHV0aWxpemVk 28158
IHdvcnRod2hpbGU= 28159
IER5bGFu 28160
RVNF 28161
IHZlcnRleA== 28162
dMSx 28163
IEZpcg== 28164
IHphdw== 28165
IEdlZA== 28166
INCd0LDQvw== 28167
ZHo= 28168
IGN1cnNvcg== 28169
IHN3aXBl 28170
IGluZXZpdGFibHk= 28171
IHBvc3RlcnM= 28172
IGluY2xpbmVk 28173
IGdyZWV0aW5n 28174
IGRpc2FwcG9pbnRtZW50 28175
44G+44Gn 28176
IHJlbGHDp8Ojbw== 28177
VFQ= 28178
IHJhYmI= 28179
IE1haW5l 28180
IGFuYWx5emVk 28181
RkU= 28182
INCf0L7Quw== 28183
IFNhbmRyYQ== 28184
IHBsYWd1ZQ== 28185
QVJF 28186
IHbDpHI= 28187
IFZpdg== 28188
dW1lZA== 28189
aGFuZG8= 28190
aG91ZXR0ZQ== 28191
IEJhaWxleQ== 28192
5LiN6YGO 28193
eXNvbg== 28194
IHNlbXVh 28195
IGhhcmRjb3Jl 28196
4oKs 28197
0ZbQvA== 28198
w6lyYQ== 28199
T1RI 28200
IGZvcmVpZ25lcnM= 28201
IFBhbGVzdGluaWFu 28202
IHByb3ByaW8= 28203
0LDQvdC40Lk= 28204
IG15dGhz 28205
V0g= 28206
IG5pbnRo 28207
IENyZWF0b3I= 28208
0LvQvtC8 28209
IEZsaXA= 28210
IGVtYW4= 28211
IGtpxZ8= 28212
emllaA== 28213
IEVhcm5lc3Q= 28214
c3lzdGVt 28215
luyXkA== 28216
IGFybWllcw== 28217
IE91dHNpZGU= 28218
IGhhcnVz 28219
5rqW 28220
0L7QtNCw0YA= 28221
IHZpc2l0b3I= 28222
562U 28223
IHN0cmVuZ3RoZW5pbmc= 28224
IDky 28225
dmlv 28226
IOumrA== 28227
IGdyZWVkeQ== 28228
IHBvcXVpdG8= 28229
dWRlcg== 28230
IEtvcGY= 28231
IOuLpOydjOyXkA== 28232
IHNlaXM= 28233
w6F0aWNv 28234
IHRydXN0aW5n 28235
w61w 28236
IEVtbQ== 28237
bGVlbg== 28238
INin2YTZhg== 28239
IHJlY3J1aXRtZW50 28240
IEZpbGlw 28241
INmD2YQ= 28242
Q2xpbnQ= 28243
INCy0LXRgQ== 28244
YXVmdA== 28245
IGRvbWluYXRl 28246
IHJlc3Rv 28247
IGtyYQ== 28248
w6Fp 28249
IENhaXQ= 28250
cm93cw== 28251
IGNvdW50cnlzaWRl 28252
IDE5NDU= 28253
0LDRhtC40Y4= 28254
INC00Lg= 28255
IGtlcm5lbA== 28256
bG92 28257
IGNhbGN1bGF0aW5n 28258
2K/Ypw== 28259
IFdhbHQ= 28260
IGVtcG93ZXJpbmc= 28261
IGNoYXNzaXM= 28262
bGluZWFy 28263
0LPRgw== 28264
IG5vdmE= 28265
IHV5 28266
IDY5 28267
IGVuY29tcGFzcw== 28268
dHJs 28269
IGNvbXB1dGF0aW9uYWw= 28270
IHdvcm1z 28271
IG5oaeG7gXU= 28272
IGFzdHJvbmF1dHM= 28273
IHZlcw== 28274
IHN5dHU= 28275
IGRlbWFuZGVk 28276
IGNz 28277
IE1vbA== 28278
IGA= 28279
IGNoYW50 28280
IHRoZXJlYnk= 28281
IHBlbmlz 28282
IGVtb2M= 28283
d3lu 28284
0YPQttC1 28285
IHRyZWFk 28286
w7NsZQ== 28287
IGRlZXBlc3Q= 28288
IG1hY2hl 28289
IFZlbnQ= 28290
IEFtc3RlcmRhbQ== 28291
44Ob 28292
IHJlYmVs 28293
IDYx 28294
INCy0LrRg9GB 28295
dWZmcw== 28296
IGRvxJ9ydQ== 28297
IE5hcG9sZQ== 28298
zq7Pgw== 28299
IHdvcmtvdXRz 28300
IEdsYWQ= 28301
0L3QtdGB 28302
IHRlbnNpb25z 28303
IFNoaWZ0 28304
IEd1ZXI= 28305
7YyQ 28306
IOy5nOq1rA== 28307
0JY= 28308
IGltcGxhbnQ= 28309
w6p1 28310
6riA 28311
IGF1dGhvcml6ZWQ= 28312
Q0VS 28313
IFJW 28314
IGhpbA== 28315
bGV2 28316
Y2ltZW50bw== 28317
IFVGTw== 28318
7IOI 28319
6KiC 28320
d29y 28321
IGRhbmNlcw== 28322
IFBpeGVs 28323
55yL5LiA5LiL 28324
IHRyb3R6ZGVt 28325
IG9idGVu 28326
IEFsZnJlZA== 28327
IGNvc3RseQ== 28328
IFN0YW5sZXk= 28329
IHRlcnJvcmlzdHM= 28330
IFdpZA== 28331
heuLiOuLpA== 28332
IGxlaWNodA== 28333
7J207Iqk 28334
IGRvYnJ6ZQ== 28335
IGhlc2l0 28336
IGVyesOkaA== 28337
IGVpbmlnZQ== 28338
IGhlYnQ= 28339
0YHQtQ== 28340
IHVucHJlZGljdA== 28341
Q8OzbW8= 28342
cmVtb3M= 28343
IFRoYW5rZnVsbHk= 28344
IHB1cnNl 28345
Y2hz 28346
YW5jZXI= 28347
dWxvcw== 28348
c3R1ZA== 28349
5pyJ5rKS5pyJ 28350
IG5ldXJvbG9n 28351
IEFuY2llbnQ= 28352
T3V0 28353
YXdzemU= 28354
IG9wcG9zZQ== 28355
IGFudGlib2RpZXM= 28356
IFNvbWVob3c= 28357
cm9wb2xpdGFu 28358
a3Rvcg== 28359
INGB0YLQvtGA0L7QvdGL 28360
IHJvY2tldHM= 28361
IGRpc2FibGU= 28362
IGNhdGFzdHJvcGg= 28363
tOye 28364
IGN5bg== 28365
INC00YDRg9C30YzRjw== 28366
IGluc3RydWN0b3Jz 28367
ZW1hYWw= 28368
IGV0d2E= 28369
IHl1YW4= 28370
IEdyb3VuZA== 28371
IHByZW1pZXJl 28372
0YfQuNCy 28373
IHNhaW50 28374
eWJh 28375
IGtvaw== 28376
IGNvbnRyYWN0b3Jz 28377
IOqwgQ== 28378
INeQ15w= 28379
IGhlYWRsaW5l 28380
IGNvbXBsZXRhbWVudGU= 28381
IGluZXhwZW5zaXZl 28382
IHZpdQ== 28383
IEdyYW5kZQ== 28384
IGJsZWVk 28385
66y8 28386
IDcz 28387
IHRvZGF2w61h 28388
IFJ1c2g= 28389
IEVsZGVy 28390
6rCA64qU 28391
IFJvdQ== 28392
INC20LXQvdGJ 28393
IE1pcmE= 28394
IGRlaW5l 28395
IGthcm1h 28396
IHVtbQ== 28397
IGVudHNjaGU= 28398
IEhvbG9jYXVzdA== 28399
IGRpc2NvdmVyaWVz 28400
YW1lbnRz 28401
IHJhaXNvbg== 28402
IGJ1cmdlcnM= 28403
QmFjaw== 28404
IGdkeQ== 28405
IEFH 28406
IERhdw== 28407
7JWg 28408
aGVhZGVk 28409
IENsYXI= 28410
SW5zdA== 28411
IExpZXV0ZW5hbnQ= 28412
IEFmRA== 28413
IENlcw== 28414
IHBlcnNvbmFsaXplZA== 28415
IGludGVyZmFjZXM= 28416
4LiI4Liw 28417
INGA0LXQtg== 28418
IHN1aWM= 28419
IHN0YXJ2aW5n 28420
IG94aWRl 28421
IGRlY29yYXRlZA== 28422
IERV 28423
IOyYiOyBmA== 28424
IHF1bw== 28425
IGRpc3RvcnRpb24= 28426
5q61 28427
IOuoueyWtOs= 28428
IHN0YWtlcw== 28429
5piO55m9 28430
IHN5bnRheA== 28431
IGJp4bq/dA== 28432
dGh5 28433
aWNpZQ== 28434
IGJyYXNpbGU= 28435
aXNpcw== 28436
UkM= 28437
IHNob29r 28438
IGRlcHRocw== 28439
IENvc3Rh 28440
IHZvY2Fscw== 28441
IGNvYXN0ZXI= 28442
IGZhbG91 28443
ZXR0bGU= 28444
IGtlbm5lbg== 28445
IGRlcml2ZQ== 28446
IGFpZHM= 28447
INCd0LjQug== 28448
IGVudHdpYw== 28449
IHZlcnRpY2FsbHk= 28450
IM0= 28451
IFNVVg== 28452
IGZpcmV3b3Jrcw== 28453
IHNwZWNpZmljcw== 28454
5Lqk 28455
IGluc2lzdGVk 28456
IGRlc2hhbGI= 28457
IEdvbno= 28458
bG92ZQ== 28459
IE1pbGl0YXJ5 28460
IFBpZXJyZQ== 28461
IOKI 28462
IFdob3Nl 28463
IHBlcmZ1bWU= 28464
IM+AzrU= 28465
IGxvd2VyZWQ= 28466
IGNyb3NzZXM= 28467
IHRyYW5zbGF0ZXM= 28468
IGFycmliYQ== 28469
w61kbw== 28470
IExldg== 28471
5YWn 28472
IENpYW8= 28473
IHNjaG9sYXJzaGlwcw== 28474
IGdlc3R1cmVz 28475
INGA0LXQt9GD0LvRjNGC0LDRgg== 28476
IHF1ZXN0w6Nv 28477
IENvbG9uZWw= 28478
IEJvdHQ= 28479
2LHZgQ== 28480
TklORw== 28481
IFdhdGNoaW5n 28482
IFB1cnBsZQ== 28483
0YHRgtGA0LDQvQ== 28484
IGV4ZWN1dGl2ZXM= 28485
IEtyaXM= 28486
b3JuZXlz 28487
0LXQvdC90YvQuQ== 28488
IGNvYXRlZA== 28489
xKk= 28490
IHBhcmtlZA== 28491
INGB0LLQtdGC 28492
ISEhISE= 28493
IEZsb3lk 28494
xLFzxLE= 28495
emnEhw== 28496
IG1vdGl2YXRl 28497
IEVsb24= 28498
bGVhbg== 28499
hpM= 28500
IGlw 28501
IG5pxbw= 28502
IEV4cGVyaWVuY2U= 28503
IFRpbmE= 28504
IEtvbGxlZ2U= 28505
IEFtYmFzc2Fkb3I= 28506
aW55YQ== 28507
IHRoZWZ0 28508
IGhldXJlcw== 28509
IE15c3Q= 28510
IG1haXNvbg== 28511
bGVi 28512
IGJvd2xz 28513
IELDvHJnZXI= 28514
IFJvb3NldmVsdA== 28515
UlA= 28516
6rCA7JqU 28517
IERlbGljaW91cw== 28518
ZXJkaW5ncw== 28519
IEFzc29jaWF0ZQ== 28520
b3Vzc2U= 28521
IENvcnQ= 28522
IFJlcGVhdA== 28523
IEdsb3J5 28524
IGNvbnRhZw== 28525
4LmA4Lil 28526
IFBhcmFk 28527
IEtlcnJ5 28528
IOq/ 28529
IFdhdmU= 28530
5b+F 28531
IGdhdGV3YXk= 28532
55CD 28533
IeOAjQ== 28534
IHRyYW5zY2VuZA== 28535
IGRhbWFnZXM= 28536
IHRhaWxz 28537
IGdyYXZpdGF0aW9uYWw= 28538
IFNoaWVsZA== 28539
IHByaW1pdGl2ZQ== 28540
IGNhcnJpZXJz 28541
IEh1YXdlaQ== 28542
2YLYrw== 28543
IGZlbGl6 28544
IE1pYQ== 28545
5YOV 28546
INC/0YDRj9C80L4= 28547
INC/0YDQvtC40YHRhdC+0LTQuNGC 28548
IE11cnBoeQ== 28549
IEFjdGl2 28550
44OD44Kv 28551
IGRpc2NvbWZvcnQ= 28552
15HXlA== 28553
IEtlbGw= 28554
IENlbnR1cnk= 28555
IHNwYWdoZXR0aQ== 28556
IER1cmNo 28557
IGNpZXJ0bw== 28558
IEVtcHJlc3M= 28559
IGd1dHM= 28560
bmVn 28561
INC00L7RgdGC0LDRgtC+0YfQvdC+ 28562
IHZvbHVudGFyeQ== 28563
5aSx 28564
IHNxdWlycmVs 28565
5qyi 28566
44Gh44KJ 28567
IE1heg== 28568
tOyLrA== 28569
INCy0Lg= 28570
44Kn 28571
INGC0LDQutC40YU= 28572
IFNoYXJvbg== 28573
IGVudGh1c2lhc3RpYw== 28574
aXJlbWVudA== 28575
IO2emOuTpA== 28576
IHBvdHJ6ZQ== 28577
IGluaXRpYXRlZA== 28578
44On 28579
IMWbcm9k 28580
IOydtOumhA== 28581
IHJlbWFrZQ== 28582
IGN1bG1pbg== 28583
IGNvbmZ1c2U= 28584
bWl5b3I= 28585
dXJhcg== 28586
Q1RPUg== 28587
IGJ1bm55 28588
IOWkpw== 28589
5LiN6IO9 28590
ZWxw 28591
IHZhbXBpcmU= 28592
IGlsbHVtaW4= 28593
IEhlbmQ= 28594
INC60LDRh9C1 28595
IFNhbHY= 28596
INC60LDQvdCw0Ls= 28597
IHBvcnRh 28598
IGFzc2hvbGU= 28599
IHN1cHBvcnRlcg== 28600
IHNrZXB0aWNhbA== 28601
IGtuZWFk 28602
IOyYrA== 28603
ZXph 28604
IHF1w6o= 28605
IERI 28606
IHJvZHo= 28607
b3duZXJz 28608
IHBsb3Rz 28609
IGRlbGF5cw== 28610
IGJlbG9uZ2Vk 28611
IGFoaA== 28612
IGNhcnZlZA== 28613
IHJpc2Vu 28614
IG9yZGVu 28615
cGhvbnk= 28616
aXNzeQ== 28617
ISEhISEhISE= 28618
IG9sZHXEn3VudQ== 28619
IHJvc2Vz 28620
IGludHJpbnM= 28621
IEFuZ3N0 28622
IGZpbmFsZW1lbnQ= 28623
7Ked 28624
U09VTkQ= 28625
IGluZHVs 28626
sIw= 28627
INeV15Q= 28628
Y2h5 28629
0LDQutGB0LjQvA== 28630
IG5nZ2Fr 28631
IGxpeg== 28632
IGVsZWN0b3JhbA== 28633
IFNoYXdu 28634
cmljaWE= 28635
IGFyc2Vu 28636
IFBlcA== 28637
IDIwMzA= 28638
IHRyb3BoeQ== 28639
IHNtb290aGVy 28640
IGVycmU= 28641
IGNyYXNoZXM= 28642
IHNjaG5l 28643
IGFzaQ== 28644
IE1hw58= 28645
0YPQu9C4 28646
0YfQtdGB0LrQuA== 28647
aWV2ZXM= 28648
UkVBTQ== 28649
IHN0aXJyaW5n 28650
44OA 28651
dXN0YQ== 28652
IGludmVy 28653
c2lnaHQ= 28654
b3JkdQ== 28655
b29y 28656
IMSDbg== 28657
IHBlcm1pdHRlZA== 28658
0YDRjA== 28659
IGNoYWxr 28660
44KI44GX 28661
IHRhdHRvb3M= 28662
IFJlbGF0aW9ucw== 28663
IEhveQ== 28664
a3NhbQ== 28665
IGRlbnRpc3Q= 28666
IOuvuOq1rQ== 28667
IHNvZmE= 28668
INGU 28669
IGZvcm1l 28670
2YLYqQ== 28671
IOuyoA== 28672
IGVtYnJhY2Vk 28673
bWls 28674
IHN1bmdsYXNzZXM= 28675
IOqwlA== 28676
IHNlYW1sZXNz 28677
IGJlZXA= 28678
w6RjaHN0 28679
IHN3ZWV0cw== 28680
IHNlbWFpbmU= 28681
IGlycmVsZXZhbnQ= 28682
IGRlc2Vudm9s 28683
z4HPiQ== 28684
INC/0YDQvtC40LfQstC+0LQ= 28685
YW5ncw== 28686
IGFyb21h 28687
IHBvb2xz 28688
IGdp4bud 28689
IFVn 28690
IGNsaW1iZWQ= 28691
IHRyZW5kaW5n 28692
IHNlcGVydGk= 28693
IEJhcnI= 28694
IHDFgg== 28695
IE9yaWdpbmFsbHk= 28696
INqv 28697
dXR0bw== 28698
irjr 28699
INC60L7RgtC+0YDRi9GF 28700
INC30LDRhQ== 28701
IGVpZ2VuZW4= 28702
IG11cmRlcmVy 28703
ZXJuYW1l 28704
xZ4= 28705
IGFubm91bmNpbmc= 28706
IFBsYXRmb3Jt 28707
IGV4cGxhbmF0aW9ucw== 28708
IHByZXNlbnRl 28709
IE5hc8SxbA== 28710
IG9ycGhhbg== 28711
IEZvcnRuaXRl 28712
cm9zcGVjdA== 28713
ZXJlZGl0aA== 28714
IOyXhuyWtA== 28715
IE5JSA== 28716
d2FnZW4= 28717
IHJlbWVk 28718
p4Dr 28719
bW9udA== 28720
IEplZmZyZXk= 28721
cHJvbQ== 28722
IGbDvG5m 28723
INC90LDQt9Cw0LQ= 28724
IGN1Y3VtYmVy 28725
IFN1bW1pdA== 28726
5Yid 28727
p6Q= 28728
0J3QkNCv 28729
IEpldA== 28730
IGNhbWJpbw== 28731
0YPQudGC0LU= 28732
IGN1Ymlj 28733
IGRpc3Byb3BvcnRpb24= 28734
ZXJleg== 28735
IG1hZG5lc3M= 28736
55eb 28737
IHRpbnQ= 28738
IGZ1ZXJvbg== 28739
IGt5 28740
IGJpcGFydA== 28741
44G+44Gb 28742
U2Ft 28743
IOu9 28744
IHJpdg== 28745
IFRhbms= 28746
IOuGkw== 28747
IHJlbmRlcmVk 28748
xZtsxJk= 28749
Y29uZHM= 28750
IGRpc3J1cHRpb24= 28751
IGluY29udmVu 28752
IHF1aXNlcg== 28753
IGRlbmlhbA== 28754
IGdhbGF4aWVz 28755
IHNvdmVyZWlnbg== 28756
IHBvbHNr 28757
z4HPjg== 28758
IG1leA== 28759
IGNhcmFjdGVy 28760
IExlZ28= 28761
YW5kZW4= 28762
Lici 28763
IO2UjOs= 28764
IGNvbXByZXNzb3I= 28765
IE1vdmll 28766
IGFwcGxpY2FudHM= 28767
emllaGVu 28768
IHZlZ2V0YXRpb24= 28769
IGJlbGxl 28770
IEdPT0Q= 28771
IEJhdQ== 28772
IHJlc2VudA== 28773
c2V4 28774
YW1lbnRvcw== 28775
INeU15bXlA== 28776
IG92ZXJsb2Fk 28777
IHNpbGljb25l 28778
0LXRgdGC0L3Qvg== 28779
IGRlbmtlbg== 28780
IGRlZmluaXQ= 28781
IFdhc24= 28782
IGFsdGVyZWQ= 28783
IFNvbw== 28784
IFdpbmc= 28785
aW5kcmU= 28786
IE5QQw== 28787
z4HOrQ== 28788
IFR3ZW50eQ== 28789
IExpZWJl 28790
IGhvbWVsZXNzbmVzcw== 28791
b3VsZGVy 28792
INCY0YLQsNC6 28793
0YHQutCw0Y8= 28794
IGN1YXRybw== 28795
IEhhcnZleQ== 28796
IHBoaWxhbg== 28797
IEJlZXQ= 28798
IHBvbGljaW5n 28799
IEFsZXhhbmQ= 28800
INC80L7Qu9C+0LQ= 28801
IG3DvHM= 28802
IGhpem8= 28803
67O064uk 28804
INC/0L7Qt9Cy0L7Quw== 28805
INC/0YvRgg== 28806
0L7Rh9C10LzRgw== 28807
IO2DnA== 28808
IGNyeXB0b2N1cnJlbmN5 28809
IGxvcm8= 28810
IHN1bW1hdGlvbg== 28811
IGJha2FsxLFt 28812
IG5ldXJvcw== 28813
2KU= 28814
INC80L7QttC10Lw= 28815
IMO8c3Q= 28816
IHByZWxpbWluYXJ5 28817
IGhvcm5z 28818
IFRJ 28819
2YPZhA== 28820
WU8= 28821
IGhpbmdl 28822
IHJlcGFpcnM= 28823
IGJvbmRpbmc= 28824
IGJpemU= 28825
INGI0YI= 28826
IG1vdGl2ZQ== 28827
IE5pZ2VyaWE= 28828
MTIw 28829
YmxvY2s= 28830
IGF2aWF0aW9u 28831
IEtvbW11bg== 28832
INC+0LrQsNC3 28833
IHRlbmhh 28834
IGVkdWNhdGluZw== 28835
IHN0YWF0 28836
5raI 28837
INGB0LrQvtC70YzQutC+ 28838
IGZyaWdodGVuZWQ= 28839
IHNlZWtz 28840
0YDRg9GI 28841
cXVlbnQ= 28842
IE5vdQ== 28843
IHByYXQ= 28844
IFNob3Q= 28845
V29yaw== 28846
a2FyYW5n 28847
IExpZ2h0bmluZw== 28848
bm9sZHM= 28849
cm9sbGVk 28850
Z2xhc3M= 28851
IGNyZWRpYmlsaXR5 28852
SVRZ 28853
IGF0bW9zcGhlcmlj 28854
IGhhdmlh 28855
w6RuZGVybg== 28856
Y2hlZXJz 28857
VGhlc2U= 28858
IENlbGw= 28859
IG1hZ25lcw== 28860
IEJyYXZv 28861
c2Vhc29u 28862
IMWfZXlsZXI= 28863
8J+O 28864
d2hpdGU= 28865
IE1C 28866
IHN0YWNrZWQ= 28867
IDc0 28868
INC00LDQstCw0Lk= 28869
IHBhdmU= 28870
INC+0YU= 28871
IGRhdGFzZXQ= 28872
IHJldG91cg== 28873
IG1hdHVyaXR5 28874
IHF1YXNl 28875
IDkz 28876
IFN5bQ== 28877
IGJyaWVmaW5n 28878
IGN1bHR1cmFsbHk= 28879
IOy3qA== 28880
aW5oYXM= 28881
IG1hZGFt 28882
IGFqdWRhcg== 28883
IFRpYmV0 28884
IGxlYWtz 28885
Y2lsZQ== 28886
IHRoZWF0ZXJz 28887
7Jio 28888
44OW 28889
NzI= 28890
IFdhc2g= 28891
IFF1YWxpdHk= 28892
IEl2YW4= 28893
IEJlbnQ= 28894
aWdhdG9y 28895
IEdlc2NoaWNodGU= 28896
IHJlYWN0aXZl 28897
IDE5MDA= 28898
5qGI 28899
IGNvbnRyYWRpY3Q= 28900
IHppZW1saWNo 28901
IGNvaG9ydA== 28902
4bun 28903
IHBlc3RpYw== 28904
IG9yYXo= 28905
IHRlbGxlbWVudA== 28906
6b4= 28907
IE5vd2FkYXlz 28908
Y3Jldw== 28909
U3RldmU= 28910
IGZpY3Rpb25hbA== 28911
IGlsaw== 28912
44GC44Gj 28913
IGdhc29saW5l 28914
emFt 28915
IHBhbmNha2U= 28916
w6huY2lh 28917
IG11aXRvcw== 28918
IGJ1cnk= 28919
IGtvcA== 28920
IElR 28921
IHJlc2VydmF0aW9u 28922
IFVwZGF0ZQ== 28923
IGplag== 28924
IEV5ZXM= 28925
5Y+R 28926
IHZpdmU= 28927
IGNoY2U= 28928
IEluaQ== 28929
cmVzcG9ucw== 28930
IHJlZmxlY3RpdmU= 28931
IFdhbg== 28932
0ZbQtw== 28933
IGVuY2E= 28934
IGVtYm9k 28935
IEJ1cmdlcg== 28936
IGFjYWRlbWlh 28937
IENpcmM= 28938
INC/0YDQtdC6 28939
IGFubGFt 28940
IHBoaWxhbnRocm9w 28941
IEJhxZ8= 28942
IEF1ZGk= 28943
IHZvc3Q= 28944
5L2g55+l6YGT 28945
IHJlcGVy 28946
UGV0ZXI= 28947
IGNvbnNvbGVz 28948
IHNjcnV0 28949
IFR1cm5lcg== 28950
INCx0YvQsg== 28951
SUlJ 28952
6Ki0 28953
IEZsaWdodA== 28954
4LiW 28955
IFJhdmVu 28956
IGNvcnJvcw== 28957
ZmVybg== 28958
IHByb3Zh 28959
IFNldg== 28960
IHJlY2lwcm8= 28961
IDE5ODU= 28962
IG51ZXZh 28963
IGRhYg== 28964
44CB44CM 28965
IG1leg== 28966
IFN0YXJr 28967
cHBpbmdz 28968
0L7RgdGC0Lg= 28969
7Kad 28970
IGZyYW1pbmc= 28971
INCg0LDQtw== 28972
IHBvc3Rw 28973
IFNoYW5ub24= 28974
INC60YPRgA== 28975
IGpha2J5 28976
aWVubmVudA== 28977
IE1hcHM= 28978
IFJldmVsYXRpb24= 28979
INGB0YLQsNC7 28980
7Jq0642w 28981
IGRldmFudA== 28982
IEdpdmluZw== 28983
IFdBUw== 28984
INC60L7Qs9C+ 28985
IHJlbWE= 28986
IFJD 28987
bsOt 28988
IHNsaXBwZWQ= 28989
IFJhbXM= 28990
IHdlZXQ= 28991
IG1hc2N1bGluZQ== 28992
IEVj 28993
IHJlb3A= 28994
IFBsYW50 28995
IE1BWQ== 28996
IHNwaWtlcw== 28997
IG5venpsZQ== 28998
IFdpa2lwZWRpYQ== 28999
IENvaA== 29000
SVNTQQ== 29001
Y2hsb3NzZW4= 29002
7KeA66W8 29003
IOuvuOs= 29004
IE5lZGVy 29005
Sm9zaA== 29006
INCg0L7RgdGB0LjQuA== 29007
IDE5ODc= 29008
IFRoZW9yeQ== 29009
ZWtr 29010
IHV0YW4= 29011
INC00L7QvNCw 29012
Y2h1 29013
INGB0LE= 29014
IGFwcm92ZQ== 29015
VkVO 29016
dWVwcmludA== 29017
IDg0 29018
5ryC5Lqu 29019
Q29y 29020
IHJpY2hlcg== 29021
IHNhbmR3aWNoZXM= 29022
YXRzdQ== 29023
0YjQuNGF 29024
IGxhdHQ= 29025
fn5+fg== 29026
ZnJpZW5kcw== 29027
IGRlcm5pw6hyZQ== 29028
IHN0ZXJlbw== 29029
INGN0LrRgdC/ 29030
IHByb3RlY3Rpb25z 29031
IGhhdXQ= 29032
RXZlcnlvbmU= 29033
IGVudGVycHJpc2Vz 29034
IE1vc3RseQ== 29035
IFNwb3RpZnk= 29036
IFNleA== 29037
IHVuZw== 29038
jOulvA== 29039
IGFjdGl2aXNt 29040
Y3RpY2E= 29041
b3JpZ2luYWw= 29042
INC/0YDQvtCz0YDQsNC8 29043
IGJyb2Njb2xp 29044
4KY= 29045
0L7Qs9GA0LDRhA== 29046
IHNla2FyYW5n 29047
IGNyYWZ0aW5n 29048
INCx0LDQvQ== 29049
44G744Gp 29050
IFJheg== 29051
IG5haXZl 29052
IHNjcm9sbGluZw== 29053
IG51bWVyaWNhbA== 29054
IHNjaGVkdWxpbmc= 29055
IGFwYXJ0bWVudHM= 29056
540= 29057
IHN0cmV0Y2hlcw== 29058
YWNleQ== 29059
IEhFUg== 29060
44K6 29061
IHppbmM= 29062
IGRhcm4= 29063
IGPDqWw= 29064
IHdhcmRyb2Jl 29065
IHJlZGlyZWN0 29066
IGp1bQ== 29067
IFN0cmFuZ2U= 29068
IG7DoG8= 29069
IGV4cGVyaW1lbnRpbmc= 29070
w6lyw6k= 29071
IHZvdWxleg== 29072
IGdlYmU= 29073
IEthbm4= 29074
IMSR4buZ 29075
IE1heGlt 29076
IEvDtm4= 29077
IEdsYXM= 29078
IHBvbGlzaGVk 29079
IG51bWE= 29080
SWNo 29081
IHJpdHVhbHM= 29082
IFNJ 29083
0LjRgtC10LvQuA== 29084
IGluZmlsdA== 29085
IHNjYXJm 29086
b3BoeQ== 29087
IHlpbmU= 29088
IGNpdmlj 29089
IE1lbmc= 29090
w6RuZ2U= 29091
1aU= 29092
aGlzdG9pcmU= 29093
IE9rZQ== 29094
IOyYhg== 29095
IHNvbGx0ZW4= 29096
IDgy 29097
6aas 29098
IHByZXNjcmliZWQ= 29099
IER1YmFp 29100
IEVsdGVybg== 29101
IG5hdGlvbndpZGU= 29102
IHNrYXRpbmc= 29103
aWFyeQ== 29104
IHJld2FyZGVk 29105
IG1vcmFsaXR5 29106
IE1hZ2dpZQ== 29107
IE9oaGg= 29108
IEZhaHJlbg== 29109
b2x2ZWQ= 29110
5pe25YCZ 29111
IGRldXhpw6htZQ== 29112
dGVjaG4= 29113
cm9sZQ== 29114
IGxlaWRlcg== 29115
IEpBWQ== 29116
INC40L3RhNC+0YDQvA== 29117
IGNhZmZl 29118
cmVpY2hlbg== 29119
IGthcnQ= 29120
IEN1dGU= 29121
ZmZlY3RpdmU= 29122
IGJ1bGx5 29123
YWdhcg== 29124
IGNvbW1vZGl0eQ== 29125
IG9icmln 29126
T1VS 29127
IHVucGxlYXNhbnQ= 29128
bm94 29129
SnVs 29130
b2xpdGg= 29131
0YLQvtGP0Yk= 29132
IEJlbGxh 29133
IGRvbGxz 29134
IEhvZmY= 29135
IGFkdmlzb3Jz 29136
IHRyYW5zZmVycw== 29137
IEdva3U= 29138
IDEyMDA= 29139
aW5ob3M= 29140
UGFs 29141
IOuYkQ== 29142
IHJlcHQ= 29143
IGFjY29tcGxpc2htZW50 29144
IHdlYXZl 29145
IG92ZXJzaWdodA== 29146
IHVuaGVhbHRoeQ== 29147
IGZpbHQ= 29148
IHB1ZGRpbmc= 29149
IE1pZ3VlbA== 29150
IGNodWNrbGVz 29151
5Y+w54Gj 29152
dmVyc2lvbg== 29153
IGNvbmZlc3Npb24= 29154
dmFsdWU= 29155
IHRyaXVtcGg= 29156
IHNhaXI= 29157
IOuFuA== 29158
IGFydGU= 29159
IE1hdGVyaWFs 29160
dXRp 29161
IGxpcXVvcg== 29162
IEJheWVybg== 29163
IE1haWw= 29164
IO2WpQ== 29165
0YHQutC+0Lw= 29166
IGNoZWFwZXN0 29167
INGH0LDRgdGC0Lg= 29168
IEpvYnM= 29169
IENhbnlvbg== 29170
aGFybWE= 29171
YWxleQ== 29172
YW5kcm8= 29173
IGFwcGVhcmFuY2Vz 29174
cHJvZg== 29175
INC+0Lc= 29176
bGFnZW4= 29177
IC8v 29178
INC70LjRiNGM 29179
IHJlY292ZXJpbmc= 29180
0LTQtg== 29181
cHN5 29182
44Oi 29183
IHN3aWZ0 29184
IFNwaW4= 29185
5biI 29186
IHNlaW5lbQ== 29187
IGRvbHBo 29188
ZsO8aHI= 29189
w6J0 29190
IGFsdGlqZA== 29191
IE1hcnR5 29192
IEhvY2g= 29193
IHByZWRhdG9ycw== 29194
IHZvcmhlcg== 29195
INCU0LDQstCw0Lk= 29196
IGZyYWdtZW50cw== 29197
IHBhc3RyeQ== 29198
IGNvbW1lbg== 29199
IFNhbmE= 29200
IOqxtOuNsA== 29201
dXNzZW4= 29202
IHRlbGE= 29203
IE5pbmE= 29204
bGVr 29205
IGNyaWVz 29206
IHRoaWdocw== 29207
IEZsZXg= 29208
IEJ1eno= 29209
44Q= 29210
VXM= 29211
IHBhc28= 29212
IGRlY2xpbmVk 29213
IE55 29214
YmFsYW5jZQ== 29215
IG1hc2E= 29216
IGpvcw== 29217
44Gq44KL 29218
INCh0L/QsNGB0LjQsdC+ 29219
YWNodQ== 29220
bG91ZA== 29221
IHBlbmE= 29222
IFdhbGQ= 29223
IGVsaW1pbmF0aW9u 29224
INCy0LXRgdGM 29225
b3JhZ2U= 29226
IG1pc3VuZGVyc3RhbmRpbmc= 29227
IGVuZG9yc2U= 29228
IG9nw7NsZQ== 29229
IGdyZWVk 29230
IGtsZWlu 29231
15zXlA== 29232
UkVZ 29233
IEVhdGluZw== 29234
IHNlbWluYXI= 29235
IEJpcnRoZGF5 29236
IHF1ZWxsZQ== 29237
IE11bHRp 29238
IHRpcmFy 29239
IHBlcmNo 29240
IGxhdm9y 29241
IEppYQ== 29242
IG11dGF0aW9ucw== 29243
IGNpZ2FyZXR0ZXM= 29244
2YjYrA== 29245
IGNvdXNpbnM= 29246
IGNhcHN1bGU= 29247
IGhvcnJpZmlj 29248
IHN0dXI= 29249
IHplaWd0 29250
bnV0cw== 29251
IG1lYW53aGlsZQ== 29252
IENvbGlu 29253
IGdvYmllcm5v 29254
IGd3 29255
IHVoaA== 29256
IEpFUg== 29257
c3BlY2lmaWM= 29258
IGFsbGVnYXRpb25z 29259
IOupiw== 29260
IEVsbGE= 29261
b29rZWQ= 29262
IEZpdA== 29263
YWZmbGU= 29264
IEFwcsOocw== 29265
IER1Y2s= 29266
IGNlbGx1bGFy 29267
Y8Ozdw== 29268
INGH0YPQstGB0YLQsg== 29269
Z2Vub21tZW4= 29270
7Iqk7Yq4 29271
IGxhaW4= 29272
aXNvbA== 29273
IGhvbGRlcnM= 29274
IGJvb3N0ZXI= 29275
IFNhc2hh 29276
0YvQstCw0LXRgg== 29277
gbw= 29278
IHNlcGFyYXRpbmc= 29279
IHJlaW5mb3JjZW1lbnQ= 29280
INC+0LTQvdC+0Lk= 29281
7JeG 29282
SURF 29283
IE9wdGlvbg== 29284
cGhvbg== 29285
IHBsYWlz 29286
IENhbWI= 29287
IO2ZmA== 29288
IHVuY29tbW9u 29289
Ijo= 29290
bWl5b3J1bQ== 29291
bW9p 29292
YWNqZQ== 29293
0LDQttGD 29294
1bY= 29295
IGdlbXM= 29296
w7xsZXI= 29297
b29scw== 29298
IGVuenltZXM= 29299
IGtpZG5hcHBlZA== 29300
IGtldGNodXA= 29301
dGFsaw== 29302
IHphY2g= 29303
IHdhc2hlcg== 29304
44CC44CC 29305
IEFyY2hpdGVjdA== 29306
dmVudWU= 29307
IFBsYW5uaW5n 29308
6YCB 29309
IFNhdmlvcg== 29310
INCz0YDRg9C/0L8= 29311
7Yq8 29312
YXJ5YQ== 29313
IHByb2Nlc28= 29314
IGxpbWJz 29315
IHJlYWxpemVz 29316
aWFuZGVy 29317
RlM= 29318
YWpp 29319
IHVuaXRl 29320
IOydmOs= 29321
IHBvc3PDrXZlbA== 29322
cmFpdHM= 29323
IEFncmU= 29324
24zaqQ== 29325
7ISc64+E 29326
5o6J 29327
INCy0LXQuw== 29328
INC80LXRgdGP 29329
YW5vcg== 29330
UGF0 29331
IGRlcm5pZXI= 29332
z4PPhM61 29333
INC60LDQutCw0Y8= 29334
IGzDpHNzdA== 29335
5o6w 29336
IE1laA== 29337
IG5naA== 29338
IGFtYXRldXI= 29339
6KuW 29340
RmU= 29341
IOq2gQ== 29342
IHNpdHVhY2nDs24= 29343
IHNlZGFu 29344
IGNsZWFuc2luZw== 29345
bGFzdGluZw== 29346
IGNvbW11bmlzdA== 29347
QU5F 29348
IGlycmVndWxhcg== 29349
IHNvdXQ= 29350
IENhcm5leQ== 29351
IGFsbGVtYWFs 29352
IG11Y2jDrXM= 29353
IGxpYnJv 29354
0K3RgtC+ 29355
INCw0L8= 29356
IGNvbnRpbnVhdGlvbg== 29357
IExvcg== 29358
PyIs 29359
cXVpbg== 29360
IGNoYXJhY3Rlcml6ZWQ= 29361
YWplcw== 29362
IHNpZ2h0cw== 29363
INGP0LfRiw== 29364
IFVoaA== 29365
6Lez 29366
YmlydGg= 29367
ZG9uZw== 29368
IGhhYmxhbmRv 29369
IHN5bXB0b20= 29370
57WC 29371
IGNhcGFjaXRvcg== 29372
IHRyYW5zcG9ydGVk 29373
IGlnbm9yYW50 29374
INC90LjQutC+0LPQtNCw 29375
IGRyaXA= 29376
IEV2YQ== 29377
IGFkamVjdA== 29378
IG1hc3NpdmVseQ== 29379
IEV0aGk= 29380
IENpcmNsZQ== 29381
IHJhaW5mYWxs 29382
IE1vdXNl 29383
IHJlZnVuZA== 29384
IFp3 29385
YXNzZW1i 29386
IDIyMA== 29387
IE9yZA== 29388
6KeS 29389
IHZlaW5z 29390
IEdpYW50 29391
IG3Do2U= 29392
IHZhcA== 29393
IG1pc3Nlcw== 29394
zr/Phc+C 29395
TW8= 29396
IEVudHdpY2s= 29397
SU5U 29398
2YbYqg== 29399
IHRoZW9yZXRpY2FsbHk= 29400
IHRlYXJpbmc= 29401
IHRyb3VibGVk 29402
cHJlbQ== 29403
IHJlcGV0aXRpdmU= 29404
IOKW 29405
IGhlYXZlbmx5 29406
IEFtYmVy 29407
INC/0L7Qu9C+0LY= 29408
IO2VtOyk 29409
IHZvd2Vs 29410
YW5raW5n 29411
IFdpcnRzY2hhZnQ= 29412
IGlycg== 29413
IGNvenk= 29414
IHVuZmFtaWxpYXI= 29415
IFBvcnM= 29416
IOunnuyVhA== 29417
IFRpbW90aHk= 29418
0YHQvtC70Y7Rgg== 29419
cGV4 29420
IFZJUw== 29421
KSg= 29422
IHN1cGVyc3Q= 29423
IGltcHJvdg== 29424
IEJlbmc= 29425
IGRpc2Nvbm5lY3RlZA== 29426
IGFwdA== 29427
0YDQtdC9 29428
IEV4dHJh 29429
INCx0LXQuw== 29430
c2hvcA== 29431
ZGluZ3M= 29432
IENvbm5lY3RpY3V0 29433
7LCs 29434
IEdD 29435
5Y+W 29436
YmVo 29437
SmVyZW15 29438
IEJhdHQ= 29439
44G4 29440
YXRoYQ== 29441
IFp1c2FtbWVu 29442
c2NyZWFtcw== 29443
IGdyYXM= 29444
YWZmdA== 29445
IEluaXRpYWxseQ== 29446
IEJyZXR0 29447
IHNwZWNpZmljYXRpb25z 29448
IHNlYXdlZWQ= 29449
IG9hdGg= 29450
IGZvdW50YWlu 29451
INC60L7RgtC+0YDQvtC5 29452
IFN0ZWlu 29453
6IGy 29454
IENvcmludGg= 29455
IGNvbmp1Zw== 29456
5bem5Y+z 29457
IGNvbXBlbnNhdGU= 29458
IOuKkOuCjOydtA== 29459
IG9uemU= 29460
IHNraW5jYXJl 29461
QnJpYW4= 29462
IHNlcnZpcg== 29463
fX0= 29464
IFZpaw== 29465
IHVuaW50 29466
IHN1cHBsaWVycw== 29467
IGJhbGNvbnk= 29468
IGVuZXJnaWE= 29469
b21ldHJpYw== 29470
0LfRjw== 29471
IHNpZ2g= 29472
IFRPTQ== 29473
IFB1cmU= 29474
eXR0 29475
0YvRgQ== 29476
IFJhaW5ib3c= 29477
IFBpdHRz 29478
15nXng== 29479
IHN0YXR1ZXM= 29480
aGVhZHM= 29481
IGNvdXBsZWQ= 29482
6Yyi 29483
IGhlcmQ= 29484
5L2T 29485
IGV4Y2x1ZGVk 29486
IGdpbHQ= 29487
INGO 29488
IHN3b2pl 29489
IFN2ZXI= 29490
NjM= 29491
aXNzYW50 29492
IGTDvHJmZW4= 29493
oIjr 29494
IGtpc3Npbmc= 29495
b29m 29496
5Lul5LiK 29497
IGN1cnNlZA== 29498
IHNob3dlcnM= 29499
IHN3aW5naW5n 29500
IHJlcHJvZHVjZQ== 29501
44Go44GE44GG44GT44Go 29502
IHPDpHR0 29503
ZWxjb21l 29504
IGZ1bmRhbWVudGFscw== 29505
IGFsbW9uZA== 29506
IHDDqQ== 29507
IHdlbGxiZWluZw== 29508
IGh1bnRlcnM= 29509
5b6A 29510
U2Vj 29511
k5zrprQ= 29512
IGVtaXNzaW9u 29513
IHBzeWNob2xvZ2lzdA== 29514
IGJldHJheWVk 29515
IFJleW5vbGRz 29516
TEVT 29517
IHBvbGxpbmc= 29518
IG5lZ2F0aXZlbHk= 29519
IGNvbWJpbmVz 29520
15zXkA== 29521
0LDRgNCw 29522
zrvOu86s 29523
IFR1cm5z 29524
T1RU 29525
INeU15k= 29526
YWlzb24= 29527
IGFpcmxpbmU= 29528
IHJlc3RyaWN0aW9u 29529
d2Fs 29530
IGF1cmFpdA== 29531
IExlYmFub24= 29532
IE1PUg== 29533
IG1vbmtleXM= 29534
w6luZXI= 29535
0ZbRlw== 29536
IG1vdGhlcmY= 29537
INmH2LDZhw== 29538
IGZldQ== 29539
w7xocmVu 29540
IGh5Z2llbmU= 29541
ZW50ZWVu 29542
RGVz 29543
IGRpc3NpcA== 29544
RXN0 29545
IHNhaW50cw== 29546
IHBvdGFzc2l1bQ== 29547
IHJlY2tvbg== 29548
Q2xpbnR1cw== 29549
IG1hbmlmZXN0YXRpb24= 29550
IEFwcHJv 29551
IEluc3BlY3Q= 29552
IHZlbnRpbGF0aW9u 29553
IGhlbG0= 29554
IGthcmE= 29555
4Liy4LiZ 29556
IGZhdm9yYWJsZQ== 29557
IOyViuyVmA== 29558
IEhpc3Bhbmlj 29559
4Lic 29560
INeU15s= 29561
IHZhbGlkYXRl 29562
IFJlc2lkZW50 29563
IGNvbWVueg== 29564
YmVpdGVy 29565
ZXJlcg== 29566
5LiA6LW3 29567
IGRhZG8= 29568
YXRjaGluZw== 29569
bWV0cm9z 29570
IEhpbg== 29571
IER1bQ== 29572
IGhhesSxcg== 29573
IE5hdGFsaWU= 29574
IGVuY3J5cHRpb24= 29575
0L7Rh9C60LA= 29576
bW1h 29577
aG91c2Vz 29578
IGFuYWx5dGljYWw= 29579
IERhbmc= 29580
Zmlyc3Q= 29581
5q2M 29582
57qM 29583
IEVuYw== 29584
Y2FuZG8= 29585
IGx1ZHpp 29586
d2FydA== 29587
IHN0YXRpc3RpYw== 29588
IOyCsA== 29589
IGNvbW1lbnRpbmc= 29590
IGNvb3JkaW5hdGVk 29591
IEh5cGVy 29592
5Zo= 29593
IEJlcnQ= 29594
55y+ 29595
IEhpcA== 29596
a2Vt 29597
w7xuw7w= 29598
IHphbA== 29599
IO2VmOuKlOuNsA== 29600
IFJvYm90 29601
6Zax 29602
cmF3bg== 29603
IHJoZXRvcmlj 29604
dWxsYWg= 29605
IERpZXQ= 29606
IHRha2ljaA== 29607
IHBvc3Nlc3NlZA== 29608
k5zripQ= 29609
IHdha2Vz 29610
IFJhZg== 29611
TWFydA== 29612
IGVjYw== 29613
IEZN 29614
IGRpZmlj 29615
IEFsbGV6 29616
IGN1cmVk 29617
5a2m 29618
IFF1YWQ= 29619
IGJlbGU= 29620
IGpvdXJuYWxz 29621
IHRhZA== 29622
IHNvY2lhbGVz 29623
5oeC 29624
IHdoYXRz 29625
IEJhc3M= 29626
IGplc3RlbQ== 29627
IFNhZGx5 29628
IFNvdXJjZQ== 29629
IMO8w6c= 29630
YWx0dW5n 29631
aWVydGVu 29632
IGp1bGxpZQ== 29633
aWZh 29634
INCa0L7RgA== 29635
IERvb3I= 29636
INCd0LDQtA== 29637
INC30LTQvtGA0L7Qsg== 29638
IHJ1bW9y 29639
IHBpZXM= 29640
INC/0LXRgNC1 29641
INC+0YLQsg== 29642
0LXQvdC90YvQtQ== 29643
SG9zdA== 29644
IFNvcGhpZQ== 29645
YW50ZW4= 29646
QW55 29647
IEF1Zmc= 29648
56iL 29649
IEhEUg== 29650
IFJvY2tldA== 29651
cmVzc28= 29652
IHZlcmRl 29653
IHByw6lzaWRlbnQ= 29654
IGluZG9vcnM= 29655
IHN0YWdnZXI= 29656
IHN0YXRv 29657
IERpYWw= 29658
IGJ1enppbmc= 29659
ZW1lcg== 29660
INCS0YHRkQ== 29661
INC00LXRgNC10LI= 29662
IHBvdXY= 29663
IHN0cmFuZHM= 29664
IOqyg+ydtA== 29665
IFBhcmw= 29666
0L7QutC+0Lk= 29667
IHNpcA== 29668
ICgq 29669
w6RuZ3Q= 29670
IGRlYmVy 29671
IEFpbg== 29672
IGRyYXN0aWNhbGx5 29673
IFNsb3dseQ== 29674
IEJyaWc= 29675
IFRvcmFo 29676
IGFjaGU= 29677
ID8/Pw== 29678
IERvYg== 29679
a2FubnQ= 29680
TWFyeQ== 29681
IHN0YW0= 29682
IERlbW9u 29683
cGxh 29684
IEZyZXVuZA== 29685
IEJlbm4= 29686
IGhpZ2hz 29687
INqp2LE= 29688
IFByZXBhcmU= 29689
IHByb3h5 29690
IGNhbXBv 29691
IEF1Z2Vu 29692
o6jr 29693
IENobG9l 29694
aWN1bGFybHk= 29695
eW91bmc= 29696
IOOBjA== 29697
qZTr 29698
IHNjcmF0Y2hpbmc= 29699
IGdsYWM= 29700
IGdlbWVpbnNhbQ== 29701
YW5hbA== 29702
YWNha3PEsW4= 29703
IEZvcnVt 29704
ZW5uaWFs 29705
IFJlc291cmNlcw== 29706
44Go5oCd44GE44G+44GZ 29707
IG1laXN0ZW4= 29708
IEZlbGw= 29709
IHVuYW5pbQ== 29710
IFRC 29711
IFNlbGJzdA== 29712
5oY= 29713
IGludGltaWRhdGluZw== 29714
IEdlZsO8aGw= 29715
IOy9lOuhnA== 29716
5ouJ 29717
aWRvcg== 29718
aWNpb25lcw== 29719
YXJzYQ== 29720
XS4u 29721
YXpv 29722
IGtlbmRp 29723
IFRhZ2U= 29724
dGVybWlu 29725
IFByb3plbnQ= 29726
TWF5YmU= 29727
bMOp 29728
IHF1ZXN0aQ== 29729
IG1lbWVz 29730
IGNvcnJl 29731
IFZJUA== 29732
IEdhbGxlcnk= 29733
IHVyZ2VuY3k= 29734
IG5vY2hl 29735
IGtpbmRseQ== 29736
IE1lcmVkaXRo 29737
IHbhuq15 29738
INin2YTYqA== 29739
IEVzdGFkbw== 29740
5Ye65L6G 29741
enVn 29742
b3F1ZQ== 29743
IG9iZXNpdHk= 29744
T2Zm 29745
IEV1cm9wZWFucw== 29746
w7Zk 29747
7Lm06w== 29748
IGhvb3A= 29749
IGVuam95cw== 29750
IENoaXA= 29751
cGF0aWVudA== 29752
IG1pY3Jvc2NvcGU= 29753
IGxlZ2l0aW0= 29754
INGP0LLQu9GP0LXRgtGB0Y8= 29755
z4POuQ== 29756
YXJnZW50 29757
IHNoYW0= 29758
IGxpY2Vuc2luZw== 29759
b2xpYQ== 29760
U29ycnk= 29761
cmFtYQ== 29762
IGFjY2VsZXJhdGVk 29763
IHd5bQ== 29764
IGZhaXJuZXNz 29765
IFJlYWRpbmc= 29766
IHNsYWNr 29767
IERvaw== 29768
emnEmWt1asSZ 29769
IHJ1YmJpbmc= 29770
0LDRgtGD 29771
IGFsbG9jYXRlZA== 29772
anVuZw== 29773
IHBhaW5z 29774
IHdpbmRpbmc= 29775
IGdlbGl5b3I= 29776
IENV 29777
bW90 29778
Y29jaw== 29779
IFBvc2l0aW9u 29780
YnJvcw== 29781
IGxpdmVzdHJlYW0= 29782
IEJyYWlu 29783
7LCp 29784
IHByemVr 29785
IEVp 29786
IENvY28= 29787
0LHQsA== 29788
IHNob3ZlbA== 29789
44OP44OP 29790
ZWE= 29791
IGNob2NvbA== 29792
IHJlYmVsbGlvbg== 29793
IHNob3dj 29794
IEhhbG8= 29795
IGRpdmlkZW5k 29796
bWlzc2lvbg== 29797
IHVzYW5kbw== 29798
IFsi 29799
IGZhbGVp 29800
5pu4 29801
QmxhY2s= 29802
IFN1cmVseQ== 29803
IMW7 29804
IHBoaWxvc29waGVy 29805
5L2g5Lus 29806
IG92ZXJoZQ== 29807
IEJvcm4= 29808
IG9iamV0aXZv 29809
IDEyOA== 29810
c2NoZWlk 29811
IE5hemlz 29812
IHNvbGNoZQ== 29813
bGlmdA== 29814
Y2VkZQ== 29815
YWRvcnM= 29816
IG1hcnNobQ== 29817
IExPUkQ= 29818
lOydtO2BrA== 29819
IG93bmluZw== 29820
Q29udA== 29821
IGxhbmRzY2FwZXM= 29822
IGxlbmRpbmc= 29823
IEF1dGhvcml0eQ== 29824
0L7QstC+0Lk= 29825
b3F1 29826
IFNlcw== 29827
IEZlcnJhcmk= 29828
IHJlc3BvbnNhYmls 29829
IHbDoXJpb3M= 29830
IGRlbGlj 29831
IGVtYmFyaw== 29832
IGVtYnJvaWRlcg== 29833
IGZyYW1ld29ya3M= 29834
IHNpbW1lcg== 29835
IG5hY2lvbmFs 29836
IHJlbWFpbmRlcg== 29837
IFZpZWxsZWljaHQ= 29838
IHF1aWVyZXM= 29839
7JeU 29840
IHRlc3Rvc3Rlcg== 29841
aWhlbg== 29842
IE96 29843
w6hsZQ== 29844
IHBvcnRyYXllZA== 29845
zrrOtQ== 29846
IFBvbGl0aWs= 29847
IGFwZXJ0dXJl 29848
IGJsYW5k 29849
aW5kdXN0 29850
INC+0LHRgNCw0YI= 29851
IFRob3Vz 29852
QmF5 29853
IGRhbmRv 29854
IHNoZXI= 29855
IGFkbWlzc2lvbnM= 29856
IENyZXc= 29857
INGW0L0= 29858
U0lOR0lORw== 29859
IG91bmNl 29860
IGl5 29861
IGJhc2ls 29862
IG92ZXJ0aW1l 29863
IHRocmVhdGVu 29864
IHBhcnRuZXJlZA== 29865
IENhbm4= 29866
YXZhbmE= 29867
INC30L3QsNC10YLQtQ== 29868
6YCZ5Lqb 29869
INC+0YLRgQ== 29870
IFR1ZG8= 29871
7L2U 29872
IOuGgOs= 29873
ZmVs 29874
IHJlYXJy 29875
IGlud2FyZA== 29876
IFJvZ2Vycw== 29877
4LmD4Lir 29878
IHR3ZWFr 29879
IGRyeWVy 29880
Y2Vzc2lvbg== 29881
IHJpZ29yb3Vz 29882
IERhYXI= 29883
b21pY3M= 29884
IGZhdHM= 29885
dmFk 29886
IHppcHBlcg== 29887
YWNjZXB0YWJsZQ== 29888
IGRlbW9uc3RyYXRpbmc= 29889
IFl1bQ== 29890
IGJlYXU= 29891
IHJvc3Rlcg== 29892
IHByZWRvbWluYW50bHk= 29893
0LXRgNGD 29894
bmluZ2Fy 29895
IHRyaWFuZ2xlcw== 29896
IHRleHRpbmc= 29897
IGJlcnJpZXM= 29898
IOyCrOynhA== 29899
6ZSZ 29900
YWRkZXI= 29901
IGZhaXRlcw== 29902
IEltYWdl 29903
bGVyZQ== 29904
IGJvdW5kcw== 29905
IExhdXI= 29906
IOyVhOustOs= 29907
IG1pbw== 29908
IHVzYQ== 29909
INiw 29910
IHRvZW4= 29911
IEphbmc= 29912
xb5l 29913
Y2hvZA== 29914
YW5hbg== 29915
INC+0LHRgNCw0LfQvtC8 29916
IHBlcnNldmVy 29917
IFN3ZQ== 29918
IGF1Z21lbnQ= 29919
5LiD 29920
dWdnbGluZw== 29921
acOocmVtZW50 29922
aXN0bGVz 29923
YWNqxJk= 29924
OTE= 29925
IG1haA== 29926
IEtJUg== 29927
RGll 29928
IGRvd25oaWxs 29929
IDE5Njg= 29930
0L7RgNC+0YjQvg== 29931
5bm5 29932
b2dyYXBoaWNz 29933
IHTDpHNzw6Q= 29934
6rKg7KOg 29935
INC70LjRhw== 29936
QVVESU8= 29937
INC/0LvQvtGF 29938
IHByb3Bvc2luZw== 29939
6aC7 29940
IHRlbXB0ZWQ= 29941
IGNvbnZlcnRpbmc= 29942
IExlaHI= 29943
IHBlcnNvbmU= 29944
IEZlZWxpbmc= 29945
7Ja07KO8 29946
b21icmVz 29947
INec15k= 29948
IGd1cnU= 29949
IGRlbWVudA== 29950
0L3QuNC3 29951
0LjRgtC10LvQtdC5 29952
IGNvbXBhw7E= 29953
5pyq 29954
5biM5pyb 29955
IHJlZG8= 29956
IGNvbmR1Y3Rvcg== 29957
bWlh 29958
IGlkb2xz 29959
IE11bA== 29960
IGluZXg= 29961
IHTDpG3DpA== 29962
IGltcGFjdGluZw== 29963
IGRheWxpZ2h0 29964
Z2ls 29965
IGhlbGZlbg== 29966
IGVudHNwcmVjaA== 29967
IHdpxJlrcw== 29968
IHNjcmlwdHVyZXM= 29969
IGRpc21pc3NlZA== 29970
44Oz44OI 29971
IFBvZGNhc3Q= 29972
2YXYsQ== 29973
IGFubnVhbGx5 29974
IHVzYWJsZQ== 29975
IGxpYnJl 29976
0L7Qt9C8 29977
IHJ1YmJpc2g= 29978
55qE5Lq6 29979
IGNvbnRpbnVhcg== 29980
IGh1bWlsaQ== 29981
IHNwZWVjaGVz 29982
0YDQsNGH 29983
YmFyZA== 29984
NzE= 29985
Pjw= 29986
b2xvZ8OtYQ== 29987
d2VhbHRo 29988
IG1lZGl0YXRl 29989
k6TsnZg= 29990
IENyYWZ0 29991
6KeJ5b6X 29992
5pmu 29993
cml2 29994
IEFnYWluc3Q= 29995
IGNlcmFtaWM= 29996
ZXNww6hyZQ== 29997
IGNvbXBldGVudA== 29998
IEhvcGtpbnM= 29999
IGtpbG9z 30000
IGdyYXZlbA== 30001
IHBpc3Rvbg== 30002
IGZyaWVuZHNoaXBz 30003
IGVzY3Jl 30004
IHZveg== 30005
IEdlc2VsbHNjaGFmdA== 30006
IHVudGVyc3TDvHQ= 30007
IG11ag== 30008
IHdhcm5pbmdz 30009
cG9z 30010
IFByb2Zlc3Npb25hbA== 30011
d3N6eQ== 30012
b2RsZQ== 30013
YmFuZHM= 30014
IHRlYW13b3Jr 30015
c3RlbGx1bmc= 30016
IGR4 30017
5Y2K 30018
IGF0dG9ybmV5cw== 30019
IHdlaXRlcmU= 30020
44WL44WL44WL 30021
IE9yaWdpbmFs 30022
15nXlw== 30023
IGJyb2FkY2FzdGluZw== 30024
INC/0LXRgNCy0YvQuQ== 30025
dWNoaQ== 30026
IGhldXJl 30027
IGdyYWJz 30028
IFdPUg== 30029
IFBsYWlk 30030
TWlu 30031
IHBheg== 30032
IFB1aXM= 30033
dW11 30034
aXRhdGVz 30035
IGNvYXRz 30036
IGJ1ZW4= 30037
IGhlaXI= 30038
IHBuZXVt 30039
16nXqA== 30040
ZW5zZXI= 30041
IEpVREdF 30042
IGJsb25kZQ== 30043
4bmb 30044
IGdhaw== 30045
IHPEsWs= 30046
IHF1b3RlZA== 30047
IGVxdWlwbw== 30048
IHdpc2hpbmc= 30049
w61jaWE= 30050
IHZlcmJz 30051
57WE 30052
IENhbmFkaWFucw== 30053
IGdvdmVybmluZw== 30054
IEV2YW5z 30055
RXVybw== 30056
IGdlbnJlcw== 30057
IHVudGVyc2NoaWVk 30058
IEJlY2t5 30059
s7zqsozsmpQ= 30060
IGVpbmdl 30061
IFJhaXNl 30062
b2xhbmQ= 30063
IFN0cmF0ZWc= 30064
IGVyZXM= 30065
IFZldGVyYW5z 30066
IGJyZWFrb3V0 30067
IHNhbnTDqQ== 30068
IGFkZWw= 30069
IGludmVzdGlnYXRlZA== 30070
IHBldXI= 30071
IGFnaWxl 30072
IHJhaWxyb2Fk 30073
YW5za2E= 30074
INC10Lk= 30075
IGV4cG9z 30076
YXRvcmllcw== 30077
IENvbnRlbnQ= 30078
IHRydXRocw== 30079
IFRyYWls 30080
IGd1YQ== 30081
IHBvcmVz 30082
IHdyaXRpbmdz 30083
IFVocg== 30084
IFRoYXRz 30085
IGljaW5n 30086
T0M= 30087
IFByb2R1Y3Rpb24= 30088
IGNhcm5l 30089
SVNT 30090
IG5pbmd1w6lt 30091
bm9u 30092
IHZpY2lvdXM= 30093
15XXlA== 30094
IHJlY29ubmVjdA== 30095
IGNlbnRyZXM= 30096
IEtlbQ== 30097
IGNyZWFzZQ== 30098
IOydtOuvuA== 30099
0LDQudGC0LXRgdGM 30100
INCx0L7RgA== 30101
IEhhecSxcg== 30102
INGB0YPQtA== 30103
IMO6bmljYQ== 30104
b3dhxYI= 30105
IGFkaGVy 30106
aHVh 30107
Wlo= 30108
IHByZWNpc28= 30109
IGN1cnJlbnRz 30110
IHNlYXNvbmVk 30111
IElvVA== 30112
IEJpc2hvcA== 30113
6KiI 30114
c3RlZA== 30115
IEJlcm5hcmQ= 30116
7KSY 30117
5rK7 30118
IEdsZW5u 30119
IGt0w7NyeW0= 30120
4Li34LmI 30121
IGFzdHJvbG9n 30122
IEtvdA== 30123
5aSc 30124
IHBhcmZvaXM= 30125
IGZvcndhcmRz 30126
IFdpxJk= 30127
IM6Y 30128
IG5hbm8= 30129
6LuN 30130
c3Vi 30131
IEJyaWxs 30132
IGdyaXQ= 30133
IGNpdGVk 30134
Z2Fkbw== 30135
IG1lbHRz 30136
IGZvcmPDqQ== 30137
4paI4paI 30138
IGJham8= 30139
IGRpc2NyZXRpb24= 30140
sLA= 30141
YXRpdml0eQ== 30142
IHNpdHVhdGVk 30143
44Or44Kv 30144
0YnQtdC1 30145
5Zyw5pa5 30146
INC/0YDQuNC90YbQuNC/ 30147
YW1heg== 30148
IGFxdWFyaXVt 30149
IGRpc3NvbHZl 30150
IEdvZHM= 30151
U3VwZXI= 30152
IGFtaWQ= 30153
ems= 30154
IOOBhA== 30155
6aCQ 30156
YW1wZg== 30157
IGhlbGE= 30158
JyE= 30159
IGRldmVsb3BtZW50YWw= 30160
IERpc2U= 30161
INGA0LDQsdC+0YLQsNC10YI= 30162
IHNuYXBzaG90 30163
5aW95aW9 30164
1bg= 30165
IFl1ZQ== 30166
IEh1bGs= 30167
IERvb20= 30168
IEZlbGl4 30169
IHLDqWY= 30170
TWFsZQ== 30171
57eK 30172
cGhhbnRz 30173
RU5T 30174
IE1lY2hhbg== 30175
IEdvbGY= 30176
5YaN6KaL 30177
IGdlbmVyb3NpdHk= 30178
w6R0emU= 30179
IHVubG9ja2Vk 30180
IOOCkg== 30181
7YOB 30182
b2NhbHlwc2U= 30183
QWxyaWdodA== 30184
IOqwnOs= 30185
INeQ15HXnA== 30186
IEtlZXBpbmc= 30187
IGNvbGxhYm9yYXRpbmc= 30188
Y2hpZWY= 30189
IEZlcm5hbmRv 30190
IGNoZWZz 30191
IO2UvOu2gA== 30192
IHNraXBwZWQ= 30193
IHBlcnNvbm4= 30194
IGF4ZQ== 30195
Y2hleg== 30196
IGV4dHJhY3Rpb24= 30197
IEFW 30198
IEdpYmJz 30199
IO2c 30200
IHPEsQ== 30201
SUFN 30202
Vmlldw== 30203
IEdSQU5U 30204
IOuquA== 30205
IHZlcmlmaWNhdGlvbg== 30206
IGRlcGljdGVk 30207
IE1veg== 30208
b3V4 30209
IHR1bA== 30210
IHNjYW5uZXI= 30211
IGNvbWVkaWFu 30212
IFZvbGtz 30213
IEpFRkY= 30214
6KiC6Zax 30215
p4Q= 30216
IGRpc3RyYWN0aW9u 30217
csOh 30218
IElOVEVS 30219
IHNpbmNlcg== 30220
INee16o= 30221
INep16A= 30222
IGNvbnN0cnVjdGl2ZQ== 30223
YXJm 30224
IOuIhOs= 30225
IGVjbw== 30226
cmFtb3M= 30227
IHJlbmV3ZWQ= 30228
aW5lbWVudA== 30229
IFVi 30230
IFBlcHBlcg== 30231
7KeA6rCA 30232
IERhcndpbg== 30233
IG1lcmNoYW5k 30234
IHbDoXJpYXM= 30235
w6hjZQ== 30236
Tkc= 30237
IOychO2VtOyEnA== 30238
INCw0LrRgtC40LI= 30239
IFVudGVycw== 30240
2LnZhA== 30241
IGludHJpYw== 30242
b21tYQ== 30243
aWV2aW5n 30244
IENhcm9saW5l 30245
5ZOB 30246
IFBSRVM= 30247
IHBlcmZvcm1lcg== 30248
IGF1dG91cg== 30249
44G+44Gb44KT 30250
IHV0dGVybHk= 30251
IHN5bnRoZXNpcw== 30252
IGxlc2JpYW4= 30253
IHJldHJpZXZl 30254
IG1hbmVpcmE= 30255
IGltcGFpcg== 30256
IG1lbnRvcmluZw== 30257
IFNvdWxz 30258
IEdvUHJv 30259
0YDQsNGC0Yw= 30260
IGNvc2U= 30261
IFNTRA== 30262
SVJF 30263
IHVwZnJvbnQ= 30264
IEF1bg== 30265
IGdhbWVy 30266
IGxpdHQ= 30267
IGFnZ3Jlc3Npb24= 30268
IExpa2V3aXNl 30269
IEJldHR5 30270
IERhcnQ= 30271
IERMQw== 30272
aXNobWVudA== 30273
7J6l7J2E 30274
IOWvuQ== 30275
57uP 30276
Y3JlYW0= 30277
IEJhYnlsb24= 30278
IG51Zw== 30279
YnJhcg== 30280
IGF5bsSx 30281
YW1pbHk= 30282
YmlrZQ== 30283
YWhhaGFoYQ== 30284
bG95ZA== 30285
IG1pcmE= 30286
IHBlcm1l 30287
IEdhbWluZw== 30288
IGZpcm13YXJl 30289
TWE= 30290
IGFzc2lzdGVk 30291
YXRpY3M= 30292
IOyVnuycvOuhnA== 30293
IE1lbnRhbA== 30294
bmllanM= 30295
IEl6 30296
b3fEhQ== 30297
IHRvdWdoZXI= 30298
IGRlZWQ= 30299
6Ium 30300
IHN0eWxpc2g= 30301
IFRvb2xz 30302
IEhhbXA= 30303
IHN1bnNjcmVlbg== 30304
IGFydGljdWxhdGU= 30305
aXll 30306
0LjRhA== 30307
IFNwcmVhZA== 30308
IEhBVkU= 30309
IHN3aXJs 30310
IHNwb25zb3Jpbmc= 30311
5LuL 30312
aW92YXNjdWxhcg== 30313
bWVzaQ== 30314
IHJlbGF4YXRpb24= 30315
INGB0LLQvtC40YU= 30316
IG1hcmdpbnM= 30317
IHNhxJ8= 30318
IFByaWRl 30319
IM+Ezr/Phc+C 30320
0LjRhtC4 30321
ZW5jaQ== 30322
RG9lcw== 30323
IGNvcnBzZQ== 30324
IGVuZHVyYW5jZQ== 30325
IO2emA== 30326
7Lm0 30327
IGhhaXJjdXQ= 30328
IGludGVycnVwdGVk 30329
IHdpbmR5 30330
IENhbGVi 30331
z4HPhw== 30332
IFBvdXJxdW9p 30333
IGhvbGlzdGlj 30334
dWNsZWFy 30335
IFdob2xl 30336
5aOr 30337
QWN0 30338
IGdhbGxvbg== 30339
Y2FkZQ== 30340
IFJlZ2lvbmFs 30341
cm9hZHM= 30342
IFNjaG5l 30343
w6FuZw== 30344
INC40LfQvNC10L0= 30345
44KI44Gt 30346
IG1lbnVz 30347
IHNwbGl0dGluZw== 30348
IHByaWNlZA== 30349
IM6T 30350
IHVzZXJuYW1l 30351
INCe0Yc= 30352
IGNvbXByZXNzZWQ= 30353
eWlu 30354
IGd1YXJkaWFu 30355
IGdvb2Y= 30356
IGNoZWNrbGlzdA== 30357
IGludGVyY2hhbmdl 30358
IGV4cGVkaXRpb24= 30359
IGV4dGVybg== 30360
IGluZnJhcmVk 30361
ZW5nbw== 30362
IGRlbnlpbmc= 30363
IHBhY2tldHM= 30364
b25lbnQ= 30365
QkI= 30366
IEluY3Jl 30367
IHNpbmk= 30368
w59lcg== 30369
w6hn 30370
bWFhbA== 30371
Z2VuZXJhdGlvbg== 30372
IG1pbm9yaXRpZXM= 30373
IGxsZXZhcg== 30374
IG5vbWluYXRpb24= 30375
IGNvbnNpZA== 30376
INec16I= 30377
bXXFnw== 30378
IEVzYw== 30379
IG51bWVyYXRvcg== 30380
IGthaWs= 30381
IGt0w7NyeWNo 30382
aWVzZW4= 30383
IHbDqg== 30384
IFVTUw== 30385
IFByaXZhdGU= 30386
INC+0LTQvdC+ 30387
IGFsw6lt 30388
w610dWxv 30389
IGxpbWI= 30390
IGZvcmdpdmVu 30391
IGRpc2Nsb3N1cmU= 30392
z4TOrw== 30393
IG5pbmfDum4= 30394
IHRoZXJhcGV1dGlj 30395
IG5lZ290aWF0aW5n 30396
IE5pa2U= 30397
ZW5zZWZ1bA== 30398
IGluY2Fw 30399
IGZsYWdzaGlw 30400
dG93bg== 30401
4og= 30402
IM+Azr/Ouw== 30403
IHdvbHZlcw== 30404
IHZpb2xhdGlvbnM= 30405
IEFybm9sZA== 30406
IGludGVydmVuZQ== 30407
IGhlYXRlcg== 30408
IHJlY3Vyc29z 30409
IG1haWQ= 30410
6rK8 30411
INC00LDQstCw0LnRgtC1 30412
IENlbGVicg== 30413
IGNhcGU= 30414
IFN0eQ== 30415
YWluZW4= 30416
c2l0ZQ== 30417
Ymlq 30418
INC/0L7Qu9GM0Lc= 30419
IGZyYW1lZA== 30420
IHB1Ymxpc2hlcnM= 30421
INGH0YPRgtGM 30422
IHRlbXB0YXRpb24= 30423
IGNlcnRlemE= 30424
IGV4ZW1wdA== 30425
7Iq5 30426
c2VsbGluZw== 30427
IFRhc2s= 30428
aG9vbg== 30429
IENvYw== 30430
IFBhcmtz 30431
IHJlcGV0aXRpb24= 30432
INGC0YPQtNCw 30433
IGVuc2w= 30434
IGRlxJ9pxZ8= 30435
IE9ybGFuZG8= 30436
IE1haW50ZW4= 30437
5q2i 30438
b2N1bWVudA== 30439
IEhD 30440
IHNjb290ZXI= 30441
INC90LDQv9C40YE= 30442
IHRpZ2h0ZXI= 30443
IHRlYXNl 30444
IHJlbW92ZXM= 30445
IGtpamtlbg== 30446
INGB0YPRidC10YHRgtCy 30447
IHRow6k= 30448
INCy0YvQs9C70Y/QtA== 30449
IHJlbGlldmU= 30450
IG1pdMOk 30451
IHN0YXRpb25hcnk= 30452
w7ZmZg== 30453
cGFibGU= 30454
IGFydGVy 30455
IGTDqWY= 30456
cmF0aXZl 30457
IGNvbmVjdA== 30458
IHNhZGRsZQ== 30459
IERpYW5l 30460
IGNvbW1lbW9y 30461
ZmVuZGlt 30462
U8Ot 30463
IO2BtOs= 30464
IG1hbmdl 30465
YXR0ZQ== 30466
IGFycm9nYW50 30467
IHJvYm90aWM= 30468
IGdpw6A= 30469
5piv55qE 30470
IG5laWdoYm91cmhvb2Q= 30471
aXNzb24= 30472
INC00LLQuNC2 30473
IFJJ 30474
IE5vcm1hbg== 30475
YnJhbmQ= 30476
YW1hdGlvbg== 30477
IHJhem9y 30478
IG11cmRlcnM= 30479
INGC0YM= 30480
IHdzenlzdGtpbQ== 30481
IHV0aWxpdGllcw== 30482
IG1pY3Jvc2NvcA== 30483
6r8= 30484
IGRhcXVp 30485
b2xsYXI= 30486
INCU0LDQstCw0LnRgtC1 30487
IGFubsOpZQ== 30488
IGtpbG9tZXRyZXM= 30489
IGhvbW9zZXh1YWw= 30490
IGFyY2hpdGVjdHM= 30491
44Gh44Gv 30492
IG5peWU= 30493
TEVS 30494
IG1pY3JvcGhvbmVz 30495
IFN0dW5kZW4= 30496
IGNvbnNlY3V0aXZl 30497
aWVuZGE= 30498
dsOkbmQ= 30499
REVS 30500
IGxpZnRz 30501
IE1lYXQ= 30502
IHNhdmV6 30503
7ZaI642Y 30504
TWVu 30505
IGRpc21hbnQ= 30506
6rGw66W8 30507
IGluc3VsYXRpb24= 30508
IHNjYWxs 30509
IHNwb29reQ== 30510
IHBhcmM= 30511
IGJhbGxldA== 30512
IFdoYXRzQXBw 30513
IGZyYW5j 30514
IGRlbGliZXJhdGU= 30515
IO2FjA== 30516
IG1hcnM= 30517
IFp1cg== 30518
UHI= 30519
ZGlzY2lwbGluYXJ5 30520
IG9ic2Vzc2lvbg== 30521
0LzQtQ== 30522
IG1hcmNoaW5n 30523
IEVtZXJnZW5jeQ== 30524
aWd1b3Vz 30525
IHN6eQ== 30526
IExhbmRz 30527
IGJvYXJkaW5n 30528
INC/0L7Rh9GC0Lg= 30529
IGVudnk= 30530
IGNvbXBhc3Npb25hdGU= 30531
IG1lcmNp 30532
IGRlc2lyYWJsZQ== 30533
ZGFsZQ== 30534
IGNhbsSxbQ== 30535
IEFudGFy 30536
dGVtcHM= 30537
IGNvbmZpZ3VyZWQ= 30538
IENvbXBhcmVk 30539
bmVo 30540
aWNhdGluZw== 30541
IG5pY2tlbA== 30542
2YjZgg== 30543
2YPZiNmG 30544
b3Blcw== 30545
IGZvcm11bGFz 30546
INCV0YHRgtGM 30547
IHBvYmw= 30548
IFBK 30549
IEx1ZA== 30550
5LuK5Zue 30551
IEJyaWQ= 30552
IEhvZw== 30553
IEJyaXM= 30554
SmVu 30555
IHNoYWRpbmc= 30556
IFlhcw== 30557
IGRpc3R1cmJlZA== 30558
IHJlY29tbWVuZGluZw== 30559
IGPDqQ== 30560
IEhPVw== 30561
7JeI7Ja0 30562
IHJldmVyc2Vk 30563
IEludGVyZXN0aW5nbHk= 30564
aW94aWQ= 30565
5YWt 30566
IOyYpOy8gOydtA== 30567
4bq/dQ== 30568
eHg= 30569
IG91YWlz 30570
IFlvdVR1YmVycw== 30571
IFJvc2E= 30572
IEhhdXB0 30573
amFkaQ== 30574
IHZsb2dz 30575
IGN1bHR1cmE= 30576
IExlYWRlcnNoaXA= 30577
IEhlcA== 30578
IGlsbHVt 30579
tOuPmQ== 30580
IGN1c3RvbWl6ZWQ= 30581
IG1hcmNh 30582
IHF1YXRybw== 30583
INC90LDQsw== 30584
IFNwYWNlWA== 30585
IEVpZ2Vu 30586
YXN0aW5n 30587
IG9sZHXEn3U= 30588
IGZvcnRz 30589
44GJ 30590
cmltZW50 30591
aWVuY2lh 30592
IHRlbmly 30593
cm9mZmVu 30594
IDE5Nzk= 30595
IGNpZQ== 30596
IOuQmOqzoA== 30597
IGVzY3Jp 30598
z4zPgg== 30599
7Y+s 30600
dXp6eQ== 30601
Q29uZw== 30602
7J247J20 30603
R3JlYXQ= 30604
c2ls 30605
w6ljaA== 30606
44Go44GL 30607
IG11bHRpYw== 30608
IERpc2s= 30609
spU= 30610
IGZhemxh 30611
IGxldmFudA== 30612
IGFiYWpv 30613
dXJyeQ== 30614
c3RydQ== 30615
IOuoueuKlA== 30616
IGFjY2Vzc29yeQ== 30617
INC00LLQuNCz 30618
IFJpZA== 30619
MjAxOQ== 30620
IGRvd25zdHJlYW0= 30621
5pW4 30622
IGtheg== 30623
dXRhbg== 30624
IGNoYXJjb2Fs 30625
IGFmZWN0 30626
d3U= 30627
IGNvbnRleHRz 30628
IGZlYXJlZA== 30629
IOyEpA== 30630
IGhpc3Rvcmllcw== 30631
IGZhcw== 30632
ZW5zaWJsZQ== 30633
IGNvY29h 30634
aWxsYXI= 30635
Z2VvbnM= 30636
IHNwaXJpdHVhbGl0eQ== 30637
IFBldw== 30638
IHBoYXJtYWN5 30639
IHBhc3Npb25z 30640
IGJvcw== 30641
IGFsbMOh 30642
IHRocml2aW5n 30643
IFJlYWN0 30644
IG9jY3VweQ== 30645
IHdpdGhkcmF3YWw= 30646
IGFsbG93YW5jZQ== 30647
IEZyYWt0aW9u 30648
IGJ1ZGRpZXM= 30649
IGlkbGU= 30650
IGRpc3NvbHZlZA== 30651
IHByZXZhbGVudA== 30652
IG1pbGl0YXI= 30653
IHNlbnNpbmc= 30654
IHBvamF3 30655
IGFuY29yYQ== 30656
IGFidW5kYW50 30657
IGhhaXJzdA== 30658
44GC44KM 30659
IHR3ZWU= 30660
IG7DpGNoc3Rl 30661
IE3DtmdsaWNoa2VpdA== 30662
IGhvbw== 30663
dWZmaWNpZW50 30664
IGZhbnRhc3Q= 30665
IGVkaWJsZQ== 30666
IOuWqOyWtOw= 30667
7JuD 30668
IHZlaW4= 30669
dWNjaQ== 30670
IGRldm90aW9u 30671
IGNvbmNlYWxlcg== 30672
aW5jb21l 30673
IHJlY3ljbGVk 30674
IOyKpO2DgA== 30675
IHBvbnRvcw== 30676
IGRlc3N1cw== 30677
IHbDqXJpdA== 30678
IHJlZmxlY3Rpb25z 30679
IEFB 30680
IHRha2Vhd2F5 30681
YmFyZQ== 30682
IENvbnRhY3Q= 30683
ZWls 30684
IEhlYXI= 30685
IG1pcmFj 30686
IEdlcmlsaW0= 30687
INGB0LDQvNGL0Lk= 30688
IHZpdm8= 30689
IGtpbG9ncmFtcw== 30690
IENyaW0= 30691
w7t0 30692
Nzg= 30693
IHNpbmNlcmVseQ== 30694
cmF6 30695
IOuztQ== 30696
IGFycml2 30697
IGNvbmNlcHRpb24= 30698
IFBlcnNpYW4= 30699
IHNqw6Rs 30700
IHN0YXJyaW5n 30701
IOyVhOustA== 30702
IEZvcmV2ZXI= 30703
0LXRgdGC0Yw= 30704
IHZlaWw= 30705
IHN1YnRpdA== 30706
b2RrYQ== 30707
INC+0YLQvdC+0Yg= 30708
IGNvb2tz 30709
0LXQvdGP 30710
S2F5 30711
IG5pw7Fvcw== 30712
IFBob25l 30713
IHN0aXRjaGluZw== 30714
IGZpbmdlcnByaW50 30715
6aKY 30716
zrvOrA== 30717
IGRlZGljYXRl 30718
IExvYg== 30719
IGJsYWNrcw== 30720
IEJsZQ== 30721
Ym91dA== 30722
IMSRYW5n 30723
IGVrcw== 30724
IHNxdWFzaA== 30725
IEvDvA== 30726
b2Rp 30727
IG7GsOG7m2M= 30728
IHZveWFnZQ== 30729
IHBsYXlmdWw= 30730
INil2YTZiQ== 30731
YW5pYw== 30732
IGNvbmRlbW4= 30733
IELDtnlsZQ== 30734
IFBvbGl6ZQ== 30735
44K/44O8 30736
IGF5dWRh 30737
IHBhbQ== 30738
4LmE4Lib 30739
IEthdGh5 30740
0LXQtNC40L0= 30741
0L3QvtCy0LA= 30742
IGJyaWc= 30743
ZWdlcg== 30744
IGVhZ2xl 30745
IHZpc2lvbnM= 30746
IO2VreyDgQ== 30747
IHNoaXR0eQ== 30748
IGhvdHQ= 30749
IEJyaXR0 30750
dXRvcnM= 30751
RU5URQ== 30752
5puy 30753
IHBob24= 30754
IEJpbmc= 30755
INC/0L7QtNC00LXRgNC2 30756
c3ByaW5n 30757
5pav 30758
ZXR0ZW4= 30759
IHBpbGdy 30760
IGVkaXlvcg== 30761
0LXQvdGC0Ys= 30762
YWdnaW8= 30763
IGp1bA== 30764
IGNvbXByZW5k 30765
dGVpbA== 30766
INiy 30767
IHBlcmZvcm1lcnM= 30768
IGluZmFtb3Vz 30769
IE1L 30770
56o= 30771
5rOB 30772
b3RsZQ== 30773
ZWZm 30774
IEhhc2g= 30775
IGNvd2FyZA== 30776
IEJSQQ== 30777
IERE 30778
IGNvbWlkYQ== 30779
IHBsYXRh 30780
IGZsYXA= 30781
IE1laHI= 30782
cmlidXRpb24= 30783
IFllbWVu 30784
IG15c3Rlcmllcw== 30785
IMSweWk= 30786
IHN0ZWxs 30787
IGV5ZWxpbmVy 30788
IGRlbGVz 30789
IG5haWxlZA== 30790
IGlsbG5lc3Nlcw== 30791
IHN0YWNrcw== 30792
IHRyYWJhamFy 30793
Zmxvd2Vy 30794
Y2l1 30795
IGNydWRl 30796
IHN1YnN0YW50aWFsbHk= 30797
IGhvbWVt 30798
IG5lcGhldw== 30799
IHN0YW1wcw== 30800
IGNhcmJz 30801
0YzRgtC1 30802
bW9vdGg= 30803
IHR1bm5lbHM= 30804
YWNpZQ== 30805
5rOi 30806
IFNlw7E= 30807
IEhlcmE= 30808
IOyVhOuLiOyXkOyalA== 30809
IFd5b21pbmc= 30810
IEhETUk= 30811
IExpcw== 30812
dWNpw7Nu 30813
IHN0ZWVy 30814
0L7Rjg== 30815
0LjRgtCw 30816
TlQ= 30817
IOyWvOq1tA== 30818
IHBhbG1z 30819
IG5lb24= 30820
0L7QstCw0L3QuNGP 30821
IGZpbHRlcmluZw== 30822
IGpvdWVy 30823
IEjDtg== 30824
INC90LXRgQ== 30825
6rKg7Ja07JqU 30826
IDgx 30827
IHN0b3J5bGluZQ== 30828
IHByemVw 30829
IHRoYW5raW5n 30830
IEJvZWluZw== 30831
IHNvZnRseQ== 30832
amVt 30833
0LDQu9GM0L3Ri9GF 30834
IGZsYXNobGlnaHQ= 30835
INC/0YM= 30836
IFdPTUFO 30837
4bqvYw== 30838
w61jaA== 30839
IGx1eHVyaW91cw== 30840
IHfDvG4= 30841
IGltcGFjdGZ1bA== 30842
IGNvbnNvbg== 30843
cmV1 30844
aXJyaW5n 30845
aWZ0ZXI= 30846
IGNvbnN0aXR1ZW50cw== 30847
6JC9 30848
IDk0 30849
IFRvdQ== 30850
Z29t 30851
IOyDneqwgeydhA== 30852
IHN0ZXJlb3R5cGVz 30853
IG1vxbxsaQ== 30854
5YiG5Lqr 30855
gqg= 30856
IHBlbmNpbHM= 30857
INGB0LvQvtC2 30858
IGlocmVt 30859
IEJlc2No 30860
IEtvaA== 30861
IEVudHNjaGVpZA== 30862
IGxlaw== 30863
IGbDtnJz 30864
IHRvdGFsbWVudGU= 30865
IGxpdmVseQ== 30866
IGVudHJvcHk= 30867
IGRpc2Nlcm4= 30868
INCX0L3QsA== 30869
IGRvdg== 30870
IG15dGhvbG9neQ== 30871
6KiY5b6X 30872
YXBhbmVzZQ== 30873
IGFwcHJveGltYXRl 30874
0LDRgtC40LI= 30875
aWZpYWJsZQ== 30876
IFNlbw== 30877
5YCS 30878
tOyLrO2eiA== 30879
IOyYtw== 30880
IHRlbXBvcmFs 30881
IGlU 30882
IGVzdGF0 30883
0LrQuNC8 30884
IHNwcmluaw== 30885
IGdydW5k 30886
IGluZmFudHJ5 30887
IHNjaGFmZmVu 30888
57SE 30889
IGFuaw== 30890
cmlhZ2Vz 30891
IFllb24= 30892
IE1vcm9j 30893
IGludmFzaXZl 30894
gZQ= 30895
IHBhcmVudGluZw== 30896
IFJpcw== 30897
aWJpbGU= 30898
IG1vZHM= 30899
5b2i 30900
INC/0YDQvtCy0LXRgA== 30901
IFRoaW5n 30902
IFdoZXJldmVy 30903
IGFja25vd2xlZGdpbmc= 30904
IHBhd24= 30905
dW1tZXI= 30906
b3Ji 30907
Njk= 30908
IHJldHJvdXZl 30909
IHJlbGllcw== 30910
IEhpZ2h3YXk= 30911
IGF3ZQ== 30912
44Gn44GZ44GL 30913
aXRhaXJl 30914
IGFwcGxpY2FudA== 30915
IGFpc2xl 30916
d29ybQ== 30917
IHBheWxvYWQ= 30918
IGNhcnJl 30919
IEJhY2g= 30920
5qC8 30921
IOy5nOq1rOs= 30922
0L3QuNC1 30923
IGl0w61z 30924
b25uYWlzZQ== 30925
c29s 30926
6I+v 30927
YWxnaWE= 30928
IHJvY2tpbmc= 30929
IGJlc3Rlbg== 30930
cml0ZXM= 30931
Xl4= 30932
0LjQvdC+0Lk= 30933
IGJhaXhv 30934
IOq4sOyWtQ== 30935
0L7RgtGA0Lg= 30936
c2lt 30937
IGluY2Fybg== 30938
64uk7J2M 30939
IGxpY2s= 30940
c2lkZWQ= 30941
IDcx 30942
Zm9yZGVy 30943
IHJlc29uYW5jZQ== 30944
IHRlZ2Vu 30945
IG1ldGFwaA== 30946
b3dzZXI= 30947
INeQ16DXl9eg15U= 30948
P+OAjQ== 30949
IHNwaWVsZW4= 30950
IHZvbGxleQ== 30951
lOydtO2BrOyXhQ== 30952
bG9va2Vk 30953
IHNlbnRlbmNlZA== 30954
IG11bHRpcGx5aW5n 30955
IGlkZWFscw== 30956
IHdhaHJzY2hlaW5saWNo 30957
IGRlcG9zaXRz 30958
YmlsaXI= 30959
IGVmZmV0 30960
aWxsb24= 30961
iOunjA== 30962
IHRlc3RpbW9u 30963
IHphd3N6ZQ== 30964
INC/0YDQvtGG0LXRgdGB 30965
IExhdg== 30966
5LiN6Yyv 30967
IHRyYXZhaWxsZXI= 30968
IGxhaXNzZQ== 30969
IE1vdW50YWlucw== 30970
INGA0L7QsQ== 30971
IGV4YW1pbmVk 30972
aXR1cw== 30973
V2Fz 30974
0LvRiw== 30975
IGF0dHJpYnV0ZWQ= 30976
IOyKuQ== 30977
IEJhcm9u 30978
IGdlcA== 30979
IGF0dGVudA== 30980
IENvbGxlY3Rpb24= 30981
IHRoZWF0 30982
IENhaQ== 30983
IHdlbGxz 30984
IGh1bWFubw== 30985
55eF 30986
IEhhc3Q= 30987
INGF0L7RgtGP 30988
Y3phcw== 30989
IHBlcm1pdHM= 30990
IGxlZ2c= 30991
IGVwbw== 30992
IEZlbg== 30993
IHRoaQ== 30994
IEZvaQ== 30995
IMOpbGVjdA== 30996
IDgz 30997
IG92ZXJ0aA== 30998
IOisneisnQ== 30999
IHRlbmFudA== 31000
6LK3 31001
TmV4dA== 31002
IHByYWlzZWQ= 31003
c2VjdXJpdHk= 31004
IEltcGFjdA== 31005
5Li65LuA5LmI 31006
IHZvdWNo 31007
IG5lZ8Oz 31008
IHVudmU= 31009
IGNyaXRpY2l6ZQ== 31010
IEtlbnlh 31011
IHRhY3RpYw== 31012
IGxvZ3I= 31013
IHBvaXM= 31014
IHBhcGE= 31015
c3BlYWtz 31016
8J+R 31017
aXNwZXJz 31018
IHN1cnBsdXM= 31019
IGNvbGRlcg== 31020
5Y2X 31021
5ZCs 31022
cGxldHM= 31023
IFZpZW5uYQ== 31024
IExlYWQ= 31025
IGFlcmlhbA== 31026
IFRhaA== 31027
0LXQvdGC0L7Qsg== 31028
IEdyZWVrcw== 31029
Q2Ft 31030
IG3DoXhpbQ== 31031
IGt1aW4= 31032
Y2hpbw== 31033
IGRlbW9uc3RyYXRlcw== 31034
YW5vcw== 31035
IENlcnQ= 31036
INGN0L0= 31037
IGJsb2dz 31038
IOyEnOyauA== 31039
IGJlYW1z 31040
0LjQutC+0LI= 31041
IHByb21wdGVk 31042
IGZyaWdodGVuaW5n 31043
IFBvcnNjaGU= 31044
44GI44Gm 31045
bGFyxLFuxLE= 31046
IGNoaWxsaW5n 31047
aXNwaGVyZQ== 31048
IGZsYXNoaW5n 31049
IEthcmQ= 31050
YnJlYWQ= 31051
IGV4aA== 31052
IHR5Y2tlcg== 31053
IGVjb2xvZ2ljYWw= 31054
IE1hZQ== 31055
INee15DXldeT 31056
IOuCmOuPhA== 31057
0LvQvtC9 31058
eXNz 31059
IHBlcmd1bnQ= 31060
IHByaXg= 31061
aXp6YXJk 31062
IGNhbmNlcnM= 31063
IDkx 31064
c3VzcA== 31065
IEl0ZW0= 31066
xZ9h 31067
IHBlc3Q= 31068
IHRha8SF 31069
IGx5bXBo 31070
IFBhdHJp 31071
ZmlsbA== 31072
IHJlY29ubmE= 31073
IG9wdGltaXNt 31074
IG1pbWlj 31075
IOyynA== 31076
IE1hZGFtZQ== 31077
b2N5 31078
bGluaW5n 31079
5ZGK6Ki0 31080
ZXJtZQ== 31081
IGZvbGRlcnM= 31082
IGN6xYI= 31083
dWNoYXI= 31084
IGN1cnNv 31085
IGJyZWFjaA== 31086
0L3QuNGC0Yw= 31087
IHBhbWnEmQ== 31088
IGVsaWc= 31089
IGF1dG9w 31090
Rmxvdw== 31091
IHByb2dyYW1tZWQ= 31092
IFByb2Nlc3M= 31093
IGZpZ3Vy 31094
IFNG 31095
IEVsZXM= 31096
IHByb2dyYW1tZXM= 31097
IGRpenp5 31098
7Iuc6rCE 31099
INC70LjQsdC+ 31100
IHNuaWZm 31101
IFNlYmFzdGlhbg== 31102
IEh5ZQ== 31103
IDQwMDA= 31104
IHBlcm1pdGU= 31105
5qKd 31106
INC30LDRiQ== 31107
IGd1aXQ= 31108
IERhaXM= 31109
IGFjY29yZGFuY2U= 31110
IG1vZHVsYXI= 31111
b2dlbmVvdXM= 31112
5ouN 31113
IHBvdXF1aW5obw== 31114
IGFydGlsbGVyeQ== 31115
IGx1YnJpYw== 31116
IHZvbGNhbg== 31117
IE5I 31118
8J+k 31119
IGRlYW4= 31120
Umg= 31121
IG1pbmlzdHJl 31122
5Z2Q 31123
IEludg== 31124
IEJ1bGdhcg== 31125
IERhdGVu 31126
6I4= 31127
SW0= 31128
IG9yaWdpbmF0ZWQ= 31129
IE5peG9u 31130
aW50ZWdy 31131
IGxhY2tz 31132
IE5hY2h0 31133
7Ja064KY 31134
Y2FtZXJh 31135
IHJhZGlzaA== 31136
a2l5ZQ== 31137
IGFuZ2Vz 31138
IHByw6lm 31139
anVr 31140
IEJlZQ== 31141
IEJV 31142
INCy0L7RgdC/ 31143
IEJU 31144
w6ptZXM= 31145
IFN0w7xjaw== 31146
IEluaw== 31147
5oiW6ICF 31148
IFNlcmdlYW50 31149
IE11bHRpcA== 31150
IGhpw6diaXI= 31151
INCh0LDQvA== 31152
IETDqQ== 31153
b2xwaA== 31154
7Ja4 31155
IGltcGF0 31156
IOyViuqzoA== 31157
INGC0LDQutC+0LPQvg== 31158
INC90LDQstC10YDQvdC+0LU= 31159
IHVucHJlZGljdGFibGU= 31160
IG1lbmQ= 31161
IOyXhuyWtOyalA== 31162
IGpha2llxZs= 31163
IGFubmk= 31164
IGRvbm7DqQ== 31165
IEtpcnN0eQ== 31166
IHJlY3Rhbmd1bGFy 31167
IGVtcGV6YXI= 31168
IEV4Y2hhbmdl 31169
6rCU 31170
IMOpY29ub20= 31171
44GT44KT 31172
ZWxpbg== 31173
cmVpYnQ= 31174
INeU16Q= 31175
IGNlbWV0ZXJ5 31176
IGVzcGHDsW9s 31177
b2xpbg== 31178
0LvRjtC0 31179
IGdyw6JjZQ== 31180
YWxsZW4= 31181
IFBoaWxvcw== 31182
IEVyc3Q= 31183
IOyDiA== 31184
IFZpZA== 31185
R2l2ZQ== 31186
T0g= 31187
zrzOvw== 31188
IFBhcmU= 31189
IG1ldGFib2xpc20= 31190
IG1hcGxl 31191
IGF4bGU= 31192
IER5 31193
IGtvbW1l 31194
z47OvQ== 31195
IGdyZWF0bmVzcw== 31196
IHZlcmlmaWVk 31197
IHNww6k= 31198
IEZhaHJlbmhlaXQ= 31199
IEJyZW4= 31200
IENvbmZlZGVy 31201
IGhpc3RvaXJl 31202
IGVsaW1pbmF0aW5n 31203
IEFkZGluZw== 31204
IEFiaQ== 31205
5p2O 31206
IGhvc3BpdGFsaXR5 31207
dGlt 31208
IGJvbml0bw== 31209
IHBhcnRlcw== 31210
INC00YDRg9Cz0LjRhQ== 31211
IFNoYXk= 31212
IFNlZA== 31213
IHJlZ3JldHM= 31214
0Y/QvNC4 31215
IHRlbmFudHM= 31216
6YCf 31217
IFBUUw== 31218
IGRldmk= 31219
IExhdGU= 31220
dWV6 31221
IHPDtnls 31222
44K7 31223
IOyerOuwjA== 31224
IHRvZ2dsZQ== 31225
IG1hc2tpbmc= 31226
0LDQu9GM0L3QvtCz0L4= 31227
IHBlcnPDtm4= 31228
IGFtZXJpY2Fu 31229
Zmlr 31230
IFJHQg== 31231
ZW5zb24= 31232
IEtB 31233
d3d3dw== 31234
INGA0LXQsw== 31235
bWV0aWNz 31236
IGVkdWNhdG9y 31237
44K344Or44Kv 31238
cGFyaw== 31239
0LXQu9GM0LfRjw== 31240
YXJ1cw== 31241
0YDQtdGC 31242
IGZlaXRv 31243
IGNob2ly 31244
IGxhcmdv 31245
IGVlbnM= 31246
IHdhdHRz 31247
IFNpbmdsZQ== 31248
IHN1c2NlcHRpYmxl 31249
aWNlcg== 31250
INCy0LrQu9GO0Yc= 31251
IHB1cw== 31252
7ZmY 31253
RW5n 31254
IGZhbnRhcw== 31255
IHNwZWNpZmljYXRpb24= 31256
IGNvbmZyb250ZWQ= 31257
IENvbHVtYnVz 31258
0LjQstC10YI= 31259
YXLEsW0= 31260
IGNhZmZlaW5l 31261
bXVuaXRpb24= 31262
IG1pZ3JhbnRz 31263
bGlkZQ== 31264
aXRhdGlvbnM= 31265
IEdlbWU= 31266
4bqr 31267
IHBsYW5uZXI= 31268
IHN0aW11bGF0ZQ== 31269
IGFwcm94aW0= 31270
Y2V1 31271
IE5vbQ== 31272
IHZvZw== 31273
INGA0LDRgdGC 31274
IGVuc2XDsQ== 31275
IHNlbGxlcnM= 31276
IGd1dGVu 31277
emQ= 31278
Q2Fs 31279
IGRlc2NyaXB0 31280
IHJlY29uY2lsaWF0aW9u 31281
emluaG8= 31282
4bmHYQ== 31283
44GY44KD44GC 31284
YWN5ag== 31285
IENPTA== 31286
c2F3 31287
IO2ZleyduA== 31288
IHZhcml0 31289
IHBhcnRuZXJpbmc= 31290
IGRldGVudGlvbg== 31291
IGJvbWJpbmc= 31292
Y2xhcHBpbmc= 31293
aWVuY2llcw== 31294
b25kdQ== 31295
QU1F 31296
IOqwmeyKteuLiOuLpA== 31297
Y8OtYQ== 31298
INC/0L7RgdGC0L4= 31299
IEFTTVI= 31300
IGhvbWVwYWdl 31301
IHNpw6g= 31302
YW50aGE= 31303
IFBvbGw= 31304
IGlnZW4= 31305
Y3ljaA== 31306
IOqwkeyekOq4sA== 31307
IGNvbnNpZGVyYWJseQ== 31308
5LuW55qE 31309
IEFyaXN0 31310
IHdpdGhzdGFuZA== 31311
IHF1YWxpdGF0aXZl 31312
IEtyYWZ0 31313
INGN0LvQtdC60YI= 31314
IEJlYWQ= 31315
0LXQutGC0LjQsg== 31316
IGNydXNoaW5n 31317
7LOQ 31318
IG5hdnk= 31319
2Yjaug== 31320
c2hv 31321
IG9haw== 31322
aXBwZXJz 31323
IHNvaWxz 31324
IHBpZ21lbnQ= 31325
IGV2aXRhcg== 31326
44OH 31327
IGZ1c2U= 31328
IERhbGU= 31329
OiI= 31330
IGNvbXBsw6h0ZW1lbnQ= 31331
IGtlbA== 31332
4LmG 31333
IHF1YXRyZQ== 31334
IFVN 31335
IOunkOs= 31336
5qC5 31337
w61y 31338
IGxlaXN1cmU= 31339
IEhvdXNpbmc= 31340
IGZvbGRz 31341
ZXN0aW9u 31342
QVJT 31343
IG1hc2g= 31344
dXJwb3Nl 31345
IGFjY3VtdWxhdGVk 31346
IFN0dWZm 31347
6Kqe 31348
IHRhcGVz 31349
INGB0LjQu9GM0L3Qvg== 31350
IExPVkU= 31351
IDE5ODI= 31352
IHNjYXJz 31353
IGNhcGl0YWxpc3Q= 31354
IE5lZA== 31355
IHNvZnRlbg== 31356
IG5vdGFibHk= 31357
IGZvcmPDqW1lbnQ= 31358
IFJhdW0= 31359
INC90LXQvtCx0YXQvtC0 31360
IHRyYWRlbWFyaw== 31361
IGZlcnRpZw== 31362
ID8h 31363
5peg 31364
IHJlaW5mb3JjZWQ= 31365
IHJlY2hhcmdl 31366
IFB1dHRpbmc= 31367
IHZpbGxhaW5z 31368
IGhhbmRpYw== 31369
IGFkdmVydGlzZW1lbnQ= 31370
2KrZig== 31371
INGB0YPQvA== 31372
IFJpbGV5 31373
15XXkdc= 31374
5Lqs 31375
T3M= 31376
2KfYsg== 31377
Qm95 31378
IHNxdWlzaA== 31379
b2NrZXQ= 31380
IHRlc3RpZnk= 31381
5ryU 31382
INec157X 31383
INC80LDRgdGB 31384
bWFudWVs 31385
IEFya2Fuc2Fz 31386
aWZmZQ== 31387
IGFuYWx5c3Rz 31388
IERlYWY= 31389
IGrDsw== 31390
IGdyb2Nlcmllcw== 31391
IFdoZWVs 31392
INGA0LjRgQ== 31393
IGPDsm4= 31394
IENvYg== 31395
IHByaXNvbnM= 31396
w6h2ZQ== 31397
IENhYmluZXQ= 31398
IHBvc2Vk 31399
IGd1ZXJyZQ== 31400
IExsb3lk 31401
IGNsZXJr 31402
IGNyaXNlcw== 31403
IFNobw== 31404
IE9yZQ== 31405
IEZvb3RiYWxs 31406
IEFkdmlz 31407
IFpoZW5n 31408
6I0= 31409
IEFNWQ== 31410
IHVuZm9y 31411
IG1vbmFzdGVy 31412
IGNvbXBpbGU= 31413
IGltbW9ydGFs 31414
YXRhYmxl 31415
IHBhcmFubw== 31416
IHRpdmVy 31417
IFN0ZXBo 31418
IEZ1w58= 31419
IGRpc2NvbnRpbg== 31420
IHJpcGU= 31421
IGhhY2tpbmc= 31422
IHNpZW5kbw== 31423
IHNlZ3Vybw== 31424
YWx0cmVz 31425
IGFuZGVyZXM= 31426
IOumrOs= 31427
IGV4cG9ydHM= 31428
5q2l 31429
IHRhYmlp 31430
IOq4sOuLpOs= 31431
IGJvdGhlcmluZw== 31432
IHBpY2tsZQ== 31433
IEJSSUFO 31434
IGFsdGFy 31435
INC/0YDQuNCx 31436
IHRyYW5zZmVycmluZw== 31437
IFZvcnM= 31438
INmH2Yg= 31439
IFph 31440
IEZyYW5jZXM= 31441
IGJyb3dzZQ== 31442
ZW1pdA== 31443
IGNoZXdpbmc= 31444
IEZyZWRkeQ== 31445
IGVkaXRvcnM= 31446
w6RsbGU= 31447
IO2MgA== 31448
IFNxdWU= 31449
IEN1bHR1cmFs 31450
YXdr 31451
IFNhY2hl 31452
IENhcmJvbg== 31453
4bqvdA== 31454
Rkw= 31455
IE5HTw== 31456
cGXFgg== 31457
IFNvdQ== 31458
IGh2b3I= 31459
dW5pbnRlbGxpZ2libGU= 31460
IOuylQ== 31461
IMKw 31462
aWlu 31463
INei150= 31464
IGRlcnJpw6hyZQ== 31465
IGN6eW0= 31466
IEFwb3N0 31467
IHJlZ2FyZGVy 31468
IGFncmFkZQ== 31469
IENhbmR5 31470
IG1hcmU= 31471
IGludHJvZHVjZXM= 31472
YmlyZHM= 31473
IHVuaXF1ZWx5 31474
IG11aw== 31475
IGNvb2tlcg== 31476
IGNyZXdz 31477
IGplaXRv 31478
RVJU 31479
toTr 31480
bmlzc2U= 31481
IGVm 31482
IGNhcnRl 31483
IFlhaw== 31484
IFBBVA== 31485
0LjQvdC+ 31486
Ym9ra2k= 31487
IG1hdGVz 31488
IGRpc3RpbnQ= 31489
IOy9lOuhnOuCmA== 31490
IHnEsWw= 31491
IM66zqzOvQ== 31492
IGNvbmZpZ3VyYXRpb25z 31493
ZW5nYQ== 31494
cmVjaHQ= 31495
SGFwcHk= 31496
44KE44Gj44Gm 31497
aW52ZXN0 31498
IHJlY29uc3RydWN0 31499
INGN0YLQvtC80YM= 31500
IG1vc3F1ZQ== 31501
cmF1bQ== 31502
IHZveWV6 31503
IE5CQw== 31504
IOyekOyLoA== 31505
IHN0dXJkeQ== 31506
INC60LDQvw== 31507
IGFuc2No 31508
YWxpZA== 31509
IG1hc2lo 31510
IFJFUA== 31511
IOy9lOs= 31512
IGRlZHVjdA== 31513
IHNhbGly 31514
d3VyZg== 31515
aWxvdA== 31516
IE11dHRlcg== 31517
b2xkcw== 31518
IEZFTUE= 31519
IEJpYg== 31520
IG5laWdoYm9yaW5n 31521
IGJsaXNz 31522
IO2YvA== 31523
0LvQuNGB0Yw= 31524
INGC0YDQtdCx 31525
IOWwseaYrw== 31526
IGdyZW5hZGU= 31527
IGVnYWw= 31528
IGZpbmVseQ== 31529
IHBldGFscw== 31530
IGtlZXI= 31531
IGNoeWJh 31532
IHNraXBwaW5n 31533
IHRoaXJ0ZWVu 31534
IGdyYXZ5 31535
IFNBVA== 31536
NjE= 31537
INC90L7Qsw== 31538
IG1pbnM= 31539
SVRF 31540
IHNvemlhbA== 31541
7ZWY66m07ISc 31542
cnVrdHVy 31543
INCy0L7Qt9C80L7Qtg== 31544
INC+0L/Rj9GC0Yw= 31545
IGFydGg= 31546
IEN1YmFu 31547
IHRyZWFzdXJlcw== 31548
IGZlcnRpbGl6ZXI= 31549
IGF3YWtlbmluZw== 31550
IOuwseyLoA== 31551
IHJhbGw= 31552
IGRlcGljdA== 31553
IFBhYmxv 31554
IG5pbmV0ZWVu 31555
IHdhdHQ= 31556
IGVudGlyZXR5 31557
S1M= 31558
IFdvb2Rz 31559
U2No 31560
INqp2Yg= 31561
IERyeQ== 31562
44Ge 31563
dXZl 31564
IHJlY29uc3RydWN0aW9u 31565
IGFuYXRvbXk= 31566
iOulvA== 31567
IGJhYmE= 31568
IGxpc3RlbmVy 31569
IHNoYXJwZW4= 31570
IFBlcnU= 31571
INCy0YvQtw== 31572
IHJlY3JlYXRpb24= 31573
IGluaXRpYXRl 31574
IGNhbG9y 31575
IE5hag== 31576
Z2Vl 31577
IEZlZWxz 31578
IFNuYXBjaGF0 31579
IFRldA== 31580
IE5lc3Q= 31581
IERhZg== 31582
IEZpbmlzaA== 31583
INGC0LDQutC40Lw= 31584
w7pj 31585
aXplbnM= 31586
IHNwaW5z 31587
IGVtYnJ5 31588
IHBhc3NhZ2Vz 31589
IGNpZW50 31590
IGp1c3RpZmljYXRpb24= 31591
5LuW6Kqq 31592
IG9sbWF6 31593
IGZsb29kZWQ= 31594
IGVtb2pp 31595
IGVtYnJhY2luZw== 31596
IGRpc2NhcmQ= 31597
IEJhc2lj 31598
YWdvZw== 31599
IOychO2VtA== 31600
IGFzeWx1bQ== 31601
ZXJpbg== 31602
IGZpbQ== 31603
IG5pbmph 31604
IGF1dG9tYXRl 31605
IGFsbGVyZ2lj 31606
w7/Dv8O/w78= 31607
YW1hbQ== 31608
INC80LDRgA== 31609
IE9p 31610
w6R1cw== 31611
IGluZHVjdA== 31612
IEJFTg== 31613
IHrFgg== 31614
IGthxbxkeQ== 31615
IEFNUA== 31616
bsSb 31617
U3VyZQ== 31618
IHF1aWw= 31619
IGVzcGVj 31620
cm9r 31621
QlNDUkk= 31622
IGxpZWJl 31623
cHVz 31624
YWNoc2Vu 31625
IGNyaWNrZXQ= 31626
64qQ 31627
IEZyYW1l 31628
ZWtrw7xy 31629
YXJi 31630
IHDFmQ== 31631
0LjRgdGB 31632
IHplZ2dlbg== 31633
IGRvdWJsZXM= 31634
IERyZQ== 31635
dGVzdA== 31636
aW5zcA== 31637
Ym95cw== 31638
IG3Do28= 31639
IFZlcnNl 31640
IG11c2N1bGFy 31641
IE1BTEU= 31642
IGR1bHU= 31643
IG9jY2FzaW9uYWw= 31644
TG8= 31645
Y29ub21pYw== 31646
IHZhaw== 31647
IHJlbWVkeQ== 31648
5aSg 31649
IOKZquKZquKZqg== 31650
dmVt 31651
IMO2bmVt 31652
IGthcsWfxLE= 31653
IFNoYXJw 31654
aHVy 31655
IOuwqeuylQ== 31656
IGdyYW5kc29u 31657
IGFrdGl2 31658
IFRocm9uZXM= 31659
IOyViOyXkA== 31660
IHRvdHM= 31661
IHN1YmQ= 31662
IFBhdWxh 31663
IGdyYXZlcw== 31664
IEJyZW50 31665
INC90LjQutGC0L4= 31666
IHPDtno= 31667
IGNyZWM= 31668
IFZsYWRpbWly 31669
55ar 31670
INC/0L7QuQ== 31671
ICIt 31672
IHBzeQ== 31673
YXRyaQ== 31674
aWRhbg== 31675
IGHDum4= 31676
IHN0YW5kYXJkaXplZA== 31677
7LmY6w== 31678
INC60YDQvtCy 31679
IFpodQ== 31680
c29tZXRoaW5n 31681
IDc1MA== 31682
IG11amVyZXM= 31683
IGFpdA== 31684
6Ze0 31685
YWd1 31686
IGNvcnJlY3RlZA== 31687
aWtrYQ== 31688
ZWxlZA== 31689
IENhcmVlcg== 31690
b3d5bQ== 31691
IHJvb21tYXRl 31692
IGRlc2NlbmRhbnRz 31693
IE5hcG9sZW9u 31694
INCU0L4= 31695
7ZaI7Ja07JqU 31696
IGJ1bnVu 31697
IE1pY2hh 31698
57ea 31699
IGRlc2NvYg== 31700
UEk= 31701
IHBhbGFicmE= 31702
IHRyYWNrZWQ= 31703
IGRlcGVuZGVuY2U= 31704
IEJhcmFjaw== 31705
5YGH 31706
IGZlcnRpbGl0eQ== 31707
IFNvdXRod2VzdA== 31708
IGluY29tcGxldGU= 31709
IGNvbXVuaWM= 31710
IGNvbXByaXM= 31711
IFJlc3RhdXI= 31712
IGFjcm9u 31713
zrrOsQ== 31714
IGFwcHJlbnRpY2Vz 31715
IG11c3N0 31716
IEFicg== 31717
IHBlbnRydQ== 31718
IENvbnNvcnQ= 31719
IEF2ZWM= 31720
IGR1bXBsaW5ncw== 31721
TFI= 31722
IHdzenlzdGtpZQ== 31723
IHN3YW1w 31724
0L3QtdCy 31725
dWdnbGU= 31726
IHdhdGVyY29sb3I= 31727
IHByb3Rvbg== 31728
IEVzcGHDsWE= 31729
b2NraW5n 31730
0L7QstCw0Ls= 31731
IHRha2lt 31732
VmVyeQ== 31733
IGRlbWVudGlh 31734
IMWfZXlp 31735
SmFj 31736
IE1hY0Jvb2s= 31737
IExpdg== 31738
ZmZpY2llbnRz 31739
IEh1bnQ= 31740
IG92ZXJsYXk= 31741
5oSf6Ka6 31742
IFNreXBl 31743
cHVua3Q= 31744
IGNvbmZpbmVk 31745
IEFkcmlhbg== 31746
2LHZgw== 31747
IEplZXA= 31748
IGVucXVhbnRv 31749
IGFuZXN0 31750
0L7RgtCy0LXRgg== 31751
INC80LXQvdGM 31752
IGlycmlnYXRpb24= 31753
4buRbg== 31754
IGVpZ2h0ZWVu 31755
IFBvbg== 31756
IHJlc2N1ZWQ= 31757
IDE5ODM= 31758
csO8 31759
amFl 31760
IEplb25n 31761
IGFtYXppbmdseQ== 31762
IEZEUA== 31763
IGJhY2tzdGFnZQ== 31764
Y3Vl 31765
IM+Dz4TOt869 31766
INin2YTYtQ== 31767
IGxpdmVzdG9jaw== 31768
IFdhcm5lcg== 31769
IG1ham9ycw== 31770
44OB44Oj 31771
IGNvb3BlcmF0aXZl 31772
IEJyYWR5 31773
cmFpbmVk 31774
cmllYg== 31775
INeR157X 31776
INC00L7QstC+0LvRjNC90L4= 31777
IEZF 31778
IGxlYWtlZA== 31779
IE1lcmN1cnk= 31780
IHBlcnN1YWRl 31781
IHRyYW5zZm9ybWVy 31782
IE5vcndlZw== 31783
IOyXrOufrA== 31784
IHpyb2JpxIc= 31785
IGNhcmRpb3Zhc2N1bGFy 31786
IENyYXNo 31787
IGdvc3NpcA== 31788
0LDRgdGC0Yw= 31789
IOyqvQ== 31790
IHN3ZXB0 31791
IEhvcm4= 31792
IEF0w6k= 31793
IGJ1a2Fu 31794
IEthdw== 31795
S1k= 31796
IFN0b3JpZXM= 31797
R2FyeQ== 31798
IGdhcmRlbmluZw== 31799
IFF1aWNrbHk= 31800
IEZhbGNvbg== 31801
IG92YXQ= 31802
Y8Sx 31803
IENvbXBsZXQ= 31804
IERhdGU= 31805
INC/0YDQuNC8 31806
IGzDpHVmdA== 31807
IEF1ZHJleQ== 31808
IFdlbnQ= 31809
IHBlbMOtY3Vs 31810
IGNhcnJpYWdl 31811
IHVuYWNjZXB0YWJsZQ== 31812
bnltaQ== 31813
INGB0LvRi9GI 31814
IHRlcnJl 31815
dWVsbGVtZW50 31816
RUVFRQ== 31817
IHBoYXJtYWM= 31818
aMO1ZXM= 31819
IHppY2g= 31820
IG1pZ3JhdGU= 31821
IEZyeQ== 31822
w7FhbmE= 31823
IE11aXRv 31824
RU9WRVI= 31825
IGZvcnRyZXNz 31826
IENvbXBhbg== 31827
IEpTT04= 31828
b3JkbnVuZw== 31829
IHdhcnRv 31830
IHVuZ2Vm 31831
7IWU7ISc 31832
INGA0L7Qug== 31833
IHBhZGRsZQ== 31834
SmFyZWQ= 31835
IHN1Ym1pdHRpbmc= 31836
IGxhdGNo 31837
IGZ1Zw== 31838
INC60L7RgQ== 31839
IEVm 31840
IGxhdW5jaGVz 31841
IGZ0 31842
b3RlY2hu 31843
IHRyYXZlbGxlZA== 31844
2KfZgQ== 31845
6YGV 31846
IHByb2No 31847
IGRlZGlt 31848
ODM= 31849
IHJlYm91bmQ= 31850
IExV 31851
cGF0aA== 31852
INGB0L/RgNCw0LI= 31853
IMO2bA== 31854
IO2CpA== 31855
IHByaXZhdA== 31856
IHRyYWN0b3I= 31857
IEF0dGVudGlvbg== 31858
U2Vy 31859
IGNvc2Vz 31860
w6FyaWE= 31861
cGFs 31862
IOydgA== 31863
IHN1Y2Nlc3Nvcg== 31864
IGNvbm5lY3RvcnM= 31865
INGD0YHRgtCw0L3QvtCy 31866
IGdlbm9jaWRl 31867
IHN1ZmZpY2llbnRseQ== 31868
IEFpeMOy 31869
IHN0YWJpbGl6ZQ== 31870
IGNvbmdlc3Q= 31871
IGNhcnZpbmc= 31872
IHpvc3Q= 31873
INCx0YvRgdGC0YDQvg== 31874
IHNob3J0ZXN0 31875
IGxpdmVs 31876
IDg5 31877
6YGK 31878
IGVyaw== 31879
IHBvcnRyYWl0cw== 31880
4KWA 31881
6Jg= 31882
Ym9hdA== 31883
bGxhaA== 31884
QU5D 31885
IGVtcGlyaWNhbA== 31886
IEVjaG8= 31887
IE5lZGVybGFuZA== 31888
6L+Z5LmI 31889
TmV0 31890
IGN1aWRhZG8= 31891
IFJvbWE= 31892
IGNhbGY= 31893
IGdpYW50cw== 31894
IEV4cGxvcmVy 31895
IENvbGxlY3Q= 31896
YWxpdGlvbg== 31897
IERlc3Rpbnk= 31898
IGF1c2dl 31899
IEVkdQ== 31900
IENsbw== 31901
IGVhcnJpbmdz 31902
IFRyYWNr 31903
IFJPUw== 31904
IEJlbGxl 31905
55m+ 31906
IHB1ZWRh 31907
IGRheXRpbWU= 31908
IHN1cHBsaWVy 31909
IFNW 31910
IEV4aGFsZQ== 31911
IGdhbGVyYQ== 31912
Y291cnNl 31913
IGNlbnRpbWV0ZXI= 31914
IEJhc3Q= 31915
bXVk 31916
IHNhbmdhdA== 31917
IFBoeXNpY2Fs 31918
IHByaXZhdGVseQ== 31919
IHRyYXRh 31920
bHlubg== 31921
aWxsaQ== 31922
IOuplOydtO2BrOyXhQ== 31923
IGNyeXN0YWxs 31924
IHBvZHM= 31925
4bqjbg== 31926
aW5hdG9y 31927
IFJlY29yZHM= 31928
5a6Y 31929
xJ9pbWl6 31930
aXNzZW1lbnQ= 31931
aGFyZQ== 31932
aGFkb3c= 31933
IERL 31934
IOyVjOqzoA== 31935
IHd5bg== 31936
IHJlcXVlc3Rpbmc= 31937
IERvbm5h 31938
IOyXtOyLrO2eiA== 31939
aW5lYQ== 31940
IGV4ZXJ0 31941
IER1bmNhbg== 31942
INCy0LXRhw== 31943
IEhhaA== 31944
4KSC 31945
IExpZg== 31946
IEZpbmRpbmc= 31947
IE5vdg== 31948
INC30L3QsNC6 31949
INC+0YQ= 31950
IFF1w6g= 31951
IHF1YXJ0ZXJiYWNr 31952
INGE0LDQug== 31953
IGJpcGFydGlzYW4= 31954
xJ9pbg== 31955
IG7DqWNlc3M= 31956
IHJlZmVyZW5kdW0= 31957
IGNvbXBpbGVy 31958
IHByb2JhYmls 31959
0LXQtNC4 31960
IHRyYWRlcg== 31961
5piT 31962
IFJ1bQ== 31963
Z2VtZQ== 31964
IGRpbw== 31965
IGLEmWR6aWVteQ== 31966
IM+Azqw= 31967
6r64 31968
15XXmA== 31969
IOCklQ== 31970
INCx0LvQsNCz 31971
IHNjYWxw 31972
IFBhdXNl 31973
IGNhcHRpb24= 31974
IGVuZGFuZ2Vy 31975
IGVubGFy 31976
IHJvdHRlbg== 31977
44OD44OI 31978
IHdhaA== 31979
6IKJ 31980
IGR6aQ== 31981
IEluc3RhbGw= 31982
QXk= 31983
IGNyZWFy 31984
0LXQvdGC0LA= 31985
IHdlaWdoaW5n 31986
IGJ1dHRlcmZsaWVz 31987
IEdhc3Q= 31988
5LqV 31989
aG9ybg== 31990
d2Fyeg== 31991
SUNFT1ZFUg== 31992
INC90LDQudGC0Lg= 31993
IGNvZWZmaWNpZW50cw== 31994
57Ch5Zau 31995
IFNwZW5jZXI= 31996
IEhpZ2hlcg== 31997
IGNvd29yaw== 31998
5aiY 31999
INC60L7RgtC+0YDQvtC1 32000
IG1vbml0 32001
IGR5c2Z1bmN0aW9u 32002
INGB0YLQsNC90L7Qsg== 32003
IHRvdXJuYW1lbnRz 32004
IG95c3Rlcg== 32005
Qk4= 32006
IHRydWQ= 32007
c2xvdw== 32008
IFBlbm55 32009
IE9keXM= 32010
w6Zy 32011
IGZvdQ== 32012
IGVuam95bWVudA== 32013
0LDRgtGL 32014
IHd5Z2zEhWRh 32015
0LDQu9GM0L3QsNGP 32016
IFByb3RlY3Q= 32017
IG1veQ== 32018
IGNsYXc= 32019
IHN1c3BpY2lvbg== 32020
IHNhY3JpZmljZWQ= 32021
IGdvc3Rv 32022
Qmln 32023
IGFnZ3Jlc3NpdmVseQ== 32024
IHZvcm5l 32025
44Og 32026
IGJsYW1lZA== 32027
IFNlaHI= 32028
16TXqA== 32029
Y2l0bw== 32030
IHNlYWxz 32031
IG11amVy 32032
IFdlaXJk 32033
IGZvcmVucw== 32034
IGNvbnRyaWJ1dGVz 32035
ZXN0cmE= 32036
IHBvZw== 32037
TE9M 32038
IGhhY2VybG8= 32039
0L7RgtGM 32040
ZmljdGlvbg== 32041
Nzk= 32042
zrvOvw== 32043
5aSn5qaC 32044
5aOw 32045
INGC0L7QsQ== 32046
IEdT 32047
IENsYXJh 32048
aXRleg== 32049
IGFkdm9jYXRpbmc= 32050
IO2UhOs= 32051
c3VuZw== 32052
IHZlcnRpY2Vz 32053
IG5hdmlnYXRpbmc= 32054
IGV1cm9ww6k= 32055
55qG 32056
IHNsb3dlZA== 32057
IGZvcmVncm91bmQ= 32058
IEluZHVzdHJpYWw= 32059
IGFkb3Jl 32060
7Iut 32061
IGNyw6llcg== 32062
5p6X 32063
Y2huaXR0 32064
IHVuYXdhcmU= 32065
IGN1cmx5 32066
ZW50YXI= 32067
IGxlcg== 32068
IHByb2hpYml0ZWQ= 32069
IEhlcm9lcw== 32070
IFJlZWQ= 32071
dWNh 32072
IHNtb2s= 32073
IGt1bm5h 32074
emVpdGln 32075
aW1tZW4= 32076
IEx1bg== 32077
INCw0LHRgdC+0LvRjtGC 32078
IGRlZ2xp 32079
IHZpbGxhZ2Vycw== 32080
IHByZXNldA== 32081
emVwdA== 32082
dWRz 32083
IGVtaXQ= 32084
5L2g6KaB 32085
IOuJ 32086
64qU7KeA 32087
0L3QsNC60L4= 32088
IG9zw7Ni 32089
IDE5Njk= 32090
INCQ0YA= 32091
IG1hbmNobWFs 32092
IEJyb2Nr 32093
IG1hbnRyYQ== 32094
IFdJTA== 32095
YmFjaA== 32096
aW7DpA== 32097
ZWxhcw== 32098
a2Vsbg== 32099
IGRpc2NpcGxl 32100
IHF1YWxj 32101
IGRlaHlk 32102
7J20652864qU 32103
QWY= 32104
7ISx7J20 32105
Unlhbg== 32106
IHB1cHBldA== 32107
INC00YDRg9Cz0LjQtQ== 32108
IHJ1ZA== 32109
IHBlbmRpbmc= 32110
UGx1cw== 32111
IOyViuydhA== 32112
IGLhu4s= 32113
IFNlZ2E= 32114
w6dl 32115
IHByb2dyYW1tZXI= 32116
Ymxp 32117
IHVubA== 32118
IGVuc2xhdmVk 32119
IHNvY2nDqXTDqQ== 32120
xIFo 32121
IGluaGVyaXRhbmNl 32122
IEJhbmds 32123
ZXJtYWlk 32124
IHByYWN0aXRpb25lcg== 32125
IFN0YWxpbg== 32126
IFVzZXI= 32127
Y2libGU= 32128
IGNhcmRpYWM= 32129
IEtvcmVhbnM= 32130
IGR1bXBlZA== 32131
INeU15nXlA== 32132
w6Fpcw== 32133
IGh5ZHJhdWxpYw== 32134
b3VidGVkbHk= 32135
IFBpdA== 32136
IHBpY25pYw== 32137
IGJlaMO2dmVy 32138
INGB0LzQvtCz 32139
IGJyYWtpbmc= 32140
6buR 32141
dXRhcg== 32142
IOyEuOs= 32143
dWJs 32144
IMO8eg== 32145
IG1hamVzdHk= 32146
IGJlcnM= 32147
dXRhYmxl 32148
IGhvdHRlcg== 32149
54Wn 32150
24zZhg== 32151
IGJpYXNlcw== 32152
IHN1YmplY3RlZA== 32153
IG5hdWdodHk= 32154
IGNpcmN1cw== 32155
44GX44GL 32156
IEltbWVkaQ== 32157
IFN0ZWZhbg== 32158
IFRyaXBsZQ== 32159
ZW5r 32160
IHdpdA== 32161
IHJlY3ljbGU= 32162
ZW1pZQ== 32163
ZGF0ZWQ= 32164
IHVubG9hZA== 32165
IHBvcHVsYQ== 32166
Y2hpbg== 32167
IHlpZWxkcw== 32168
IGVuZ2xpc2g= 32169
IEJvbm5pZQ== 32170
IHNwaWRlcnM= 32171
w4E= 32172
IGVyb3Npb24= 32173
6YOo5YiG 32174
IE5JQ0s= 32175
0LjRj9GF 32176
IGltcGFydA== 32177
INC60L3QuA== 32178
IHJlc29sdXRpb25z 32179
IGxpdGhpdW0= 32180
IGNvbnZlcmdlbmNl 32181
IFRhcmE= 32182
INC00LLQtQ== 32183
dGhz 32184
IENpbmR5 32185
5oiR6KaB 32186
5bmr 32187
IERJRQ== 32188
IGFzc3VyYW5jZQ== 32189
INC+0L/QuNGB 32190
IGJ1Y2tldHM= 32191
IGN1ZXM= 32192
IFF1aWV0 32193
IHNpbWlsYXJpdHk= 32194
IGZvdW5kYXRpb25hbA== 32195
IE1pbmlzdA== 32196
5ru/ 32197
IHBpYW4= 32198
IGNlbnRy 32199
IG51bWI= 32200
IG1vbmtz 32201
dWpvdXJk 32202
ZW56aWU= 32203
IHNrYXRlYm9hcmQ= 32204
IGRsYXRlZ28= 32205
INGB0L7Rgg== 32206
IEFF 32207
IG1hc3RlcnBpZWNl 32208
IFNvbG9tb24= 32209
IFJlZGRpdA== 32210
IHJpb3Q= 32211
YWJs 32212
IEpheno= 32213
IGVsZWN0cm9tYWduZXRpYw== 32214
IGluc2VjdXJl 32215
IENvbXBldA== 32216
Z2VyaWVz 32217
0L7QsdC+0LQ= 32218
oNeV 32219
8J+S 32220
IHNlbmF0b3Jz 32221
IEJyaXNiYW5l 32222
IEFsYg== 32223
dXR0ZXJpbmc= 32224
IEFsbG93 32225
emVybw== 32226
IHBhaQ== 32227
INCQ0LvQtdC60YE= 32228
IERpc3BsYXk= 32229
IEJsYWRl 32230
IEFwcHM= 32231
IHDDpA== 32232
INC00LXRgdGP 32233
IHF1ZWxsYQ== 32234
IEdhbw== 32235
0LXQvdC90YvRhQ== 32236
IHNwb2lsZXJz 32237
IGdhbGxvbnM= 32238
INmE2Yo= 32239
IFppb24= 32240
5pyJ5LiA 32241
b25pZQ== 32242
cmFndA== 32243
IENoYW5k 32244
IOuzkQ== 32245
IGJsdW50 32246
IHVzdQ== 32247
IEthZA== 32248
cmFrdA== 32249
IGNpbmVtYXRpYw== 32250
IGFtbXVuaXRpb24= 32251
cmVuZQ== 32252
IGZvdXJ0ZWVu 32253
IENhcm4= 32254
Y3JpdA== 32255
IHRlbnVyZQ== 32256
dnU= 32257
IHByaW5jaXBhbG1lbnRl 32258
IGFsbGVlbg== 32259
6YCZ5LiA 32260
IGtvbXBsZXR0 32261
IGTDvG55 32262
SmFtZXM= 32263
IHJlY2VwdG9y 32264
IG9uZXNlbGY= 32265
Z3VydQ== 32266
IG1lcmNoYW50 32267
bGluZXNz 32268
IG92ZXJsb29rZWQ= 32269
IGhhcm1vbmlj 32270
6ZW/ 32271
aWVzbw== 32272
15XXng== 32273
Y29sbQ== 32274
INC/0YDQvtC10LrRgg== 32275
IEFkYQ== 32276
2KfYsw== 32277
VGlt 32278
IHJlY3VycmluZw== 32279
IHByb2NlZWRz 32280
IFBhcnRpY3VsYXJseQ== 32281
IERvd25sb2Fk 32282
ZXRyaWNhbA== 32283
IG1hdHJpY2Vz 32284
IHByb3llY3Rv 32285
YW5jaWVz 32286
IFVobQ== 32287
IGNhdmVz 32288
IOyWtOugpA== 32289
IExlYWY= 32290
INC+0LHRi9GH 32291
IOydtOycoA== 32292
RXVyb3Bl 32293
IHTEhQ== 32294
IHB1bHM= 32295
IHRha2llZ28= 32296
0J3QtQ== 32297
R1U= 32298
IGZvcnM= 32299
z4HOsw== 32300
IGZvdG9z 32301
ICkp 32302
IOuppOs= 32303
IGFxdWlsbw== 32304
IEt1cmQ= 32305
77iP 32306
cHRpYw== 32307
IERvcnQ= 32308
IG1pc2VyeQ== 32309
YXVzbw== 32310
5Yqf 32311
Y2h1Y2tsaW5n 32312
IFJpZGdl 32313
IO2WiOyKteuLiOuLpA== 32314
ICoqKg== 32315
5a6i 32316
IEhtbW0= 32317
IGdlb2dyYXBoaWM= 32318
IGFueXM= 32319
IHRhbHZleg== 32320
IHNrZWxldA== 32321
IHNpZ25hdHVyZXM= 32322
IGxpdGVycw== 32323
kOuptA== 32324
INGB0LLQvtC10LPQvg== 32325
IHNraWluZw== 32326
INCc0L7RgQ== 32327
IGFkb3B0aW5n 32328
IGhhZnQ= 32329
IHN5bW1ldHJpYw== 32330
IExpcXU= 32331
IHRoeXJvaWQ= 32332
IG1pc2lu 32333
bHVkZQ== 32334
IGh1bGw= 32335
IFhE 32336
IEd1c3Q= 32337
emVpY2g= 32338
IHZpYnJhdGlvbnM= 32339
IGVzZW1w 32340
INCy0YHRjg== 32341
IFF1ZW0= 32342
IMO8YnJpZw== 32343
IFNrZQ== 32344
IEx5bmNo 32345
cm9vbXM= 32346
YXJ0ZXQ= 32347
ZmVzdA== 32348
IGZyw7xoZXI= 32349
IGx1cmU= 32350
5LiN5aW95oSP5oCd 32351
IOyVjOyVhA== 32352
IFdJTg== 32353
IFJZQU4= 32354
INC60L7RgtC+0YDRg9GO 32355
IEthc2g= 32356
INeU154= 32357
IHNhZmVn 32358
IEhhbGxlbHVqYWg= 32359
INC00LLRg9GF 32360
IHN0YXBsZQ== 32361
IHNlZGltZW50 32362
IEFjdHM= 32363
IGJsYW1pbmc= 32364
IG1haW5sYW5k 32365
IHNwb3J0aW5n 32366
IGRlY29yYXRpb25z 32367
IGV4ZWN1dGluZw== 32368
IHBhcmFu 32369
IERvbGxhcg== 32370
IHByb2plY3Rpb25z 32371
IGNvbW1pc3Npb25lZA== 32372
IGJvdXI= 32373
w7Zt 32374
IHN0ZWFtZWQ= 32375
IOutmA== 32376
IHBldHJvbA== 32377
IGNlbHVsYXI= 32378
5bi2 32379
IEh1bmdhcnk= 32380
IHJlbnRlZA== 32381
INCy0LDRgNC4 32382
YmJpZQ== 32383
IHPDqWN1cg== 32384
w7xsbA== 32385
IHN3aW5ncw== 32386
YmV0d2Vlbg== 32387
INC40YI= 32388
ZXN0cm8= 32389
IG5pZW1hbmQ= 32390
IOyCvA== 32391
IFBhcmRvbg== 32392
ZXNzZXM= 32393
IE1JRA== 32394
IGNlbnRyYWxpemVk 32395
IEFsaWVu 32396
Y3Vsb3M= 32397
IGNyaXNl 32398
6KOh6Z2i 32399
IGNsYXNzZQ== 32400
YmVpdGV0 32401
acSfaQ== 32402
IHdoYWxlcw== 32403
IHBlcmltZXRlcg== 32404
IHR5aW5n 32405
IHN0cm9ueQ== 32406
IGxpa2V3aXNl 32407
IFB1bmNo 32408
RGE= 32409
IEJhcHRpc3Q= 32410
IHNvcnRpbmc= 32411
IGl2 32412
IO2VqQ== 32413
IHJlaGFi 32414
IGV0YQ== 32415
cml2ZXI= 32416
IHNhaQ== 32417
44GE44Gf44Gg 32418
b2R1cw== 32419
44GK6aGY44GE44GX44G+44GZ 32420
IGVzc2F5ZXI= 32421
IHR1cnRsZXM= 32422
IEhhenJhdA== 32423
IGZhYnJpY3M= 32424
IGNhdml0eQ== 32425
IHBvbmlld2HFvA== 32426
IHNjaGxlY2h0 32427
IHNhbHNh 32428
xZ9la2vDvHI= 32429
IHNlYXRpbmc= 32430
IGVjb25vbWlzdHM= 32431
IG1hbmc= 32432
IHNlZ3VpbnRl 32433
IHJhbmc= 32434
IHJhdGlvcw== 32435
IGNvbnN0ZWxs 32436
IGxvbmd0ZW1wcw== 32437
dWF0aW5n 32438
IHNwb2lsZWQ= 32439
IHJlY2lwaWVudHM= 32440
IHNuaXBlcg== 32441
5LmL5YmN 32442
7Iq164uI6rmM 32443
IHdw 32444
IExJTktF 32445
IGZsYXJl 32446
IEFkcmk= 32447
w7Fhcw== 32448
IGJhY2ts 32449
bcOkw58= 32450
IEJlbmQ= 32451
IHdvcmtsb2Fkcw== 32452
INGB0YPQvw== 32453
IDE5NzU= 32454
0LjQvNGB0Y8= 32455
0LDQvdC1 32456
INC80L7QvQ== 32457
IGFzcGlyYXRpb25z 32458
IEFlcg== 32459
INCz0L7QstC+0YDQuNGC0Yw= 32460
IFFpYW4= 32461
5aaI 32462
IGNvbXByb21pc2Vk 32463
IHlvbGs= 32464
0LvQsNGB0YI= 32465
IGhlbWVu 32466
cm92ZQ== 32467
ZGVucw== 32468
INC60L7QvNC80LXQvdGC 32469
IC0tLQ== 32470
IGZsdW9yZXM= 32471
0L3QvtGB 32472
IExpdmVycG9vbA== 32473
INGB0L7QsdC+0Lk= 32474
IFp3ZQ== 32475
IGx1bWlu 32476
IE9H 32477
4bg= 32478
aG9sbQ== 32479
cHJvZml0cw== 32480
U04= 32481
IHByb3BvcnRpb25z 32482
IG1pY2E= 32483
IEJvaA== 32484
IEF0bGFz 32485
IHVuc3VyZQ== 32486
IHRvdXJpbmc= 32487
IG5pZWQ= 32488
IHTEmQ== 32489
IGltcGVyYXRpdmU= 32490
IGRlbWVr 32491
IFNoZXJpZmY= 32492
cmFuY2U= 32493
IGhvbWVsYW5k 32494
IEhhaWw= 32495
IEdhbno= 32496
eW1t 32497
TW9u 32498
5Ya3 32499
dmlkYQ== 32500
IGRlc2Fycm9sbA== 32501
5oqA 32502
IGludHJpZ3Vpbmc= 32503
IEh1Z28= 32504
IOOCgg== 32505
6aw= 32506
0LDRhg== 32507
IFdpxJlj 32508
YXR0ZWQ= 32509
IOyVhOuLiOqzoA== 32510
IFZhcmk= 32511
w6Fk 32512
IHN1cnJlYWw= 32513
IGRpc3Bhcml0aWVz 32514
IG3Dsw== 32515
dWxsZW4= 32516
IOyeiOuLpOqzoA== 32517
INC/0L7QttCw0LvRg9C50YHRgtCw 32518
IG1haW5z 32519
IGVqZWN0 32520
IG1ldGhhbmU= 32521
IG1hcmdpbmFsaXplZA== 32522
IGNoaWxsaQ== 32523
csOocw== 32524
IHllbQ== 32525
5L2g5piv 32526
IENodW4= 32527
IGRlYnRz 32528
IGRvd25sb2FkaW5n 32529
IEF0aGVucw== 32530
aXNpZXJ1bmc= 32531
cnlu 32532
IHRla24= 32533
IFF1aW5kaQ== 32534
6ZyA 32535
IHRhcmFm 32536
IGjDqQ== 32537
IGNvbnNjaW91c2x5 32538
IGZpeGVz 32539
dWNrbGU= 32540
bWF5xLFu 32541
IGZyZWk= 32542
IHNwYQ== 32543
IOynhO2WiQ== 32544
INin2YTYsA== 32545
INGD0Lo= 32546
bGV0dA== 32547
IG9sbXXFnw== 32548
IGNoZWVzeQ== 32549
4Liy4LiB 32550
bmFpcmU= 32551
IHdpZGVu 32552
IGxpZW4= 32553
IGVzY2FwaW5n 32554
aWdncw== 32555
IEJsaWNr 32556
Y8SF 32557
IOyEnOs= 32558
INeU16E= 32559
INCy0L/QtdGA 32560
b3Bob25l 32561
aWVsbA== 32562
IFNVQlNDUkk= 32563
IGxpb25z 32564
IOq3uOqygw== 32565
IGluc3BpcmVz 32566
IGd1YXJhbnRlZXM= 32567
IGNvbWXDp2E= 32568
IEdyb3dpbmc= 32569
IG5lZ2xpZw== 32570
IEZyYW5rZg== 32571
IGdlZ2ViZW4= 32572
IMSR4bqndQ== 32573
IGVuZGxpY2g= 32574
IOyNqA== 32575
IFRU 32576
IExpdGg= 32577
z4DOsQ== 32578
YXN0ZXJu 32579
IEF6ZXI= 32580
IGx1bmFy 32581
aGlj 32582
INC90LDRgNC+0LQ= 32583
IG5lbmh1bQ== 32584
6LeR 32585
IFNhbHZhZG9y 32586
IFByb2dyZXNz 32587
IHByaXZpbGVnZXM= 32588
IOuPmeyViA== 32589
IGFudGFnb24= 32590
IEltcGY= 32591
IGRlc2N1Yg== 32592
IExlaQ== 32593
IOyDiOuhnA== 32594
0YfQtQ== 32595
IGTDs2xhcmVz 32596
IE1lZ2hhbg== 32597
IFdpcmU= 32598
dG9v 32599
YXlpbmc= 32600
dXNj 32601
IHR1ZA== 32602
IGFwcGVhbHM= 32603
ZWR1Yw== 32604
IHBhbmU= 32605
IGpp 32606
IGRlY2tz 32607
IEFsdGVy 32608
IOWwsQ== 32609
7ISk 32610
5YiG6ZCY 32611
IHByb2R1Y3Rpb25z 32612
IFdJTExJQU0= 32613
IGltcGxpZWQ= 32614
IGZ1bGZpbGxtZW50 32615
IEFhaA== 32616
IHNhamE= 32617
eHVz 32618
IM6azrHOuQ== 32619
w6Bz 32620
dWNjaA== 32621
0L7QutC+ 32622
IERpc2NvcmQ= 32623
IFNZ 32624
anNr 32625
IFdhbGxhY2U= 32626
dW5jdGlvbg== 32627
RGFuaWVs 32628
IGvDtnQ= 32629
aWphaA== 32630
IG1hcmNoZQ== 32631
IGRpc2dy 32632
IG11bmdraW4= 32633
IGFsbWE= 32634
s7U= 32635
IGV4dGVuc2l2ZWx5 32636
IEZsb3Jlbg== 32637
IEFsbGlzb24= 32638
44Kx 32639
2YrZhQ== 32640
IGp1dmVu 32641
IFJlbmFpc3NhbmNl 32642
IGZ1bmRyYWlzaW5n 32643
IENoYW9z 32644
IHBhcmFseQ== 32645
IG5hcnJhdG9y 32646
IGVjb3N5c3RlbXM= 32647
QXNo 32648
IG1pdGlnYXRpb24= 32649
IEF1am91cmQ= 32650
IElkZWU= 32651
ISw= 32652
IMK9 32653
IGxhbmRsb3Jk 32654
IGRlZmVjdHM= 32655
IGFjcmU= 32656
dWxzaXZl 32657
IGFsZ2Fl 32658
cGVr 32659
IGVtYmE= 32660
IFJvYw== 32661
6Zui 32662
a3NvbQ== 32663
w6RjaGU= 32664
IGxldWs= 32665
IGxldmVyYWdpbmc= 32666
IOq3uOugh+yngA== 32667
IFBhbG0= 32668
IMOkdmVu 32669
IGxpcw== 32670
IEluc3A= 32671
IFJpdGE= 32672
IEFiYg== 32673
aXRobQ== 32674
IHN1cGVydmlzaW9u 32675
IHJldmlzaXQ= 32676
IHBpxJk= 32677
IGV1aA== 32678
IGZhZGVz 32679
IG1vdHRv 32680
5Y2h 32681
0LXQt9C2 32682
IFNoaW0= 32683
IHJlbGV2YW5jZQ== 32684
IG9v 32685
IG9zdGF0 32686
bmljYQ== 32687
IGNob2l4 32688
IEZhY3VsdHk= 32689
IOykkeyXkA== 32690
IEFib3Zl 32691
INC90LXQsdC+0LvRjNGI 32692
IHNlcXVlbmNpbmc= 32693
IG51dHJpZW50 32694
IGNvbnF1ZXJlZA== 32695
IGRpZ2VzdGl2ZQ== 32696
IGJhY2tkcm9w 32697
IExvcmk= 32698
YWlsYWJsZQ== 32699
R2FtZQ== 32700
IG5lZ2xlY3RlZA== 32701
b21vcnBo 32702
aWxsYWg= 32703
IGtuZQ== 32704
IHNpaXTDpA== 32705
IHdvcmtzcGFjZQ== 32706
IFZlbmljZQ== 32707
IEtuZQ== 32708
0YnQvg== 32709
hYA= 32710
IEhhc3M= 32711
IHZpdGE= 32712
nbzrqbQ= 32713
IGxheXM= 32714
w6puY2lhcw== 32715
w6lyaWNh 32716
IExs 32717
5rGC 32718
IENvY2E= 32719
IFdIWQ== 32720
6Iie 32721
IHJvdXRpbmc= 32722
IHBlcm1pc3Npb25z 32723
IGRpbmdz 32724
cHJlbmQ= 32725
cHJvZ3JhbQ== 32726
IGNyb2NvZA== 32727
YnJhbA== 32728
QUFBQUFBQUE= 32729
YWdpdA== 32730
IE7DpA== 32731
IGdla29tbWVu 32732
YXR0ZW4= 32733
IHJlZmVyZW5jZWQ= 32734
IHBhaXJpbmc= 32735
IFBhcnRuZXI= 32736
IENvcm9uYXZpcnVz 32737
0ZbRgQ== 32738
6L2J 32739
INeU15M= 32740
IGVzcGVjw61maWM= 32741
YXJzaQ== 32742
cXVlbGxl 32743
IHNwb250YW5lb3Vz 32744
54ax 32745
IOqyg+ydhA== 32746
INCf0L7RgdC70LU= 32747
INin2YTYrw== 32748
IFNob3V0 32749
INC90LDQuw== 32750
IGRpc2d1aXNl 32751
IEpvcmQ= 32752
IHdlZQ== 32753
IG1pZWpzYw== 32754
IHNlcnVt 32755
IHBsYWlzaXI= 32756
IGNyZWRpYmxl 32757
IGLDpQ== 32758
IEFK 32759
bWFyZXM= 32760
IHJvZHM= 32761
IGVyYW4= 32762
44G+44GC 32763
IHDDpMOk 32764
IFVB 32765
IFVua25vd24= 32766
INmE2YU= 32767
IFJhYmJp 32768
IGxhYXQ= 32769
IGhhaXJzdHlsZQ== 32770
INi6 32771
6YGL 32772
IGNhY2g= 32773
IFdyaXRpbmc= 32774
0L7Rh9C60Lg= 32775
YWJhZA== 32776
IHN0cmFpZ2h0ZW4= 32777
LS0i 32778
d2lmZQ== 32779
IGhvdHRlc3Q= 32780
IHB1bnlh 32781
IEZhc2hpb24= 32782
Z3JpZmY= 32783
IFFS 32784
b3RjaA== 32785
INCc0L7QttC10YI= 32786
Q2xvdWQ= 32787
IFN0cmlrZQ== 32788
IEhlaW4= 32789
IOecn+eahA== 32790
IGxlaQ== 32791
IEZsb3c= 32792
d2Vncw== 32793
IGhhYnI= 32794
5Ymb5Ymb 32795
bmFobWU= 32796
zIE= 32797
IHBsZWFzaW5n 32798
b3BwaW5n 32799
IOq1rOuPhQ== 32800
IGRyYW4= 32801
IGJhbmdz 32802
IDc5 32803
IHNrZXQ= 32804
IGNhdmFs 32805
IE1hY3Jvbg== 32806
IHdlaWdodGVk 32807
IG11dGVk 32808
IG51ZXN0cmFz 32809
RUVQ 32810
IG1hdGhlbWF0aWM= 32811
IE1SSQ== 32812
YWd1cw== 32813
IHRoZXJhcGllcw== 32814
zrjOtQ== 32815
IHVucGw= 32816
IGNvbW1lbmNlcg== 32817
ZnVsbA== 32818
IHRvd2Vscw== 32819
IHBydWU= 32820
IGxpY2Vuc2Vz 32821
15vXldec 32822
INCf0L7Rh9C10LzRgw== 32823
IHBvaW50bGVzcw== 32824
Qnll 32825
IGVsaWdpYmlsaXR5 32826
IHNjcmFwZQ== 32827
IGFidXNpdmU= 32828
IE1hbnQ= 32829
IGpldW5lcw== 32830
dGFs 32831
IFByaW5jaXA= 32832
IE9ydGhvZG94 32833
IG1lbG9k 32834
INC80LDRgtC10YDQuA== 32835
IHByb3NlY3V0b3I= 32836
IG9waW9pZA== 32837
INGD0LLQtdGA 32838
IEJlZW4= 32839
IOygkeyihQ== 32840
IGR5bmFzdHk= 32841
IGFqdWRh 32842
IGVudHJlZw== 32843
IHdlaWdoZWQ= 32844
IGV1cmU= 32845
IEJlbQ== 32846
IGFibm9ybWFs 32847
ODI= 32848
IEpS 32849
IEFrdA== 32850
IEJyaQ== 32851
w7p0 32852
IHN0YWdu 32853
ISo= 32854
IHdlZ2Vu 32855
IGxlYWtpbmc= 32856
IFdvcmRz 32857
IE1hdQ== 32858
IHZ1ZQ== 32859
IExpYW0= 32860
0LDQvdC40LXQvA== 32861
IGNsaW5pY2lhbnM= 32862
IFB1bXA= 32863
IGbDtnJzdA== 32864
Py4uLg== 32865
IGF1dG9tb3RpdmU= 32866
IE93ZW4= 32867
enVzYWdlbg== 32868
IEh1bmRyZWQ= 32869
IGRlY2VudHJhbGl6ZWQ= 32870
IGJ1bGJz 32871
INec15s= 32872
IHByb3ZpbmNlcw== 32873
IE1pbGFu 32874
ODE= 32875
a2Fz 32876
IOuTow== 32877
IGZvcsOnYQ== 32878
IHJpZ2h0bHk= 32879
5bO2 32880
csSF 32881
IHZlbnVlcw== 32882
IHdhaQ== 32883
IHByZWRpY3Rpbmc= 32884
IFdpRmk= 32885
IOq2geq4iA== 32886
2LHZiA== 32887
INeU15Y= 32888
Y2VudHVyeQ== 32889
IGdyYWR1YWw= 32890
IFByb2JsZW1l 32891
IOyXhQ== 32892
IGNvcGluZw== 32893
IEJydXM= 32894
IHBlYW51dHM= 32895
aXJ0c2NoYWZ0 32896
INC30LDQuw== 32897
IFRyb3k= 32898
IHNwZXJt 32899
IE1pdGFy 32900
IFTDvHJraXll 32901
Z3JhbmQ= 32902
pq0= 32903
INee16E= 32904
IHBhbnM= 32905
IEtub3dsZWRnZQ== 32906
YmVybHk= 32907
INCV0LPQvg== 32908
IGRhbmNlZA== 32909
IEZyb3N0 32910
IEJ1cmc= 32911
IGJpdGluZw== 32912
7KCV7J2E 32913
bWVhbA== 32914
IGhlcm9pYw== 32915
IG1vdGhlcmJvYXJk 32916
IExpY2h0 32917
44Gj44E= 32918
bGxhbg== 32919
0LDQudC9 32920
INGA0Y/QtA== 32921
IOC5gOC4 32922
b25lbg== 32923
aXJpZQ== 32924
QXJ0 32925
cmFuZw== 32926
zr3Otw== 32927
IG5ld2Jvcm4= 32928
IGFtaXM= 32929
INin2YjYsQ== 32930
IHNvcGhvbQ== 32931
IENhcmVmdWw= 32932
IHByb3NwZWN0cw== 32933
ZW5zZW4= 32934
IHRocmlsbA== 32935
IFZp4buHdA== 32936
QWRhbQ== 32937
cml0aW9u 32938
ZW50cmlj 32939
dWRlbg== 32940
IGNlcnRpZmljYXRlcw== 32941
IGFzaGVz 32942
6Kq/ 32943
cGxheWluZw== 32944
IHNhZGVjZQ== 32945
IG9zdA== 32946
IGFpcnBsYW5lcw== 32947
0YDQvtC6 32948
b25lcg== 32949
IG1hZ25lc2l1bQ== 32950
IGdvZGRhbW4= 32951
IDE5NzI= 32952
IFNjaHVsZQ== 32953
IHRlbWF0 32954
IHBhcnRvdXQ= 32955
4K+C 32956
IGludmU= 32957
IFNjaWVudGlzdHM= 32958
IEh1ZHNvbg== 32959
d2lubmluZw== 32960
Y2Vrc2lu 32961
IGNvbmdyZXNzaW9uYWw= 32962
b3J1 32963
IHJvcGVz 32964
0LLQtdC0 32965
IG1hZHJl 32966
IGZlcnJ5 32967
IENvaGVu 32968
IFByZWQ= 32969
IHZhZ3k= 32970
INCx0LXRgdC/ 32971
IG11bHRpbQ== 32972
IGRyYWluYWdl 32973
IHNpbXVsYXRvcg== 32974
Z2lnZ2xlcw== 32975
IFN0YWRpdW0= 32976
0L7QsdGJ 32977
IG5vdGljZXM= 32978
IGNyYXdsaW5n 32979
IGdyb3VwZQ== 32980
5Y+4 32981
IGt0b8Wb 32982
IFlvZ2E= 32983
IG1lZGlkYQ== 32984
INGF0LLQsNGC 32985
IExpdGU= 32986
IHJhdg== 32987
b3JhbWE= 32988
IGRpc2NvcmQ= 32989
IERJUkU= 32990
IHRlaA== 32991
IE51cnM= 32992
57KJ 32993
IHBpdGNoZWQ= 32994
IGJhcmtpbmc= 32995
IENva2U= 32996
d2lhZA== 32997
IHBvcHVsYXRlZA== 32998
6Zmk 32999
cGVsbGVk 33000
INCx0L7Qsw== 33001
IHBld25v 33002
IEN1YmU= 33003
IHJlY3J1aXRlZA== 33004
6YCZ56iu 33005
IENhcmE= 33006
xLHEn8SxbsSx 33007
aW1hdGVk 33008
INGI0LrQvtC7 33009
aWNpb25hbA== 33010
INC/0YDQvtGE 33011
IGNvbnRhbWluYXRpb24= 33012
IMO6bHRpbW9z 33013
IGZlYXJmdWw= 33014
IGVsZXBoYW50cw== 33015
dXNp 33016
IGlUdW5lcw== 33017
IFN3YW1p 33018
6rw= 33019
IOyEpOuqhQ== 33020
IFJpY2hhcmRz 33021
IG1hZ25ldHM= 33022
IFJpY2h0dW5n 33023
IExlZ2lvbg== 33024
6I+c 33025
IGtpdHR5 33026
IGtpc3NlZA== 33027
IHdhdGVyaW5n 33028
IGNvbm8= 33029
IFBhbGVzdGluZQ== 33030
aWRpcg== 33031
IG1hemU= 33032
IGZsdWlkcw== 33033
IFByb2R1Y2Vy 33034
IEtyc25h 33035
5aW95ZWm 33036
bGFm 33037
INeQ15U= 33038
IG1pZXN6 33039
IFhpbmc= 33040
b2ludGVk 33041
c2Vpbg== 33042
IEZ1aw== 33043
IERlcHJlc3Npb24= 33044
IER1dHk= 33045
IFBhbnRoZXI= 33046
IHN1bmQ= 33047
IHJlZmVyZQ== 33048
IGV4Y2x1c2lvbg== 33049
IG5hdmFs 33050
IFdpbnN0b24= 33051
IHNsb2dhbg== 33052
IGh5cG90aGV0aWNhbA== 33053
IGVsZXZhdGU= 33054
66C5 33055
IGNhYmXDp2E= 33056
IEdlc3VuZA== 33057
bWV0ZXI= 33058
IOyVhOuLiOuptA== 33059
IGNsb3VkeQ== 33060
4oCmPw== 33061
IFNjaHJpdHQ= 33062
IEpT 33063
7I0= 33064
IFNwcmluZ3M= 33065
IEJhdHRlcg== 33066
t7A= 33067
IHRhaWxvcg== 33068
IFBUU0Q= 33069
IEdlbnQ= 33070
IGJhxJ8= 33071
IHNwYXR1bGE= 33072
IGNyYXk= 33073
IExlZ2lzbA== 33074
IHPDug== 33075
IGxldmU= 33076
4Liy4Lih 33077
IGVyYWQ= 33078
IGRvbmc= 33079
IGRlcm0= 33080
IEJhbmtz 33081
aWNobw== 33082
5YWI55Sf 33083
IEZyYW56 33084
cmF2ZWw= 33085
6YGU 33086
0L7Qu9C+ 33087
IGZsdXRl 33088
IEVr 33089
IGpveWZ1bA== 33090
IGNoYXNlZA== 33091
IExhcmdl 33092
T3Zlcg== 33093
IGVudHJlcHJlbmV1cmlhbA== 33094
IGNvbnNpZGVycw== 33095
0YPQtdC8 33096
b3Bh 33097
IGRvcm1pcg== 33098
IEVsZW1lbnRhcnk= 33099
IHByenlwYWQ= 33100
0YPRgdC60LA= 33101
INC+0YfQtdGA 33102
dWdlbmU= 33103
IHRlbmlkbw== 33104
IGx1Z2FyZXM= 33105
66U= 33106
INGH0LDRgdGC 33107
IHNhbw== 33108
IGJyYWlk 33109
IFZlcmU= 33110
IFJlaWNo 33111
IFBvc3M= 33112
IGluYW4= 33113
d2FuZA== 33114
cmVm 33115
IG1vbnRyZXI= 33116
IDE5ODE= 33117
55Wq 33118
YXPEsW5kYQ== 33119
IGNocm9tZQ== 33120
IFRyaW5pdHk= 33121
IGV4cGxvaXRhdGlvbg== 33122
IFNlbnNl 33123
IENNUw== 33124
IE5vYmxl 33125
IOyEoO2DnQ== 33126
IHN3ZWxsaW5n 33127
ZWxlY3Ryb25pYw== 33128
XT8= 33129
IGJydXNoaW5n 33130
IGxpcXVpZGl0eQ== 33131
IEhvb2s= 33132
IENvbm5vcg== 33133
IEFsdW0= 33134
IGd1Y2tlbg== 33135
c3VpdGU= 33136
IHdpZWxl 33137
IGJhcnJlbHM= 33138
IFJlZ2Vs 33139
IE1lbnQ= 33140
IFRyaXA= 33141
IEJydXNo 33142
IEVyaWs= 33143
dXJhdGU= 33144
yZly 33145
IEN5cg== 33146
b3VibGU= 33147
IEJlY2Nh 33148
IHBhc3N3b3Jkcw== 33149
xbE= 33150
Ym9yZw== 33151
IHZlbmRv 33152
IENsYXVz 33153
IEZheg== 33154
aW5kZXN0 33155
IGRlY2Vhc2Vk 33156
IGNvbXBhcmlzb25z 33157
IExDRA== 33158
IFBvcms= 33159
IGV2ZW50dWFs 33160
IHBhdHJlb24= 33161
IGluYWJpbGl0eQ== 33162
IGV4dGluY3Rpb24= 33163
IOyii+yVhO2VmOuKlA== 33164
INGB0L7RgQ== 33165
YWp1 33166
INeR15DX 33167
IHNvZm9ydA== 33168
IGRlc3RpbmVk 33169
IFJpbg== 33170
IG1vdXRocw== 33171
IE5hdMO8cmxpY2g= 33172
IHByZXNlcnZpbmc= 33173
IGxpbXA= 33174
6buo 33175
b2N1c2Vk 33176
0LjQvdCz 33177
IGV4cG9zaW5n 33178
IM6+ 33179
640= 33180
bGF1Z2g= 33181
IGhpc3M= 33182
44Gg44GL44KJ 33183
IGluZGll 33184
IGRldGFs 33185
0YDQsNCy0YHRgtCy 33186
IHRyw6pu 33187
5pWw 33188
IG9nbmk= 33189
IHNpbXBsZW1lbnRl 33190
IDE5Nzg= 33191
IGdvbw== 33192
IDE5Njc= 33193
IGdlbnVn 33194
aMO2 33195
IGhpc3TDsw== 33196
5a6f 33197
IGxvYnN0ZXI= 33198
Y2VuZG8= 33199
IHRlaWw= 33200
IGFsbGV2aQ== 33201
MDAwMA== 33202
T0xE 33203
IHBlc29z 33204
IGJvbnVzZXM= 33205
IGFtaQ== 33206
IHJldml2YWw= 33207
IEhvcnNl 33208
IHNhY2s= 33209
VGFsaw== 33210
IG11bGhlcg== 33211
INC/0L7RgdGC0L7Rj9C9 33212
IEhvb2Q= 33213
SHVo 33214
IOu2gQ== 33215
IGh5dW5n 33216
IE1lZXRpbmc= 33217
IGltcG9ydGE= 33218
IOywvuyVhA== 33219
IFZlcm4= 33220
IHN0cmlwcGVk 33221
IHJlZnVzZXM= 33222
IHF1YWxpZmljYXRpb25z 33223
b3Bs 33224
gOuPhA== 33225
aXjDrQ== 33226
IGRpYWI= 33227
aXRpbWU= 33228
Zmxvd3M= 33229
IGluYWM= 33230
IEdvbmc= 33231
IG1lYW5pbmdsZXNz 33232
IGNvdXJhZ2VvdXM= 33233
IG1pY3JvYmk= 33234
YXp5 33235
aGlzdA== 33236
IHZvbHVudGVlcmluZw== 33237
VklF 33238
IHZpb2xhdGVk 33239
IHN5bXBhdGh5 33240
IEVkaXQ= 33241
5aW95YOP 33242
ZWxlY3RyaWM= 33243
cHJvZHVjdA== 33244
IHBhbmRlbWlh 33245
IGdlb21ldHJpYw== 33246
IENvbnZlcnM= 33247
Z3Jl 33248
IGdsdXQ= 33249
aXN0ZWQ= 33250
INin2YTZgw== 33251
IENoYWlu 33252
IFByZXNlbnQ= 33253
IFlpbg== 33254
INGB0L7Qsw== 33255
IFZsb2c= 33256
IOyWtOuouA== 33257
IGRvbm4= 33258
IGhpdGNo 33259
dWNraW5n 33260
44GK44GE 33261
d2FsZA== 33262
cmlzaw== 33263
IGhhcmk= 33264
IEtlbnM= 33265
IElkb2w= 33266
INCy0L3QuNC80LDQvdC40LU= 33267
IHRvZGQ= 33268
IHNtYXNoZWQ= 33269
IGludmFyaQ== 33270
INC60L7QvdGC0YA= 33271
IGF1dGlzdGlj 33272
7J6l64uY 33273
UmVz 33274
0LTRiw== 33275
Y2hhdQ== 33276
IHNlbHY= 33277
IGjDpHR0ZW4= 33278
4KS/ 33279
IGV4cGVjdHM= 33280
z4HOtw== 33281
IGHDp8Sxaw== 33282
IEhUVFA= 33283
bGXFnw== 33284
IHN3ZWVwaW5n 33285
IEJldGE= 33286
IGNvdW50ZXJwYXJ0cw== 33287
YWJpbGU= 33288
IFNpbXM= 33289
Q3M= 33290
IHJlcGFy 33291
c3F1 33292
IHByb3ZpbmNpYWw= 33293
IHNoYXJlaG9sZGVycw== 33294
IHJ1bnRlcg== 33295
IGdlZGFjaHQ= 33296
IFRlZW4= 33297
IGdyYW5kcw== 33298
55Si 33299
YWdsZXM= 33300
IHJvY2t5 33301
dmVucw== 33302
IHJpdmFscw== 33303
dW5hbA== 33304
IHJlYWN0cw== 33305
66k= 33306
IG1lcmN1cnk= 33307
IEx1aWdp 33308
INC+0LM= 33309
IEpVU1Q= 33310
IGxvZA== 33311
IGNvcnRleA== 33312
d2ln 33313
IGxha2g= 33314
7KSR7JeQ 33315
IFZpYw== 33316
IE11bmQ= 33317
IG1hcHBlZA== 33318
IERlbGw= 33319
IERydWNr 33320
IGxpZmVz 33321
0LDQu9GM0L3QvtC1 33322
aXZpZHVhbA== 33323
YWTEsW0= 33324
IGF0cmF2 33325
IEZsdWc= 33326
IEtsZWlu 33327
6rGw7JW8 33328
4Lir4LiZ 33329
IGFwcGxp 33330
4K6+Pw== 33331
w7x5b3J1bQ== 33332
INC40L3RgtC10YDQtdGB0L3Qvg== 33333
IGRpc2luZmVjdA== 33334
Pi0= 33335
IGNoYW1wYWduZQ== 33336
IGtsYQ== 33337
b3BlcnM= 33338
VHJhbnM= 33339
IERlc2VydA== 33340
IGN1bHRpdmF0ZQ== 33341
IEZ1Y2tpbmc= 33342
aWRlbGl0eQ== 33343
INGC0LDQvQ== 33344
IGluY3Vi 33345
IHRlbXU= 33346
IGxlYXJuZXI= 33347
Zm91bmRlcg== 33348
IFN5bA== 33349
44KA 33350
IGZhdG8= 33351
emllcg== 33352
IOyXhuydtA== 33353
IOyIqA== 33354
IHBzeWNobw== 33355
INGC0LXQu9C10YQ= 33356
IHJlZ2FyZGU= 33357
IHJlcHJlc2VudGF0aW9ucw== 33358
IGxpdGlnYXRpb24= 33359
IHNwYW5u 33360
dWx0cw== 33361
Ymlvcg== 33362
6KaL44Gm 33363
5LiN5aSa 33364
IFN1cnZleQ== 33365
IExFRHM= 33366
IHRyw6Q= 33367
IGzDqm4= 33368
IGFudGlveGlk 33369
0LXRgNC+0Lw= 33370
IGluZHVjdGlvbg== 33371
IGZvb2xlZA== 33372
w6R0emxpY2g= 33373
INCz0L7QstC+0YDRj9GC 33374
IEZhY3Q= 33375
dW1iYWk= 33376
IHdpZ2dsZQ== 33377
Tk9VTg== 33378
IGTDqXZlbG9wcA== 33379
IENsYXJv 33380
IOy4 33381
66w= 33382
44Gq44KT44Gg 33383
IGFjY3VtdWxhdGU= 33384
IG1haW50YWlucw== 33385
64Q= 33386
IEZpZ2h0ZXI= 33387
7Yag 33388
IG1hdGlu 33389
IGNvdXBvbg== 33390
IHN0dW50 33391
IGRlYnV0ZWQ= 33392
5b6F44Gj44Gm 33393
IHByYWc= 33394
0LjQstCw0LXQvA== 33395
NzM= 33396
IGV4cHJlcw== 33397
IOyYpOu5oA== 33398
INC/0LXRgNGB0L7QvQ== 33399
IGNhbGN1bHVz 33400
IGFicnVwdA== 33401
IEluc3BlY3Rvcg== 33402
b3VydA== 33403
5paZ 33404
xbpuaWVq 33405
aW50ZW5zZQ== 33406
QmE= 33407
IGxvdW5nZQ== 33408
IGFzdGhtYQ== 33409
IEhpw6c= 33410
qrs= 33411
IGVkaXRvcmlhbA== 33412
IHNlaXpl 33413
IGvEsXI= 33414
IG1vdXZl 33415
IHRpZXJyYQ== 33416
IHRlc3Rvc3Rlcm9uZQ== 33417
IHJo 33418
IEtpbmdzdG9u 33419
RUxMRQ== 33420
IFJlcHJlc2VudGF0aXZl 33421
IDE5NzQ= 33422
IGliYQ== 33423
VHM= 33424
IHNvcnRh 33425
ICg/KQ== 33426
INiq2Yg= 33427
IOuCtOugpA== 33428
IGJla29tbXQ= 33429
IHNwaXJpdHVhbGx5 33430
IGRpc3RvcnRlZA== 33431
TWFk 33432
IHJlaW0= 33433
w6FuaA== 33434
IE90dG9tYW4= 33435
IFJlbGln 33436
IEVscw== 33437
IHJldGFpbmVk 33438
IExhdWdocw== 33439
5oC7 33440
IFNBUw== 33441
INC60L7Qu9C40YfQtdGB0YLQstC+ 33442
15XXqteo 33443
IGlubm92YXRl 33444
IGtvcms= 33445
INGA0LDRgdGB0LrQsNC30YvQsg== 33446
b25kZXJl 33447
aXZp 33448
YXll 33449
b3VudHk= 33450
INC/0L7Qu9GD0YfQsNC10YLRgdGP 33451
IGJ1bnM= 33452
5YWr 33453
IHnDvHpkZW4= 33454
IHN1cmdlcmllcw== 33455
2KPZhg== 33456
IGJhbmtydXB0Y3k= 33457
d2VsdA== 33458
IHNpYW1v 33459
IGRhcmtlc3Q= 33460
IEhhbm4= 33461
Z2dh 33462
IGZvcm1hcw== 33463
IERq 33464
bmFtZWQ= 33465
IHNoaWVsZHM= 33466
dWVsbGVy 33467
IEZldw== 33468
IGxhY2U= 33469
IGZ1cmlvdXM= 33470
IFlV 33471
IHNvY2lldGFs 33472
IGp1ZGdlbWVudA== 33473
IERvcw== 33474
IGphYg== 33475
bGF3cw== 33476
IHJlaW52ZW50 33477
IEthdGhlcmluZQ== 33478
IENob2k= 33479
YWRvd3M= 33480
IHJhbnM= 33481
b2Rlbg== 33482
IE1pZHdlc3Q= 33483
bsSxbg== 33484
IGRlcG9ydA== 33485
IERpcA== 33486
57SF 33487
IGF0ZW5jacOzbg== 33488
IENvdXJ0bmV5 33489
aXZpZGFk 33490
INqp24E= 33491
IGVmZmljYWN5 33492
IEJyb29rcw== 33493
IHJlZmVycmFs 33494
INC60L7QvdGG 33495
IG1hbGljaW91cw== 33496
IGtpcg== 33497
IEdvZGRlc3M= 33498
IGZ1bmt5 33499
IGludGVyaW0= 33500
IEvDtnJwZXI= 33501
IOyWvOun 33502
a3Vy 33503
INC60LvQuA== 33504
IHRydWNz 33505
Z2VzZXR6 33506
IHp1Zw== 33507
IEdsw7xjaw== 33508
IE1pbnV0ZQ== 33509
IHByZXN0aWdpb3Vz 33510
IG5pZXo= 33511
IGNvbmNlbnRyYXRpb25z 33512
0LvQsNGB0YLQuA== 33513
IFNpcw== 33514
IFZpdGFtaW4= 33515
a292 33516
IFBCUw== 33517
INC90LXQtQ== 33518
IHJldGFpbGVycw== 33519
IGNvbnZlbnRpb25z 33520
IFNhbWFudGhh 33521
IHByb3VkbHk= 33522
Sm9yZGFu 33523
IEpBU09O 33524
YXRr 33525
IHRyaXN0ZQ== 33526
IHN0w6Ry 33527
IHJlaXRlcmF0ZQ== 33528
IHBvc3Rlcmlvcg== 33529
IDE5NzM= 33530
IFBpbmU= 33531
IEp1bGlldA== 33532
IHBlZGly 33533
a2ls 33534
IG92ZXJsYXBwaW5n 33535
IGV4Y2x1ZGU= 33536
IGVjb27Ds20= 33537
IGFjY2VwdHM= 33538
IFN0ZXI= 33539
5rG6 33540
IOyatOuPmQ== 33541
ZXN0YWI= 33542
IHR1Zw== 33543
YXJn 33544
IGxpdnJv 33545
2KfYtQ== 33546
IHNlYW1z 33547
IGJ1cmF5YQ== 33548
IGVsbG8= 33549
IFRN 33550
IFBhdw== 33551
IEluZGV4 33552
RXhj 33553
IGluc3BpcmF0aW9uYWw= 33554
IGR1bms= 33555
6LCB 33556
YWt0ZXI= 33557
IGNvbmRpdGlvbmVy 33558
IFNhbHV0 33559
xYJlYw== 33560
IOyJvQ== 33561
INGD0LfQvdCw 33562
IFJvbWVv 33563
ZnJ1aXQ= 33564
IFlP 33565
IGNo4buJ 33566
0LHRgw== 33567
Ym9ucw== 33568
IHJlcHJvZHVjdGl2ZQ== 33569
IG9yYWRh 33570
IO2aqA== 33571
IHRlbnRhcg== 33572
IG1hw7FhbmE= 33573
44Ks 33574
IHNvbHZlbnQ= 33575
SmVzc2ljYQ== 33576
IExlZ2Fs 33577
IHR1YQ== 33578
IHNpYw== 33579
IEVR 33580
YXVrZWU= 33581
7Iuc64uk 33582
IMWedQ== 33583
IGFkaGVyZQ== 33584
IFR1bA== 33585
IOCuhg== 33586
IHRleHRib29rcw== 33587
IEZpZnRo 33588
IGV4cGVyaQ== 33589
IGNoaWM= 33590
IGhlYXA= 33591
aW5lbHk= 33592
YXRyYQ== 33593
VHdv 33594
IGhlbGVtYWFs 33595
IGZyZW4= 33596
5o6o 33597
IGJpc2hlcg== 33598
2KfYtA== 33599
IOyEoOyDnQ== 33600
IFRhZ2Vz 33601
IHPhu7E= 33602
IGJ1bGxpZWQ= 33603
2KQ= 33604
IGJlbmVmaXRlZA== 33605
IFByZXZpb3VzbHk= 33606
INGN0YTRhA== 33607
2Y0= 33608
IHNlbmF0ZQ== 33609
IE1vcm0= 33610
aWprZQ== 33611
IEZsdQ== 33612
IGluY29ycG9yYXRpbmc= 33613
amFjaw== 33614
INC/0LjRgg== 33615
IGltcGx5 33616
IGhhY2tz 33617
IFJJQ0g= 33618
INC60LLQsNGA 33619
INC/0YDQtdC60YDQsNGB 33620
IGRlcGVuZGVuY3k= 33621
IOyaqQ== 33622
IOyxhQ== 33623
IHfDpGhyZW5k 33624
IHN1bGxh 33625
IFBpdHRzYnVyZ2g= 33626
IGVzZW1waW8= 33627
vOuhnA== 33628
cHJvdA== 33629
IFJvc2Vu 33630
IEluZGVwZW5kZW5jZQ== 33631
IHBhcnNsZXk= 33632
aWVnZW4= 33633
IGhhdw== 33634
IGFxdWVsbA== 33635
IENBUA== 33636
INGA0LDQsdC+0YLQsNGC0Yw= 33637
IENsaWZm 33638
aW9uYXI= 33639
IHNlY3VyaW5n 33640
5oiR5YCR55qE 33641
zr3OtQ== 33642
IHV0aWxpcw== 33643
IGNvdWxl 33644
IFBpbmc= 33645
IHRyZWs= 33646
IGZhaw== 33647
IGVub3JtZQ== 33648
IOyLqw== 33649
6K6p 33650
IGRvdWJsaW5n 33651
INC90YDQsNCy0LjRgtGB0Y8= 33652
IGhlZA== 33653
aG92ZW4= 33654
IFN0YW5kaW5n 33655
IG3DrW4= 33656
IEppbWlu 33657
IG1vbmFyY2g= 33658
IGNva2U= 33659
IG1y 33660
IGNsaWM= 33661
w40= 33662
IGltcGVhY2htZW50 33663
IGR1cmFiaWxpdHk= 33664
IHZhcmlvcw== 33665
IGNvbW1lcmNpYWxz 33666
IGdyZWV0aW5ncw== 33667
IFJp 33668
IEFwcHJlY2k= 33669
7J6I64qU 33670
IHLDqXN1bHQ= 33671
w6lydA== 33672
IHNhbHV0ZQ== 33673
IHBvZGVyaWE= 33674
IHN1bnJpc2U= 33675
dmVjaw== 33676
IHJlbHVjdGFudA== 33677
IGNvbW1pc3Npb25lcg== 33678
5b+1 33679
w6J0ZQ== 33680
IEtlbm55 33681
IFNpcmk= 33682
44OD44OX 33683
IOuKmA== 33684
IEVF 33685
IHVuY2g= 33686
0LrQvtC9 33687
INin2YTYpQ== 33688
IGJlbHRz 33689
IGhhc3M= 33690
INC80L7Rjw== 33691
IGRpc3BsYWNlZA== 33692
IGFicmE= 33693
zq3Ouw== 33694
IHNjcmF0Y2hlcw== 33695
IGNvbWV0 33696
IGF1dGhvcml6YXRpb24= 33697
IExMQw== 33698
IHByb2R1aw== 33699
IHJlaGFiaWxpdGF0aW9u 33700
5Z4= 33701
0ZbRhw== 33702
dWRpbmc= 33703
b2xpdA== 33704
IDEwNQ== 33705
IGV4cGFuZHM= 33706
IGFsdHJp 33707
IEtvbW1lbnQ= 33708
IGFuZg== 33709
UGw= 33710
IE1hbmE= 33711
ZmVk 33712
IGJyaQ== 33713
IG9yYQ== 33714
R3M= 33715
IEd1cg== 33716
dWNrbGFuZA== 33717
IGp1bmN0aW9u 33718
IGlyb25pYw== 33719
IEZlZWQ= 33720
IHByYWt0 33721
IEhhbW1lcg== 33722
jOuPhA== 33723
IFRyYWN5 33724
57Wx 33725
IEFzaWRl 33726
0L3QtdCz0L4= 33727
INC40YHQv9C+0LvRjNC30L7QstCw0YLRjA== 33728
IHphag== 33729
IGVxdWl0YWJsZQ== 33730
IGN1cmI= 33731
IOOBk+OCjA== 33732
IGRlcml2YXRpdmVz 33733
IHB1cHBpZXM= 33734
IEtlbm5ldGg= 33735
IENvbXBs 33736
aWdyYW0= 33737
IEdhcmNpYQ== 33738
KSI= 33739
IEhhcmJvcg== 33740
ZXN0aWFs 33741
IOS+hg== 33742
IGVycw== 33743
5rk= 33744
IHVud2FudGVk 33745
IGJlbGFuZw== 33746
0LDQs9C+ 33747
ZW1i 33748
ZG9z 33749
IOyZnOs= 33750
IEJ1ZGdldA== 33751
IGJhdHRsaW5n 33752
2K3Yqg== 33753
a29r 33754
0L3QsNGH0LDQu9Cw 33755
IHBsYWc= 33756
IGNhbnRpZGFk 33757
IGdydXBvcw== 33758
IHBsdWdpbnM= 33759
bGVyaW5p 33760
INC40LzQtdC10YI= 33761
IHNvenVzYWdlbg== 33762
b2xpY3M= 33763
IHB1ZWJsbw== 33764
IHJlbWluaXM= 33765
csOkbg== 33766
IE1vcnJpc29u 33767
IGxpbmhh 33768
IGJyZWF0aHM= 33769
IFRhc3Rl 33770
IGVuZnJlbnQ= 33771
IERvY2tlcg== 33772
INC00LXQvQ== 33773
IGV0aG5pY2l0eQ== 33774
IHdvYg== 33775
IHN1ZmZlcnM= 33776
IHRyYW5zaXRpb25pbmc= 33777
IFJhbmdl 33778
xJlkenk= 33779
INC60LDRgg== 33780
IHN5bmVy 33781
IGRvbnV0 33782
IHByb2JhYmlsaXRpZXM= 33783
IE9tYXI= 33784
V2hpY2g= 33785
dWlzaA== 33786
aXNpbg== 33787
IGRlbW9z 33788
IOyggOq4sA== 33789
IOuYkeqwmQ== 33790
INC10LTQuNC9 33791
IGNlcnZl 33792
IGpva2E= 33793
SUFO 33794
IGtpbG9tZXRlcg== 33795
IGhvcml6b250YWxseQ== 33796
IEJoYWc= 33797
IC0+ 33798
IE1vbml0b3I= 33799
IGtub3dsZWRnZWFibGU= 33800
IGZhdg== 33801
IHBpbm5lZA== 33802
IGVCYXk= 33803
aWNrZXI= 33804
IOyeoOq5kOunjA== 33805
IFhpYW9taQ== 33806
IGNhcGl0 33807
IG5w 33808
IDE5NjU= 33809
aG9l 33810
IG5vaw== 33811
IFNhZ2U= 33812
INC90LXQu9GM0LfRjw== 33813
IFRvdw== 33814
Z2Ft 33815
IGRpY2Vu 33816
IFNVQlNDUklCRQ== 33817
IHJlYm9vdA== 33818
IHBhag== 33819
IOuztOyXrOs= 33820
IHRoaWNrZW4= 33821
IFJlYWxpdHk= 33822
aWTDpG4= 33823
TmE= 33824
IOqyg+ydgA== 33825
ISEp 33826
IHJvdXRpbmVz 33827
INC+0LTQvdC+0LPQvg== 33828
IGV4dGluZw== 33829
IOymnQ== 33830
IHN1bGZ1cg== 33831
IGNhcnZl 33832
IGFzdGVyb2lk 33833
IFdhcnJpb3I= 33834
IHBob3RvZ3JhcGhlcnM= 33835
IHBlbGw= 33836
IGNyb3Nzb3Zlcg== 33837
5oiR55+l6YGT 33838
IGhhY2Vtb3M= 33839
IE5lag== 33840
IHNldHRsaW5n 33841
IGlybQ== 33842
IEJvb2tz 33843
aWVudMO0dA== 33844
IGVzcGFjaW8= 33845
IFNjaG9sYXJz 33846
IGRvb21lZA== 33847
IElSUw== 33848
d29obA== 33849
IHNlZ3Vl 33850
IOuIhOqwgA== 33851
IHByYXRpYw== 33852
QlQ= 33853
IENvbnNpZGVyaW5n 33854
IEJ1ZmZhbG8= 33855
IHRyYWluaW5ncw== 33856
IGdlYnJ1 33857
IEdsZWljaA== 33858
IHBpcmF0ZXM= 33859
IGVudmVsb3A= 33860
IHJlb3Blbg== 33861
aW1hdA== 33862
IHRlZQ== 33863
IHN1ZWQ= 33864
ZmVo 33865
INeU16c= 33866
IGRpZXRz 33867
IGp1bnRvcw== 33868
YXN0bw== 33869
IG1pc3VuZGVyc3Rvb2Q= 33870
IHJ1aW0= 33871
IGNsYXNzaWZ5 33872
INC/0YDQvtC00YPQug== 33873
IGluc2U= 33874
IGlsbHVzdHJhdGVk 33875
IGNvcnJvc2lvbg== 33876
IGFjY3JlZA== 33877
IEF1bnRpZQ== 33878
INC/0YDQuNCy0LXRgg== 33879
IExJVkU= 33880
IHJlaw== 33881
IHJlY2VpcHQ= 33882
5Yiw5bqV 33883
IEJhcmJpZQ== 33884
IFNuYWtl 33885
dHVybg== 33886
SmVmZg== 33887
44GK44GK 33888
lYQ= 33889
Vk9JQ0VPVkVS 33890
Y29sbA== 33891
IHJ1bm5lcnM= 33892
7KCc6w== 33893
b3Nvcw== 33894
bW9vbg== 33895
IGtleW5vdGU= 33896
IEluc3RpdA== 33897
U1BFQUs= 33898
IHBsdWdz 33899
IGN1cnY= 33900
IFl1cmk= 33901
IFRoZXJlcw== 33902
IFBz 33903
IM68z4DOvw== 33904
IGNvbnZlcnRlcg== 33905
IHJlZmluZQ== 33906
IGJhZGFzcw== 33907
IM6/zrk= 33908
IHJlZ2Vu 33909
YXp6aQ== 33910
2YrZgQ== 33911
IHNlaXplZA== 33912
IGnDp2Vy 33913
aWxlZQ== 33914
IHVwc3RyZWFt 33915
IGJ1ZHM= 33916
IHBpbQ== 33917
IO2VmOujqA== 33918
IGFsbHVkZWQ= 33919
IHRoZW1lZA== 33920
IGNvbnNpc3Rpbmc= 33921
IGJvbnM= 33922
dW51eg== 33923
INC/0YDQvtCy0L7QtA== 33924
IExvdmVseQ== 33925
4KWL 33926
IHBhcmFjaA== 33927
IFN0YWF0cw== 33928
6ZqK 33929
IHNlbGVjdGl2ZQ== 33930
IGZhc2U= 33931
IEdlb3JnZXQ= 33932
IGNvY2FpbmU= 33933
IHJlcHJvZHVjdGlvbg== 33934
IExhcmE= 33935
IExE 33936
IGdo 33937
Sm9u 33938
IGzDpQ== 33939
IOuRkOs= 33940
IHR5cGVk 33941
IEJhbmE= 33942
65Oc6w== 33943
IHNhdm9yeQ== 33944
IFpvbWI= 33945
c3RhbmRlbg== 33946
IHBlZGVzdHJpYW4= 33947
IGRpZmbDqXJlbnRz 33948
IOyLuA== 33949
6Imv 33950
IGNvbXBsYWluZWQ= 33951
56aP 33952
INCa0YLQvg== 33953
INec16Q= 33954
YWxpxZtteQ== 33955
IG1vcnRhcg== 33956
IHZlcmRpY3Q= 33957
IHN1ZmljaWVudGU= 33958
IE1pbGxpb24= 33959
bWl0dGVs 33960
aW5hbHM= 33961
INin2YTYrg== 33962
0LDRjtGB0Yw= 33963
IG1pxJlkenk= 33964
IE9sZQ== 33965
IGludmVydA== 33966
Y3p5xIc= 33967
0L7Qt9C80L7QttC90L4= 33968
c3RhcnRlcg== 33969
IGF1ZGl0b3I= 33970
IFNjb3V0 33971
Y2hpZW4= 33972
IFN2ZXJpZ2U= 33973
dWZmbGVk 33974
IHplaG4= 33975
IEF1Y2tsYW5k 33976
IGFyZ2VudA== 33977
IDE5NzY= 33978
IEhvZQ== 33979
IGJvdGhlcnM= 33980
IHNvY2lhbGlzdA== 33981
IHBsaWVycw== 33982
IGVtZXJnZW4= 33983
IFhQ 33984
0LXRgNC+0LI= 33985
TW9yZQ== 33986
IExldmk= 33987
IEFuZGVycw== 33988
aWJpbGlkYWQ= 33989
IFBhcmVudHM= 33990
IGluZHVjZWQ= 33991
7Ja07KQ= 33992
IGJhbGFuY2Vz 33993
INCy0YvRiA== 33994
IHN1Ym1hcmluZQ== 33995
U3RhcnQ= 33996
IGRyaWVz 33997
IHZvbHZlcg== 33998
IHRpY2tpbmc= 33999
Y290dA== 34000
IGZhag== 34001
cHLDqXM= 34002
IFNhYmI= 34003
INC30LDRhw== 34004
INC/0L7QutGD0L8= 34005
IGJhcHRpemVk 34006
IEJyaWxsaWFudA== 34007
INCR0L7Qsw== 34008
IG1vdHM= 34009
Yml0cw== 34010
IGxhdHRpY2U= 34011
5oiR6Lef5L2g 34012
IGNvcmlhbmRlcg== 34013
IHJlc2lkZW5jeQ== 34014
eW5j 34015
IHBpZXJ3c3p5 34016
IEtub2Nr 34017
IFphcA== 34018
INCV0LI= 34019
6rKs 34020
5bCP5b+D 34021
IHVuZXZlbg== 34022
IEphcw== 34023
b2Rvcg== 34024
57+S 34025
NzQ= 34026
IFNpdGU= 34027
IGFjb250ZWNldQ== 34028
eW1wdA== 34029
IHRyaWxvZ3k= 34030
IGxhbnRlcm4= 34031
IFp1Y2tlcg== 34032
dmFyaQ== 34033
d2VsbGluZw== 34034
IFBvdGF0bw== 34035
Z29tZXJ5 34036
IHJlYWN0ZWQ= 34037
IENocm9u 34038
IGplZGU= 34039
YmVlbGQ= 34040
IHR3ZW50 34041
IGxhY3Q= 34042
5qiC 34043
IHLDqXNl 34044
IHJlbGVudA== 34045
IGZ1cm5hY2U= 34046
IHdpZGdldA== 34047
IGVhcnRocXVha2Vz 34048
IEFkanVzdA== 34049
aWxpdA== 34050
INij2Yg= 34051
IGhlYXJpbmdz 34052
IGRlZmVuZGFudA== 34053
aXJzaW5peg== 34054
IGJhc2s= 34055
Y2ph 34056
nKg= 34057
IHJpZmxlcw== 34058
IGluc3RhbA== 34059
IEZvcmdpdmU= 34060
cGljYWw= 34061
INCe0YfQtdC90Yw= 34062
IHBldGl0ZXM= 34063
IGhw 34064
IHJlbm93bmVk 34065
IElubg== 34066
IOyjvOyEuOyalA== 34067
IGVtcGhhc2l6ZWQ= 34068
6Zeu6aKY 34069
IOyeiOyjoA== 34070
IOqyg+ycvOuhnA== 34071
44KG 34072
xZM= 34073
Z2lsaQ== 34074
RGF2ZQ== 34075
IGV4aGF1c3Rpbmc= 34076
xYJ1Zw== 34077
IHNjaGVtYQ== 34078
zrzOrA== 34079
Y3ljbA== 34080
IGF1dGFudA== 34081
IHBhcmNlbA== 34082
IG1hdGVyaWE= 34083
IEJlcnJ5 34084
INGB0LDQvNC4 34085
IGV4dHJhY3RlZA== 34086
IFNheWluZw== 34087
aXNtYXRpYw== 34088
INC/0L7Qv9GA0L7QsQ== 34089
IG5ldXJvbg== 34090
Z3JhcGg= 34091
nOuptA== 34092
IGVuY2xvc3VyZQ== 34093
IEpvaGFubg== 34094
IGFmdGVybWF0aA== 34095
0YLQvtCx 34096
IHXFvHk= 34097
IHNhbXA= 34098
MzYw 34099
IE1laQ== 34100
IHRhY28= 34101
IHJlY2VwdG9ycw== 34102
IHB1bmNoZXM= 34103
IEhvamU= 34104
INmH2YbYpw== 34105
PSIj 34106
IEFuZ3VsYXI= 34107
IG11c2lxdWU= 34108
IHJvbA== 34109
IMOx 34110
c3RlcnJlaWNo 34111
IGNsYW0= 34112
IFRyZWFzdXJ5 34113
Y2hlbWljYWw= 34114
IGFwYXI= 34115
IGFwcGVuZA== 34116
IGZvcmJpZA== 34117
IEhhbWJ1cmc= 34118
0LDQutC+0LI= 34119
IOq4iA== 34120
aWxkYQ== 34121
IHByZXBhcmF0aW9ucw== 34122
IG1vZ8SF 34123
IGNhbWlubw== 34124
RXJpYw== 34125
IEJsaW5k 34126
6IiH 34127
5bm055qE 34128
IERpc2NvdmVyeQ== 34129
7Lig 34130
54i2 34131
IGludGVycHJldGVy 34132
IGJyZWQ= 34133
IFBzYWxt 34134
IGRlZmVuZGVk 34135
7Ims 34136
IEVyZmFocg== 34137
IFBlYWNo 34138
IG1vb25z 34139
IE9zdA== 34140
IHNww6ljaWFs 34141
IGFycml2ZXI= 34142
IFdpcw== 34143
dWNp 34144
IHJvYm90aWNz 34145
SVZF 34146
IHNpZWdl 34147
YXJsYQ== 34148
IHNlcGFyYXRlcw== 34149
IFRD 34150
7Y+w 34151
cXVpc2l0ZQ== 34152
IHBhcmVudGhlc2Vz 34153
0LjQutC1 34154
56uZ 34155
IHRyb3Vz 34156
5bu6 34157
INGB0LjQu9GM 34158
IGJlZXJz 34159
INC/0LvQsNGC 34160
44GZ44GU44GE 34161
IHNvbGE= 34162
IGTDqHM= 34163
bWluZ2hhbQ== 34164
aWt0ZQ== 34165
IG9vcHM= 34166
IHR3aXRjaA== 34167
5bCH 34168
z4g= 34169
IFNob3VsZG4= 34170
dXZyZQ== 34171
IGxlZXI= 34172
Y3JpcHRpb25z 34173
IGV5ZXNoYWRvdw== 34174
IEd1bw== 34175
IFBvd2VsbA== 34176
IHN1cHVlc3Rv 34177
IGFuYQ== 34178
cmFscw== 34179
IE1vbnRyZWFs 34180
IHN1cmZpbmc= 34181
INCf0LXRgNCy 34182
157XlQ== 34183
IG1pbGxpc2Vjb25kcw== 34184
IHN1YnVyYnM= 34185
IHBsYW5ldGE= 34186
0YPRiNC60LA= 34187
aHJsaWNo 34188
IEhZ 34189
INiz25I= 34190
IE1N 34191
IEVmZg== 34192
5Y+v5oSb 34193
IEhT 34194
YW5zb24= 34195
IOyngeygkQ== 34196
IHN1bw== 34197
IGRlcGxveWluZw== 34198
IGt1bnQ= 34199
dGVyaW5n 34200
IGVyZWN0 34201
7J6l7J20 34202
IOydjOyLnQ== 34203
IHNwZWNpbWVu 34204
IS4uLg== 34205
5oiR6Kqq 34206
IGxpZ25l 34207
IGtvbnN0 34208
YWRlcXU= 34209
IOyDge2DnA== 34210
IGFjY2Vzc2Vk 34211
IFBvbGU= 34212
a2lsbA== 34213
IOuyhOs= 34214
IGF1dGhlbnRpY2l0eQ== 34215
IGFwcGVsbGU= 34216
dWxsZQ== 34217
IHJldmlzaW9u 34218
IGdvYXRz 34219
0LPQu9C4 34220
IHBhdQ== 34221
IFJhbmdlcg== 34222
IEltYWc= 34223
YXV0aG9y 34224
IGV2ZQ== 34225
IE1lc3Nlbmdlcg== 34226
IG5heQ== 34227
IHdob2xlcw== 34228
w6R0dGU= 34229
IG9ud2FyZHM= 34230
IERlcG9pcw== 34231
IO2RnO2YhA== 34232
IFNBUlM= 34233
IHdzenlzdGtpY2g= 34234
IGRlc3RydQ== 34235
dW1iaW5n 34236
IGNvbXBhdGliaWxpdHk= 34237
IG1pc2luZm9ybWF0aW9u 34238
b2RvcmU= 34239
IEZhdm9y 34240
ZWtv 34241
j4w= 34242
d2F1a2Vl 34243
IFRlYWNoaW5n 34244
IEtP 34245
IGJldHRpbmc= 34246
IHF1ZXN0cw== 34247
IHZpdnJl 34248
INC80YPQt9GL 34249
IHNhZ2E= 34250
IHN3ZWxs 34251
IGdlaGU= 34252
5oCO6bq85qij 34253
INC+0YDQs9Cw0L3QuNC3 34254
IGdpZGU= 34255
IEdyb3Nz 34256
IGRhbGVq 34257
IGNsYXdz 34258
4buZYw== 34259
IHByZWp1ZGljZQ== 34260
IGluc2lnbg== 34261
aWhvb2Q= 34262
IHBsZWQ= 34263
IGTDs25kZQ== 34264
IFBvbGl0aWNhbA== 34265
IHByZW1pc2Vz 34266
dW5kZXJ0 34267
2LnYqg== 34268
b25uZW4= 34269
IGVzcGHDp28= 34270
IGbDqQ== 34271
IEhhcnJpc29u 34272
IENlbnN1cw== 34273
IGNhcmRpbw== 34274
IGRpeQ== 34275
IG1pbGlldQ== 34276
IGpvdXJuw6ll 34277
IFJlbGVhc2U= 34278
TklF 34279
IE11aw== 34280
aWTDqWU= 34281
4buNaQ== 34282
IGnDp2luZGU= 34283
npk= 34284
IHJlc29uYXRl 34285
IG1vbGVz 34286
IEZseWluZw== 34287
IEdsb3JpYQ== 34288
IFBhc3Rvcg== 34289
IEFyZW5h 34290
5aW95LiN5aW9 34291
Tk9O 34292
0L7Qu9C+0LI= 34293
IGFsbMOt 34294
b21hdA== 34295
7Ja064+E 34296
IGNhcmFjdGVyw61zdA== 34297
IGRlY2xpbmluZw== 34298
0ZbRjw== 34299
YW5jbw== 34300
IEluZm9ybQ== 34301
IGJhcmdhaW4= 34302
IGJ1c2hlcw== 34303
IE5hdHVyYWxseQ== 34304
IHJlY2h0cw== 34305
IFRlbnNvcg== 34306
IFBhdHJpY2lh 34307
IHByaW5jaXBpbw== 34308
IE11bWJhaQ== 34309
IHdvbWI= 34310
IG5vc3RyYQ== 34311
IGRpbGVtbWE= 34312
IGlyZ2VuZHdhbm4= 34313
IDE5NjQ= 34314
IGVuZXJnw61h 34315
INC90LDRgA== 34316
IHNlZ3JlZ2F0aW9u 34317
IEF0aGxldA== 34318
IMK7LA== 34319
IHllbmk= 34320
IFNlaXQ= 34321
IHZlbm9t 34322
IGRha2lrYQ== 34323
IOuPjOs= 34324
IMOJbA== 34325
IGZ1cw== 34326
IE1vZw== 34327
pr3ri4jri6Q= 34328
IHJlbWFy 34329
IFRlZGR5 34330
IGJyZWFzdHM= 34331
aWNhbnM= 34332
5pS255yL 34333
a2Fw 34334
IGjGoW4= 34335
IEpQ 34336
44Oz44K/ 34337
IHJlc3VycmVjdA== 34338
IOyduOs= 34339
aGVyaWNhbA== 34340
IGZvdG9ncmFm 34341
IEpvc8Op 34342
IGxpdmVsaWhvb2Q= 34343
IGJpYmxp 34344
dGVyaQ== 34345
IHZvcnN0ZWxsZW4= 34346
IEFBQQ== 34347
IGFzc2Vzc2luZw== 34348
WUE= 34349
IHNwbGVuZA== 34350
IGV4Y2F2 34351
IGJhcHRpc20= 34352
eWxs 34353
d293 34354
TWFj 34355
IHBsYXN0aWNz 34356
dGVva2Jva2tp 34357
IGludMOpcmVzc2FudA== 34358
IGNvbW1hbmRlZA== 34359
IGZhbW91c2x5 34360
INCY0LvQuA== 34361
IE1hbnVlbA== 34362
IHNvdXRod2VzdA== 34363
IGRlZm9ybWF0aW9u 34364
w61jdWxv 34365
INC90LDRhdC+0LTQuNGC0YHRjw== 34366
IFBhdHRlcg== 34367
ZGVncmVl 34368
IGN6xJlzdG8= 34369
Ii0= 34370
IOyFiw== 34371
IG1hbmdlcg== 34372
IFRydXN0ZWU= 34373
gOumrA== 34374
IHB1bnRvcw== 34375
aXZhYmxl 34376
IHZvbGF0aWxl 34377
IOuKkA== 34378
IGluc3RhYmlsaXR5 34379
IGNpZWw= 34380
Y2nEhQ== 34381
IHB1cml0eQ== 34382
0L3QvtGB0YI= 34383
U2ls 34384
ZWRhcg== 34385
5Zmo 34386
Tk9VTkNFUg== 34387
IHNwZWxsZWQ= 34388
R0VS 34389
IHNhbmN0dWFyeQ== 34390
IGFjY2VsZXJhdGluZw== 34391
IHNjb3V0 34392
INC/0YDQtdCy 34393
ZmFocmVu 34394
44GT44Gh44KJ 34395
IOuCmOyYqA== 34396
IHBvY3rEhXQ= 34397
IE1ldQ== 34398
a2Fhcg== 34399
s7Tqs6A= 34400
YWtyYQ== 34401
RG93bg== 34402
IMOEcg== 34403
IEVsaXRl 34404
IGFsbG9ucw== 34405
IG1heW9ubmFpc2U= 34406
IFN1c3RhaW4= 34407
cHJpc2luZ2x5 34408
IHN1cGVydmlz 34409
IOq3uOugh+yjoA== 34410
IHVuZW1wbG95ZWQ= 34411
IGZyZXNobHk= 34412
INee16I= 34413
IERo 34414
IHRhY2tsaW5n 34415
IG9ncg== 34416
IOy0iOs= 34417
44KI44KN 34418
IGxvZnQ= 34419
YXJhaA== 34420
IEFpcmw= 34421
IERpcg== 34422
INCc0L7QttC90L4= 34423
IGJvb2tpbmc= 34424
IENSQQ== 34425
IGh0dHBz 34426
IGNob2tl 34427
IGdvd24= 34428
IG5vaXRl 34429
IHphYw== 34430
aXN0b2w= 34431
IHNlY3Jl 34432
IHJlc2VtYmxlcw== 34433
IGN1YWQ= 34434
7IKs6rCA 34435
c2hvdw== 34436
IGJsYW5j 34437
IGFndQ== 34438
IFByaW50 34439
YXN0ZWQ= 34440
IFdlYXRoZXI= 34441
aXBs 34442
IG9ic2N1cmU= 34443
IGNvbnRl 34444
b3VnaHM= 34445
KTs= 34446
IERhbWU= 34447
5LiA55u0 34448
IGNsYXJpZmljYXRpb24= 34449
IGludGltYWN5 34450
IHVwaG9sZA== 34451
IE1pcnJvcg== 34452
IHdhZ29u 34453
eGlkZQ== 34454
IGNsb2c= 34455
YXBwZXI= 34456
IEltbWVkaWF0ZWx5 34457
w7pkZQ== 34458
IHRvdWNoZG93bg== 34459
IHJvb2Z0 34460
0LDRiNCw 34461
IMOnxLFrdA== 34462
IGxhaXNzZXI= 34463
IFVucmVhbA== 34464
ZW5zaXRpdmU= 34465
IDEyMw== 34466
IHBsYXN0ZXI= 34467
IGR1Y2tz 34468
IGV0bWU= 34469
IGJpc2hvcA== 34470
YnJldmk= 34471
IGJpYw== 34472
5LiL5Y67 34473
IHJ1bnRpbWU= 34474
IGFtYml0aW9ucw== 34475
0LzQsNGC 34476
IFdlaW4= 34477
IE1hcmk= 34478
IO2KuOs= 34479
IHJlc29sdmVy 34480
IG5nw6B5 34481
IFJpc2U= 34482
44KI44GG44Gr 34483
IENydXM= 34484
IG1lcmNoYW5kaXNl 34485
IGVsaQ== 34486
IHN0YXRld2lkZQ== 34487
IG93bA== 34488
6YGg 34489
5pS5 34490
IHR3aXN0aW5n 34491
IGNvbnRhbWluYXRlZA== 34492
IENvbW1lcmNl 34493
aHl0aG0= 34494
IMOI 34495
IOyLpOs= 34496
IG11c3N0ZQ== 34497
dWly 34498
IHN1bXM= 34499
IFNvbWV3aGVyZQ== 34500
44OO 34501
IGthbWk= 34502
IGFpcmVk 34503
IEFORFJFVw== 34504
IOq6 34505
IHZpZW5kbw== 34506
IGFudGlib2R5 34507
IGFic29sdW1lbnQ= 34508
IHByb3Rlc3RlcnM= 34509
IFF1w6liZWM= 34510
c3RhZHQ= 34511
U2hhdW4= 34512
IGNoYW1iZXJz 34513
IFdlYXI= 34514
IEVmZmVjdHM= 34515
IGhhemFyZHM= 34516
IG5laQ== 34517
IGNvcmF6w7Nu 34518
IOG8 34519
IFNH 34520
lKk= 34521
IOyXreyLnA== 34522
IGNvbWZ5 34523
IENvZHk= 34524
IHBlbnNhbmRv 34525
IGdhbnNrYQ== 34526
IEFjcm9zcw== 34527
w7ZsbGln 34528
YWJ5dGU= 34529
IHdlZGdl 34530
IGthbGlhbg== 34531
IHNpZ3Vl 34532
ZW5kZXM= 34533
IEdyb8Of 34534
IHV0aWxpc2Vy 34535
IGZsb3du 34536
0LDQvdC40Y4= 34537
IGxldmFy 34538
cmVzdHJpYWw= 34539
IGlsbHVzdHJhdGlvbnM= 34540
IGFzbMSxbmRh 34541
QkxFRVA= 34542
INC00L7RgdGC 34543
IHR1cnJldA== 34544
IHN1aXRjYXNl 34545
emnEmWtp 34546
IHNrZXRjaGVz 34547
IGFjcmVk 34548
IFJlaQ== 34549
IHRzdW4= 34550
IFNhZw== 34551
IHRoaXJkcw== 34552
IEtJUkJZ 34553
cmFp 34554
IGh1bWFub3M= 34555
IHJlY29tbWVuZHM= 34556
IGV4dHJhb3JkaW5hcmlseQ== 34557
IGNvbW1lbmNlbWVudA== 34558
S04= 34559
b3Bleg== 34560
INeR16k= 34561
IGxldGhhbA== 34562
IEVzdGFtb3M= 34563
IGluc3BlY3Rvcg== 34564
IFNlb2s= 34565
ZXVu 34566
IG9mZnNob3Jl 34567
IGdldHRpbg== 34568
eWVhcnM= 34569
IFNpbGVuY2U= 34570
IE5hdHVy 34571
dXB1bg== 34572
IHRyenk= 34573
IG5vZ2V0 34574
IGhhbWJ1cmdlcg== 34575
IFByYWlzZQ== 34576
w6luZA== 34577
IDE5NzE= 34578
eWxpZQ== 34579
a3JpdA== 34580
IOyDneqwgeydtA== 34581
55qu 34582
IG1vbWVudG9z 34583
IGVzdMOp 34584
IGRpc3NlbWlu 34585
IGdpZ3M= 34586
IGRlc2Fm 34587
IGF2aXM= 34588
IFpvbw== 34589
IOyViuydgA== 34590
aMOkbmc= 34591
5Y+l 34592
aGFrZQ== 34593
IEJpc20= 34594
IHJldGhpbms= 34595
IE1hbGNvbG0= 34596
IGlkZW50aWZpZXM= 34597
bG93ZXI= 34598
aXhlbA== 34599
IHR2w6U= 34600
a2Vk 34601
aWVyeg== 34602
IMO2ZmZlbnRsaWNo 34603
IHByb2NsYWlt 34604
c29vbg== 34605
bG9s 34606
IGxvaQ== 34607
IGJpdHRlbg== 34608
cm9sbG8= 34609
IHNlcm1vbg== 34610
IGVzcXU= 34611
IGphY2tldHM= 34612
IGdyw6FmaWM= 34613
INC/0L7QutCw0LfRi9Cy 34614
IGNhYmV6YQ== 34615
Y2hvZHpp 34616
IHBlbHZpcw== 34617
IG5vc3RhbGdpYQ== 34618
IGJyZXc= 34619
IHNob3J0Y3V0cw== 34620
IEFkZW3DoXM= 34621
IHN1cGVyZmljaWFs 34622
5YWp5YCL 34623
IGJvY2E= 34624
IOaIkeaYrw== 34625
aW1lbnRvcw== 34626
5Zug5Li6 34627
IHNwcm91dHM= 34628
6aOb 34629
IEpvbmFz 34630
IEZsb3JlbmNl 34631
c3RhdGlj 34632
ZGF1Z2h0ZXI= 34633
Kik= 34634
xYJieQ== 34635
ZmFzaGlvbg== 34636
IEdpbmdlcg== 34637
IOunpOs= 34638
IGh1c3RsZQ== 34639
dXRvcw== 34640
INGC0Y/Qtg== 34641
IEzDtnM= 34642
16nXmded 34643
YW55Y2g= 34644
dHViZXI= 34645
IHRpZHk= 34646
IGZyb250YWw= 34647
IHdoaXNrZXk= 34648
IGh1bWlk 34649
IM6f 34650
IHJpZGdl 34651
IG1hcmlu 34652
IGJpZW50w7R0 34653
IENhcnJpZQ== 34654
Y2h3 34655
IHRhaHVu 34656
IEVyZ2Vi 34657
RlI= 34658
IOygleu2gA== 34659
IFNvbGRpZXI= 34660
IGVubGlnaHRlbm1lbnQ= 34661
IGV4YW1pbmluZw== 34662
IE5vdHJl 34663
IGVyYW0= 34664
IFN1bm55 34665
IGxheWVyZWQ= 34666
IERhenU= 34667
cmFkZXM= 34668
5aW95ZCD 34669
INC90LDRiNC10Lk= 34670
IHRpbWJlcg== 34671
IG1hbm5lcnM= 34672
IEJpcm1pbmdoYW0= 34673
IG1pbmlhdHVyZQ== 34674
b21ldGVycw== 34675
IGZpbGxlcg== 34676
IFJpcA== 34677
IEtvbWI= 34678
b3duZXI= 34679
7L8= 34680
aWRpYW4= 34681
IGRlbcOhcw== 34682
INmI2Ko= 34683
IHByZWNhdXRpb25z 34684
IGdvdmVybm8= 34685
emVsZg== 34686
IENvbXBsZXRl 34687
5biD 34688
IFBoYW50b20= 34689
44G+44Ga 34690
INC90LXQtw== 34691
INC60LDRgNGC 34692
IEFudHdvcnQ= 34693
IFBmaXplcg== 34694
IEZyYW5jbw== 34695
IHfFgg== 34696
IGZyaWc= 34697
ZXNwZXI= 34698
IGthbGU= 34699
IGZpbG1tYWtlcg== 34700
IGt1cnQ= 34701
IGludmFsaWQ= 34702
5bGA 34703
YXJlbGxh 34704
xINuZw== 34705
cmFtZW50bw== 34706
IG51dHJpdGlvbmFs 34707
IGRpY3RhdG9ycw== 34708
IGFmaW4= 34709
IGZ1enp5 34710
IEdpbmE= 34711
w7N0 34712
IEV4dHJlbWFkdXJh 34713
IGRlbW9uc3RyYXRpb25z 34714
IE1vbnRnb21lcnk= 34715
7ZW07ISk 34716
IEdhbmRoaQ== 34717
44Od 34718
572u 34719
IHJldW5pb24= 34720
IGpha2nFmw== 34721
IFp1Zw== 34722
T1VHSA== 34723
bGlmdGluZw== 34724
IOCy 34725
4bmb4bmj 34726
ZWI= 34727
IFdPVw== 34728
IFNoaXZh 34729
b21ldHJ5 34730
IHdpbGRseQ== 34731
IHRlbmRlZA== 34732
IG1lZ2Fw 34733
7LKY 34734
IG5hdXNl 34735
IGdlcmVr 34736
44OL 34737
IE1hcmNlbA== 34738
IG5lc3Rl 34739
2K7YsQ== 34740
IGZlaA== 34741
5YaF 34742
c3VzcGVuc2VmdWw= 34743
IFdyZXN0bGU= 34744
IFBhbGVzdGluaWFucw== 34745
IEdPUkQ= 34746
aXlldA== 34747
INGA0LDQtNC4 34748
IHZlcnN1Y2hlbg== 34749
IHRyYW5zaXN0b3I= 34750
INCf0YDQvtGB0YLQvg== 34751
INC/0L7QvdGA0LDQsg== 34752
IHJoeW1l 34753
IFZlcm1vbnQ= 34754
cGxhdHo= 34755
6K6w 34756
IMSwxZ90ZQ== 34757
IEhhZw== 34758
INCY0Lw= 34759
INGA0LDRgdGB0LrQsNC3 34760
IG1ldHJvcw== 34761
IEluZmluaXR5 34762
d29sZg== 34763
aWJhbA== 34764
ZnRpZw== 34765
INqG 34766
IO2YueyLnA== 34767
IG9nZ2k= 34768
IGRpc3Bvc2l0 34769
INC/0YDQuNC7 34770
INCy0YvQv9C+0Ls= 34771
IHRow7Rp 34772
IEtFTk4= 34773
IGhhbmRpbmc= 34774
YWN0dXM= 34775
IHRhY29z 34776
IGZvcm1lcmx5 34777
IENvcmludGhpYW5z 34778
44Gr44Gv 34779
0YbRltGX 34780
IHBhZHJl 34781
IGNvbmdyZWdhdGlvbg== 34782
5pE= 34783
ZmVydA== 34784
IHN1Ymly 34785
YWlzZXI= 34786
cXVh 34787
YXJhb2g= 34788
IEN1cnJ5 34789
IOyViuuKlA== 34790
0LXQu9GO 34791
IGZ1c3M= 34792
IGJvb3R5 34793
IGxvd3M= 34794
IGhvbW1lcw== 34795
IE1I 34796
IERpc25leWxhbmQ= 34797
d2VudA== 34798
IHJlc2lkdWU= 34799
IGJlZXBpbmc= 34800
6LyV 34801
w6R0dGE= 34802
IG1vdWxk 34803
IFByb2pla3Q= 34804
c3RhbGs= 34805
IGFydGlmYWN0 34806
IEFudHJhZw== 34807
IEFNRA== 34808
IENyeXB0 34809
IOuplA== 34810
IEZlbGlwZQ== 34811
IENPQg== 34812
ZWx1 34813
IHNlbGZpZXM= 34814
IFNhbnRp 34815
Y2h1dHo= 34816
INCj0LrRgNCw0Zc= 34817
Z2VzYW10 34818
IGZsb2Nr 34819
amF6 34820
cGxhaW4= 34821
IHdyaW5rbGVz 34822
IHJlYWlz 34823
IHBhbGpvbg== 34824
IGVtcG93ZXJtZW50 34825
IGF0dGVuZGVlcw== 34826
cHBh 34827
IG5lZGVu 34828
0L7QvdGL 34829
IHRpbWVmcmFtZQ== 34830
IENoZXJyeQ== 34831
IGlkw6ll 34832
IGdhZw== 34833
IGRvbmtleQ== 34834
IMO0bmc= 34835
IEhhcmU= 34836
6Zqb 34837
IEthcmE= 34838
IGFjb21wYW4= 34839
cGxhY2Vz 34840
aW1pZW50b3M= 34841
IEhhbW0= 34842
0LHQuA== 34843
dWJlbg== 34844
aWxpeW9y 34845
IHRoaXJzdA== 34846
IGtyeQ== 34847
IEdlb3JnZXRvd24= 34848
16DXlA== 34849
IG9yY2g= 34850
IGhlYXJ0YmVhdA== 34851
IHRyYW5zZm9ybWF0aW9ucw== 34852
ZXN0b25lcw== 34853
IEtI 34854
IGNhcnRvb25z 34855
IGFuY2k= 34856
IHdvcnRobGVzcw== 34857
IHRhaWxvcmVk 34858
cHU= 34859
QW1lcmljYW5z 34860
IHBpbGVz 34861
IE1vbmtleQ== 34862
IGJhc2lu 34863
IFRlbXBlcg== 34864
IFBhaW50 34865
IHB1bmNoaW5n 34866
IGJhaWs= 34867
IE9ha2xhbmQ= 34868
dnJl 34869
xZ9hbGxhaA== 34870
eWRk 34871
IGNhc3VhbGx5 34872
b2R1 34873
IGNvZGVk 34874
IE5vcndlZ2lhbg== 34875
IFZpbmNl 34876
IHByZW1hdHVyZQ== 34877
IFByb21pc2U= 34878
0LXQutGB0YI= 34879
IGRldmFzdGF0ZWQ= 34880
IFByZW1pdW0= 34881
IFBhcmFt 34882
IMOWeWxl 34883
dW11eg== 34884
UE8= 34885
cmF0b3Jz 34886
IGxhbXBz 34887
IHRlcnJpdG9yaWFs 34888
IGJhY2tib25l 34889
bGlzdGVk 34890
RFk= 34891
INin2YTYsQ== 34892
IHB1cnN1ZWQ= 34893
IENvbW1vbnM= 34894
IOqzoQ== 34895
bG9ja3M= 34896
ZWRvcg== 34897
IGNvbmNlaXZlZA== 34898
Z2VyZQ== 34899
IGRpc2FwcGVhcmluZw== 34900
IFN1bGw= 34901
IOyXsOs= 34902
IGhvZmZl 34903
IGRldG94 34904
7ZSM 34905
IHJldGly 34906
IOuBneuC 34907
IHBlcmd1bnRh 34908
IEJPWQ== 34909
57K+ 34910
IHBlbm4= 34911
5p2l5LqG 34912
aMOpcw== 34913
aG9u 34914
IGNhdGFzdHJvcGhpYw== 34915
IGF1c3Q= 34916
IHRvcnNv 34917
IOyWtOuKkA== 34918
IOyCrOuejOuTpOydtA== 34919
IG1hcnZlbG91cw== 34920
IEhhcmxleQ== 34921
YWNoaW5l 34922
IHRp4bq/ 34923
aXR0bw== 34924
IEnDrW0= 34925
eWxvbg== 34926
IHNodXRkb3du 34927
Licn 34928
IGFwb2xvZ2llcw== 34929
IENvbW11bmljYXRpb24= 34930
INCz0L7QstC+0YDRjg== 34931
44GC44O8 34932
4oSi 34933
w612ZWlz 34934
YWN1bg== 34935
IHJldGFpbmluZw== 34936
IGNvbnRyYWRpY3Rpb24= 34937
IEFEQU0= 34938
Q09N 34939
QnJ5YW4= 34940
IE1vbnNpZXVy 34941
IGFkYXB0aW5n 34942
0KjQkA== 34943
IFNjcg== 34944
w6RuZGVydA== 34945
IHBsYXVz 34946
5LuK5aSp55qE 34947
IG9uc2V0 34948
IGFzc2lzdGFudHM= 34949
IHZhbHZlcw== 34950
IHNjYXR0ZXI= 34951
IFJ1c3Q= 34952
YXdpYQ== 34953
IHJlYWRpbmVzcw== 34954
IHBhaXM= 34955
IGJpYmxl 34956
IGFtYmllbnRl 34957
INCw0LzQtdGA0LjQug== 34958
IHVuY29uZA== 34959
IGthbGs= 34960
5Yqo 34961
IG1vYw== 34962
dW5u 34963
IGFjdHU= 34964
IGh1bW1pbmc= 34965
aXNzaW1v 34966
IFBhdHJvbA== 34967
Z293 34968
44Ok 34969
IFRIRVk= 34970
IEJvZGVu 34971
IEJpZQ== 34972
IHJlZWw= 34973
INGD0YHQu9C+0LI= 34974
IGVuZGVhdm9y 34975
IFBlcmlvZA== 34976
dXN0b21lZA== 34977
bWFscw== 34978
YWxvbg== 34979
Qm94 34980
IM+DzrHPgg== 34981
IG9tZGF0 34982
IGFsdHJl 34983
IEhlaA== 34984
a2Fk 34985
IHByb3RlY3Rvcg== 34986
IGRvbWluYW5jZQ== 34987
b2R5bmFtaWM= 34988
IGNvbW11bmljYXRlZA== 34989
a8O2 34990
IHByZWRlY2Vzc29y 34991
IEx1aw== 34992
IEZsb3dlcg== 34993
IOOBqQ== 34994
cG9xdWU= 34995
0YLQuNGA0L7Qsg== 34996
IHJldHJvc3BlY3Q= 34997
IGRlY2lzaXZl 34998
IGV4ZW1wZWw= 34999
e1w= 35000
IFLDvGNr 35001
cml0ZQ== 35002
IFpldXM= 35003
IGNhbG9yaWU= 35004
IGF0dHJhY3Rpb25z 35005
IEhpbnRlcg== 35006
IHVobQ== 35007
IO2MkA== 35008
IHJ1bGVycw== 35009
IGRpc2NvdXJhZ2Vk 35010
IGFjb250ZWNlcg== 35011
IGFjY2VudHM= 35012
IE9wdGlt 35013
IEFsZw== 35014
a2lkcw== 35015
MjAyMQ== 35016
IExpbmRzYXk= 35017
IGZpbG1tYWtlcnM= 35018
cHJvd2Fk 35019
IHRlcnVn 35020
64u0 35021
IFNvbW1lcg== 35022
MjAxOA== 35023
IGJvcnJvd2luZw== 35024
IFRyYW5zZmVy 35025
0L3QvtC/ 35026
YXJpYXM= 35027
IGhlYWRwaG9uZQ== 35028
7Lyc 35029
IHRyYW5zbGF0aW5n 35030
IGF1Zmdl 35031
4K6q4K6f 35032
d2Vpcw== 35033
YXZhbnQ= 35034
cGFpZA== 35035
YmFieQ== 35036
IHRvdWdoZXN0 35037
IHJlcGVhdHM= 35038
IFRlcmVzYQ== 35039
TG9yZA== 35040
IGFjYWJhcg== 35041
IFJpZGU= 35042
ZGly 35043
IGxlbmc= 35044
IGR3YQ== 35045
IGhlYWRhY2hlcw== 35046
IG7hu69h 35047
INC90LDRgdGC0L7Rj9GJ 35048
IGJvaWxz 35049
IGxvbmdpbmc= 35050
cmlhcw== 35051
w7NyaW8= 35052
IFBhcmFkaXNl 35053
IFNlw7Fvcg== 35054
ZXJkZW0= 35055
IHJlaW5zdA== 35056
IHNhbGFyaWVz 35057
IGluc2VjdXJpdHk= 35058
xYJvxZtjaQ== 35059
INCw0LHRgdC+0LvRjtGC0L3Qvg== 35060
aW5rZW4= 35061
IEVkZHk= 35062
dWRvcw== 35063
IGR1bW15 35064
0JrQsNC6 35065
c2l4 35066
IGluYm94 35067
4bqp 35068
UGVvcGxl 35069
4buTbmc= 35070
IG9yZ2FuaXplcnM= 35071
ZmluZA== 35072
IMO8bA== 35073
IENPTQ== 35074
xbxh 35075
d2VpbGU= 35076
Q29tbWVudGFyeQ== 35077
7Yq466W8 35078
IE1pdHRlbA== 35079
a3Vz 35080
6JuL 35081
4KSo 35082
aXJhbA== 35083
IGdhcm1lbnQ= 35084
zrnOus6s 35085
IHN0b29s 35086
cGF5ZXJz 35087
IHNoaW1tZXI= 35088
IE9sbGll 35089
IEplxbxlbGk= 35090
6L+Y5pyJ 35091
IDE5Nzc= 35092
IGpldXg= 35093
IGV4dGluY3Q= 35094
IFRyYW5zcG9ydGF0aW9u 35095
IE1ha2Vy 35096
IGpvaG4= 35097
IHJpY2hlc3Q= 35098
IHRyYXVtYXQ= 35099
IGxpZWdlbg== 35100
tOulvA== 35101
6L+Z6YeM 35102
IHVucmVzdA== 35103
IFN0cmF3 35104
5ouc5ouc 35105
IGNvbWE= 35106
IEtyaXN0ZW4= 35107
INCa0L7QvdC10YfQvdC+ 35108
IEJyeWNl 35109
INGP0LrRlg== 35110
IHBlYXJscw== 35111
INC/0L7QvdC40LzQsNGO 35112
IGFkZGl0aW9ucw== 35113
IGFzeW1wdA== 35114
INC80LXQvdGM0YjQtQ== 35115
IHNjYW5z 35116
Q2hpbGQ= 35117
IEhpZGU= 35118
0LrRg9GO 35119
ZXRhcw== 35120
IGRhbms= 35121
IHBsZWFz 35122
IGVzc2F5cw== 35123
IGpldHM= 35124
5YWS 35125
INCy0LXQtA== 35126
IHBvc2l0aXZlcw== 35127
aG9m 35128
LSk= 35129
enpv 35130
IHN0YXJ0ZXJz 35131
IHNtaWxlZA== 35132
IDE5NDQ= 35133
cXVpZXJh 35134
IHJvaw== 35135
IHB1ZXN0bw== 35136
Tmljbw== 35137
IHNpbXVsYXRpb25z 35138
IOC2 35139
IGludHJpZ3VlZA== 35140
IE92ZXJ3YXRjaA== 35141
5ZaC 35142
c2lnaA== 35143
YmFp 35144
IOunkOqzoA== 35145
aWTDqQ== 35146
IGNyYWJz 35147
4bqtcA== 35148
IElyYXFp 35149
7J2066W8 35150
0YLRjw== 35151
IFNvcGhpYQ== 35152
IEROUw== 35153
IMO2bmVtbGk= 35154
IEx1bw== 35155
naQ= 35156
IENvdW5zZWw= 35157
bGlnZW4= 35158
0LDQvdGM0YjQtQ== 35159
IHRydW1wZXQ= 35160
IGRhcGF0 35161
IEpN 35162
IEVWRVJZ 35163
IOWwjeS4jeWwjQ== 35164
5aSi 35165
IExheWVy 35166
IGPDtA== 35167
0L3QsNC7 35168
IEpvbw== 35169
IEhhY2s= 35170
IHN1bnQ= 35171
IExlb25hcmQ= 35172
IEZpcmViYXNl 35173
w6RuZ2Vy 35174
IGV4cGxvZGluZw== 35175
dm95 35176
IOymkA== 35177
INGB0LXRgNGM 35178
IHNldmVyaXR5 35179
IGJlc3RpbW0= 35180
57WQ5p6c 35181
IHRpcmluZw== 35182
IHByb2N1cmVtZW50 35183
IGRpcGxvbWFjeQ== 35184
IGRlY29yYXRpdmU= 35185
INmK2Kc= 35186
IHBlbmV0cmF0aW9u 35187
1as= 35188
IG91dHJpZ2h0 35189
RU5F 35190
IFVuaQ== 35191
b2RsZXM= 35192
IHplcm9z 35193
IGRlbGlnaHRmdWw= 35194
am0= 35195
IGRvcG8= 35196
5rKh5LqL 35197
IHBvc2l0aXZpdHk= 35198
IFZJU1RB 35199
IFJlc291cmNl 35200
7YOA6w== 35201
0YjQuNC1 35202
Q2FybA== 35203
IHBpcGluZw== 35204
IGNob3BwaW5n 35205
IEdhbnpl 35206
w7xzcw== 35207
IEFv 35208
IHNoYXR0ZXJlZA== 35209
IERldGVjdGl2ZQ== 35210
IHVuZG91YnRlZGx5 35211
IGhhbGx1Yw== 35212
IGVuY2g= 35213
0YvRh9C90L4= 35214
0YPQu9GP0YA= 35215
aXNlc3Rp 35216
IHBlZGFscw== 35217
IGR1cnVt 35218
pO2U 35219
bGFpbWVy 35220
IHByb3ByZQ== 35221
Q3U= 35222
IHRyYW5zbGF0b3I= 35223
IGNhxYI= 35224
IOq3uOqxuA== 35225
IGNhxYJ5 35226
VUE= 35227
IHJldmlzZWQ= 35228
INC/0L7QtNC+0LE= 35229
IEFydGljbGU= 35230
IEhhaXRp 35231
IMOT 35232
IEN0cmw= 35233
IHJvem0= 35234
bGFpdA== 35235
IGxldHp0ZQ== 35236
aXNwZXJpbmc= 35237
ZGlzcGxheQ== 35238
IGFsdW1pbml1bQ== 35239
IHBhbGFicmFz 35240
IGNvbm9jZXI= 35241
IHppdHRlbg== 35242
IGRpcmln 35243
5Y+q5pyJ 35244
IGJyYWluc3Rvcm0= 35245
IHdpZmk= 35246
IFBhcnRpY2lw 35247
IHZpZXdwb2ludA== 35248
IFF1YW4= 35249
IGhpZXJhcmNo 35250
V2VsY29tZQ== 35251
5a++ 35252
IG9mZmVu 35253
IFJlY292ZXJ5 35254
Z2Fubw== 35255
V291bGQ= 35256
IHJlcHJv 35257
IHBlcmNlcHRpb25z 35258
IGRlbWFzaQ== 35259
IEJhbmdsYWRlc2g= 35260
IEluY3JlZGlibGU= 35261
IGxldHp0 35262
IGJlaGF2aW5n 35263
IGFzdG9uaXNoaW5n 35264
IOKG 35265
IOuCqOyekA== 35266
6LWw5LqG 35267
44OU 35268
IEdPUkRPTg== 35269
Q0FS 35270
PyEi 35271
IFByZXN0 35272
IOunnuyVhOyalA== 35273
IHRhbmQ= 35274
IGxhc2g= 35275
54o= 35276
aWZpY2FudA== 35277
IGludG9sZXI= 35278
INCz0LXRgNC+ 35279
IHRldQ== 35280
YXNv 35281
INGB0L7QstC10YI= 35282
IHRyYXZlbGVycw== 35283
IFN5bmQ= 35284
INCy0LXRgNGB 35285
Rm9uZGE= 35286
YWTEsQ== 35287
IHRyYW5zY3JpcHRpb24= 35288
IHRpdGFuaXVt 35289
IHR3aXN0cw== 35290
IGdlYXJib3g= 35291
ZW5zYXRpb24= 35292
ZmF0 35293
Q29sbA== 35294
IENvbW1vbndlYWx0aA== 35295
em9u 35296
IFBvbGl6ZWk= 35297
IEFQUExBVVNF 35298
ZnJ5 35299
IEp1ZGE= 35300
ZXN0ZWVt 35301
IHNvY2s= 35302
IEp1Z2VuZA== 35303
INC60YHRgtCw0YLQuA== 35304
IERybw== 35305
IHByb2NoYWluZQ== 35306
44O844Or 35307
IGxpa3NvbQ== 35308
IEVuZXJnaWU= 35309
IE1hcmluYQ== 35310
IDIzMA== 35311
IOqwgOyEnA== 35312
dW1waW5n 35313
IGxvbmU= 35314
57Sa 35315
IGZvbnRz 35316
IGJ1c2luZXNzbWFu 35317
IHBseQ== 35318
IGRvZQ== 35319
Z3JpZA== 35320
IE1pbHdhdWtlZQ== 35321
IEVkZW4= 35322
ISIu 35323
INuM24E= 35324
b2dlbnM= 35325
IHRlYXNlcg== 35326
IHF1acOpbg== 35327
IGluY2VudGl2 35328
Z292ZXJu 35329
IGNoaWxkY2FyZQ== 35330
IHNuZWFrZXJz 35331
IGltcHJpc29uZWQ= 35332
wq4= 35333
0LjRgtC10YHRjA== 35334
YW5idWw= 35335
IHJlZ2Fpbg== 35336
IHRyYW5xdWls 35337
UmVkbmVy 35338
6Zuo 35339
SUZB 35340
IGlkZW9sb2dpY2Fs 35341
IG1heW9yw61h 35342
IGJ1cmVhdQ== 35343
ZXRlcm0= 35344
IERJRA== 35345
7Iq3 35346
IHdhdmluZw== 35347
IGJlYg== 35348
IMOhcg== 35349
INC60LI= 35350
IGVudm95 35351
YW51dA== 35352
0LjQutGD 35353
IEVudmlyb25tZW50 35354
IEFzc2Fzcw== 35355
44KT44Gn 35356
IEJyZWFk 35357
INCi0YPRgg== 35358
IHN0YWlyY2FzZQ== 35359
IERpc2Vhc2U= 35360
IGF1Y3Vu 35361
IOuLiA== 35362
IGNvbmZyb250YXRpb24= 35363
IDE5NDE= 35364
IGlyb255 35365
IHdvcnNo 35366
44KM44KL 35367
IGZpY2s= 35368
IE5hb21p 35369
IGJhY2tzaWRl 35370
aWV1eA== 35371
S2Fw 35372
IHZlZGVyZQ== 35373
IGxlbmd0aHk= 35374
IGJyZWFrZXI= 35375
IFJvbGxl 35376
IHByZWRhdG9y 35377
IG5vc3Nvcw== 35378
IGFkdmVydGlzZQ== 35379
6LOH 35380
0YDQvtC00LU= 35381
UmVkbmVyd2VjaHNlbA== 35382
cmV0ZW4= 35383
IGNvbGxlY3RvcnM= 35384
xLHEn8SxbcSxeg== 35385
IHRyaWc= 35386
IGF4ZXM= 35387
aW50ZXJz 35388
IHBlbmFsdGllcw== 35389
IE9zbWFu 35390
IEplbm5h 35391
IGZsYWtlcw== 35392
IHRyYWluZXJz 35393
IHN0dW5uZWQ= 35394
IFNjcm9sbA== 35395
IFBpcA== 35396
INC90LDRgdGC 35397
IG5ow6A= 35398
IFNtYWNr 35399
4bqrbg== 35400
cmF0b3M= 35401
INGA0LDQsdC+0YLRiw== 35402
IHVjeg== 35403
IExlbW9u 35404
IFNpbmQ= 35405
IHBzeWNoaWM= 35406
IEFiZw== 35407
IG1hbW1hbHM= 35408
IGltbWVyc2l2ZQ== 35409
IGJvdHM= 35410
IHZlcnNjaGllZGVuZQ== 35411
IGdlcmFs 35412
IGZvbGxvd2Vy 35413
IOS7lg== 35414
IHNlZ3VyaWRhZA== 35415
IGltbWVyc2Vk 35416
ZmVpdG8= 35417
Y3Jvc3M= 35418
IMO2bGQ= 35419
7YOE 35420
IOOBk+OBrg== 35421
INeU15nXkA== 35422
IEppYW4= 35423
IGJpbGl5b3I= 35424
YXJlYQ== 35425
IGthZg== 35426
IGdvZHQ= 35427
55u45L+h 35428
IOuwqeyGoQ== 35429
IGRldHJpbWVudA== 35430
5qWa 35431
0ZbQuw== 35432
IMSRw6J1 35433
IGNobG9yaWRl 35434
w7hyZQ== 35435
bGVp 35436
IG1vbnRl 35437
IGRpZmbDqXJlbnRlcw== 35438
4K+BLg== 35439
IGNhcmVnaXZlcnM= 35440
IGluYWRlcXU= 35441
IGZhcmV3ZWxs 35442
INGC0LjQv9Cw 35443
b250ZWM= 35444
IEVwaA== 35445
SEhI 35446
IFRvZG9z 35447
INCh0KjQkA== 35448
IHRyb3Y= 35449
IGxpZ2U= 35450
IGPDtG5n 35451
IENpdg== 35452
IGNhcGF6 35453
IFZhbGxhaGk= 35454
IHF1ZXN0ZQ== 35455
IHJlcGxpY2E= 35456
2LPYqA== 35457
em5h 35458
INGB0LvRg9C2 35459
IFBU 35460
d2F2ZQ== 35461
aWVuaQ== 35462
IHJlbGllZA== 35463
ZGV2ZWxvcA== 35464
IGRlbWU= 35465
IEFtYW4= 35466
IFsuLi5d 35467
IGNvbXBsaW1lbnRz 35468
dWFpcw== 35469
IO2MqA== 35470
IHNtZWxsaW5n 35471
IGRhZHVyY2g= 35472
2YjYqg== 35473
IG9yYW5nZXM= 35474
INC70LDQuQ== 35475
IHN0YWJpbGl6YXRpb24= 35476
5YCN 35477
44KM44Gf 35478
5qW9 35479
IGFwcGxpYW5jZXM= 35480
IGht 35481
g5DrqbQ= 35482
b2R5bmFtaWNz 35483
IGNpxJk= 35484
IENvdHQ= 35485
TU9O 35486
IE1hbmc= 35487
5pSv5oyB 35488
IGFsbGVyZGluZ3M= 35489
zrnOus6u 35490
c2hvdHM= 35491
IHRz 35492
IEfDtnI= 35493
IENIQVI= 35494
IDoo 35495
IHdyYXRo 35496
IGZpcXVl 35497
IGbDvGhyZW4= 35498
IHRlc3RhbWVudA== 35499
IF5e 35500
4bmb4bmj4bmHYQ== 35501
QUxE 35502
IHRleHRv 35503
IERvZ3M= 35504
IHNpYg== 35505
IHBhdGhldGlj 35506
b2Nrcw== 35507
IHJhZGljYWxseQ== 35508
IE1PUkU= 35509
IEpBTUVT 35510
IGluZ2w= 35511
IFRlY2huaWNhbA== 35512
IHBvcmNo 35513
IFVU 35514
INC+0LHRj9C30LDRgtC10LvRjNC90L4= 35515
IHJlbmV3YWw= 35516
IGFlc3RoZXRpY3M= 35517
aWt1bQ== 35518
IGJldmVyYWdl 35519
ZGVybg== 35520
IHByZWRpY3RpdmU= 35521
IGNodXk= 35522
IFJlZ2FyZGluZw== 35523
IEZvcndhcmQ= 35524
INmI2YQ= 35525
IGNvbnRleHR1YWw= 35526
IGR3YXJm 35527
IHByZWhl 35528
IGdvdmVybmVk 35529
hYQ= 35530
IHRyYWJhbGhhcg== 35531
IG5lZ8OzY2lv 35532
INCx0L7Qu9GM0YjQvtC5 35533
0LXRh9Cw0YI= 35534
INC00YPRhQ== 35535
IGZsb29kcw== 35536
IGJvd2xpbmc= 35537
IE9C 35538
IEjDpHI= 35539
IGdyYWRpbmc= 35540
7KO864qU 35541
IGdhcnM= 35542
ZGxpbmc= 35543
IHJhaw== 35544
64g= 35545
Y3JlYXQ= 35546
INGJ0LU= 35547
IG5laWdoYm91cnM= 35548
Zm9vZA== 35549
UXVlcnk= 35550
IGhlcm9pbg== 35551
aWNlcHM= 35552
IEtpbmRh 35553
TkVU 35554
IG1hcmk= 35555
IGltaXRhdGU= 35556
IGFjaHRlcg== 35557
IHNldHRsZW1lbnRz 35558
cmFyZQ== 35559
Y2Npb25lcw== 35560
IOuTnA== 35561
IGZpaw== 35562
aXR1bmc= 35563
INC80LDQutGB0LjQvA== 35564
IGVsZg== 35565
IGRhbGxh 35566
IFBvbHNjZQ== 35567
IFB1bA== 35568
0KfRgtC+ 35569
IE1vcmdlbg== 35570
2K3ZhQ== 35571
IHN1cHJlbWFjeQ== 35572
IGt5cw== 35573
IEh1cnJpY2FuZQ== 35574
IEdUQQ== 35575
IEZlaA== 35576
IGZpbmFsbWVudGU= 35577
bXVuZA== 35578
IEtyaWU= 35579
w6lwb3F1ZQ== 35580
IFR1Y2tlcg== 35581
SVRU 35582
IGx1cg== 35583
IGRpcHBpbmc= 35584
w6R2 35585
IGVlcnN0ZQ== 35586
IEZsaW50 35587
YmlsZHVuZw== 35588
4Li54LmJ 35589
IHRvaW0= 35590
IHByYWN5 35591
IHRyYW5zZm9ybXM= 35592
IHNwZWVkaW5n 35593
IHByZXNlbnRlcg== 35594
IGZlbGxvd3M= 35595
ZmlsbGVk 35596
aWV6YQ== 35597
IGFkdmlzaW5n 35598
IEludGVydmlldw== 35599
0LjQs9GA 35600
d2Vocg== 35601
IERhbnRl 35602
cHR1cmU= 35603
iOusuA== 35604
r7jr 35605
kJA= 35606
IENvdW50ZXI= 35607
IGNyaXN0 35608
IOynnA== 35609
IGpldW5l 35610
INGB0YLRgNCw0Yg= 35611
IG1pZcSH 35612
IHR1dG9y 35613
IG1hc2FsYQ== 35614
IHBvd2RlcmVk 35615
IG5hdQ== 35616
IEZyZWRlcmljaw== 35617
IGJpbGxpbmc= 35618
IEVpc2Vu 35619
INC00L7QsdGA 35620
IG1lc3Q= 35621
5r0= 35622
IHNuaXBw 35623
IG1vbm8= 35624
IEFsbw== 35625
IE1lcmN5 35626
w6lyaWVuY2U= 35627
IGNhc3VhbHRpZXM= 35628
IEFOTk9VTkNFUg== 35629
5LuO 35630
IHRvY2Fy 35631
IGJhY3RlcmlhbA== 35632
SG8= 35633
IHN0cmVhaw== 35634
IEpFTk4= 35635
IHBsYXN0 35636
0YHQu9C10LQ= 35637
IHJlYXBw 35638
IHBheWNoZWNr 35639
IG1pbmVycw== 35640
aGFidA== 35641
IEphcA== 35642
0L3Rg9GC 35643
IHJlZGVtcHRpb24= 35644
IHF1aXI= 35645
aG5saWNo 35646
IGFjY3VtdWxhdGlvbg== 35647
IHNob3Zl 35648
IGFkcmVuYWxpbmU= 35649
TWFrZQ== 35650
IEhlcm4= 35651
b3NzaW5n 35652
IFZpbA== 35653
dWJieQ== 35654
aGVydHo= 35655
YnJlYWtz 35656
IHNwdXI= 35657
IERhaGE= 35658
VVNUSU4= 35659
IGNvbnRpbnVlcg== 35660
IFNhdWw= 35661
44Gu44Gv 35662
IO2PrQ== 35663
IOuQmOuptA== 35664
IOunkOyUgA== 35665
INC+0LY= 35666
IHN1c3BlY3Rz 35667
IGxhcXVlbGxl 35668
IE11Y2hhcw== 35669
IHbDtmxsaWc= 35670
dWxlbg== 35671
IGltcHJlcw== 35672
IGxvYmI= 35673
ZW5lZQ== 35674
INC90LDQtg== 35675
VGE= 35676
IHLDqWFsaXTDqQ== 35677
IFJleA== 35678
IGhhcnZlc3Rpbmc= 35679
IGVzdHI= 35680
5rY= 35681
b3NwYWNl 35682
T1NT 35683
IGRpc3R1cmJhbmNl 35684
YXNzaWM= 35685
IElzYWI= 35686
IGTDqWNvdXY= 35687
IEhhbXBzaGlyZQ== 35688
IG9ybmFtZW50 35689
IGx1w7Ru 35690
IFVX 35691
IGrEhQ== 35692
6YKj5LmI 35693
IHJlc3BlY3Rv 35694
IGNvbXVuaWRhZA== 35695
IGNvbWlnbw== 35696
YWduYQ== 35697
IGludHJpbnNpYw== 35698
IEFsdW1uaQ== 35699
IHNlc2xlcmk= 35700
IGVzdGltYXRpb24= 35701
4oCU4oCU 35702
IHByb2R1aXQ= 35703
44CC44CN 35704
INCy0YA= 35705
IHdoaXJs 35706
IGFjY2Vz 35707
w6d1 35708
IHZhcmlhYmlsaXR5 35709
IHZvZGth 35710
aXRzdQ== 35711
IGludGVybnNoaXBz 35712
IGFsbG9jYXRl 35713
UlI= 35714
7ZuI 35715
IGluc3RydWN0aW9uYWw= 35716
dGFudA== 35717
IOCuheCupA== 35718
IGludml0ZXM= 35719
IGhhaw== 35720
IHNjYXJlcw== 35721
IGVjbGlwc2U= 35722
0L/QvtCy 35723
0LrQvtC70Yw= 35724
YXRpdmFz 35725
IHN0YWJiZWQ= 35726
IERPTQ== 35727
5LiN5Yiw 35728
cm9vdHM= 35729
IFBpY3R1cmU= 35730
7Zi8 35731
IENIQQ== 35732
aWVj 35733
xLHEsQ== 35734
aGFub2w= 35735
IG1pc3VuZGVyc3RhbmQ= 35736
UmF5 35737
IHJvYWRtYXA= 35738
b2N1bWVudGVk 35739
aXppb25l 35740
IE9saXZl 35741
cmlmdA== 35742
INeU16A= 35743
5q+N 35744
bGVzdA== 35745
Ozs= 35746
IEVB 35747
6ZyA6KaB 35748
0L7QtNGD 35749
IGhvYmJpZXM= 35750
IGJ1cmlhbA== 35751
44Gr44Gh44Gv 35752
0KQ= 35753
bGVnZQ== 35754
IEhK 35755
IG9iamVjdGlvbg== 35756
IOOBrQ== 35757
Y3Rvcnk= 35758
IGluY3JlbWVudGFs 35759
IGd5bW4= 35760
IGVwaWRlbWk= 35761
0YHRi9C7 35762
w5E= 35763
IGFkdmFuY2VtZW50 35764
IHBhcmNo 35765
TmV3cw== 35766
IGF5cg== 35767
0LvQsNC8 35768
INec16k= 35769
IGRpcGxvbWE= 35770
44Gh44KD44KT 35771
IHJvYmJlZA== 35772
T25seQ== 35773
IGluY3Vy 35774
IGNoYW50aW5n 35775
IO2VtOuPhA== 35776
IHJpY2hlcw== 35777
IENhcm1lbg== 35778
IG5vc3Rybw== 35779
zrvOrQ== 35780
IFBvd2Rlcg== 35781
4LmA4Lir 35782
IOyeiOycvOuptA== 35783
IGdlcsOnZWt0ZW4= 35784
IFBpa2FjaHU= 35785
0LXQvNC+0L0= 35786
T0xM 35787
IHBsYW5ldGFyeQ== 35788
IHNsb3dz 35789
IGNsb2Nrd2lzZQ== 35790
YWxpb24= 35791
IOyM 35792
IHZlcm4= 35793
IGhvbW1l 35794
IGVuZHBvaW50 35795
IGlubm9jZW5jZQ== 35796
IGVsZW1lbnRvcw== 35797
IHNvcGhvbW9yZQ== 35798
IG5vdGlvbnM= 35799
IENvdWxkbg== 35800
cHVy 35801
IHphdA== 35802
IG9ic2Vzcw== 35803
IG1vdGl2bw== 35804
IEt1Yg== 35805
IERydWc= 35806
QW50 35807
IFBsYXllcnM= 35808
IEh1bWFucw== 35809
IG1lbGVl 35810
IFdpbGRsaWZl 35811
IFZQ 35812
IHZvbGNhbmlj 35813
IGNvbWlu 35814
IEd1YW5n 35815
IM+EzrnPgg== 35816
INC+0YHQvtCx0LXQvdC90L4= 35817
IFNpemU= 35818
TGlzdGVu 35819
IEFhYQ== 35820
YXBwcm8= 35821
IGJhcmJhcg== 35822
IFBhcmtpbnNvbg== 35823
0L3Rj9GC0Yw= 35824
5Y2w 35825
IHVuZGVyZXN0aW1hdGU= 35826
IHN1YnN0aXR1dGlvbg== 35827
IGNvc21ldGlj 35828
5LiL5qyh 35829
IHdpbGxlbg== 35830
IGJlaWRl 35831
YW5uaQ== 35832
IGNvbmRpdGlvbmVk 35833
IERlYmJpZQ== 35834
IGlzdG8= 35835
IEVkd2FyZHM= 35836
7JuM7JqU 35837
INGC0L7Qsg== 35838
IGFiYnJldmk= 35839
IE3DvG4= 35840
IFByaW5j 35841
IExpYW5n 35842
IHN0aW5r 35843
IHJhZGlvYWN0aXZl 35844
44GG44KP 35845
IGFjb250ZWM= 35846
IHVuY29u 35847
IFR1cmJv 35848
44GQ 35849
IGtpc3Nlcw== 35850
5piv5LuA6bq8 35851
0LXRgtGA0L7Qsg== 35852
IGZyb250aWVy 35853
IFNweQ== 35854
IEJlbGFydXM= 35855
IENCUw== 35856
4buX 35857
YW1vdG8= 35858
7ZWc642w 35859
INGB0YLRgNC+ 35860
IEVuZmlu 35861
IGJyZWFkdGg= 35862
6Ziy 35863
IENhZmU= 35864
IERhZsO8cg== 35865
IEJvdXI= 35866
YXJhcw== 35867
IGJsdWVwcmludA== 35868
YW7EsQ== 35869
IGNvbnN0YW50cw== 35870
IGF0dGFja2Vy 35871
IEZvcm11bGE= 35872
emHEhw== 35873
IHNvd2ll 35874
IGV5ZWJyb3c= 35875
b2Jvb2s= 35876
IHNldHplbg== 35877
56ys5LiJ 35878
b25zaWRlcg== 35879
YXduaW5n 35880
IHPDtnlsZXll 35881
IGludmFkZWQ= 35882
IHByb25vdW5z 35883
IGRvYnJ5 35884
U2k= 35885
INCl0L7Rgg== 35886
IHZvbGxleWJhbGw= 35887
IGxhbWVudA== 35888
aXNjaGVz 35889
YXJtZQ== 35890
YXBp 35891
IFdpa2k= 35892
0LvQuNGI 35893
IGthc2lo 35894
IHBlc3M= 35895
INGE0L7Rgg== 35896
IFN1bA== 35897
5b63 35898
IHBzZXVkbw== 35899
IG1lbW8= 35900
IOyXsOyKtQ== 35901
INC00L7Qu9C70LDRgNC+0LI= 35902
INC/0LXRgNC10Lw= 35903
IFJlYWNo 35904
bWlyYWw= 35905
YWx0ZWQ= 35906
IHN0YXR1dA== 35907
cmVhZGluZw== 35908
IHPDtnlsZWQ= 35909
IExpbmRzZXk= 35910
IEFobWFk 35911
67aA6w== 35912
INCh0LXQs9C+0LTQvdGP 35913
IHByenlnb3Q= 35914
IGh5c3Rlcg== 35915
VVJF 35916
IE5laWdo 35917
UmVwb3J0ZXI= 35918
IEJ1bnU= 35919
IFRyZWF0eQ== 35920
IFJhbms= 35921
IEZhbWU= 35922
aW5pc2hlZA== 35923
IGdlYXJlZA== 35924
IGNvbXBvc2U= 35925
b2RpYQ== 35926
IExvbg== 35927
IGplc3RlxZtteQ== 35928
IERJUkVDVE9S 35929
IGVsa2Fhcg== 35930
IFZpZWw= 35931
15DXqQ== 35932
eW50aGlh 35933
5Lim 35934
IG3DqHJl 35935
IFRvbWF0bw== 35936
IGV4YXRhbWVudGU= 35937
bmnEmQ== 35938
IEZyZWk= 35939
IERpZg== 35940
IG9wZW5pbmdz 35941
IGdyYXBoaWNhbA== 35942
INGD0LTQvtCx 35943
INCy0YHQvw== 35944
IFdlZWtseQ== 35945
0LXQstCw 35946
IGhhbmdz 35947
IHVuc2FmZQ== 35948
IGVtYmxlbQ== 35949
IEtvbGxlZ2lubmVu 35950
YWxheQ== 35951
IGtzaQ== 35952
IGhpZGVz 35953
IG9sbWF5 35954
IGVudHN0ZQ== 35955
IGFydGhyaXRpcw== 35956
w59lcmRlbQ== 35957
IGJpbm5lbg== 35958
IGxpc3RlbnM= 35959
IEhlc3M= 35960
5YaN5L6G 35961
IExvdWlzZQ== 35962
bGRlbg== 35963
0LXQvdGB 35964
IFZlcnNpb24= 35965
IEFncmljdWx0dXJl 35966
7Iqk66W8 35967
0LzQsNC9 35968
64Sk7JqU 35969
IHdpbmVz 35970
IElORg== 35971
cnVs 35972
IEpL 35973
xLF5b3JsYXI= 35974
c2hpZWxk 35975
cmVhdGg= 35976
IHRlcnVz 35977
IEx1bQ== 35978
IGFudGljaXBhdGlvbg== 35979
IGFjY3VzdG9tZWQ= 35980
IE1pbmE= 35981
IHdpZWxk 35982
aW/DqA== 35983
bWVyYQ== 35984
IGNvdW50ZG93bg== 35985
IGNsaW5n 35986
IGNvbW1lbmQ= 35987
IGZha3Rpc2t0 35988
IGRlZmVuc2Vz 35989
IGNvY2twaXQ= 35990
INC60L7QvNCw0L3QtA== 35991
IGRpc2h3YXM= 35992
IFRoYW5vcw== 35993
IGtpZG5leXM= 35994
IHNlaGU= 35995
IG1pY3JvYmVz 35996
IGN1ZmY= 35997
INCy0YvRgdC+0Lo= 35998
IFNwaWN5 35999
562J562J 36000
4K614K6w 36001
Y3VsdXM= 36002
b3Jj 36003
576F 36004
aXhlcw== 36005
IENyZWRpdA== 36006
IHJhag== 36007
IGJyaW5ndA== 36008
IE5pc3M= 36009
IGdyaW0= 36010
IFNPTA== 36011
IHRlbmlt 36012
IFN1ZGFu 36013
IFNwYXJ0 36014
IHByb21vdGVz 36015
IE5vc3Nh 36016
INGB0L7RgdGC0L7Rj9C90Lg= 36017
IOywqQ== 36018
IHVuY29udA== 36019
IExpYmVyYWw= 36020
INCi0L7Qu9GM0LrQvg== 36021
IFZpZWxl 36022
IGt0w7NyZWo= 36023
ICoqKio= 36024
TWF4 36025
INCn0YLQvtCx0Ys= 36026
MzUw 36027
IO2YvOyekA== 36028
IOu2hOuTpOydtA== 36029
IHdhcnA= 36030
IHRlbmdh 36031
IHN5bXBhdGhldGlj 36032
IGJpemk= 36033
IFphY2s= 36034
aWVkbw== 36035
IOuJtOw= 36036
cGllbA== 36037
INGC0L7Quw== 36038
IHNjYWxlZA== 36039
IFBFVEVS 36040
IENPTU0= 36041
IENhbWU= 36042
IGNhdGFzdHJvcGhl 36043
IHN3ZWF0eQ== 36044
aWdyYXRpb24= 36045
IHN0dWZmaW5n 36046
IM+Azr/Ou8+N 36047
IERyaXZlcg== 36048
enlzdA== 36049
VGVjaA== 36050
IGFzc2Vzc2Vk 36051
IFN1cmZhY2U= 36052
xLFyxLFt 36053
c3Vy 36054
bGVyd2VpbGU= 36055
INC00L7Qsw== 36056
IHNodXR0aW5n 36057
IGZyYWN0aW9ucw== 36058
INGB0L7Quw== 36059
ZXZlcnlvbmU= 36060
IGVybg== 36061
INCd0L7Qsg== 36062
IGRlZmVuZGVycw== 36063
IHZlcnN1Y2h0 36064
44Oz44OA 36065
IHBvbGl0eQ== 36066
INCf0L7QvQ== 36067
dmVyc3TDpG5k 36068
IGJyb3dzZXJz 36069
IHRyYW5zZm9ybWF0aXZl 36070
IGRpY3RhdGU= 36071
IExFR08= 36072
IG5pbmd1bmE= 36073
6rSR 36074
IHBpeno= 36075
IEhhcm9sZA== 36076
IExvcGV6 36077
2r7bjA== 36078
YW7EsXo= 36079
YXRjaGV0 36080
2YrYqg== 36081
IGxlcm5lbg== 36082
IOq3gOyXrA== 36083
IGhvdXNlZA== 36084
IGNsZWFuc2U= 36085
IFdBVA== 36086
bGFyYXRpb24= 36087
IGJ5dGVz 36088
IHR1Y2tlZA== 36089
IGZhdWx0cw== 36090
0LTQvg== 36091
Rlg= 36092
IOyWvOuniOuCmA== 36093
IGRlZm9ybQ== 36094
IGNvbnRyYWN0aW5n 36095
IFRJTUU= 36096
aXJzZQ== 36097
IG5lYmVu 36098
IGNlcmM= 36099
IEFybXN0cm9uZw== 36100
IHRlc3Rlcg== 36101
IHBhcmZhaXQ= 36102
IGplYWxvdXN5 36103
IHRveGlucw== 36104
IGRpc2JlbA== 36105
0YPRgNGL 36106
aW1wcmVzc2lvbg== 36107
IHByb3N0YXRl 36108
IGZpcmV3YWxs 36109
IGNsYXNzaWNz 36110
0LXRh9GM 36111
IHNvY2lhbGlzbQ== 36112
IGdyYWNpb3Vz 36113
INGB0L3QvtCy0LA= 36114
INC00L3Rjw== 36115
IGJ1cm5lcg== 36116
IE1pbm9y 36117
IOyasOumrOs= 36118
IGplZGVz 36119
IGNvbnRpbnV1bQ== 36120
IGhvdHM= 36121
IG9jY3VycmVuY2U= 36122
IGFkbWluaXN0ZXJlZA== 36123
INC30LDQvNC10YI= 36124
IGhlc2l0YXRpb24= 36125
IGRyaWxscw== 36126
ZXJjYQ== 36127
INCy0YLQvtGA0L7QuQ== 36128
IHN0ZWFkaWx5 36129
IGluc2FubGFy 36130
IGloYW4= 36131
7ZE= 36132
IGhlbHBlcg== 36133
IFNlbmlu 36134
5YGc 36135
0L7QstCw0L3QuNC1 36136
IEVSSUM= 36137
Ymxh 36138
IEFjYWRlbWlj 36139
IGh1bWFuaXRpZXM= 36140
YmxhY2s= 36141
dW1weQ== 36142
b3J0ZXg= 36143
IOygiOs= 36144
INil2YY= 36145
IGRpc2Nsb3Nl 36146
IEVsaWphaA== 36147
IM67zq0= 36148
IFF1ZXI= 36149
2KjZhA== 36150
44Kh 36151
VGVsbA== 36152
YXJsZQ== 36153
0ZbRgA== 36154
IGF1Z21lbnRlZA== 36155
IOu5hOyKtw== 36156
IGFuZHJvaWQ= 36157
4KSk 36158
YXJtYQ== 36159
IHN6ZXI= 36160
Z2VvcmQ= 36161
IGdlZWs= 36162
IHlldXg= 36163
IHBvbmc= 36164
IOOBneOBhg== 36165
IHRvcnR1cmVk 36166
IEJhdGg= 36167
emln 36168
YXNvbmFibGU= 36169
IG5ldHM= 36170
IGJhcnU= 36171
IEZsYXQ= 36172
IFZhdGVy 36173
IFRlcnJvcg== 36174
IEF2bw== 36175
IGNlcmVtb25pZXM= 36176
cm9l 36177
2YHYsw== 36178
T3Bz 36179
IGh5dmlu 36180
IGFwcmVzZW50 36181
b2xvcg== 36182
INC40LPRgNGL 36183
b3J0b24= 36184
IOq3uOuerA== 36185
IGxvb2tpbg== 36186
IFRZ 36187
IE1pbnQ= 36188
QWRk 36189
IG1pdGU= 36190
IFNtb2tl 36191
IG5vdGE= 36192
IG1vc3M= 36193
IEFiZW5k 36194
IOy7qA== 36195
IGV4YWdnZXJhdGVk 36196
ZmlyZXM= 36197
IHJlZGlzdA== 36198
ZmZpdGk= 36199
IG9wZW5uZXNz 36200
6rCQ7J20 36201
ZW5kZXU= 36202
0LXQvdC90L7QuQ== 36203
V2F0Y2g= 36204
IGF2YXRhcg== 36205
IFBleQ== 36206
dXJ1bg== 36207
IHNlbnph 36208
IOyngOyXrQ== 36209
IE5hdG9taWFzdA== 36210
IGVtZXJnZW5jZQ== 36211
cmF5cw== 36212
IGNyYWZ0ZWQ= 36213
Z2FyeQ== 36214
44Gg44GR 36215
w7xuZw== 36216
LSI= 36217
IGhhY2tlZA== 36218
IHN0cmF5 36219
ZW5jaWU= 36220
ZW1v 36221
IGNvbWVu 36222
IEvEsXo= 36223
IEphc21pbmU= 36224
IEhpbmRp 36225
bWFuYXM= 36226
IGluZmluaXRlbHk= 36227
ZW1vbg== 36228
7J24642w7JqU 36229
amFr 36230
IHJvYXJpbmc= 36231
w6lyaXF1ZQ== 36232
c3dlaXNl 36233
IFJvbGV4 36234
5aCx5bCO 36235
IFN0dWFydA== 36236
Ym5i 36237
IGRpYWdub3Nl 36238
IGNvaGVyZW50 36239
IE1K 36240
5rqW5YKZ 36241
IHBpa2U= 36242
bGF2 36243
IG9yY2hlc3RyYWw= 36244
0LDRgdGC0Lg= 36245
IHRlcm1pbmFy 36246
IGdhdGhlcmluZ3M= 36247
IGNvbXBsaWFudA== 36248
IHVwZ3JhZGluZw== 36249
IHJlZ3VsYXRvcg== 36250
IGxhbsOn 36251
6YCj 36252
IG1lcmNoYW50cw== 36253
dGF3YQ== 36254
IG1vbml0b3JlZA== 36255
IHJlbmRyZQ== 36256
5Lik 36257
IHVudGVyd2Vncw== 36258
YW5ndWFyZA== 36259
Z2FyZA== 36260
IEJlbG93 36261
ZHVpbm8= 36262
INCm0LU= 36263
IGltcGVkYW5jZQ== 36264
7Jyh 36265
5Lu9 36266
IGFrdHVlbGw= 36267
IFZhdGlj 36268
5a2p 36269
IHN0ZXdhcmRz 36270
IGJyaWdodGVzdA== 36271
IGtlbm4= 36272
IGthdQ== 36273
IE1hdHJpeA== 36274
IEJhcms= 36275
IPCfkQ== 36276
IHRhcGVy 36277
IGNhc2lubw== 36278
16jXlA== 36279
eXNpY2Fs 36280
IGJ1aWxkZXJz 36281
IGN6xYJvd2ll 36282
IE5lcGFs 36283
ICEi 36284
IHRlcm1l 36285
IGlubnljaA== 36286
IG1hdGhz 36287
IGRyYWZ0ZWQ= 36288
IEJhbGs= 36289
IGhlc2l0YW50 36290
IHZvbHRhcg== 36291
IHJldml2ZQ== 36292
INGE0LjQu9GM0LzQsA== 36293
IGFzc2Fzc2lu 36294
IFNvbHV0aW9ucw== 36295
IGR1ZWw= 36296
IGJlYXJpbmdz 36297
4LiE4Liw 36298
IHJvb2tpZQ== 36299
aWthdA== 36300
IGJpc2N1aXRz 36301
IGNvcmRz 36302
0YPQstCw0YLQuA== 36303
QVJJTg== 36304
IHByb2dyZXNzaW5n 36305
IEdpcg== 36306
IHBlbmV0cmF0ZQ== 36307
IFN0b3JhZ2U= 36308
ZWlnaHQ= 36309
INGC0YDRgw== 36310
IGRvbsOtdA== 36311
IHNpemlu 36312
IG91dGRhdGVk 36313
INC90LDRiNC4 36314
IGFmZmly 36315
IHNwb29ucw== 36316
IG9uaQ== 36317
IGZsYW5r 36318
IEdvbA== 36319
aMOj 36320
IHDDqXJp 36321
IGhvbm9yYWJsZQ== 36322
IEJyZWF0aGU= 36323
c2NlbmVz 36324
IG9idmlhbWVudGU= 36325
0LjQutGB 36326
INep157X 36327
IHNtb290aGll 36328
nojr 36329
IGRpbWU= 36330
IO2WiOyWtOyalA== 36331
IGFwcGVs 36332
IENhdGhvbGljcw== 36333
IHNpbmdsZXM= 36334
IGxhdGVu 36335
IMOnw7xua8O8 36336
IFZhZGVy 36337
5o+b 36338
IHZhcmTEsQ== 36339
IElzdGFuYnVs 36340
Z3LDqQ== 36341
IEVsc2E= 36342
w6ts 36343
IGludmVjZQ== 36344
IGNyYW5l 36345
IG9iZQ== 36346
IFNoYXJr 36347
IHNtYWNr 36348
IHJlc3RvcmluZw== 36349
Llw= 36350
IOu5oOs= 36351
IGZhZGVk 36352
dW1iZXJz 36353
U2luZ2luZw== 36354
IGRlcHJlc3Npbmc= 36355
dGhlc3Q= 36356
IFdhaHI= 36357
IG11bHRpdHVkZQ== 36358
0YDQsNCy0YHRgtCy0YPQudGC0LU= 36359
cmlqaw== 36360
ZWth 36361
IGNvbXBsZXRlcw== 36362
IFdlbGxz 36363
IHJveQ== 36364
IFByYXk= 36365
IEthbGF1 36366
aXppbg== 36367
aWHFgmVt 36368
IGxvY29t 36369
IE5hc2h2aWxsZQ== 36370
IFBlbnRhZ29u 36371
66+4 36372
IE5FVw== 36373
xIXEhw== 36374
w61zcw== 36375
IG1hcnJ5aW5n 36376
IGZldWQ= 36377
7ZmV 36378
5oCl 36379
KSE= 36380
IE9wZXJhdGlvbnM= 36381
0YPRlA== 36382
IG1vamU= 36383
IGluc3RydWN0ZWQ= 36384
IOuIhOq1rA== 36385
INeU15I= 36386
INC/0L7QvNC+0YnRjNGO 36387
IHNhYmlh 36388
7JWY7Ja07JqU 36389
cGxhbmU= 36390
cHJp 36391
INC/0L7Qu9C90L7RgdGC0YzRjg== 36392
IEtpdHR5 36393
IHByw7Nwcmlv 36394
ZWRlcmU= 36395
IGludGVyZXNhbnRl 36396
INC00LU= 36397
IGNvbmRlbnNlZA== 36398
IGF2ZW50 36399
VE9S 36400
IGdyZWFzeQ== 36401
QVJL 36402
b3J0YQ== 36403
QUo= 36404
IGRpc3JlZw== 36405
IGNvcnJlY3Rpb25z 36406
IHN0ZXJv 36407
IGluZmx1ZW56YQ== 36408
IGRlc3Nlcw== 36409
IGJhbGxvdHM= 36410
IG1lZ2V0 36411
IG1hZmlh 36412
IGLDtmw= 36413
bm9zdA== 36414
INGB0YLQsNGC0Yw= 36415
IHJlc3BvbmRlcg== 36416
IGhpbnRlbg== 36417
Z3Jhdg== 36418
4Lit4Liw 36419
eW5jaHJvbg== 36420
IHZpZW5z 36421
IHNhbW8= 36422
IGR0 36423
cGFubnQ= 36424
IMWbd2lhdA== 36425
INC30LDQv9C40YE= 36426
IG1lcmdlZA== 36427
IGtlcA== 36428
IG1pc2xlYWRpbmc= 36429
IGRpZ2Ftb3M= 36430
IGFtbW9u 36431
6L6b 36432
Y2hldA== 36433
IOqwgOyguA== 36434
IHVuaQ== 36435
IOuQmOuKlOuNsA== 36436
INC90LDQv9GA0LDQsg== 36437
INC60L7RgtC+0YDQvtCz0L4= 36438
IGFuaW1hdGU= 36439
15XXkNc= 36440
0LXRgNCy 36441
IG1pbmNlZA== 36442
IGthdW0= 36443
44GC44GB 36444
z4DOtQ== 36445
0LvQtdCz 36446
ZXhpc3Rpbmc= 36447
IHBsYXRhZm9ybQ== 36448
IEtSSVM= 36449
7Jug 36450
IEZhbWlsaWVu 36451
IExpYnlh 36452
IGJpb2RpdmVyc2l0eQ== 36453
IGlkaW90cw== 36454
aXJkaQ== 36455
IHN6eWI= 36456
IFJvbGxpbmc= 36457
w7xjaHQ= 36458
INGD0LTQuNCy 36459
0YHRg9C0 36460
IHJlYWxpemFy 36461
IGNhbm5lZA== 36462
INGA0LDQvQ== 36463
IG1ldGFib2xpYw== 36464
IEJlZWY= 36465
IGtpbGth 36466
0LvRjtGB 36467
IHJlZ2lzdHJ5 36468
0LzQvtGC0YDQuNGC0LU= 36469
IHZpZWzDpA== 36470
IG9kYw== 36471
IGNvbmRlbW5lZA== 36472
5qmL 36473
ZmFs 36474
IERpbA== 36475
d2/Fm2Np 36476
QXc= 36477
IHN0YXRpc3RpY2FsbHk= 36478
IHNvZ2Vu 36479
IEJFVEg= 36480
IHNoYXZpbmc= 36481
5bm4 36482
b2NhbA== 36483
IEZ1bm55 36484
IHBlYWNlZnVsbHk= 36485
IGFkZGljdGl2ZQ== 36486
IEluc2VydA== 36487
bGF1Zg== 36488
IGV4cGVyaWVuY2lh 36489
6aaW5YWI 36490
0LjRgtC10LvRjw== 36491
w61nZW4= 36492
w6FnaW5h 36493
IGFiZG9tZW4= 36494
7ZWc64uk 36495
aWN1cw== 36496
aW1hbmE= 36497
7I2o 36498
YXJjaGluZw== 36499
IGtvbmtyZXQ= 36500
7JWY6w== 36501
0LXQutCw 36502
b3VmbA== 36503
aXZlbA== 36504
IG51ZGU= 36505
w6h0cmVz 36506
IG1vbnNpZXVy 36507
IGNsYXNo 36508
IHRoZXJhcGlzdHM= 36509
IGN1YmVk 36510
IHJldHJvdXZlcg== 36511
IHdhdmVmb3Jt 36512
IHBvdGVt 36513
IEZvcm1lcg== 36514
aXNpw7Nu 36515
5bqc 36516
INeQ150= 36517
dW5kb3M= 36518
IE1laW51bmc= 36519
2LXZhA== 36520
IEp1ZGU= 36521
IG7DpXI= 36522
IExlb25hcmRv 36523
IENyaXN0bw== 36524
IEdPVA== 36525
0YHRgtGA0YPQug== 36526
TEFO 36527
IGfDpW5n 36528
IGTDqWI= 36529
IEZyYW5rZnVydA== 36530
IGNyYXBweQ== 36531
IGxpbA== 36532
YW5uw6ll 36533
INC80LXRgdGC0LU= 36534
UkVU 36535
IE5lcg== 36536
IENPU1RB 36537
IGplZGVt 36538
IGN1cnRhaW5z 36539
IGl0ZXJhdGlvbnM= 36540
IHVuYXY= 36541
IHBsYXF1ZQ== 36542
b3J1bQ== 36543
IM62 36544
IG7Dum1lcm9z 36545
IGRlc2Fw 36546
sr0= 36547
IGNvbXBpbGVk 36548
IHJlZmxl 36549
IHJhbmtpbmdz 36550
IHJlcGFpcmVk 36551
INCd0LDQv9GA 36552
IGRvd25sb2Fkcw== 36553
IGFybW91cg== 36554
INeZ15XXqteo 36555
IGxvbmdldml0eQ== 36556
IFRPTkVS 36557
INC60L7QvNC80LXQvdGC0LDRgA== 36558
IGN6ZWdv 36559
IG5vdGlmeQ== 36560
IGFpcnBvcnRz 36561
IGVuZHVyaW5n 36562
bGV0dGU= 36563
IGFwcGFyYXQ= 36564
IGhhYmls 36565
4buHYw== 36566
bmFk 36567
SUNP 36568
IEJyYWg= 36569
IHNlZ8O6bg== 36570
IGdvdmVybm9ycw== 36571
a2FoYQ== 36572
IFNjaGx1c3M= 36573
IG9kcG93aWVk 36574
aXJ0aW5n 36575
IHJlbXBs 36576
IEFib3JpZ2luYWw= 36577
aWRlbnRhbGx5 36578
IGVuaGFuY2luZw== 36579
bGljdGluZw== 36580
IEhhd2FpaWFu 36581
IHN0cml2aW5n 36582
IE5pZXQ= 36583
IHpuYWN6eQ== 36584
IG9iZWRpZW5jZQ== 36585
IG7DpWdvdA== 36586
IGV4cGlyZWQ= 36587
IDE5MTg= 36588
cHJlc2VudGVk 36589
IHByb3dhZA== 36590
IFRlcnI= 36591
IFByaW5jZXRvbg== 36592
IG1vcmdlbg== 36593
IGF0dHJhY3Rpbmc= 36594
IFNpZ21h 36595
aWduZXI= 36596
IFJlY2h0cw== 36597
IFBla2k= 36598
IG1ldGh5 36599
IGhhbW0= 36600
IGRpcmVpdG8= 36601
IGRlbGVnYXRpb24= 36602
0LjQstCw0Y7Rgg== 36603
IGdpbg== 36604
WW91bmc= 36605
IGRlcGVuZGVuY2llcw== 36606
IEJyYWRsZXk= 36607
YnVkcw== 36608
IGZpcw== 36609
IHB5dGFuaWU= 36610
IGludGVyY29ubmVjdGVk 36611
IGVtYmFpeG8= 36612
IFNhcw== 36613
IHJ1aA== 36614
IFNpY2h0 36615
U3Vy 36616
IHN1cGVyYg== 36617
IFNhYmJhdGg= 36618
IERhbmdlcg== 36619
a29s 36620
IGhvdQ== 36621
c3VwcA== 36622
IE5hY2lvbmFs 36623
IHN1Y2Nlc3Npb24= 36624
IHbDoQ== 36625
IE1hw59uYWhtZW4= 36626
IEplc3NpZQ== 36627
IElkYWhv 36628
Zm9yZXN0 36629
hZg= 36630
INee15M= 36631
INij2Yo= 36632
IHN3ZWV0aGVhcnQ= 36633
IG5lYXRseQ== 36634
IEV2YW5nZWw= 36635
6rOh 36636
IFN1aXRl 36637
w7pibGljYQ== 36638
INGD0LvQuA== 36639
IEFubm91bmNlcg== 36640
bGlnaA== 36641
IHNlbnNhdGlvbnM= 36642
IHNoZWx0ZXJz 36643
IGhhcnQ= 36644
IHNxdWVlemluZw== 36645
IFJpdmVycw== 36646
IENvb2tpbmc= 36647
7LGF 36648
cGVyc29uYWw= 36649
IG1hbm9z 36650
0ZHRgtGB0Y8= 36651
d2lq 36652
IGdvZ2c= 36653
IE1pbGxp 36654
IEZQ 36655
w7xuc3Q= 36656
IExT 36657
IHNwcmF5aW5n 36658
IGZhdXg= 36659
IGF1dG9ncmFwaA== 36660
b2xvZ2lj 36661
IHRvcm1lbnQ= 36662
IGVuY3J5cHRlZA== 36663
4buF 36664
IGVzdHJl 36665
57m8 36666
4LE= 36667
IHN0dW1ibGVk 36668
IGFpZGVy 36669
IHNhYmVu 36670
eHRlcg== 36671
IENpdGllcw== 36672
IFTDvHJr 36673
64ul 36674
Y2hpbmU= 36675
IHRvcHBpbmc= 36676
IHBvaXNvbmVk 36677
IFJvbWFuaWE= 36678
15PXmQ== 36679
gOuhnA== 36680
INC/0L7RgNGP0LQ= 36681
IGNoaXJwaW5n 36682
IOyZhOs= 36683
15HXog== 36684
IGN1YW50bw== 36685
IGRvbmF0aW5n 36686
IFJlZ2VudA== 36687
IEJlcnVm 36688
IGRpc3RyYWN0aW5n 36689
IHN0YW1pbmE= 36690
IERhcnJlbg== 36691
IOy2lQ== 36692
bGlzdHM= 36693
ZGFs 36694
Y2h1c3M= 36695
IGVjb25vbWlzdA== 36696
44GI44O8 36697
b3JndA== 36698
IGlzdGl5b3J1bQ== 36699
6L+b 36700
IFN1cnByaXNl 36701
IEhhbw== 36702
IOy1nOqzoA== 36703
IEdX 36704
IElubmVy 36705
IHF1aWVyZW4= 36706
IG1pbmRlZA== 36707
IHN1cGVyY29tcHV0ZXI= 36708
IGRpYWdyYW1z 36709
7Yqc6w== 36710
6rKg7Ja0 36711
INC+0LHRitGP0YE= 36712
IGVzdGFiYW4= 36713
IGRlc3Ryb3lz 36714
IEJyZWFraW5n 36715
IGthcsSxxZ8= 36716
IHJlYnVpbGRpbmc= 36717
nOuMgA== 36718
0LvQuNCy0L4= 36719
IFNhdWNl 36720
IEZ1c2lvbg== 36721
15XXntc= 36722
IFF1aW5u 36723
IGdhdWNoZQ== 36724
INmI2KM= 36725
IMg= 36726
55Oc 36727
IHRlY2hubw== 36728
IGRpc3BhdGNo 36729
IGHFn2s= 36730
IGVpbnplbA== 36731
IEdtYWls 36732
554= 36733
IOqwnOyduA== 36734
INGB0LXQvNGM 36735
IGpvdXJuZXlz 36736
IGlodA== 36737
IGZpYnJl 36738
IGRyYW1hcw== 36739
b3VjaGVk 36740
IHJlbmFtZQ== 36741
INC+0L/QtdGA 36742
IHBvbw== 36743
IERydQ== 36744
INC40YLQvtCz 36745
IHphc3Q= 36746
IGNveg== 36747
IHp1Y2No 36748
IG9idGFpbmluZw== 36749
IGNvbW11dGU= 36750
IHN1Ym1lcg== 36751
IFZpc2g= 36752
IFJhYmI= 36753
b2dn 36754
IGh1dA== 36755
7ZaI7Ja0 36756
5q+U5aaC 36757
ZXJlbWk= 36758
IM68zrE= 36759
IGRpc2t1dA== 36760
INCx0YPQug== 36761
IGltcGFpcmVk 36762
ZGVwZW5k 36763
INmI2Kc= 36764
INGA0YPQug== 36765
INCx0LDRgA== 36766
IG94aWRhdGlvbg== 36767
IHNpdHVhw6fDo28= 36768
yZlu 36769
dcOnw6Nv 36770
IHNhZ3Rl 36771
IFNFUg== 36772
IENha2U= 36773
IHR1cm1lcmlj 36774
IEthaw== 36775
YnVuZw== 36776
IEvhuZvhuaPhuYdh 36777
IHBvaXNvbmluZw== 36778
IHNsaXBwaW5n 36779
IFNheXM= 36780
5bCx5Y+v5Lul 36781
w7JuZw== 36782
55+z 36783
wqs= 36784
IENsYXVkaWE= 36785
IENoYXJhY3Rlcg== 36786
0L3QuNGG 36787
Y29hdA== 36788
IHByb2dyZXNzZWQ= 36789
IEZlcmd1cw== 36790
IOyYpOuK 36791
IG9hdA== 36792
b3JkYWJsZQ== 36793
IExleQ== 36794
IEhlcmF1cw== 36795
IHJlc3VsdGFkb3M= 36796
IEtheWxh 36797
IHJpZmY= 36798
IGNoZWdvdQ== 36799
IHhp 36800
IHNwYWNpb3Vz 36801
IHJlY29nbmlzZWQ= 36802
IGVjaA== 36803
IFRpZQ== 36804
IGxhdW5jaGVy 36805
Smlt 36806
IHN1cHByZXNzaW9u 36807
IEltcG9zc2libGU= 36808
IGd1aXRhcnM= 36809
IEZvdXJpZXI= 36810
0LjRh9C10YHQutC40Lk= 36811
IFRoZXJhcA== 36812
IEthZg== 36813
Y2VudGVyZWQ= 36814
INGB0L7QvtGC0LLQtdGC 36815
IGtsaW0= 36816
IGNhcmJvaHlkcmF0ZXM= 36817
aWduYW50 36818
IEFzdHJvbg== 36819
IGVtcGxl 36820
IGRyYXN0aWM= 36821
INC80LjRgNC1 36822
0LLQuNC9 36823
dXc= 36824
IHByZXR0aWVy 36825
IGRvbnV0cw== 36826
IEF0aGVuYQ== 36827
IGRpc3NlcnQ= 36828
IHBsYW50ZQ== 36829
IHVyYW5pdW0= 36830
7J2M6w== 36831
YXLDqQ== 36832
IHJ6ZWN6 36833
IGRpc3BsYXlpbmc= 36834
5oiy 36835
IHNhcmM= 36836
csOjbw== 36837
IHRhbXBvY28= 36838
IHBoaWxvc29waGVycw== 36839
IFJlY2h0 36840
5pOa 36841
IGNvbWVudGFyaW9z 36842
eXNl 36843
IOycpA== 36844
IG1pc2U= 36845
IEdpbg== 36846
INC90L7QvA== 36847
IEZST00= 36848
bGluZXI= 36849
YXRpZg== 36850
IHNwb8WCZWM= 36851
eGE= 36852
INGC0YDRg9C0 36853
IHdhZw== 36854
6riw7JeQ 36855
IE1H 36856
IG9mZnNwcmluZw== 36857
IFVuZGVyc3RhbmRpbmc= 36858
5Y+q5piv 36859
T1JB 36860
IHdoaXJyaW5n 36861
IHN1cnJlbmQ= 36862
IHBva2Vy 36863
IG1vbnVtZW50cw== 36864
IOKZqQ== 36865
IG9yZ2FuaXNlZA== 36866
IFNvemlhbA== 36867
IEZhY3Rvcnk= 36868
0YXQsA== 36869
IHJlc2VtYmxl 36870
0LfQtA== 36871
IGV4cGxvc2lvbnM= 36872
IHBheXJvbGw= 36873
IG9tbg== 36874
IEpvcmdl 36875
zrnPgw== 36876
IGZyYWN0dXJl 36877
IHBlcnNlY3V0aW9u 36878
IGRlbWFpcw== 36879
RUNI 36880
LCk= 36881
IGNyaWFy 36882
IEpPU0g= 36883
IGRlbW9ncmFwaGljcw== 36884
IDE2MDA= 36885
IGN1cnJlbmNpZXM= 36886
IFRpcHM= 36887
IOmAmeWAiw== 36888
IFJlZmVy 36889
IERhbmNpbmc= 36890
IGluY29uc2lzdGVudA== 36891
IGRlaA== 36892
IGltbWVucw== 36893
IG1laXN0 36894
IGltcGF0aWVudA== 36895
IGJlaGF2ZXM= 36896
5p2+ 36897
IOuCtOyaqQ== 36898
IGJhY2tzdG9yeQ== 36899
IGFncmVlaW5n 36900
IMWB 36901
aWhpbg== 36902
IHRlbXBlcmF0dXJh 36903
IEJhY2tncm91bmQ= 36904
IG51dHplbg== 36905
IOuFuQ== 36906
IE3DpG5uZXI= 36907
IGNvbGxhYm9yYXRpb25z 36908
IEtvcw== 36909
6YGO5Y67 36910
IG5pZ2h0bWFyZXM= 36911
65Ox 36912
IFF1ZWVuc2xhbmQ= 36913
IGFzc29jaWF0ZXM= 36914
IEtvaw== 36915
IGZhY3RvcmlhbA== 36916
IEh5dW5n 36917
IOq3uOuLpOydjA== 36918
IGZpbGhv 36919
IGVsw6l0 36920
IO2WieuztQ== 36921
sLE= 36922
IGdlZnVuZGVu 36923
IHNlbWljb25kdQ== 36924
IGNvdW5zZWxvcnM= 36925
IFVwcGVy 36926
IEF1Yg== 36927
aWNrZXJz 36928
VmVy 36929
IG5vcnRod2VzdA== 36930
IE1haW50ZW5hbnQ= 36931
IExha2Vz 36932
0LDRj9Cy 36933
aW50w6k= 36934
7LC9 36935
INCz0LDQtw== 36936
IGdpb3Ju 36937
IGRpZ2l0YWxseQ== 36938
IENpcmN1aXQ= 36939
7LyA 36940
44KK44G+44GX44Gf 36941
IGNoZWVyZnVs 36942
IFBldGVyc29u 36943
IERhbmlzaA== 36944
YXRpdm9z 36945
IGxpa2Vu 36946
IGhhcmJvcg== 36947
0LDQu9C40YHRgg== 36948
eGU= 36949
IGN1cmxz 36950
IFJob2Q= 36951
RW5k 36952
IEVU 36953
IGFjcXVhaW50 36954
IEtlbHZpbg== 36955
IHRyaWY= 36956
IEF3YXk= 36957
7J6Q64qU 36958
dnM= 36959
IHDDoWdpbmE= 36960
IGlubGV0 36961
IFNhbnRvcw== 36962
IOyasOyZgA== 36963
IHlhcMSxeW9yc3Vu 36964
dGhlbWU= 36965
IHNvdWZm 36966
IGluamVjdGVk 36967
IHDDs8W6bmllag== 36968
aXZlcnNv 36969
YW1wZWQ= 36970
IGRhaGVy 36971
IGRhZ2dlcg== 36972
INC70Y7QsdC40Lw= 36973
IHR1bW15 36974
IGVubGlnaHRlbmVk 36975
Y2VudHM= 36976
IERhaA== 36977
IGN1ZXN0 36978
5L6G6Kqq 36979
SUxZ 36980
INeR16g= 36981
IGJhbmdpbmc= 36982
IEVtaWw= 36983
IENsZXI= 36984
IEJvcmRlcg== 36985
0LjQttGD 36986
IHByZXNlbnRlcnM= 36987
IFNUVUQ= 36988
Y29pbnM= 36989
IO2ZjQ== 36990
IHBlcmtz 36991
IHBhcmFw 36992
IGNlcnRhaW5lcw== 36993
IExvcmU= 36994
w7ZzdA== 36995
IE1BUlRJTg== 36996
IGJpb3M= 36997
IHdoZXJlYnk= 36998
dmVydHM= 36999
IE1pcmFuZGE= 37000
IHN0aXA= 37001
5r6k 37002
YW5kZXo= 37003
15vXnA== 37004
dWppbg== 37005
IOq+ 37006
IGFsbGVyZ2llcw== 37007
cGxhdGU= 37008
IHlhcMSxbA== 37009
IHVuZGVydGFrZQ== 37010
IOuCmOqwgA== 37011
UGFydA== 37012
IGvEsXrEsW0= 37013
aGd1cnU= 37014
44GC44Go 37015
IEpvaG5z 37016
IGV5ZWxhc2hlcw== 37017
IGRyYWluZWQ= 37018
IHN0w6Vy 37019
44GC44KK44G+44GZ 37020
IEphZGU= 37021
IGNhbGVuZA== 37022
ZmlsbQ== 37023
IG1lc2E= 37024
IGx1ZHppZQ== 37025
IGF0dHJhY3Rz 37026
IGp1aWNlcw== 37027
INC60LjQuw== 37028
IG5pZXV3ZQ== 37029
IG1lbmNpb24= 37030
IGlnbml0aW9u 37031
IGJsYWRkZXI= 37032
YW5kYWFn 37033
IEV4dGVuc2lvbg== 37034
7YKo 37035
ZmVlZA== 37036
INmI2Yc= 37037
IHNwdW4= 37038
IHTDpHQ= 37039
0L7RgNC+0YI= 37040
dHlhcmQ= 37041
cm9uaWNz 37042
IEh1Z2U= 37043
0YPQttC0 37044
c3RyaW5n 37045
IHVuanVzdA== 37046
IHByYXdu 37047
IGZyb3N0aW5n 37048
IGRpc2FwcGVhcmFuY2U= 37049
aW9zYQ== 37050
IGNhcmRp 37051
IFByaWVzdA== 37052
IGNpZW50w61maWM= 37053
5ZOq6KOh 37054
INCS0LDRgQ== 37055
IOu2gO2DgQ== 37056
IHRoaWV2ZXM= 37057
IHBoeXNpcXVl 37058
IEV1Z2VuZQ== 37059
INCx0LvQuNC3 37060
IG1vbm9wb2x5 37061
IGJpb2dyYXBoeQ== 37062
IGhvxZ8= 37063
IHTDtg== 37064
bWFj 37065
IHNob2Nrcw== 37066
7IS46w== 37067
aGl0 37068
IHNudWc= 37069
IGluY2w= 37070
IGRlZGlj 37071
IHVsdHJhcw== 37072
INC40LfQstC10YHRgg== 37073
IHV0aWxpemF0aW9u 37074
INGB0L7QstC10YDRiNC10L3QvdC+ 37075
IHNlcnZp 37076
c3RhZw== 37077
MTgw 37078
IHNld2Vy 37079
IENob2ljZQ== 37080
IGRpc2NoYXJnZWQ= 37081
IEpE 37082
0L7Qu9C10YI= 37083
INC60LLQsNGA0YLQuA== 37084
IHRlbGVzY29w 37085
IEplxZtsaQ== 37086
IE5hbmE= 37087
Y2FsZQ== 37088
INGC0L7QvQ== 37089
bW1t 37090
5LqG5ZCn 37091
IGdlaGFidA== 37092
64Kg 37093
5oqV 37094
4LiZ4LiZ 37095
IGV0aGVy 37096
IHplbg== 37097
IHJlc2VhcmNoZWQ= 37098
IEN6eWxp 37099
5a6M5YWo 37100
d29ya2Vycw== 37101
IOqyveywsA== 37102
IHNoZXJpZmY= 37103
YWxsbw== 37104
IHRpcG9z 37105
IHByb3NlY3V0aW9u 37106
IGZyb2dz 37107
IGZhbHQ= 37108
amQ= 37109
IO2MlA== 37110
IGZpbHRlcmVk 37111
IE9mdA== 37112
IOyN 37113
IGRpc2Zy 37114
IE11c3Rhbmc= 37115
IHdvYWg= 37116
IFJFQUxMWQ== 37117
INC80L7Qs9C70Lg= 37118
IGVudHJhZGE= 37119
INC40LPRgNCw 37120
IG1peGVz 37121
INCw0LLRgtC+0LzQvtCx 37122
0Jk= 37123
IHNoaW4= 37124
IHBhcmFub3JtYWw= 37125
IHNvbWVwbGFjZQ== 37126
IGRpc2hvbg== 37127
ZXRhYW4= 37128
IGZ1ZXJ0ZQ== 37129
2bk= 37130
IGRvb20= 37131
7Iic 37132
IGV4aXN0ZW50aWFs 37133
IGJ1bGQ= 37134
IFNESw== 37135
INC/0YDQsNCy0LTQsA== 37136
IHR1cm5vdmVy 37137
IOyXrOq4sOyXkA== 37138
IOCkuQ== 37139
IG1vZGVsZWQ= 37140
IGJ1Z8O8bg== 37141
IGV4cGVyaW1lbnRhdGlvbg== 37142
IG1vcm5pbmdz 37143
IG1lZG8= 37144
U3Rldmll 37145
IHBsYXlhYmxl 37146
IGFpcmxpbmVz 37147
Z21lbnRz 37148
IOq4sOu2hA== 37149
IFRvbWI= 37150
IE1WUA== 37151
QVVESUVOQ0U= 37152
IGNoZWNrb3V0 37153
IHBhc3N0 37154
IGJlaXNwaWVs 37155
IExpbmtz 37156
aGVhdnk= 37157
IHF1ZXN0aW9uYWJsZQ== 37158
IOyTsOs= 37159
IHNpbGw= 37160
IG1hbmlwdWxhdGVk 37161
IExvcmVu 37162
IOycvA== 37163
IHZlcmdl 37164
w6Fr 37165
SUVT 37166
IHNhYm90 37167
IEN1c3RvbWVy 37168
YWxlxbx5 37169
IG5vbWluZWU= 37170
IEdhZA== 37171
IG5vdXZlbGxlcw== 37172
IFNQRQ== 37173
aXN0bGluZw== 37174
IG92YWw= 37175
0L7QsdGA0LDQtg== 37176
aWZ0eQ== 37177
6YeO 37178
IGJlemVs 37179
eWV0 37180
IGZyZWlnaHQ= 37181
IEhhbsSxbQ== 37182
csOtYQ== 37183
IHpvbmluZw== 37184
IGluZGVt 37185
IELDvA== 37186
IGZlbWluaXNt 37187
IHZvaXg= 37188
IG9maWNpYWw= 37189
IGRpeW9ydW0= 37190
u5A= 37191
IGFyb3Nl 37192
IHBhcmFy 37193
7J247KeA 37194
IE1hcnRpbmU= 37195
IExlY3Q= 37196
IHJlc3Rlcg== 37197
IGRyb3duaW5n 37198
dXlh 37199
Y2lkYQ== 37200
IEFyaWVs 37201
IDAy 37202
INeU15Q= 37203
57Sg 37204
IFdlcnQ= 37205
0KLRiw== 37206
IHdpZG93 37207
IHBhcmNobWVudA== 37208
IGNvdHRhZ2U= 37209
IFhM 37210
IFNsYWNr 37211
IE5FUw== 37212
IHJvYmU= 37213
IGdpbW0= 37214
IGNhbWluaG8= 37215
IEhhcnBlcg== 37216
IGNpdHJ1cw== 37217
IGZpcmVmaWdodGVycw== 37218
IGRvcGFtaW5l 37219
ZWxldHM= 37220
IGRlbW9jcmF0 37221
7KCc66Gc 37222
IHBsYXliYWNr 37223
b2o= 37224
INC/0YDQvtC6 37225
IFN1bGxpdmFu 37226
c2VtYmxl 37227
IFdvcnRo 37228
IE11c3RhZmE= 37229
4Liy4Lij 37230
IG1ldHM= 37231
6ZaA 37232
0LvQvtGB0Yw= 37233
IGluZXJ0aWE= 37234
IHVuaWZvcm1z 37235
6Laz 37236
w6lyaW8= 37237
15XXqNeU 37238
w6ludA== 37239
IOCukg== 37240
INGB0LDQvNGL0YU= 37241
IHZvdWxhaXM= 37242
IFppbW1lcg== 37243
6rKg6w== 37244
INC90L7RgQ== 37245
ZW5jaWFz 37246
IHJlbGFjacOzbg== 37247
IOqxuOs= 37248
IGZhY3Rpb24= 37249
IGdvc3A= 37250
0L/QvtC70L7Qtg== 37251
bmFw 37252
aGFr 37253
IHByb2NlZWRpbmdz 37254
IOyGlA== 37255
7JWE64uI 37256
IOyekOq4sA== 37257
IHdlcmQ= 37258
IHNvZg== 37259
IHNjaGxpbQ== 37260
IGZsYXZvcmVk 37261
IHF1YWRyYXRpYw== 37262
IEJvb3Q= 37263
IHB1YmxpY2l0eQ== 37264
IENhcm8= 37265
ID8i 37266
0L3QuNGG0LA= 37267
bWFuaWE= 37268
IFNVUg== 37269
IEJVUg== 37270
bGFuY2U= 37271
w6l0aWNh 37272
IHpvYmFjenk= 37273
IHRyaW8= 37274
c2FtYQ== 37275
IHRhxZ8= 37276
IGFzeW1t 37277
cmVzc2Vy 37278
INiq2Lk= 37279
INC/0LXRgQ== 37280
IGJlZ2lubmluZ3M= 37281
bGFkxLFt 37282
INCx0YvRgdGC0YA= 37283
IG1vbw== 37284
IEdlbmV2YQ== 37285
IOWcqA== 37286
ZXJ1cw== 37287
Ym9yYWg= 37288
IHJlZnVzaW5n 37289
YnVsbA== 37290
IFdhaXRpbmc= 37291
IEluZGl2aWR1YWw= 37292
IGFub255bQ== 37293
aW1lbnM= 37294
IG1lZGlkYXM= 37295
IGZyYWdyYW50 37296
IGRpcmVjdGVtZW50 37297
IOyVhOuniA== 37298
dXJpYQ== 37299
IHNwaGVyaWNhbA== 37300
IGFiZ2U= 37301
IFZpY3Rvcmlhbg== 37302
IHNwZWN0YWNsZQ== 37303
IFJvZHJpZ3Vleg== 37304
IG9jdXA= 37305
IE7DpHI= 37306
bWFya3M= 37307
bmd1bG8= 37308
IEx1Y2k= 37309
IHNob3V0ZWQ= 37310
IHJlZ3VsYXRvcnM= 37311
xJ9pbmk= 37312
IGRpc2VudA== 37313
INGA0YvQvQ== 37314
64Ko 37315
IOyCtOs= 37316
IHByb2Jsw6htZXM= 37317
IEZpbmdlcg== 37318
YXNzZW1ibGU= 37319
IHBlYXI= 37320
IGRyb2l0ZQ== 37321
IEV2ZXJ5d2hlcmU= 37322
dGFt 37323
0L7RgtC40LI= 37324
0LLQvtC5 37325
b3JkaW5hdGU= 37326
IExhaw== 37327
IG3hu5tp 37328
IFRlbGV2aXNpb24= 37329
IGV4cG9uZW50aWFsbHk= 37330
YXZhcw== 37331
IGJsZXY= 37332
IE1U 37333
5L+6 37334
Q29ubmVsbA== 37335
IOq1reuvvA== 37336
INGB0LLQvtC40Lw= 37337
IGFjaGE= 37338
IER5bmFzdHk= 37339
Smlu 37340
IHRvcmU= 37341
IGZsb3I= 37342
INC80L3QvtCz0LjQtQ== 37343
5rKS5LqL 37344
b3dhbg== 37345
YmFo 37346
IOyjhA== 37347
IENlbGE= 37348
IOy1nOq3vA== 37349
IHBlcm1ldHRyZQ== 37350
IGFicmFz 37351
IHZlcnN0ZWhlbg== 37352
IGVzY29ydA== 37353
IFRoZW0= 37354
w6Rya2U= 37355
cG9ydGVy 37356
IGthaGthaGE= 37357
IGhlY3Q= 37358
IGRhdQ== 37359
d2Fo 37360
b2x2ZQ== 37361
IEFnZXM= 37362
c2NoYWZ0 37363
IFN0ZWxs 37364
bmVsbGU= 37365
IEVuc3VpdGU= 37366
INCS0YHQtdC8 37367
IGNyw6lk 37368
IFBQ 37369
bG9yZHM= 37370
Z3J1bnRpbmc= 37371
IGNvbnRyYWN0aW9u 37372
R290 37373
IGFjcXVpcmluZw== 37374
IHNvcHI= 37375
IHBvaXNvbm91cw== 37376
Uk5B 37377
IGFuYXI= 37378
IEhvZg== 37379
Jyk= 37380
IHJlbWFya2FibHk= 37381
IGludGVybmFjaW9uYWw= 37382
w7xja2U= 37383
aW5xdQ== 37384
IGR1eQ== 37385
IGJlYXN0cw== 37386
IExBTg== 37387
IHByZWNlZGVudA== 37388
IFJQTQ== 37389
5ZGo 37390
IHNlbG9u 37391
IG1vcnRl 37392
IGNvbWXDp291 37393
0Y/Qu9Cw 37394
IGludGVycHJldGluZw== 37395
IEJ1cmtl 37396
0YLRgNCw 37397
IOydtOufrA== 37398
IHBlc3NpbQ== 37399
IE5vaw== 37400
7Yyd 37401
RmVtYWxl 37402
IOyLpO0= 37403
mYA= 37404
IHN0aW11bGF0aW9u 37405
IHNsaWNr 37406
IOqwgOuKlA== 37407
INC60LDQtw== 37408
IEhCTw== 37409
IHBhcGllcg== 37410
IGvDtm5udGVu 37411
0YPQsdC70Lg= 37412
IENvbnN0YW50 37413
U1BFQUtJTkc= 37414
IGt0w7NyxIU= 37415
IGNvc21ldGljcw== 37416
IFRyZW5k 37417
IHJvYmJlcnk= 37418
IHRpdHQ= 37419
IGdqb3J0 37420
IGRpZXRhcnk= 37421
oIw= 37422
IEtpcmJ5 37423
INC/0YDQuNC80LXRgNC90L4= 37424
IHF1YWxpZmljYXRpb24= 37425
IOyViQ== 37426
IGNhYmluZXRz 37427
IGh0dHA= 37428
IEVyaWNh 37429
576p 37430
IGRpc2FkdmFudGFnZXM= 37431
IGNoYXR0ZXJpbmc= 37432
eXo= 37433
ZmVpdA== 37434
IGd1aWxk 37435
IEVURg== 37436
IERyYWdvbnM= 37437
IEhFUkU= 37438
dmVudGg= 37439
2YTYp9mF 37440
IG1hcmNow6k= 37441
RGFt 37442
IHBob3Rvbg== 37443
IGVzdGFibGU= 37444
TWFn 37445
IG9saGFy 37446
IGNvdXBsaW5n 37447
IEhpbGZl 37448
IFdpemFyZA== 37449
INC80LDQu9C+ 37450
aGVscA== 37451
IGzDrW5lYQ== 37452
IOyr 37453
IHN0YW5kYWxvbmU= 37454
IG1vcmFsZQ== 37455
IHp3ZWl0ZQ== 37456
44KI44KN44GX44GP 37457
w6RocnQ= 37458
IGRvdHRlZA== 37459
IGRyaXBwaW5n 37460
IEZsYWc= 37461
6Z2S 37462
cm9ja2V0 37463
cmF0ZWd5 37464
aXJpbQ== 37465
IO2VmOuptOyEnA== 37466
IHNvZ2VuYW4= 37467
IFVubw== 37468
IFNjaHV0eg== 37469
IGVzdGlsbw== 37470
IFN1YnM= 37471
IERhaXN5 37472
0J3QtdGC 37473
Jy4uLg== 37474
IHBsYXRpbnVt 37475
IGJpcmw= 37476
IFNvdmk= 37477
IHZpb2xhdGU= 37478
0YPQtdGC0YHRjw== 37479
cmlsbA== 37480
IHRyYXo= 37481
IHNuaXA= 37482
IGN1bXBs 37483
4Lit4LiB 37484
IGN1aw== 37485
6YWS 37486
IFBhcmxhbWVudA== 37487
IGh5cGVydA== 37488
IHB1bHA= 37489
IHRvbmd1ZXM= 37490
YXR0bw== 37491
IGJ1c2Nh 37492
aWhu 37493
RVJP 37494
INmK2Lk= 37495
IHZhcmlhcw== 37496
IE1hcmlhbg== 37497
IGJvdW5kZWQ= 37498
IHBpdGNoaW5n 37499
IGRlZmljaWVuY3k= 37500
IEJsZXNzZWQ= 37501
IEV4ZXJj 37502
dWNocw== 37503
IG5oxrBuZw== 37504
5pys5b2T 37505
IHJhcGVk 37506
aGFsZXM= 37507
IG1hbGE= 37508
cGlj 37509
IDQwMQ== 37510
xZtuaWVq 37511
YXJpbmE= 37512
65Ok7J2E 37513
b3R0aQ== 37514
INC00L7Qu9Cz0L4= 37515
IHRyYWNrZXI= 37516
IFNoZWxieQ== 37517
IHZhbmlzaGVk 37518
IGJha2VyeQ== 37519
S2FwxLE= 37520
SmVzdXM= 37521
IEtS 37522
Sk8= 37523
hbg= 37524
IGRpc2Nz 37525
7ISv 37526
7KeA6w== 37527
15nXpg== 37528
ZW1hcnk= 37529
S2VuZHJh 37530
IHnDvGs= 37531
w7xja3Q= 37532
IHZheg== 37533
IGt1cA== 37534
YWt0dQ== 37535
INGB0L/QsNGB0LjQsdC+ 37536
IGFpaw== 37537
IG51cnNlcnk= 37538
IGVuZGFuZ2VyZWQ= 37539
w6ptZW1lbnQ= 37540
ZW1hdGljcw== 37541
IHJlc3BvbmRlcnM= 37542
IFJlcHJlc2VudGF0aXZlcw== 37543
IHNjdWxwdHVyZXM= 37544
aWdrZWl0ZW4= 37545
IGRlcGw= 37546
IGludGVycHJldGF0aW9ucw== 37547
IGRlYWRsaW5lcw== 37548
IDE5NDI= 37549
w5c= 37550
IHN1Z2Fycw== 37551
ZW11 37552
bGl2ZWx5 37553
IHJlY3JlYXRpb25hbA== 37554
IGRpc3RvcnQ= 37555
IHVuZGVyc2NvcmU= 37556
IHVucXVvdGU= 37557
IHNhZmVzdA== 37558
IHN3b2xsZW4= 37559
IGFuYWx5c2Vz 37560
IGNvbW1lbmPDqQ== 37561
5aa5 37562
YW5kaW4= 37563
INCl0L7RgNC+0YjQvg== 37564
IGRpYXJy 37565
44G+44GB 37566
emllc3Q= 37567
IHRvb3RoYnJ1c2g= 37568
6aC76YGT 37569
dWF0aW9ucw== 37570
IGNhZGU= 37571
IGJhY2tsYXNo 37572
aGluZA== 37573
IHJpc3F1ZQ== 37574
emVzcw== 37575
IOydtOyVvOq4sA== 37576
IGVzcGVyYXI= 37577
IHRyYW5zbGF0aW9ucw== 37578
aW9uZWQ= 37579
Z3JvYW5z 37580
INC/0YPRgg== 37581
IGdlbmV0aWNhbGx5 37582
6YCg 37583
IGhhcHBpZXN0 37584
IHdlcms= 37585
YXRvb24= 37586
IG11c2k= 37587
IGZ1bsOnw6Nv 37588
IOyeheuLiOuLpA== 37589
INGA0LDQuQ== 37590
IGJldm9y 37591
QkxBTks= 37592
IHJlcGVudGFuY2U= 37593
UHV0 37594
IHBvdHJ6ZWI= 37595
IHNhbGE= 37596
IGNhbXBh 37597
V0VS 37598
IGRlY8OtYQ== 37599
IHPDqWN1cml0w6k= 37600
IEFwcHJlY2lhdGU= 37601
0YfQuA== 37602
IFJhbmRvbQ== 37603
67OE 37604
a2Fo 37605
IG3Dtmo= 37606
IHPDpGdlcg== 37607
INeZ15vXldec 37608
IDE5MA== 37609
eHR1cmVz 37610
RXU= 37611
IGfDpA== 37612
INeR16o= 37613
IENyb2F0 37614
YXBv 37615
UExF 37616
IHBlcnNpc3RlbmNl 37617
5Yqp 37618
IGJsZW5kcw== 37619
IHRyZWZmZW4= 37620
IFNhbnRpYWdv 37621
eWRpYQ== 37622
YWxkbw== 37623
IFRlbnNvckZsb3c= 37624
IER1YWw= 37625
44Oc 37626
IGNoaWZm 37627
7Je0 37628
IGNvbnRyYWN0ZWQ= 37629
IHNlZ3JlZw== 37630
IEZhaXJ5 37631
IHdpc2VseQ== 37632
IHZ1bG5lcmFiaWxpdGllcw== 37633
IGhhbmRoZWxk 37634
IGdhZGdldHM= 37635
IGJvxZ8= 37636
IFBvcHVsYXI= 37637
IGN1cnZhdHVyZQ== 37638
66y4 37639
IE1BUlk= 37640
7J207Io= 37641
IGZvcm11bGF0aW9u 37642
IGNlbGVyeQ== 37643
IGJsdXJyeQ== 37644
IFRT 37645
YWxleg== 37646
IHdz 37647
IHByb2dyYW1t 37648
IFN0YWNr 37649
IEpJTQ== 37650
0L7QstCw0LvQuA== 37651
xLFsbA== 37652
IHDDqHJl 37653
IEthbnll 37654
IERlbGF3YXJl 37655
IOOBoA== 37656
IGRhdW50aW5n 37657
INCx0LXRgQ== 37658
IFN0dXBpZA== 37659
Ymln 37660
ZmZpY2lhbA== 37661
IHByZWNpcGl0YXRpb24= 37662
IHBsdW5n 37663
4bulYw== 37664
YnVyc2U= 37665
IGRhcmxl 37666
IGNyaXBw 37667
IHBpb25lZXI= 37668
IGRpc3B1dA== 37669
IHNlYW4= 37670
44GT44KT44Gq 37671
IHJlc2lzdG9y 37672
IGFsbGVpbg== 37673
aXBwbGVz 37674
YXJlbA== 37675
IGVuZG9ycw== 37676
enVzdA== 37677
INGA0LXQsdGP0YLQsA== 37678
ZWRlZA== 37679
IOy5tOuplOs= 37680
IGxsZXZh 37681
IGtlbm50 37682
INCx0LDQuw== 37683
IERvY3VtZW50 37684
IEtuaWdodHM= 37685
IGJ1Y2tsZQ== 37686
IOyJrA== 37687
IGFsaw== 37688
IEV2ZXJ5ZGF5 37689
YXR0ZXJz 37690
IHRvaWxldHM= 37691
IGp1Z2Fy 37692
IOyeiOyngA== 37693
IGdlbmF1c28= 37694
IExhbmRlc3JlZ2llcnVuZw== 37695
44Gj44Gx 37696
aWpl 37697
IHRyYWlsZXJz 37698
IFRpZ2Vycw== 37699
IGdpdHRp 37700
IGZvcmdpdmluZw== 37701
IGNvbmN1cnJlbnQ= 37702
IFZ1 37703
IO2Kue2eiA== 37704
IEJST1dO 37705
b3VuZGVk 37706
Ijs= 37707
IHRyZW1i 37708
IHRpZXQ= 37709
INGA0LXQttC40Lw= 37710
IG51dHNoZWxs 37711
0LXQu9C40Yc= 37712
IGxvc2Vycw== 37713
cmljdGluZw== 37714
IHJlZGVlbQ== 37715
ZGVmaW5lZA== 37716
TmljZQ== 37717
IGJyb2FkYmFuZA== 37718
S08= 37719
IHRlYXNpbmc= 37720
IHBhcnRpc2Fu 37721
xLFtYQ== 37722
IOyerOuvuA== 37723
IEpvdXJuZXk= 37724
IHNsb3Blcw== 37725
dW5pbmc= 37726
Z3J1bnRz 37727
IHTDpGxs 37728
IHVuY292ZXJlZA== 37729
IG15xZtsxJk= 37730
IEVzdGhlcg== 37731
5LqO 37732
IEhlYWx0aHk= 37733
IOuwkQ== 37734
csOpZQ== 37735
IHBvbGFyaXphdGlvbg== 37736
IGZsYXY= 37737
IGNhbWJpYXI= 37738
IHly 37739
IFJhbmNo 37740
IHNwbGl0cw== 37741
IHRyb3V2w6k= 37742
5ZyL5a62 37743
IHJlY29yZGVy 37744
IGTDqXBhcnQ= 37745
2YjYqA== 37746
IEtyeQ== 37747
IGludGVyZXNzYW50 37748
IGVkZXJpbQ== 37749
xZt3aWFk 37750
aWxhdGVyYWw= 37751
d3JpZ2h0 37752
IHBvdXJyYQ== 37753
w6p0ZXI= 37754
IGNhbWVs 37755
4Z4= 37756
IHJhcGlkZW1lbnQ= 37757
IG1lag== 37758
IHN0aWZmbmVzcw== 37759
QURBUw== 37760
IGRpZmZlcnM= 37761
IGFsb3Q= 37762
IFNpZw== 37763
0Y/RgtC10LvRjA== 37764
IGFic3RyYWN0aW9u 37765
5ZyY 37766
IGtlaW5lcg== 37767
Z3J1cHA= 37768
IFNoZXJsb2Nr 37769
7ZiU 37770
IGNpdGU= 37771
IG92ZXJmbG93 37772
IHThuqFp 37773
w7pjYXI= 37774
YnVsYQ== 37775
IGNvbmp1bnRv 37776
IENJ 37777
IG1vZGVyYXRvcg== 37778
IGluZGlyZWN0bHk= 37779
IGFsbGVpbmU= 37780
4oI= 37781
0YjQuNCx 37782
INCx0LDQsQ== 37783
IGRhbmFjaA== 37784
IDE5Mzk= 37785
IHByb21ldA== 37786
IGRlc3RpbmF0aW9ucw== 37787
IElsbHVzdA== 37788
zrnOus+M 37789
IHNhYmVz 37790
IGhlaA== 37791
IEdlc2V0emVudA== 37792
IE1peg== 37793
0LXQvdC60L4= 37794
IE15cw== 37795
0Kw= 37796
IEp1ZGFpc20= 37797
IG11c3RhY2hl 37798
IHN0aW1tdA== 37799
IEdhemE= 37800
IHZvbHRl 37801
IG51bw== 37802
IG3Ds24= 37803
IENvbXB1dA== 37804
4Li54LmI 37805
IFJhZGk= 37806
IGV4Y2VwdGlvbmFsbHk= 37807
IGFzc3VtZXM= 37808
6ZaL5b+D 37809
44GI44Gw 37810
aW5mb3Jt 37811
IHNocmluZQ== 37812
5pOK 37813
IGltcGxpY2F0aW9u 37814
IEZpdHo= 37815
5rKS6Zec5L+C 37816
IS4= 37817
IGx0 37818
IGFsbG95 37819
IGV0aGlj 37820
IG1vbmFzdGVyeQ== 37821
7Iuc7KOg 37822
aWNhw6fDo28= 37823
IGNvb3JkaW5hdGluZw== 37824
IE1vdG8= 37825
IG92ZXJsb29r 37826
IGNob2lz 37827
IGFudGliaW90aWM= 37828
IE1pbm5l 37829
IEJK 37830
IEFwYQ== 37831
b3JpYW4= 37832
IHNwaWxsZWQ= 37833
SmFt 37834
IGh1c2JhbmRz 37835
IGNyZWF0aW9ucw== 37836
IGHDsQ== 37837
w7xzc2Vs 37838
IOydtOyaqQ== 37839
IGFuYWx5c2U= 37840
cm9zZQ== 37841
IHB1bmNoZWQ= 37842
IHByZXNxdWU= 37843
IGFzdHJvbm9teQ== 37844
IHNjaHdpZXJpZw== 37845
IEVib2xh 37846
IGNpcw== 37847
IGFjZXQ= 37848
IEZY 37849
ZW5kcmU= 37850
IOydjOyVhQ== 37851
IHdlYnBhZ2U= 37852
IGZyZWFrZWQ= 37853
IGxhdHRl 37854
IOy/oA== 37855
IOuouOs= 37856
TmV2ZXI= 37857
R3Jh 37858
7ZmU66W8 37859
ZXllZA== 37860
IOuwnOudvA== 37861
IGVzcGVyYQ== 37862
IGFwYXJlY2U= 37863
cmHDp8Ojbw== 37864
IGRpc3J1cHRpdmU= 37865
IEpvaW50 37866
dXJvdXM= 37867
cmVhcw== 37868
IHF1ZXLDrWE= 37869
IGRpc3RyaWJ1dGlvbnM= 37870
IGV4cG9uZW50 37871
7LmY66W8 37872
IGRs 37873
emhvdQ== 37874
IEhlYXJpbmc= 37875
5beu5LiN5aSa 37876
IENyYXc= 37877
IGZsb2F0cw== 37878
b3VuY2Vk 37879
TGFi 37880
V29ybGQ= 37881
IGJ1cmRlbnM= 37882
IGF1dGhvcml0YXJpYW4= 37883
IEJvbHQ= 37884
INC+0LTQvdGD 37885
IHBpZ2Vvbg== 37886
IGRpc3RyYWN0aW9ucw== 37887
IEhlcmF1c2ZvcmRlcg== 37888
IHplc3Q= 37889
ZXNj 37890
IHNoYWtlcw== 37891
YXRhcw== 37892
INmF2LQ= 37893
aG9sZXM= 37894
IHRoaW5rZXJz 37895
YWx0YQ== 37896
IGFyY2hl 37897
IFN1aw== 37898
YW5oYQ== 37899
IHRlbXB0aW5n 37900
IHlvdXR1YmVy 37901
IHbDrA== 37902
IGR6aWHFgmE= 37903
IFZhdGljYW4= 37904
UGFyaw== 37905
IHN1cGVycw== 37906
IE5pa2tp 37907
64qQ6w== 37908
b3Jhbmc= 37909
cmFtaWVudA== 37910
6ay8 37911
IOqwluqzoA== 37912
IGRlc3NlcnRz 37913
IGF2ZXJl 37914
IEdyZWdvcnk= 37915
IOuTpOyWtOyY 37916
IGNvc3Rpbmc= 37917
IENsaW5pYw== 37918
IHJlYmVscw== 37919
IE1vYg== 37920
IGJ1bmxhcg== 37921
IFlvdXJz 37922
ZXJ0aW1l 37923
IHJldGFsaQ== 37924
bWFyYQ== 37925
YXR1cw== 37926
YWxsZXM= 37927
INC00YA= 37928
INC00LjRgQ== 37929
IGRpc2NvdW50cw== 37930
IEdVWQ== 37931
INC60LDQutC+0LU= 37932
IEV4cGVyaW1lbnQ= 37933
cmVtZW50 37934
IFhpYW5n 37935
IGJhdGU= 37936
V0U= 37937
IHNwZWNpYWxpemU= 37938
IGRlaXR5 37939
IExva2k= 37940
bWFn 37941
IE5pdA== 37942
V2VzdA== 37943
IG1hdGVybmFs 37944
IHF1aXM= 37945
5Z+65pys 37946
YnJva2Vu 37947
IGxhc2Vycw== 37948
IGhha2s= 37949
IEFuZ2Vscw== 37950
IG1hc3Rlcnk= 37951
YW50aXM= 37952
VGlmZmFueQ== 37953
ZWVl 37954
55E= 37955
b3JlbQ== 37956
IGluYWNj 37957
IGp1cmlzZGljdGlvbnM= 37958
IEthcmRhc2g= 37959
5py6 37960
SWw= 37961
IFNpbm4= 37962
5YuV55S7 37963
IGF0aGxldGljcw== 37964
Y8SZ 37965
IGxvb3NlbHk= 37966
IGRpZXRh 37967
QWc= 37968
ID8/ 37969
IOuMgO2RnA== 37970
IHN1cGVydg== 37971
IG51dHJpdA== 37972
IGRyaWZ0aW5n 37973
IOyEoOyDneuLmA== 37974
INC/0L7QvdGP0Ls= 37975
IFZpY3Rvcnk= 37976
2YTYqQ== 37977
15XXoNeU 37978
INC/0LjRiA== 37979
IHNoYXZlZA== 37980
IG1lc3VyZQ== 37981
b25kZW4= 37982
2YPYsQ== 37983
IGV4aWxl 37984
IERlc2Rl 37985
IFBpbnRlcmVzdA== 37986
IGF0dGFjaG1lbnRz 37987
IGhvbWJyZXM= 37988
IGZpbmVz 37989
IOyEuOyDgQ== 37990
IHNsZWVwcw== 37991
IFRhY28= 37992
IElSQQ== 37993
cmlvcw== 37994
IG9sbA== 37995
ZXRlcw== 37996
IHVudXQ= 37997
ZmFzaGlvbmVk 37998
IHRyZWJhbGw= 37999
IE5lYXJseQ== 38000
INGA0LXQsNC70YzQvdC+ 38001
IGNoaWw= 38002
6YCx 38003
xJ9h 38004
IE1FTA== 38005
cm9zY29w 38006
IENH 38007
IHZlbmdl 38008
IGRpc2h3YXNoZXI= 38009
YWxnaWM= 38010
IG1vZGlmaWVy 38011
IGVtYmFzc3k= 38012
dGltZXI= 38013
ZW1pY3M= 38014
IGludHJpY2F0ZQ== 38015
IGV2ZXQ= 38016
IOuMgOuwlQ== 38017
IGlzb3Q= 38018
INC90LDRg9GH 38019
IFF1aXo= 38020
cmVzbw== 38021
zrTPjg== 38022
IHllbGxlZA== 38023
IGZlZGVy 38024
RUxMRVI= 38025
IGV4Y2VlZGVk 38026
b25hcw== 38027
aWNhbm8= 38028
INC20LjQstC+0YI= 38029
IE1hbw== 38030
IEthenV0bw== 38031
IOOFi+OFi+OFi+OFiw== 38032
IGZyb250bGluZQ== 38033
IEh1bmdhcmlhbg== 38034
IMO8YmVyYWxs 38035
YXdhdA== 38036
IGdyaXBz 38037
acOnw7Vlcw== 38038
YXJueWE= 38039
IM2h 38040
IHNlaWQ= 38041
IGFuYWs= 38042
IGFjYWJvdQ== 38043
7ZWR 38044
IG5vdG9yaW91cw== 38045
IEdvZHppbGxh 38046
IG92ZXJjb21pbmc= 38047
IFBlbmQ= 38048
IG9sYWJpbGly 38049
w7xsbWU= 38050
IGVyaGFsdGVu 38051
44KJ44GE 38052
6re5 38053
IE1ldGVy 38054
IHN0YWFu 38055
T2w= 38056
IGNoYXRz 38057
IEJ1ZW5vcw== 38058
w612ZQ== 38059
YWx1YWJsZQ== 38060
IHN0cmF0ZWdpY2FsbHk= 38061
IGNvbXByaXNlZA== 38062
INC/0LXRgNGB0L7QvdCw0LY= 38063
IHdhbm4= 38064
IENlbg== 38065
0L3QuNGC0LU= 38066
n4E= 38067
INGC0L7QsdC+0Lk= 38068
aWFk 38069
IGthcmRlxZ9pbQ== 38070
IENvbmdyZXNzbWFu 38071
cmVhbWluZw== 38072
aG9tbWU= 38073
IGNvbW11bmF1dA== 38074
IGFsY29ob2xpYw== 38075
IHBpY2tsZWQ= 38076
IGFjb3Jk 38077
cG9zaXRpb24= 38078
ZWfDs2w= 38079
IHRyb3VibGluZw== 38080
IE1hcmNoZWc= 38081
IHp1bWluZGVzdA== 38082
IHNlYW1sZXNzbHk= 38083
IG9sdW4= 38084
IFRWcw== 38085
INC/0YDQsNC60YLQuNGH0LXRgdC60Lg= 38086
IGJhY2tlbmQ= 38087
44GT44KT44Gr44Gh44Gv 38088
aWRhYmxl 38089
IGdhZGdldA== 38090
IGZhw6dv 38091
IE1hcmNoZWdpYW5p 38092
IOuwpA== 38093
IGFjY2lkZW50YWw= 38094
IExQ 38095
IGVsZGVzdA== 38096
IEFkbWlyYWw= 38097
IG7Eg20= 38098
bGV2ZXI= 38099
IHBhc3RlbA== 38100
IGZvbmRv 38101
Q29ubmll 38102
IHRlcmNlcg== 38103
IHBhY3Q= 38104
IE1vbnRl 38105
IG1lYXRz 38106
IFNNUw== 38107
IEF1c3RyYWxpYW5z 38108
57w= 38109
UmhldHQ= 38110
IGV4YWN0ZW1lbnQ= 38111
IOu5vA== 38112
IE1PRA== 38113
56E= 38114
IFJhcHQ= 38115
IE5vY2g= 38116
IGFib3J0 38117
IE5hdmFs 38118
IEZ1amk= 38119
SU5URVI= 38120
INC90L7QstGL0Lk= 38121
IG1pZWpzY2U= 38122
IElDVQ== 38123
IEdyYWR1YXRl 38124
IEdsZW4= 38125
YXJkaQ== 38126
IMiY 38127
IHNvbGRlcg== 38128
IHByb2Zlc3Npb25z 38129
IG9ydGhvZw== 38130
b21u 38131
aW50cm9kdQ== 38132
IERlbmlzZQ== 38133
7J6Q66W8 38134
IGNvcnJlc3BvbmRlbmNl 38135
QU1B 38136
IGluZmxpY3Q= 38137
IGZhbmQ= 38138
IEfDvA== 38139
INGH0LXRgg== 38140
IHRyYWNlZA== 38141
IHBhdGVudHM= 38142
IGFtYnVzaA== 38143
IGxvdHRh 38144
ZmZlcg== 38145
IFdhZ25lcg== 38146
IGltcGVyc29u 38147
IGV4dHLDqm1lbWVudA== 38148
2YLYqg== 38149
Y29uZHVjdA== 38150
QXR0 38151
IE11ZWxsZXI= 38152
IEFsaWNpYQ== 38153
IGN5Yw== 38154
IGhhY2tlcg== 38155
IHR5cw== 38156
IGhhaWw= 38157
INC30LDRj9Cy 38158
IHBhc3Nv 38159
IOy2lOqwgA== 38160
IM6I 38161
IHBhY2thZ2Vk 38162
IEN5bnRoaWE= 38163
aGVldA== 38164
5Lit5Zu9 38165
IE5pc3Nhbg== 38166
IFF1ZXN0bw== 38167
6ag= 38168
ZGlk 38169
IM68zrnOsQ== 38170
IEVsbGlz 38171
IEFuYWx5c2lz 38172
Y2Vtb3M= 38173
IGFzZWc= 38174
IE15c3Rlcg== 38175
IENhbw== 38176
IHR1dg== 38177
IEluZHVzdHJ5 38178
7KO86rOg 38179
b3RhbA== 38180
IHBlcXVlw7Fv 38181
YnJhcw== 38182
IGNvbXByZWhlbmQ= 38183
IFNpbXBzb24= 38184
0YHRgtCy0LjQtQ== 38185
b2NyYWN5 38186
0LjRh9C10YHQutC4 38187
IE11c2g= 38188
IExhdXJpZQ== 38189
IHRyaWFuZ3VsYXI= 38190
IFByZXNlbnRz 38191
IEt1bmRlbg== 38192
57S5 38193
5q2m 38194
IElzcw== 38195
IERlY2s= 38196
4buDbg== 38197
IERhcmtuZXNz 38198
IGluZmxhbW1hdG9yeQ== 38199
ZXJlbWlhaA== 38200
IHdhcm1lZA== 38201
dmV5YXJk 38202
IE1lbW9yeQ== 38203
ZXR0eQ== 38204
IHRheHBheWVycw== 38205
4LiT 38206
2KE= 38207
IHByYWN0aXNl 38208
64us6w== 38209
IGRyaWxsZWQ= 38210
bcO8xZ8= 38211
bG9nbw== 38212
IEZhY2g= 38213
pOuhnA== 38214
IMO8YnJpZ2Vucw== 38215
IGtvbm50ZW4= 38216
IG5vcm1hbG1lbnRl 38217
IGFyZ3Vlcw== 38218
aWxpbmd1YWw= 38219
sOulvA== 38220
ZWdhbA== 38221
IHRyYXZhaWxs 38222
b3Z5 38223
0LDRgtC+ 38224
IHJ1dGg= 38225
IExpZ2h0cw== 38226
IGNvbnNpc3RlZA== 38227
15HXqNeZ150= 38228
IHN0ZXJlb3R5cGU= 38229
IHBheWVy 38230
IFJlZQ== 38231
IEFpcmJuYg== 38232
IGRyb3duZWQ= 38233
IFpvZQ== 38234
IGNhbm9weQ== 38235
IGJhcnI= 38236
INC90L7Rhw== 38237
IHBhZ2Fu 38238
IGphcnM= 38239
IHLDqg== 38240
ZXJ2ZXI= 38241
5oi/ 38242
aWViZW4= 38243
IGVzcGVjdA== 38244
IEZp 38245
IHVud2lsbGluZw== 38246
IHRlY2huaWNpYW4= 38247
4bq3dA== 38248
bWVtYmVy 38249
IENhbmFs 38250
2LPZhQ== 38251
IGxpZWJlcg== 38252
IGluZmVyZW5jZQ== 38253
IGhvbm9yaW5n 38254
5ZG1 38255
IENhbXBhaWdu 38256
IGxpbmVhZ2U= 38257
IFN0cmVzcw== 38258
IHZpY3Rvcmllcw== 38259
IGRlamE= 38260
16M= 38261
w6p0ZXM= 38262
YmxpY2s= 38263
INC80LXQvdC10LU= 38264
b3Rocw== 38265
IENvdXBsZQ== 38266
SmFzb24= 38267
IE5pY29sYXM= 38268
0LXQutGB 38269
bGli 38270
IGhlcnJhbWllbnQ= 38271
INeQ15XXnteo 38272
INCy0LjQtNC40Lw= 38273
bWlsbGltZXRlcg== 38274
IHNpbGhvdWV0dGU= 38275
IGRyaXZld2F5 38276
IGNoZXJpc2g= 38277
44Wg44Wg 38278
IHJhbnNvbQ== 38279
IGludGVyZGlzY2lwbGluYXJ5 38280
IFBvcnRhbA== 38281
IHRyYWc= 38282
dGhvb2Q= 38283
IHRlZGlvdXM= 38284
IGdsb3NzeQ== 38285
IHByw6lwYXI= 38286
IENheQ== 38287
IFRvb2s= 38288
IEJvdHRvbQ== 38289
IHppZw== 38290
5as= 38291
5Y2x 38292
cmVwcmVzZW50ZWQ= 38293
4LmA4Lil4Lii 38294
IGRlc2Fycm9sbG8= 38295
7ISc6w== 38296
IHZpc2Nvcw== 38297
IG1pbGxpZ3JhbQ== 38298
IEd1bmQ= 38299
IGZlcm1lbnQ= 38300
ZHJ1bQ== 38301
IGRyYXdlcnM= 38302
TGF1Z2g= 38303
IHBlbG9z 38304
IHBhdmVtZW50 38305
IG1lbW9pcg== 38306
YXZhaXQ= 38307
IDIwNTA= 38308
pOulvA== 38309
IHJhesOzbg== 38310
IGZsb3VyaXNo 38311
IHN0ZXJu 38312
5LiI 38313
IENodW5n 38314
IHNlcnBlbnQ= 38315
IEdlbnRsZW1lbg== 38316
55yf55qE5b6I 38317
a29vaw== 38318
IGx1dA== 38319
aW1wb3J0ZQ== 38320
cGFyZW50 38321
IHdzeg== 38322
IHNjcmVl 38323
IE1pdGFyYmVpdGVy 38324
5be0 38325
bXV0 38326
IOyWmOq4sOulvA== 38327
IHNlbWJsZQ== 38328
IE9X 38329
IGludmVzdGlnYXRvcg== 38330
IENoZXJ5bA== 38331
IEdlcmFsZA== 38332
IHByZXJl 38333
IGNvbXBhcmVz 38334
bnl0 38335
IGRpZmVyZW7Dp2E= 38336
Py0= 38337
IHF1w6E= 38338
16jXmQ== 38339
U2Vu 38340
IGhlcHM= 38341
IGdyYXR1aXQ= 38342
IGNvbnNvcnQ= 38343
IFNUT1A= 38344
IFByb3Rlc3RhbnQ= 38345
IGVsZWN0cm9kZQ== 38346
4pc= 38347
IHNlY3VyZWx5 38348
0LjRh9C10YHQutC+0Lk= 38349
IHTDpMOk 38350
IHJlZ2lzdGVycw== 38351
IEhlYXZlbmx5 38352
b2dseQ== 38353
aXNzw6Q= 38354
IFBoeXNpY3M= 38355
IE1lcmtlbA== 38356
IHLDqXY= 38357
6Zmi 38358
IGVyYXNlZA== 38359
IFNhY3JhbWVudG8= 38360
IGNvZmZpbg== 38361
IGV4YWNlcg== 38362
IGxhbno= 38363
IHBvZXRz 38364
dWxpZg== 38365
IOy5mOs= 38366
IE5lcmQ= 38367
IE5DVA== 38368
IEhvdXI= 38369
bmVobWVy 38370
npjrj4Q= 38371
IFByaW5jaQ== 38372
U3c= 38373
bWllcw== 38374
YXJtZWQ= 38375
IEJlYXRsZXM= 38376
IHByb3BhZ2F0aW9u 38377
IGV4Y2hhbmdlZA== 38378
IGN1bXVsYXRpdmU= 38379
IOynkeyXkA== 38380
IGRlZmVhdGluZw== 38381
5oqx 38382
YmVscw== 38383
IHdlcw== 38384
IE9keXNzZXk= 38385
5L2g5oOz 38386
YXZpb3I= 38387
IOychOyXkA== 38388
IGJyaXQ= 38389
IGhpam8= 38390
REFZ 38391
INin2YTYqtmK 38392
INCh0LXRgNCz 38393
0YPQutCw 38394
ZWRzacSZ 38395
IGltcG9z 38396
IGVsbGFz 38397
IGZpcmVhcm1z 38398
IE5S 38399
INeR15A= 38400
INCf0L7QutCw 38401
YXdp 38402
IOyEseqztQ== 38403
IHB1cGlscw== 38404
IFRhY2s= 38405
IGZyYXNl 38406
IFNoaXA= 38407
IHN0YWQ= 38408
5Lic 38409
IEdyZWF0ZXI= 38410
dW51bg== 38411
aW1tdW5n 38412
Z3Jvd24= 38413
IE5YVA== 38414
IEFtZXJpY2Fz 38415
Zm94 38416
IG1hbnRlbg== 38417
6aCQ5YKZ 38418
INGB0L7Qug== 38419
IHJpa3Q= 38420
bGVjdHJpYw== 38421
ZGVlcA== 38422
INC30L3QsNC10YjRjA== 38423
IGJlbnV0 38424
IEluZnJhc3Q= 38425
IEVtaXI= 38426
INC+0YLQv9GA0LDQsg== 38427
IEtpbWNoaQ== 38428
IEZpbm5pc2g= 38429
tOyggQ== 38430
aW5haXJl 38431
IG9pa2U= 38432
5riF5qWa 38433
IGhvc3RhZ2U= 38434
IEJ1dHRvbg== 38435
2YLZig== 38436
ZWtpbmc= 38437
IEthemFraA== 38438
IGNvbWZvcnRpbmc= 38439
IHNvZw== 38440
IGdyZWV0ZWQ= 38441
Z3VpdGFy 38442
cGF5ZXI= 38443
IHJlbGF0aW9uYWw= 38444
IGNvbnN0cnVpcg== 38445
54m55Yil 38446
b3BpYW4= 38447
IFZvbHVtZQ== 38448
aWV0aA== 38449
0YHRgtCy0L7QvA== 38450
dXJyZWN0aW9u 38451
bGnFm215 38452
IGhlbWlzcGhlcmU= 38453
IEJlYW4= 38454
SUdO 38455
IGvDtnTDvA== 38456
IEZhbGxvdXQ= 38457
IGJyYWNl 38458
57m857qM 38459
z4DOrA== 38460
IEhBUw== 38461
IGfDqQ== 38462
IGNoYXJhY3Rlcml6ZQ== 38463
4bq3Yw== 38464
IE1pbGt5 38465
IHR1bW9ycw== 38466
IG51aXQ= 38467
IEdheg== 38468
IOyeiOuLpOuKlA== 38469
INCz0LDRgA== 38470
ZXNzbWVudA== 38471
IEFiZQ== 38472
IOu9kQ== 38473
IEVpbnNhdHo= 38474
SklO 38475
asOk 38476
Q3J5 38477
IFByb21pc2Vk 38478
INGB0LXRgNC0 38479
b2t1cw== 38480
IHNjYWxhYmxl 38481
INC/0L7RgdC80L7RgtGA0LXRgtGM 38482
w7xja2xpY2g= 38483
IHJlYWxpc20= 38484
IG1heW8= 38485
IGp1dmVuaWxl 38486
IGhlYWRsaWdodHM= 38487
IGfDtnLDvMWf 38488
IFJlZm9ybQ== 38489
IGhhbHZlcw== 38490
Y3puZQ== 38491
IGJyZWFrdXA= 38492
xbxlag== 38493
IHLDpHR0 38494
RGF5 38495
IOydvOuzuA== 38496
IG11ZXJ0ZQ== 38497
IHR1bmVz 38498
IFNtaWxl 38499
cmVjb3Jk 38500
IHJlY2hlcmNoZQ== 38501
YXRpc2ZpZWQ= 38502
IHBvemk= 38503
IGNlbGVicmF0aW9ucw== 38504
aXNleHVhbA== 38505
IFJPQg== 38506
dGhpcmRz 38507
IEZvcnR1bmU= 38508
INGC0L7QuQ== 38509
IGJyYW5kZWQ= 38510
bG9v 38511
IGR1ZA== 38512
IHJhbmRvbWl6ZWQ= 38513
IGNvbWJpbg== 38514
5LiA5Lqb 38515
aWVyYW4= 38516
Y3plbmlh 38517
jeODqw== 38518
IGN1cmF0b3I= 38519
IGFydGVyeQ== 38520
INGD0Yg= 38521
INGH0LjRgg== 38522
IHN1YnNpZGllcw== 38523
IGJsb3Nzb20= 38524
IFR3aWxpZ2h0 38525
IGh5dsOk 38526
IFBvbXBl 38527
IENpc2Nv 38528
INCf0YDQvg== 38529
IGJpcmk= 38530
IGdlcm4= 38531
IHJlYnVpbHQ= 38532
IHdjemU= 38533
IGJlbmVmaWNp 38534
IGRydW1tZXI= 38535
IHNvbGlkcw== 38536
IGRpeW9yc3Vu 38537
44GC44KK44GM44Go44GG44GU44GW44GE44G+44GX44Gf 38538
bGF0ZWQ= 38539
IG11ZGR5 38540
IGhvbG9n 38541
IGNsYXBz 38542
IFJpbmdz 38543
IE9rZXk= 38544
IEJyYXZl 38545
IHZhbHVhdGlvbg== 38546
IG1pZ3JhbnQ= 38547
IGludGVybWl0dA== 38548
IGVpZ2VuZQ== 38549
aWxpYXJ5 38550
44O844OI 38551
bWFya3Q= 38552
a3I= 38553
IFJpYg== 38554
4buZaQ== 38555
IGFjY3VzYXRpb25z 38556
IGFyYWI= 38557
d2FzaA== 38558
IEJhcmR6bw== 38559
IHVnaA== 38560
ZXN0ZXJz 38561
b3BocmVu 38562
IGFsaW1lbnRvcw== 38563
IFV6 38564
1oI= 38565
IDY1MA== 38566
INC/0YDQuNC10YU= 38567
Rkk= 38568
IHNhbXBhaQ== 38569
IHBhcmzDqQ== 38570
aGVzaW9u 38571
IHPEsXI= 38572
IGFwcGFyYXR1cw== 38573
IGNvcnJlbGF0ZWQ= 38574
IFByaW5jaXBhbA== 38575
IGNvcnI= 38576
IE9mZmljaWFs 38577
0LjRh9C10YHQutC40LU= 38578
IHRlcm1pbmFscw== 38579
U2hvdWxk 38580
IHZhY3Vu 38581
IHN0ZWxsdA== 38582
IG1vb2k= 38583
ZXR6dW5n 38584
INC60YDQsA== 38585
IGRhaQ== 38586
INC/0L7Qtg== 38587
VGVhbQ== 38588
IFBQRQ== 38589
INCe0YE= 38590
IExlYWg= 38591
IEl2eQ== 38592
eXN0 38593
IHVoaGg= 38594
IG5pZ2h0dGltZQ== 38595
IHRyZW5keQ== 38596
IHNlY3VyaXRpZXM= 38597
IGNvbnRpbmVudHM= 38598
IGZpcnN0aGFuZA== 38599
IFZlcm9u 38600
IOuCrg== 38601
IGJyb3dzaW5n 38602
IENhZGE= 38603
dHJv 38604
IHRyYW1w 38605
cmVpYg== 38606
IGVyc3RtYWw= 38607
aXJsZXI= 38608
IHBzaWM= 38609
IGdldGly 38610
IE5Q 38611
IGR6aWVjaQ== 38612
0L7QsdGA0LDQtw== 38613
IG1hZ2ljaWFu 38614
IHNjcnV0aW55 38615
IHNsYWI= 38616
IE9U 38617
aXN0eQ== 38618
aXJpZXM= 38619
b3Jlc3Q= 38620
IHRhc2tlZA== 38621
IG1vcmFsbHk= 38622
7JW87KeA 38623
dXN0ZXJlZA== 38624
IGZvb2xz 38625
IGlycmVzcG9ucw== 38626
IGVpbmY= 38627
IHZp4buHYw== 38628
IHNjb3I= 38629
IHBpbGxvd3M= 38630
IEdlZ2Vu 38631
IHR1dHRl 38632
IHF1YXJ0ZXJseQ== 38633
IGRpZG50 38634
IEd5bQ== 38635
IEV0aGVy 38636
INir 38637
0LvQuNGI0LrQvtC8 38638
IHNpZ25hbGluZw== 38639
IE5vZGU= 38640
IERvbmNz 38641
IHlhaA== 38642
IEthbmFs 38643
IGZhZGluZw== 38644
ZXRpbg== 38645
IGluZmx1ZW5jZXJz 38646
IG1lZGFscw== 38647
IGVuZ2luZWVyZWQ= 38648
IGZlcm1lbnRlZA== 38649
6rKg7KeA66eM 38650
IEJlZXRob3Zlbg== 38651
157XqQ== 38652
aW5lbnRhbA== 38653
IOyVjOugpA== 38654
w7x0ZmVu 38655
YWxueWE= 38656
IG92ZXJl 38657
IGRlbmt0 38658
0LDQutGC0LXRgA== 38659
IOKY 38660
IG5lY2VzaXQ= 38661
IGdlbmVyYXRvcnM= 38662
Z3Jhc3M= 38663
INC/0L7QtNGD0Lw= 38664
bGllw59lbg== 38665
QmFy 38666
nOuPmQ== 38667
INC00LXRgtC10Lk= 38668
IHN1Y2tpbmc= 38669
IHN0ZW5jaWw= 38670
IHByaW1v 38671
IEJyZWF0aA== 38672
c3Ryb20= 38673
IGltbWVuc2VseQ== 38674
IGFwcHJlaA== 38675
7KCV7J20 38676
UG9w 38677
IGpvbmc= 38678
IEdpdWw= 38679
IEFESEQ= 38680
IGjDtnJlbg== 38681
IGVsbw== 38682
aXZlbnQ= 38683
IHJ1cw== 38684
IG91dHJhZ2VvdXM= 38685
IG1hc3RlcmVk 38686
IOy7pA== 38687
2YjZgQ== 38688
aXBlcw== 38689
IFJ1ZHk= 38690
SmFjb2I= 38691
IGJ1bGxpc2g= 38692
IHRhcHBlZA== 38693
IGZhdWQ= 38694
aXpvcGhyZW4= 38695
INGB0L7RhQ== 38696
IERhcmxpbmc= 38697
IDE5NjM= 38698
IFByZXZlbnRpb24= 38699
spQ= 38700
IGFiZG9taW5hbA== 38701
c3RvbmVz 38702
IGF2YWllbnQ= 38703
4buVaQ== 38704
bWFrZQ== 38705
IHNhcmU= 38706
IEluc3RhbnQ= 38707
0LrQsNC8 38708
IGtlZXBlcg== 38709
IGJsYW5rZXRz 38710
44Gn44GX44KH44GG 38711
IHN3ZWF0cw== 38712
IE1pbm5lYXBvbGlz 38713
5YWo6YOo 38714
IGdlbm9tbWVu 38715
IGZhc3Rlbg== 38716
IEJydXNzZWxz 38717
5ZG8 38718
IGNhZmV0ZXI= 38719
IGFic29yYmluZw== 38720
IGhhZ28= 38721
IEVsbW8= 38722
IGd1c3Rv 38723
IFlhcA== 38724
TcO6c2ljYQ== 38725
IHRlcnQ= 38726
IGJhbmRh 38727
IG1pbHk= 38728
IHRoZXJlYWZ0ZXI= 38729
IFN0b2NraG9sbQ== 38730
IENhcnNvbg== 38731
IGNhbGlicmF0aW9u 38732
YXZhxZ8= 38733
YW5zYQ== 38734
aWtrZQ== 38735
IGZvcmVzZWU= 38736
IHF1YWxjaGU= 38737
IGRlc3Rl 38738
5qQ= 38739
w7xuw7x6 38740
IGZvcmdl 38741
RGlz 38742
ZXN0ZW4= 38743
IM60zrnOsQ== 38744
IGVuY2Fwcw== 38745
IEdlc3By 38746
IGNoZXJjaGVy 38747
aWNrZXRz 38748
0YLQvtGA0Ys= 38749
Q3I= 38750
INCi0LDQutC20LU= 38751
IHJhYmJpdHM= 38752
IERvdA== 38753
aGVpdGVu 38754
IGNhdXNhbA== 38755
IEZvc3Rlcg== 38756
YWrEhWM= 38757
IGJlcmVpdA== 38758
IGF5dWRhcg== 38759
6auZ 38760
44Gz 38761
c29uZw== 38762
Y29tYg== 38763
IGZyaW5nZQ== 38764
IGN5YmVyc2VjdXJpdHk= 38765
IOucqA== 38766
IGtpZXI= 38767
IGJlc2Now6RmdA== 38768
INC60L7QvdGG0LU= 38769
IGZhY2lsaXQ= 38770
IE5hbWVu 38771
IGJpbGF0ZXJhbA== 38772
dHg= 38773
IFdpc3NlbnNjaGFmdA== 38774
IG51YW5jZXM= 38775
IHJpcHBpbmc= 38776
IGZ5 38777
IFNpY2hlcmhlaXQ= 38778
IEdoYW5h 38779
b2xvbg== 38780
IHRvcHBlZA== 38781
IE1vcm9jY28= 38782
IHJhZGlhbA== 38783
IExFRQ== 38784
IEFuZHJlYXM= 38785
ZWRk 38786
IOyXtOs= 38787
IEFpcmxpbmVz 38788
44GT44KN 38789
IHZhbG9yZXM= 38790
6rec 38791
SHk= 38792
INC30LDQtNCw0Yc= 38793
IEtlbmRhbGw= 38794
INGF0LDRgA== 38795
IFZhbXA= 38796
IHB5dGhvbg== 38797
IG1hbmFnZWFibGU= 38798
IEdlbnRl 38799
b2lzZQ== 38800
aWNpYXJ5 38801
IGltcG9zcw== 38802
IEJ1bm55 38803
aWVzdGE= 38804
QW5kcmV3 38805
IHNlcnQ= 38806
IENlYw== 38807
enphcmVsbGE= 38808
IGF1dG9tb2JpbGU= 38809
IFRpZXJl 38810
YWxsb3dz 38811
5YaG 38812
IOuwgA== 38813
IFNjb3Jw 38814
IEplbGx5 38815
YWdhcmE= 38816
IFN0cmV0Y2g= 38817
IHJlZGVm 38818
IGV4YWNlcmI= 38819
IFNIQQ== 38820
w6lm 38821
b3JzYQ== 38822
IGZsYXdlZA== 38823
IE5vZWw= 38824
PyE/ 38825
IHByb2NlbnQ= 38826
IG1lbnN0cnU= 38827
INC/0YDQvtGH 38828
IGluZmFudHM= 38829
8J+OtQ== 38830
cGF1c2U= 38831
IFJhY2luZw== 38832
IDE5NDg= 38833
IHN1cGVyaW50ZW5kZW50 38834
aWRvcmVz 38835
aWR5 38836
YnJhaGlt 38837
IHVubHVja3k= 38838
IHBlcms= 38839
YW5jaQ== 38840
IOunjOuCmA== 38841
INCc0L7RgdC60LI= 38842
IGZpbmFucw== 38843
IGRpZmVyZW5jaWE= 38844
oIjsnbQ= 38845
6YWN 38846
T1JZ 38847
IFRhYw== 38848
24zYpw== 38849
IGRlc2Vt 38850
INCy0LDQttC90L4= 38851
IEpV 38852
IOyeiOyeluyVhOyalA== 38853
IM6d 38854
IGluZm9ybWF0aW9ucw== 38855
IEhFTA== 38856
aHN0 38857
INC/0L7Qs9C+0LLQvtGA 38858
IHZvaXR1cmU= 38859
IHJldXM= 38860
w6RuZGln 38861
INC/0L7RhdC+0LY= 38862
amluZw== 38863
IGRydQ== 38864
YWx0cmE= 38865
IHByb2R1aXRz 38866
IGtpdGU= 38867
IGV5ZWJhbGw= 38868
IEJlbHQ= 38869
IFJlc3RhdXJhbnQ= 38870
IGdhbWI= 38871
IHBvcnJpZGdl 38872
aXR0ZXJz 38873
IGNvbnZlcnRz 38874
IHlhcmTEsW0= 38875
IG3DoXhpbW8= 38876
d2lydHNjaGFmdA== 38877
IO2VmOuCmOs= 38878
IOykgA== 38879
IGljZWJlcmc= 38880
IHZvcmJlaQ== 38881
IDI1Ng== 38882
b2NyYXRpYw== 38883
IHJlY2tsZXNz 38884
b25uZXI= 38885
IG3DunM= 38886
IGxvZ2ljYWxseQ== 38887
IFByaXNvbg== 38888
IE5ldHo= 38889
IHZhY2FudA== 38890
IG5pbW10 38891
IEhBUlI= 38892
INC30L7Qsg== 38893
IERlZQ== 38894
cmluZ2U= 38895
bmllc3Q= 38896
IFJ1bGVz 38897
7Iqk65+9 38898
Y3Vzc2lvbnM= 38899
IGZsb3JhbA== 38900
IGNvbnN0cmFpbmVk 38901
IGRpZmZlcmVudGlhdGlvbg== 38902
IFF1ZWJlYw== 38903
INuB24zaug== 38904
IHDDumJsaWNh 38905
aXRlbA== 38906
IGFjY29tbW9kYXRpb25z 38907
IEdyw7w= 38908
7Zw= 38909
IHBpY2tsZXM= 38910
0LjRh9C10YHQutC40YU= 38911
IGNvbW1pc3Npb25z 38912
IEJhZWs= 38913
IMOnb2N1xJ8= 38914
IE1lZGl1bQ== 38915
IHBlcmlvZGljYWxseQ== 38916
IHdvbmRlcmZ1bGx5 38917
IHN0YWZmaW5n 38918
7JuQ6w== 38919
cmlyZQ== 38920
Zmxl 38921
IE1jTA== 38922
INGC0LXQvw== 38923
INC/0LXRgNC10Lo= 38924
0L3QvtC70L7Qsw== 38925
IO2BrOqyjA== 38926
55m854++ 38927
IHByb3NwZXJvdXM= 38928
IFNwaXJpdHVhbA== 38929
IENoaWNr 38930
RElB 38931
INCf0YDQuNCy0LXRgg== 38932
IHBlcsOt 38933
0YzRjtGC 38934
IGNvbnN1bHRhbnRz 38935
IEVhcmw= 38936
5LuK5bm0 38937
IHJ1aW5pbmc= 38938
0L7RgNC1 38939
IHBlbnNlcg== 38940
IHRha2llag== 38941
IHN0cmVuZ3RoZW5lZA== 38942
IExpcXVpZA== 38943
0L7QvdC10YY= 38944
0LDQstCw0YLRjA== 38945
IGNhbWVy 38946
IGRpc2FncmVlbWVudA== 38947
IGJhdGhpbmc= 38948
IFlvc2g= 38949
YWFs 38950
cHJlY2hlbg== 38951
UklTQURBUw== 38952
IHN1cGVyc3Rhcg== 38953
5oGt 38954
0LvRj9GC0Yw= 38955
IG5pYg== 38956
IFRoZXJt 38957
IERBTklFTA== 38958
IHBhdw== 38959
IGxpcXVpZHM= 38960
IGNhcGFjaXQ= 38961
YXJrZW4= 38962
IHZhZ2luYQ== 38963
IG1hc2hlZA== 38964
IGVtZXJnZXM= 38965
eXNjeQ== 38966
IHVucmVsYXRlZA== 38967
IEd1aWxk 38968
IGludmVydGVk 38969
aXRpdmVz 38970
VHJh 38971
IGJlZ3I= 38972
IGFsdGU= 38973
7KeV 38974
44KB44Gm 38975
INGA0LDQt9GA0LDQsdC+0YI= 38976
ZmluZGVy 38977
INC00LDQu9C10LU= 38978
INCx0LvQsNCz0L7QtNCw0YA= 38979
d2Fsa2Vy 38980
IGNyYXRlcg== 38981
YXNzYWRvcnM= 38982
cmVuY2Vz 38983
aW5za2k= 38984
IEtJTQ== 38985
IEVsbGlvdA== 38986
MjAxNw== 38987
IFNy 38988
aW5rYQ== 38989
YW5vdg== 38990
IOyemOuquw== 38991
IHByb3ByaWV0YXJ5 38992
ZGlzcGxheXN0eWxl 38993
INGB0LjQvA== 38994
INC40LfQsQ== 38995
IFBhbmVs 38996
IGluc3RpbmN0cw== 38997
IENvbW11bmljYXRpb25z 38998
6bq7 38999
bWlkdA== 39000
IOunjOuTpOyWtA== 39001
INGB0LvQvtCy0LA= 39002
IEdpbGJlcnQ= 39003
55uu5YmN 39004
0KLQsNC6 39005
dm9vcmJlZWxk 39006
0LXRjtGB0Yw= 39007
YXJ5bg== 39008
cXVleg== 39009
IGRhcnQ= 39010
0ZbRiA== 39011
IEh1dA== 39012
U2Fs 39013
IHNvdXRoZWFzdA== 39014
IHBlc3RpY2lkZXM= 39015
IGhlbGljb3B0ZXJz 39016
IGVuZHVyZWQ= 39017
aWFkYQ== 39018
IGJyZXdpbmc= 39019
7Jes6w== 39020
INGB0LLQvtCx0L7QtA== 39021
IFNhaW50cw== 39022
IEZyYW7Dp2Fpcw== 39023
IEVjb25vbWljcw== 39024
IGRpc2xvYw== 39025
b3Bob2JpYQ== 39026
Q2FtZXI= 39027
IG5lZ290aWF0ZWQ= 39028
INGB0YLQsNC70Lg= 39029
7Iqk7YE= 39030
b2dpZQ== 39031
IHRzdW5hbWk= 39032
IHBlZWxlZA== 39033
IG1vdGl2YXRpb25z 39034
6Kit 39035
b3N0YXQ= 39036
Zmxhbg== 39037
IERBQw== 39038
IGthdg== 39039
J1JF 39040
IFBlYXJzb24= 39041
YmJl 39042
Y3plbmll 39043
IGF0ZW7Dp8Ojbw== 39044
7Ya166C5 39045
44Gj44Gh 39046
INGD0LTQsNGA 39047
IGludHJvZHVjdG9yeQ== 39048
IEljaQ== 39049
64yA6w== 39050
YWthdA== 39051
IHRyZW5jaA== 39052
IHByb2NlZWRlZA== 39053
IENvaW4= 39054
IGRlcmVjaG8= 39055
IFJlZGU= 39056
5q+b 39057
0LDQvdC90YvQuQ== 39058
IGluY2FyY2VyYXRlZA== 39059
IFJpY2htb25k 39060
Um9jaw== 39061
IFBhdg== 39062
IEthcm1h 39063
dWdlcw== 39064
IGNvbnRlw7o= 39065
67mE 39066
IOq3uOunjA== 39067
IEdvbmU= 39068
IHdzcMOzxYI= 39069
IFJhaG1lbg== 39070
dW5rZW4= 39071
IOykkeyalO2VnA== 39072
IGli 39073
IGF0dGFjaGluZw== 39074
SGF5 39075
IHN1a2E= 39076
7I25 39077
IHBpdm90YWw= 39078
IFJlc3BlY3Q= 39079
w61kYQ== 39080
SUI= 39081
IFZlcmFudHdvcnQ= 39082
d2lldA== 39083
IGZvcmVuc2lj 39084
0YDQuNGB0YI= 39085
INC/0YDQuNC90YbQuNC/0LU= 39086
IG1hcmtpbmdz 39087
IGtldHRsZQ== 39088
IE9wZXJh 39089
IERvY3RvcnM= 39090
IHNocmVkZGVk 39091
IHJlY3Vlcg== 39092
IHZpZ2ls 39093
IEZhaWw= 39094
IGVudHJldg== 39095
INC00YPRiA== 39096
IG91dGJyZWFrcw== 39097
6LWw5ZCn 39098
IM+Azr8= 39099
IHJvZ3Vl 39100
YW5nbGVk 39101
IHllYXJseQ== 39102
IENyZWVk 39103
IHdhbQ== 39104
IGxvdHVz 39105
6rO86w== 39106
44CB44CB 39107
IFNwaXQ= 39108
IEl0dQ== 39109
IHN0cmFpbnM= 39110
IHN0YW1wZWQ= 39111
IHBsYWludA== 39112
IHBvdGlvbg== 39113
IGNvbnNvbGlkYXRpb24= 39114
6KmV 39115
0L7Rh9C60YM= 39116
IHZsb2dnaW5n 39117
IHNsYXRl 39118
IEF1ZnQ= 39119
IEluY29y 39120
4burbmc= 39121
p5A= 39122
ZW5o 39123
IGhlacOf 39124
IGRvbWVzdA== 39125
IFN0cm9t 39126
5Y2z 39127
YWtpcw== 39128
IGZyYWdlbg== 39129
IGZpbmVy 39130
IFN1Zw== 39131
IHVwaGlsbA== 39132
IMOpw6lu 39133
4oCmKQ== 39134
INGB0L7Qvw== 39135
IENvcmV5 39136
IHNpZWJpZQ== 39137
IG11c2U= 39138
IGNsb3Zlcw== 39139
IHBvdXM= 39140
IEZpbmFueg== 39141
IFJvdXRl 39142
YW1hdA== 39143
IG11dHVhbGx5 39144
INCy0L3Rg9GC0YDQuA== 39145
IFNlbGVuYQ== 39146
65Q= 39147
IEdhdXNzaWFu 39148
67aA7YSw 39149
INeR15s= 39150
IGVqZXJj 39151
5b6u 39152
a2Vh 39153
IEdlcnJ5 39154
IFNpYw== 39155
5aSn55qE 39156
IDE5NjY= 39157
aWVzZQ== 39158
IGZvc3NpbHM= 39159
IGVzdGFk 39160
IEthbmU= 39161
Y2nEhw== 39162
IOycoO2KnOs= 39163
INC/0LDQvA== 39164
IENydWlzZQ== 39165
aW50w6lyaWV1cg== 39166
IGJla2FubnQ= 39167
IFBvZGU= 39168
IGRlbWFuZGVy 39169
UmVt 39170
IGludmFkZQ== 39171
IGRlY29yYXRpbmc= 39172
cm9waWM= 39173
IGNvd2JveQ== 39174
IFBob3Rv 39175
b3BvbGl0 39176
IOy7rOufrOs= 39177
IHJlYXA= 39178
IGhhbmR3cml0aW5n 39179
4LmE4Lij 39180
IOua 39181
INio2LnYrw== 39182
IE10 39183
2YA= 39184
IHNwYWNlc2hpcA== 39185
IG5hdGlvbmFsaXNt 39186
IGNvdW5jaWxz 39187
IEdyaWZmaW4= 39188
IEFobWVk 39189
IGNsaWNo 39190
IE9M 39191
d2w= 39192
IFBpbG90 39193
5a6u 39194
IGFjcm9ueW0= 39195
IGdlbHM= 39196
IGVsZWN0cm9seQ== 39197
6JM= 39198
INC80L3QvtC5 39199
IGVwaXNvZA== 39200
IERpZXNlcw== 39201
IEFUUA== 39202
IGVkaXlvcnVt 39203
IGV4cHJlc3Nlcw== 39204
IGV4aGliaXRz 39205
Q29tbQ== 39206
INC60YDRg9C/ 39207
IG1hdGFy 39208
IDIwMjU= 39209
IEFydGVt 39210
dmFzaXZl 39211
csOg 39212
IGJlxZ8= 39213
6buD 39214
IGxpemFyZA== 39215
IGZpbGxl 39216
IOyniOusuA== 39217
INC80L7RiQ== 39218
IHTDvHI= 39219
IGN1bHByaXQ= 39220
IHdvdmVu 39221
IEFOWQ== 39222
bmlt 39223
IHRheQ== 39224
IHByb21pbg== 39225
IGFjb21wYQ== 39226
IGlkw6k= 39227
IGJvaWxlcg== 39228
IFRoZW1lbg== 39229
IGF2ZW51ZQ== 39230
IE11ZA== 39231
INC90L7QstGL0LU= 39232
IHdpdG5lc3Npbmc= 39233
IGxhbmNl 39234
IENIQU4= 39235
IEJldmVy 39236
2KrZhQ== 39237
IGNoZW1vdGhlcmFweQ== 39238
S2luZw== 39239
IGLEmWTEmQ== 39240
IGF0dWFs 39241
IHRpdmU= 39242
IHRhbGtpbg== 39243
IHF1ZWRhcg== 39244
aWXDnw== 39245
ZWRlbA== 39246
IOyWtOygnA== 39247
IGpvZ2Fy 39248
IMO2cg== 39249
IHVuZGVydGFraW5n 39250
IFN0cmVuZ3Ro 39251
IG1pbGjDtWVz 39252
IFdpbmU= 39253
IE1vbHQ= 39254
6K6y 39255
44GR44KM 39256
IHVuZGVybWluZQ== 39257
IEFyY2hpdmVz 39258
dmFuYQ== 39259
bWVyY2lhbA== 39260
TUM= 39261
IGNhc3Rl 39262
0L/RgA== 39263
IGxlZ2lzbGF0b3Jz 39264
dWxhdG9ycw== 39265
w6puaW8= 39266
IOuNsOs= 39267
INGF0L7RgtC40YLQtQ== 39268
INC90LXQug== 39269
IHN1cm4= 39270
IGNvbnNjaQ== 39271
IFBPVw== 39272
IGN1bGluYXJ5 39273
IEtBVA== 39274
IEZvbGtz 39275
0YvQstCw0LXQvA== 39276
INCy0L7Qug== 39277
44GR44KL 39278
c2VydmljZQ== 39279
cHRz 39280
INC/0L7QsdC10LQ= 39281
5piv5ZWK 39282
IHRlbnRz 39283
IG5vcmQ= 39284
U1RF 39285
IHJlcHVibGljYW4= 39286
IHd5aw== 39287
IG1pbmlvbnM= 39288
6JmV 39289
IG1lbWFuZw== 39290
amVzdA== 39291
IGNvbXBhcmF0aXZl 39292
IHR5bGU= 39293
Y2FyYm9u 39294
YmVkaW5ndA== 39295
a3Nlbg== 39296
IG5lZ2F0aXZpdHk= 39297
IHNqw6Rsdg== 39298
IGTDug== 39299
5omA5pyJ 39300
IHJlY2FsbGVk 39301
Y3Jh 39302
IFRhZGE= 39303
INGA0YPQutC4 39304
INC+0L/RgNC10LTQtdC7 39305
IHByb2NyYXN0 39306
IGpvZ29z 39307
IE9v 39308
IEhlYXJ0cw== 39309
IMOpY2g= 39310
IGtzacSFxbw= 39311
IGNvYXJzZQ== 39312
IFR1YmU= 39313
IEdyZWVucw== 39314
IMOpbg== 39315
IGR1bWJiZWxs 39316
INGC0Lg= 39317
IHF1ZXJlcg== 39318
2KfYrQ== 39319
z4POtc65 39320
INC/0YDQsNCy0LjQu9GM0L3Qvg== 39321
INC/0LDQvw== 39322
IGNvbXByYQ== 39323
IHTDqXI= 39324
IEFudGVz 39325
IG9wdGltdW0= 39326
IGJpc2N1aXQ= 39327
zrrOuQ== 39328
YWN6ZWdv 39329
IOyLnOqwhOydtA== 39330
IE1hcmluZXM= 39331
dmVybw== 39332
IHZhY2NpbmF0aW9ucw== 39333
IHBldHR5 39334
cml0ZXJz 39335
INCw0Ls= 39336
Y291bnRyeQ== 39337
IGNvdW50ZXJz 39338
IGF0dGVuZGFudA== 39339
IEh1aQ== 39340
44Go44GE44GG44GT44Go44Gn 39341
Y2th 39342
0YHRgtCy0LXQvdC90YvQuQ== 39343
Z3V5 39344
IHRyaWNrZWQ= 39345
IFJFRA== 39346
IHRocmlsbGluZw== 39347
z4DOv865 39348
IHBpZ2d5 39349
IGFudW5jaQ== 39350
T1JURVI= 39351
IFZhbHVl 39352
IHJvbmQ= 39353
IEFEQQ== 39354
IHBvc2Vy 39355
aG9yZXM= 39356
IFJvbGFuZA== 39357
k68= 39358
IG5vaXI= 39359
INep15DX 39360
67Cc 39361
aWVtYW5k 39362
INC/0L7RgtC10YA= 39363
6rOz 39364
IOqxsQ== 39365
IGZvcm1hdHRpbmc= 39366
IExlZA== 39367
6KeA55y+ 39368
IGtpbGxlcnM= 39369
IMSR4bqleQ== 39370
IGhhYXI= 39371
YWdhaW4= 39372
ITwv 39373
IHNvbWV0aGlu 39374
IGNvdWdoaW5n 39375
IG5hdmU= 39376
IHByb3NwZWN0aXZl 39377
IEhL 39378
IFJlc2N1ZQ== 39379
bWF5YmU= 39380
Z2dlcg== 39381
INGA0LDQsdC+0YLRgw== 39382
15XXnNed 39383
dGFpbHM= 39384
7ZWY7ZWY 39385
IGV5ZWxpZA== 39386
IGN1c3RvbWl6YXRpb24= 39387
YXZpbGlvbg== 39388
IHByb2NoYWlu 39389
IGdsYXpl 39390
5oOF5rOB 39391
U2lt 39392
INC+0L/QsNGB 39393
IG1vc3F1aXRvZXM= 39394
IGZlbnQ= 39395
IGNhcGFjaXRpZXM= 39396
IGFwb3N0bGVz 39397
IGFsdHVyYQ== 39398
IOusuw== 39399
IHNlcm9udA== 39400
IEFueXRpbWU= 39401
pbTripQ= 39402
IGNvc3BsYXk= 39403
IHNwYWM= 39404
IHNhbWVu 39405
44OE 39406
dWNj 39407
acOocmVz 39408
IHNpYmxpbmc= 39409
IENvY2s= 39410
IOuPhQ== 39411
INC/0YDQtdC00YHRgtCw0LLQu9GP 39412
IGluc3RhbGxtZW50 39413
IGRpamU= 39414
IE1DVQ== 39415
IEVI 39416
IE5pbmc= 39417
IHByZXBhcmVz 39418
IGh5cG9jcg== 39419
cHR5 39420
IGthZMSxbg== 39421
IEZyb3plbg== 39422
aGF1bA== 39423
IEt5bGll 39424
6YCZ5qij55qE 39425
IHNodWZmbGU= 39426
IGVsZW1lbnRhbA== 39427
IGF1w59lcg== 39428
IEtOT1c= 39429
IEFMSVNTQQ== 39430
WkE= 39431
7LKg 39432
576O5YWD 39433
IHJlY2l0ZQ== 39434
IHNjcmli 39435
IDExNQ== 39436
5LyR 39437
IHN0YXJyZWQ= 39438
IGxlcXVlbA== 39439
IGJyZXdlcg== 39440
IE9wcG9ydHVu 39441
IHLDpA== 39442
IGNob3BzdGlja3M= 39443
IEthaA== 39444
IEV0aGlvcGlh 39445
IGhhbmRtYWRl 39446
IGVyZm9sZw== 39447
IER6 39448
aXR0ZW5z 39449
6KqN54K6 39450
0LLQsNC7 39451
zrfOvQ== 39452
5Yqe 39453
44OT 39454
YnJpbmdlbg== 39455
IHVucGx1Zw== 39456
IG9mZnM= 39457
IGhlcm1hbg== 39458
bGllZA== 39459
YXNvbmlj 39460
IFNlcmJpYQ== 39461
IEd1YXRlbQ== 39462
IC4uLiI= 39463
IGVycmVpY2hlbg== 39464
IGFtYmlndW91cw== 39465
IFdoaXRuZXk= 39466
enVm 39467
TUFORA== 39468
oLU= 39469
IHNxdWVlemVk 39470
44Gd44GG44Gg 39471
eWFz 39472
6b6N 39473
IFNob2Nr 39474
IHV0aWxpc2U= 39475
dWtv 39476
Ym9sdA== 39477
IG1vdGlm 39478
IGlubWF0ZXM= 39479
IGNvcnJ1cHRlZA== 39480
IGNvbmNyZXQ= 39481
IENyaXRpY2Fs 39482
IFNpbmdpbmc= 39483
INGE0YPQvdC6 39484
6a2U 39485
bm92YQ== 39486
cmViYmU= 39487
ZHQ= 39488
VW5pcw== 39489
IHdlYmNhbQ== 39490
IGNhbW91Zmw= 39491
S2Vu 39492
IGxhd3N1aXRz 39493
IENvbnN1bWVy 39494
IHJlY29sbA== 39495
IGtsZWluZXI= 39496
IEZJRkE= 39497
IDE5NjI= 39498
6K2m 39499
IG1hbGFk 39500
IOywvQ== 39501
IMOldA== 39502
IGluZmx1ZW5jZXI= 39503
IEFydGlzdA== 39504
c3Rp 39505
44Gq44KL44G744Gp 39506
4Lin4Lii 39507
eXPFgg== 39508
IEJpYW4= 39509
iOuEpA== 39510
IGZpcmVwbGFjZQ== 39511
IEFwcGxpY2F0aW9u 39512
IG1uaWVq 39513
IGFjaWRpYw== 39514
IE1vcm1vbg== 39515
c3Nh 39516
5YuZ 39517
IHNuZWFreQ== 39518
IG9qb3M= 39519
IHZvdWQ= 39520
IERhaQ== 39521
IGdyYXNzcm9vdHM= 39522
IFVuYmVsaWV2YWJsZQ== 39523
IEdhYmU= 39524
IEV4dHJlbWU= 39525
IGhhc3NsZQ== 39526
IGNvYg== 39527
bXVtYmxpbmc= 39528
UGFzcw== 39529
jOufrA== 39530
IHN5c3RlbWF0aWNhbGx5 39531
IHNldmVudGVlbg== 39532
z4DOtc65 39533
4pmh 39534
INC60L7Rgg== 39535
IHNlbmRpcmk= 39536
IGJhdGhyb29tcw== 39537
IFN0ZXJu 39538
IEFyZHVpbm8= 39539
6Lk= 39540
Y3JpYmluZw== 39541
IHJlb3BlbmluZw== 39542
IGNlcnY= 39543
cGVl 39544
QVJJ 39545
IGNhZHJl 39546
IEFuY2g= 39547
TGVl 39548
IE1BWA== 39549
IG3DpG5u 39550
IGNob3Jlcw== 39551
IGFkZXNzbw== 39552
5p2R 39553
IE5pZw== 39554
IGRpc3NlcnRhdGlvbg== 39555
IFZheQ== 39556
U1RBTEs= 39557
0LDQutCw 39558
YXZhdA== 39559
56C0 39560
IHB1bmt0 39561
IHBhZGRpbmc= 39562
IFRlbXBs 39563
IGVqZQ== 39564
IO2EsA== 39565
IGF6dA== 39566
IOuMgO2GteuguQ== 39567
IHJlYXJyYW5nZQ== 39568
w6FjaA== 39569
IOyCrOuejOuTpA== 39570
IGZyZWFraW4= 39571
Y3JpcmU= 39572
IOy7pOs= 39573
IEV4cGxhaW4= 39574
IM+Ez4nOvQ== 39575
IGJvZGlseQ== 39576
IExlaXN0 39577
IHNpZ3Vp 39578
IGJ1bmtlcg== 39579
IGF6dWw= 39580
IEhhdXNo 39581
U3Vi 39582
INCQ0L3QtA== 39583
INC60YDQsNC5 39584
IGlsbGVnYWxseQ== 39585
IE11eQ== 39586
IEZlaQ== 39587
IEJhbmFuYQ== 39588
IHNjaG9sYXJseQ== 39589
IFByenk= 39590
IE1vc3M= 39591
IEZpbHRlcg== 39592
IOyWtOuWoQ== 39593
IE1heHdlbGw= 39594
dGVuc2U= 39595
IGxvbmdpdHVk 39596
IGxhbmdzYW0= 39597
INee16c= 39598
c21pdGg= 39599
aXphZGE= 39600
INC90L7RgNC80LDQu9GM0L3Qvg== 39601
IFZvbGw= 39602
IEVsZW5h 39603
5pa56Z2i 39604
INGF0L7RgtGM 39605
IERhYmVp 39606
IGNvbnNlcnZhdGl2ZXM= 39607
IHByw7Nwcmlh 39608
IERpZXNlcg== 39609
IEJyZW5kYQ== 39610
b29raWU= 39611
IGJhbmM= 39612
44Ov 39613
7J207KY= 39614
7JuD7J2M 39615
IGtlaA== 39616
IHdlZGRpbmdz 39617
IHRodW5kZXJzdG9ybQ== 39618
5pS+5b+D 39619
IENvb3JkaW4= 39620
7IiY6rCA 39621
IHByemVjaQ== 39622
6ZKx 39623
T1NTVEFMSw== 39624
bWFhbg== 39625
IOqxtOs= 39626
INio2Yc= 39627
IMW8YWQ= 39628
IHlhY2h0 39629
IGfDtnQ= 39630
IGJsZWFjaA== 39631
IHNob3J0ZW4= 39632
INGB0YLQsNC70L4= 39633
dXNhbg== 39634
IOyekOyXsA== 39635
IGRlcnM= 39636
eGlz 39637
jZTri4g= 39638
IHF1YW50aWRhZGU= 39639
IG9wcHJlc3NlZA== 39640
INC30LDQutC+0L3Rhw== 39641
5LiI5aSr 39642
44GI44GI 39643
INGH0LXRgtGL 39644
INCd0LDQv9GA0LjQvNC10YA= 39645
dWxw 39646
5oCW 39647
2YLZiNmE 39648
0L7Rh9C1 39649
zqzOuw== 39650
emVuaXU= 39651
IGZvcm1hdGlvbnM= 39652
IHNwYXJrZWQ= 39653
IEVudHdpY2tsdW5n 39654
YWxscw== 39655
IHZpdmly 39656
IGV4cGlyYXRpb24= 39657
b3RpbmU= 39658
INCn0LXRgA== 39659
IFR1cm5pbmc= 39660
IHRhcmlmZnM= 39661
IG5hc3TEmXA= 39662
IGFiaWRl 39663
aWtzaQ== 39664
IGZsYXNoZXM= 39665
IGRpc3B1dGVz 39666
IOyytA== 39667
IG1lcmFr 39668
IGVub3Jtb3VzbHk= 39669
emFobA== 39670
IGbDvGhydA== 39671
0LLQvtC9 39672
INC30LDQstC40YE= 39673
IHBlcnNldmVyYW5jZQ== 39674
IGRpdmlkZW5kcw== 39675
IGNvbnRlc3RhbnRz 39676
IHByb3N6xJk= 39677
IEZyYW5rZW4= 39678
44KN44GG 39679
IGV4cGxvcmVy 39680
IGJ1ZmZhbG8= 39681
4oCV 39682
IGVjb2xvZ3k= 39683
IHNjYWxhcg== 39684
IGNyYW4= 39685
zrXPhM6xzrk= 39686
xbx5xIc= 39687
IOyalOs= 39688
IGdpYQ== 39689
IEdvZw== 39690
IFByaXY= 39691
IOunkOydhA== 39692
IFJlYXNvbg== 39693
cmFrdGlvbg== 39694
IERlYm9yYWg= 39695
IGtpdHRlbg== 39696
IEVkaW4= 39697
5Lm+ 39698
cGllag== 39699
IOuLtA== 39700
IG3DoXF1 39701
IGJpZGRpbmc= 39702
IGFmZmluaXR5 39703
IGFpa2E= 39704
Zm9saw== 39705
IENvbnNl 39706
IGRldXRzY2hlbg== 39707
6IY= 39708
IGRlYml0 39709
xLHEn8Sxbg== 39710
aXNlbA== 39711
IOykkeq1rQ== 39712
IOutkOqwgA== 39713
IHRydXN0d29ydGh5 39714
IFN0YXJ0ZWQ= 39715
5pWR 39716
w7xyZA== 39717
INC/0L7QvdGP0YLQvdC+ 39718
IHNjaWVudGlmaWNhbGx5 39719
UG9kcw== 39720
Q1JPU1NUQUxL 39721
IHByZWd1bnRhcw== 39722
IGNhbG1pbmc= 39723
IFByZW1pZXJl 39724
15vXqQ== 39725
INGF0L7Qu9C+0LQ= 39726
IGNhcGl0YQ== 39727
IHRvbWE= 39728
IG11cm0= 39729
IGZ1ZXJ6YQ== 39730
IEhhbmk= 39731
5oiR5pyJ 39732
w7xm 39733
YXJsb3M= 39734
IGjDpHVm 39735
44GR44Gm 39736
IG9zb2J5 39737
amVnbw== 39738
INC/0LjRgQ== 39739
IGNhbG1seQ== 39740
aWRldA== 39741
YnVjaA== 39742
Z29uZQ== 39743
IHZpc2Nvc2l0eQ== 39744
IG1vZGFs 39745
IGdlc2Ft 39746
IEh6 39747
IG11bmljaXBhbGl0aWVz 39748
IGNpcmN1bGF0aW5n 39749
b2xpbmE= 39750
U2hv 39751
6aKR 39752
IEJlbmVk 39753
b2x1 39754
IHJlc3Rz 39755
IGzDpW5n 39756
INCe0LTQvdCw0LrQvg== 39757
IHByemV3 39758
IHBlcHA= 39759
IG1hcnJpYWdlcw== 39760
IEJJRw== 39761
YW5kYW4= 39762
IG1hZ2ljYWxseQ== 39763
IGJhYnlz 39764
IOuMkw== 39765
IGhhY2tlcnM= 39766
QmFieQ== 39767
IE1vbnN0 39768
IGNpZXI= 39769
IEFyYWJz 39770
INC80LDQs9Cw0Lc= 39771
IEluZG9uZXNpYW4= 39772
44GE44GG44GT44Go 39773
IE1hcmt0 39774
IGRhY2h0ZQ== 39775
IFNjaMO8bGVy 39776
IFZORA== 39777
IHNwaWVsdA== 39778
IHBlcmx1 39779
44K0 39780
5a2Y 39781
INC/0YDQvtGF0L7QtA== 39782
IHNhbHRlZA== 39783
IGltcHJvdmlz 39784
IEluc3Ry 39785
dmVsbWVudGU= 39786
IG5lc3M= 39787
IGZ1bmd1cw== 39788
IGNvbGxhYm9yYXRvcnM= 39789
IFZpcnVz 39790
ZXN0YXI= 39791
IHByb2plY3Rvcg== 39792
INCf0YDQsNCy 39793
IGFnaWxpdHk= 39794
15nXoNeV 39795
ZXJlbA== 39796
INCy0L7Qt9Cy 39797
INCx0LDQtw== 39798
IENhdGh5 39799
xJ91 39800
INCz0L7QstC+0YDQuNC7 39801
YmlsaXR5 39802
IExhbmM= 39803
IEtpbWJlcmx5 39804
IEJyaWVm 39805
5YW3 39806
IHV0dmVjaw== 39807
IGdvZ2dsZXM= 39808
IHByZXNjaG9vbA== 39809
56eN 39810
QVRIRVI= 39811
IG1vdGl2ZXM= 39812
IEJvbmc= 39813
RVg= 39814
IGNoaWxseQ== 39815
IEFkdmlzb3J5 39816
4oCL4oCL 39817
INC60L7RgtC+0YDQvtC8 39818
IHRyYWl0b3I= 39819
IGRlbWFzaWFkbw== 39820
INGG0LXQvQ== 39821
INC80L7QuA== 39822
5Z6L 39823
IG11bHRpZg== 39824
7JSs 39825
IEFsZXhpcw== 39826
IHppZXQ= 39827
IFJhbWE= 39828
YnJhbmNl 39829
IHNhbmN0aW9u 39830
aXRvdXM= 39831
15XXmg== 39832
IOuztOuC 39833
0YHRgtCw0L3QvtCy 39834
6Laj 39835
INGA0LXRgQ== 39836
IENodXJjaGlsbA== 39837
INC/0YDQtdC3 39838
IElP 39839
IEdlZQ== 39840
IEdhdGhlcg== 39841
YXRvcmk= 39842
VHlsZXI= 39843
INC90LXQvNC90L7Qtg== 39844
IGLDpWRl 39845
IEtpbGxlcg== 39846
IHR1YmVy 39847
IFJhbWFkYW4= 39848
4b8= 39849
aWVodA== 39850
IHN0cmFuZ2VseQ== 39851
0LvRgw== 39852
IHJlZGVzaWdu 39853
IGluY3VtYg== 39854
IGJlcmFiZXI= 39855
IFZvbGtzd2FnZW4= 39856
bWV0YWw= 39857
ZHp5 39858
cGNpw7Nu 39859
IOyViuyVhA== 39860
5ZSx 39861
5aS0 39862
IEdvb2RuZXNz 39863
0LjQstCw0LXRgtGB0Y8= 39864
YmFobg== 39865
IEFudGFyY3RpY2E= 39866
0LXQutGC0L7RgA== 39867
IGhvbWVvd25lcnM= 39868
emVpZ3Q= 39869
IO2YhOyerA== 39870
7KeA64+E 39871
IGdlb2dyYXBoaWNhbA== 39872
dGhpbmtpbmc= 39873
IGdvc3Rh 39874
IEltYW0= 39875
dWxpZmxvd2Vy 39876
ZGFn 39877
YW5udA== 39878
YWtvdg== 39879
IGRvd253YXJkcw== 39880
7LK06rCA 39881
Q1VCRQ== 39882
INCa0YHRgtCw0YLQuA== 39883
INC/0L7Qu9C+0LI= 39884
IHBsYXRlYXU= 39885
44GE44GN 39886
4bil 39887
IGNobG9yaW5l 39888
IGFjY2VsZXJhdG9y 39889
IHNvbHZlcw== 39890
IEdyYXNz 39891
cGlhbm8= 39892
INqp2Kc= 39893
INio2Ko= 39894
IFJvY2hlc3Rlcg== 39895
INmH2Yo= 39896
IGNvbGxlY3Rz 39897
jZTrnbw= 39898
IENoZWVy 39899
bGluZ2Vu 39900
INGA0LDQt9Cz 39901
IGFtw6lyaWM= 39902
aHRh 39903
RUNU 39904
IGFydGlmaWM= 39905
IFBheVBhbA== 39906
aGFuYQ== 39907
U3RlcGhlbg== 39908
IEdlc3Q= 39909
cGhhbHQ= 39910
IHJlcGxpY2F0aW9u 39911
IFdpbGxpZQ== 39912
IG5ldXRy 39913
IGlycmF0aW9uYWw= 39914
IGRhZG9z 39915
IEFpZA== 39916
a2Ft 39917
YW50ZXI= 39918
INC00YPQttC1 39919
IGRldG9u 39920
IGhhcmU= 39921
IGJldHM= 39922
YmFnYWk= 39923
IHN0YWluZWQ= 39924
IHBsYXVzaWJsZQ== 39925
IHBlZWxpbmc= 39926
IGNyw610 39927
IGdyb3Rl 39928
7Law 39929
pbTqsow= 39930
YWx0ZXQ= 39931
UGhvbmU= 39932
Rmls 39933
U1FM 39934
IGdlZmFsbGVu 39935
5Y+U 39936
IHNhw7pkZQ== 39937
IFRhbWls 39938
Y291cw== 39939
INCz0LvQsNCy0L3QvtC1 39940
IGF0cmF2w6lz 39941
dXNzaWE= 39942
IHp3ZWl0ZW4= 39943
IEVsdmlz 39944
IG1vdmVy 39945
IGxpbWl0ZQ== 39946
6L+9 39947
YXJleg== 39948
pbTqs6A= 39949
IEtyYW5rZW4= 39950
w7xyZQ== 39951
IOyViuyVhOyalA== 39952
IHRow6BuaA== 39953
IHByb2ZvdW5kbHk= 39954
IGJlZHJvb21z 39955
IHRvb3RocGFzdGU= 39956
IEFjY2VwdA== 39957
w6l0aWNv 39958
IGvDvMOn 39959
IEFyeQ== 39960
YWRpbg== 39961
IGdyYW51bGFy 39962
ZWN0ZWQ= 39963
IG1lbmphZGk= 39964
IGNvbXBldGVuY2U= 39965
ZG9j 39966
IHNwYXJrbGluZw== 39967
IOyii+ydhA== 39968
IGNvbnN0cnVjdGluZw== 39969
IGFtdXNlbWVudA== 39970
IEluc3VyYW5jZQ== 39971
IEZldWVy 39972
IHJlbm92YXRpb24= 39973
c3VjaA== 39974
cGxhdA== 39975
IHByb3N0aA== 39976
IGJleQ== 39977
IENvbXBsZXRlbHk= 39978
IHpvZA== 39979
YWxu 39980
VmljdA== 39981
IGNvbmZpcm1z 39982
w6R0eg== 39983
4pY= 39984
aGFtbWVy 39985
INC30L3QsNC10YI= 39986
IGFkbWlyZWQ= 39987
oOulvA== 39988
IEZydWl0 39989
ZXJ0ZW4= 39990
IG5pZWNl 39991
IFRpbnk= 39992
IHBsdW1iaW5n 39993
ZXJtYQ== 39994
INC70LXQs9C60L4= 39995
IHdpbmRzaGllbGQ= 39996
INGB0LzQtdGA 39997
IGJ6dw== 39998
IGFib2xpdGlvbg== 39999
IFNhZGhndXJ1 40000
IHByZWFjaGVk 40001
IENyZWF0aW5n 40002
54mb 40003
cGVyZWQ= 40004
IHZvbG9udA== 40005
IHF1aW50 40006
IHByaW50ZXJz 40007
IG5lZ3Jv 40008
IGdyb3NzZQ== 40009
IFRoeQ== 40010
IEZlbGxvd3M= 40011
5o6l5LiL5L6G 40012
IHN0YW5pZQ== 40013
IG5ld2NvbQ== 40014
IEh1ZQ== 40015
IEZyZXVuZGU= 40016
IENvbnN0cnVjdGlvbg== 40017
IGFkdmVyc2l0eQ== 40018
IG5lZ2F0aXZlcw== 40019
IGhhemFyZG91cw== 40020
IGNvbXBlbGxlZA== 40021
IHdvaw== 40022
IE95 40023
0L/QsA== 40024
qqjr 40025
IHJlbmRleg== 40026
IG92ZXJj 40027
IHdlYXZpbmc= 40028
INC40LTQtdGC 40029
IHByb3NlY3V0b3Jz 40030
IGF1ZGlvYm9vaw== 40031
IGFuY2VzdG9y 40032
IHVuZGVyZ29pbmc= 40033
IHBvdW5kaW5n 40034
44GC44KK44GM44Go44GG44GU44GW44GE44G+44GZ 40035
IO2SgA== 40036
IOy2pA== 40037
IHR1bGVl 40038
IOyXtOw= 40039
IHpvYWxz 40040
IG5laW4= 40041
6a2a 40042
IG9rZQ== 40043
IEpveWNl 40044
IG51ZA== 40045
IGRpbGlnZW5jZQ== 40046
IExhYnM= 40047
IHZlbnRz 40048
IGFuY2VzdHJhbA== 40049
4Lir4Lih 40050
INC80YPQttGH 40051
IG5vbcOpcw== 40052
6KGo56S6 40053
d2FsaQ== 40054
cWluZw== 40055
IE11bHRpcGxl 40056
IENvbnN1bHQ= 40057
IGlzdGVkaQ== 40058
IERveQ== 40059
YWthaA== 40060
IGRpc2NpcGxpbmVk 40061
IGFsdGVybmF0aW5n 40062
55I= 40063
IHZlcm1l 40064
INC+0Yk= 40065
IHRvdGE= 40066
IFByYWc= 40067
IHN3b3Ju 40068
IGJlYmVy 40069
IEF1ZmdhYmU= 40070
7Jq06w== 40071
6L6m5rOV 40072
IHl1cA== 40073
IHJlY2xhaW0= 40074
b251dA== 40075
IGF1Y3VuZQ== 40076
IGFtcGg= 40077
IMWbd2ll 40078
IGFh 40079
aXNjb3Zlcg== 40080
IEFyZw== 40081
Y2llxbw= 40082
IGRlc3Nhcw== 40083
IFfDpGg= 40084
4bu5 40085
INC00LDQstC90L4= 40086
IHNpbGVudGx5 40087
YXJj 40088
IO2bhOuztA== 40089
IHR3ZWV0aW5n 40090
IE9uZA== 40091
6aGe 40092
pqzrqbQ= 40093
IGJvd2Vs 40094
7IWo7Ja07JqU 40095
6IGK 40096
T1NF 40097
IHByb3Bpbw== 40098
IEt1bnN0 40099
a3VuZw== 40100
IGRvbm7DqWVz 40101
IEhvcml6b24= 40102
IEZyb2c= 40103
5YCL5Lq6 40104
IGFyaXN0 40105
w6Js 40106
INC60L7Qtg== 40107
IHNlZ3VuZG9z 40108
IFNob3J0bHk= 40109
IENyb3dk 40110
aXJhbg== 40111
IHfFgmHFm2Np 40112
IExhYw== 40113
aWRlbnRl 40114
IOqwgOyekA== 40115
IGxlbg== 40116
IFNVUw== 40117
IE1vdG9ycw== 40118
IFRyZW50 40119
b21pZQ== 40120
IHRyYW5zbWl0dGVy 40121
IEFzc2Fk 40122
IHBzeWNoaWF0cmlj 40123
INC20LjRgtGM 40124
IG91dGxpbmVz 40125
IGVmZmVjdGl2ZW1lbnQ= 40126
IFJlbGlnaW9u 40127
cHJlaA== 40128
INC00L7Qu9C20L3QsA== 40129
IM2hwrA= 40130
IENvbnNlcnZhdGlvbg== 40131
IOG7 40132
INC30LDQuQ== 40133
IHJlc2lkZQ== 40134
IGNvbXBsZXRv 40135
S0VO 40136
IOuCmOyYpOuKlA== 40137
IHN1YnVyYmFu 40138
IHLDqXBvbmRyZQ== 40139
INGA0LDQt9C70LjRhw== 40140
IGdhbGxlcmllcw== 40141
IHJhcHQ= 40142
5oSf6Kyd 40143
KS4uLg== 40144
IGNydWVsdHk= 40145
IFZNd2FyZQ== 40146
7Yis 40147
IGhhecSxcg== 40148
IGdyb3VwaW5n 40149
IFJpZGVy 40150
IHN5bGxhYmxl 40151
IGJlaXNwaWVsc3dlaXNl 40152
IHNhZmVndWFyZA== 40153
IHBlbMOtY3VsYQ== 40154
YXJ0aQ== 40155
INCh0L4= 40156
IGNoZWdh 40157
INC60L7QvNGD 40158
IHNlaXNt 40159
IGhhcm1sZXNz 40160
IFdhcnJpb3Jz 40161
44GE44Gk 40162
INC/0YE= 40163
IHNoYW1lbGVzcw== 40164
IEJhdW0= 40165
aW5zdGFsbA== 40166
IHRvb2xraXQ= 40167
IHBpcGVsaW5lcw== 40168
IHB1c3N5 40169
IGNvbmNlYWw= 40170
IHByb3Rlc3Rpbmc= 40171
b2Nob25k 40172
IGR1YQ== 40173
IFBvc2U= 40174
IGhlbGl1bQ== 40175
IFVY 40176
aWtsZQ== 40177
IFN1ZmY= 40178
IOyEuOqzhA== 40179
aW5nZXJz 40180
INGB0LvRg9GH0LDQuQ== 40181
IGRlc2NlbmRpbmc= 40182
IOaykuaciQ== 40183
IG1vbnRhZ2U= 40184
SGlnaA== 40185
IOydtOyW 40186
IElkaQ== 40187
INeR16E= 40188
IGV4cHJlc3NpdmU= 40189
56eL 40190
INC/0L7Qu9C10Lc= 40191
IHBvbmU= 40192
IGFkb2xlc2NlbnQ= 40193
0LDQvdC90YvQtQ== 40194
IGFzc2Fzc2luYXRpb24= 40195
d2Vpc2Vu 40196
ZW1hdGljYWxseQ== 40197
YXV0aA== 40198
IHVyZw== 40199
IGdhbmhhcg== 40200
IGZ1bmRv 40201
IFJob2Rl 40202
INC40YHRgtC+0YDQuNC4 40203
IGNvbXBhcnRpbA== 40204
5pWi 40205
IGRpbWluaXNoZWQ= 40206
IGFwcHJlbnRpY2U= 40207
INCR0YPQtA== 40208
IHBob3RvbnM= 40209
IGPDs2Q= 40210
5bmV 40211
5qyK 40212
b25haw== 40213
IGFkZWxhbnRl 40214
IGNodQ== 40215
b3BpYw== 40216
IGFpeMOt 40217
ZWRkYXI= 40218
IENvbmdyYXRz 40219
bW9y 40220
5aW95ZCn 40221
IHJlc2VydmF0aW9ucw== 40222
IFRvYnk= 40223
IEtlcm4= 40224
IHJhemVt 40225
IGZvcmdlZA== 40226
IGhvcnJpZnlpbmc= 40227
2YrYuQ== 40228
IEpvaW5pbmc= 40229
44Op44Kk 40230
IEF1dGg= 40231
ZGFo 40232
IGNvbnNpZw== 40233
IGludGltaWRhdGVk 40234
IHBlcmlwaGVyYWw= 40235
IG1lbm8= 40236
IGRldGVjdGluZw== 40237
IHRlb3I= 40238
IHRhZ2dlZA== 40239
IG5vc3RhbGdpYw== 40240
IOuvuOyViA== 40241
5YC8 40242
IHZlcmRp 40243
IGxhYmVsaW5n 40244
0L/QvtC0 40245
YXN0ZXM= 40246
IHZpc3Q= 40247
IGN5dA== 40248
IGZsaXBz 40249
0YDQuNC3 40250
YmFsYW5jZWQ= 40251
44Gq44GP 40252
INC+0YjQuNCx 40253
IGRlc3Rpbg== 40254
bGFzc2U= 40255
ZXJlaQ== 40256
IGthbG8= 40257
IGFycXU= 40258
IHBsYW5v 40259
IG9yZGluYW5jZQ== 40260
IGNvbXBpbGF0aW9u 40261
IFZvY8Oqcw== 40262
IEVjbw== 40263
IOy2lOyynA== 40264
IGVuY2ltYQ== 40265
IEdhcnJldHQ= 40266
IENvcmQ= 40267
w7Zsa2Vy 40268
IEFycm93 40269
IHByb3RvbnM= 40270
LOKAiw== 40271
IOyymOs= 40272
IHNjYW5k 40273
IGJlaWdl 40274
Y29uZw== 40275
IGJpa2luZw== 40276
IFRM 40277
0YPQvdC0 40278
IOyGlOyngQ== 40279
IFZpbGxh 40280
IEpBQ0s= 40281
5Lul5Y+K 40282
IMO2xJ9yZW4= 40283
IHRlbWFz 40284
IEt5dW5n 40285
SmVubg== 40286
IGN1ZA== 40287
IGltcG9zaW5n 40288
IGNvbW1hbmRtZW50cw== 40289
IE1lYW5z 40290
IETDpHI= 40291
IHJlY29tZW5k 40292
IGRpc3Bvc2l0aW9u 40293
2KfZhw== 40294
IHRodQ== 40295
IHJlZHVjdGlvbnM= 40296
IGRpdQ== 40297
INeV15DX 40298
INC40YHRgdC70LXQtA== 40299
dGhyZW4= 40300
IGxhZG9z 40301
IFJC 40302
aXhlZA== 40303
IOyP 40304
RnI= 40305
c3RpbGw= 40306
IG9sbWFz 40307
Q0hVQ0s= 40308
IO2GoA== 40309
IEluZGVwZW5kZW50 40310
0JLQng== 40311
IHBpdHM= 40312
IHVuZGVydGFrZW4= 40313
IGbDuHI= 40314
IE5hdw== 40315
IOyekeyXhQ== 40316
IHNoZXBoZXJk 40317
IGxhbmd1ZQ== 40318
IEphYg== 40319
IERydW0= 40320
IEVsZWt0 40321
5ous 40322
44GY44KD44Gq44GE 40323
4buRdA== 40324
IOydtOyqvQ== 40325
IGJlZ2lubmVu 40326
IEZ1cnk= 40327
4buDdQ== 40328
c2VjdGlvbnM= 40329
IHNwcmF5ZWQ= 40330
IG3DoXI= 40331
IFZvbHQ= 40332
IFNlb25n 40333
0LjRgtC10Ls= 40334
ZHVjdGlvbg== 40335
YXNhbg== 40336
IGp1ZGdtZW50cw== 40337
aW1hYW4= 40338
nteq 40339
IHNpZW50bw== 40340
IEFDVA== 40341
IEJI 40342
ZGV2 40343
IOyii+yVhO2VmA== 40344
IGpvcm4= 40345
SVNUSU4= 40346
IHJvYXI= 40347
IGltbWVyc2lvbg== 40348
YWZmbGVz 40349
IHRyYWluZWU= 40350
IEJpbGxib2FyZA== 40351
cmVzc2Vz 40352
IFdhcm0= 40353
IFJvYmVydG8= 40354
IHV0aWxpeno= 40355
IElnb3I= 40356
IHJhc2g= 40357
IGFuYWx5dGlj 40358
aXJhbQ== 40359
IHN5bW1ldHJpY2Fs 40360
IGxpZmVzcGFu 40361
IGVhdGVy 40362
IEJsb29tYmVyZw== 40363
YXRlcmlhbA== 40364
IOuvvw== 40365
IGlzdGVy 40366
IGludmFsdWFibGU= 40367
IGFzc2lzdGluZw== 40368
IHNoYWNr 40369
zrzOsc+EzrE= 40370
amlz 40371
ZW5peg== 40372
INC/0YDQtdC00LvQvtC2 40373
IGRlY2xhcmluZw== 40374
IFZpa2luZw== 40375
IEFzc2lt 40376
IGV4cGVuZGl0dXJl 40377
IHBvc2luZw== 40378
IE9udW4= 40379
IGluaWM= 40380
0LDRjtGC0Yw= 40381
cmV2 40382
IG1pZWRv 40383
IGZpbHRoeQ== 40384
IElC 40385
IERpc2NvdmVy 40386
aWNodGV0 40387
bWlsbGlvbg== 40388
toTrk6TsnbQ= 40389
IGFtYmlndQ== 40390
IEZseW5u 40391
YmFyZHppZWo= 40392
IGluY29tcA== 40393
0LDQstC90L4= 40394
emlh 40395
IGluZmx1ZW5jaW5n 40396
IHdvcmxkbHk= 40397
IFNhbGVzZm9yY2U= 40398
emV0 40399
IHBhcnRpY3VsaWVy 40400
IEtvY2g= 40401
IDE5NDM= 40402
IHRvbmVy 40403
INGN0LrRgdC/0LXRgA== 40404
IHN1c2NyaQ== 40405
IHRyaWdnZXJpbmc= 40406
SUNFUw== 40407
7Iqk6rCA 40408
zrTOsQ== 40409
0YDQsNCx0L7Rgg== 40410
IGFmdGVyd2FyZA== 40411
cGluZQ== 40412
IElM 40413
YXJldGg= 40414
INC/0LDQuw== 40415
IHNha2Vy 40416
IDE5NDc= 40417
QUY= 40418
dXlvcnN1bg== 40419
IOyKpOs= 40420
IHF1YW50aWZ5 40421
IG1lbnRvcnNoaXA= 40422
IGxsZWdh 40423
IFRhbWFyYQ== 40424
IG9wdGltaXppbmc= 40425
IGZyb250cw== 40426
b3N0ZXJz 40427
IGVzcXVlcg== 40428
IHN1Ym1pc3Npb25z 40429
IGFubmlo 40430
IHN1Y3Rpb24= 40431
bHVlbmNl 40432
Y2hpZWRlbg== 40433
SU5HUw== 40434
INeR15Q= 40435
INGB0YbQtdC9 40436
IHdpZWx1 40437
IG9iamV0bw== 40438
IGJvb2Jz 40439
IEdlc2Now6RmdA== 40440
IGVhcmJ1ZHM= 40441
INGA0LDQvdGM0YjQtQ== 40442
IHJvdXRpbmVseQ== 40443
IGNvbGxhZ2Vu 40444
0L7QtNGL 40445
IENpbm5hbW9u 40446
IGJhaXg= 40447
2K/ZhQ== 40448
ZnJhZ2U= 40449
INC60L3QvtC/ 40450
IGRlY2VwdGlvbg== 40451
IHVuZXhwZWN0ZWRseQ== 40452
IHNtZWxsZWQ= 40453
IGxvb3M= 40454
IGhpZ2hsaWdodGVy 40455
IOq4sOuzuA== 40456
IEdsYXNnb3c= 40457
b3dhbmE= 40458
bW4= 40459
IEplcmVtaWFo 40460
IERhdGFi 40461
aWV0ZQ== 40462
IGJhdw== 40463
IHByb3BpYQ== 40464
IHByb3ByaQ== 40465
T09PT09PT08= 40466
aW5rZXI= 40467
IHBlcnR1cmI= 40468
IEZha2U= 40469
7J207JY= 40470
aW1taW5n 40471
IHVuZG9jdW1lbnRlZA== 40472
IHRyYWJhamFuZG8= 40473
IHJvYW0= 40474
INC00L7Qu9C20L3Qvg== 40475
IGFyYmU= 40476
IGFuaQ== 40477
YXRhbA== 40478
IGFyYWRh 40479
IEFuZGE= 40480
IOybgA== 40481
IEJyYW5jaA== 40482
b2lyZXM= 40483
IG91dHNpZGVy 40484
ZG9sbGFy 40485
5b2T54S2 40486
aXNzZXM= 40487
YmVhbnM= 40488
IEdpZw== 40489
552h 40490
cmFkb3M= 40491
IFN1dA== 40492
IExhbmNl 40493
ZWRzacSZYmlvcg== 40494
IGNvbGE= 40495
b25lbnRz 40496
IHJlY29uc2lkZXI= 40497
44K544OI 40498
IG1vbmRv 40499
44Oz44ON44Or 40500
IHVuc3VjY2Vzcw== 40501
IEvDpA== 40502
6L65 40503
IHJlZ2Vs 40504
IGJpc29n 40505
ZXR1cw== 40506
IHVucmF2ZWw= 40507
IHN3ZWV0aWU= 40508
IHJlcHLDqXNlbnQ= 40509
b3VyaW5n 40510
IGdyb3VuZHdhdGVy 40511
IEJldw== 40512
IHNjcmF0Y2hlZA== 40513
IGNhc3NldHRl 40514
IGNpZGVy 40515
cGlz 40516
INGB0LDQvNCw 40517
IGdsb2JhbGl6YXRpb24= 40518
IGRlZ3JhZGF0aW9u 40519
IGRlZ2VuZXI= 40520
IFJvc2ll 40521
aWNrdA== 40522
IG92ZXJ3ZWlnaHQ= 40523
IE1FTQ== 40524
IGd1YXJkaWFucw== 40525
IGNvbnNlYw== 40526
SG1t 40527
5oiR5Zyo 40528
INC/0L7RgtGA0LXQsQ== 40529
IG1ldmE= 40530
IGdyYWZmaXRp 40531
IGZsaXJ0 40532
IEJQ 40533
IGp1c3Rv 40534
IFRob3VzYW5kcw== 40535
55Sc 40536
n6zsmrQ= 40537
Lio= 40538
IFJBVw== 40539
IGZsdW9y 40540
aXlp 40541
YW50YWw= 40542
amVk 40543
IFNoZW5n 40544
IEVsaXNl 40545
IENoYXJnZQ== 40546
7J207Yq4 40547
IGNvbmVz 40548
bmllcw== 40549
Z2lh 40550
INC90LDRh9Cw0LvQsA== 40551
IERoYXJtYQ== 40552
IOuLpOyWkQ== 40553
IGZhdm9ycw== 40554
IFRydW5n 40555
aGV0dG8= 40556
IHBvenc= 40557
IGxvbmdv 40558
IGtlbHU= 40559
IGRpZ2VzdGlvbg== 40560
IEVpZw== 40561
IFRIRVJF 40562
IHRpZXJz 40563
IHN1bms= 40564
IG15c3RpY2Fs 40565
enVi 40566
IMOJdA== 40567
IGFudGljaXBhdGluZw== 40568
IFZpbmU= 40569
WVk= 40570
IGNvbmNlbnRyYXRpbmc= 40571
IEFncmVlbWVudA== 40572
INC+0LrQvtC70L4= 40573
IGxpZHQ= 40574
IFlhbw== 40575
INGB0LvQuNGI0LrQvtC8 40576
csOt 40577
SVNUSU5DVA== 40578
IE9GRklD 40579
IHNvYWtpbmc= 40580
IHNpaWhlbg== 40581
IHJlZmVyZW5jaW5n 40582
IFRhbXBh 40583
YW5leQ== 40584
IHJlc3B1ZXN0YQ== 40585
IENvYWxpdGlvbg== 40586
INGB0L7Qs9C70LDRgQ== 40587
YW5raW5k 40588
IOub 40589
IFl1bW15 40590
67Cw 40591
IG9uYw== 40592
dWnDp8Ojbw== 40593
IHRoZW8= 40594
IG11cmFs 40595
IFRlYWNoZXJz 40596
IHdhaXRz 40597
IHJlbnRpbmc= 40598
IEhhcm1vbg== 40599
IGXFnw== 40600
IE11bmljaA== 40601
7Zmc 40602
7Ja8 40603
Y2FyZHM= 40604
IHJvdWdl 40605
IG7Dqm4= 40606
Y2x1Yg== 40607
IHVuc2Vlbg== 40608
IGRlcHJlY2k= 40609
IGNvbXB1dGVk 40610
IHdpcGluZw== 40611
IEVsbGk= 40612
aWRlbnRpZmllZA== 40613
IGNsdXR0ZXI= 40614
cm9sZXVt 40615
IHRlbGVm 40616
IGxldmVsaW5n 40617
IFdvb2R5 40618
IEd1cw== 40619
IEJlbm5ldHQ= 40620
IHNpdGlv 40621
acWC 40622
IHBvc3Nlc3Npb25z 40623
IE5hdGFzaGE= 40624
b2xkb3du 40625
INGB0L7QvtCx0Yk= 40626
IExpYw== 40627
IOunjOuToA== 40628
IGxvcnNxdWU= 40629
d2Vo 40630
INC80LDQvA== 40631
bGl0ZXI= 40632
YWRvbW8= 40633
IGZpbmk= 40634
z47Pgg== 40635
INGD0LHQuNC5 40636
IGluZGlzcA== 40637
IHRlbGV2aXM= 40638
IHDDoQ== 40639
IENyZW8= 40640
w61sbA== 40641
IGd1cg== 40642
IE1BTA== 40643
INGA0LDQt9C90YvRhQ== 40644
IHppZWhlbg== 40645
IGZhc2hpb25lZA== 40646
IGRlYmF0aW5n 40647
IFNvdXA= 40648
IFByb3ZpbmNl 40649
6re466CH 40650
IGltcHJvcGVy 40651
IGltYWdlbg== 40652
INGB0LTQtdC70LDQuw== 40653
IGxvZ29z 40654
IGV2ZW50bw== 40655
6KeG 40656
4bqjbw== 40657
bGFyZGE= 40658
INC90LDQt9GL0LLQsNC10YLRgdGP 40659
IHZlcmY= 40660
IHNjcmVlbnNob3Rz 40661
15XXk9ei 40662
IEF1cm9yYQ== 40663
IEJhbGk= 40664
dGVyZWQ= 40665
IGNvbnRhZ2lvdXM= 40666
IGNvbXBhcnRpcg== 40667
dmVuaWRvcw== 40668
cmlrZQ== 40669
INCy0YvQs9C70Y/QtNC40YI= 40670
IGZyZWVkb21z 40671
bmljYXM= 40672
oKTshJw= 40673
IHJlZHV6 40674
IEVjdQ== 40675
IGFib25u 40676
IFNFw5E= 40677
IEJpdGNo 40678
IHByb2pldG8= 40679
0LjRh9C90L4= 40680
ZXR0cmU= 40681
QU5OQQ== 40682
dGhhbms= 40683
IEFP 40684
5omA5Lul5ZGi 40685
YXJuaXNo 40686
aWXDn2Vu 40687
IHJpcHBsZQ== 40688
IHBhbnRyeQ== 40689
IEdI 40690
zrPOsQ== 40691
IOydtOuyiOyXkA== 40692
IHZhbGlkYXRlZA== 40693
IGJydXNoZWQ= 40694
IEVtaW4= 40695
IERhcnRo 40696
ZXNpbg== 40697
LC4= 40698
IHZhbGxl 40699
IGplcnNleQ== 40700
dWxhbg== 40701
UmVhZA== 40702
IFJhbmdlcnM= 40703
IHNvb3RoaW5n 40704
IGNvbXBsZW1lbnRhcnk= 40705
IFZlcmtlaHI= 40706
YWNha3Q= 40707
IGJhdGh0 40708
IE5E 40709
U29u 40710
IO2ZlOyepQ== 40711
IEF2aQ== 40712
IFNBTA== 40713
YWlzc2U= 40714
IHNlbWFpbmVz 40715
IFN1cnY= 40716
d2llcg== 40717
INCy0LjQtNC10Ls= 40718
IHNpZXRl 40719
lOuPhA== 40720
IFJhbXNheQ== 40721
IFF1ZWVuc2Jvcm91Z2g= 40722
IE1lbmdl 40723
IEZvb2Rz 40724
IHRoZW9sb2dpY2Fs 40725
IFsj 40726
INCy0L7QvdC4 40727
IGltbWlu 40728
aW9zaXR5 40729
IEFiZ2VvcmQ= 40730
IEFjaG8= 40731
IMOU 40732
IHN0YWlucw== 40733
IHJlYWxpc3RpY2FsbHk= 40734
IGZhc2hpb25hYmxl 40735
IENFT3M= 40736
IFNraWxs 40737
INCy0LbQtQ== 40738
IGRldmVy 40739
IFBsdWc= 40740
5qo= 40741
UG9k 40742
IGxvYWY= 40743
IGdlYnJhY2h0 40744
IGFic29yYnM= 40745
IEdyYW5ueQ== 40746
IG1hbHdhcmU= 40747
YWfEmQ== 40748
IGNpdmlsaXphdGlvbnM= 40749
IM+B 40750
IGjDpGx0 40751
0KHQog== 40752
Z3JlYXQ= 40753
IGxheWVyaW5n 40754
c2luZ3M= 40755
INCy0ZbQvQ== 40756
IHJlY29nbml6YWJsZQ== 40757
IHdvag== 40758
IHdldGVu 40759
56ys5LiA5YCL 40760
zrPOvw== 40761
U3R1ZGVudA== 40762
IGTDqWZpbg== 40763
cGxlYXNl 40764
ZW5jaA== 40765
IGF0dGlj 40766
IE90dGF3YQ== 40767
IG9wdGVk 40768
IGNhcHRpdg== 40769
IG3Fgg== 40770
IFlB 40771
IFdhbmQ= 40772
IGJvdW50eQ== 40773
IDI3MA== 40774
IHNwZWN1bGF0ZQ== 40775
IGVuaGFuY2VtZW50 40776
IGNvbW1vZGl0aWVz 40777
IE1pbHRvbg== 40778
ZWo= 40779
YWxvbQ== 40780
RGFz 40781
IGNvb2xkb3du 40782
16jXkNec 40783
INeQ16Q= 40784
IHdjemXFm25pZWo= 40785
IGVsb25n 40786
IGRpb2Rl 40787
aW5hw6fDo28= 40788
IElyaXM= 40789
IEli 40790
IHN1bW1vbmVk 40791
IHJlc3Bl 40792
IFJhY2g= 40793
5rOo5oSP 40794
IMK7Og== 40795
6YaS 40796
IHZ1cg== 40797
IG1vdmltZW50bw== 40798
IGZsdWVudA== 40799
IEV2b2x1dGlvbg== 40800
IEJ1dHQ= 40801
aWZpY2FjacOzbg== 40802
lJTslrQ= 40803
INGN0L3QtdGA0LM= 40804
IG1hbmlwdWxhdGluZw== 40805
IHBvc2l0aXY= 40806
0LzQvtGB 40807
IHdpeg== 40808
IGludG94 40809
zq3PgQ== 40810
0LXQvNGB0Y8= 40811
aXZlc3Nl 40812
aW1pemk= 40813
IOyauA== 40814
IGtub2Nrcw== 40815
IGNvbmdlc3Rpb24= 40816
IElkZWFsbHk= 40817
IEhvbGRpbmc= 40818
IHBvYnJl 40819
IEpVTA== 40820
IOu2hOuTpOydgA== 40821
IM6xzro= 40822
IEZlcmd1c29u 40823
IExhYm9yYXRvcnk= 40824
cmljaHRlbg== 40825
cm9waHk= 40826
cHJvZHVjdGlvbg== 40827
YXNzdW5n 40828
SVRB 40829
IHNpw6hjbGU= 40830
16jXqg== 40831
Y2lzaW9u 40832
INek15Q= 40833
IElyZW5l 40834
YW5jYQ== 40835
IOyCrOqzoA== 40836
IHBpbnBvaW50 40837
IGRlc2lnbmF0aW9u 40838
xZ9hbQ== 40839
bMSxxZ8= 40840
YWF0 40841
IG7DpWdyYQ== 40842
IG15dGhpY2Fs 40843
IERlY2xhcmF0aW9u 40844
IOyeoeyVhA== 40845
IGJ5dGU= 40846
LuKZqg== 40847
RGVs 40848
IO2NvA== 40849
IG51dHJpdGlvdXM= 40850
INGA0YPQsdC70LXQuQ== 40851
5YKz 40852
U0FZ 40853
TWFzdGVy 40854
INGE0L7RgtC+0LPRgNCw0YQ= 40855
IOuSpOyXkA== 40856
IG5laA== 40857
IGRva3VtZW50 40858
56qB 40859
IGN6YXN1 40860
IGNvbnRpbnVh 40861
IFNpbGVudA== 40862
IHRlbnNvcg== 40863
IHRhbnRh 40864
IGlyZ2VuZHdv 40865
IExFVA== 40866
IFNoYWt0 40867
bGFtYQ== 40868
Y2hsYWc= 40869
IGRpbmdlbg== 40870
0YHRgtGA0LA= 40871
IGVocmxpY2g= 40872
IE1hY2h0 40873
cmVscw== 40874
w6BjaWVz 40875
dmlkZW8= 40876
IG5hdHVyYWxl 40877
IFNURVZF 40878
dW1t 40879
QkFDSw== 40880
IDcyMA== 40881
44Gn44GX44Gf 40882
IG1vbWVuY2ll 40883
IFN3YW4= 40884
IHRlY2huaWNpYW5z 40885
IGdlZWhy 40886
IE1lbmQ= 40887
UmVn 40888
IHNjYWZm 40889
IGFpZGU= 40890
IOuztOuKlA== 40891
IHByZXNzZXM= 40892
bGVyZGU= 40893
XCc= 40894
IHVsdHJhc291bmQ= 40895
IGRpc2NsYWltZXI= 40896
IE1pdHM= 40897
IEhvbGlkYXk= 40898
IGV4dGVybmFsbHk= 40899
IEZhdGU= 40900
SU5P 40901
IENhdHM= 40902
67CV 40903
dW1v 40904
Y29udHJvbA== 40905
IHRoZUNVQkU= 40906
dGlj 40907
aWVydW5ncw== 40908
INC30L3QsNC60L7QvA== 40909
IGZyZWVzdHlsZQ== 40910
TUFOREFSSU4= 40911
IGlzZQ== 40912
YXVydXM= 40913
6Kix 40914
IFN0cmF0ZWd5 40915
IEJlYW0= 40916
csOkZ2U= 40917
IGV4cGxvaXRlZA== 40918
44GI44Gj 40919
aWRpcw== 40920
IGNoaW1l 40921
IFBlbmluc3VsYQ== 40922
IG1lcml0cw== 40923
IGFsdHJv 40924
IFRPUA== 40925
IFNlbnM= 40926
IEthbnQ= 40927
b3Jhcw== 40928
IHJveWFsdHk= 40929
IElERQ== 40930
5aSJ 40931
cmFjeQ== 40932
IFRIT00= 40933
b21vcw== 40934
IGzDpG5nZXI= 40935
IG51bWJlcmVk 40936
VW0= 40937
IE5peWU= 40938
zrjOtw== 40939
enlrYQ== 40940
bGltZQ== 40941
IFBlcnNvbmVu 40942
IHZhbGlkaXR5 40943
IGNvbnRyYXQ= 40944
IENvbWlj 40945
w6dvbnM= 40946
IEhlaWRp 40947
IHpn 40948
IHJlbmFtZWQ= 40949
IGN1bWlu 40950
IEpG 40951
aW5lbA== 40952
IGVuZm9yY2Vk 40953
IGNoYW1h 40954
0LvQuNGH0L3Qvg== 40955
4bq7 40956
INC00LXQvdC10LM= 40957
IHByb2Z1bmQ= 40958
IHBlbHZpYw== 40959
IHBhbGF2cmE= 40960
IGV4dHJhcw== 40961
IGFua2xlcw== 40962
7JeQ7ISc64+E 40963
IFRG 40964
IGluc2FuZWx5 40965
INC80Y/RgQ== 40966
IHLDqXBvbnNl 40967
IGfDtnN0ZXI= 40968
IEJCUQ== 40969
INGD0YfQsNGB0YI= 40970
IHNoYWtlbg== 40971
44Kr44Oz44K/ 40972
IGFsbW9uZHM= 40973
ZGlzaA== 40974
IFBH 40975
IEJsaXp6YXJk 40976
0YzQvtCz0L4= 40977
IOOF 40978
IGtuYXBw 40979
VG9v 40980
IHVuZGU= 40981
IG1vdW50cw== 40982
0L7QvNC40L3QsA== 40983
IG5vcnRoZWFzdA== 40984
IGNlbnNvcnNoaXA= 40985
0Y/RgtGM0YHRjw== 40986
bHI= 40987
IGxhd21ha2Vycw== 40988
IHPDpWRhbg== 40989
IGluc2lkZXI= 40990
IGNsZWFudXA= 40991
IE5hZGE= 40992
w7Nj 40993
IGhhcnZlc3RlZA== 40994
IERlc3B1w6lz 40995
7ZqN 40996
IHJlZHVuZGFudA== 40997
RU5B 40998
IGRlbGVnYXRl 40999
IGJ1cmc= 41000
IEFsaXNvbg== 41001
5paw6IGe 41002
IGNlbGVzdGlhbA== 41003
IHNpbm5lcnM= 41004
IG1hcnR5cg== 41005
IFBlcm0= 41006
IHNwZWNpbWVucw== 41007
IG1pdG9jaG9uZA== 41008
IG1hcmF2aWw= 41009
IGNhdmFscnk= 41010
IGFycmF5cw== 41011
IGFubmV4 41012
IGxhYm9yYXRvcmllcw== 41013
IEJ5eg== 41014
IGF0YWM= 41015
INGB0LvQvtC20L3Qvg== 41016
IHRvcGw= 41017
IGdlcmk= 41018
IENvbWJhdA== 41019
0YHRj9GC 41020
ZWtlbg== 41021
INCS0LvQsNC0 41022
IGFqdXN0 41023
IG1hcnF1ZQ== 41024
IGxvb2tvdXQ= 41025
IExvbA== 41026
IHJvb2Z0b3A= 41027
IE9yaW9u 41028
INCx0L7QuQ== 41029
IGhlYXJ0YnJlYWtpbmc= 41030
IGRldHRv 41031
emg= 41032
w6R0dGVy 41033
Y2VyYQ== 41034
IGhlYXRz 41035
IGFudGlxdQ== 41036
IHVuZmluaXNoZWQ= 41037
IEthenU= 41038
xLFsxLE= 41039
IHNsaWdodGVzdA== 41040
bGVv 41041
IHbDpXJh 41042
IHZlcnNjaGllZGVuZW4= 41043
IGxvdGlvbg== 41044
5L2g5bCx 41045
5oy6 41046
0YjQtdCz0L4= 41047
Y3Rpb25hbA== 41048
IOydtOyg 41049
ZHJhZ29u 41050
IHJlc29uYXRlcw== 41051
IGlubQ== 41052
YXZpYw== 41053
IGZ1bGZpbA== 41054
IOq4sOuMgA== 41055
IGp1c3RhbWVudGU= 41056
INC00L7RgdGC0YPQvw== 41057
IOq3uOqxtA== 41058
IHJlY29uY2lsZQ== 41059
IFNjaMO2bg== 41060
IEF2b2lk 41061
6rmA 41062
J0Q= 41063
IGNvbmZpbmVtZW50 41064
IO2R 41065
IG1vdGl2YXRpbmc= 41066
IEJyaXR0YW55 41067
IOOBmQ== 41068
IHNjcmVhbWVk 41069
b2JqZWN0 41070
IGRlY3JlZQ== 41071
IHRyYXZhaWxsZQ== 41072
aXNzaWJsZQ== 41073
IGJ1c3RlZA== 41074
cHJvY2Vzcw== 41075
IG1hc3NhY3Jl 41076
IG5naMSp 41077
aWx5bg== 41078
INCy0YDQvtC00LU= 41079
IHBvZXRpYw== 41080
IG5o4bqldA== 41081
IGlyb25pY2FsbHk= 41082
dXN1 41083
bmlv 41084
IHN0YWdpbmc= 41085
b21lZGljYWw= 41086
bGVhc2Vk 41087
IOyDiOuhnOyatA== 41088
IE5a 41089
YWN0aW5n 41090
IEJhdHRsZWZpZWxk 41091
cGxheWZ1bA== 41092
Vmk= 41093
IHNlw7FvcmE= 41094
IHByb21wdHM= 41095
bGljaGtlaXQ= 41096
IMOnxLFrYXI= 41097
amlhbmc= 41098
IHBpY2t5 41099
IENhdmU= 41100
IG1pcmFjdWxvdXM= 41101
IEh1Z2hlcw== 41102
MjAxNg== 41103
IHh1 41104
IERvcm90aHk= 41105
IHZpcnR1ZXM= 41106
IHJldHJhY3Q= 41107
IHR5cg== 41108
IGNoYXJpc21hdGlj 41109
IGJvbGE= 41110
6bw= 41111
IOunkOyUgOs= 41112
IHBhcmVudGFs 41113
IG1pbGxpb25haXJl 41114
YXJpYXQ= 41115
5pS/5bqc 41116
IGludm9rZQ== 41117
xbxlbmll 41118
IGV4dHJlbWVz 41119
IEFrdQ== 41120
aXZpZGFkZQ== 41121
IO+3ug== 41122
IOyLnOyyrQ== 41123
IEdhcmxpYw== 41124
UklB 41125
INC00L7RgQ== 41126
IFBvbnQ= 41127
IG1pbGo= 41128
ZWxsaQ== 41129
IHJhY2tldA== 41130
IGNvbXBldGl0 41131
IFdoaXM= 41132
IHJlYWx0 41133
aWdubWVudA== 41134
ZXN0cmU= 41135
IHBlcm5haA== 41136
IE9wZW5pbmc= 41137
IEZT 41138
IERlbW9rcmF0ZW4= 41139
YWNlbWVudHM= 41140
IHdvcmxkdmlldw== 41141
IHBsYXlvZmZz 41142
IENBRA== 41143
IMOpdGFudA== 41144
IHllbWVr 41145
IHNlbnRpbWVudHM= 41146
b2RlbA== 41147
YnVzdGVy 41148
YcWf 41149
IEtZ 41150
Y3rEmQ== 41151
IHNjaMO2bmU= 41152
YXBl 41153
IFJhc3BiZXJyeQ== 41154
IGNyZWRpdGVk 41155
IEhpZGRlbg== 41156
IHNhdXNhZ2Vz 41157
cnVjZQ== 41158
IEJldg== 41159
aWxhbnRybw== 41160
IHBva2Vtb24= 41161
IOqwgOqyqQ== 41162
IHByb2NlZWRpbmc= 41163
IHZlaW8= 41164
IDE3NQ== 41165
6Lg= 41166
bWF4 41167
IGZyYXRlcg== 41168
7KCE7JeQ 41169
IGVnZW50 41170
IDI1MDA= 41171
dXNjaA== 41172
VHViZQ== 41173
IGFtcGxpZnk= 41174
IHByYXdk 41175
IG9kb3I= 41176
IFNjYW4= 41177
IHBsb3R0aW5n 41178
aXRobWV0aWM= 41179
IHJlc2lnbmVk 41180
IFNDT1RU 41181
IHN0ZXJlb3R5 41182
IGRvYWJsZQ== 41183
IENvbXBsZXg= 41184
2YHZig== 41185
dMSxbQ== 41186
0YDQuNCz 41187
bGFyZGFu 41188
ZXNv 41189
REVO 41190
IGhvb2RpZQ== 41191
IENBVA== 41192
2KfYtw== 41193
IGJvbmRlZA== 41194
IEJ1cm5z 41195
0L7Qv9Cw0YE= 41196
IHLEmQ== 41197
zrXOuc6x 41198
INC+0YLQtNC10LvRjA== 41199
IHRpbWVsZXNz 41200
IFZpag== 41201
IFBhbmFtYQ== 41202
IHJlb3JnYW4= 41203
IFTDpA== 41204
IFBsdXRv 41205
T3Jhbmdl 41206
INC/0L7QudC0 41207
IEJyaXN0b2w= 41208
dWNlZA== 41209
IOuQmOyWtA== 41210
IHVuYmVkaW5ndA== 41211
YWRsZQ== 41212
IHZvbHVudGVlcmVk 41213
IG1pZWxp 41214
IEVkaW5idXJnaA== 41215
aWthbA== 41216
IGFsdGVu 41217
IEFyc2Vu 41218
IG1vdXZlbWVudA== 41219
IGFudGlxdWU= 41220
IGJo 41221
IEhlcnM= 41222
IHNhdXRl 41223
IGFzcGlyZQ== 41224
IHNwaGVyZXM= 41225
IFdhbQ== 41226
4bqvbQ== 41227
IHdpcGVz 41228
IDI4MA== 41229
IFZlaA== 41230
IGNvbG9jYQ== 41231
0LDRhA== 41232
INCy0L7Qt9C80L7QttC90L7RgdGC0Yw= 41233
IHBoeXNpb2xvZ2ljYWw= 41234
aHdh 41235
ZXR1 41236
IHByb2xvbmdlZA== 41237
IGV4cGVyacOqbmNpYQ== 41238
INCy0LjQtNC90L4= 41239
IHF1YXJhbnQ= 41240
IHB1ZWRhbg== 41241
6JQ= 41242
dmluZQ== 41243
IFVTREE= 41244
cGhlbQ== 41245
IGZvcm1pZGFibGU= 41246
IGZsYXR0ZXI= 41247
7Ja07KeA 41248
IGLDqW4= 41249
4LmB4LiV 41250
IOusvOuhoA== 41251
IGZhY3Rpb25z 41252
IExlYXZpbmc= 41253
INeQ16rXlA== 41254
IEV4cGVydA== 41255
ZGlv 41256
IFZlcmQ= 41257
44G/44Gf44GE 41258
IHNpbnQ= 41259
2YbYrw== 41260
bnVtYmVy 41261
IG93ZWQ= 41262
IGluZHVjZQ== 41263
IEZyZWRkaWU= 41264
YWJv 41265
IEZpbGlwaW5v 41266
r7zr 41267
YmVsaWV2YWJseQ== 41268
YXRobG9u 41269
YW1hYW4= 41270
IGRldmVuaXI= 41271
IEdvcw== 41272
IEplbmtpbnM= 41273
YmFpdA== 41274
IGJpbnM= 41275
IE1JQ0g= 41276
dXlvcnVt 41277
aWdyYWRl 41278
aXNzbw== 41279
IOyXtA== 41280
IOyVhOu5oA== 41281
IGRpYXJyaGVh 41282
IHRvcm5hcg== 41283
YWRkaW4= 41284
IHVuZ2Vmw6Rocg== 41285
IHJlc3Ryb29t 41286
IHBzeWNoaWF0cmlzdA== 41287
IEtpY2tzdGFydGVy 41288
IGdlcmE= 41289
IGFscmVk 41290
IFdyYXA= 41291
z4zPgw== 41292
IHNpbm5lcg== 41293
Q0hFRVJJTkc= 41294
IGtpbG93 41295
IGRldGVybWluYW50 41296
IGRlbW9uaWM= 41297
aWRlbmNlcw== 41298
Y2hhcw== 41299
IERlZA== 41300
5byV 41301
IHN0dW1ibGU= 41302
IFVycw== 41303
IGRlY2VpdmVk 41304
IFRFUg== 41305
IEPDsw== 41306
ZWxsZWQ= 41307
IG5vdHdlbmQ= 41308
IOyngOq4iOq5jOyngA== 41309
IHBhcnRpZG8= 41310
IGRlc2NlbmRlZA== 41311
IHZhcmTEsXI= 41312
IGVuYWN0ZWQ= 41313
IGN6xJnFm2Np 41314
5bel5L2c 41315
IHRyYWluZWVz 41316
IGF1ZGlibGU= 41317
IG1hbGY= 41318
IHZlbw== 41319
w6xu 41320
IEdQQQ== 41321
IEFwcGU= 41322
5YK3 41323
IHJ1dA== 41324
IENhcmxh 41325
a2FjaA== 41326
IHNhdmlvcg== 41327
aXRjaGVk 41328
IGNsaW1heA== 41329
0LDRgtC10LvRjw== 41330
IE1jQ29ubmVsbA== 41331
0L7Qu9GP 41332
ZXJleWU= 41333
INGB0L7Qt9C9 41334
IGNhYm8= 41335
IFNuZQ== 41336
IEFmZm9yZGFibGU= 41337
IHNhcsOg 41338
IGxlZ2l0aW1hY3k= 41339
IHNjYXJjZQ== 41340
Li4uPC8= 41341
IDEwOA== 41342
IGFjdW0= 41343
IEZyYW5rbHk= 41344
IHJhZGlhdG9y 41345
IGdlbmVyYWxz 41346
IGRpdmlkZXM= 41347
IGNoZWVzZWNha2U= 41348
IHNvcmNlcg== 41349
IG1pc2NvbmNlcHRpb24= 41350
IGhhcmRzaGlwcw== 41351
IE9uZVBsdXM= 41352
w7x5b3JzdW4= 41353
IFNvdmlldHM= 41354
IEl0YWxpYQ== 41355
aWNraQ== 41356
IEFmdGVyd2FyZHM= 41357
IHJpZGljdWxvdXNseQ== 41358
IGdkemllxZs= 41359
IE5vdGVz 41360
2YPYp9mG 41361
IHJvbWFu 41362
IG9yZ2FuaXplcg== 41363
IGNvdXJ0eWFyZA== 41364
INGH0LXQu9C+0LLQtdGH 41365
IFdpdG5lc3M= 41366
INC/0Y/Rgg== 41367
IENoaWxs 41368
IFZhbHZl 41369
IM6szrvOuw== 41370
IEtQ 41371
Y2hsdXNz 41372
IGRlZmxlY3Q= 41373
IFRvbmk= 41374
IGNsYWly 41375
IHN0YWNraW5n 41376
5L2O 41377
cmFzemFt 41378
IFNvbnJh 41379
44Gj44Gh44KD 41380
IEF0YXJp 41381
IHBhc8Oz 41382
IGNoYXJtcw== 41383
YW5zdA== 41384
IHRlcmNl 41385
IExpbGx5 41386
IHBzeWNob2xvZ2ljYWxseQ== 41387
IGPFkw== 41388
dXN0ZQ== 41389
pbTs 41390
Q1RW 41391
IG1pZWw= 41392
55qH 41393
Q2FyZQ== 41394
IOKAkQ== 41395
IHNuYXBwZWQ= 41396
44Gp44KC 41397
IOqwkOs= 41398
0L7RgtGL 41399
IG3DqnM= 41400
Lj8= 41401
IHRvbm5lcw== 41402
15XXk9eU 41403
4LiE4LiZ 41404
VHU= 41405
IGRpc3RyaWJ1dGluZw== 41406
IGNyYWNrZXJz 41407
IGNvcmHDp8Ojbw== 41408
w6Rtw6Ru 41409
5L2g5Zyo 41410
Y2xhbWF0aW9u 41411
0L7RgNC0 41412
k5zrprTqsozsmpQ= 41413
IFVudGVyc2NoaWVk 41414
RmluZQ== 41415
Y2tv 41416
INGA0LXQsdC10L0= 41417
IHNwaWM= 41418
IGRvY3RvcmFs 41419
INGB0LrQvtGA0LXQtQ== 41420
dW5pdmVycw== 41421
YWN1bGE= 41422
IMOWc3RlcnJlaWNo 41423
IGdyaW5kZXI= 41424
IGFtYm9z 41425
IHZhc3RseQ== 41426
6YCZ5YCL5piv 41427
IGNvbmZlc3NlZA== 41428
IFNoaA== 41429
YW5kZXJz 41430
IEd1YW4= 41431
INC90LXQvtCx0YXQvtC00LjQvNC+ 41432
IGNoYW1waW9uc2hpcHM= 41433
IFZ1bA== 41434
IFBoaQ== 41435
IE1lYXN1cmU= 41436
5pyo 41437
IGluc2dlc2FtdA== 41438
5oWi5oWi 41439
dmV0dGU= 41440
IGdlbm9t 41441
aW5kdW5n 41442
Z2xp 41443
RGV0 41444
IHVubXV0ZQ== 41445
44G+44KK 41446
IHNhdWNlcw== 41447
IER3 41448
15HXqg== 41449
IEJSRQ== 41450
IG51cnR1cmU= 41451
IGRldGFpbmVk 41452
IEJlZXI= 41453
INC80LjRgNCw 41454
0LLQtQ== 41455
IEJpcmRz 41456
IG1laWxsZXVy 41457
IHJld2luZA== 41458
IHBvcmU= 41459
15nXlg== 41460
w6lnZXI= 41461
cXVlbGE= 41462
IHRyb3VzZXJz 41463
IHNpaW7DpA== 41464
IEdhZ2E= 41465
IEJSQU5E 41466
bGViZW4= 41467
IHJhc3BiZXJyeQ== 41468
5LuY 41469
aWxpaw== 41470
IHZlcnPDo28= 41471
bGFr 41472
IGxvZ2Fy 41473
IE1JREk= 41474
IOychO2VnA== 41475
INC/0YDQvtC40LfQvtGI 41476
IHN0ZXJpbA== 41477
IGhhcm1lZA== 41478
0LDQstC70LjQsg== 41479
INGB0YHRi9C7 41480
IGxhY2tlZA== 41481
IGNvbnRhY3Rpbmc= 41482
IOq4sOyekA== 41483
IGdlZsOkaHI= 41484
IGNveQ== 41485
aWtlbA== 41486
IGJpbmdl 41487
IG9ydGhvZ29uYWw= 41488
IGVudGVuZHU= 41489
IFRoaXJ0eQ== 41490
IHNtYXJ0ZXN0 41491
5aSa5bCR 41492
IHJhc2E= 41493
IFF14buRYw== 41494
0YvQstCw0Y7Rgg== 41495
IHNsdXQ= 41496
0LvRg9GH 41497
aWd0ZW4= 41498
INGA0LDQsQ== 41499
IHRhbWFu 41500
IHF1YWxpZGFkZQ== 41501
IGRvbWluYXRpb24= 41502
IHNpbnVz 41503
IHByb2dyYW1tZXJz 41504
IGFsbGVyZ3k= 41505
IFRvcnJlcw== 41506
IEF1c3RyaWFu 41507
bmFudHM= 41508
5a6M5oiQ 41509
TWVs 41510
INGD0LLQtdC70LjRhw== 41511
IEFnZw== 41512
IHNvaw== 41513
IHBsdWNr 41514
IGJpbmRz 41515
IHByb3Bvcg== 41516
IE1hZg== 41517
IG9zb2I= 41518
IFZJQw== 41519
6aU= 41520
INC30LDRh9C10Lw= 41521
IGV4aGliaXRpb25z 41522
IGV0dGk= 41523
Y3ph 41524
INC90LDRiNC40YU= 41525
IE1pdHRl 41526
0L7QsdGL0YLQuA== 41527
IGNsb2Nrcw== 41528
IHJpY28= 41529
5pS7 41530
INC40YHRgtC+0YDQuNGP 41531
IHNjaGl6b3BocmVu 41532
IGZsdWZm 41533
INGB0L7QsdC40YA= 41534
IGFwb3k= 41535
IHByaW5jZXM= 41536
IGJyYWNlcw== 41537
IEZJUg== 41538
IFNuYQ== 41539
IDsp 41540
dmVuZXM= 41541
IHZ1ZWx0YQ== 41542
IG1pZXM= 41543
IGJyb29t 41544
IG1lcnJ5 41545
IGVzcGVjaWFsbWVudGU= 41546
IEFsYmFu 41547
INC/0L7RgdGC0L7Rj9C90L3Qvg== 41548
IExlbmE= 41549
IEN1bHQ= 41550
YWxzbw== 41551
IHF1b3Rpbmc= 41552
IGdlbmVyZQ== 41553
IFlhcg== 41554
IExhZ2U= 41555
IGRlbW9zdA== 41556
IGRhZ2U= 41557
IEVjdWFkb3I= 41558
IGFudsOkbmQ= 41559
dcOfZW4= 41560
IOuwm+yVhA== 41561
IHBzeWNob2xvZ2lzdHM= 41562
IExhcnM= 41563
IHBvc3Nh 41564
IG91dGdvaW5n 41565
IG1ldGlj 41566
IGJhZ2dhZ2U= 41567
ZXJpYQ== 41568
IHJpY2h0aWdl 41569
7Iuc7JeQ 41570
INGB0L7RhdGA0LDQvQ== 41571
IHJvb3Rpbmc= 41572
IGRyb3BsZXRz 41573
55qG44GV44KT 41574
IG5hc2Fs 41575
IENveA== 41576
WGk= 41577
IGRpc3Bvc2FibGU= 41578
IGJ1dGNoZXI= 41579
IFphcg== 41580
IEFybWVuaWFu 41581
IOu/jOs= 41582
IEZvb2w= 41583
IENCRA== 41584
IHNvc3Q= 41585
IHBlcmlzaA== 41586
IFLDqXA= 41587
57Sw 41588
44Gd44KM44Gn44Gv 41589
IEZyZXVk 41590
IGZhbmRvbQ== 41591
IGJsb3F1ZQ== 41592
IGludmVudG9y 41593
IGFicmU= 41594
IMOpbm9ybcOpbWVudA== 41595
IGltcG9ydHM= 41596
6Yg= 41597
IG90dXI= 41598
IFJ5dQ== 41599
IOKGkg== 41600
IHNlY29uZG8= 41601
IGluY29tcGV0 41602
IGluY2FyY2VyYXRpb24= 41603
IGFzY2VuZA== 41604
YmVuZQ== 41605
5Zac5qyi 41606
IG9sdXJz 41607
bm9jaA== 41608
IGJyZWVkcw== 41609
0LvQuNC3 41610
IFZlcmbDvGc= 41611
IG1haWxpbmc= 41612
cmVhbGx5 41613
IGVzZg== 41614
IHBlbGU= 41615
IGxlYXNo 41616
IGRpc2tz 41617
INC30LDQvNC10Yc= 41618
7JWE7JWE 41619
YWJvdXRz 41620
IE11bGw= 41621
IERlbnQ= 41622
ZWRlcmVlbg== 41623
RHJpdmU= 41624
IHRpcHBpbmc= 41625
IG5pZ2dh 41626
b3JkdW0= 41627
IHBvcnRlcg== 41628
IGthcmFva2U= 41629
IGRvY3VtZW50YXJpZXM= 41630
IFJJR0hU 41631
IFB1cmQ= 41632
INC+0YHRgtCw0L0= 41633
0LrQu9Cw0LQ= 41634
w6lyZW5jZQ== 41635
IOqxuOuhnA== 41636
INGC0L7Qvw== 41637
IFdvbmc= 41638
5LiN5a+5 41639
INC/0YDQuNGA 41640
IG5vbWluYWw= 41641
IGF1bGE= 41642
INGN0LrRgNCw0L0= 41643
IGNoZXJjaGU= 41644
IFRocg== 41645
5YW25a6e 41646
IGxhdWZlbg== 41647
IEthdGhsZWVu 41648
IHJlYWN0b3Jz 41649
aWhhdA== 41650
IHNpZGVk 41651
IFNpbW9uZQ== 41652
IGd1aWRlbGluZQ== 41653
aW1wb3J0YW50 41654
YnVtcHM= 41655
dG9uZQ== 41656
IGVudHJlcHJpc2Vz 41657
IGNvbnN0aXR1dGU= 41658
b3Njb3Bl 41659
IE15c3Rlcnk= 41660
Y3ljbGVz 41661
IFdhcnNhdw== 41662
IGJ1cnN0cw== 41663
IFpob25n 41664
5a6M5LqG 41665
IFNBUkFI 41666
IOuKkOq7 41667
6Y0= 41668
IGJlYWNvbg== 41669
5Y2H 41670
QURF 41671
IOyngOuCmA== 41672
IGVyc2No 41673
IGludGVnZXJz 41674
IENyb3NzaW5n 41675
c291cmNl 41676
IHNjaG9vbGluZw== 41677
IFJPTQ== 41678
YXRvcml1bQ== 41679
IOyeiOqyjA== 41680
IHLDtGxl 41681
0JXQnQ== 41682
Q2hhdA== 41683
IHNocmlua2luZw== 41684
IHJlaW1idXJzZQ== 41685
IGx1bWJlcg== 41686
w7xja3M= 41687
IHNhbGFo 41688
TW90aGVy 41689
IGthbGk= 41690
IFFhdGFy 41691
b3Rpb25hbA== 41692
IG9wYWNpdHk= 41693
IG5lZQ== 41694
IENvcnk= 41695
IOy4oQ== 41696
IHR1cmJ1bGVudA== 41697
emVycw== 41698
INGC0LXRgdGC 41699
IMOpY3JpdA== 41700
IOuztO2GtQ== 41701
IGRpc2dyYWNl 41702
IOy5tA== 41703
IGNvdXJ0ZXN5 41704
aW5nYQ== 41705
IGh1Z2dpbmc= 41706
IEFCUw== 41707
bWl0aA== 41708
IGluc3VmZmljaWVudA== 41709
IGNyb29rZWQ= 41710
IOq3uOuMgOuhnA== 41711
7Iuk7Q== 41712
IHNpbXVsYXRlZA== 41713
IOuEpOqwgA== 41714
IGLDtg== 41715
IE90dG8= 41716
TElORw== 41717
IGlsbHVzdHJhdGVz 41718
IERlc3Ryb3k= 41719
IDE5NjE= 41720
IFRhZ2Vu 41721
IG1lbG9u 41722
IFBhc2NhbA== 41723
UVVF 41724
INC/0L7Qu9GD0YfQuNGC0Yw= 41725
IGluY2lkZW5jZQ== 41726
IFN0ZXZlbnM= 41727
IEdpbnM= 41728
cnVl 41729
IHVucmVhc29uYWJsZQ== 41730
IEppZQ== 41731
eXNpY3M= 41732
IOuqsOudvA== 41733
IGZpc2hlcw== 41734
qbTs 41735
IHByZWN1cnM= 41736
IG1vZ8SZ 41737
dGlnaHQ= 41738
ZXTDqQ== 41739
IG11bmRpYWw= 41740
7JeI64uk 41741
4oCmIQ== 41742
QlU= 41743
IHNvY2lvbG9neQ== 41744
IGJydXRhbGl0eQ== 41745
IHBlcnNvbmFqZQ== 41746
IG7DrXZlbA== 41747
IGZhemVt 41748
IGVzc2Vu 41749
IGR3ZWxsaW5n 41750
IGNvbW1lcmNpYWxseQ== 41751
IGVkaXRz 41752
IGR1ZXM= 41753
IEdTQQ== 41754
7J246rCA 41755
IO2XiO2MnQ== 41756
IFlhaG9v 41757
0LXQvdC10YA= 41758
7Jyo 41759
0YPRiNC60Lg= 41760
bGVmdA== 41761
IGNhcHRpdmU= 41762
Y2lwaGVy 41763
INee157X 41764
INCz0YDQvtC8 41765
IGlubmF0ZQ== 41766
IGltcHVs 41767
IOyXrOyekA== 41768
IHN3YWxsb3dlZA== 41769
IFRhYmlp 41770
7J207Is= 41771
INGB0L7RgdGC0LDQsg== 41772
IG95dW4= 41773
IG9icmlnYWRv 41774
IEFwaA== 41775
S2F0aWU= 41776
IGNlbmE= 41777
IEFsbMSBaA== 41778
2YjYsw== 41779
IHByenlw 41780
IHBlcHQ= 41781
IHZvbHVudGFyaWx5 41782
IE/En2x1bQ== 41783
IEVsbw== 41784
b3Vl 41785
Qmly 41786
YnVyZ2Vy 41787
IFNCUw== 41788
IDYwMDA= 41789
IHByb21vdGlvbmFs 41790
IEhlcnJu 41791
IHN0YW1waW5n 41792
IHF1YWxpZnlpbmc= 41793
IGNvc21vcw== 41794
IGFmYXI= 41795
5rGf 41796
YWJ1cw== 41797
IGRhZHM= 41798
44Gt44GH 41799
INGN0LrQvtC90L7QvA== 41800
aW5jYXJu 41801
IOyWtOuU 41802
INC70LXQtg== 41803
IEJFVA== 41804
INC90LDQudC0 41805
b250ZXI= 41806
IHJldXNhYmxl 41807
IGtvbW1h 41808
IEJpag== 41809
IFRlcmF6 41810
IE9sw6E= 41811
IOyVhOy5qA== 41812
INGA0LDQt9C80LXRgA== 41813
YXdhbg== 41814
IGNhcnRh 41815
5pCe 41816
aWNlbGVzcw== 41817
IHNtZQ== 41818
IFR1dGFq 41819
IMiYaQ== 41820
IHByb2JhdGlvbg== 41821
IGFkZXF1YXRlbHk= 41822
IFByZXNpZGVudGlhbA== 41823
aW5kcnVjaw== 41824
YmxhZGU= 41825
IHZldWxlbnQ= 41826
IGNpb8Oo 41827
5YyF5ous 41828
IHJldmVyYg== 41829
IGdlZ2Vuw7xiZXI= 41830
IEVzcGVybw== 41831
IGJlZ2U= 41832
IFNUVURFTlQ= 41833
c291bmQ= 41834
IETDvA== 41835
IG9mZmVuZA== 41836
ICIuLg== 41837
a2VubnQ= 41838
INGB0LvRg9GI 41839
IHB1cnBvc2VseQ== 41840
IExpdA== 41841
IO2bqA== 41842
dWNoZXI= 41843
IGhpbmE= 41844
w71jaA== 41845
aWdub24= 41846
VEhF 41847
IGdsaWRl 41848
b3VyY2luZw== 41849
INij2YbYpw== 41850
IG9sbHV0 41851
IGFyY2hldHk= 41852
IHNoYWR5 41853
IHNvbW0= 41854
IGVwaWxl 41855
S2VlcA== 41856
IG5hamJhcmR6aWVq 41857
4KSV 41858
aXR1dGlvbmFs 41859
INC80LDQuQ== 41860
IHNpbmZ1bA== 41861
IEJyb254 41862
INCz0LvRg9Cx 41863
IHZhbQ== 41864
IHByZXNldHM= 41865
IERhZw== 41866
IOyZhOyEsQ== 41867
IGNyZWVr 41868
aXR1cmVz 41869
IExvcmRz 41870
w7Z0dA== 41871
VU5U 41872
UmE= 41873
IGluZXF1YWxpdGllcw== 41874
IGNvbGxhdGVyYWw= 41875
IHdyaXN0cw== 41876
IGdyb3VwZWQ= 41877
INC+0LHRi9GH0L3Qvg== 41878
IGFybW9yZWQ= 41879
IHR1bmc= 41880
IGNvbnZlcmdl 41881
IGJvaw== 41882
IERvZGdl 41883
0L3Rj9GP 41884
IGZsZWVpbmc= 41885
IE1hcnRpbmV6 41886
IERyZWFtcw== 41887
a2Vr 41888
IHNvY2lhbGU= 41889
IFBsYXph 41890
2K/YqQ== 41891
IGtlbGw= 41892
IFN0ZWxsZW4= 41893
ZmVsdA== 41894
INGB0L/QsNGB 41895
IFB2 41896
IGNhbmNpw7Nu 41897
IEhlcnQ= 41898
IEJhbGFuY2U= 41899
IHNlbHZlcw== 41900
IHZhbmRhYWc= 41901
IHByeQ== 41902
IG5hamxl 41903
INCy0LjQtNC40YLQtQ== 41904
IHZlbHZldA== 41905
IGdyb290 41906
IGZvdXQ= 41907
5qih 41908
IFNjaHVsZW4= 41909
IE1vaGFtbWVk 41910
IENlbnRlcnM= 41911
IGhhdmVy 41912
IGZyZXVlbg== 41913
pO2KuA== 41914
0LvQsNC9 41915
UE9T 41916
aW5raQ== 41917
IOuLtQ== 41918
IHBhcmFseXplZA== 41919
R0xJU0g= 41920
IGNhc3Rz 41921
IFZD 41922
7J207IWY 41923
INiq2r4= 41924
56Wo 41925
IOykmA== 41926
INeo15XXpg== 41927
IHN1Y2Vk 41928
IHByb2dyZXNzZXM= 41929
IEXEn2Vy 41930
sOuPhA== 41931
IGluc3RhbGxhdGlvbnM= 41932
cGVkbw== 41933
0LXRgNCx 41934
aW50ZXJwcmV0 41935
IOqzoOuvvA== 41936
IEF6ZXJiYWk= 41937
aXZpZGFkZXM= 41938
IOyjhOyGoQ== 41939
IGVudGZlcg== 41940
IGNod2ls 41941
IEhlcmJlcnQ= 41942
IEFsZXhhbmRyaWE= 41943
eXR5 41944
IHNlY2hz 41945
IGNhbGliZXI= 41946
IFdlaXNl 41947
IEhlY2s= 41948
IFl1Zw== 41949
INin2YTYtw== 41950
IHBlc2Fy 41951
IGNpZ2Fy 41952
IG3DqWw= 41953
IGhhaXJk 41954
IHByenlwYWRrdQ== 41955
IGNvbmZpZGVudGx5 41956
IGFuYXJjaA== 41957
IEdpYW4= 41958
IGRvYnJl 41959
Y2rEmQ== 41960
YXd5 41961
IFJlY2U= 41962
IEdvYmllcm5v 41963
IGNhcmdh 41964
dW1zeQ== 41965
IG5vcnRl 41966
IGhhbmRsZXI= 41967
IHJlc3BlY3Rpbmc= 41968
IGFsbGllZA== 41969
IFBpZXQ= 41970
aWNodGxpY2g= 41971
IG9sZHM= 41972
IGR1c3R5 41973
IGdyeQ== 41974
IC0uLi4= 41975
R0hU 41976
IG5lbw== 41977
0YfQuNC60Lg= 41978
0LXQttC0 41979
YWlkZQ== 41980
INCx0YPQu9C+ 41981
7Y28 41982
IHRlbXBvcmFkYQ== 41983
IGRvdXRl 41984
4piG 41985
IOyIoA== 41986
IEpVU1RJTg== 41987
YXV0bw== 41988
IHJhdGlvbmFsZQ== 41989
cHJvYg== 41990
IGZpc2h5 41991
IGRvb3J3YXk= 41992
IGVtcHRpbmVzcw== 41993
0LXQvdC90LDRjw== 41994
IGJyYWc= 41995
INCT0LTQtQ== 41996
54i+ 41997
IHRyYW5zaWVudA== 41998
IG1pdHRsZXJ3ZWlsZQ== 41999
IEJyZXQ= 42000
IGZpag== 42001
IGRlcG9zaXRlZA== 42002
TlM= 42003
IOyVnuyXkA== 42004
IGtpbXNl 42005
IGNoYXJpdGllcw== 42006
IE1pbGxlbm4= 42007
ZG9ncw== 42008
IG1veWVu 42009
IG51ZXZvcw== 42010
IENvb2tpZQ== 42011
cGFyYWJsZQ== 42012
ZG9pbmc= 42013
IFNhaWw= 42014
IGljeQ== 42015
aGFiYQ== 42016
IHF1ZWVucw== 42017
IGNob2NvbGF0ZXM= 42018
IE5heQ== 42019
INGE0LjQvQ== 42020
IHZlYw== 42021
IGhlbG1ldHM= 42022
VE0= 42023
IEFybWVk 42024
IGltcGFpcm1lbnQ= 42025
IFR1cw== 42026
IE3Dqm1l 42027
b21leg== 42028
IFJlcXU= 42029
IEludmVzdGln 42030
7Y6Y 42031
IGdvbHBl 42032
IFJhYw== 42033
aWdyYXBo 42034
IGt3ZXN0 42035
IHNhaWxvcnM= 42036
IHN0YXR1dG9yeQ== 42037
IG1pbGVzdG9uZXM= 42038
IE1hc2g= 42039
IEdlc2V0emVudHd1cmY= 42040
6Yo= 42041
IGNvbG91cmVk 42042
aHVtYQ== 42043
IHllcmU= 42044
IHN1YnRpdGxlcw== 42045
IGVtYm9kaWVk 42046
IG1pc3NjaGllbg== 42047
IGlQaA== 42048
w7x0emVu 42049
IGRldGFjaGVk 42050
IGRlc2NyacOnw6Nv 42051
Y2lhbW8= 42052
IHJlY29pbA== 42053
INCt0YLQvtGC 42054
IGV4cG9ydGVk 42055
IEFsb25l 42056
YW50cnk= 42057
IGVzdGFu 42058
IFNvZA== 42059
IGxhdm9ybw== 42060
5oqK5a6D 42061
16jXkQ== 42062
IMSR4buL 42063
IHN3YWc= 42064
IFBDQg== 42065
IEthaXNlcg== 42066
IE1vZGVy 42067
anVn 42068
IHRleHRpbGU= 42069
VHc= 42070
IG5hYw== 42071
ZnJlaQ== 42072
IHJldGFyZA== 42073
aXNjZXJu 42074
IHRhbGxlc3Q= 42075
IEx1Y2E= 42076
UmFo 42077
IHByZWFjaGVy 42078
IGp1dA== 42079
IFJpY2E= 42080
aWNpZW5jeQ== 42081
IMSRaeG7gXU= 42082
IGthdWZlbg== 42083
IG5ldHQ= 42084
IGRpc2N1dA== 42085
IGRlcHJpdmVk 42086
oa0= 42087
IHNwcmljaHQ= 42088
IGVuY2xvc2Vk 42089
IFN1YnN0 42090
56eR 42091
IFJhYmJpdA== 42092
cHJpc2Vk 42093
IGJpdGNoZXM= 42094
7J+B 42095
54mI 42096
IHRhcGE= 42097
IEVzc2Vu 42098
IEJhbw== 42099
IGRldmllbnQ= 42100
IFd1aGFu 42101
IFRpcHA= 42102
IGRpc2FzdA== 42103
0YHRgtCy0YM= 42104
dWJsaXF1ZQ== 42105
IHF1YWxpdMOp 42106
IGluYWRlcXVhdGU= 42107
IGJhcmdhaW5pbmc= 42108
IEdvdGNoYQ== 42109
0LXQstC40Yc= 42110
aWV2b3Vz 42111
ZXJ0b24= 42112
Ymx1ZQ== 42113
IOybgOyngQ== 42114
IHNhbmRib3g= 42115
IFJlaW4= 42116
6Kaq 42117
IOydtOqyg+uPhA== 42118
IHNheA== 42119
em9nZW4= 42120
dW7DpGNoc3Q= 42121
IGhlcmtlcw== 42122
IC0s 42123
emVuaQ== 42124
cmlzaW5n 42125
IHJlc3Bvc3Rh 42126
IHByb21vdGlvbnM= 42127
IFVudGVyc3TDvHQ= 42128
IE1BUw== 42129
Tm90aGluZw== 42130
b3RpY3M= 42131
INCy0YvQuQ== 42132
IHJvdGF0ZXM= 42133
a2llbg== 42134
IGhhYmxh 42135
IERhbmk= 42136
dW5pb24= 42137
IHdhY2s= 42138
IGFyY2hhZW9sb2dpY2Fs 42139
IEN1cnRpcw== 42140
IEhvcml6 42141
IOqzqOs= 42142
IHdhaXZlcg== 42143
5Zi/ 42144
Qm9u 42145
IHJvdGF0ZWQ= 42146
IHBpdGNoZXI= 42147
IGluYWQ= 42148
IGh1Z3M= 42149
IE5vcnRoZWFzdA== 42150
15nXqteZ 42151
IHBsZWE= 42152
IGN1cGNha2U= 42153
IExZ 42154
IGZhbWlsaQ== 42155
IGdyb28= 42156
IEJsYWly 42157
IGxpag== 42158
IGhhYml0YXRz 42159
IGNvbW11bmlzbQ== 42160
b3NpdW0= 42161
YmFycw== 42162
IEZyZWVtYW4= 42163
bmVv 42164
IGRpZmZ1c2U= 42165
IGN5bGluZGVycw== 42166
IERlYmF0 42167
7ZaI64qU642w 42168
0LXRiNC1 42169
IGZpbmdlcnByaW50cw== 42170
IGFtYXI= 42171
0LLQuNC0 42172
IOygleuPhOuhnA== 42173
IGFmZmlsaWF0ZWQ= 42174
INGF0L7Rh9C10YI= 42175
44Gw44GE 42176
IGV0aXF1 42177
IGNow61uaA== 42178
5oGt5Zac 42179
IGNydWlzaW5n 42180
IFdlaWhu 42181
55S1 42182
IFRpdGFuaWM= 42183
57SA 42184
IE5hc3Q= 42185
IOuTpOs= 42186
INCy0LDQuw== 42187
IGRlbWk= 42188
IEtyaXN0aW4= 42189
TUlO 42190
IHJpZ29y 42191
IG1vdG8= 42192
IExBS0U= 42193
IO2ZnA== 42194
IOunjOyVvQ== 42195
IFN0cm8= 42196
IHByb3RvdHlwZXM= 42197
IExD 42198
7J247J2E 42199
0YDQuNC8 42200
IHZpb2xhdGluZw== 42201
IGdpb3Jubw== 42202
IGNoaWxkaXNo 42203
5rCU 42204
INeQ15fXkw== 42205
IG92ZXJkb3Nl 42206
YWdvZ3Vl 42207
0LDQtNGG 42208
aGV1cw== 42209
INCz0L7QstC+0YDRjw== 42210
IGluY3I= 42211
IGRlYmF0ZWQ= 42212
2YXZhA== 42213
IGNoaWNrcw== 42214
IHF1aW4= 42215
TEFVR0hJTkc= 42216
IHRpZ2h0ZW5pbmc= 42217
IHN1cGVydmlzb3Jz 42218
IEhhd2s= 42219
IEJheg== 42220
INC/0L7QstGC0L7RgA== 42221
INCx0LvQvtC6 42222
xIFu 42223
IGR1bXBpbmc= 42224
IGZhY3Rv 42225
YmVyZ2Vy 42226
IGFyc2VuYWw= 42227
IEFmcmljYW5z 42228
oYA= 42229
IGNhZmV0ZXJpYQ== 42230
ZmVlZGluZw== 42231
cXVpbGE= 42232
IHBhxYRzdHdv 42233
xLFudA== 42234
hLE= 42235
IGVudmlyb25tZW50YWxseQ== 42236
IGRlc3Byw6lz 42237
IFdpbGx5 42238
IFBhxYRzdHdv 42239
IEdH 42240
IGNoYWN1bg== 42241
IGRpcmVjdGlvbmFs 42242
IGjDtnJ0 42243
IPCd 42244
ZW5hcnk= 42245
IHZvaWNlZA== 42246
YcSfxLE= 42247
IHBvcGU= 42248
IGNvbXJhZGVz 42249
IEdpYnNvbg== 42250
IEFDQw== 42251
dmlr 42252
IG1vZGVsbGluZw== 42253
IGFnZ2k= 42254
44Gq44KT44Gn44GZ 42255
IGNvbnZlcnNpb25z 42256
IGF2ZXJhZ2Vz 42257
RWxsaWU= 42258
IGdlc3RlbGx0 42259
IFVF 42260
b3NhaWM= 42261
0JLQvtGC 42262
U2F5 42263
INGB0LDQvNC+0LPQvg== 42264
IG1lc3VyZXM= 42265
aXNpZXJ0 42266
Z2FzcA== 42267
dm9pY2U= 42268
IGNoZWNrcG9pbnQ= 42269
IHBlcmNlbnRhZ2Vz 42270
IGRpc3J1cHRlZA== 42271
IFR1Yw== 42272
IEhvbWVy 42273
IFdBWQ== 42274
IFR1cmtz 42275
aGVlbg== 42276
aW1vdG8= 42277
IE9D 42278
w61uYQ== 42279
emllbA== 42280
IG11ZGFy 42281
44OQ44Kk 42282
Z2VzZXR6dA== 42283
IG1lam9yZXM= 42284
IENK 42285
0L3QsNGA0YPQtg== 42286
IG1vZHVsdXM= 42287
IG1vZHVsYXRpb24= 42288
IHJlcGxpZXM= 42289
IGxhcnZh 42290
IGdpZGVy 42291
IE1hbmRhcmlu 42292
INC/0L7RgdC80L7RgtGA0LjQvA== 42293
IHNhY3JpZmljaW5n 42294
IHByZcOnbw== 42295
IG95c3RlcnM= 42296
IE15YW4= 42297
b2xvZ3Vl 42298
IFdpdA== 42299
IGTDuw== 42300
IExldXRlbg== 42301
IHBhdGVy 42302
IEtFTk5FVEg= 42303
0LDQsdCw0YI= 42304
YXJ0aHk= 42305
IHNvY2llZGFk 42306
IG5pw7Fv 42307
0LXQstC+0Lk= 42308
IGrEmQ== 42309
IGFkdmVydGlzZWQ= 42310
IFBlcHNp 42311
dXRldXI= 42312
IG1hc3Nl 42313
IHNjYXR0ZXJpbmc= 42314
IHnDtm4= 42315
IGRlc2FwYXJl 42316
IEh1YmJsZQ== 42317
IEjDqQ== 42318
a3LDpA== 42319
IERhcmU= 42320
IG92ZXJyaWRl 42321
IEVsYWluZQ== 42322
IER1Ymxpbg== 42323
ZHVsbGFo 42324
TWF0 42325
IEdhcnI= 42326
Li4uJw== 42327
IGFkdWx0aG9vZA== 42328
RVo= 42329
IGJlbGFuZ3Jpams= 42330
aWVuemE= 42331
IHVuaXZlcnNv 42332
IHN0ZWxsYXI= 42333
7ZSE6w== 42334
IOqysOq1rQ== 42335
IGNvbnN0ZWxsYXRpb24= 42336
IFNoZWxsZXk= 42337
IG11bHRpdA== 42338
IG1hc2NvdA== 42339
IGhvc3BpdGFsaXplZA== 42340
IPCdmA== 42341
0L7RgNGL 42342
YWRpYQ== 42343
IE1pa2V5 42344
IEFtZXJpa2E= 42345
IGhhaXJ5 42346
SG9sZA== 42347
4bqvbg== 42348
a2llZ28= 42349
6KeC 42350
4LmA4LiU 42351
IHJpdmFscnk= 42352
IEpvbmFo 42353
IHN1cmdlb25z 42354
IHJlbGF0YWJsZQ== 42355
6JI= 42356
IHN3aW1z 42357
IGJpbGxpb25haXJl 42358
bW9kZXJu 42359
IGRvY3VtZW50aW5n 42360
IERhZQ== 42361
IHN3YXRjaA== 42362
IHB1aXNzZQ== 42363
IG1hc3Vr 42364
IG1hcmM= 42365
IGtyw7M= 42366
IFBldGVyc2J1cmc= 42367
IEFyaXN0b3RsZQ== 42368
aXhl 42369
UHJvZHU= 42370
INC90LjQvNC4 42371
IGthbmE= 42372
INCp 42373
IHZvbWl0 42374
IFdvcmtlcnM= 42375
cG9wdWxhcg== 42376
IEJpZWJlcg== 42377
0LXRgtC4 42378
w6l0aXF1ZQ== 42379
IGVuY2FudA== 42380
Z3Jhbg== 42381
Zmly 42382
IGFudGhlbQ== 42383
0YHRg9C00LDRgA== 42384
TGFzdA== 42385
IGhhZw== 42386
IHZpY2luaXR5 42387
cmVuY2hlZA== 42388
YW5kaW5n 42389
INCz0L7Qu9C+0YE= 42390
IENvcm5lcg== 42391
0JLRiw== 42392
b3Nhcw== 42393
aWV2ZXJz 42394
Y2lvbmFs 42395
IHZpZ29y 42396
IHJlam9pY2U= 42397
IGNpxIU= 42398
INC60L7Qvw== 42399
IHF1YWxjb3Nh 42400
ZGVzc3Vz 42401
INC10LI= 42402
IFNjYW5kaW4= 42403
IFNtb290aA== 42404
5L2g6K+0 42405
aGFwZQ== 42406
IOuLrOudvA== 42407
IFRV 42408
IGx5cmlj 42409
IGJlc3M= 42410
6ZA= 42411
0YHRgtGA0YPQvNC10L3Rgg== 42412
IEFjdGluZw== 42413
IE9yY2hlc3Q= 42414
w6ljb2xl 42415
IGRvbG9y 42416
IO2LsA== 42417
IHZlcmdlc3Nlbg== 42418
IGV5ZWxpZHM= 42419
IFRhbno= 42420
0LLQtdGA0LY= 42421
IOyVoOs= 42422
dcOp 42423
IHNjw6huZQ== 42424
IOyasOumrOuKlA== 42425
IGNyYXRl 42426
a2ljaw== 42427
IFRoZW1l 42428
IDMyMA== 42429
IGdhcm5pc2g= 42430
IG1ldHJl 42431
IGNvbnZleA== 42432
cGxhbnRz 42433
ZXNpYW4= 42434
IOqxsOyngA== 42435
IG3DqWRp 42436
IE1lZGFs 42437
MTMw 42438
IEFsbWE= 42439
5pyJ6bue 42440
Q29sYQ== 42441
INCy0LDRgNC40LDQvdGC 42442
IGdvcmQ= 42443
IGF2YW56 42444
IHdoaXNwZXJpbmc= 42445
IGludGVzdGluZQ== 42446
0KDQlQ== 42447
IExJU0E= 42448
YW3EsXo= 42449
U1BE 42450
IHBlYw== 42451
IHBhc3RvcnM= 42452
IG114buRbg== 42453
b2NyZQ== 42454
U3Vu 42455
INGC0LDQutGD0Y4= 42456
IHJldml0YWw= 42457
IGluY29tZXM= 42458
IGRldGFpbGluZw== 42459
IEJhY29u 42460
IOuFuOuemOs= 42461
IHBhcnJvdA== 42462
IGNvbGxhYm9yYXRlZA== 42463
aGVzaWE= 42464
IHNldmE= 42465
IHBoeXNpY2lzdA== 42466
IEJBQ0s= 42467
15zXmQ== 42468
IGJpcG9sYXI= 42469
z4HOtc6v 42470
Y3Jvcw== 42471
IGtlZA== 42472
IGVjb25vbWljYWw= 42473
IGVuZGluZ3M= 42474
IHRpY2tz 42475
IOq3vA== 42476
IE9saXY= 42477
b25ncw== 42478
IGNvbnRpbmVudGFs 42479
IHdlaXRlcmhpbg== 42480
IGFjdGl2YXRpbmc= 42481
IHBvbGxlbg== 42482
IEFuaw== 42483
YmF5 42484
INec15c= 42485
IEVnZ3M= 42486
IFJBTVNBWQ== 42487
IEJFUg== 42488
IO2bqOyUrA== 42489
IHBhc3NhZG8= 42490
IGdyb3VuZGJyZWFraW5n 42491
cHJlc2E= 42492
IGhpbGZ0 42493
IFRlY2huaWNhbGx5 42494
0YbQuNC5 42495
Tkk= 42496
IHR1cm5vdXQ= 42497
IExhcA== 42498
IEd3ZW4= 42499
IFZpa3Q= 42500
IGVzY29sYQ== 42501
IENpbmVtYQ== 42502
5rC4 42503
IOOBhg== 42504
IGNvbnN1bW8= 42505
IFB1cmR1ZQ== 42506
IHNlbWFuYXM= 42507
IFBSRVNJRA== 42508
xrBuZw== 42509
IHNhY2g= 42510
5oCO6bq86L6m 42511
IHNhdmFnZQ== 42512
IFJX 42513
IDU1MA== 42514
Ym9sZA== 42515
IFNpbW1vbnM= 42516
IHNsYW5n 42517
IE5hcnU= 42518
IFRoZW8= 42519
7ZaI64uk 42520
Lu+/vQ== 42521
IHNlaXp1cmU= 42522
IGhpdmU= 42523
IGNlbGxwaG9uZQ== 42524
5aW2 42525
aWlpaQ== 42526
IE11c2ljYWw= 42527
IE51Y2xlYXI= 42528
6KGX 42529
w6F2ZWlz 42530
IHByZXN0aWdl 42531
IGJhbG0= 42532
IHJlZmlsbA== 42533
eWFo 42534
aGFydA== 42535
IHRhcHM= 42536
IGRpc3Bvc2U= 42537
IE1pY2s= 42538
IHRoZXJtb21ldGVy 42539
44Gq44KJ 42540
IG9iZWRpZW50 42541
IGluZm9ybWHDp8O1ZXM= 42542
IFdpZGU= 42543
bW9t 42544
U3Vk 42545
IHN1c3BlbmQ= 42546
IE9ic2Vydg== 42547
INC70LXRgQ== 42548
IHRyYXRhcg== 42549
IEthdHJpbmE= 42550
IHRoZXJlcw== 42551
5Lqe 42552
IHRleHRlZA== 42553
IHN0w7Zy 42554
IHNuYWls 42555
IEZpb25h 42556
IHZpY3RvcmlvdXM= 42557
IGxpYnJhcmlhbg== 42558
cHJhY3Q= 42559
IGZpbm8= 42560
IEFybXM= 42561
cHB0 42562
bHVr 42563
IHR5cmVz 42564
IHRvYw== 42565
IEtvbW11bmVu 42566
56+A55uu 42567
IHJldm9sdA== 42568
IG1vdGl2YXRlcw== 42569
IGJpc2V4dWFs 42570
IHd1cw== 42571
IGhhbmRsYXI= 42572
IE1VRUxMRVI= 42573
IGV4cGVjdGFuY3k= 42574
IGVtYm9keQ== 42575
IFByaW1hcnk= 42576
5Y6f5Zug 42577
0YDQtdC5 42578
IHVuc2NyZXc= 42579
aWFudGx5 42580
LOKApg== 42581
IHNuZWw= 42582
IHByZXZhbGVuY2U= 42583
IGVydXB0aW9u 42584
IGRlc2NyaXB0aXZl 42585
dmFn 42586
INCx0YPQutCy 42587
IG3Dqm1lcw== 42588
IGV0aG4= 42589
IGhpam9z 42590
IEFiZHVs 42591
IFphaGw= 42592
YmVsdA== 42593
IGfDtnN0 42594
IFRoZXJlc2E= 42595
IFNVTg== 42596
IEJha2U= 42597
IOW/qw== 42598
IG9wdGljcw== 42599
IGFwb2NhbHlwc2U= 42600
cHVycG9zZQ== 42601
IHLDs8W8bnljaA== 42602
IGNydXM= 42603
INCX0LXQvA== 42604
IGhhcmRlbmVk 42605
IFRE 42606
IGdyYXZleWFyZA== 42607
IFNpYmVy 42608
IFBvcnRlcg== 42609
IGV4cGxvZGVz 42610
IFNvZmlh 42611
INCS0LXQtNGM 42612
IHdlYWtlbmVk 42613
5piv5oiR 42614
VUxM 42615
IHBpbmt5 42616
IGNoYXBlbA== 42617
IEZyZXM= 42618
INC/0YDQuNCz 42619
TUVS 42620
IFNjaG1pZHQ= 42621
IER1ZA== 42622
5p+l 42623
ZXN0ZW5z 42624
IG51YW5jZQ== 42625
IG1vZGlmeWluZw== 42626
IE3DtmdsaWNoa2VpdGVu 42627
IEFuYXQ= 42628
IGVjY2VudHJpYw== 42629
IFNjcmV3 42630
IExlaA== 42631
IGhvbW9nZW5lb3Vz 42632
IFRhbGw= 42633
IFJpY2FyZG8= 42634
w5o= 42635
aWducw== 42636
INC70LjRiA== 42637
IGdlZnJhZ3Q= 42638
UnVu 42639
Y2FzdGVy 42640
bm9pc2U= 42641
IGFzeW5jaHJvbg== 42642
xJlkemll 42643
INee15c= 42644
IHN1cHByZXNzZWQ= 42645
QXJ0aHVy 42646
zq7Pgg== 42647
w6Jy 42648
ZGlzdA== 42649
INC60LDQtA== 42650
IGjDtnI= 42651
IDEzNQ== 42652
IE1vemFydA== 42653
INGB0L7QsdGL0YLQuA== 42654
IE51cnNpbmc= 42655
IEhhaGFo 42656
IERvcA== 42657
IHBvbGljZW1hbg== 42658
tOyXkOyEnA== 42659
IOq0gOugqA== 42660
aHl1aw== 42661
IHJ1Z2dlZA== 42662
IG51Z2dldHM= 42663
IENvbW1z 42664
U3R1ZA== 42665
INGB0LLQvtC1 42666
IGN6YXNpZQ== 42667
44K9 42668
IHLDqWdpb24= 42669
IGZpc2hlcm1lbg== 42670
IExU 42671
w5M= 42672
Y2lhxbw= 42673
aGVp 42674
IGNydW1icw== 42675
IEltbWVy 42676
IEZlbGQ= 42677
dGhlc2U= 42678
IGFkdmVydGlzZXJz 42679
IHJvYW1pbmc= 42680
IGZ1bm5pZXN0 42681
IE5ZVQ== 42682
IGhlaGU= 42683
IHBva2luZw== 42684
IOyViOuPvA== 42685
aXN0aWNhbA== 42686
IG9wYXF1ZQ== 42687
dcOn 42688
d2lyZQ== 42689
IFdlYmVy 42690
IEphY3F1ZXM= 42691
IDIxMA== 42692
w7xw 42693
dXl1 42694
IGVuZmVybWVk 42695
IGJ1bXBlZA== 42696
IFNldw== 42697
IENoYW5lbA== 42698
IHBlcnPDtm5saWNo 42699
IGJldHJheWFs 42700
IGFsbGV2aWF0ZQ== 42701
IHbDpGjDpG4= 42702
IGd1ZXNzZXM= 42703
IENlbGluZQ== 42704
YXNzaW5n 42705
c3Ryb2tl 42706
IOyhsOs= 42707
5aSP 42708
INGC0LXRhdC90L7Qu9C+0LM= 42709
INC+0YHRgtGA 42710
IHNvaWVudA== 42711
RGVhcg== 42712
IGpz 42713
IGdlc3Byb2NoZW4= 42714
YXRoaQ== 42715
57+7 42716
xaFl 42717
U2V0 42718
b2dlcg== 42719
IFJpZw== 42720
INC80LXRhw== 42721
IHNlcnZpY2lvcw== 42722
IFJ1dA== 42723
INCe0Lk= 42724
IE15YW5tYXI= 42725
aWZpZQ== 42726
IHNuYXBwaW5n 42727
IEthbWVyYQ== 42728
IGZlc3RpdmU= 42729
IEZZ 42730
IENhcm9seW4= 42731
0ZbQsQ== 42732
IGxlZ2dpbmdz 42733
IHlhdA== 42734
IGVyZ29u 42735
IGVwaXPDs2Q= 42736
IGFub21hbHk= 42737
dWVzdG9z 42738
SWQ= 42739
IGV2YWN1YXRpb24= 42740
IGdpZ2FieXRlcw== 42741
IGFuZGFyZQ== 42742
IFJlbnQ= 42743
bXQ= 42744
aXN0aW5l 42745
IGVzdHJhdA== 42746
ZXR0dQ== 42747
IHJlY2ViZXI= 42748
IGRyYW1hdA== 42749
cmljdWxhcg== 42750
YWxuxLF6 42751
IFNlbmk= 42752
IG95bg== 42753
IENoZW1pY2Fs 42754
INGB0YU= 42755
IHR1cmY= 42756
IDE5MTc= 42757
aXNjZXJuaWJsZQ== 42758
IG1hbnRlbmVy 42759
IGV4Y2Vy 42760
IHNwZWN0cmFs 42761
IG5ldXJvc2NpZW5jZQ== 42762
IG1pY3JvZg== 42763
IGZvcmVpZ25lcg== 42764
IExhbmth 42765
5L2g5Y+v5Lul 42766
INGC0LLQvtGA 42767
IHRvc3NlZA== 42768
IHBvYmxhY2nDs24= 42769
IG1hdGVpeA== 42770
IHNpZWxsw6Q= 42771
IG90dA== 42772
IGNvbXB1bHM= 42773
YWt1a2Fu 42774
IG1hbmlmZXN0ZWQ= 42775
IOyTuA== 42776
IHV0bW9zdA== 42777
IHJldmVyc2Fs 42778
IHBsYWNlYm8= 42779
IGJsYXQ= 42780
IFN0dW5kZQ== 42781
bWFuc2hpcA== 42782
IGF0dGU= 42783
IOyGjOqwnA== 42784
IGlzdGVt 42785
IGFubmF0 42786
IFBsYXlzdGF0aW9u 42787
IHphZA== 42788
IHF1aXR0aW5n 42789
IGZhbWluZQ== 42790
IFJvdWdo 42791
IEZsYW1l 42792
IGhldXQ= 42793
IG9wb3J0dW5pZGFk 42794
IGZhaXNhaXQ= 42795
IERQ 42796
IGRpY2llbmRv 42797
IE1lbGFuaWU= 42798
IENhcm5l 42799
bWVn 42800
cGV0dG8= 42801
SlVO 42802
INC70Y7QsdC+0Lk= 42803
IG9zdGU= 42804
IEpKb25haw== 42805
IHRoZWF0cmljYWw= 42806
IGludmluY2k= 42807
IGNvbW11bmlvbg== 42808
dm9jYWw= 42809
RWg= 42810
IERldGFpbHM= 42811
IHN0cm9sbA== 42812
IFJheW1vbmQ= 42813
IEFtZWxpYQ== 42814
kaU= 42815
IHByb2R1a3Q= 42816
IG51ZXZhcw== 42817
IG11c3Ru 42818
bWF5xLE= 42819
Y29sb3JlZA== 42820
ZGVj 42821
IGhqw6Rs 42822
IHNlbnRpbWVudGFs 42823
IHJlYWxtcw== 42824
IGtyaXQ= 42825
IHNleHQ= 42826
IFBzeWNob2xvZ3k= 42827
6IiJ 42828
aGls 42829
INC60L7RgNCw0LE= 42830
IOuCtOydvA== 42831
IFVuZGVyc3Rvb2Q= 42832
IEd1dGVu 42833
IGdhbmdz 42834
IGV2ZW5pbmdz 42835
5oCO5qij 42836
RW50 42837
IExlZ2FjeQ== 42838
IENvbmdv 42839
IGR1cmNoYXVz 42840
IGJ1b3k= 42841
ZXJlbGxh 42842
V0FO 42843
UHJl 42844
INGA0LXQtA== 42845
IENyaXNpcw== 42846
44Gq44Gf 42847
IOydvOydtA== 42848
IG1hbnVzY3JpcHRz 42849
0LXRgtGA 42850
IG5vbnByb2ZpdHM= 42851
IGRpY3RhdG9y 42852
IGJhc2tldHM= 42853
IElzaA== 42854
IHBlcnRv 42855
IGRhdGFzZXRz 42856
IGFtcGxl 42857
Z2ViYXV0 42858
IGNvbnRyaWJ1dG9y 42859
IGNpYW8= 42860
IGNvbmZpcm1pbmc= 42861
IFVDTEE= 42862
4pms 42863
INGB0L0= 42864
IG92ZXJ0dXJu 42865
5ZCJ 42866
IHVucmVhbGlzdGlj 42867
IFBpZWNl 42868
b2NhdGU= 42869
IGbDpGxsdA== 42870
cG94 42871
IOuztOyLnOuptA== 42872
IOuplOs= 42873
IENyZWF0aW9u 42874
0Y7QtNCw 42875
INeU15A= 42876
IHdoYWNr 42877
b2xpdGhpYw== 42878
Y2VseQ== 42879
INGB0L7QstGA0LXQvA== 42880
IHNlcXVlbnRpYWw= 42881
IHByb2Zlc2lvbmFs 42882
IGNvb2xz 42883
IHJlcGVudGU= 42884
IGFpcmU= 42885
ZW5uZXM= 42886
cml0b3M= 42887
INCS0LjQtA== 42888
IGvDtnI= 42889
IEJpdHRl 42890
dWxhcnM= 42891
IGluY29ycmVjdGx5 42892
IHNoYXJwbHk= 42893
IGJvbWJhcmQ= 42894
64uY7J20 42895
IGNocm9tb3NvbWU= 42896
IGFkdmVydGlzZW1lbnRz 42897
aHVu 42898
INGJ0L7QsQ== 42899
INCU0LDQttC1 42900
IGJhdGh0dWI= 42901
IFNubw== 42902
2ZDZkQ== 42903
IGJ1ZmZldA== 42904
IEdyaWQ= 42905
IEJyZXc= 42906
aXNldA== 42907
IEltcG9ydGFudA== 42908
w7xtw7x6 42909
IHZldG8= 42910
IFdlcms= 42911
IFNoYW0= 42912
a3Jh 42913
aWxlZW4= 42914
aGVhcmQ= 42915
IGRyYWluaW5n 42916
IGtsYXNz 42917
IGJha2F5xLFt 42918
Y3R1cmU= 42919
5L2g6Kqq 42920
YW1vdXI= 42921
IHNwb25zb3JzaGlw 42922
IGRpc3RpbGw= 42923
IHBhdGlv 42924
IGtvbWI= 42925
IG92ZXJ3aGVsbWluZ2x5 42926
IEphbWFpY2E= 42927
dWl0ZW4= 42928
TGl0dGxl 42929
IExPVA== 42930
dGHEhw== 42931
IGNvbW1hbmRlcnM= 42932
IFdhdHRz 42933
IE9wdGlvbnM= 42934
7J2066m0 42935
QUNU 42936
IGluZGlzcGVucw== 42937
IEZvcnNjaA== 42938
b3RvbQ== 42939
IM6tz4fOtc65 42940
IHByYWlzaW5n 42941
IOyYgeyDgeydhA== 42942
IGFtYW4= 42943
IGh5cG5vdA== 42944
dGhtcw== 42945
IG5hc3plag== 42946
IG1vdXJuaW5n 42947
IFNBWQ== 42948
Y3lq 42949
INCz0L7RgdGD0LTQsNGA 42950
IGNhdQ== 42951
bWVl 42952
IHRhZGk= 42953
TWVk 42954
IGNhbGlkYWQ= 42955
44Of44O8 42956
IHN0cmlwZQ== 42957
IM61zr0= 42958
IEthdHk= 42959
IEVzY2FwZQ== 42960
IOOCkw== 42961
IG3DvHNzdGU= 42962
INin2YTYpw== 42963
0LrRgg== 42964
IGpvYmJhcg== 42965
IEplanU= 42966
b3Jhcg== 42967
IFNlcsOh 42968
IE1lc3Np 42969
w6F6 42970
IFRyYW4= 42971
IHBpZXJjaW5n 42972
IGFyaXRobWV0aWM= 42973
IHN0YWdnZXJpbmc= 42974
IHBsdWdnaW5n 42975
IEtBUg== 42976
dmw= 42977
tOyY 42978
IFJlZ2llcnVuZw== 42979
IE9jenl3acWbY2ll 42980
IEVkZ2Fy 42981
IGNvbmR1Y3Rpdml0eQ== 42982
eWVsbGluZw== 42983
dmFpcw== 42984
YWRpYW4= 42985
IGJ1bGt5 42986
INGB0YDQsNCy 42987
INC/0YDQvtC8 42988
IHBhdmVk 42989
IGJlbmRz 42990
IFNraWxsc2hhcmU= 42991
IE1tbW0= 42992
IEhvcnJvcg== 42993
IHR1bWI= 42994
IGdvb2Z5 42995
IE1lb3c= 42996
15nXnNeV 42997
IFdhc3M= 42998
IFNjYWxl 42999
IFJhaw== 43000
IHByb2plY3Rpbmc= 43001
IGxpbmd1aXN0aWM= 43002
IFdvcmxkcw== 43003
ZW5zZW1ibGU= 43004
IHBlZ2E= 43005
c3RvcHBhYmxl 43006
IGltYmFsYW5jZQ== 43007
IMO4 43008
IHRocmlsbGVy 43009
0LrQvtC70YzQutGD 43010
IGxlZnRvdmVycw== 43011
IGNhdmVhdA== 43012
IFNUUg== 43013
dW5kYWk= 43014
IHdhdGVyeQ== 43015
IE1hcmlu 43016
44Oz44Kw 43017
IGVnZ3BsYW50 43018
IEpC 43019
2YXZg9mG 43020
dmlkaWE= 43021
IEZJTg== 43022
aWNhYmxl 43023
IHBvZG9i 43024
IGNvaGVzaXZl 43025
IFZlcmbDvGd1bmc= 43026
IFBsYXRv 43027
0LDRgNC40Yk= 43028
IGtvdA== 43029
INCf0L7QvA== 43030
INC00L7QutGD0Lw= 43031
IGltcGxhbnRz 43032
aXNzZXo= 43033
QnJl 43034
IGdhc3Bz 43035
IFRFRA== 43036
cmF0bw== 43037
Skk= 43038
IGF2ZW51ZXM= 43039
IENob25n 43040
bGFkxLE= 43041
2LHYtg== 43042
IGluaWNp 43043
IFN1YmFydQ== 43044
5pWF 43045
6YGK5oiy 43046
4LiL 43047
IGFjaHQ= 43048
IEFyY2hpdGVjdHVyZQ== 43049
INCy0LXRidC4 43050
IERldk9wcw== 43051
IHRvcHBpbmdz 43052
IG9ic29s 43053
YWluYQ== 43054
IEJhbmdrb2s= 43055
ZXN0cnVjdA== 43056
IGtvYg== 43057
IOuTrw== 43058
INGA0LDQt9C90YvQtQ== 43059
IHJlZQ== 43060
IGJpanZvb3JiZWVsZA== 43061
IERlbW9jcmFjeQ== 43062
4LmA4Lij4Liy 43063
INC60L7QvdGC 43064
IHNlw6c= 43065
IHJhaGF0 43066
IHBhcmxpYW1lbnRhcnk= 43067
IEJhc2g= 43068
5oqT 43069
emlhxYI= 43070
SVRDSA== 43071
IEJ1YmJsZQ== 43072
a3TDsw== 43073
V2hvYQ== 43074
IGZsYXRz 43075
5pWI 43076
em5l 43077
IHNlcnZpY2lv 43078
IERldw== 43079
1bjWgg== 43080
IHVudGVyc3TDvHR6ZW4= 43081
IFdpbmRz 43082
6YKj5Liq 43083
IOyWmOuKlA== 43084
IGV2YWx1YXRpb25z 43085
IHJlY2E= 43086
IGVsdmVz 43087
Y2hlZXI= 43088
IGphbA== 43089
IHJlc3RlZA== 43090
IHF1aWVuZXM= 43091
IEJyb29rZQ== 43092
IOuniOydjOyXkA== 43093
IGludGVu 43094
IG9hdHM= 43095
IHJlZmVyZWU= 43096
IHBuZXVtb25pYQ== 43097
IGRlbHZl 43098
cGVhY2U= 43099
ZW55 43100
IG1vc3RyYQ== 43101
IENhbm5vbg== 43102
z4HOv8+N 43103
INCQ0Ls= 43104
IG1vbnVtZW50YWw= 43105
zr/Pjc68zrU= 43106
aW1tZXJz 43107
YXZpYW4= 43108
INC00LXQu9Cw0LXRgg== 43109
IHBpdGNoZXM= 43110
IEdyb3Zl 43111
IHNlbWluYXJz 43112
IHLDqWN1cA== 43113
IFZvb3I= 43114
IGRldmVu 43115
IGRC 43116
IGJvb3N0aW5n 43117
ZWdhbg== 43118
IHdlbHQ= 43119
IEd1YXRlbWFsYQ== 43120
IG1pbGVhZ2U= 43121
IGJlaGFuZA== 43122
IFdhYXI= 43123
IFN1cmY= 43124
IGNhdWxpZmxvd2Vy 43125
IFR5cg== 43126
IG1pdGVpbmFuZGVy 43127
IGRhcmluZw== 43128
IFNpdHRpbmc= 43129
ZGxlZA== 43130
IHJlc2VudG1lbnQ= 43131
bcOkw59pZw== 43132
IGZpbG1tYWtpbmc= 43133
d2FydHM= 43134
dGhvdWdodA== 43135
b2xvZ2lxdWU= 43136
IENPUg== 43137
IGFjY291bnRlZA== 43138
IGFwZXI= 43139
IElOVA== 43140
b2xhcmU= 43141
IGFjb21wYcOx 43142
6K2Y 43143
IMahaQ== 43144
5Lmd 43145
IG1lcm1haWQ= 43146
IEJlbnRsZXk= 43147
YXRvcmU= 43148
IHByZW4= 43149
IGV0aGFub2w= 43150
IGFzdHJvbm9tZXJz 43151
c2VhdA== 43152
a2VlcGVycw== 43153
IGV4ZW1wdGlvbg== 43154
IGFtbw== 43155
IOuCmOyEnA== 43156
IGluaGFs 43157
IGJvd3M= 43158
0YHQutGD0Y4= 43159
MzAwMA== 43160
IGZlcm1lbnRhdGlvbg== 43161
IHNpbmtz 43162
IGNvbWVyY2lhbA== 43163
IHN0dW1w 43164
IGNlbGU= 43165
IFNpc3RlcnM= 43166
IFJlZ2lzdGVy 43167
IHNvb3J0 43168
IG5hdG9taWFzdA== 43169
IOq3uOumvA== 43170
IMWeZXk= 43171
IGh5cGVk 43172
IFJhZmFlbA== 43173
IEVpcw== 43174
IEJhc2ls 43175
IEFzc2Fzc2lu 43176
IEFkZQ== 43177
csOlbg== 43178
IG9ubGFy 43179
IG1vdmltaWVudG8= 43180
IGFkZGl0aW9uYWxseQ== 43181
IHNsaXQ= 43182
IENocnk= 43183
IEludGVydmlld2Vy 43184
15zXpw== 43185
IGRpc2w= 43186
IGxpZ2dlcg== 43187
0YPQutC4 43188
YmVyaXNo 43189
INGA0Y/QtNC+0Lw= 43190
QVJPTg== 43191
XSws 43192
IGx1bWnDqHJl 43193
IG9sdmlk 43194
IGZyZXVl 43195
IFRpbmc= 43196
IEvDtg== 43197
IGdlbw== 43198
IGR5ZWQ= 43199
44Gn44GN 43200
0YjQtdC5 43201
IMW8eWNpZQ== 43202
IGll 43203
IHRheHBheWVy 43204
IHBlxYI= 43205
IGTDqWNpZMOp 43206
IGPFk3Vy 43207
IGVudHdpY2tlbHQ= 43208
IEhR 43209
S0s= 43210
b2Rhcg== 43211
IGhvbmU= 43212
IGNvbmZpYW5jZQ== 43213
IGlzc3Vpbmc= 43214
IGRpYWdub3N0 43215
IOyehA== 43216
INC60YDRg9GC 43217
INC60LDRgQ== 43218
IMO+ 43219
IHJlc3RyaWN0aXZl 43220
IENhc3Rybw== 43221
IHXEnw== 43222
IGVtcHJl 43223
IE1vbw== 43224
IEZpZ3VyZQ== 43225
cGhvbmV0aWM= 43226
UHJvZg== 43227
INC/0YDQtQ== 43228
IHRpbHRlZA== 43229
IE5lZ2F0aXZl 43230
IExpbWl0ZWQ= 43231
bWVubw== 43232
bGFtYXRpb24= 43233
IHRydXN0ZWVz 43234
IGludGVuc2VseQ== 43235
IGHDp8SxbA== 43236
IFVzZWQ= 43237
IHp1bA== 43238
IGFwcHJlY2lhdGl2ZQ== 43239
IHRpbmM= 43240
IGNvbnF1ZXN0 43241
INi52YbYrw== 43242
IHN1aWNpZGFs 43243
IG11bGhlcmVz 43244
IGRldGFjaA== 43245
IGthbWVyYQ== 43246
IEFpclBvZHM= 43247
SU5ESVNUSU5DVA== 43248
0LPQu9C40Lk= 43249
IOuDhA== 43250
IHdyZXN0bGU= 43251
5rSX 43252
IGZpcmVhcm0= 43253
IGxpcmU= 43254
cHJh 43255
IGpld2Vscw== 43256
IENvcm5lbGw= 43257
IO2VoOqyjOyalA== 43258
IHN1Y2tlcg== 43259
IG5vbWJyZXV4 43260
IEZlcm0= 43261
7JuQ7J20 43262
IFBpcw== 43263
INC40LfRg9GH 43264
IG1pdGVu 43265
IGNldg== 43266
IFVSTHM= 43267
IENBUw== 43268
IOWPr+S7pQ== 43269
ZmluZGVu 43270
IGJyYXZlcnk= 43271
INGB0LvQvtCy0L4= 43272
IG5lbmh1bWE= 43273
IGVuY3VlbnRyYQ== 43274
IFNoaXJsZXk= 43275
IHBlcmNlcHQ= 43276
ZnJhbWVz 43277
IFJvdmVy 43278
IEFsYmVydGE= 43279
b2Nj 43280
IOudvOqzoA== 43281
IHPDunBlcg== 43282
IHByZXN1bWU= 43283
IGdsYW5k 43284
IHBhY2luZw== 43285
IG5ldXJvdA== 43286
IHNubw== 43287
IHBsb3R0ZWQ= 43288
IHBhxYRzdHdh 43289
IE93bmVy 43290
IERlZmVuY2U= 43291
cmlkZ2Vz 43292
IHdhbGxwYXBlcg== 43293
b25pYW4= 43294
QnJv 43295
IEFyaWFuYQ== 43296
55u05o6l 43297
a3J5 43298
IG5hcnJhdGlvbg== 43299
IGNyaWFuw6dh 43300
IEFscmlnaHR5 43301
IOydvQ== 43302
IOyTsOqzoA== 43303
IGxpYmVyYXRlZA== 43304
IGV4Y2VlZHM= 43305
IGRvbWluYXRpbmc= 43306
IGJha8Sxbg== 43307
bGs= 43308
IHNsYXBwZWQ= 43309
0JfQtA== 43310
dW1lbnRhbA== 43311
Z2V0dGFibGU= 43312
IFJveg== 43313
IEd1bA== 43314
b3V2ZXJ0 43315
IHNtYXNoaW5n 43316
YXp1amU= 43317
U2ly 43318
IGdyYXRlZA== 43319
5L2g5pyJ 43320
QVRU 43321
IGFydGljdWxhdGVk 43322
IHN0b3Jh 43323
IGV4dHJhdGVy 43324
4buJ 43325
z4PPiQ== 43326
d2ly 43327
IE1ldGU= 43328
SW1w 43329
IGhvb3I= 43330
cGhhc2U= 43331
INGH0YPQtA== 43332
INCx0YDQsNGC 43333
IGlkYWc= 43334
IGNpbnE= 43335
IGFwYXJlY2Vy 43336
IElDRQ== 43337
5YiX 43338
IHF1aWV0ZXI= 43339
IGZhbHNjaA== 43340
YWRpYw== 43341
INC/0LvRjtGB 43342
IE1lbnU= 43343
dXhl 43344
IFTDtGk= 43345
IE1JTA== 43346
IEhhag== 43347
dmVyYnM= 43348
IHR1YmluZw== 43349
IG1hY2hzdA== 43350
IGRhbGw= 43351
VGVy 43352
IGdlbGVu 43353
IGN1Y3VtYmVycw== 43354
IHdpZGdldHM= 43355
IGRldnJhaXQ= 43356
IG1pa2U= 43357
IGludHJh 43358
7ZWt 43359
IMOF 43360
IEh1bmQ= 43361
5qeL 43362
cXVhcnRlcg== 43363
IGV3 43364
IGtlbHVhcg== 43365
IG1hdHM= 43366
IFRyaWNr 43367
IEluZmluaXRl 43368
nqg= 43369
IHBlYWM= 43370
IFByb3Rl 43371
4KWI 43372
IDE3MDA= 43373
IFJhaXM= 43374
4LmK 43375
w6RobHQ= 43376
aWZpY2E= 43377
YWltZXI= 43378
YcSH 43379
IGFrbA== 43380
IFZvbHZv 43381
IFR5c29u 43382
IFJvbmc= 43383
aXJzaW4= 43384
IOKZpQ== 43385
IHBhcm9keQ== 43386
bmF0aW9uYWw= 43387
cG9k 43388
YXlk 43389
YW1ibGVk 43390
IGdvdmVybm1lbnRhbA== 43391
IGNvbmZvcnQ= 43392
aWNpZGVz 43393
IG5hc3pl 43394
IFNoZXBoZXJk 43395
IEtvbnRha3Q= 43396
IGRpc3Byb3BvcnRpb25hdGVseQ== 43397
INC60LvRjtGH 43398
IHTDrXR1bG8= 43399
IHNpbmE= 43400
IGNvbXBvc2l0aW9ucw== 43401
IFBG 43402
IHZlcmts 43403
IHN1aXZyZQ== 43404
IGFzdGE= 43405
IHN0YWtlaG9sZGVy 43406
IHNhbW1h 43407
IEJMQUNL 43408
IG5vZGln 43409
IGxldmE= 43410
IGp1ZWdvcw== 43411
IGVybnN0 43412
IGJvdHRvbXM= 43413
IFNpZ25hbA== 43414
IHBvbGx1dA== 43415
IGR1cmE= 43416
TXVzaWs= 43417
INC60L7QvNC90LA= 43418
INCy0YHQtdC5 43419
YWx0ZXI= 43420
IFN0ZWY= 43421
IEJpZ1F1ZXJ5 43422
IFZlcmFudHdvcnR1bmc= 43423
IOuLueyXsA== 43424
IHF1aXp6 43425
IExldHRlcg== 43426
IEludmVzdG1lbnQ= 43427
0YjRgg== 43428
kOuNsA== 43429
IGVuY29kaW5n 43430
IHTDpG5rZXI= 43431
IEt3 43432
YW5uaWU= 43433
5Yud 43434
MTEw 43435
IHp3eQ== 43436
IOynpw== 43437
IGRhdw== 43438
ZXN0w6Q= 43439
IGRlY2VpdmU= 43440
IEzDpG5kZXI= 43441
aXNrbw== 43442
IHBvZHN0YXc= 43443
IFBoYXJhb2g= 43444
7LOk 43445
6ZmQ 43446
w7psdA== 43447
IHR5w7Y= 43448
IG11c2lteQ== 43449
6LOq 43450
IHBj 43451
IE5U 43452
IENvc3Rjbw== 43453
IOWwjw== 43454
IM+Dzr/PhQ== 43455
IHVuaW4= 43456
cm91bmRz 43457
IHJlbWluZGVycw== 43458
IHB1aXNxdQ== 43459
IGtyaWpnZW4= 43460
IHdvcmtmbG93cw== 43461
bmV0ZW4= 43462
IOuQmOyngA== 43463
IHNsZWVr 43464
IGNvd29ya2Vycw== 43465
YW1pZW50b3M= 43466
IHdpdGNoZXM= 43467
YmFhcg== 43468
ZXRpZXM= 43469
IHVubmF0dXJhbA== 43470
IFNpY2s= 43471
IEVmZW5kaQ== 43472
44Oz44OA44Ob 43473
amNpZQ== 43474
IGNoYW1hZG8= 43475
7JiA7Iq164uI64uk 43476
IHByemVkc2nEmWJpb3I= 43477
IGJvb2tzdG9yZQ== 43478
IOyeoOq5kA== 43479
IFNlcGFy 43480
YW5naQ== 43481
RXZldA== 43482
IGVtZXJnZW5jaWVz 43483
IFhNTA== 43484
0L3QtA== 43485
pbTrqbQ= 43486
IOq/iA== 43487
IOuTpOqzoA== 43488
IHN1dA== 43489
IFdpeg== 43490
5bGV 43491
IGR5bmFtaWNhbGx5 43492
b3BlcmF0aW9u 43493
ZG90 43494
IGluZWZmaWNpZW50 43495
Y2xlYXJz 43496
IG11bmRhbmU= 43497
IFZlcm9uaWNh 43498
6Iy2 43499
2LHYqg== 43500
cG9zZQ== 43501
cGFp 43502
IG55bG9u 43503
IGF1bWVudGFy 43504
IGFsbHRzw6U= 43505
dmFr 43506
IGNhcGFjaWRhZA== 43507
IFdyZXN0bGluZw== 43508
IGZlcnRpbGU= 43509
IG3DqWc= 43510
IE5hbm8= 43511
0LDRgtC10LvQuA== 43512
IOyWtOyp 43513
IHRvY2E= 43514
IEVn 43515
4oE= 43516
IOyz 43517
bHVlbnQ= 43518
IHNvbGVt 43519
IGNpbmVtYXQ= 43520
IFF1ZWw= 43521
IG9yYml0cw== 43522
IEhhcm0= 43523
cmljYW5lcw== 43524
IGJsdXJyZWQ= 43525
5aaC5L2V 43526
INin2YTYsNmK 43527
IGppbg== 43528
IGdyZW5hZGVz 43529
IGF0cm9j 43530
IHdoZXJlaW4= 43531
IHJlcGxlbg== 43532
IENvbWljcw== 43533
ZWRhYW4= 43534
IGRlbmlt 43535
IGVtYmFycmFzc21lbnQ= 43536
IEdvbWV6 43537
IEJ1c2Fu 43538
aXZpdGllcw== 43539
IHNhbGl2YQ== 43540
IG1lcms= 43541
IGlsZ2lsaQ== 43542
INC60YDRg9Cz 43543
IG9jY3VwYXRpb25hbA== 43544
IFNhaGli 43545
U3Rh 43546
IGFkdmlzZXI= 43547
IFRydWx5 43548
IFlFQUg= 43549
IOyeiOuKlOuNsOyalA== 43550
emV3 43551
YmFyZW4= 43552
IHN0b2w= 43553
IGJlbG9uZ2luZ3M= 43554
IFJlc2VhcmNoZXJz 43555
IGVmZW5kaW0= 43556
z4XPhw== 43557
xYLEhWN6 43558
IFVuZw== 43559
IEp1Yg== 43560
IGNlcmVicmFs 43561
4buHdQ== 43562
INem16g= 43563
INC/0L7QtNCw0YA= 43564
IG1hcmNoZWQ= 43565
IGF3YWtlbg== 43566
IGFrbw== 43567
IGFjZXB0 43568
IGluaXRpYXRpb24= 43569
6K+J 43570
bG90 43571
IHfFgmFz 43572
IE1vbmdvbA== 43573
dXRyYWw= 43574
IHRlbnRhbmc= 43575
IGludmVyc2lvbg== 43576
IOydtO2bhA== 43577
IGxvaw== 43578
xYJieW0= 43579
UlM= 43580
IHN0b3M= 43581
IGludGVyYWN0cw== 43582
IENhbGVuZGFy 43583
IHZhbmlzaA== 43584
IHBoeXNpb2xvZ3k= 43585
IGxpbmVhcmx5 43586
IEpZ 43587
xJ9hbg== 43588
ZnVuZGVk 43589
aXppZXJ0 43590
IHptaWFu 43591
IEdyaWxs 43592
IHVuYmVsaWV2YWJseQ== 43593
b3RlY2hub2xvZ3k= 43594
IENhcnM= 43595
INmG24E= 43596
IEZvbGdl 43597
IEJldmVybHk= 43598
w6Rpc2NoZW4= 43599
IGF1bWVudG8= 43600
7JuM7ISc 43601
IG1haWxib3g= 43602
IHN0ZWVkcw== 43603
IFBlYWs= 43604
5ben 43605
IHd5a29y 43606
IHByYXdkYQ== 43607
0LjRgtGL 43608
IGRpc2NvdXJz 43609
IGFjY3VzZQ== 43610
Y2Vzc28= 43611
dWlyZQ== 43612
INC/0L7Qv9Cw0LQ= 43613
IHRoYQ== 43614
IG1lYXN1cmFibGU= 43615
YmVlcGluZw== 43616
IElubmVu 43617
INC/0Y/RgtGM 43618
IGNvbXBldGVk 43619
IEl0YWxpYW5z 43620
IGVuY29udHJh 43621
IG5pZXc= 43622
IGZpbHRyYXRpb24= 43623
INC/0YDQvtGE0LXRgdGB 43624
IHBhamFtYXM= 43625
IGNpbGFudHJv 43626
IFNvYw== 43627
THVj 43628
IOq5gOs= 43629
IE9kZA== 43630
IGh5ZHJhdGlvbg== 43631
0LzQvtCy 43632
IHBseXdvb2Q= 43633
IENvbXBldGl0aW9u 43634
0LjQt9C90LXRgQ== 43635
ZmxpZ2h0 43636
IEJlaXQ= 43637
Ym91cmc= 43638
IGNvaWxz 43639
IGPDom1lcmE= 43640
IGFtZW5kZWQ= 43641
xIFt 43642
QW5nZWw= 43643
IFN0YWN5 43644
Zmxv 43645
IG5vcm1hbGU= 43646
IGNvbnNvbmFudA== 43647
IGFjY29tcGFueWluZw== 43648
0LrRlg== 43649
IGlycml0YXRlZA== 43650
IGbDpXR0 43651
IGNyb2NvZGlsZQ== 43652
kJjripQ= 43653
IGFsYmVpdA== 43654
IFBoaWxvc29waHk= 43655
57Sv 43656
xYY= 43657
eXRpYw== 43658
IHLDqGc= 43659
IGZyYW7Dp2E= 43660
IGF0dGVudGl2ZQ== 43661
SGFt 43662
IGFscmVkZWRvcg== 43663
5p2/ 43664
c2Vp 43665
INGB0LLQuNC0 43666
IGdpbWJhbA== 43667
IGNoaW5h 43668
IPCfjrY= 43669
INCS0LDQvA== 43670
IHN0aW11bGF0aW5n 43671
IE9yYQ== 43672
eXRlcw== 43673
IGhlZnQ= 43674
IGhhdGVycw== 43675
IGNvbXBsZXhlcw== 43676
IDAz 43677
csOzZA== 43678
Y2xlYXI= 43679
IGJlc3RlaHQ= 43680
55WZ6KiA 43681
d255 43682
bW9pbA== 43683
IHNsb3BweQ== 43684
IGluc2lnbmlmaWNhbnQ= 43685
IGR1YmJlZA== 43686
IOuWoA== 43687
IGNvbnNpZ28= 43688
0LvRg9GI0LDQuQ== 43689
U24= 43690
INeU16Y= 43691
IM6M 43692
IG5hZHppZQ== 43693
IGZyZXNobWVu 43694
dGFh 43695
IHV3YWfEmQ== 43696
IEZhdm9yaXRl 43697
IENyaW1pbmFs 43698
IGV2aWRlbg== 43699
IHN5bWI= 43700
TGVz 43701
IEJlYXU= 43702
dW5lZA== 43703
cGxlbWVudA== 43704
QWM= 43705
IGRlcm1hdA== 43706
IE5vbGFu 43707
0YvQvw== 43708
IHNpdHQ= 43709
IGV2ZXJsYXN0aW5n 43710
IGVzdGF2YW0= 43711
INC80LjQug== 43712
IGtow6Fj 43713
IGludml0 43714
IHRyZWJsZQ== 43715
IGppZw== 43716
bWFuaQ== 43717
IHR1dm8= 43718
IFJVUw== 43719
IEVyZGU= 43720
IER6acSZa3VqxJk= 43721
IGJsdWViZXJyaWVz 43722
a2VsbA== 43723
YWNpb25z 43724
54i3 43725
0LLQuA== 43726
TEVU 43727
IHNwcm91dA== 43728
IHNwb3I= 43729
IGLDqm4= 43730
IE1vbmE= 43731
IENvbnRhaW4= 43732
IEtleXM= 43733
0L7Qt9GP 43734
IGZ1bmNpw7Nu 43735
IHJhcHBlbGxl 43736
IGV2b2x2ZXM= 43737
IHNjcmFwaW5n 43738
IGNvbWVudMOhcmlvcw== 43739
IHByYXRpcXVl 43740
IGF1eGlsaWFyeQ== 43741
IFNwb25nZQ== 43742
0YHQutC40Lw= 43743
dXZv 43744
INGB0LDQvNC+ 43745
IHNhbms= 43746
IGhpZ2h3YXlz 43747
IGludmVudGlvbnM= 43748
INC40L3QvtCz0LTQsA== 43749
IGNyZWF0aXZlbHk= 43750
IGJlbmNobWFya3M= 43751
b25jw6k= 43752
YWxhbA== 43753
IHNvdHRv 43754
IGNhbHZlcw== 43755
IE1vdg== 43756
IGxhdmVuZGVy 43757
IGV5ZWJhbGxz 43758
IGF3YWl0aW5n 43759
IFBhdHk= 43760
2YTZhw== 43761
IGVtYnJvaWRlcnk= 43762
IGR1aA== 43763
IGNhbWFy 43764
IEJPQg== 43765
IHNwYWNlZA== 43766
IGfFgm9z 43767
0LDQtdC80YHRjw== 43768
IGVzY2FwZXM= 43769
IFJvZ3Vl 43770
emN6 43771
6J4= 43772
rOulvA== 43773
IE1vxbxl 43774
INC10YHRgtC1 43775
IEJ1cmFkYQ== 43776
6Yyy 43777
d2Q= 43778
dXV1dQ== 43779
IHNhc2g= 43780
IEx1Yg== 43781
IG5vdGVib29rcw== 43782
IG1hZQ== 43783
IGNvbmZsaWN0aW5n 43784
IHN1bW1lcnRpbWU= 43785
YWNhcw== 43786
IGJhdWVu 43787
Ymxvd2luZw== 43788
4bqhbw== 43789
IOyWuOygnA== 43790
5LuK5pel44Gv 43791
IFNlbmhvcg== 43792
IGlQaG9uZXM= 43793
IFF1YXJ0ZXI= 43794
IOygnOuMgOuhnA== 43795
dcOf 43796
IOuniOustOs= 43797
IHNldHRsZXJz 43798
IGNyZXN0 43799
IHRyYW5zYw== 43800
5pu+ 43801
IHJpb3Rz 43802
IGNsb25lcw== 43803
IE9wcmFo 43804
zq/Otg== 43805
IHBhbHM= 43806
Li4uLi4uLg== 43807
44GU44GW44GE44G+44GZ 43808
INGA0L7RgdGB 43809
IExhc2Vy 43810
IHphY3p5 43811
IHNldmk= 43812
IHJlZ2VuZXJhdGlvbg== 43813
7Je8 43814
d291bGQ= 43815
IMO8emVyaW5l 43816
IFN0cmHDn2U= 43817
IHZlbmdlYW5jZQ== 43818
IHJlcg== 43819
IFNhZmFyaQ== 43820
IEhFWQ== 43821
55Wr 43822
IHNhY2Fy 43823
IGltYWdlbQ== 43824
IEJ1bmRlc3Q= 43825
bWVzYW4= 43826
IFBhc3Rl 43827
IHNpeno= 43828
INC/0L7RgdGC0YPQvw== 43829
15TXlQ== 43830
dHJhZA== 43831
IGZyYW7Dp2Fpc2U= 43832
IEJvdQ== 43833
IGJhcnJl 43834
IFpoaQ== 43835
IEdlZXo= 43836
aWhhZA== 43837
IHJlY29ub2M= 43838
IHBlbGln 43839
IGluZGljZXM= 43840
IOuwlOuA 43841
IGNvbmR1Y3Rpb24= 43842
IOyVhQ== 43843
IHpla2Vy 43844
IGZ1bQ== 43845
IFfDvHI= 43846
YnJlYWtlcg== 43847
IHNwcml0ZQ== 43848
Q3Jvd2Q= 43849
IG9wZW5lcg== 43850
IG9sdg== 43851
IGJ1ZW5hcw== 43852
IFNpbGs= 43853
IEhJTQ== 43854
a29w 43855
Y29tcGw= 43856
IHBvc3Nvbm8= 43857
s4A= 43858
IG9zY2lsbGF0b3I= 43859
IFNpdGg= 43860
6IOh 43861
0LDQttC4 43862
IHJhZnQ= 43863
aGFsbA== 43864
IHNjaG5lbGxlcg== 43865
IGltcG9ydGluZw== 43866
IGFzc2VtYmxpbmc= 43867
IHViaXF1 43868
IGFjdGl2YXRlcw== 43869
YWNjaQ== 43870
k5zrpbw= 43871
IGNvbXBvc2Vycw== 43872
IEFDTA== 43873
Q29uZg== 43874
IOy9mA== 43875
INC90LXQutC+0YLQvtGA0YvQtQ== 43876
IGNhbmRpZXM= 43877
5Yqg5YWl 43878
IE11c3M= 43879
4LmD4LiK 43880
IGR1ZGE= 43881
0L3QuNC60L7QvA== 43882
bWVkZW4= 43883
IOyWtOuVjA== 43884
IFllc2h1YQ== 43885
emFn 43886
aG9kb3U= 43887
IGFsb3Vk 43888
IFBhbG1lcg== 43889
aW1pemU= 43890
44K344On 43891
IG1hcml0aW1l 43892
IGNvbW11bmFs 43893
IGJhZGdlcw== 43894
IHJ1Z2J5 43895
IG1hcnNobWFsbG93 43896
IGZpZXJ5 43897
IGFjY291bnRhbnQ= 43898
IGFibGE= 43899
IE1vbnJvZQ== 43900
IEZvbnQ= 43901
IEJvb3N0 43902
IEJhcm5lcw== 43903
YW5zd2Vy 43904
IEJ1cm5pbmc= 43905
IOS4jeaYrw== 43906
IGFuZ2Vm 43907
IFdlc2xleQ== 43908
bGxz 43909
7LU= 43910
16nXnA== 43911
aWxpxZtteQ== 43912
15DXnw== 43913
YW11cmE= 43914
IEZ1ag== 43915
IHBhbmk= 43916
IFRyb3A= 43917
YXJiZWl0ZW4= 43918
IHJ1ZQ== 43919
IFJhcmU= 43920
w6RuZ2Vu 43921
INGB0LzQvtGC0YDQtdGC0Yw= 43922
INCa0LDRgA== 43923
IE1UVg== 43924
Ym9hcmRpbmc= 43925
XVs= 43926
IOugiOs= 43927
c3RhbmJ1bA== 43928
cGllbHQ= 43929
IEhhcmR5 43930
IEVuZ2FnZW1lbnQ= 43931
IERpZW5zdA== 43932
IHfDpHJlbg== 43933
IGZ1ZWdv 43934
IGVzdHJ1Y3Q= 43935
IGNhbGFt 43936
IFJlc3BvbnNl 43937
IOOChA== 43938
IE1vaGFtbWFk 43939
IHJlc2lzdGluZw== 43940
IGR1cmFudA== 43941
6IGv 43942
5Ya1 43943
IE9MRUQ= 43944
IHZlcno= 43945
bcOkbg== 43946
INmG25I= 43947
IHBhcmFub2lk 43948
IEF3YXJl 43949
IEVuZ2luZWVycw== 43950
IHByb2NlZHVyYWw= 43951
IHBlcnNvbm5hZ2U= 43952
IGZhcmtsxLE= 43953
6aGG 43954
Zmxvd2luZw== 43955
INC80LXRgdGC0LA= 43956
IEJhcmU= 43957
aXN0ZW0= 43958
IHBvY3rEhXRrdQ== 43959
IHBlcnNvbmFqZXM= 43960
IOyWtOugtQ== 43961
rYk= 43962
INCl0L7RgtGP 43963
IHVuc2V0dA== 43964
IEFic29s 43965
IOG6pXk= 43966
IE1BWU9S 43967
0L/QvtC70L3QtQ== 43968
IGluZm9ybWluZw== 43969
IGFtcHM= 43970
0J/RgA== 43971
IOutlA== 43972
YWVkYQ== 43973
INeU15HX 43974
4bqlbg== 43975
a2VsaWpr 43976
IGF0aGVpc3Q= 43977
IHRyb3V0 43978
IG5ldWVz 43979
IE5va2lh 43980
bWFjaGVu 43981
IHdob2xlc2FsZQ== 43982
xLFyZA== 43983
SW5z 43984
INGN0L8= 43985
IHByaWNr 43986
IEtpbmRlcm4= 43987
4LiX4Liz 43988
IGNsYXNzeQ== 43989
IMOubnQ= 43990
IFNob3BpZnk= 43991
INGB0L7RgA== 43992
INC30LDQutGA0Ys= 43993
enVr 43994
IHVuaXZlcnNhbGx5 43995
IHRlYXNwb29ucw== 43996
IHJlY291bnQ= 43997
IG7DpWdvbnRpbmc= 43998
IFh1ZQ== 43999
aXNpw6htZQ== 44000
IHdlYWtlc3Q= 44001
IHRlxZ9la2vDvHI= 44002
IG1hdGhlbWF0aWNhbGx5 44003
IEhvcw== 44004
IO2VnOuLpA== 44005
IHBhcnRhZ2Vy 44006
IERhcnI= 44007
6ro= 44008
IM61zro= 44009
IGdlcm1z 44010
IGdlbGly 44011
IGR1bA== 44012
LC0= 44013
IOyWuOs= 44014
INee16Y= 44015
INGP0YA= 44016
IHF1b3RpZA== 44017
IHByenlzeg== 44018
IGhhcmRuZXNz 44019
IGFxdWF0aWM= 44020
IEp1bmdsZQ== 44021
IFBDUg== 44022
IEVsaW90 44023
IG9zdHI= 44024
IG1hcGE= 44025
ZXNzw6Q= 44026
IEdJUg== 44027
IERyaXZpbmc= 44028
IFNhbWk= 44029
IE1lZGllbg== 44030
IENvbXBhbmllcw== 44031
IFBoYXJt 44032
c2VpdHM= 44033
IFJpbQ== 44034
IM6/z4DOvw== 44035
IHdlaXRlcmVu 44036
IHBpenphcw== 44037
IEx5ZGlh 44038
IEhlaWdodHM= 44039
IHNpbmNlcml0eQ== 44040
IG5vc3Nhcw== 44041
IGTFgg== 44042
IGFsYXJtaW5n 44043
IENhdWM= 44044
INGB0LzRi9GB 44045
ZmFjaW5n 44046
YmFncw== 44047
V1c= 44048
INi02Yo= 44049
IGNvdXJ0cm9vbQ== 44050
IFBoaWxsaXA= 44051
IOqyg+yymOufvA== 44052
IFNwaWVsZXI= 44053
44KP44GL 44054
IGthbnQ= 44055
IGFkbWl0dGluZw== 44056
44OB44Oj44Oz44ON44Or 44057
IGNvbnRhaW5tZW50 44058
5byg 44059
IHJlbW92YWJsZQ== 44060
IGp1bXBlcg== 44061
Zm9jdXNlZA== 44062
INC40YLQvtCz0LU= 44063
INCi0LXQvA== 44064
IHZhc2U= 44065
IFVTQw== 44066
IE1vbmF0ZQ== 44067
IEphY29icw== 44068
IEhPTA== 44069
aWtlZA== 44070
ZXJ3ZWlzZQ== 44071
IGdvb2RpZXM= 44072
IGhvbWFnZQ== 44073
15vXqdeZ15U= 44074
IHF1YWlz 44075
IGluaWNpYWw= 44076
IGd1YXJkaW5n 44077
IGRheno= 44078
IGNvbWJvcw== 44079
INGD0L/RgNCw0LI= 44080
IFRhbGVudA== 44081
5aWH5oCq 44082
IMOzcg== 44083
IGludGVybWl0dGVudA== 44084
IE1jQ2FydGh5 44085
IHNwYW5z 44086
IHR5cmU= 44087
IHF1eQ== 44088
6IiI 44089
anV0 44090
IFplbnQ= 44091
IGdhdA== 44092
5aSn5ZOl 44093
IHNjYWZmb2xk 44094
IG5lY2VzYXJpbw== 44095
IFphaGxlbg== 44096
IFNBTkQ= 44097
IFBV 44098
RXZlcnl0aGluZw== 44099
LS0tLS0tLS0tLS0tLS0tLQ== 44100
INCy0LfRj9GC0Yw= 44101
IHNwYXJrcw== 44102
IHBlbmR1bHVt 44103
157Xnw== 44104
IOyDieq5 44105
IG11bHRpcGxpZXI= 44106
INC70LDQtNC90L4= 44107
dXJhdA== 44108
IHVwc2V0dGluZw== 44109
6KGA 44110
YmFr 44111
IOy1nOuMgA== 44112
IGFuw6Fs 44113
IEpPRQ== 44114
IGtvc3Rlbg== 44115
IFBhdHR5 44116
IEd1aW4= 44117
Y2tlZA== 44118
IEVneXB0aWFucw== 44119
IENpdGl6ZW5z 44120
16jXmw== 44121
INCV0YnQtQ== 44122
INC50L7Qs9C+ 44123
IHNub3dmbA== 44124
IGxla2tlcg== 44125
IGFjb3N0 44126
IEJhYmU= 44127
IGdhbWJsZQ== 44128
IGFkamVjdGl2ZQ== 44129
0LrQuNC80Lg= 44130
b3lz 44131
IG1vbnRyZQ== 44132
IEh5dW5kYWk= 44133
IG1vaXN0dXJpemluZw== 44134
IG1venphcmVsbGE= 44135
T09P 44136
IGZhY3VsdA== 44137
IGRvZXQ= 44138
IGZlYXJsZXNz 44139
IGVzcHJlc3Nv 44140
IGFsbG9yYQ== 44141
IENpbmM= 44142
44O844K4 44143
IGNvbnRlw7pkbw== 44144
IFBlbG9zaQ== 44145
IG1pbmRlcg== 44146
cm9vdA== 44147
IO2VoOs= 44148
INC/0LDQtA== 44149
IENhbGxpbmc= 44150
IENvbmZpZw== 44151
IENvbnNvbGU= 44152
aW5za3k= 44153
w6luZXJnaWU= 44154
IHNvbGl0YXJ5 44155
0L7QtNC1 44156
IGd1YXJkZWQ= 44157
MTYw 44158
INC/0YHQuNGF 44159
IFNoYXA= 44160
IHRpdHJl 44161
b2xvZ25l 44162
INC/0LDRgNGD 44163
IFBSRQ== 44164
44O844OJ 44165
IGxu 44166
IE1pdGds 44167
IENhcnJ5 44168
IHNwaW5k 44169
IENhbnRvbg== 44170
IGtpbmdkb21z 44171
cmVtbw== 44172
IHJhZ2luZw== 44173
IGluY2FwYWJsZQ== 44174
IFdS 44175
5YaN6KeB 44176
INGB0L7QsdGB0YLQstC10L0= 44177
INC60LDQutC40YU= 44178
IFNIRQ== 44179
64u57Z6I 44180
IHNjYXJjaXR5 44181
IHBlcmRl 44182
IGV4aXRz 44183
IFNpbmdlcg== 44184
IHN1cHBlcg== 44185
IG11bmljaXBhbGl0eQ== 44186
IERpdmVyc2l0eQ== 44187
IHRpcm8= 44188
aWVscw== 44189
IGzDrWRlcg== 44190
IGJsdWZm 44191
IGF0cmE= 44192
bHlz 44193
IG1haGQ= 44194
IGPDs2RpZ28= 44195
IEhhcmxlbQ== 44196
cnVsZQ== 44197
aWNpdHk= 44198
IHNpbXBsaXN0aWM= 44199
IEtvbnN0 44200
5YGl 44201
RUxMSQ== 44202
IGbDtnJzdGE= 44203
IGNvbnN0aXR1dGVz 44204
INGB0YLQvtGA0L7QvdGD 44205
IHVyZ2Vk 44206
IFBhbmRh 44207
7LCo6w== 44208
cmVjZQ== 44209
IHBhdHJpb3Q= 44210
IENydXNo 44211
IHdpbms= 44212
0L7QudGC0Lg= 44213
dXJhbsOnYQ== 44214
IHNlaXp1cmVz 44215
IGVsZWN0cm9k 44216
IERvbmtleQ== 44217
IElV 44218
IE1PUw== 44219
IGFsa2Fs 44220
7LSJ 44221
YmVzb25kZXJl 44222
IHBhcmFsbGVscw== 44223
IGJpdHRlcm5lc3M= 44224
w6R0dHJl 44225
ZXNzaW9uYWw= 44226
IHNveWJlYW4= 44227
IGNvbGxhYg== 44228
IFJlcG9ydGluZw== 44229
5aeU 44230
INC60L7QvNC/0LDQvdC40Lg= 44231
IHdzenlzY3k= 44232
IENydW5jaA== 44233
aXNlZW4= 44234
IGFtYmFzc2Fkb3Jz 44235
IENoZXY= 44236
5Y2I 44237
0L7QstGL0LU= 44238
c2Nh 44239
INGA0LXRiNC40Ls= 44240
0L7RgtC+ 44241
IGdsZWljaHplaXRpZw== 44242
bWVybg== 44243
w7xzdA== 44244
IEhhZQ== 44245
s7TqsqDsirXri4jri6Q= 44246
IHNob3Jlcw== 44247
IGRlcHJlc3M= 44248
IGFob3I= 44249
IFN0ZXVlcg== 44250
YWho 44251
IHJldmlzZQ== 44252
INGB0LDQvNGL0LU= 44253
amF0 44254
IGhlcmJhbA== 44255
IGN1w6FudA== 44256
IGJ1bmE= 44257
bmllanN6ZQ== 44258
RmluYWxseQ== 44259
15XXlg== 44260
Y2pl 44261
IOyeiOqxsOuToOyalA== 44262
IOuCmOuI 44263
IHByemVzdA== 44264
44O844Og 44265
bGljYQ== 44266
IER1Y2g= 44267
5bCN5bCN 44268
0ZbQudGB0Yw= 44269
cGFzc2Vu 44270
IHNhdGlzZmllcw== 44271
IEFkZGl0aW9uYWw= 44272
IGPDoW1hcmE= 44273
0LXRh9C10L3QuNC1 44274
IHBvbXA= 44275
IOunkOydtA== 44276
IE1pbGxz 44277
0LXQstC40LQ= 44278
IHJlc3BlY3RhYmxl 44279
IGZpbGFtZW50 44280
IHZlbmRlcg== 44281
IG1hdHRlcmVk 44282
b3VyZQ== 44283
7Li1 44284
S29yZWFu 44285
IGVzdHVkaW8= 44286
IGNhY3R1cw== 44287
IFZpdmU= 44288
IFJhZw== 44289
IGNvbXBsaXF1w6k= 44290
INmI24E= 44291
IHRhbw== 44292
pr8= 44293
U2luY2U= 44294
IGplb3BhcmQ= 44295
IFNlbGw= 44296
5bqU 44297
IOyYmw== 44298
IGtldG8= 44299
IGludGVsaWc= 44300
IEFuZ2Vi 44301
IHRpZGVu 44302
IHNvY2lv 44303
IHJlbWluaXNjZW50 44304
IGNhcmVnaXZlcg== 44305
U3BhY2U= 44306
IEV4ZXJjaXNl 44307
IEJlY29tZQ== 44308
w6p0cw== 44309
YWtr 44310
IS4u 44311
INGB0L/RgNC+0YE= 44312
IM6xz4DOvw== 44313
IHNob290aW5ncw== 44314
IGFwZQ== 44315
IFNhbW15 44316
IEt1bmc= 44317
IGN1w6Fs 44318
IEx1cA== 44319
5p2f 44320
5L6G5Yiw 44321
INGB0YLRg9C0 44322
IHN3ZWV0ZXI= 44323
IGNvbXVt 44324
IEFkcw== 44325
aHl1bmc= 44326
INCx0YPQtNGD0Yk= 44327
IHdhZmZsZQ== 44328
IE9yYg== 44329
IGxhdXQ= 44330
IGZvcmVjYXN0aW5n 44331
5ao= 44332
IHJhcHBpbmc= 44333
IHByZWZlcnM= 44334
IGJlbno= 44335
IG5paw== 44336
IEJhaG4= 44337
IHNhbmRpbmc= 44338
IGltbWluZW50 44339
INC/0YDQvtCx0LvQtdC80Ys= 44340
IGRvaXZlbnQ= 44341
0L7Qu9Cw 44342
IMW8eWNpYQ== 44343
aWh1 44344
IGV4aXN0ZW0= 44345
IEludGVyaW9y 44346
IFRha2Vz 44347
IHRvZGRsZXI= 44348
IGRpY3RhdG9yc2hpcA== 44349
IFNtaXRoc29u 44350
IEFsbGFodQ== 44351
z47Pgc6x 44352
7JWY7Iq164uI64uk 44353
IFZvdGU= 44354
IFNtZWxscw== 44355
0L7QtNC90L4= 44356
IGhpbmRzaWdodA== 44357
VlI= 44358
IFBhdGNo 44359
IEphaHJlcw== 44360
IHNvdXZlbmly 44361
IG5ldXRyb24= 44362
IGxvbmd0aW1l 44363
IHNheWlu 44364
5LmQ 44365
YXNha2k= 44366
INC+0YHRgtCw0L3QvtCy 44367
IGV4cGVsbGVk 44368
IGNyeXB0b2N1cnJlbmNpZXM= 44369
IE11cmRlcg== 44370
IENpdGl6ZW4= 44371
V0FZ 44372
IHBsdQ== 44373
IGxlbW9uYWRl 44374
IGNvbnZlbmllbnRseQ== 44375
IEhJ 44376
IDIwMjM= 44377
16nXldeq 44378
0LDRhtC40L7QvQ== 44379
IOubsA== 44380
INmE2YPZhg== 44381
INC90LXQvNC90L7QttC60L4= 44382
IHVudXNlZA== 44383
IG1haW9yaWE= 44384
IGFzdHJvbG9neQ== 44385
IERvd250 44386
Tmljaw== 44387
IHByZW9jY3Vw 44388
IGRlbWFpbg== 44389
157Xog== 44390
INCy0L7QtNGL 44391
IFNhbnNrcml0 44392
IHByw6p0 44393
IHN0cmFuZGVk 44394
IHJlZmlu 44395
INC/0YDQuNC90LjQvA== 44396
INC/0L7QstC10YDRhQ== 44397
4K+NPw== 44398
IHpyb2I= 44399
IGludGVydHc= 44400
IERhdmlkc29u 44401
0LvQtdC90LA= 44402
INC/0L7QvdGP0YLRjA== 44403
IFJlbm8= 44404
INC/0L7Qu9GD0YfQuNC70L7RgdGM 44405
IGNvcnJlc3BvbmRlbnQ= 44406
IFVyYW4= 44407
ZWxzZQ== 44408
wrfCtw== 44409
IHR1dG9yaW5n 44410
IGdyYW5kZGF1Z2h0ZXI= 44411
bHVkZWQ= 44412
IHN0ZXNzbw== 44413
IGjhur90 44414
IGdlZ2FuZ2Vu 44415
INCd0JA= 44416
IGFudGln 44417
YmFja2dyb3VuZA== 44418
IGdlZGFhbg== 44419
IGZhdm9yZWQ= 44420
IEVtbWFudWVs 44421
IGlvZA== 44422
IGNsYW1wcw== 44423
IGNvbXBsZQ== 44424
IEFkdmFuY2U= 44425
IOyeiOqzoOyalA== 44426
IFJveA== 44427
IOyXkOs= 44428
IGludGVzdGluZXM= 44429
IHBlcmN1c3Npb24= 44430
IGxlZ2l0aW1hdGVseQ== 44431
IEV0ZXJuYWw= 44432
ZmFtaWx5 44433
YWxvZw== 44434
QnJhZA== 44435
0LXQvdC40YLRjA== 44436
INGB0L3QsNGH0LDQu9Cw 44437
IGNlcnRh 44438
IGFra29y 44439
IM61zrTPjg== 44440
IG9jdGF2ZQ== 44441
IFZhYw== 44442
0LzQvtGC0YDQuA== 44443
IMOJdGF0cw== 44444
IGxvbmd1ZQ== 44445
IGRpc3NvY2k= 44446
0YDRj9C0 44447
aGVpbg== 44448
IHBhbnRhbGxh 44449
IGluZGljYXRpb25z 44450
IEx0 44451
IEdyYWRl 44452
6KOd 44453
b2luZQ== 44454
YnVn 44455
IFZlcml6b24= 44456
IEFsw6lt 44457
IHZpZW5uZW50 44458
INGH0LjRgdGC 44459
IEJlbmk= 44460
IFRzY2g= 44461
IFRQ 44462
IGluc3VsdGluZw== 44463
IFdlaWdodA== 44464
IGFkYXB0YXRpb25z 44465
IGhhYsOtYW4= 44466
IGNsaXF1ZQ== 44467
b8WbY2k= 44468
anVuYQ== 44469
IHN1Y2hlbg== 44470
IEdvZXM= 44471
IEV4b2R1cw== 44472
Q2hv 44473
IGFudGlz 44474
IO2MjOs= 44475
c2V2ZW4= 44476
INGH0LDRgdC+0LI= 44477
IGJhbGxpc3RpYw== 44478
em9ueQ== 44479
SUNJQQ== 44480
INC/0YDQtdGB0YI= 44481
IHNpbXBsZXNtZW50ZQ== 44482
IENvbGxhYm9y 44483
RnJlZA== 44484
INGC0LXQu9C10YTQvtC9 44485
IFJhdmk= 44486
7ZW07KQ= 44487
0L/QtdGA0LI= 44488
IOyeiOycvOuLiOq5jA== 44489
IMOzdA== 44490
IGFsZWc= 44491
w7pw 44492
IGRpc3JlZ2FyZA== 44493
IGluZGVudA== 44494
Y2xvdWQ= 44495
Y2hsYWdlbg== 44496
IGl0ZXJhdGU= 44497
IGdlbmVyYWxpemVk 44498
44GX44G+44GX44Gf 44499
4KS5 44500
ZWxlcmk= 44501
IGRpc2FzdHJvdXM= 44502
INGB0YLQsNC70LA= 44503
s5E= 44504
S05PV04= 44505
IHJpY2huZXNz 44506
IGNvbnNjaWVudA== 44507
aWNodHM= 44508
INGN0LvQtdC8 44509
2KjYrw== 44510
aXJlbnM= 44511
IGhhdW50aW5n 44512
cnVjdHVyZXM= 44513
YXR0YWNr 44514
IGN1cGNha2Vz 44515
c3F1ZQ== 44516
IG5hc3plZ28= 44517
IGFudGhyb3BvbG9neQ== 44518
44Gf44Gg 44519
44G144G1 44520
Y2hhZQ== 44521
IGRpc2NvdmVycw== 44522
IFBlcnNvbmFsaXR5 44523
IM6kzr8= 44524
IGRpxJ9lcg== 44525
5Y2A 44526
INC90LXRkQ== 44527
IEFuaXRh 44528
IFvimao= 44529
IENhcm0= 44530
IEJlbm55 44531
7Iqs 44532
IHB1cGls 44533
IG9jYXM= 44534
w6RsbGV0 44535
asWbxIc= 44536
5aSn5LiI5aSr 44537
YW1lbnRhbA== 44538
INC+0YLQvdC+0YE= 44539
IHBpZA== 44540
IGFybXA= 44541
UkVF 44542
INC+0YLQutGA0YvQsg== 44543
IHVkYQ== 44544
IFN5bmRyb21l 44545
IFN0YW5kYXJkcw== 44546
44GI44KL 44547
IHBvaW50ZXJz 44548
IGVuYW0= 44549
IFRpZw== 44550
w616 44551
INC90LDQvNC4 44552
IHVuY2hhbmdlZA== 44553
IHR1cm1vaWw= 44554
4bupbmc= 44555
ISEi 44556
NTAwMA== 44557
IOusvOyWtOs= 44558
IG1lcmdpbmc= 44559
IGVudHNjaGVpZGVu 44560
5Ye65p2l 44561
Zm9ybWU= 44562
IHRyaW1tZWQ= 44563
IGRhcmVk 44564
IGFzcGlyYXRpb24= 44565
IE15dGhpY2Fs 44566
IEhlag== 44567
IEFsZWo= 44568
0YbQvg== 44569
0L7RgtGD 44570
WmU= 44571
INC40L3RgdGC0YDRg9C80LXQvdGC 44572
IFJUWA== 44573
IGxvY2FsaXplZA== 44574
55qE6K+d 44575
IHN1cnJvdW5kcw== 44576
IGVtcGllemE= 44577
IGNsYXNl 44578
IOC4gQ== 44579
IFJhcGlk 44580
b21pbm91cw== 44581
aWdhaWw= 44582
INGI0LjRgA== 44583
IGzDpg== 44584
IHphc2Fk 44585
IHVuZm9sZGluZw== 44586
PyE/IQ== 44587
IOyInOqwhA== 44588
IFBvbHNraQ== 44589
IEthdWY= 44590
IENlbHQ= 44591
aXRpYw== 44592
IHRvb2xib3g= 44593
IFBvY2tldA== 44594
IOyEnOuhnA== 44595
IGJlbGtp 44596
IGFkbWlyYXRpb24= 44597
cGhy 44598
IFByb2R1a3Q= 44599
IFRydWNr 44600
44GO 44601
IGRyYXXDn2Vu 44602
d2HFgg== 44603
IEhlYnJld3M= 44604
IO2VmOqyjA== 44605
IEFDRQ== 44606
dXJnZW5jZQ== 44607
YXVyYWlz 44608
IGNoYXJpdGFibGU= 44609
xLF0 44610
IGFybWFz 44611
IEdlZGFua2Vu 44612
cmVhdGluZw== 44613
cG9ydGU= 44614
IGltcHJpbnQ= 44615
ZsOkaA== 44616
INC/0L7QtNGF0L7QtA== 44617
IG91dHNldA== 44618
4Lin4LiB 44619
0LXQvdC90L7Qs9C+ 44620
Q2xhc3M= 44621
IHZhbml0eQ== 44622
IFZPSUNFUw== 44623
IDI2MA== 44624
cmVzaWRlbnQ= 44625
VVNF 44626
IOqwgOyatOuNsA== 44627
6b0= 44628
IHRocm91Z2hwdXQ= 44629
IGN1bWE= 44630
7Jqx 44631
44O844Oz 44632
INC/0LvQvtGJ 44633
IHBhcnRpcw== 44634
IEFuaW1hdGlvbg== 44635
p4jr 44636
Q3Jl 44637
w7Z0emxpY2g= 44638
IG1hZ2c= 44639
IGNsdW1zeQ== 44640
IGJvdHRsZW5l 44641
IGJpcmxpa3Rl 44642
IEdhbWI= 44643
INeb158= 44644
IG1ldHJvcG9saXRhbg== 44645
6K+l 44646
5o6S 44647
T29o 44648
IG9iamVjdGlvbnM= 44649
INmF2Ko= 44650
INC80LXQuw== 44651
IHJlbW5hbnRz 44652
IFhhdmllcg== 44653
UmljaA== 44654
IG9sc2E= 44655
IFBpbGw= 44656
IGdyb2Fucw== 44657
IE5hcnVob2RvdQ== 44658
IENvbnRyYWN0 44659
0LDQtNCw 44660
bmFp 44661
INGE0LjQtw== 44662
IG9wcw== 44663
4bqhdA== 44664
IHBhcmFjaHV0ZQ== 44665
IG5lbGw= 44666
IEVudHNjaGVpZHVuZw== 44667
15zXmded 44668
IHRydXRoZnVs 44669
IHNoYXJwZXI= 44670
IGJ1cmVhdWNyYWN5 44671
Y2FydA== 44672
INC40L3Rgg== 44673
d2llaw== 44674
IHdpbGxpbmdseQ== 44675
IEhlcm1hbg== 44676
IG1laHJlcmU= 44677
IGVsaXRlcw== 44678
IEFybW9y 44679
44OI44Of44O8 44680
IGVtYm9yYQ== 44681
IFJlY29nbg== 44682
INC70Y7QsdC70Y4= 44683
IEV4Y2VsbGVuY2U= 44684
aWJlbA== 44685
IGV4cG9ydGluZw== 44686
7LK07KCB 44687
S2VsbHk= 44688
Q2FtZXJhbWFu 44689
IHNsaXBz 44690
IGZpZ3VyYQ== 44691
IOOBoQ== 44692
IGtvbGw= 44693
IFBhbmRlbWll 44694
54+t 44695
IHRpbWVk 44696
bGllw59saWNo 44697
INee15s= 44698
IHBlcsOtb2Rv 44699
5b+X 44700
aXZhdA== 44701
IHF1ZXN0aW9ubmFpcmU= 44702
IHDDqXJpb2Rl 44703
56m2 44704
IHNpZ2hz 44705
IGFsbGVnaWFuY2U= 44706
IFhW 44707
IEtlbnN1a2U= 44708
IEdlc3VuZGhlaXRz 44709
IHBvc2l0aXZv 44710
IEphbmVpcm8= 44711
IFNFRQ== 44712
INin2LPYqg== 44713
IEtlbHNleQ== 44714
dG9iZXI= 44715
IM6xzrvOu86s 44716
IFBhcmVudA== 44717
IERheXRvbg== 44718
IEJpbGRlcg== 44719
b3VyYWdl 44720
IHNlcmVz 44721
IG11Y2jDrXNpbW8= 44722
IFJlYWxt 44723
IE9GRklDRVI= 44724
ZXJzb25pYw== 44725
44KC44Gu 44726
b255YQ== 44727
IOq4iQ== 44728
IGFuY2VzdHJ5 44729
IEp1cmFzc2lj 44730
IGNlbnRpZ3JhZGU= 44731
4bqldQ== 44732
dWrEhWM= 44733
bWFucw== 44734
IHRpbw== 44735
IE1vxbw= 44736
IHRyYWdlbg== 44737
IHN0YXJlZA== 44738
IHNjaGVtYXRpYw== 44739
IHBhc3NvdQ== 44740
IG1lYXRiYWxscw== 44741
xYJvxZvEhw== 44742
IHN5bmNocm9ub3Vz 44743
IHBlcm1pcw== 44744
YXJpYWw= 44745
IHplcg== 44746
IHBhcml0eQ== 44747
IEF2YXRhcg== 44748
aW5kZWVy 44749
ZXN0b24= 44750
IG1laWTDpG4= 44751
IENseQ== 44752
tIk= 44753
IGVzdHJvZ2Vu 44754
IGNlbnRpbWV0 44755
55m6 44756
IGNvbnZpY3Rpb25z 44757
IHBvc3NpYW1v 44758
IHBlcmR1 44759
IHBhdGhvZ2Vucw== 44760
IFF1aW4= 44761
IFByb2dyYW1z 44762
IFBvaW50cw== 44763
cmFtZW50 44764
cmFpbA== 44765
IHZ5 44766
IGdyYWZ0 44767
IGJhcnQ= 44768
IExvdHVz 44769
4Kg= 44770
IOuztOyLnA== 44771
cmFtZXI= 44772
RmF0aGVy 44773
IOucuw== 44774
INeU150= 44775
IHRyYXplcg== 44776
IHRhcms= 44777
w6hjZXM= 44778
Zm9ydGg= 44779
INGB0LTQtdC70LDQu9C4 44780
IHp1Y2NoaW5p 44781
IHdha3R1 44782
IGVudGVydGFpbmVk 44783
IE1pbGxpYXJkZW4= 44784
IHNoYWt5 44785
IHByemVkZQ== 44786
uIzr 44787
IHJldmVyc2libGU= 44788
IE5BVQ== 44789
dWlucw== 44790
w6lyw6p0 44791
YW5uZW4= 44792
IEh1bnRpbmc= 44793
IEZlbGxvdw== 44794
w6lsaW9y 44795
IHJvdGF0aW9ucw== 44796
IGdyYW5ueQ== 44797
eHRvbg== 44798
INGB0YLQsNC90L7QstC40YLRgdGP 44799
INC90LDRh9Cw0Ls= 44800
IGFydGVyaWVz 44801
cmnDsw== 44802
INC/0L7Qu9GM0LfQvtCy 44803
INCR0Ys= 44804
IG5vdmVsdHk= 44805
cG91bmQ= 44806
IHdlaXJkZXN0 44807
IGJvaXM= 44808
w6ltaWU= 44809
dXBs 44810
QVRB 44811
IHRlaGQ= 44812
IE5pcg== 44813
c8SxbsSxeg== 44814
ISIs 44815
5ZGK6K+J 44816
IGltbW9ydA== 44817
IGVsaw== 44818
0LDQvdC40Yc= 44819
IGZhYnJpY2F0aW9u 44820
IE5vaXNl 44821
IEF2YW50 44822
2LHbjA== 44823
d2F0 44824
IHdob29zaGluZw== 44825
INeb15k= 44826
INCX0L3QsNGH0LjRgg== 44827
IGNlbnRyaWY= 44828
YW5zaW5n 44829
U291bmQ= 44830
IOudvOs= 44831
IGNhcHRpb25z 44832
4LON 44833
IG9yZ2Fz 44834
IGRvbHBoaW5z 44835
IEJsZW5k 44836
IFRhag== 44837
IENDVFY= 44838
IGlub20= 44839
IGVkaXRpb25z 44840
IGJ1cm5vdXQ= 44841
IGLDpHR0cmU= 44842
IENhc2E= 44843
b3ZpY2g= 44844
IG1vbHRlbg== 44845
IGJsaW5kZm9sZA== 44846
IEd1ZQ== 44847
5pe26Ze0 44848
IHNwaW5uZXI= 44849
IG3DtmdsaWNoc3Q= 44850
IFbDoA== 44851
ZW5lY2E= 44852
IG3DqWRpY28= 44853
5bm55Zib 44854
w6FzdGljbw== 44855
IGFyZA== 44856
IFN1bmRheXM= 44857
IFJlbW90ZQ== 44858
IOyWvOuniA== 44859
IHRyxrDhu5tj 44860
7IWo6w== 44861
IGRvcHA= 44862
IGJlxJ8= 44863
aWNhbmE= 44864
IOuCmOykkeyXkA== 44865
546H 44866
IGhvbGluZXNz 44867
ZGlyZWN0 44868
IOyYge2ZlA== 44869
IGN1bHBh 44870
IFN0aXRjaA== 44871
bGlnaHRseQ== 44872
0LDQvNC10L0= 44873
INC80LXRiA== 44874
INC/0LXRhw== 44875
IHlodGU= 44876
b3NwaGVyZQ== 44877
IOyTsOuKlA== 44878
w6lr 44879
IHNlcmlvdXNuZXNz 44880
IGdhcm1lbnRz 44881
IGNvbmNpc2U= 44882
IFNK 44883
IHZlcmxvcmVu 44884
IHBhcmVjZXI= 44885
IFVOQw== 44886
7Iqk7YOA 44887
IGVuZmFudA== 44888
IGJvbWJlcg== 44889
IEdpZnQ= 44890
IOyii+uLpA== 44891
IHJoeXRobXM= 44892
IEtsYXI= 44893
5Lq65rCR 44894
b3duaWs= 44895
IFJldmVyZW5k 44896
IGVtaXR0ZWQ= 44897
bGFzc2Vu 44898
IHJldmVuaXI= 44899
IGFyaXNpbmc= 44900
IHByZWNpc2FtZW50ZQ== 44901
IGludGVycG9s 44902
IFRlbmVtb3M= 44903
b2JlZA== 44904
IHRlY25vbG9naWE= 44905
IG5lcmVkZQ== 44906
IFZpc2E= 44907
IHNhdmE= 44908
IGVzY3JldmVy 44909
IGFzc2F1bHRlZA== 44910
IEZsZWlzY2g= 44911
IENvdW5jaWxsb3Jz 44912
IOqwgOq5jA== 44913
IGJlZ2c= 44914
IERldmVsb3Blcg== 44915
IEJyb256ZQ== 44916
IEJvbnVz 44917
INeo16c= 44918
ZmFjdA== 44919
IGVuZGxlc3NseQ== 44920
IG1hY2Ft 44921
IHJ6ZWN6eXdpxZtjaWU= 44922
IGhvdmVyaW5n 44923
w6hnZQ== 44924
IHBvb3Jlc3Q= 44925
IFNjaGVk 44926
bWlsZQ== 44927
aXNzZW1lbnRz 44928
YWPEgw== 44929
IOumvQ== 44930
IHZhY2Npbg== 44931
IGZ1dHVyaXN0aWM= 44932
IFdpbmRvdw== 44933
0L/QsNGA 44934
INGA0L7RgQ== 44935
IGxvd2Vycw== 44936
YWNz 44937
INCQ0LvQtdC60YHQsNC90LQ= 44938
IEFsZXJ0 44939
aWVtZQ== 44940
IENhdWNhcw== 44941
IGphd3M= 44942
IGh1bnRlZA== 44943
7Je9 44944
INio2YY= 44945
INec16DXlQ== 44946
IHR1cmJpbmVz 44947
IGx1bXBz 44948
IEFsbGllcw== 44949
YWhsdA== 44950
IHN1YnNjcmlwdGlvbnM= 44951
IG5vdXZlYXV4 44952
dWdlcg== 44953
Ym9uZXM= 44954
IGJlcnJ5 44955
IOyEoOusvA== 44956
IE1hbnVmYWN0 44957
IEx1bmNo 44958
6re4656Y 44959
IGh5ZHJhdGVk 44960
IGFjaGVp 44961
IFlheg== 44962
IFRpYmV0YW4= 44963
IFF1YW50dW0= 44964
IEplcm9tZQ== 44965
INC+0YnRg9GJ 44966
0L7QstCw0L0= 44967
bW90aW9u 44968
IENvbnRyb2xsZXI= 44969
ZW5lcmdldGlj 44970
INGB0LrQvtGA0L4= 44971
IHZvd2Vscw== 44972
INGD0LbQsNGB 44973
IGhvb2Y= 44974
IEJ1bGxldA== 44975
aW1hZ2lu 44976
16DXmded 44977
IGVuZ2FnZW1lbnRz 44978
IEJsdWVz 44979
IGHDsWFk 44980
IGZwcw== 44981
IGNhdGVycA== 44982
IHPhu5E= 44983
IFRyaWJl 44984
57aa 44985
0L/QvtC9 44986
aWZlcmF0aW9u 44987
IHJ1bWFo 44988
IFB1bmo= 44989
bGFi 44990
IGNvbXByZWhlbnNpb24= 44991
YnJpbmdpbmc= 44992
V28= 44993
IHRpaw== 44994
IGFueWhvdw== 44995
5Lul5YmN 44996
w6F0aWNhcw== 44997
IHNpdHplbg== 44998
IGtvbGF5 44999
IENvbmZlZGVyYXRl 45000
IENhbGxlZA== 45001
IG5hc3p5Y2g= 45002
IGR6acSZa2k= 45003
IGNsb2Fr 45004
IEdvb2c= 45005
IEFzaGU= 45006
6LGh 45007
ZW5hbg== 45008
INC80YvRiA== 45009
INCy0LXRgg== 45010
IFNwbw== 45011
IFNrZXQ= 45012
IEhlbmRlcnNvbg== 45013
aWxhaA== 45014
INCx0LXQt9C+0L/QsNGB 45015
IHNla2FsaQ== 45016
7Ja06rCA 45017
IHNuYXJl 45018
IHLhurFuZw== 45019
IGbDtnJzw7Y= 45020
c3p5Y2g= 45021
IMO8YmVycw== 45022
IHN0cmF0w6ln 45023
IOy6kOs= 45024
IHJhcHBlcnM= 45025
IGNlcA== 45026
IEhhc3Rh 45027
IGhvcnJpYmx5 45028
IGZyw7xo 45029
INio2Lk= 45030
IG1hbnRsZQ== 45031
44CF 45032
ZnVuZGluZw== 45033
IHp1c3Q= 45034
IFBlbnM= 45035
c2Vk 45036
IO2XpA== 45037
IGdlcmVraQ== 45038
IGFsYXJtcw== 45039
IFdoYQ== 45040
IE1hcmt1cw== 45041
YWtzaQ== 45042
INCQ0LvQtQ== 45043
a2xvcmU= 45044
IMOpbmVy 45045
IHRpbGRl 45046
Ym94aW5n 45047
IOyEng== 45048
IGVuY29udHJhbW9z 45049
IFBoYXI= 45050
0L3QsNC60L7QvA== 45051
w7NzdA== 45052
IMSwcw== 45053
IOuLmA== 45054
IHNxdWF0cw== 45055
IHByZXRlbmRlZA== 45056
IGRleg== 45057
IOq0nOywruyVhA== 45058
amFjaA== 45059
65286rOg 45060
IO2ZleynhA== 45061
IEFuc2No 45062
aW1lcms= 45063
IGNvbmp1Z2F0ZQ== 45064
IHBlbmluc3VsYQ== 45065
IGdvcmlsbGE= 45066
IHBob3RvZ3JhcGhlZA== 45067
IEF1bnF1ZQ== 45068
IGVudHJlbg== 45069
IERldXRzY2hlbg== 45070
IEFsYWRkaW4= 45071
IOustOyEnA== 45072
IFN0ZWxsYQ== 45073
IEVsZWN0aW9u 45074
b3V0aW5l 45075
R3JhbmQ= 45076
IFdhaw== 45077
IFNlcmdpbw== 45078
aG9yc2U= 45079
YWhvbg== 45080
IEZhbWlsaWVz 45081
IGhhdGluZw== 45082
IEJldHQ= 45083
4LiZ4Liw4LiE4Liw 45084
IGN1cmxpbmc= 45085
IElzcmFlbGlz 45086
INec15DX 45087
IE15ZXJz 45088
IHNjYW5uZWQ= 45089
IEJFQw== 45090
aWxlcmk= 45091
IGNhbGxl 45092
IE1pbmg= 45093
IG1pY3Jvbg== 45094
IGNvbmR1Yw== 45095
w612 45096
INCy0L7Qt9GM 45097
IGFjdGlvbmFibGU= 45098
IFRydXN0ZWVz 45099
IHRpZWY= 45100
IGhlYWRlcnM= 45101
IGFuaW1hbGVz 45102
7JuA 45103
0LvQvtGF 45104
dW5pdHk= 45105
bHlh 45106
IGphbmdhbg== 45107
IGhhbmk= 45108
IGNhc2luZw== 45109
IGrDs3ZlbmVz 45110
IFNwbGl0 45111
IENhcmxv 45112
IEJlaW0= 45113
5bCN5LiN6LW3 45114
IG51YW5jZWQ= 45115
IHRlZGR5 45116
IENsYW4= 45117
w6RjaGVu 45118
cGllcg== 45119
INC00L7Qv9C+0LvQvQ== 45120
IGRpYXBlcg== 45121
ZWZmZWN0aXZl 45122
IE5pYWdhcmE= 45123
IHdhcnQ= 45124
IGNvcnJv 45125
IEthbXBm 45126
enRl 45127
IGTDqXZlbG9wcGVtZW50 45128
IGF0dGFja2Vycw== 45129
IFNoZXJtYW4= 45130
IDE5MTQ= 45131
IG1lb3c= 45132
IFDDpQ== 45133
7Lo= 45134
Y2l0 45135
IGNvdXBl 45136
IOq3uOuLpOydjOyXkA== 45137
IGh1bW91cg== 45138
IGNvbGU= 45139
IFdhcm5pbmc= 45140
IFRpbA== 45141
Y2FsbQ== 45142
YnVhdA== 45143
IGNpbmU= 45144
a2llag== 45145
S2V2aW4= 45146
IG1pbGxpZ3JhbXM= 45147
15PXqA== 45148
YXJpYW1lbnRl 45149
IG9ybw== 45150
IEhvZA== 45151
ZXJ0b3M= 45152
IGxpaGF0 45153
IGZ1bGxlc3Q= 45154
IGdyYW5kaQ== 45155
INCx0L7Qug== 45156
IHdob2xseQ== 45157
IG1haGRvbGw= 45158
IGNvbnRyb2xs 45159
IEJ1bnVu 45160
6IqC 45161
IGRpcHBlZA== 45162
IHJlZ2nDs24= 45163
INmE2Yg= 45164
INCx0LDQsw== 45165
IHByZW1pZXJz 45166
IGNo4buL 45167
IOaJgOS7pQ== 45168
6LGG 45169
aWRleg== 45170
IHF1b3Rh 45171
IGdoZWU= 45172
YXJrYW4= 45173
IGdlbGF0aW4= 45174
IENsZXJr 45175
YmJsZXM= 45176
IFBhaWdl 45177
IHN0YWdlZA== 45178
IHNvY2lhaXM= 45179
IEJpemlt 45180
IHZlbG9jaWRhZGU= 45181
IG1hbGFyaWE= 45182
IHNob3J0ZW5lZA== 45183
IHNhbHV0 45184
IEhlaGU= 45185
IHbhu4s= 45186
IFRhaXdhbmVzZQ== 45187
IEFycmk= 45188
Z3Jlcw== 45189
5Y675LqG 45190
KCk= 45191
cmlhZA== 45192
kZDr 45193
IOOBvuOBmQ== 45194
IG1hc2N1bGluaXR5 45195
TFA= 45196
IOuWoQ== 45197
IHTDqXJtaW4= 45198
IFbDpA== 45199
IFNlaXRlbg== 45200
IHJlc3BlY3RmdWxseQ== 45201
w6Fv 45202
IHRvdGFsZW1lbnQ= 45203
IHNjcmFwcw== 45204
IGluZnJpbmc= 45205
IEJvc2U= 45206
YW1hcg== 45207
IEx1aXph 45208
IEFSTQ== 45209
INC/0LvQvtGF0L4= 45210
IG1laWxsw6Q= 45211
IERpb24= 45212
5byA5aeL 45213
IHNvdWhh 45214
IGdlc2NoYWZmdA== 45215
IGNvbnZvbHV0aW9u 45216
IOKAkeKAkQ== 45217
IDE0NA== 45218
bGluZ3Q= 45219
IG3DpG5uaXNr 45220
IGd1c3RhZG8= 45221
IGNvaW5lZA== 45222
IEx1bHU= 45223
5a6D55qE 45224
b3BvdA== 45225
IFByYXllcg== 45226
IHJvYXN0aW5n 45227
IGNocm9tb3NvbWVz 45228
6aOv 45229
0LXQu9C1 45230
Qmx1ZQ== 45231
IEVyZm9sZw== 45232
6Ieq55Sx 45233
INC/0YDQuNC00YPQvA== 45234
IHJpc2tpbmc= 45235
IEd1YXJkaWFucw== 45236
IDIwMjQ= 45237
w6hzZQ== 45238
INCx0YPQtNGC0L4= 45239
IGNvbnNlcnZl 45240
IEJyaW5naW5n 45241
IEFzdHJh 45242
4LmA4LiC 45243
INC60LDQutGD0Y4= 45244
cmVzcGFjZQ== 45245
INCe0L8= 45246
INCy0L7QutGA0YPQsw== 45247
5oWL 45248
IG1hc2tlZA== 45249
IFNoeQ== 45250
IE5pbQ== 45251
ZW5kYXM= 45252
IO2PrOyduA== 45253
IOuqqOyWkQ== 45254
IHZhbGV1cg== 45255
IE5lZ3Jv 45256
IENEcw== 45257
aW5rbGluZw== 45258
IG1vbnTDs24= 45259
IEhvbmQ= 45260
UmVhbA== 45261
IGZ1bGxuZXNz 45262
IFdob29wcw== 45263
IFNoYW5r 45264
IEJyYW4= 45265
IHRyYW5zbHVj 45266
IGVycg== 45267
IEdhcmRlbnM= 45268
b3l1 45269
IGFmZmlybWF0aXZl 45270
5LiL5LiA 45271
IHBvdHRlcnk= 45272
bGl2ZQ== 45273
aWF1 45274
bW91bnQ= 45275
IGZsdWN0dWF0aW9ucw== 45276
5Z+O 45277
w61lbQ== 45278
IHB1bHNlcw== 45279
IGNyaWFuw6dhcw== 45280
zq/Osc+C 45281
IGJhc3Rh 45282
RU5OSVM= 45283
INC60L7RgNC/ 45284
IEZ1bms= 45285
IOmAmQ== 45286
w6VydA== 45287
INC30LDRgtC10Lw= 45288
IHBhcmFzaXRlcw== 45289
44OZ 45290
IGFpcmZsb3c= 45291
IFh1YW4= 45292
R8O8bG1l 45293
IGJsb29taW5n 45294
IG11bW15 45295
IGJhbw== 45296
IENsYXA= 45297
YW50aWNz 45298
c2tpbg== 45299
Y2VudHJpYw== 45300
YmVmb3Jl 45301
IFJJQ0hBUkQ= 45302
IEhhaG4= 45303
VEFLRQ== 45304
INGC0YDQtdGC0Yw= 45305
IHByZXNzdXJlZA== 45306
IEt1cno= 45307
aXN0aQ== 45308
INC90LDRiNC10LPQvg== 45309
IHNlbWljb25kdWN0b3I= 45310
IENsaW50 45311
IHBsdXA= 45312
IE9yaWdpbg== 45313
IEV2ZW50cw== 45314
IOqxseyglQ== 45315
bXBmZW4= 45316
TkVZ 45317
IERX 45318
IOu2ge2VnA== 45319
IGluZm9ybXM= 45320
IGZvcnNr 45321
IGFtaWdh 45322
IENpbmNpbm4= 45323
U3Ry 45324
IHBhcmlzaA== 45325
IOy7pO2U 45326
IHNpemk= 45327
IHBsYW50YXRpb24= 45328
IGJsaXZlcg== 45329
INC/0L7Qu9C40YI= 45330
IHN1YmRpdg== 45331
IHJhbnQ= 45332
IHByaW5jaXBhbHM= 45333
5ZCm 45334
IGt1bm5l 45335
w7xnZW4= 45336
YXJlc3BhY2U= 45337
IHZhbGxhaGk= 45338
IGNvbGxhcHNpbmc= 45339
2KfZhNmF 45340
IGxpZGVy 45341
IHRhbWE= 45342
IGdhZ25lcg== 45343
cm9sbGU= 45344
IOunkOyUgOuTnOs= 45345
IGNhdGhlZHJhbA== 45346
IFdlYnM= 45347
IFBvbGl0aWNz 45348
44GX44G+ 45349
44Gj44Gm44KL 45350
IERlbmlz 45351
IHR1bw== 45352
IHJlZnJhY3Q= 45353
IGRpc2ludGVncg== 45354
c3Rlcw== 45355
INC70Y7QsdC+0LI= 45356
IHdpbHQ= 45357
IHRydXN0cw== 45358
IGtvbXVu 45359
IEJhc2tldA== 45360
fiEh 45361
bmFl 45362
INCa0L7Quw== 45363
IHN5bGxhYmxlcw== 45364
IEhlbnJp 45365
IE5hYg== 45366
2YjYuQ== 45367
IHdu 45368
IGthbXA= 45369
IFByYWd1ZQ== 45370
IEJyZWFrZmFzdA== 45371
IOq3uOuftA== 45372
IGNodXQ= 45373
IDMzMA== 45374
IEluZHVzdHJpZXM= 45375
5LiN566h 45376
IGnFn2k= 45377
IEdvbGRtYW4= 45378
IMSwbnM= 45379
dXNzYQ== 45380
aXRoZQ== 45381
hJA= 45382
IFNPVU5E 45383
0LDQu9GM0L3Ri9C8 45384
Lig= 45385
INCz0L7RgNCw0Lc= 45386
IGRhZ2VnZW4= 45387
IOuu 45388
IHdhaXRlcg== 45389
bGVuZ3Ro 45390
IM+Dz4TOsQ== 45391
IGNodW5reQ== 45392
U2E= 45393
IHJ1c3R5 45394
IEp1ZGl0aA== 45395
NzUw 45396
IGVwb3h5 45397
7Lmg 45398
5Y+y 45399
bWV0cm8= 45400
IHJlamVjdGluZw== 45401
IHNxdWlzaHk= 45402
IHBsdXBhcnQ= 45403
IG3DqXRo 45404
IGFzcGlyaW5n 45405
IERyYW1h 45406
IHVwbGlmdA== 45407
p4jri6Q= 45408
Li4uLi4uLi4uLi4uLi4uLg== 45409
oKTsmpQ= 45410
IHTDqWNuaWNh 45411
IHBhc2FuZG8= 45412
VGhvc2U= 45413
INGA0LDQt9C00LXQuw== 45414
IG1lZGlvY3Jl 45415
IE5pY2tlbA== 45416
IHN1cGVyaGVyb2Vz 45417
IG1pc3Npb25hcnk= 45418
IFBhcmVjZQ== 45419
IHJvdGF0aW9uYWw= 45420
IHByZXR0 45421
44Gd44GG44Gd44GG 45422
IGxhbWE= 45423
IGNhbnlvbg== 45424
IGJldGVy 45425
IFByb3Zvc3Q= 45426
IGh2aXM= 45427
IGRlYWN0aXY= 45428
IEhlbHM= 45429
cGZsaWNodA== 45430
U29tZXRoaW5n 45431
IFBpZXJjZQ== 45432
IOqygOywsA== 45433
bHVuZ2Vu 45434
IHNpemluZw== 45435
IGxhdGl0dWRl 45436
IE5vbmV0aGVsZXNz 45437
b21uaWE= 45438
IFNhYnJpbmE= 45439
IER5bmFtaWM= 45440
5YO5 45441
b250YQ== 45442
7IaQ 45443
IGRpcmVjdGl2ZQ== 45444
IERlcG90 45445
IGZ1ZWxlZA== 45446
IGV4cGlyZQ== 45447
IGNvbcO6bg== 45448
IFNleHVhbA== 45449
IEdvcmU= 45450
IHJlc3RsZXNz 45451
IEpBS0U= 45452
0YLQtdGA0LXRgQ== 45453
INGC0YDQsNC9 45454
IEhvbHo= 45455
5bCG 45456
IEFjdG9y 45457
5p2v 45458
Y2FsbA== 45459
IGVtYWlsZWQ= 45460
IFBlYXI= 45461
0YPQtNC4 45462
0YDQsNC7 45463
IG3DoHk= 45464
IENIRUVSSU5H 45465
5a6J5YWo 45466
IHJldGFpbGVy 45467
IHByb3Ry 45468
IGRpc2NhcmRlZA== 45469
IEhJUw== 45470
IGV2YW5nZWxpY2Fs 45471
IEVsc2U= 45472
IGV4cGxvcmVz 45473
IGNyaXRpY2l6aW5n 45474
aWZpaw== 45475
IHdoaXBwaW5n 45476
IG9waXM= 45477
b3VzZWQ= 45478
RnJlZQ== 45479
IO2MrA== 45480
IG1pY3M= 45481
cnVubmluZw== 45482
T2I= 45483
aXRpw6k= 45484
IG5lY2VzaXRh 45485
IERvbWluaWNhbg== 45486
IEJhZ2g= 45487
IHRlbmRlbmNpZXM= 45488
IE1ldHJvcG9saXRhbg== 45489
xZFs 45490
INC30L3QsNC10Lw= 45491
IFphbQ== 45492
IERlYWRwb29s 45493
YWxlxbw= 45494
IGludmVzdGlnYXRpdmU= 45495
IFByb251bmNpYXRpb24= 45496
IGVtdWxhdGU= 45497
IGJhbmNv 45498
IC3imao= 45499
5Yi7 45500
IG92ZXJhcmNoaW5n 45501
bGljaGVz 45502
INCy0L7Qt9Cy0YDQsNGJ 45503
IFNjYXJ5 45504
IEtpYQ== 45505
5Zyf 45506
cm9udGluZw== 45507
aW5uZWQ= 45508
INuB2Yg= 45509
7IiY66W8 45510
576O5ZGz 45511
d2Vs 45512
IOuzhOuhnA== 45513
IHVuaW50ZW50aW9u 45514
YWFT 45515
IG5pY2VzdA== 45516
IFRlc3Rpbmc= 45517
IElTSUw= 45518
b2dlbm91cw== 45519
INif 45520
IGxpZXV0ZW5hbnQ= 45521
IGJyYXVjaA== 45522
IFRpcg== 45523
ZHJpdmU= 45524
IHRvbGVyYW50 45525
IHNob290ZXJz 45526
IOyYiOu7kA== 45527
5q66 45528
b250b24= 45529
IHRlcmlh 45530
aWV0ZXQ= 45531
Um9u 45532
bGVpZ2g= 45533
Z2Fl 45534
IG9sbWFr 45535
IENsb25l 45536
c29sZA== 45537
IHNrZWxldG9ucw== 45538
IGluY3VtYmVudA== 45539
0L7QvNC1 45540
Q09O 45541
IGxldmVu 45542
IG1pbGxlbm5pYWxz 45543
IGVxdWF0b3I= 45544
IEZlZGVy 45545
IEFsZXhhbmRyYQ== 45546
IHZyaWo= 45547
IEhlYWx0aGNhcmU= 45548
IO2VkQ== 45549
IGVtcGhhc2l6aW5n 45550
IGRpYWxvZ3Vlcw== 45551
IGNoaWxsZWQ= 45552
IHByb3c= 45553
IFBhc3Npb24= 45554
IExhZGVu 45555
YXJpZXN0 45556
YXBocmFn 45557
IGFkZGl0aXZl 45558
IFN0YWF0 45559
IE5lcHQ= 45560
IEhBTQ== 45561
4LmA4Lit 45562
ZGF5cw== 45563
IO2WiOuNmA== 45564
IHZvaWxh 45565
INGF0Ls= 45566
IERldXRzY2hl 45567
cXVpcg== 45568
T3Blbg== 45569
IHJhbmdlZA== 45570
IGxldmVycw== 45571
IE1hbnNpb24= 45572
cGFyZWQ= 45573
IFRpdGFucw== 45574
YXRvaXJl 45575
IGVuZ2FnZXM= 45576
eWV6 45577
bmFkZW4= 45578
IG9ic3RydWN0 45579
IEVtbXk= 45580
5ZWG 45581
sKU= 45582
IHRyb3Bo 45583
IHRha2Vhd2F5cw== 45584
Ky4= 45585
dHljem5pZQ== 45586
aMOpc2l0ZXo= 45587
IHBvZMOtYQ== 45588
IOyjvOuKlA== 45589
IGNpdGF0aW9u 45590
IEFxdWE= 45591
IGRlYnVnZ2luZw== 45592
0LLQsNC9 45593
IOuLueyLoA== 45594
INin2YTZig== 45595
IGluc3RhbnRhbmVvdXM= 45596
IEF1dHVtbg== 45597
IGtlcGFkYQ== 45598
IGdldGFu 45599
aGluaQ== 45600
eW50aGVzaXM= 45601
INC/0LXRgNC4 45602
IE1hY2Vk 45603
UGFj 45604
dW50dQ== 45605
QnJh 45606
INCz0L7RgNCw0LfQtNC+ 45607
IDE5NTk= 45608
INGC0LXQvNC/0LXRgA== 45609
IHNhbmU= 45610
IE9VUg== 45611
YXN1 45612
IOustOyX 45613
IHZhbGxleXM= 45614
IGxpc3Rpbmdz 45615
IHByemVkc3Rhdw== 45616
IGd1bW15 45617
IGNvcnRpc29s 45618
IE9icmln 45619
IEFsbGllZA== 45620
0L7QttGD 45621
IGfDqW7DqXI= 45622
IGRvY3M= 45623
IENoaWxp 45624
IEFiZHVsbGFo 45625
S2l0 45626
IGNvbnRyaWJ1dG9ycw== 45627
0LPQvtGA 45628
0LvQtdGA 45629
IGJpbmRlcg== 45630
IG1vZMOobGU= 45631
7YWQ 45632
IGludGVpcm8= 45633
bWlz 45634
ZmVyYQ== 45635
2KfYsA== 45636
TWFuaWE= 45637
IO2ZnOuPmQ== 45638
IOu0kOyalA== 45639
IEpheg== 45640
57uT 45641
0ZbQu9GM0LrQuA== 45642
cmlzaG5h 45643
IOq1sA== 45644
IHRhbWFuaG8= 45645
IGFwcGxpYW5jZQ== 45646
IFJlc2lzdGFuY2U= 45647
IExPT0s= 45648
IEh5cA== 45649
IEhlaWw= 45650
RmlyZQ== 45651
dWp1 45652
IGhlYWxz 45653
IG1hbHQ= 45654
IFZFUlk= 45655
INGF0L7Rh9C10YjRjA== 45656
IGxpbmdlcg== 45657
IE5hcnI= 45658
IFJlZ3VsYXI= 45659
IExvb3A= 45660
IExlbm8= 45661
IHNvcnRpZQ== 45662
IFNlcnZl 45663
IOydtQ== 45664
IEx1ZWdv 45665
aXR0w6Q= 45666
IHVuZGVz 45667
6LO9 45668
5aaC5p6c5L2g 45669
IHNsaXBwZXJz 45670
IG9uZGE= 45671
IMSQw6J5 45672
IHRhcGVk 45673
IHRyYXZlcnNl 45674
IHJlbGF0aXZpdHk= 45675
IFlvc2hp 45676
Y2pvbg== 45677
aWxhdGVk 45678
YWN0aXZlbHk= 45679
INCh0L7Qsg== 45680
5oiR6KeJ5b6X 45681
IFBPTA== 45682
0KDQmA== 45683
aW5mbGFtbQ== 45684
Y2hlZXJmdWw= 45685
INee15DX 45686
ID4+Ww== 45687
bWluc3Rlcg== 45688
INCy0LvQuA== 45689
IGlkZW50aWZpZXI= 45690
IExhbWJkYQ== 45691
IHRyb3M= 45692
IGZsYXdsZXNz 45693
IGRldHJpbWVudGFs 45694
IGJ1bmxhcsSx 45695
V2Fy 45696
IHJlZ2nDo28= 45697
55yf55qE5piv 45698
IEJpa2U= 45699
Y2Vzc29ycw== 45700
IGPDuW5n 45701
IFJO 45702
IOq9gw== 45703
IGvDvMOnw7xr 45704
IEJlZ2lubmluZw== 45705
7Zi46w== 45706
IGdld2U= 45707
IGRlbm90ZQ== 45708
IEFsYmVydG8= 45709
IHByb2Jpb3Q= 45710
IG9kZQ== 45711
IG1vbGFy 45712
IGJ1cnN0aW5n 45713
YXNzdW1lZA== 45714
IGZvb3RwcmludHM= 45715
dmVkYQ== 45716
IHN0ZXJvaWRz 45717
IGZsYW1pbmc= 45718
IEVsbGVy 45719
IGVya2VubmVu 45720
w6R0emVu 45721
IGxpZmVjeWNsZQ== 45722
IERPVQ== 45723
IEthcmVuYQ== 45724
IEd1ZXJyYQ== 45725
6L+Y5piv 45726
IHNpbmlzdGVy 45727
IHBvZMOpaXM= 45728
IHBhcmFi 45729
IG9rbw== 45730
IG1hdMOpcmk= 45731
IGNhcmlj 45732
c29uYXJv 45733
IHByYXRpY2FtZW50ZQ== 45734
0YPRgdCw 45735
IGNvbXVucXVl 45736
IHZpZ2lsYW50 45737
IHJlZ2ltZXM= 45738
IFNob290aW5n 45739
IHJhaWRz 45740
IE5vcmE= 45741
IFdpZWRlcg== 45742
bWVucw== 45743
INGB0L7QtA== 45744
IOqyveyasOyXkOuKlA== 45745
INCy0YXQvtC0 45746
IGF1dG9iaQ== 45747
IFNjaG4= 45748
IFJvYmJpZQ== 45749
IEZpdG5lc3M= 45750
INC60L7QvdGE 45751
IHBlbmd1aW4= 45752
0LzQvtGC0YDRjw== 45753
INC80LjQvdC40Lw= 45754
cGxheXM= 45755
IGRlbGVnYXRlcw== 45756
TWVy 45757
IHNpc3RlbQ== 45758
IE1pY2hhZWxz 45759
bWFsZQ== 45760
2KfYuQ== 45761
IGPDoWNo 45762
IEjDpA== 45763
INeZ15XXk9ei 45764
IHN1cGVycG93ZXI= 45765
IHN0cm9u 45766
IHJvdmVy 45767
IGTDqXBlbmQ= 45768
6Zmz 45769
IHJldGlyaW5n 45770
IHZhbXBpcmVz 45771
IG1lcmRl 45772
IENoYW5naW5n 45773
IHRhbWU= 45774
IHNwb2tlc3BlcnNvbg== 45775
IGNheQ== 45776
IGZsaXJ0aW5n 45777
IEdyw7Y= 45778
IHfDpHI= 45779
IHd5Yg== 45780
IGNvZXVy 45781
4bqhbmg= 45782
IOyZgOyEnA== 45783
IGNvbm5haXM= 45784
IEh1bmRyZWRz 45785
IEJlYQ== 45786
IM6xz4A= 45787
cHJ1Y2g= 45788
IHNvY2llZGFkZQ== 45789
IFdoaWxzdA== 45790
IEthaXQ= 45791
ZXNwYWNl 45792
IGNoaWE= 45793
IEVybQ== 45794
IOuwlOq/ 45795
IGZlbmNlcw== 45796
IE1vcnRhbA== 45797
6rKB 45798
INCz0YDQsNGE 45799
IEhvbWVsYW5k 45800
IEpVTg== 45801
aXNzdA== 45802
IHBhcmxhcg== 45803
IHNwb3J0eQ== 45804
w6lv 45805
IGRlZXBlbg== 45806
IEJlaGF2aW9y 45807
6YCP 45808
5ZOI5ZOI5ZOI 45809
IGVycmFuZA== 45810
IHJvdGFyeQ== 45811
IFdlbGxpbmd0b24= 45812
V2luZA== 45813
IG1lc2VsYQ== 45814
4bqjbmc= 45815
aWVuZGU= 45816
IGV4Y2VsbA== 45817
IEdlbml1cw== 45818
IEVkdWFyZG8= 45819
5pyJ5Lq6 45820
IMWfdW51 45821
IMSwc3RhbmJ1bA== 45822
IHByb2R1dG8= 45823
IOOFjuOFjg== 45824
T0ZG 45825
IHdvbGx0 45826
54iG 45827
IOuJtOyKpA== 45828
IGxhc3M= 45829
IGhlcnR6 45830
IGFyb21hdGlj 45831
INC30LLQvtC9 45832
IGF1dG9j 45833
IEx1c3Q= 45834
IDExMg== 45835
IM6X 45836
IHJldmlld2Vycw== 45837
IHJlY2VwdGl2ZQ== 45838
5bCN5LqG 45839
w6JuZA== 45840
b2dsbw== 45841
IOyVhOuLmQ== 45842
IG5nbw== 45843
0ZbRgtC4 45844
w6V0 45845
Y29ubw== 45846
IHRla3Jhcg== 45847
IOyjvOqzoA== 45848
IGdlbG1pxZ8= 45849
IGJlZHRpbWU= 45850
IEFyZ2g= 45851
QURB 45852
INCz0L7RgNC+0LTQsA== 45853
IMSH 45854
IGFsbGlhbmNlcw== 45855
Z2lnZ2xpbmc= 45856
IHllcmRl 45857
IHNwaWVz 45858
IGd1dGVz 45859
w6dp 45860
IGFsbHRpZA== 45861
IExhaA== 45862
npDr 45863
IGRva8WCYWQ= 45864
2YjZig== 45865
IHRveGljaXR5 45866
IGNhbmNlbGxhdGlvbg== 45867
IDE5NTg= 45868
ZHJv 45869
IOyekeydgA== 45870
IE1vdG9yb2xh 45871
IG11bHRpbg== 45872
IGVudGh1c2lhc3Rz 45873
IE1pZ2h0eQ== 45874
IENvY29udXQ= 45875
OuOAjA== 45876
IFBpY3R1cmVz 45877
IHNhbmdyZQ== 45878
IGJsaW5raW5n 45879
b2xlc29tZQ== 45880
IOyKpO2DgOydvA== 45881
RlA= 45882
IGJvb21pbmc= 45883
INC00LXRgdGP0YI= 45884
IHJhdGNoZXQ= 45885
IHRpbWVsaW5lcw== 45886
bGVuZXNz 45887
IGNhZ2Vz 45888
IEdvb2RuaWdodA== 45889
b21ldGltZXM= 45890
IGN1bm5pbmc= 45891
IFJpc2s= 45892
dWxlZA== 45893
ZGFkZQ== 45894
IHByYXRh 45895
IGd1c3RhcsOtYQ== 45896
YW11cw== 45897
IEppbnBpbmc= 45898
IGVzdHJ1dA== 45899
IGRlc2NvYnJpcg== 45900
IE3EgQ== 45901
IEFsbGFu 45902
IOWIhg== 45903
INec16c= 45904
IHByZXNlcnY= 45905
IFN0cmF3YmVycnk= 45906
xI8= 45907
THU= 45908
IGtybw== 45909
IFJlcG9ydHM= 45910
7IWU7JW8 45911
IHZhbHQ= 45912
IHBvdXZhaXQ= 45913
IGFwcGFy 45914
IEJvbmU= 45915
IHByZWZlcmFibHk= 45916
IFJlcMO6YmxpY2E= 45917
5bCx5Yiw 45918
IGhlcnpsaWNo 45919
IGNoaW1uZXk= 45920
IMOnZXY= 45921
IHZpc2Fz 45922
IHZlcnI= 45923
IGN1bHRpdmF0aW9u 45924
IEFybWVuaWE= 45925
INCy0LTRgNGD0LM= 45926
IGNvY2tybw== 45927
cmV0Y2hlZA== 45928
YXJ0eg== 45929
INC70Y7QtNGP0Lw= 45930
IHBvbMOtdGljYXM= 45931
IFBhbno= 45932
IEFLQQ== 45933
IOuIjOufrA== 45934
IGVycm8= 45935
IGNhbXBlcg== 45936
IDEwMg== 45937
4KS4 45938
ZG9uZQ== 45939
IGhvYXJk 45940
INCf0L7RgtC+0Lw= 45941
amVvbmc= 45942
IGRlc3Rh 45943
cGFr 45944
IGluaW0= 45945
IGdyb3dlcnM= 45946
IE1lc3NhZ2U= 45947
IGVsZWN0b3I= 45948
ZW5nYWdl 45949
IEZvcmJlcw== 45950
IENpbmNpbm5hdGk= 45951
IGRpZmbDqXJlbmNl 45952
ZGY= 45953
IHNwYXI= 45954
IGF3YWl0cw== 45955
IFVTU1I= 45956
IFJpc2luZw== 45957
IEhvxZ8= 45958
IGZvb3Rpbmc= 45959
IGNvbmRpY2lvbmVz 45960
0YLQvtGA0L7Qsg== 45961
IGNsaW5pY2lhbg== 45962
IERpc2t1c3M= 45963
5aOT 45964
16jXkg== 45965
16U= 45966
aXRlaXQ= 45967
Z3Jlbg== 45968
IGNoYXJpc21h 45969
IGxldWtl 45970
IGlycml0YXRpbmc= 45971
IGNpcmNh 45972
IFJob2Rlcw== 45973
IHBpb3I= 45974
IGhhbmRpY2Fw 45975
cm95YWJsZQ== 45976
IHZ1bGw= 45977
T0c= 45978
IGluw61jaW8= 45979
aWVyaQ== 45980
IHNwbGFzaGluZw== 45981
IGRlbWlzZQ== 45982
IGFzc2lzdGly 45983
0YfRgtC+ 45984
IGNvdmVydA== 45985
IEd1ZA== 45986
4LiJ 45987
a2zDpHI= 45988
IOyekOq+uA== 45989
IHZlcsOkbmRlcnQ= 45990
IFJFTQ== 45991
IENvbnZlbg== 45992
YXRnZQ== 45993
IHBpZXJ3c3pl 45994
IGNsZXJneQ== 45995
bGluZ3Rvbg== 45996
bGl2 45997
VlBO 45998
INGB0L7QttCw0Ls= 45999
IEhhdGU= 46000
44Go44GT44KN 46001
z4bOvw== 46002
IFJlc3BvbnM= 46003
0L7Qt9C0 46004
IGV0bWVr 46005
IGNoZW1pbg== 46006
2YXYqQ== 46007
IOqwgOyhsQ== 46008
VHJl 46009
IHVtYXM= 46010
IEJ1cnRvbg== 46011
IHBhdHJpYXJjaA== 46012
IFNtaXRoc29uaWFu 46013
pZg= 46014
TW9vbg== 46015
QWly 46016
IG1lZGlvcw== 46017
IGVyYXNlcg== 46018
IHdvbGx0ZW4= 46019
IHBhcmVpbA== 46020
IEJpbGxpZQ== 46021
5oq9 46022
0LXRgNGC0LI= 46023
IHBhcmxhbWVudA== 46024
IGFnb255 46025
IFFVRQ== 46026
c2VxdWVudGx5 46027
QW5vdGhlcg== 46028
IFdoZXc= 46029
IEFubnVhbA== 46030
IHNlYmVu 46031
7IOB7J2E 46032
dmFsdWVz 46033
npzrp4w= 46034
IHNpbm9u 46035
ZXJlYWw= 46036
IEVubGlnaHQ= 46037
IENoZW1pc3RyeQ== 46038
IENhdGFsdW55YQ== 46039
IGRvY3Ry 46040
YW50b24= 46041
IHN0dWs= 46042
IFBsYXRl 46043
IEthcmRhc2hpYW4= 46044
IGZpbG9z 46045
IFdldA== 46046
INC/0L7Qv9GL0YI= 46047
IHVua25vd25z 46048
IFNjaG9u 46049
IEJhbGR3aW4= 46050
IHRlbGVzY29wZXM= 46051
IEd1Y2Np 46052
b3hpZGU= 46053
IENvbnNlcnZhdGl2ZQ== 46054
7ISx7J2E 46055
IGhpbmF1cw== 46056
UG93ZXI= 46057
IOqxtOqwlQ== 46058
IHByZXZhaWw= 46059
b3JtYW4= 46060
bWFjaGluZQ== 46061
IDE5NDY= 46062
IHVuYmVs 46063
IHNjaGF1dA== 46064
IHBpZWw= 46065
ZWVudGg= 46066
IG9iamVjdGl2ZWx5 46067
IGNoYWtyYQ== 46068
YXVkaW8= 46069
IGNoaWNvcw== 46070
IFZhdWx0 46071
5bCI 46072
IG1lZGljaW5hbA== 46073
IFRhaWw= 46074
V2hpbGU= 46075
IGFzcGhhbHQ= 46076
IGZyb3pl 46077
IEVL 46078
dW5jaGluZw== 46079
bm9zaXM= 46080
MjAxNQ== 46081
IEdyaQ== 46082
IG9kZGx5 46083
IE3DpHI= 46084
IEFlZw== 46085
Y29sbw== 46086
UGFy 46087
IOuTpOyWtOs= 46088
IHZpbmRlbg== 46089
IE9WRVI= 46090
IGljZWQ= 46091
IHNjb3Jw 46092
IGhhYw== 46093
cXVhbGlmaWVk 46094
INGD0LLQuNC00LXRgtGM 46095
ZXJtbw== 46096
SEVO 46097
IHNvaQ== 46098
IG11bHRpcGxlcw== 46099
IGxheW91dHM= 46100
IGJsaW5kbmVzcw== 46101
IEJvd3Nlcg== 46102
INC/0L7QtNGC 46103
IMOO 46104
dmVudGlvbmFs 46105
IG1hdGE= 46106
bWFkxLE= 46107
IGdlZXo= 46108
IGNhZGVuY2U= 46109
IHdhxbxuZQ== 46110
IENocmlzdGll 46111
dmVuZ2U= 46112
Q2FsbA== 46113
IHR1cm5hcm91bmQ= 46114
IGJsb2I= 46115
INCv0Lo= 46116
IFZvaWNlb3Zlcg== 46117
IHBlcmls 46118
IEphaW1l 46119
IEhPWQ== 46120
bGFuZQ== 46121
IHNlYmVs 46122
IER1bw== 46123
IEhpc3RvcmljYWw= 46124
IGRuaQ== 46125
IGdlbWE= 46126
eWs= 46127
IHNhYmVt 46128
4bqvbmc= 46129
IHZhcnM= 46130
IFJvbm5pZQ== 46131
IFJvbmFsZG8= 46132
IFBlcnF1w6g= 46133
bnNpbm4= 46134
aGFpcg== 46135
IHJlbGVudGxlc3M= 46136
IGx5bg== 46137
IHRyYXZlbGVy 46138
5oCO6bq85LqG 46139
bmluZQ== 46140
IGFudGlt 46141
IOy8gA== 46142
IHNub3diYWxs 46143
INGF0LDRgNCw0LrRgtC10YA= 46144
IGludGVybnM= 46145
IGNvbnN0aXR1ZW5jeQ== 46146
INCd0LDQvA== 46147
15zXnA== 46148
VkVM 46149
IHZpa3RpZ3Q= 46150
IGFwb3lv 46151
2YTYqA== 46152
IGphcmQ= 46153
IGhlaWdodGVuZWQ= 46154
0YDQvtGB0YI= 46155
IFNNSVRI 46156
INC00LXQu9Cw 46157
IHJlcGFpcmluZw== 46158
IHJpZ3Q= 46159
IFNoZWlraA== 46160
IEJyaXRuZXk= 46161
IGV2ZXJ5dGltZQ== 46162
IGFkdmVudHVyb3Vz 46163
b2NrZXk= 46164
ZXJudA== 46165
IGF0YXF1ZQ== 46166
IEFsdGVybmF0aXZlbHk= 46167
ZWZmZWN0 46168
IHBhbGF2cmFz 46169
IEVsbGlvdHQ= 46170
IHLDqXVzc2k= 46171
IGh5cGVydGVuc2lvbg== 46172
IE1hbnVhbA== 46173
IHByb3BoZXRpYw== 46174
IGhhbmRj 46175
0YzQtQ== 46176
IHJlZnJhaW4= 46177
IFNxdWlk 46178
7J6h 46179
INC60L7QvNCw0L0= 46180
w6RsbGVu 46181
IGxsZWfDsw== 46182
IGJhc2g= 46183
aW9ueQ== 46184
INGB0LrQu9Cw0LQ= 46185
INC60LDQsQ== 46186
IGNhcmVsZXNz 46187
IFBvb2w= 46188
IHRyw6Fz 46189
IGZpbHM= 46190
IFNjaHI= 46191
IHNwcmF3ZA== 46192
IE1vbmF0ZW4= 46193
IHVuZm9yZ2V0dGFibGU= 46194
IENvdHRvbg== 46195
IGluY29udmVuaWVudA== 46196
IFJY 46197
b3Jpcw== 46198
IGh1bWJsZWQ= 46199
16rXlw== 46200
INii2b4= 46201
IGluY3Jlw60= 46202
IEtvbW1lbnRhcmU= 46203
6IiS 46204
cmFjacOzbg== 46205
IHZhbnRhZ2U= 46206
IFNlYWw= 46207
IOydtOqxsOulvA== 46208
IGpvdWU= 46209
44Gd44GG44Gn44GZ44Gt 46210
IOyYpOuemA== 46211
INC40YHQv9GL0YI= 46212
b2Jlbg== 46213
IGdyYXRl 46214
IGNvbnRyb2xl 46215
IFBlcmN5 46216
xYJhZGE= 46217
IHNpbXVsdGFuZW91cw== 46218
IHByb3RvdHk= 46219
IGdyb8OfZXI= 46220
IGJld3Vzc3Q= 46221
aW5pemk= 46222
IHBhc3NpZXJlbg== 46223
IEhhcHBpbmVzcw== 46224
5YmH 46225
c2hp 46226
Z2VodA== 46227
IHN0YXRpb25lZA== 46228
IEVyZ2Vibmlz 46229
IGRpcmVjdGFtZW50ZQ== 46230
IHN1cnZpdmVz 46231
IHBlcnNvbmVz 46232
QkVSRw== 46233
IHZvbWl0aW5n 46234
IGNvbmhlY2Vy 46235
IGFkam91cg== 46236
IENpdmlj 46237
cGVp 46238
YnVyc3Q= 46239
IOuLpOuLiA== 46240
6Y8= 46241
IHNsZWQ= 46242
IHBsYXRhZm9ybWE= 46243
IFNlY3Q= 46244
IERlZmlu 46245
55m76Yyy 46246
w6lub20= 46247
Y2huZXQ= 46248
IHByb2ZpdGFiaWxpdHk= 46249
IGVycmVpY2h0 46250
4buPaQ== 46251
Y2F0aW9u 46252
IOyngOq4 46253
IHBlcmRyZQ== 46254
IGZlbG9ueQ== 46255
IDE5NTc= 46256
5oiR5b6I 46257
IHVuc3VjY2Vzc2Z1bA== 46258
IG5hZ3lvbg== 46259
IGVsYXN0aWNpdHk= 46260
IGZhY2FkZQ== 46261
IGVhcnRobHk= 46262
INCw0LzQtdGA0LjQutCw0L0= 46263
IGNvbm4= 46264
Y2xh 46265
RHU= 46266
IHBvbGl0aXF1ZXM= 46267
IGhhbG8= 46268
aWFudGVz 46269
INC80L7QtdC5 46270
44Oz44OJ 46271
dG9uZXM= 46272
ZWxpZXI= 46273
6K6a 46274
aHRha2luZw== 46275
IHdpY2h0aWdl 46276
IGFubm8= 46277
IExvaw== 46278
aWxsaW9ucw== 46279
IHZpdmVy 46280
IHNvbGNoZW4= 46281
IHN1Zg== 46282
IFNhbHo= 46283
IE52aWRpYQ== 46284
enVnZQ== 46285
IFNwaWtl 46286
VmlkZW8= 46287
IHR3b3I= 46288
IEFsYQ== 46289
6JGJ 46290
IGhhbnlh 46291
IEFkbQ== 46292
7J21 46293
IFBhdGllbnRlbg== 46294
IE9uaW9u 46295
IEtvYmU= 46296
IFNjZW5l 46297
IFJhc2g= 46298
5qiZ 46299
0YDQsNGB0YI= 46300
aXN0YW5p 46301
R2VuZXJhbA== 46302
bGV5ZQ== 46303
aW1iYXA= 46304
IGNvbmNlYWxlZA== 46305
IEZyaWRheXM= 46306
IFdvb2w= 46307
INC90L7QstGL0YU= 46308
2LTYsQ== 46309
IOqysOqzvA== 46310
IGplZG9jaA== 46311
tOyLnA== 46312
k6Trj4Q= 46313
IOyepeuCnA== 46314
dWt0 46315
TG91 46316
IOuoueyWtA== 46317
IEV4cGVjdA== 46318
INC00L7QvNC+0Lk= 46319
IGlycmVzcG9uc2libGU= 46320
IGFjZXJjYQ== 46321
IFp1c3Q= 46322
16jXmA== 46323
VUk= 46324
IHlvdXR1YmVycw== 46325
IFBvc2l0aXZl 46326
IHNvY2lvZQ== 46327
IHNuYXRjaA== 46328
6IOM 46329
IHJlZnJlc2hlZA== 46330
IG5vbWluYXRpb25z 46331
IFBhdHQ= 46332
IG9ic29sZXRl 46333
IGRlbWnFnw== 46334
5Y+k 46335
b3JtdcWf 46336
IOyGlOynge2eiA== 46337
IGZsYQ== 46338
IGNyYXppZXN0 46339
IFppZQ== 46340
IFTDug== 46341
emVw 46342
aWNlbQ== 46343
IOupi+yeiA== 46344
IGN5bmljYWw= 46345
44Gd44KT44Gq 46346
IHRyZXNw 46347
IGNyYXo= 46348
1aXV 46349
IG5lbGxl 46350
IG1waA== 46351
IE5lcmVk 46352
IEtvYg== 46353
IEVjaw== 46354
qLjri4g= 46355
SmFu 46356
INCi0L7Qs9C00LA= 46357
IGRlY2k= 46358
IFZvZw== 46359
IGJ1YmJsaW5n 46360
6YCA 46361
w7ph 46362
IHByb2R1Y3Rvcw== 46363
aWJlcmFs 46364
IHJlcGxpY2F0ZWQ= 46365
IEltcHJvdmU= 46366
aWxsYXJ5 46367
Q2hh 46368
IHLDqWR1 46369
g5DtlZjrqbQ= 46370
IGNvbm5vdA== 46371
IEtyaXQ= 46372
INC00YPRhdC+0LI= 46373
IHRyZWFkbWlsbA== 46374
IFBX 46375
INC30L7QstGD0YI= 46376
IGNsYW1z 46377
IGRyYWZ0aW5n 46378
IDE5NTY= 46379
dW50YQ== 46380
IGV4cGVuZGl0dXJlcw== 46381
IEhvb3Zlcg== 46382
V09P 46383
0YjQtdC1 46384
IGRlZHVjdGlvbg== 46385
bW9uYXJ5 46386
IHJlY2li 46387
IHBvdm8= 46388
IOuNlOs= 46389
IFBBTA== 46390
IEJsb3c= 46391
IHd5cA== 46392
IGRlc3RhYw== 46393
ZGVhbA== 46394
R3JhZW1l 46395
IG7DqWNlc3NhaXJl 46396
IGRhbW5lZA== 46397
IDE5Mzg= 46398
IOyLpOygnOuhnA== 46399
IHRyb29w 46400
IGluc2lnaHRmdWw= 46401
IFRK 46402
INC+0YHQsg== 46403
IGZpZGVsaXR5 46404
IFNraXA= 46405
IE1heW8= 46406
66ed 46407
YXBwZQ== 46408
IGJsYXM= 46409
IFdZ 46410
IEdO 46411
Y3Rhcg== 46412
U3U= 46413
IGN1ZW50 46414
aGV3cw== 46415
IGNvcnBzZXM= 46416
QWJz 46417
IHdhc3Rld2F0ZXI= 46418
IGNpZWs= 46419
IE9udQ== 46420
IGV4cGxvc2l2ZXM= 46421
IGFybWE= 46422
IFNURVBIQU4= 46423
cG9saXRpaw== 46424
IE9zYWth 46425
dGHFgg== 46426
IHlhcMSxeW9y 46427
IGl6cXVpZXI= 46428
IGJlbGV6YQ== 46429
IFd5YXR0 46430
5ZC4 46431
IHN1aw== 46432
IHNwZWNqYWw= 46433
IGRhbmtl 46434
d2hpc3RsZQ== 46435
IGbDrXNpY2E= 46436
IEhhcnJpZXQ= 46437
IOyVhO2MjA== 46438
IHdpbGxrb21tZW4= 46439
aXBpbmc= 46440
INGB0LzQvtGC0YDQuNGC0LU= 46441
INC80L7QttC10YjRjA== 46442
IGluYWNjdXJhdGU= 46443
IGFycm9nYW5jZQ== 46444
IFJlbW8= 46445
zrPOrA== 46446
YXNzZWQ= 46447
IGRlbGl2ZXJpZXM= 46448
IHN0aW5reQ== 46449
INC/0LXRgNC10LY= 46450
amF5 46451
IHRyYW5zaXRpb25hbA== 46452
IHJlcmU= 46453
IE5HT3M= 46454
IEFUTQ== 46455
2K7Yqg== 46456
aW9sb2d5 46457
INCy0LvQsNC0 46458
IHNjaG1l 46459
IFNoaW5l 46460
7JWh 46461
cGFudHM= 46462
IHNlcmdl 46463
IHNlbmhvcg== 46464
IGFiZHVjdA== 46465
IEJyeWFudA== 46466
VkVT 46467
IGF3YWtlbmVk 46468
IExheg== 46469
cm9wb2xpcw== 46470
IExhbw== 46471
6L6b6Ium 46472
IHZpbGxh 46473
IHN1bW1lcnM= 46474
IGVudGhhbA== 46475
IDE5NDk= 46476
Vmlh 46477
IOyWtOyo 46478
IHRlbmRvbg== 46479
IHZpb2xldA== 46480
IGludGVsbGVjdHVhbGx5 46481
IGJvdW5jZWQ= 46482
YXJhdXM= 46483
IDE5MTk= 46484
IHZyYWFn 46485
IHNwZWw= 46486
IFNjaHdhcg== 46487
U2NvdHQ= 46488
IEluZG8= 46489
IOunnQ== 46490
IGNhbm9uaWNhbA== 46491
IElLRQ== 46492
IHRoYXTDrXM= 46493
IG1lbGxhbg== 46494
5q+S 46495
aWdtYXQ= 46496
Q291bGQ= 46497
Li4uPyk= 46498
IGZvYXJ0ZQ== 46499
IEt1bWFy 46500
cmVuZG8= 46501
IMOpbMOp 46502
4LQ= 46503
dmFsdWF0aW9u 46504
Y2FzZXM= 46505
IGludHVpdGl2ZWx5 46506
aG9uZw== 46507
ZXR0ZWQ= 46508
IHNvdXZlbg== 46509
IG1vcmI= 46510
IGNvcnM= 46511
IE5W 46512
IEhhc2Fu 46513
5oOF5Ya1 46514
aWV2ZWQ= 46515
IOyngOq4iOydgA== 46516
IGR1bXBsaW5n 46517
IGNvbnRyw7RsZQ== 46518
IGFtYmlndWl0eQ== 46519
5qmf5pyD 46520
IGNvZw== 46521
IFNjcmlwdHVyZXM= 46522
IGNhaQ== 46523
IGJldmVy 46524
5aSn5a626YO9 46525
IGh1aXM= 46526
IGFpbWU= 46527
IGVya2zDpHJlbg== 46528
IExN 46529
IEZleQ== 46530
6Zq+ 46531
4K6x4K6k 46532
IHN1cGVydmlzZWQ= 46533
IGpld2U= 46534
c3Bs 46535
INGG0LXQvdGC0YA= 46536
IGNvbGxpc2lvbnM= 46537
2YTZgQ== 46538
IEhvZ3dhcnRz 46539
IER1cmhhbQ== 46540
15XXow== 46541
IHBob3NwaGF0ZQ== 46542
IG92ZXJzZWU= 46543
IGluc3BlY3Rpb25z 46544
IGJyaW5j 46545
IFphaw== 46546
IHBheW9mZg== 46547
IGNoYXVk 46548
IEh1bmdlcg== 46549
w6Nvcw== 46550
dmly 46551
IGZpYW5jZQ== 46552
IGJvdWc= 46553
bGl2ZWQ= 46554
Y3J5 46555
5Zue5L6G 46556
IGpvaW50bHk= 46557
IGdpcmxmcmllbmRz 46558
IE5leHVz 46559
pqzqsqDsirXri4jri6Q= 46560
IEt3YW5n 46561
5ZOI5ZuJ 46562
5aeR 46563
xYLEmQ== 46564
IE5lZGVu 46565
aWVjZQ== 46566
IGluc2VydGluZw== 46567
5p+T 46568
IE11bW15 46569
IEdsb2Jl 46570
IGxlZQ== 46571
IGdlcm1hbg== 46572
IGNyZWFtcw== 46573
YWNobw== 46574
IGNoxrBh 46575
IEdhbGlsZQ== 46576
IGbDvHJz 46577
IGVzdGl2ZXI= 46578
Y2lkb3M= 46579
Q2hyaXN0aWFu 46580
IGxvcnNxdQ== 46581
IGN1dGVzdA== 46582
dmFsZQ== 46583
INC60YDQtdC/ 46584
IHdhcnk= 46585
IHNsaWNpbmc= 46586
IGVzcGVyYW5kbw== 46587
IFZhbmRlcg== 46588
IERlaXhh 46589
IDE5NTQ= 46590
IG3Ds3dpxIU= 46591
0ZbRlA== 46592
IHRvb2xpbmc= 46593
IHJlc3Rvcg== 46594
IHBvc2ljacOzbg== 46595
IGludGVudGFy 46596
IEFwYWNoZQ== 46597
T1VM 46598
INmI2Kg= 46599
IG1hdGnDqHJl 46600
44O844KT 46601
IGxpbmVu 46602
IGVzdHJhdMOpZw== 46603
IE11dHRh 46604
6aGv 46605
6KGM5LqG 46606
IHBhcnRpbmc= 46607
IG1pbmltaXppbmc= 46608
IGFwcHJlbmRyZQ== 46609
5pyd 46610
INCw0L3Qs9C70LjQuQ== 46611
IERvbw== 46612
IEZpcmVmb3g= 46613
Y8OzbW8= 46614
IGdlb3BvbGl0 46615
IG1ha2Fu 46616
IG1vZ2VsaWpr 46617
IM+AzrXPgc65 46618
IGPhu6k= 46619
IGluc3RhbGxlcg== 46620
IGRpYnVq 46621
IEhlYXRo 46622
bG9vcA== 46623
IEJyb2tlbg== 46624
SFlVTg== 46625
c2hlbGY= 46626
IGZpemVy 46627
IGVuaGFuY2Vz 46628
5L6L44GI44Gw 46629
INC00L7RgdGC0Lg= 46630
IFBVQg== 46631
IEtvbGxlZ2lu 46632
IGF0dGFpbmVk 46633
xL4= 46634
IG1pc3RyZXNz 46635
IE9mdGVudGltZXM= 46636
157Xmded 46637
IGJld2U= 46638
IFNvcmE= 46639
cmF1ZW4= 46640
YmF1bQ== 46641
IHJvbGxlcnM= 46642
IG1lcmluZw== 46643
IFBBQw== 46644
INC90ZY= 46645
IFLDqXB1YmxpcXVl 46646
INGC0YDQsNCy 46647
IFZhbmd1YXJk 46648
dWNpb25lcw== 46649
IOustOuMgA== 46650
IGdvdXI= 46651
r6Q= 46652
IM+J 46653
IHNhdW5h 46654
IHBlaW5l 46655
IFZhbGVyaWU= 46656
IFNpa2g= 46657
ZmVuZGltaXo= 46658
YmVybw== 46659
INGH0Lg= 46660
IGRvxZt3aWFk 46661
IEV1cm9z 46662
IGNvbW1lbnRhaXJlcw== 46663
IHR3ZWFrcw== 46664
IEZhc3Rlcg== 46665
INGA0LDRgdC6 46666
IHByb2dyZXNzaXZlbHk= 46667
IEV1Y2g= 46668
Ym9ybw== 46669
IEluZ3JlZA== 46670
Q2Fw 46671
IHVuY2hlY2s= 46672
IOyYpOuluA== 46673
IHdyZQ== 46674
IEZU 46675
w7ZydW5n 46676
IG1lbW9yaXplZA== 46677
IERpbm5lcg== 46678
IFBoZXc= 46679
b3VibA== 46680
IHB1dGE= 46681
IGFkbWl0cw== 46682
0LXQt9C00LU= 46683
b3BvZA== 46684
IHBhbmRh 46685
IGhpbmdlcw== 46686
Y2lwZQ== 46687
IHRyYW5zYWN0 46688
IHBvZGlh 46689
IHBpY3M= 46690
IGNyaXRlcmlvbg== 46691
IE9yY2hlc3RyYQ== 46692
IEJsb2c= 46693
IHNvbGVtbg== 46694
IFBpeGFy 46695
VGhyZWU= 46696
INCy0L3QuNC3 46697
IFZvbHVudGU= 46698
IFNhdmFnZQ== 46699
IFBWQw== 46700
IENhZg== 46701
IHd5a29u 46702
IGdyYWRlcnM= 46703
IGNyb3VjaA== 46704
IGNsaWNoZQ== 46705
IHNveWJlYW5z 46706
IE1VUg== 46707
IEdvbnphbGV6 46708
IE1pbWk= 46709
IEJvbHNvbmFybw== 46710
IGRpYXBocmFn 46711
IGJpbGFuZw== 46712
65CY64qU 46713
6YKj5oiR5YCR 46714
IHJlZ3VsYXRpbmc= 46715
TWM= 46716
SnVkZ2U= 46717
INC90L7Qtg== 46718
IGpha8SF 46719
aXRlc3Nl 46720
IFdpag== 46721
IGxhdGE= 46722
Z3JvYW5pbmc= 46723
UE9TSU5H 46724
INeQ15XXqteV 46725
IGhhZ2E= 46726
IGdyb3VuZGluZw== 46727
IHZpb2xlbnRseQ== 46728
IHRpbGxz 46729
IGVuZ2Fn 46730
IEhvbGxvdw== 46731
INC/0L7Qv9GD0LvRj9GA 46732
IHdwcm93YWQ= 46733
IHJlcGxhY2Vz 46734
IGZsdW9yZXNjZW50 46735
dXJnaWNhbA== 46736
aWdnbHk= 46737
IFRyYWRpdGlvbmFs 46738
dHRl 46739
INmE2Yc= 46740
IHBob3NwaG9ydXM= 46741
IGFwcm9u 46742
IFdhdGVycw== 46743
IEt1bHR1cg== 46744
0LDQstCw0Lk= 46745
IG9saXZlcw== 46746
INeU15DXnA== 46747
IHRlaWx3ZWlzZQ== 46748
IHNlbmNpbGw= 46749
IHByZW5kcw== 46750
IG5hcnJvd2Vy 46751
IGrDpHR0ZQ== 46752
IEluZm9ybWF0aW9uZW4= 46753
7IOB7J20 46754
IHN0YXJ2ZQ== 46755
IGZyaWNr 46756
IEJld2Vn 46757
4KSy 46758
IGRvbHBoaW4= 46759
IExBVUdIVEVS 46760
IElOVEVSVklF 46761
5ZSJ 46762
IHlhbmzEscWf 46763
IHRvcnBlZG8= 46764
IHNob3J0YWdlcw== 46765
7J2065Oc 46766
xLFsZMSx 46767
IHBhd3M= 46768
IG96b25l 46769
IGN1bHRpdmF0ZWQ= 46770
IEZvdA== 46771
IG5vdG9y 46772
0L3QvtC3 46773
INC60L7RiA== 46774
IHRvdWNoc2NyZWVu 46775
IEFsbHk= 46776
5pyA6L+R 46777
IOunm+yeiOyWtOyalA== 46778
INCh0LXRgA== 46779
INCy0L/QvtC70L3QtQ== 46780
IHBhcHJpa2E= 46781
IER1c3Rpbg== 46782
IGVmZWN0bw== 46783
IG9waW5p 46784
IG11dXQ= 46785
IGjhu41j 46786
IGludGVyamVjdA== 46787
xJl0 46788
IGJ1dHRz 46789
dXJleg== 46790
IFBpa2U= 46791
IEhvaw== 46792
IEd1aW5lYQ== 46793
IENhdGhlZHJhbA== 46794
IDE0MDA= 46795
Q3Jh 46796
Kyw= 46797
66eb 46798
s7Trj4TroZ0= 46799
YWJ5cmlu 46800
IHZpZGVvZw== 46801
INC+0YDRg9C2 46802
IHXFvg== 46803
IGJ1c2NhbmRv 46804
IEFzc2lzdGFuY2U= 46805
6Zm9 46806
IG1lbGhvcmVz 46807
7KG0 46808
IOuBvA== 46809
IFJK 46810
INiq2YU= 46811
IG9taW4= 46812
IG1vdG9yY3ljbGVz 46813
IFNhcHA= 46814
IHN1cHBseWluZw== 46815
IEFsZ3Vu 46816
IGFlcm9zcGFjZQ== 46817
16LXnA== 46818
b2NjdXA= 46819
bGVpc3Q= 46820
IOqxsOuKlA== 46821
IGNvbXBsZXRh 46822
YnJlcw== 46823
ISg= 46824
INCf0YDQtdC0 46825
IGRpc2FkdmFudGFnZWQ= 46826
IEF0dGVuZA== 46827
IEp1ZGFo 46828
4buLY2g= 46829
eWxlbmU= 46830
YWN0bHk= 46831
IHNldHVwcw== 46832
IGFtbW9uaWE= 46833
IFNjaHdlaXo= 46834
IFNoYW1l 46835
IGJhbmRl 46836
IEZ1ZWw= 46837
IHRyb3VibGVzb21l 46838
IG51bWVybw== 46839
IE1PTQ== 46840
INC/0YDQtdC00LvQsNCz 46841
bWVudGlvbmVk 46842
INCx0L7Qu9GM0YjQvtC1 46843
IFZpa3Rvcg== 46844
IFN0eWxlcw== 46845
IGNydWNpZmllZA== 46846
cnVjdHVyZWQ= 46847
ZW52aXJvbg== 46848
IG1vcmFscw== 46849
IG1lZGl0YXRpbmc= 46850
IGF4aWFs 46851
aXNhbmNl 46852
IEFic3Q= 46853
R3JlZW4= 46854
IOqxtOw= 46855
IHF1YWRyYW50 46856
IHBlcmdp 46857
IGNhbWVyYW1hbg== 46858
IFNlcXU= 46859
IHBhdXNlZA== 46860
IExhdWdoaW5n 46861
6reA 46862
Py4u 46863
IMW7ZQ== 46864
IHBlcm1pdGly 46865
IGRldGVjdG9ycw== 46866
IEhVRA== 46867
YXZhbA== 46868
IOyXrOq4sOq5jOyngA== 46869
IGh1YnM= 46870
IGJlc3RpbW10 46871
INCx0YPQtNC10YLQtQ== 46872
SU5URVJQT1NJTkc= 46873
IHRlbmdhbg== 46874
IGNyYXZl 46875
IEJ1bmRlc3JlZ2llcnVuZw== 46876
IEJsb29keQ== 46877
IHVzYWJpbGl0eQ== 46878
IEVhcw== 46879
IMSR4buZbmc= 46880
IDE5NTU= 46881
IGtyaWVnZW4= 46882
IGhhYml0dWFs 46883
IGVzc2VudGlhbHM= 46884
cmltaW5hbA== 46885
IHJvb21tYXRlcw== 46886
6YKj5bCx 46887
INC/0LXRgNC10YXQvtC0 46888
IG5naGk= 46889
IG1lbmluZw== 46890
IFN5bXBob255 46891
IEh1Zw== 46892
YWdnaQ== 46893
IHdpZWQ= 46894
IG1pdGFk 46895
44Gj44Gm44GE44GG 46896
dGVlbnRo 46897
aWRhxIc= 46898
U2F2ZQ== 46899
IHJvYmnEhw== 46900
IGJvdW5jZXM= 46901
sJbsl5A= 46902
c3RhcnM= 46903
IHByYWdtYXRpYw== 46904
IGNvZ25pdGlvbg== 46905
IHdyYXBwZXI= 46906
IHdhcnRlbg== 46907
YWRo 46908
IHBlbnNh 46909
IEhlcnR6 46910
IG7Emw== 46911
IFJlaWQ= 46912
IFBDcw== 46913
IE1vbGU= 46914
IC4uLi4u 46915
IHByZWNpbw== 46916
IENoYW1waW9uc2hpcHM= 46917
6rCA6529 46918
IHbDqXI= 46919
IGNvcnJpZG9ycw== 46920
IEVsZWN0cm9uaWM= 46921
U2w= 46922
INCw0LvQtQ== 46923
IG92ZXJ0aHJvdw== 46924
IGthYnVs 46925
IFJFUw== 46926
IEN5YmVycHVuaw== 46927
0L7Qs9C+0LQ= 46928
INCd0LDQsg== 46929
IHdhbg== 46930
IG1hbmlmZXN0YXRpb25z 46931
IGN1YWxlcw== 46932
IFdpc2U= 46933
IEzDtnN1bmc= 46934
IGV4Zm9s 46935
IGVhcm5z 46936
0YPRgdGC0LjRgtGM 46937
IHNhcHA= 46938
IEJyYXVu 46939
IEJSQU5ET04= 46940
7LmZ 46941
IHNhbm8= 46942
IEZFTA== 46943
0YvQstCw0LnRgtC10YHRjA== 46944
0L7QttC00LXQvdC40Y8= 46945
IHNld24= 46946
RnVu 46947
IHJlY2lwcm9jYWw= 46948
IGV4cGFuc2l2ZQ== 46949
IFRyYWZmaWM= 46950
IGt0w7NyZWdv 46951
INmI2LM= 46952
5pil 46953
IOu5qA== 46954
cHJvdmU= 46955
aWdhcmU= 46956
IGxvaA== 46957
2KfYtg== 46958
SG9wZQ== 46959
IGRldm90ZWVz 46960
IEdvbQ== 46961
IHN0ZWFscw== 46962
IFVtcw== 46963
IFR3aWNl 46964
44Ky 46965
aXlpbQ== 46966
IHJoeXRobWlj 46967
IFZvcnRl 46968
IHByZWZpeA== 46969
b21pbmF0aW9u 46970
IGRhdG8= 46971
IGN1c3RhcmQ= 46972
IFZPSUNF 46973
5bee 46974
IG1lbnk= 46975
aXN0b3Jz 46976
IO2YkQ== 46977
IOyCtOyVhA== 46978
IO2DhA== 46979
IGtvcnQ= 46980
IGFiYQ== 46981
IFZlcmE= 46982
ZXB5 46983
IOy5tOuplOudvA== 46984
IHN1Ym1lcmdlZA== 46985
IENsb2Nr 46986
IHRodW1ibmFpbHM= 46987
IGJvYXN0 46988
IEZhcmU= 46989
ISFd 46990
IMWbbQ== 46991
IGthaWtraQ== 46992
IFRlY2hub2xvZ2llcw== 46993
7Jm4 46994
44OS 46995
0LjRgtCw0Lk= 46996
5bCP5pmC 46997
INCw0YI= 46998
IGtub2Jz 46999
IHJlaWNodA== 47000
xrDhu6NuZw== 47001
Z2xpbw== 47002
IOunm+ydtA== 47003
6rCQ7J2E 47004
IGpvdGth 47005
IEhhbmR5 47006
IEhhYmVu 47007
bm91cw== 47008
IGlubGFuZA== 47009
IGFtYXpvbg== 47010
aG9vdGluZw== 47011
U0w= 47012
IGxlaXN0ZW4= 47013
fiI= 47014
IHByb3Zva2U= 47015
IFR3aXN0 47016
INeR15c= 47017
IGRlcGFydGVk 47018
6rCc66W8 47019
IGtvbnNl 47020
IENhcnd5bg== 47021
7ZWY7Iug 47022
aWRlbnRhbA== 47023
RVNDTw== 47024
IHR0ZW9rYm9ra2k= 47025
IGRpemVuZG8= 47026
57e0 47027
xLFuZGFraQ== 47028
aW1hc3U= 47029
YWZhcg== 47030
IGxhbmRmaWxs 47031
IGNvcnJlY3Rpbmc= 47032
IGNsZWFycw== 47033
IE51bW1lcg== 47034
SEFN 47035
IGNhcnRyaWRnZXM= 47036
IERpZXNlbA== 47037
cGFjZWQ= 47038
IG9ibGl2 47039
IG1veWVucw== 47040
IFNpbm5l 47041
IFByZWlz 47042
aWxpeg== 47043
INGB0LzQvtC2 47044
IGJyb2FkZW4= 47045
5LuW5piv 47046
eGVz 47047
IGNhcmJvaHlkcmF0ZQ== 47048
7Zi5 47049
c2Vvaw== 47050
IGVjaG9lcw== 47051
IGNlc3M= 47052
67CU 47053
INCx0LjQt9C90LXRgQ== 47054
IGxsYW1hZG8= 47055
IGVzc2VudA== 47056
IOydvOuwmA== 47057
IEFpcmVz 47058
cGhlbg== 47059
IHplYnJh 47060
IHN5bWJvbGlzbQ== 47061
T25jZQ== 47062
IHJhY2tz 47063
IEthZmth 47064
INGB0LXRgNGM0LXQtw== 47065
IHNpbm4= 47066
cGljaW91cw== 47067
a2Fh 47068
IG1vdGhlcmZ1Y2tlcg== 47069
IGFwcHJlbnRpY2VzaGlw 47070
IHJwbQ== 47071
IHRheGF0aW9u 47072
IGZ1cnJ5 47073
IFNhY3JlZA== 47074
INGA0LDQt9C8 47075
cG9yYQ== 47076
ZW5nZXM= 47077
IO2XiOs= 47078
INGB0LjQvQ== 47079
IHNhbml0aXplcg== 47080
IGNyaW5nZQ== 47081
IFNjYQ== 47082
0L7Rh9C90L4= 47083
IG9mZXJl 47084
IG1lbG9kaWVz 47085
IFZlbHZldA== 47086
IElocmVy 47087
IEh5YnJpZA== 47088
IEdpb3Y= 47089
IGlyZ2VuZHdhcw== 47090
IGRlcGVuZGU= 47091
IFVzZXJz 47092
IGh1bXA= 47093
ZHJpdmluZw== 47094
IHNm 47095
IHJ1dGhsZXNz 47096
4LmA4LiE 47097
IGxlbW9ucw== 47098
IGbDtnJldA== 47099
IE9q 47100
INC80LDQvNCw 47101
IGludGVycGVyc29uYWw= 47102
IGdldg== 47103
IGFibm9ybQ== 47104
0LjRgdC7 47105
INC40L3QtA== 47106
IGtvbnRyb2xs 47107
IHJlZ3Jlcw== 47108
IGxlZGdl 47109
IGVyesOkaGx0 47110
IFRhY3Q= 47111
IGFycml2w6k= 47112
IHN1YnN0YW50aXZl 47113
IHNwb29uZnVs 47114
endpc2NoZW4= 47115
b29vb28= 47116
IGNvbnRlbmlkbw== 47117
IGJlc2w= 47118
4buDbQ== 47119
a3Rlbg== 47120
SmFtaWU= 47121
IHNhbmR5 47122
5LiN5ZCM 47123
4os= 47124
IHBhc2U= 47125
IGRldHRl 47126
IEJlbGdpYW4= 47127
6rCc6w== 47128
dWxhcmVz 47129
cnVk 47130
aWdvcg== 47131
IO2MrOs= 47132
IHJlbWVkaWVz 47133
IGJsYXN0aW5n 47134
IFNpY2g= 47135
INC+0LbQuNC0 47136
IG1vbnN0cg== 47137
IG1hbmlmb2xk 47138
IGdsYXViZW4= 47139
IEVTVA== 47140
IHN0cmVhbWxpbmU= 47141
IGxvYmJ5aW5n 47142
IEdvdGhpYw== 47143
dG9pcmU= 47144
Li4n 47145
IGTDqW1vY3I= 47146
INC90LDQsdC70Y7QtA== 47147
IHdzcMOzbA== 47148
IGN6xJnFm8SH 47149
5LiL6Z2i 47150
aXPDqXM= 47151
Z2FuZ2Vu 47152
IGJlenBpZQ== 47153
cmVtbGlu 47154
6rCd 47155
U3RpbGw= 47156
IHJlc2lkZXM= 47157
IGdlbGVjZWs= 47158
IHTDqWzDqXBob25l 47159
IHBld24= 47160
IGxlb3BhcmQ= 47161
IGNvbXBsaW1lbnRhcnk= 47162
IGNyaWI= 47163
IEFuaW1hbHM= 47164
IGdlaWw= 47165
ZXNzZWw= 47166
IGdhcmRlcg== 47167
IGNhdGNoeQ== 47168
5qi5 47169
IEV0cw== 47170
IENvbW1lcmNpYWw= 47171
IERFTk5JUw== 47172
IENvb3JkaW5hdG9y 47173
IEFiaWdhaWw= 47174
ZmZmZmZm 47175
4bqlcA== 47176
IHBlcXVlw7Fh 47177
IGluamVjdGlvbnM= 47178
Y2VrdA== 47179
IHBoaWxhbnRocm9weQ== 47180
IHB1Y2s= 47181
IGNlbGVicmF0ZXM= 47182
IER1bms= 47183
IERsYXRlZ28= 47184
44G+44Gg 47185
zrTOrg== 47186
Z3JhZHVhdGU= 47187
IE1vYmls 47188
dGlsbA== 47189
YWNhbQ== 47190
IHlvbGtz 47191
IHRhbmdsZWQ= 47192
IG1hbmlhYw== 47193
IG9ibGlnZWQ= 47194
IExhaW5r 47195
IHZlcmRlcg== 47196
IERhbW9u 47197
IG11dGFudA== 47198
IGhvcHBpbmc= 47199
IHJlaW5z 47200
IGludmVydGVy 47201
IGNvbnRlbXB0 47202
16DXoQ== 47203
bGVhcm5pbmc= 47204
TWlzcw== 47205
INCT0L7RgQ== 47206
IE1leWVy 47207
6ruY7ISc 47208
6aOO 47209
15XXoNeZ150= 47210
YXNraW5n 47211
IHRyaW1taW5n 47212
IHRyZWFzdXJ5 47213
IHNlbnRl 47214
QXVzdA== 47215
IFVudGVyc3TDvHR6dW5n 47216
IENvbWVkeQ== 47217
IEFuYWtpbg== 47218
6bk= 47219
0YDRg9GC 47220
IEhhcmk= 47221
b2dyYXBoZXJz 47222
IG9hdG1lYWw= 47223
IEJvdHM= 47224
5LiN5LqG 47225
INC/0LDQu9GM 47226
IGFja25vd2xlZGdlbWVudA== 47227
eGlj 47228
IOq0gOyLrA== 47229
Z2FzcGluZw== 47230
IOOBlQ== 47231
IHRlcnJhY2U= 47232
IG9ybmFtZW50cw== 47233
IE1FUg== 47234
Y29tbWl0dGVl 47235
IOyXhuyKteuLiOuLpA== 47236
IHJpag== 47237
6bM= 47238
16bXnQ== 47239
bGVtZQ== 47240
IGxpYmVydGllcw== 47241
IGZlbGxhcw== 47242
IENvcHBlcg== 47243
YmVuY2g= 47244
IElkZWE= 47245
4buNbg== 47246
0YjQsA== 47247
IHZlcnNpw7Nu 47248
z4TOv8+N 47249
INCc0Lg= 47250
INC/0YDQuNC70L7Qtg== 47251
IGJveGVy 47252
IFRhbm5lcg== 47253
IE1veQ== 47254
7LmY64qU 47255
VGhy 47256
IHRpbmhhbQ== 47257
IHBvbGlzaGluZw== 47258
IGNvbnNlcXVlbnRseQ== 47259
IGFtZW5pdGllcw== 47260
IEtJ 47261
IEdSRUVO 47262
IEZyYW5raWU= 47263
0L3QuNGC 47264
aXR0ZWw= 47265
0YHQutC+0LU= 47266
dXJzZWQ= 47267
IHVwYnJpbmdpbmc= 47268
IHRo4bup 47269
IOyLneycvOuhnA== 47270
IHdoaW0= 47271
IGNoaW5lc2U= 47272
Y29uZmlkZW5jZQ== 47273
IEplZGVy 47274
44Gq44Gu44Gn 47275
YWpjaWU= 47276
IFRvdXM= 47277
IFBvd2Vycw== 47278
4burYQ== 47279
b3RoZXJtYWw= 47280
INCy0YvRiNC1 47281
cmFsZQ== 47282
2KfYrg== 47283
IOyngOybkA== 47284
IMOpcGlzb2Rl 47285
IHN1bHBo 47286
IGVuY2FyYQ== 47287
a3JhZnQ= 47288
YWxhcsSx 47289
IENvbWVz 47290
IGRpdnVs 47291
IFJ1ZG9scGg= 47292
IE11c2U= 47293
IHV0ZW5z 47294
IOyekOyjvA== 47295
IHBhbmE= 47296
IFZlZ2V0YQ== 47297
IFBIUA== 47298
IE5TQQ== 47299
ZW50aW4= 47300
IENhcm5lZ2ll 47301
2KfZig== 47302
acSZY3k= 47303
SGFycnk= 47304
IGbEsXI= 47305
0KHQvw== 47306
IGdsYWRseQ== 47307
IGF2ZXJhZ2luZw== 47308
7ZWY6rKg7Iq164uI64uk 47309
0LvRj9GO0YLRgdGP 47310
INCc0LXQvdGP 47311
IHF1b3RhdGlvbg== 47312
cmlyZXM= 47313
aXRjaGVucw== 47314
YXllZA== 47315
IHVuYXR0 47316
IFBlcmV6 47317
INC+0YLQvNC10YI= 47318
IHRhY3RpbGU= 47319
IEV1aA== 47320
aXNpbmk= 47321
YnVo 47322
IGhhdMSxcg== 47323
IOyeiOycvA== 47324
IHBvbGljeW1ha2Vycw== 47325
s7TshLjsmpQ= 47326
YWPEsQ== 47327
IM66zrk= 47328
IHJlZ2lzdGVyaW5n 47329
cmV0bw== 47330
IFNwcmlua2xl 47331
IEdyYW1teQ== 47332
YXh0ZXI= 47333
INCx0Lg= 47334
IHNpdHRlcg== 47335
IHByZWRpYw== 47336
IHRoaW5seQ== 47337
IHN0cnVt 47338
IGFnZ3Jhdg== 47339
IGFoYQ== 47340
2LHYrA== 47341
bWVsbG93 47342
IGNvbnN0YW50ZQ== 47343
IExhdXQ= 47344
aXN0b24= 47345
IHRyYW5zaXRpb25lZA== 47346
IENhbWJvZGlh 47347
44GE44GN44G+44GZ 47348
6Lef5aSn5a62 47349
YXJ0ZWQ= 47350
IG1pc2Y= 47351
IFB1bmt0ZQ== 47352
jOuToA== 47353
IHRyZW1ibGluZw== 47354
IGdlc3Bhbm50 47355
INi52YTZitmH 47356
INC90LjQutCw0LrQuNGF 47357
IOu2gOuTnOs= 47358
INGA0LDQt9Cy0LjRgg== 47359
IGl0Y2h5 47360
IGNpZW50bw== 47361
IHBsYWlucw== 47362
IGtpdHRlbnM= 47363
IGJhY2tsb2c= 47364
IFByZXNpZGluZw== 47365
cHRh 47366
IGhhdm9j 47367
IERhcnJpbg== 47368
INCb0Y7QsQ== 47369
IHNlZ3JlZ2F0ZWQ= 47370
IGdoZXR0bw== 47371
IGVybGVidA== 47372
IGRydWdpZWo= 47373
IFNpeHQ= 47374
5Y+D 47375
4Lij4Liw 47376
dWVuY2lh 47377
IO2VmOq4sA== 47378
IOuGjQ== 47379
IHJvYmk= 47380
IHBpb25lZXJz 47381
IG1pbGxpYXJkcw== 47382
IFdpdGNoZXI= 47383
IOustOyXhw== 47384
b3Jybw== 47385
bWFzcw== 47386
IGRpdmVyZ2VuY2U= 47387
IFJpdmVyYQ== 47388
IE5vb2RsZXM= 47389
IGVuZHJvaXQ= 47390
IEtvc3Rlbg== 47391
INC00YDRg9Cz0LA= 47392
IG3DrW5pbW8= 47393
IEthemFraHN0YW4= 47394
2KrZhw== 47395
INCy0L7Qt9C00YM= 47396
IGdlc2NocmllYmVu 47397
IE5pbA== 47398
0YHQutC4 47399
IEZyw7xo 47400
IGJldmVyYWdlcw== 47401
5rqQ 47402
IEdvbg== 47403
5pio 47404
QXJpbg== 47405
IEludHJv 47406
b2NhbHlwdGlj 47407
IGV4aGF1c3Rpb24= 47408
IFN0YXR1cw== 47409
IEJhdHRlcnk= 47410
w6lzeg== 47411
o7zr 47412
YWlyeQ== 47413
IOuztOyXrOuTnOs= 47414
IGRpc3Bhcml0eQ== 47415
2Yw= 47416
IFR1Y3Nvbg== 47417
IGJyaWdodGx5 47418
cHJvYmxlbQ== 47419
IGJpb21hc3M= 47420
6ZmN 47421
p4k= 47422
IGh1cmRsZQ== 47423
IHdhdmVsZW5ndGhz 47424
IDw8 47425
IHRlYW1lZA== 47426
RkZGRg== 47427
IFNsaW0= 47428
b21pYWw= 47429
IHVudmVpbGVk 47430
IFZlcmVpbg== 47431
2YLYtw== 47432
ZXN0cnk= 47433
IGNsw6Fz 47434
IGNoZWRkYXI= 47435
IGFjY3VzaW5n 47436
IFNjaWVudGlmaWM= 47437
INCx0YPQtNC1 47438
IEN5cnVz 47439
zrXPhM61 47440
hpPqs6A= 47441
IOuzhA== 47442
IGN1cmQ= 47443
IHJlZmVycmFscw== 47444
c2hpZnQ= 47445
5Y2V 47446
bmlrw7N3 47447
IG1pZXI= 47448
IGNvbmZyb250aW5n 47449
6rKD64+E 47450
YXds 47451
IHRyeWlu 47452
IOq3uOuemOyalA== 47453
IGNoaWFy 47454
IOyYpOuKmOuPhA== 47455
5pS/5rK7 47456
ZXNxdWU= 47457
IG1pc21vcw== 47458
IFNoYWs= 47459
IHNvY2lhdXg= 47460
IHBpxZ8= 47461
IGtpxZ9p 47462
IGN5YW4= 47463
aGF5 47464
YmV3 47465
Ym9k 47466
IM65 47467
IE1haW5seQ== 47468
0Y7RgtGM 47469
aGFiaXR1ZGU= 47470
INGB0L/QvtC60L7QuQ== 47471
6Lef5oiR 47472
IHByZWNvbg== 47473
IE1hbmR5 47474
8J+kow== 47475
aWxsb3M= 47476
IGdydXBw 47477
IGNydW1ibGU= 47478
IGNvbnN0cnVjdG9y 47479
ZXJ2aWNlcw== 47480
IGxpZ2h0aG91c2U= 47481
IENvbmNlcHQ= 47482
0LDQvdGC0Lg= 47483
YWx0cm8= 47484
aG9wZQ== 47485
IEFsbGVn 47486
7Ja066W8 47487
cGllY2Vz 47488
b3VudGVy 47489
IO2VmOuLiOq5jA== 47490
IOyduO2EsOs= 47491
IHbDqXJpdGFibGU= 47492
IHRocmVhZGVk 47493
YmxpbmQ= 47494
gpjrnbw= 47495
IHRyYXlz 47496
IEVkaXNvbg== 47497
IMOWeg== 47498
IFN0ZXZpZQ== 47499
IGxlbmRlcg== 47500
IGJyaWdhZGU= 47501
IGRldXRzY2hl 47502
bXVmZmxlZA== 47503
YmFydA== 47504
IGluc2FuaXR5 47505
IHNhdnZ5 47506
IHNlbnNhdGlvbmFs 47507
IGRlcmVjaG9z 47508
IE1Y 47509
INC/0YDQtdC/ 47510
IHRocmVhdGVucw== 47511
IHJlYWx0w6A= 47512
IGluZGljYXRpdmU= 47513
IGNob3Bz 47514
IGJlbmVmaXRpbmc= 47515
IFZlcm5vbg== 47516
IFN0cmFuZA== 47517
bnVu 47518
cXVlbnRseQ== 47519
MTAx 47520
IGVlbA== 47521
7IiZ 47522
cmludHM= 47523
INmF2LM= 47524
INio2K8= 47525
INC/0L7RgdGC0YDQvg== 47526
IHlhcG3EscWf 47527
IG9sbWFzxLE= 47528
IGllZGVyZWVu 47529
b2zDqQ== 47530
a2Vm 47531
IOuwnOyDnQ== 47532
IHJhaW5lZA== 47533
IGFsbWlnaHR5 47534
INCy0YvQtA== 47535
IENQUg== 47536
RnJl 47537
IGluaGFiaXRlZA== 47538
IGFyYmV0cw== 47539
IGFraW4= 47540
0LDRgdGC0LI= 47541
dmFuaWE= 47542
IGjDpHVmaWc= 47543
IE1hdHRl 47544
c29ycnk= 47545
SmVubnk= 47546
INCz0YDQsNC0 47547
IHdoaXQ= 47548
IGJyb2tlcnM= 47549
5a+f 47550
IGhpbmU= 47551
YXN0ZW4= 47552
INCz0YDRgw== 47553
TUI= 47554
IFBSSQ== 47555
U2Fi 47556
IHdyZXN0bGVy 47557
IGZhY2lsaXRhdGluZw== 47558
IGVoa8Ok 47559
IENyZWQ= 47560
IDEyNw== 47561
IG5vdGhpbg== 47562
IG1hbmRhdGVk 47563
5a+M 47564
0YPRgtGB0YLQsg== 47565
RnJhbms= 47566
IHdvcnM= 47567
IGR6aWXFhA== 47568
IFVuZGVyZ3JvdW5k 47569
IHpuYWpkdQ== 47570
IELDpA== 47571
IFByaW56aXA= 47572
0LDRgtC10LvQtdC5 47573
IHZldGVyaW5hcg== 47574
IHNwbGVuZGlk 47575
IHJvenA= 47576
IHBzeWNob3BhdGg= 47577
aWdvbg== 47578
IGhvcHM= 47579
IGPhuqdu 47580
IFhpYW4= 47581
IHRyb2lzacOobWU= 47582
IHByb2R1Y3Rv 47583
IGRlxJ9lcg== 47584
IENvbnRpbnVpbmc= 47585
0LjQstCw0Ls= 47586
Y8Sxaw== 47587
IG1vaXN0dXJpemVy 47588
V2hpdGU= 47589
IHNpaXM= 47590
IEV2ZXJlc3Q= 47591
aWVuY2Vk 47592
IGPhuqNt 47593
IEphcG9u 47594
tOyghA== 47595
IHRlbsOtYW4= 47596
IGVuY2FudGE= 47597
TW0= 47598
IGRyb3Bkb3du 47599
IEl5YQ== 47600
s7TrqbQ= 47601
IHdvcmRpbmc= 47602
IFNxdWVlemU= 47603
IE1hcGxl 47604
IGNsYXJpZmllZA== 47605
IE11bmljaXA= 47606
IFJvdWdl 47607
IE5pY2tp 47608
IEdvbw== 47609
dm9sdA== 47610
dGVr 47611
ZmVjdHVyZQ== 47612
ZnJlZA== 47613
YXJyaXZl 47614
44O844GE 47615
dGV6 47616
RXA= 47617
IG9icmFz 47618
IFZJRA== 47619
IFJpdg== 47620
IE1vZGk= 47621
aWJl 47622
IGFjb250ZWNlbmRv 47623
IGltaXRhdGlvbg== 47624
IGNhbW91ZmxhZ2U= 47625
IHNwYW5uaW5n 47626
IFNFQ1JFVA== 47627
IE9yZW8= 47628
7IaM66as 47629
IGh1bmNo 47630
IGNhxYJl 47631
IHNwb250YW5lb3VzbHk= 47632
IFBlcmQ= 47633
IGV0YXA= 47634
IEhvbGU= 47635
IERpc2FiaWxpdHk= 47636
IGFmdGVybGlmZQ== 47637
5oGp 47638
IHRlc3RpZmllZA== 47639
IHByZXN1cA== 47640
IHBldHJvbGV1bQ== 47641
IGNvbnRyYXJpbw== 47642
IEFzc2Vzc21lbnQ= 47643
xJ9sdQ== 47644
IHBlc3Rz 47645
IGRpbGln 47646
INCy0YHRgtGA0LXRgg== 47647
IGNvbnPDqXF1 47648
IGNhbm5vbnM= 47649
IGNhbm9l 47650
IE1pbGU= 47651
IGNpdG95 47652
IGJlZ2dlZA== 47653
IE1pbm5pZQ== 47654
xYJ5Y2g= 47655
IHByaW5jaXBl 47656
z4DPjM69 47657
bW5pZWo= 47658
IHdlcnQ= 47659
IOuLpOuTpA== 47660
YW5zZQ== 47661
IHVuY2xlcw== 47662
IHByb3ZvY2F0aXZl 47663
IGludGVyc2VjdGlvbnM= 47664
IGRlbW9jcmF0cw== 47665
IEp1bGl1cw== 47666
0LjQvdC60Lg= 47667
eWd1c2Fs 47668
INec15U= 47669
IGdqb3JkZQ== 47670
IGdhc2tldA== 47671
IEJvY2s= 47672
IMSwbg== 47673
YnJlYXQ= 47674
IEVxdWl0eQ== 47675
YXJkxLE= 47676
INC60LDQvdCw0LvQtQ== 47677
INC00L3QtdC5 47678
IHThu5tp 47679
IGZpeHR1cmU= 47680
IGFidXNlcw== 47681
IHZheWE= 47682
IG91dmVydA== 47683
IG11bHRpY3VsdHVyYWw= 47684
IGNvbnRleHRv 47685
IFNlc2FtZQ== 47686
IGTDqXBs 47687
IGNvbnNvbW0= 47688
IFBhcnRl 47689
IHBlbQ== 47690
IENvbmFu 47691
INCx0ZbQu9GM 47692
IHBlcnN1YWRlZA== 47693
IGRyYWlucw== 47694
TW9v 47695
Rk9SRQ== 47696
INCx0LDRgg== 47697
IGZvZA== 47698
IFByb2R1Y3Rz 47699
7KeE7Kec 47700
ICJb 47701
IFdpY2s= 47702
IE5hcnV0bw== 47703
0L3QsNC70Lg= 47704
cnl3 47705
IGxvZGdl 47706
IGluaA== 47707
IHZvbnRhZGU= 47708
IGRpag== 47709
IEplc8O6cw== 47710
TG9va2luZw== 47711
IGZvcmVhcm0= 47712
IEludGVncmF0aW9u 47713
IEhBUlJJUw== 47714
IHRvb2xiYXI= 47715
bGVhZGVy 47716
IHNlbGRvbQ== 47717
INCx0YDQvtGB 47718
IEtvb2s= 47719
0L7QvdC0 47720
IG1vbm9wb2w= 47721
IG1pbGxldA== 47722
IGxpcmE= 47723
IEFzaWFucw== 47724
IDE4OTA= 47725
Y2nEn2lt 47726
IGVkZW4= 47727
IElLRUE= 47728
IE5laWdoYm9y 47729
IEthenV5YQ== 47730
w7xk 47731
IHBzeWNoZWRlbA== 47732
IGVudmlzaW9uZWQ= 47733
5Z2X 47734
IO+3uw== 47735
IHd1bmRlcg== 47736
IEJ1bGdhcmlh 47737
QnJpZA== 47738
IG1hcnJvdw== 47739
IGRlcGljdGlvbg== 47740
IFRpbg== 47741
IFBoYXJpc2U= 47742
IGVpbnppZ2U= 47743
IGJsaW5kbHk= 47744
44Gb44Gm 47745
IGRlZmVucw== 47746
RGlyZQ== 47747
IHZpYnJhdGluZw== 47748
IHRyb2xscw== 47749
IGRpc3Jlc3BlY3RmdWw= 47750
IHdvZA== 47751
IHN0aW11bGk= 47752
IGNyZWVwaW5n 47753
IGNsYWlyZW1lbnQ= 47754
IHNjYXJpZXN0 47755
IGTDqWNvdXZyaXI= 47756
IDEwNA== 47757
INCy0LXRgNGF 47758
IMWCYXQ= 47759
IHLDs8W8bmU= 47760
IGJhcmxleQ== 47761
IFJlcGw= 47762
IFR3ZQ== 47763
a2tl 47764
IOOBneOCjA== 47765
IFJlZG1p 47766
IE1ldHJvaWQ= 47767
IM6uz4TOsc69 47768
Q2hlY2s= 47769
IFNFTg== 47770
IGlkbw== 47771
0YLQvtGA0LjQuA== 47772
w7Nw 47773
VU5LTk9XTg== 47774
IMOkbmRlcm4= 47775
IEp1aWNl 47776
IEdlc2ljaHQ= 47777
5bCx5pyD 47778
INC90LDRgdGC0L7Qu9GM0LrQvg== 47779
7YOV 47780
wq0= 47781
ZXhoYWxlcw== 47782
IOy0iQ== 47783
IGpzZW0= 47784
z4DPic+C 47785
IGl0dA== 47786
66qF7J20 47787
IHJlbWl4 47788
IGJsb3Nzb21z 47789
IFJlbmVl 47790
aXNhdGlvbnM= 47791
7Iqk7YSw 47792
IOuztOydtOuKlA== 47793
dWVzdGFz 47794
b3BlZGlh 47795
IEFpbQ== 47796
7J207KaI 47797
c2NlbmU= 47798
IGxlYWthZ2U= 47799
dWNrdA== 47800
U2Fk 47801
QXNr 47802
IHN1c3BlbnNl 47803
IGltcG9zdA== 47804
IFN0cmF0ZWdpYw== 47805
IEl0w61z 47806
4oCM 47807
IGtleWJvYXJkcw== 47808
IGFtdXNpbmc= 47809
b2dy 47810
aWRlcm1hbg== 47811
npY= 47812
INCy0LjQttGD 47813
IGRpcHM= 47814
IGFwb2xvZ2l6ZWQ= 47815
IFNUQVI= 47816
IGVzY3VlbGE= 47817
IENoaW5n 47818
0L3QtdC90LjRjw== 47819
IOu2gOu2hOydtA== 47820
IEZsZWV0 47821
IHNhbWI= 47822
IGVudHNwcmVjaGVuZA== 47823
IGVsZWN0cm9kZXM= 47824
IEZyZWloZWl0 47825
5oiR5LiN55+l6YGT 47826
IFNocmlt 47827
acOfZQ== 47828
IHNlbGVjdGlvbnM= 47829
IGZvcmRp 47830
IGRvc3M= 47831
0Y/Rhw== 47832
IGRpc2NyaW1pbmF0ZQ== 47833
IEF1w59lcmRlbQ== 47834
IGRlc2Vudm9sdg== 47835
IEludGVybmFs 47836
IEJlbmVkaWN0 47837
5a+G 47838
IFNoaXY= 47839
TWlzc3k= 47840
INC+0LHQvdCw0YDRg9C2 47841
INC90LDRgdGC0YDQvg== 47842
IGNvbnRyb2xhcg== 47843
IExpYQ== 47844
IG9waW9pZHM= 47845
YW50dQ== 47846
IGN1cGJvYXJk 47847
5oGQ 47848
0LPQtQ== 47849
YWNodHM= 47850
IGN1cmF0ZWQ= 47851
IHhlbQ== 47852
IHdlYXJ5 47853
IGJyZXRocmVu 47854
IGJ1ZGdldGluZw== 47855
IHBvdXJ0YW50 47856
6Zq7 47857
YWlzaWE= 47858
INC+0YLQstC10Yc= 47859
IEdJUw== 47860
zrzOsc65 47861
INep15TXldeQ 47862
IHNhdWQ= 47863
IGzhu5s= 47864
0JXQog== 47865
dWJpbmU= 47866
INC90YPQttC10L0= 47867
IGtpZG5hcHBpbmc= 47868
IGJyYXQ= 47869
IFRlcnJl 47870
IE1vbmV0 47871
IOuniOyKpO2B 47872
IGZsYXNoeQ== 47873
IElTQk4= 47874
IGZyZWVsYW5jZQ== 47875
aWFnZQ== 47876
IGp1bmdl 47877
7Lap 47878
Y2VyYWw= 47879
INGC0L7Rh9C60Lg= 47880
IGZvcm11bGF0ZQ== 47881
IEZFUg== 47882
IERhcnRtb3V0aA== 47883
7Jy866m07ISc 47884
5aKD 47885
b3dpxIU= 47886
IOuUlOyekA== 47887
IHJlZ2ltZW50 47888
IG1ldGFib2xpc21v 47889
IFBhcnI= 47890
IOy2qeu2hA== 47891
IHNhbml0eQ== 47892
IExhbA== 47893
IEfDtg== 47894
IEdsYQ== 47895
IHByb3Rv 47896
IG1pY3Jvc2NvcGlj 47897
IGthbmc= 47898
IFNjYWxpYQ== 47899
IHB1Zw== 47900
IFNjb3Jl 47901
IFNhdmFubmFo 47902
IGdhcmRl 47903
IE5PUg== 47904
5bCN5ZCn 47905
IHNjaGVpbnQ= 47906
IHDDs8WC 47907
IGNvcnJp 47908
IGJydXRl 47909
IMWCYWQ= 47910
5LuW5Lus 47911
IHN1Y2NlZWRpbmc= 47912
IGJpY3ljbGVz 47913
Tm9u 47914
IHNlZWtlcnM= 47915
IHVuY29uZGl0aW9uYWw= 47916
IHJoeW1lcw== 47917
IEdhcmFnZQ== 47918
IGludm9pY2U= 47919
IGNhbnZp 47920
bmVjaw== 47921
IGN1c3RvbWl6YWJsZQ== 47922
aXJpdHVhbA== 47923
UXVlZW4= 47924
7ZWY7Iuc64qU 47925
IHBvd2VybGVzcw== 47926
IGNzYWs= 47927
5LiN5Lya 47928
aXNvZnQ= 47929
IOygle2ZlQ== 47930
IG5ow6Ju 47931
IE1BTkQ= 47932
IEhhZg== 47933
IHJldm9sdmVz 47934
5Lmf5Y+v5Lul 47935
b3Zhbg== 47936
YXJvbw== 47937
IEdyaW5k 47938
6Zuq 47939
IGluZGlzcGVuc2FibGU= 47940
IGNvbnN1bHRlZA== 47941
IENsaW5pY2Fs 47942
QWNj 47943
IG9saG9z 47944
IG1vbnRlcg== 47945
IEhhbmE= 47946
ZXRhaA== 47947
IHZhYW4= 47948
IHRpZ2Vycw== 47949
IGNhdWN1cw== 47950
8J+Ygg== 47951
s7TsnpA= 47952
cG93ZXJz 47953
aXVtcw== 47954
IO2GoOs= 47955
IHRyYWRpY2lvbmFs 47956
IHJlc29uYXRlZA== 47957
IOyLoOq4sA== 47958
dGhlbQ== 47959
Um9iZXJ0 47960
IGVsZW1lbnRv 47961
IGFudGlk 47962
INC+0LHRgQ== 47963
IG5hdGl2ZXM= 47964
IGxvY2E= 47965
b3dtZW50 47966
IFRpZ2h0 47967
IOaAnQ== 47968
IG1lbGFu 47969
IE51ZQ== 47970
YW1pcw== 47971
IHNvcmdlbg== 47972
YXPEsW5h 47973
SG9tZQ== 47974
IFBVQkc= 47975
IGF3ZnVsbHk= 47976
IFNob3Jl 47977
IFBlcmNow6k= 47978
IExhdQ== 47979
IENpbmRlcmVsbGE= 47980
IENoZXN0 47981
IHNlbWFudGlj 47982
IGRlc2VydGVk 47983
IE1vbW8= 47984
IEhlcm5hbmRleg== 47985
Z2VuZXM= 47986
IEFkdWx0 47987
0LjRh9C10YHQutC+0LPQvg== 47988
b3NoaW1h 47989
IGNhcmFjdGVyw61zdGljYXM= 47990
IEtM 47991
tOyepQ== 47992
b2Nhcg== 47993
IGZlaGx0 47994
IGRydWs= 47995
IFBvcHB5 47996
RU5HTElTSA== 47997
IFZlcmdsZWljaA== 47998
QnJpZW4= 47999
IHJlY29tcA== 48000
INGB0LQ= 48001
IG1lcmdlcg== 48002
IG1hcmtldGVycw== 48003
IGhvbmV5bW9vbg== 48004
IHBlbnNv 48005
IGJlbGxp 48006
0LXRgtGD 48007
IGJhbmtlcg== 48008
Q2FtZXJh 48009
IFN0YWxs 48010
IFN0YW1w 48011
IEJpdGU= 48012
0LXQttC00LU= 48013
IHPDvHI= 48014
IGfDvMOn 48015
IFBhc3NvdmVy 48016
IEJ1Z8O8bg== 48017
INGB0L7QttCw0LvQtdC90LjRjg== 48018
INC90LjQtw== 48019
IG1hbnVyZQ== 48020
IGdsYWNpZXI= 48021
6KuH 48022
UkFZ 48023
dGVycm9y 48024
IHNhbGFkcw== 48025
IGh1cnJpY2FuZXM= 48026
IERlc2lnbmVy 48027
YXRvcmlv 48028
IGZhY3R1YWw= 48029
IFRhbW15 48030
INC30LLRg9GH 48031
IGludHJvZHVjdGlvbnM= 48032
IGhvdXNla2VlcGluZw== 48033
IGhhbmdlcg== 48034
64uY6w== 48035
YWt0ZQ== 48036
IENvbGE= 48037
J10= 48038
IEdlbmRlcg== 48039
0L7RgNC+0L0= 48040
aXBzZQ== 48041
aWNpYXM= 48042
IHN1Y2Nlc3NpdmU= 48043
IHBvbGl0aWM= 48044
IGjDtmhlcg== 48045
IFFpYW8= 48046
IEdpbW1l 48047
INC70L7Qtg== 48048
IHNlYg== 48049
IFdlaXRlcg== 48050
IFNha3VyYQ== 48051
IEJvdWxkZXI= 48052
IEFtw6lyaWNh 48053
cGXFgm5pZQ== 48054
IHRlY25vbG9nw61h 48055
aXNob3Bz 48056
ZnVy 48057
IG1vb25saWdodA== 48058
IGRpc3BlcnNlZA== 48059
IHJleg== 48060
0LXQvdC90L7QtQ== 48061
0LDQu9GM0L3Rg9GO 48062
IFR3ZWx2ZQ== 48063
IEhPUg== 48064
7Iuk7Z6I 48065
aWxhZ2U= 48066
IHNoYWRlZA== 48067
IHJlc3VtZXM= 48068
IFBlYW51dA== 48069
IE1JTEw= 48070
YXBvbnM= 48071
IFVGQw== 48072
IFNvbGU= 48073
IGpveXN0aWNr 48074
IE9saXZpZXI= 48075
d2FybWluZw== 48076
IHN5bGxhYnVz 48077
INC+0LHRidC1 48078
IGhp4buHbg== 48079
IGZlc3Rh 48080
IGNyYWRsZQ== 48081
IFphYw== 48082
IHJlbWVtYnJhbmNl 48083
IOqwmeyVhOyEnA== 48084
IHBpxJlr 48085
IGNvZXhpc3Q= 48086
IFZJSQ== 48087
IMOhcmVhcw== 48088
IHV3YcW8 48089
IG9ic2VydmVycw== 48090
IG3DpG5uaXNrb3I= 48091
Y29vbg== 48092
IERBTQ== 48093
IG5hc3p5bQ== 48094
IGFsbGlnYXRvcg== 48095
IEZyZWV6ZQ== 48096
IEVzdGF0ZQ== 48097
INGC0YDQsNC00Lg= 48098
IHVuZGVyY292ZXI= 48099
IG5pZXM= 48100
IEZlaGxlcg== 48101
cGxpbg== 48102
IEthYnVs 48103
aWxhdGU= 48104
IOqzoOyWkQ== 48105
IG1vcA== 48106
7IS8 48107
IGFuZGVyZXI= 48108
IEtFTEw= 48109
0L7QutC4 48110
INC20LXRgdGC 48111
IGdyYXppbmc= 48112
IGRhw60= 48113
IGNhcGl0YWxpemU= 48114
IGFwZXg= 48115
IG51cnR1cmluZw== 48116
IGNvcnRhcg== 48117
IGNvbnRyYWM= 48118
xLFtxLF6xLE= 48119
IHRhbmRlbQ== 48120
6YO95pyJ 48121
Z2VtZW50 48122
INGB0LjRgdGC0LXQvNCw 48123
IG1hbnF1ZQ== 48124
aWFqxIU= 48125
V09S 48126
INin2Kg= 48127
IGNhcnRz 48128
QU5P 48129
IOuwm+qzoA== 48130
IENlbmE= 48131
IEJpb2xvZ3k= 48132
aWRhcg== 48133
IGHFvA== 48134
ZXJuZQ== 48135
YW51 48136
IHRoYW5rZWQ= 48137
IHN1Ym1hcmluZXM= 48138
IG1hbmlj 48139
INC80L7Qtw== 48140
5LyK 48141
aW5zdGFudA== 48142
ZXNzZW50aWFs 48143
IHNhbXVyYWk= 48144
IHBhc3Rp 48145
IGFsYW4= 48146
IGJyb2No 48147
IGJha2Vy 48148
IEd1aWxs 48149
qLw= 48150
IHdpdGhkcmF3bg== 48151
64ud 48152
UGVyZmVjdA== 48153
cXVlbmN5 48154
IHN0cmVhbWxpbmVk 48155
IDEzMDA= 48156
tOuPhA== 48157
IOuWoOs= 48158
IOOBr+OBhA== 48159
IGh2YWQ= 48160
5LiA5a6a6KaB 48161
IHZlcmJhbGx5 48162
IEtvbnM= 48163
IOyhsOyLrA== 48164
IGRpZXo= 48165
5o6w5o6w 48166
IGNodWNrbGluZw== 48167
IE1paA== 48168
IHJhbGxpZXM= 48169
IG1hbnRlcg== 48170
IGVhcm5lc3Q= 48171
c3VwZXI= 48172
IGdlY2U= 48173
IFJlbmQ= 48174
IEdlcmFkZQ== 48175
amVuaWdlbg== 48176
IFZhbGw= 48177
IOyeiOuCmA== 48178
INGB0LrQsNC30LDQu9Cw 48179
IHRyYWJhbGg= 48180
INC90LDRiNC10Lw= 48181
INC80LXRhQ== 48182
aWtpdA== 48183
IG5vdW5z 48184
IG5ldXJvbG9naWNhbA== 48185
IG1vdGl2YXRpb25hbA== 48186
IE1jTWFob24= 48187
IEZpbmlzaGVk 48188
IOuztOydtA== 48189
IEZpZWxkcw== 48190
IGFkb2xlc2NlbnRz 48191
IFRpc2No 48192
IE5lYmVu 48193
IEZsb3dlcnM= 48194
IEVuZXJn 48195
IGRpcmV0 48196
IFRoaQ== 48197
IFBpY2Fz 48198
5oOc 48199
5oCO5LmI5qC3 48200
IGF2ZXRl 48201
IEZvcnM= 48202
IENoYXBlbA== 48203
TsOjbw== 48204
RXQ= 48205
INGB0L7QtNC10YDQtg== 48206
cmVubw== 48207
IHN2ZW4= 48208
IGRvc3TEmXA= 48209
bmVl 48210
IFNuYXBkcmFnb24= 48211
IElEcw== 48212
7JWY64qU642w 48213
16jXmg== 48214
IHN1bmZsb3dlcg== 48215
IHBlcnBldHVhbA== 48216
57OW 48217
IGtuaWdodHM= 48218
IGdpcmQ= 48219
IFRvbGQ= 48220
IHZvbGNhbm9lcw== 48221
IGFkdmVyc2FyeQ== 48222
IEVjb25vbXk= 48223
IGV4dHJhcG9s 48224
IGJsdWV0b290aA== 48225
IHpvb21pbmc= 48226
IHNreXM= 48227
IGdlbmlhbA== 48228
w61jdWxvcw== 48229
YW1icmU= 48230
INC80LXRgA== 48231
IHRlZW55 48232
IHN0cmVzc2luZw== 48233
7JWM 48234
T05Z 48235
IHRyYW5zbHVjZW50 48236
IHJvdW5kaW5n 48237
IGdydWVz 48238
15nXoNeU 48239
YXByw6hz 48240
IHBydWViYQ== 48241
IHBvbHlnb24= 48242
IGJsdWViZXJyeQ== 48243
IFByb2dyYW1t 48244
IHRyZW5jaGVz 48245
IHNlYmFnYWk= 48246
IHBhbGF0ZQ== 48247
IGxhdWRl 48248
IGJlaGF2ZWQ= 48249
IGxvbmdpdHVkaW5hbA== 48250
IE1vZHVsZQ== 48251
IGFkbWly 48252
zrvOuQ== 48253
R3JlZw== 48254
IHd5c3Q= 48255
IHByb3BhZ2F0ZQ== 48256
IG1vbGRz 48257
IFR1Yg== 48258
IExvdWQ= 48259
dXN0bw== 48260
IHVuc3RvcHBhYmxl 48261
IHJlaW5mb3JjaW5n 48262
6Z2e5bi455qE 48263
INC/0YDQvtCx0LvQtdC80LA= 48264
IHBvdGVuY2lhbA== 48265
IGhlbXA= 48266
7J6U 48267
4KSv 48268
IG9wdGlj 48269
IGVyZm9sZ3JlaWNo 48270
0YHRiw== 48271
0L7Qu9GM0YjQtQ== 48272
dXJzdA== 48273
IFBvaXM= 48274
IHJlc3BvbmRlbnRz 48275
IG5laG1l 48276
IEV4dGVybmFs 48277
b2xhdGU= 48278
SHl1bg== 48279
IHF1YXJ0eg== 48280
IG1hdGhlbWF0aWNpYW4= 48281
IGLDoXNpY2FtZW50ZQ== 48282
IGFpbA== 48283
7KCc66W8 48284
YXR0dXR0bw== 48285
IG5vb2l0 48286
IGFmZmxpY3Q= 48287
IE9sZ2E= 48288
6K23 48289
INC90LDRgg== 48290
IGRpdGVz 48291
IHJlYWxpZGFkZQ== 48292
IGvDpG4= 48293
IHVuaXF1ZW5lc3M= 48294
IHBhZHJlcw== 48295
IHN1YnNpZGk= 48296
IHBpZ2VvbnM= 48297
zrLOsQ== 48298
c3RhZA== 48299
IGRlcmVu 48300
INCh0LvQtdC0 48301
ZG9v 48302
INC+0L/QuNGB0LDQvdC40Lg= 48303
IGFtYmVy 48304
IGdvb3NlYnVtcHM= 48305
IGZyw6Vnb3I= 48306
IFZpdGFs 48307
IElzcmFlbGl0ZXM= 48308
d2Fzc2Vy 48309
SXNu 48310
IGNvbW1pdHM= 48311
IFNURVZFTg== 48312
IEJldsO2bGtlcg== 48313
dWl0aXZl 48314
IGxlZ2Vu 48315
IGJydWs= 48316
0LjRgNC+0LLQsNC9 48317
eW5lbg== 48318
aGVsbQ== 48319
IGdlbmVyYXRpb25hbA== 48320
IEzDpG5kZXJu 48321
zr/Ouc+Az4zOvQ== 48322
dXp1 48323
IGNhbGxlcg== 48324
0L7QvdGM 48325
w7xtw7w= 48326
IGJlc2Fy 48327
IHBsYXRz 48328
IG1pZ3JhdGVk 48329
IGphcA== 48330
IFdBUg== 48331
IGRpc3NlY3Q= 48332
IFp1c2No 48333
IFplaXRlbg== 48334
IExpb25z 48335
IERG 48336
4pQ= 48337
0LrQuNCy 48338
IHBlZGVzdHJpYW5z 48339
IE1hcmlseW4= 48340
ZG9jaw== 48341
IHlodA== 48342
IHJlaW5jYXJu 48343
IFNvbm8= 48344
IEdyb3d0aA== 48345
0YPRgdC+0LI= 48346
IGR1bmdlb25z 48347
IGJhZ3Vz 48348
a2ljaA== 48349
INGD0LrRgNCw0Zc= 48350
6Yar 48351
IEtlbGxlcg== 48352
Y2hlbWlzdHJ5 48353
SmFwYW5lc2U= 48354
IHdpbGxzdA== 48355
IGRlY29tcG9zaXRpb24= 48356
INGB0YLQtdC9 48357
IHJldml2ZWQ= 48358
7ZWZ6rWQ 48359
IMWT 48360
5L2Q 48361
7Iu4 48362
aXBweQ== 48363
IGhvdXJseQ== 48364
asOkbg== 48365
IFdvcmtzaG9w 48366
nbzshJw= 48367
IGN1YXJ0bw== 48368
IHBhdHJpbQ== 48369
IEJ1cmNo 48370
IOyeiOq4sA== 48371
IGhlcGF0 48372
IGjDoG5n 48373
IOuMgO2VtA== 48374
INCy0LDRiNC4 48375
IHJld29yaw== 48376
IHBhcnNl 48377
IMOnxLFrdMSx 48378
IFNheA== 48379
IE1vbmdv 48380
IEFhYWg= 48381
cmFtYmxl 48382
REo= 48383
IHN0YWJpbGl6ZWQ= 48384
IFNwZWVjaA== 48385
Qm9va3M= 48386
IGh1cmRsZXM= 48387
IFdP 48388
IExhbWJvcmc= 48389
IDE5MzM= 48390
IHZvcmJlcmU= 48391
IGNsaW5pY2FsbHk= 48392
IGJyZWF0aHRha2luZw== 48393
IEdhdGV3YXk= 48394
0L/QtdGA0LLRi9GF 48395
dXRlcnM= 48396
IOu5tQ== 48397
IHlldGVy 48398
IHB1bGxleQ== 48399
IG11ZmZpbg== 48400
IFByZWZlcg== 48401
IFBlbmNl 48402
IGluZm9ybWHDp8Ojbw== 48403
7Iqk7Yq46w== 48404
44K444Oj 48405
IFR1cnRsZQ== 48406
IFJlZ2luYQ== 48407
IExvYWQ= 48408
ZG9lcw== 48409
cGFuemU= 48410
uJQ= 48411
IG1pbmE= 48412
IExhdGlub3M= 48413
YW1tZXJz 48414
IFRvcnQ= 48415
IEJleW9uY2U= 48416
0LjQvNC+0YHRgtC4 48417
INCy0L7Qv9GA0L7RgdGL 48418
IGJ1bHVu 48419
6ICM5bey 48420
aW5law== 48421
YmVyZWljaA== 48422
IHBhc3R1cmU= 48423
IE9B 48424
IE1lbHQ= 48425
IEV0dA== 48426
IERZ 48427
IG9id29obA== 48428
IGxlYWd1ZXM= 48429
0YLQtdGB0Yw= 48430
INC60YPRgQ== 48431
IHZvcnM= 48432
IHRvcHA= 48433
b2dyYXBoaWNhbA== 48434
YXNzdA== 48435
IGxpbmRv 48436
IOuwne2YlA== 48437
IHLDqWZs 48438
IGNsaW1icw== 48439
IHZhcnNh 48440
IG1ldGh5bA== 48441
IEthcmVyZQ== 48442
xrDhu58= 48443
UmFk 48444
IHByZXBhcmVkbmVzcw== 48445
0L7QvdGH 48446
IE9E 48447
IENHSQ== 48448
IOCkrg== 48449
IHNwZWVjaGxlc3M= 48450
IGxhc2Np 48451
IGJvbGFn 48452
INGF0L7Rh9C10YLRgdGP 48453
IGdyaWV2aW5n 48454
IEpvaGFubmVz 48455
IENhcnJvbGw= 48456
YWRha2k= 48457
iKzr 48458
IHPFgnU= 48459
IGlubmVyaGFsYg== 48460
IGd5bW5hc3RpY3M= 48461
0L/RgNC4 48462
aWZpcXVlcw== 48463
IGthcmF0ZQ== 48464
IGRvbXU= 48465
44Gd44KM44Gn 48466
T1RIRVI= 48467
IGRlbWFuZMOp 48468
IGJvb2tsZXQ= 48469
IEt5b3Rv 48470
IHdvaA== 48471
IE1hcsOtYQ== 48472
dmlvbGVudA== 48473
SkU= 48474
IGzDs2c= 48475
IGJydXRhbGx5 48476
Y290 48477
INmF24w= 48478
IFdhcnN6 48479
5a6I 48480
d29s 48481
IG1pa8Ok 48482
IFByb25vdW5jZQ== 48483
IEJyZW5kYW4= 48484
IHJvdXA= 48485
IGl0YWxpYW5v 48486
5aaC5q2k 48487
INC60L7QvNC/0YzRjtGC 48488
IHVyZ2luZw== 48489
ZWRlcw== 48490
IGNhcmJvbm8= 48491
IFJpY2hhcmRzb24= 48492
INCd0LDRhw== 48493
IFRyYWluZXI= 48494
IENyaW1lYQ== 48495
IGRpYXBlcnM= 48496
IGNvdmV0 48497
IE1haGFy 48498
IEh1dGNo 48499
IEF1c3c= 48500
YmVydHk= 48501
IGluZGlmZmVyZW50 48502
0LrRgNC10YI= 48503
dWxkYWRl 48504
IGhhcm1z 48505
otmG 48506
bGVzaWE= 48507
IGdpbw== 48508
IE1pc3RyZXNz 48509
IEtub3g= 48510
IEZSRUU= 48511
IOujqOs= 48512
INC90LDRiNCw 48513
IGludmluY2libGU= 48514
IG1haWRlbg== 48515
IEplZXo= 48516
IGJyZXZl 48517
cG9sZQ== 48518
IGNyaXRpY2lzbXM= 48519
IFJ1c2lh 48520
4KSu 48521
cGhpbg== 48522
IENvbXBhcmU= 48523
IEJPTg== 48524
IHNuZWFraW5n 48525
IFJhaWxz 48526
IEdlcmFs 48527
IDE5NTM= 48528
SG9sYQ== 48529
INC+0L/Ri9GC 48530
IHJhaW5mb3Jlc3Q= 48531
IGJlbHVt 48532
IE9iaQ== 48533
IElTUw== 48534
44KM44Gq44GE 48535
INCh0LI= 48536
IGJsb25k 48537
IHd6Z2w= 48538
IHBvd2llZHppYcWC 48539
IGNob2tpbmc= 48540
IFNvbmdz 48541
IEJpcmF6 48542
IHllbGxz 48543
IHN0eWxpc3Q= 48544
z4zPhM61 48545
IHNjaHJlaWJlbg== 48546
IEphdw== 48547
IEVsZXZlbg== 48548
IFJpZg== 48549
Ly4= 48550
IOyYpOuenOunjA== 48551
IHRyZWF0aWVz 48552
dWZmZWQ= 48553
IOKIkg== 48554
IHJvb2Zz 48555
4LmA4Liq 48556
IOu7 48557
IHNwYXJrbGU= 48558
IEtpZXY= 48559
IEFyZ3U= 48560
ZXJlY2h0 48561
INCd0LDQtNC+ 48562
IEZJTA== 48563
IG1vbHRh 48564
IERldmk= 48565
IGNhbXBl 48566
IGJlbmV2b2w= 48567
IFRvdWdo 48568
IG1vaW0= 48569
IGV2YWN1YXRl 48570
IGVycmFkbw== 48571
5amG 48572
0YDRg9Cz0L4= 48573
IO2OmA== 48574
IM6TzrnOsQ== 48575
IHdlYWtlbg== 48576
IGlsbHVtaW5hdGVk 48577
IHNpZ2xv 48578
IFZhY2M= 48579
0LjQtdC5 48580
YWxpcw== 48581
INGD0YHRgtGA0L7QuQ== 48582
IGRvbmE= 48583
xYJvcw== 48584
w7xtYW4= 48585
IHByb2R1Y2Npw7Nu 48586
IGNsb3Q= 48587
IE1hbmdv 48588
IHVuZWFzeQ== 48589
IHNodXRz 48590
IEV4YW1wbGVz 48591
dmVsbA== 48592
ZWJl 48593
IHByb21wdGx5 48594
IFRlbGVz 48595
INC/0YDQvtGI0Ls= 48596
IHB1ZXJ0YQ== 48597
IMO8YmVyemV1Zw== 48598
IGNvY2g= 48599
c29jaWFs 48600
IEJlbnNvbg== 48601
IE1ldGg= 48602
IEV4cGVk 48603
IHN1cHBsZW1lbnRhbA== 48604
IGNvbmNlaXZl 48605
INeY15XXkQ== 48606
IGNhcHRpdml0eQ== 48607
j5nslYg= 48608
INGF0YPQtA== 48609
Zm9ybWluZw== 48610
IHVwbG9hZHM= 48611
IHR1cmJ1bGVuY2U= 48612
am9pbnQ= 48613
IHNhdGlzZmFjdG9yeQ== 48614
IEFuaW1l 48615
IHdhc2hlcw== 48616
IGxpYmVyYWxz 48617
IFN1bnNoaW5l 48618
IFJFQUw= 48619
dWJsaWs= 48620
YmluYXJ5 48621
VG9ueQ== 48622
IHBvbGFyaXplZA== 48623
IGVucmljaGVk 48624
dGFraW5n 48625
IOuBneuCmA== 48626
IHBsZWFzdXJlcw== 48627
IGV4dGVybWlu 48628
aW5lc2U= 48629
YXRs 48630
dsOkcg== 48631
0LDRgNGL 48632
IG15xZs= 48633
bmFycmF0b3I= 48634
INC+0LTQvdC+0Lw= 48635
IG5handpxJk= 48636
IG1vYmlsaXpl 48637
IG1pbGxvcg== 48638
IGF0YQ== 48639
5re3 48640
IHBvbMOtdGljbw== 48641
IHBsZWFk 48642
IHBhaW50ZXJz 48643
IFNvdw== 48644
0L7RhA== 48645
IOyYm+uCoA== 48646
INGH0YLQvtCx 48647
IHNhYm9y 48648
IFVuZGVydA== 48649
IEpFUlJZ 48650
xaHDrQ== 48651
IOuwluyXkA== 48652
IHByw6ljw6lk 48653
IGFubm90YXRpb24= 48654
IEluYXVkaWJsZQ== 48655
IHRleHR1cmVk 48656
IGZpc2hlcm1hbg== 48657
dm9yZGFu 48658
aWNoZXJ1bmc= 48659
IOyggeydtA== 48660
IGdlemVpZ3Q= 48661
IG1hbmRhdGVz 48662
IGJlYWs= 48663
IFRXTw== 48664
IEFrYmFy 48665
aWxpYW4= 48666
IHRp4bq/cA== 48667
IHN1cGVyaW9yaXR5 48668
aW5rdQ== 48669
IGx5cw== 48670
IEZDQw== 48671
IENQQQ== 48672
dXN0ZXJpbmc= 48673
bmljb3M= 48674
YW5qYQ== 48675
IGNoaWxscw== 48676
IENhZ2U= 48677
IHNlYWxpbmc= 48678
IHNhw6c= 48679
IGRlZGFucw== 48680
IEFsZ2Vy 48681
IHNwZXppZQ== 48682
IGNvbG9zcw== 48683
xLF5xLE= 48684
Y2xvY2t3aXNl 48685
IGV4YWN0YW1lbnRl 48686
IGllbWFuZA== 48687
YW3EsQ== 48688
IG1hbmRhcg== 48689
cmFq 48690
ZmFjZWQ= 48691
YWd1YQ== 48692
IOq5lOs= 48693
IGluc2Jlc29uZGVyZQ== 48694
IGRyaXp6bGU= 48695
IGRpbWluaXNo 48696
IFlvZGE= 48697
QUk= 48698
IGJpbG1peW9ydW0= 48699
IE1NQQ== 48700
YXRlZ29yeQ== 48701
INC/0LXRgNC10L8= 48702
IHBhcnRpY2lwYXI= 48703
IG5vcm1hbGl6ZWQ= 48704
IGNvbXBsZXhpdGllcw== 48705
5rSy 48706
5o6n 48707
0LDRgNC+0LI= 48708
bWlzdA== 48709
aWNoYQ== 48710
R3JvdXA= 48711
IHJlc2lsaWVuY3k= 48712
IG5vZ2xl 48713
IENOQw== 48714
cHLDvA== 48715
IHBoeXNpY2lzdHM= 48716
0L3QvtC6 48717
TEk= 48718
IHN0dWZmcw== 48719
IHNpc3RlbWFz 48720
IGludGVyZmVyaW5n 48721
IE1hcnZpbg== 48722
w6lyY2l0bw== 48723
IOyXhuqzoA== 48724
IHNvbmlj 48725
IGVxdWl2 48726
IGFib3Jk 48727
IFJhbWVu 48728
IDA5 48729
bWVkaW0= 48730
YXRpcXVlcw== 48731
INC00LXQu9Cw0Y7Rgg== 48732
IHVuYW5pbW91c2x5 48733
IHNraXJ0cw== 48734
IO2KueuzhA== 48735
IFByaXg= 48736
a2FtaQ== 48737
IGZydWl0aW9u 48738
IGJpcnRoZGF5cw== 48739
0LjQutC+0Lw= 48740
IGluYXVndXJhbA== 48741
IGNvcnJlbGF0ZQ== 48742
IFRvcnk= 48743
IOuCmOyB 48744
IGRldw== 48745
IFByZWNpcw== 48746
aWhp 48747
IOusuOygnOqwgA== 48748
IGNpdGluZw== 48749
IExhbmE= 48750
IEthZw== 48751
IHBsYXl0aHJvdWdo 48752
IFByb3RvY29s 48753
ZnJpc3Q= 48754
aG92YWg= 48755
IG1lcmNpZnVs 48756
IGJpbGluZ3VhbA== 48757
IEd1aXRhcg== 48758
cmg= 48759
IGdsYW1vcm91cw== 48760
IFZpa2luZ3M= 48761
IE9vb29o 48762
7ZWY64qU642w 48763
IFVnYW5kYQ== 48764
IGNvbGxhcHNlcw== 48765
ZW50cnk= 48766
IGFudGlveGlkYW50cw== 48767
64KY6w== 48768
0YjQsNGP 48769
IHRyaXZpYQ== 48770
IGfDpGxsZXI= 48771
IGZ1bmdp 48772
IG1pbGtz 48773
IGRpY2h0 48774
zrzOtw== 48775
cG9rZQ== 48776
INCy0YvQv9GD0YHQug== 48777
IGZlZWRlcg== 48778
IEFsY29ob2w= 48779
aG93ZXI= 48780
IGRlc2VydmluZw== 48781
IFJlYmVs 48782
aW9zaXM= 48783
IDEwMw== 48784
IGhhbmRvdXQ= 48785
IGVubQ== 48786
IGxhbmRsb3Jkcw== 48787
IGdlb2xvZ3k= 48788
cmlscw== 48789
IGNvYnJh 48790
IFZvbGQ= 48791
IFBhbmNo 48792
IEdSRUc= 48793
IHByb3Nz 48794
IGJyYWNlbGV0cw== 48795
IFZlZ2E= 48796
IHJvenVt 48797
5qy+ 48798
0LDQt9C0 48799
IEx5bmQ= 48800
IEhvbm9ycw== 48801
IHN1cnJlbmRlcmVk 48802
IGxpYnJhcmlhbnM= 48803
MTI1 48804
INGB0LjQsw== 48805
IHVuaWZvcm1seQ== 48806
IEVhZ2xlcw== 48807
7JWZ 48808
0LjRgtCw0L0= 48809
YW5kaWQ= 48810
IOygiOuMgA== 48811
INi2 48812
IGFycmVzdHM= 48813
IENTVg== 48814
IEF6ZXJiYWlqYW4= 48815
b3J0aWM= 48816
IERY 48817
IEFkdmVudHVyZXM= 48818
IGFidXM= 48819
IEZhdQ== 48820
IHNjaGxpbW0= 48821
IHJhdHRsaW5n 48822
IGNvbnN1bWVz 48823
IFRvbGtpZW4= 48824
IHJlc3VycmVjdGVk 48825
IFhZ 48826
7Yq46rCA 48827
INCy0YvRgdGC0YPQvw== 48828
IEFuZ2ll 48829
xbxlbmlh 48830
TWlj 48831
IFNoZWlsYQ== 48832
YWNodGV0 48833
IG92ZXJzdA== 48834
IGzDog== 48835
IGluZWZmZWN0aXZl 48836
5p2h 48837
5oCO5LmI5LqG 48838
5b+Z 48839
IHdpY2h0aWdlcg== 48840
IHZpbm8= 48841
IHB1bQ== 48842
IGFuZ2xlZA== 48843
IFBpb25l 48844
IE3hu7k= 48845
44Gd44KM44Gv 48846
d2/Fm8SH 48847
ZHJhdw== 48848
4Lix4LmI 48849
bWFya2V0cw== 48850
IGNhZmVz 48851
IENlbQ== 48852
4p2k 48853
IFN1aXQ= 48854
TUs= 48855
IGVtcGhhc2l6ZXM= 48856
IHRvcnRpbGxh 48857
IG1lam9yYXI= 48858
IFN1cnZpdg== 48859
Y2FzdGluZw== 48860
IGVkdWNhY2nDs24= 48861
IEd1bQ== 48862
dWVseQ== 48863
IOyXrOq4sOuKlA== 48864
IHN0cmV0Y2h5 48865
ZW7Dp2E= 48866
IHdpdGhob2xk 48867
IGV4aXRpbmc= 48868
IGVudGhhbHB5 48869
IFRyYW5zaXQ= 48870
xLFsbcSxxZ8= 48871
YWxpZXM= 48872
IHNhbHZhcg== 48873
IGxlYW5lZA== 48874
IGdyb8OfZXM= 48875
IGZpdHQ= 48876
0LDQutC4 48877
U2FyYWg= 48878
IGhvc3RlbA== 48879
IGZpbmdlcm5h 48880
IG5hZHppZWrEmQ== 48881
d2l2ZXM= 48882
UmVj 48883
IHNwb29s 48884
0LDRgtC+0LI= 48885
IEVuZW15 48886
IGZ1cnk= 48887
IGRldHRh 48888
IEZheQ== 48889
6Zqo 48890
0Y/RjtGC 48891
IGFwcm94aW1hZGFtZW50ZQ== 48892
IHNpbG9z 48893
IG1hZ2lzdA== 48894
IGNyZWU= 48895
IEtyYW5r 48896
IERPV04= 48897
IHN0YXJ0bGVk 48898
IHJlYm9ybg== 48899
IFVtd2VsdA== 48900
IFN1emFubmU= 48901
0L3QuNGG0Ys= 48902
b3V0ZXo= 48903
IEpBQw== 48904
eWFyZHM= 48905
cmFkYXM= 48906
cmF1 48907
aXB0cw== 48908
aGFpbA== 48909
IHBhcmFncmFwaHM= 48910
IG1lZ2xpbw== 48911
IGlzb2xhdGluZw== 48912
IGFjZWl0ZQ== 48913
IEhhcnNo 48914
IGN5c3Q= 48915
IEJsb2NrY2hhaW4= 48916
INGF0L7RgNC+0YjQuNC5 48917
IHZpcnR1b3Vz 48918
IGludmVzdGlnYWNpw7Nu 48919
IGRldm9pcg== 48920
IG1hc3R1cmI= 48921
IFNhbGU= 48922
2YrYsdip 48923
IM6n 48924
IFN0cmHDn2Vu 48925
IGRpa2s= 48926
IGFmb3Jl 48927
IEp1bmdrb29r 48928
IGNob2NpYcW8 48929
IERlYmF0dGU= 48930
IHdlaXJkbHk= 48931
IHZpYWpl 48932
cmVnaXN0 48933
SGVscA== 48934
IGtpbmRlcmVu 48935
IGZvcm11bGF0ZWQ= 48936
IGVuZmlt 48937
IFRvd2FyZHM= 48938
0LrQvtGX 48939
aXZlcmluZw== 48940
INC00LXRgtC4 48941
Y2hhcmdlcg== 48942
IHB1cmw= 48943
IGFjYWRlbWljYWxseQ== 48944
IE51cnNl 48945
IGRlbGV0aW5n 48946
YXlv 48947
IHJlZnVzYWw= 48948
IGRlcGljdHM= 48949
IERyYWN1bGE= 48950
IHRvYXN0ZWQ= 48951
IFpvbWJpZQ== 48952
IFN1cGVyaW9y 48953
IEJvbGQ= 48954
IHF1aXp6ZXM= 48955
IGdsZQ== 48956
NDUw 48957
IGNvbWXDp28= 48958
eW5u 48959
IHZlcnN0 48960
IE9sYWY= 48961
IHBvbW9j 48962
IFNhc2s= 48963
65g= 48964
IFRDUA== 48965
IFByb3BlcnR5 48966
7ZWY7KOg 48967
4Lic4Lih 48968
Ym9vbQ== 48969
YXJvcw== 48970
INGA0L7RgdGB0LjQuQ== 48971
INCx0YvQstCw0LXRgg== 48972
5Ye65Y67 48973
IOydtOyVvOq4sOulvA== 48974
IGNvbWJpZW4= 48975
dmFjYw== 48976
IGViZW5mYWxscw== 48977
cGFyYQ== 48978
INC30Lw= 48979
IGRlc3BlcmF0aW9u 48980
b3JkcmU= 48981
INep15zXmQ== 48982
IGdlbmVyb3VzbHk= 48983
INCe0Lo= 48984
IG9yYml0aW5n 48985
Pjwv 48986
IGVzcMOt 48987
IENPUA== 48988
5a2p5a2Q 48989
dmlzaWJsZQ== 48990
INC/0YDQtdGB0YLRg9C/ 48991
IHN0aXRjaGVk 48992
4K+ILg== 48993
IGxhdGVudA== 48994
IFByYWI= 48995
IE1jTg== 48996
IEhlYWxpbmc= 48997
IEN1cmlvc2l0eQ== 48998
Y2VydA== 48999
IOuvvOyjvA== 49000
IHBhdGllbnRseQ== 49001
IFlU 49002
Zm9yZWlnbg== 49003
IHbhuqtu 49004
IGluZHVzdHJp 49005
IGNvY2t0YWlscw== 49006
IGJyaWdodGVu 49007
IGNvbnNvbGlkYXRlZA== 49008
0LDRgNC0 49009
bHRyeQ== 49010
IGdyaWxsZQ== 49011
IGJvbmE= 49012
IGRpbGlnZW50bHk= 49013
IFdyZXN0bGVNYW5pYQ== 49014
ZXJrdA== 49015
ZW5lcmd5 49016
OTk5 49017
4K6V4K61 49018
IHRvdGU= 49019
aW9ubw== 49020
RElP 49021
IHNjaGl6b3BocmVuaWE= 49022
IHBvc3Rwb25lZA== 49023
IFFpdQ== 49024
IM+Dz4XOvQ== 49025
IHpkasSZ 49026
IHNwYW5uZW5k 49027
IERJUw== 49028
UmVs 49029
IHJoaW4= 49030
aW1tdW5l 49031
T2xk 49032
IHBsw7Z0emxpY2g= 49033
IG1vdW5k 49034
IGFzdHJvbm9taWNhbA== 49035
IEd1aWQ= 49036
IEN1bA== 49037
SEk= 49038
IMWg 49039
IHJlcG8= 49040
IE1hdXJpY2U= 49041
5LiA54K5 49042
IGJhbmRpdHM= 49043
IERlc2t0b3A= 49044
w6Rzcw== 49045
ZnRh 49046
IGxpY2VuY2U= 49047
IGltYWdpbmFy 49048
IEVudHJlcHJlbmU= 49049
eG8= 49050
IOunm+yeiOuKlA== 49051
INeU15E= 49052
IHB1bXBraW5z 49053
IGthbnNzYQ== 49054
IGrEmXp5 49055
IGNvbW11bmF1dMOp 49056
YsO8cg== 49057
IGVyaMO2 49058
IFdvbHZlcg== 49059
IFNoYXJpbmc= 49060
5Luk 49061
IHBha2Fp 49062
IGluc3VsdGVk 49063
0JzRiw== 49064
0L7Rlw== 49065
IGNvbnNpc3Rl 49066
5oyR 49067
IHlvdW5nc3RlcnM= 49068
IGdsZWljaGVu 49069
d2VkZXI= 49070
IG1vdGU= 49071
IGNsYXVzZXM= 49072
w6l0YXQ= 49073
cHJ1cw== 49074
IHdhc3Q= 49075
57uZ5oiR 49076
IENyaXNw 49077
IOeEtuW+jA== 49078
IG9mZmVuZGVycw== 49079
IGNvbnZlY3Rpb24= 49080
IGNvbmZpYW4= 49081
b2xsb3c= 49082
YW1ldA== 49083
INGX0YU= 49084
56ys5LqM5YCL 49085
ZmZpY2llbmN5 49086
IHVuZ2xhdWI= 49087
aWdhbnM= 49088
IG1hcmtldGVk 49089
IFZBTg== 49090
IHByb2NsYWltZWQ= 49091
IGPDqWx1bGFz 49092
IGNvbGxpZGU= 49093
IE9jdWx1cw== 49094
YWRvcmU= 49095
Smk= 49096
IHN1c3RhaW5pbmc= 49097
IEZhc2M= 49098
IHNldHp0 49099
IG5vc2FsdHJlcw== 49100
TW9zdA== 49101
INCy0Yc= 49102
IG5hdWM= 49103
IEJoYXI= 49104
54i454i4 49105
5oiR6Lef5L2g6Kyb 49106
IHnDqnU= 49107
IHRpbWVzdA== 49108
IHBlcnRhbWE= 49109
aXJtaQ== 49110
IHp3cg== 49111
IHZlcmJlc3M= 49112
IHZvcnRleA== 49113
IFNUQUNL 49114
2KvYsQ== 49115
uYTr 49116
lJTsmKQ= 49117
IGxpbmthZ2U= 49118
IEZyYXNlcg== 49119
ZW5hcmlv 49120
IOudvOuKlA== 49121
IOyEoOuwsA== 49122
aHRoYWw= 49123
IOq5jA== 49124
IEtow7RuZw== 49125
w4M= 49126
IHNjcmFtYmxlZA== 49127
IEVpbms= 49128
IG1pY3Jvb3JnYW4= 49129
IG5hcmNpc3Npc3Q= 49130
IEtvbWJhdA== 49131
IOunoQ== 49132
IEFHQQ== 49133
IHBlcmZla3Q= 49134
IFNlcmll 49135
ZGV0ZXJt 49136
LSc= 49137
IHBvbnl0YWls 49138
IGtvc2th 49139
7JM= 49140
IG9iZWM= 49141
IGNoZXN0cw== 49142
dmVlcg== 49143
IHVwcmlzaW5n 49144
IHN0b2tlZA== 49145
YXNzb2Np 49146
IHByb2R1w6fDo28= 49147
IFNoYXBl 49148
7KCc6rCA 49149
IOuUsA== 49150
IGpvbg== 49151
IGluYWR2ZXJ0 49152
YW50YXM= 49153
INC90LDQutC+0L3QtdGG 49154
IOWwjeWVig== 49155
IEFyc2VuYWw= 49156
IHByb3RlZw== 49157
IGxpYmVydMOp 49158
IGdsYXJl 49159
5Yia 49160
5bey57uP 49161
IHZlcmVpbg== 49162
IGluc2VydHM= 49163
IEphbmE= 49164
IHd5ZGFqZQ== 49165
xYJ1bQ== 49166
ICUu 49167
b3JpZ2luZQ== 49168
IHN5bmFnb2d1ZQ== 49169
IGZhbGxhaXQ= 49170
IGRpc29iZWQ= 49171
IGFudGlj 49172
IEN5Y2w= 49173
IGFzeW5jaHJvbm91cw== 49174
IOuyjOyNqA== 49175
IGdlc3VuZA== 49176
IGdhZ24= 49177
IHBlYQ== 49178
IGdyaW4= 49179
w6lzdA== 49180
IHNhdWM= 49181
IE3DpGQ= 49182
7ZW064+E 49183
cHBz 49184
IM61z4DOuQ== 49185
IHBldXBsZQ== 49186
IGRlYmVu 49187
IEJyZWU= 49188
INGA0L7Qu9GM 49189
INC60LDQutC40Lw= 49190
IMO6dGls 49191
IGRpc3RyaWJ1dG9y 49192
0LDQu9GL 49193
IHN3b2rEhQ== 49194
IGZvbGtsb3Jl 49195
IHJlY2VpdmVycw== 49196
IE1PTw== 49197
Ymlucw== 49198
YXN0cmU= 49199
7JWI6w== 49200
IOuEo+qzoA== 49201
IG11bHRpbWVkaWE= 49202
IGdlYmF1dA== 49203
0L7QstGL0YU= 49204
w6N5 49205
IGRhbmU= 49206
b2tvbA== 49207
ZW1pdGlzbQ== 49208
T05FWQ== 49209
IHlhxJ8= 49210
IGNoYXVmZg== 49211
5a655piT 49212
IGVzZnVlcg== 49213
xINu 49214
ZXJ0YXM= 49215
IGZvbmN0aW9ubmU= 49216
b21pbmE= 49217
IGl2b3J5 49218
IFlvdXR1YmVy 49219
IFNreXdhbGtlcg== 49220
0LjRh9C10YHQutCw0Y8= 49221
dG9p 49222
IHZleWE= 49223
IGdlbGVybnQ= 49224
IGNoYW5jZWxsb3I= 49225
IFN0YXRpc3RpY3M= 49226
IHdlbGRlZA== 49227
IG9uZGFu 49228
IFNlaQ== 49229
IG1lZGljYWxseQ== 49230
IGVuZXJnaXplZA== 49231
IFZpYQ== 49232
INCy0LjQug== 49233
IHVuaW50ZXI= 49234
IGhpZ2huZXNz 49235
IO2MlOs= 49236
IGFtcGxpZmllZA== 49237
IFNlcmdleQ== 49238
IE1pbnM= 49239
d2FybQ== 49240
cGVsbA== 49241
b3BoaWxl 49242
IGjDqA== 49243
IEJlbG8= 49244
IFNrZXRjaA== 49245
IGNoYXJhY3Rlcml6YXRpb24= 49246
YW5zZW4= 49247
INGC0YPRgA== 49248
IOOFi+OFi+OFiw== 49249
Tm90ZQ== 49250
IGtvxZ8= 49251
IGNpZXJ0 49252
Zmx1 49253
IGJhaHQ= 49254
IERvd250b3du 49255
IENSSVM= 49256
b2RpZQ== 49257
MTQw 49258
IGxpdHJlcw== 49259
IGdyaWV2 49260
5qeY 49261
IOyUqOqwgA== 49262
IHN1Y2NlZWRz 49263
IF9f 49264
ZW50aW5n 49265
IHZpbW9z 49266
IHPDrA== 49267
ZGVmZW5zZQ== 49268
IE1jRA== 49269
IE1hcmlvbg== 49270
IERvbnQ= 49271
IEREUg== 49272
IExhemFy 49273
IERBUg== 49274
IGt1dg== 49275
S24= 49276
IHNlbWJsYQ== 49277
IGFpcmJvcm5l 49278
IFZpb2xlbmNl 49279
65CQ 49280
IHJlc3RyYWludA== 49281
IHdoaXN0bGVz 49282
IHNjb2xkZWQ= 49283
IGFjY2Vzbw== 49284
IGFic29sdXRhbWVudGU= 49285
IFR5bA== 49286
IFNhcA== 49287
toDrtoQ= 49288
aXTDpHRlbg== 49289
YWRlbQ== 49290
IMO9 49291
IHByZXNjcmliZQ== 49292
IE1hZ2U= 49293
IEhlbGVuYQ== 49294
5b6I5pyJ 49295
5Lqy 49296
dnQ= 49297
IHZpZW5lbg== 49298
IHNuZWV6 49299
IG1vbMOp 49300
xrDhu59uZw== 49301
IHRyYW5zcG9ydGluZw== 49302
IExlYW4= 49303
IGt1bmc= 49304
0YPRgNCw 49305
z4TOrQ== 49306
dXRjaGVz 49307
b25kZXJz 49308
bGl5b3I= 49309
TmF0 49310
IHppag== 49311
IG1hbW1hbA== 49312
IGvDpHl0 49313
IEpvYW5uYQ== 49314
c2VudA== 49315
IOCkuA== 49316
IHZlc3RlZA== 49317
IEVyZmFocnVuZw== 49318
b2tlZQ== 49319
IGNsaXBwaW5n 49320
IExpc3RlbmluZw== 49321
ICgj 49322
ZsO2 49323
IHZpZGFyZQ== 49324
IGJyaXR0bGU= 49325
IFNUQVJU 49326
IERhbWFz 49327
IFlvZw== 49328
44KT44Go 49329
Z2FydA== 49330
IHZlcmxpZXI= 49331
IGhlYXJ0ZmVsdA== 49332
IGRvxZvEhw== 49333
7LmY6rCA 49334
LsK7 49335
IG1heGltYWw= 49336
IGRpc3RpbnRvcw== 49337
IOyZnOuDkO2VmOuptA== 49338
IHNhaWxlZA== 49339
IGNvbnZleWVk 49340
IFRpbmRlcg== 49341
IFNVUEVS 49342
0L3QuNGG0YM= 49343
Y29udHJvbGxlZA== 49344
IGZ1bno= 49345
IGJhc3RhcmRz 49346
IEdpbnNidXJn 49347
IG51b3Zv 49348
IFBlcmU= 49349
IEpFUw== 49350
IERpbmdlbg== 49351
IEJldHM= 49352
dW1iYQ== 49353
YWNjacOzbg== 49354
IOyeiOyngOunjA== 49355
IHJldHJh 49356
IExhdXJlbnQ= 49357
IHBvenk= 49358
IGdyb292ZXM= 49359
IG3DoXF1aW5h 49360
IG1pbmlvbg== 49361
IGRlaW5lbg== 49362
IFNoYXVu 49363
15nXmQ== 49364
IGhvbm9yYXJ5 49365
b3NhdXJ1cw== 49366
IHplaXQ= 49367
IGVzcGVjaWU= 49368
IEJDRQ== 49369
0LDRgtC1 49370
SnVzdGlu 49371
IFdoZWVscw== 49372
IOydtO2VtA== 49373
INio2YrZhg== 49374
IHByb3B1bHNpb24= 49375
IHBlcmNlYmVy 49376
IE5ld21hbg== 49377
5bQ= 49378
Y3Vsb3Npcw== 49379
TWk= 49380
INCw0LrQutGD 49381
IG1hc3RlcmluZw== 49382
IGzDpGg= 49383
IGZpc3Rz 49384
5LuU 49385
IG1hcmluYWRl 49386
TGlsbHk= 49387
IOuFuOugpQ== 49388
IFlI 49389
IHVyZ2VudGx5 49390
IGluZm9ybWF0aW9uYWw= 49391
IGFjb3Jkbw== 49392
aXp6eQ== 49393
44GE44GP 49394
7J207Ja0 49395
aW1hcg== 49396
IOuCmOyYpOs= 49397
IHR3ZW50aWVz 49398
IHJhc3A= 49399
IGJ1bXB5 49400
2KjYqQ== 49401
d29ya2Vy 49402
IHF1aWNrZXN0 49403
IGF0dGFjaGVz 49404
0LLQuNCz 49405
IOuCmO2DgOs= 49406
IHB1cmVl 49407
IG92ZXJzaXplZA== 49408
IHN0aXJyZWQ= 49409
IGpha2lt 49410
IGhvbWljaWRl 49411
44KC44GX 49412
aXNjaWxsYQ== 49413
IOyxmQ== 49414
IHNwZWN1bGF0aXZl 49415
IGFzc2lzdHM= 49416
bWFpbg== 49417
asOkaHI= 49418
aW5kZXQ= 49419
IMWfdXI= 49420
IGZvcmVjYXN0cw== 49421
IGRpdmVyc2lvbg== 49422
IHRhcmU= 49423
IG9nbA== 49424
IE9yZ2FuaXNhdGlvbg== 49425
IENoZXZ5 49426
IGJhamE= 49427
YW5kxLFy 49428
INmI2YTYpw== 49429
IHJhZGlhbnQ= 49430
IGxpYWlzb24= 49431
IGRlbW9rcmF0 49432
IE1BUkM= 49433
z4DOv8+F 49434
IHJ1bnQ= 49435
IHByw6ljaXM= 49436
IGdldmVu 49437
IHbDqWhpYw== 49438
IEpFU1M= 49439
U1RS 49440
IOyWmOs= 49441
IHZpc2lvbmFyeQ== 49442
IGJ1cmFkYW4= 49443
IOOBguOCig== 49444
IHJlYmlydGg= 49445
IGV4aGliaXRlZA== 49446
IE1ldGFsbA== 49447
b2xpZQ== 49448
ZWx5bg== 49449
IGZsYXZvdXJz 49450
IGVzY3JpdG8= 49451
IERlbGV0ZQ== 49452
IOyVjOyVmOyWtA== 49453
INGD0LrRgNCw0ZfQvQ== 49454
IGludGVycnVwdGluZw== 49455
IGlkZW50aWZpYw== 49456
IFN1enVraQ== 49457
IExhbmRpbmc= 49458
5Lu25LqL5oOF 49459
YW5kaQ== 49460
IGVzdHJhbg== 49461
IGNvdWxldXI= 49462
IGFncmFk 49463
IFNueQ== 49464
IOCuh+Cusg== 49465
IGFuZGVy 49466
IHJ1YQ== 49467
IHByaXNl 49468
IGxhdXJl 49469
IO2KgA== 49470
IG1vZGVyYXRpb24= 49471
IGVyZmFocmVu 49472
IGRlY29uc3Q= 49473
IFJlZXNl 49474
IFBL 49475
ZXRvcw== 49476
44GT44KM44Gn 49477
IEdyYXZpdHk= 49478
IEVyZW4= 49479
IG92ZXJib2FyZA== 49480
IG3DvHNzdA== 49481
IEVtYWls 49482
0LXRgNC8 49483
eWRp 49484
acSZZHp5 49485
IExPVQ== 49486
IEZ1w59iYWxs 49487
IFJE 49488
YWx0cw== 49489
IOyKpO2KuOs= 49490
INCa0YDQsNGB 49491
IHRlbGV2 49492
INGA0L4= 49493
IHJlc2lnbmF0aW9u 49494
IGppbmdsZQ== 49495
IFN0dWRpZW4= 49496
IElY 49497
IFNlbnRpbmVs 49498
IFBhbmc= 49499
6YQ= 49500
SmFrZQ== 49501
IHBlcnNvbmFnZW0= 49502
IG3DqWRpYQ== 49503
IENoZXJu 49504
YW50aWNhbGx5 49505
IHRo4budaQ== 49506
IHBhcmFseXNpcw== 49507
IGphcGFuZXNl 49508
IGNvbmV4 49509
IGVmaWM= 49510
IHVuZGVyc2lkZQ== 49511
IG5lb2w= 49512
IGZpYW4= 49513
0LjQvNC+0YHRgtGM 49514
IHF1aXJreQ== 49515
IHBpc3Rh 49516
IENsZW1lbnQ= 49517
bm90aGluZw== 49518
INC/0L7QtdGF 49519
IGhvcnJlbmQ= 49520
IGNvbnNvbGlkYXRl 49521
cGxveXM= 49522
ZW1ha2Vy 49523
SmVubmlmZXI= 49524
IG51bcOpcm8= 49525
IGZhbW9zbw== 49526
IE5lcHR1bmU= 49527
IO2WiOyWtA== 49528
INC/0YDQtdC30LjQtA== 49529
IHNpdGNvbQ== 49530
IHNlcmlv 49531
IG11ZQ== 49532
IGdsYW5kcw== 49533
IGLDtnJqYXI= 49534
IFlK 49535
IFJpb3Q= 49536
cGFyYWd1cw== 49537
IHNlZ3VyYW7Dp2E= 49538
IGltbWF0dXJl 49539
IE1hZG9ubmE= 49540
4LiN 49541
IGxpbmdlcmluZw== 49542
IGFjZXNzbw== 49543
IE9yaWVudA== 49544
IFJlY29tbQ== 49545
IGNvbXBsYWM= 49546
Zm91bmRlZA== 49547
YXR0ZW5k 49548
IGNpZWxv 49549
IFpoYW4= 49550
bmFpcmVz 49551
Y2Nv 49552
INeQ16A= 49553
IHN0YXRh 49554
IGNvbnRyYWRpY3Rvcnk= 49555
IFPDqQ== 49556
IFNBTg== 49557
IENvbm5pZQ== 49558
IOuLueyLnA== 49559
INGB0LDQvNC+0Lk= 49560
IG1hamVzdGlj 49561
IFBlbmd1aW4= 49562
IENPTUU= 49563
w61jaW9z 49564
cGVybw== 49565
IG1n 49566
IGZhdWM= 49567
IGNvcnJlcg== 49568
IEdvdHRlcw== 49569
IEFuZ2xv 49570
SGFy 49571
4buXaQ== 49572
IHZpdGVzc2U= 49573
IGFubm91bmNlcg== 49574
IE9tYWhh 49575
a3Vt 49576
IHNwYXJlZA== 49577
INGA0LDQt9Cw 49578
INC/0L7Qu9GD0YfQuNGC0YHRjw== 49579
IHTDpGjDpG4= 49580
INC/0L7QvdCw0LQ= 49581
IHBlcnRhaW5pbmc= 49582
IFJhdGU= 49583
aWVybg== 49584
R29sZA== 49585
IHRlc3Rl 49586
IGRlxJ9pbGQ= 49587
IGRhbXBpbmc= 49588
IFBhcnRuZXJzaGlw 49589
enlzdGE= 49590
Z2VsZA== 49591
IHNtb2tlcw== 49592
IE1hcnJpYWdl 49593
7Kq97JeQ 49594
6IWz 49595
aXNjZQ== 49596
IHRyeW5h 49597
IERpcmVjdG9yeQ== 49598
IOuCmOyYrA== 49599
IHNoYW1lZnVs 49600
IG1lbnRyZQ== 49601
IGFzc2lnbmluZw== 49602
5piv6YCZ5qij 49603
IHJlcGVydG9pcmU= 49604
IG9iamV0b3M= 49605
56ix 49606
IHVuZGVyd29ybGQ= 49607
IGVuZGVhdm9ycw== 49608
IGlnbml0ZQ== 49609
INmI2Kw= 49610
IGV4cGVyaWVudA== 49611
INCX0LDQvw== 49612
INC30LDQutC70Y7Rhw== 49613
IHZvbHRhZ2Vz 49614
IG5pZWdv 49615
IGRlZmljaXRz 49616
IGJ1ZW5vcw== 49617
IFNsZWVwaW5n 49618
IFNhbGVt 49619
IHVubG9ja2luZw== 49620
IGludGVyYWN0ZWQ= 49621
IGVudGVuZGV1 49622
IFN1cGVyaW50ZW5kZW50 49623
IHN6Y3plZ8OzbA== 49624
IHF1YXM= 49625
IHBhbGluZw== 49626
IGtobw== 49627
2KjYrQ== 49628
IGNvbGFib3I= 49629
INC/0YDQuNCz0L7RgtC+0LI= 49630
IG1hdXY= 49631
IEp1ZGFz 49632
IEFzc2lzdA== 49633
INGC0LXRgNGA0Lg= 49634
INC90LDRgdC60L7Qu9GM0LrQvg== 49635
IHN1YnNpZHk= 49636
IEVtYmFzc3k= 49637
IGRhZ2Vu 49638
IFNhbnRv 49639
6Iis 49640
16nXldeR 49641
IGFicnVwdGx5 49642
IEFkYXB0 49643
IHZhYWs= 49644
IHBvc3RhbA== 49645
IGludmVzdGly 49646
IGZpcXVlaQ== 49647
IGRvd250aW1l 49648
IFdlYmI= 49649
IE5DQUE= 49650
IEVzdG95 49651
0L7Qu9C+0YI= 49652
IOyCrOqxtA== 49653
IG5hdGlvbmFsaXN0 49654
IEthdGhyeW4= 49655
IEtvcA== 49656
6ao= 49657
U2Vhbg== 49658
T05B 49659
IEJq 49660
16LXnQ== 49661
w61i 49662
aWRhbWVudGU= 49663
INCz0LvQsNC30LA= 49664
IHVubmll 49665
IGdlbWFha3Q= 49666
IElOVEVSVklFV0VS 49667
IEhhdXQ= 49668
zq/Ovw== 49669
Z2VvaXM= 49670
d3lkZA== 49671
INC60L7Qu9C4 49672
IHRpZ2h0ZW5lZA== 49673
IHBsYW5uZXJz 49674
IGhlcnVt 49675
IGfDtnLDvG4= 49676
IGVsZWN0cm9uaWNhbGx5 49677
IGNlcmFt 49678
IOuLpOyWke2VnA== 49679
IGVwaWxlcHN5 49680
IGXEnw== 49681
bGlucw== 49682
IFNoaW55 49683
5qCh 49684
INGB0L7Qu9C9 49685
IG1hY2Fyb24= 49686
IGltcGFjdG8= 49687
IFZlZ2Fu 49688
emXFhA== 49689
IFJhcGhh 49690
IFBhcnM= 49691
IExFTw== 49692
44GK44Gj 49693
Y8O8 49694
INec15TXmdeV16o= 49695
IMOkaG5saWNo 49696
IGZsb3Nz 49697
IEFa 49698
IG3DtmNodGVu 49699
IGdyb29taW5n 49700
IGdyYXNzZXM= 49701
cmFuY2g= 49702
IHJlY2liaXI= 49703
IGJvdW5jeQ== 49704
IEhvYmJ5 49705
IHZpa3RpZw== 49706
IGJlZ2l0dQ== 49707
IFBpY2Fzc28= 49708
IEt1c2g= 49709
66qo 49710
IG9ic3RydWN0aW9u 49711
IOu2hOychA== 49712
IG1pY3JvYg== 49713
IFdlc3RtaW5zdGVy 49714
cm9wcw== 49715
ZHVs 49716
IGRldm8= 49717
IExlaHJlcg== 49718
IEFkdmlzb3I= 49719
dWNrZW4= 49720
INCx0YPQvA== 49721
IGZsYXR0ZXJpbmc= 49722
IFRydW1hbg== 49723
IFNlbXByZQ== 49724
IE1jQ2Fpbg== 49725
IEhpbmR1cw== 49726
SnVsaWE= 49727
IHdhdGVyc2hlZA== 49728
IGx1c2g= 49729
7KCE6w== 49730
QmVmb3Jl 49731
INCS0YLQvtGA 49732
IFNhYVM= 49733
IHNpdHp0 49734
IGJlZXRsZQ== 49735
IEVzc2VudGlhbA== 49736
ZW5rbw== 49737
IOuVjOuPhA== 49738
IHJldnZpbmc= 49739
IHBvb3Jlcg== 49740
IGNvZXJj 49741
IGlkZWU= 49742
IGNvw7s= 49743
YWxldA== 49744
IHpkcm93 49745
IGZlbmRlcg== 49746
Z3Jvd3Ro 49747
RElORw== 49748
IHpkZQ== 49749
5LiK6Z2i 49750
RU5UUw== 49751
IGZhY2V0cw== 49752
6Zqq 49753
dXNoaW1h 49754
IMWfZWg= 49755
IHBhcmFzaXRl 49756
IGxhcHNl 49757
IE1lZXI= 49758
IEt1bmQ= 49759
IHNsb2c= 49760
IGJydW5jaA== 49761
IENoYXJ0 49762
YXJ6 49763
IE1VUw== 49764
IG9mZmVuc2Vz 49765
IGluZ2zDqXM= 49766
IGZvbGlhZ2U= 49767
b3BsYW4= 49768
QXV0 49769
IEphY3F1 49770
dGFr 49771
aWVtYnJl 49772
IHhlbg== 49773
IG5vbWluZWVz 49774
IGJpb21lZGljYWw= 49775
w6lzdXM= 49776
IGVzdHV2 49777
z4TPjA== 49778
QVRIQU4= 49779
IO2VnOuNsA== 49780
IGhlZWQ= 49781
Y3Jvc3N0YWxr 49782
QmlsbA== 49783
IHNwb3VzZXM= 49784
INGB0Y7Qtg== 49785
IHZlcnNv 49786
IFN2ZW4= 49787
IENhdQ== 49788
Y3V6 49789
IOuztOyEuOyalA== 49790
INGF0L7Qt9GP 49791
IG1vY2tpbmc= 49792
IE9uYQ== 49793
IETDoQ== 49794
IGZydWl0ZnVs 49795
IGJhbnF1ZXQ= 49796
dWRkaW5n 49797
aW5jdGlvbnM= 49798
ZGVydA== 49799
c3Vk 49800
IGRlc2Nvbg== 49801
IEpD 49802
IMKn 49803
IHB1Ymxp 49804
64iI 49805
6YGV44GG 49806
IGVudHNjaGllZGVu 49807
IFJPSQ== 49808
44GN44Gf 49809
IOyDneqyvA== 49810
IGvDpHl0dA== 49811
eWFuaQ== 49812
c2hhdw== 49813
IHVubGVhc2g= 49814
IG1hbm5l 49815
IGhpc3RvZ3JhbQ== 49816
5oql 49817
4Lit4Liw4LmE4Lij 49818
IGdu 49819
IGZlbGxh 49820
IGVpbmdlcw== 49821
IEJ1aWx0 49822
IHJlcHJlc2VudGE= 49823
IHB1bmlzaGluZw== 49824
IG91dHNpZGVycw== 49825
0L3Rg9GC0YzRgdGP 49826
Y3VycmVudA== 49827
IGZhbWlsaWFyaXR5 49828
INC00LjQsg== 49829
IHByb2pldHM= 49830
IGFxdWVsZXM= 49831
IEdsdWU= 49832
dGhvc2U= 49833
IGluY2VwdGlvbg== 49834
IGFxdWVsbG9z 49835
IGlsbHVzaW9ucw== 49836
IGF0dGVuZHM= 49837
cmVzZQ== 49838
IHN3YXJt 49839
IHN3YWI= 49840
IHJlZ2FyZGV6 49841
IHBvc2nDp8Ojbw== 49842
IGFraGly 49843
IGV4dHJhY3Rpbmc= 49844
IGFuZWNkb3Rl 49845
IFRhbGU= 49846
INCy0LjQvQ== 49847
IGFiZ2Vz 49848
IG9sdcWf 49849
IGNvbXBsaWNhZG8= 49850
IGNvdmFyaQ== 49851
0ZbRgtGM 49852
RGVy 49853
INeZ15Q= 49854
Rm9ybQ== 49855
IOyWtOyojOuToA== 49856
IHJlYWRhYmxl 49857
IGluaGliaXQ= 49858
IGRlY2lwaGVy 49859
IEFuZ3J5 49860
cGc= 49861
4K614K6k 49862
INGB0L7QsdGB0YLQstC10L3QvdC+ 49863
IHNhbWg= 49864
IGVzY3I= 49865
IGVuY29tcGFzc2Vz 49866
IGF1c3Rlcg== 49867
IGNvbmZpc2M= 49868
IE1hbmRhbA== 49869
IH0= 49870
YXRjaGVy 49871
PSM= 49872
55qE5pe25YCZ 49873
INC60LjQvdC+ 49874
IHN0YWw= 49875
bHVuZ3M= 49876
IHZvbGU= 49877
IHJlcXVpcw== 49878
IOOCiA== 49879
IHDDqW4= 49880
IGxlY3R1cmVy 49881
IGluc2NyaXB0aW9u 49882
IGNlcnZpY2Fs 49883
IFRyZWFzdXJl 49884
IEpX 49885
Y29taW5ncw== 49886
IGV5ZXNpZ2h0 49887
IFRhaWxz 49888
w61zaW1v 49889
IHdvcmtzaGVldA== 49890
IHN3aWZ0bHk= 49891
IGNvbm9z 49892
IGVsaW1pbmF0ZXM= 49893
IEJsYXpl 49894
0LDQu9C+0LM= 49895
IHBpY3R1cmVk 49896
IGdpcmFmZmU= 49897
IExvZ2lj 49898
5ZiJ 49899
IGVucmljaG1lbnQ= 49900
Rml0 49901
IHVuaW50ZW5kZWQ= 49902
IHBlcnNlY3V0ZWQ= 49903
YWthcA== 49904
67CY 49905
IGJhcmJlcg== 49906
IGFyYmVpdGV0 49907
IFN1cnByaXNpbmdseQ== 49908
IEF1dG9i 49909
dW5rdQ== 49910
cHJvdg== 49911
IExvY2g= 49912
b2J5bA== 49913
INC/0L7QtNCz0L7RgtC+0LI= 49914
IMOpY29ub21pcXVl 49915
IHBhdHQ= 49916
IGNlYXNlZA== 49917
INGB0L/QuNGB 49918
IG51Y2xlaQ== 49919
IGlzdGU= 49920
IFdhZw== 49921
IHp1cGXFgm5pZQ== 49922
IHByb3ZlcmI= 49923
IEFow60= 49924
5Zue5Y67 49925
bGlhbW8= 49926
IHJlbGlhYmx5 49927
IHBpaw== 49928
IFRyYWRpbmc= 49929
IENvbGVtYW4= 49930
IM6xzr3OsQ== 49931
IG1hZ2FyaQ== 49932
IFBISUw= 49933
IHNoZWRkaW5n 49934
b2huZXI= 49935
IHBvcm5vZ3JhcGh5 49936
IGJlbmVmaWNpYXJpZXM= 49937
4oCi 49938
ZW5pbg== 49939
IHJlc29sdmluZw== 49940
INGB0L/QvtGA0YI= 49941
INCx0LXQsw== 49942
IG5lY3Rhcg== 49943
dWx0dXJh 49944
aW1zaWNhbA== 49945
jIDrpbw= 49946
5bm05YmN 49947
44GX44KD 49948
IHZpc8Ojbw== 49949
6YGO5L6G 49950
w7/Dv8O/w7/Dv8O/w7/Dvw== 49951
YXR0Zm9ybQ== 49952
IOunnuuKlA== 49953
IHBpbGdyaW1hZ2U= 49954
IG1hdGluZw== 49955
IFJlYXBlcg== 49956
IEJyZWY= 49957
55Sf5rS7 49958
INeR15M= 49959
IG5vdmFtZW50ZQ== 49960
IGdyaWxsaW5n 49961
IFdpcmVsZXNz 49962
IFJvbWFuaWFu 49963
0ps= 49964
7Jyg6w== 49965
aGFpdA== 49966
IEJvcmE= 49967
QVJSWQ== 49968
IGh5cG90aGVzZXM= 49969
6ams 49970
aWt1dA== 49971
IOyVhOuyhA== 49972
INGW0Lc= 49973
IG5hdGlvbmFsZQ== 49974
2KrZiQ== 49975
w7xsbHQ= 49976
IMOpbMOpbWVudHM= 49977
IFdhcmU= 49978
ICgt 49979
0LDQu9GM0L3QvtC8 49980
IGluZGljdA== 49981
IFN0b25lcw== 49982
44Gf44KB 49983
ZXhwbG9zaW9u 49984
IOuDhOyDiA== 49985
IGZlbGlj 49986
IGp1ZGljaWFyeQ== 49987
IGluY2FybmF0aW9u 49988
IGlubmluZw== 49989
IGZvcm11bA== 49990
IHNoaXBtZW50 49991
IHJlaW5kZWVy 49992
5pKt 49993
INC+0LfQvdCw0Yc= 49994
IGVudm9s 49995
dW5keQ== 49996
INC30L3QsNGC0Yw= 49997
INCy0LjQtNC10LvQuA== 49998
IGV4Y2x1ZGluZw== 49999
ZGVhdGg= 50000
IGJlcm0= 50001
IHNvcHJhdHR1dHRv 50002
IGRlYmlkbw== 50003
IFppZw== 50004
IE92 50005
IEtFVklO 50006
IFBhbGU= 50007
IE1pcmU= 50008
IGFuZGFy 50009
aW5jbHVkaW5n 50010
IHN3YXBwZWQ= 50011
IG1pc2NvbmNlcHRpb25z 50012
IHNwb25n 50013
csOpYWw= 50014
IG9yYml0YWxz 50015
IGhhc2h0YWdz 50016
b3JpdA== 50017
IG1hdXZhaXM= 50018
0LjRgdCw 50019
IGxpdnJlcw== 50020
IElQUw== 50021
IDA0 50022
w7Zn 50023
aW5zdHI= 50024
INCy0L3QtdGI 50025
IGhpY2U= 50026
aXPDqWU= 50027
IG93ZXM= 50028
IGVzaW1lcms= 50029
IFVI 50030
IGlycml0YXRpb24= 50031
IGdpZ2dsZXM= 50032
IGNvbG9uaWFsaXNt 50033
IEJsaXNz 50034
c3RyaW5ncw== 50035
IHJldW5pdGVk 50036
IFBzYWtp 50037
d2FjaA== 50038
IGNsaWZmcw== 50039
IEZhbHNl 50040
w6Rn 50041
cGlwZQ== 50042
IHdob3BwaW5n 50043
IG1lcmluZ3Vl 50044
IGJ1bmc= 50045
aW5kdXN0cmll 50046
IGxlY2hl 50047
IExveQ== 50048
IGRyaWU= 50049
IHBhc3NhdA== 50050
IG9sZWg= 50051
IGPDqXU= 50052
IEdhYnJpZQ== 50053
IHJlZWZz 50054
IGJvbWJlcnM= 50055
IGVwaXPDs2Rpbw== 50056
IFJ1Zw== 50057
IFByb3Nl 50058
b25vcw== 50059
IG9iZXNl 50060
IGdvb2c= 50061
IHBpYWNl 50062
Zmxhbnplbg== 50063
6ZKf 50064
IGZsYXBz 50065
IEFsdG8= 50066
6aOf44G5 50067
Rmlu 50068
IHJlc2l6ZQ== 50069
6re4656o 50070
6LK7 50071
TmF0aGFu 50072
nojroKQ= 50073
INGC0LDQuQ== 50074
IE5GVA== 50075
IHNuZWV6ZQ== 50076
IHNocm91ZA== 50077
acOp 50078
IHZlcmFtZW50ZQ== 50079
IGNhc2NhZGU= 50080
IE9vaw== 50081
7JeG7J20 50082
IGluZnVzZWQ= 50083
ZnBz 50084
Y2VudGVy 50085
IGdyYXBwbGluZw== 50086
IFdvaG51bmc= 50087
IFR1bWI= 50088
IEltbWE= 50089
IER1eWd1c2Fs 50090
0LXQvdGC0Lg= 50091
IHN0ZXdhcmRzaGlw 50092
IGhhcnA= 50093
IGVuZG9yc2Vk 50094
xLFsYW4= 50095
INC+0LTQvdC40Lw= 50096
IGNvbXBldGVuY3k= 50097
IGJlcnQ= 50098
IFRhbGVz 50099
IHJoZQ== 50100
IG9oaA== 50101
IOqwhOuLqA== 50102
IG1STkE= 50103
IGdhbmdzdGVy 50104
IFJ1bm5lcg== 50105
0LXQvdC90YvQvA== 50106
cGhvcmlh 50107
IHfFgmHFm2Npd2ll 50108
IHF1YXJ0bw== 50109
IG9yZ2FuaXNl 50110
IFZldA== 50111
UGFk 50112
INmF2Ks= 50113
IHN0aW5rcw== 50114
IER1bA== 50115
dWVt 50116
aXNpZWo= 50117
VG9w 50118
IHR1c3Nlbg== 50119
IEVmZW5kaW1peg== 50120
IEJvdWxl 50121
IFNsb3Zlbg== 50122
IEzDtg== 50123
0ZHQtw== 50124
0YDQuNC/ 50125
Y2F2ZQ== 50126
IGJvw64= 50127
IGFwb2xvZ2lzZQ== 50128
IE1hcmx5 50129
IEV4cG9ydA== 50130
IENhaXRsaW4= 50131
IHRhdmFsbGE= 50132
IGVudGFpbHM= 50133
IGJyb20= 50134
IENvcGVuaA== 50135
IHdhbG51dA== 50136
IGluc2lzdHM= 50137
IGN14buZYw== 50138
IFF1aXQ= 50139
IERldmljZQ== 50140
15LXnQ== 50141
IERPVA== 50142
IHZlbG9jaWRhZA== 50143
TElF 50144
Q29vbA== 50145
IHNhbml0YXRpb24= 50146
IG9saG8= 50147
IEVC 50148
IO2ZleyLpO2eiA== 50149
INCc0LjRhQ== 50150
IHp1aw== 50151
IHN1cm5hbWU= 50152
IFNjaHVsZA== 50153
cnVmZg== 50154
Y3VsdHVyYWw= 50155
INGB0YLQvtC70YzQutC+ 50156
5pma5LiK 50157
jOuNsA== 50158
IHRvcnRv 50159
IGJhY2t1cHM= 50160
0YDQuNC5 50161
cmVsYXg= 50162
IHN5bmVyZ3k= 50163
IGJ1ZmZz 50164
IGFwbw== 50165
IFdlbGxuZXNz 50166
cm91bmRlZA== 50167
IHVuaXZlcnNlcw== 50168
IGZlcmE= 50169
IHN0YW5kYnk= 50170
IFNpbHZh 50171
IEpJ 50172
ZW5zb3JlZA== 50173
IOyXhuuLpA== 50174
INCQ0LI= 50175
INC+0YLQtNC10Ls= 50176
IGbDuA== 50177
IFJvY2tlZg== 50178
IENvbXBhc3M= 50179
IEJlYXJz 50180
IOS4jeimgQ== 50181
VHVybg== 50182
IHRo4buxYw== 50183
IHBvc3NpYmlsZQ== 50184
IGVzdGVt 50185
IENyb2F0aWE= 50186
IHTDpHTDpA== 50187
IENBTA== 50188
4LmA4Lie 50189
INGB0YLRgNCw0YU= 50190
IHNhbHRz 50191
IG1pbmltYWxpc3Q= 50192
IGluY29ycG9yYXRlcw== 50193
INmG24HbjNq6 50194
YWNhbw== 50195
IHNsYW1tZWQ= 50196
IGNhbWE= 50197
VGV4dA== 50198
ISEhISEh 50199
IGFsY2Fueg== 50200
w6ltYQ== 50201
IGluY2Vuc2U= 50202
IGhhcmRlbg== 50203
IGdyYW50aW5n 50204
IE5haQ== 50205
IEZpcm1h 50206
IGh5cG9j 50207
am9i 50208
IFJI 50209
enVy 50210
0LjQu9GP 50211
IMW6 50212
IGRhcmVz 50213
YW5o 50214
IOunjO2BvA== 50215
IGN1ZXN0acOzbg== 50216
IExpbWE= 50217
5pmv 50218
IGFzc3VudG8= 50219
IElQTw== 50220
IEJlbmdhbA== 50221
IEJpZXI= 50222
IHBzeWNoZQ== 50223
IGFjcXVhaW50ZWQ= 50224
IEfDvG4= 50225
0L7Qt9C4 50226
xZtjacSF 50227
QUc= 50228
IG1hbGZ1bmN0aW9u 50229
IGFzdGVyb2lkcw== 50230
aXJleg== 50231
YW1vcnBo 50232
INGB0L7RgtGA0YPQtA== 50233
IGZyZXNod2F0ZXI= 50234
IGFycmFu 50235
INC/0YDRiw== 50236
0L3QvtCz 50237
IGRpYWJldGlj 50238
INmC2KfZhA== 50239
IG9wcHJlc3M= 50240
IGNhcGFjaXRhbmNl 50241
cGVyZm9ybWFuY2U= 50242
Y3JhdGVz 50243
IGFwb3N0bGU= 50244
IEpFTg== 50245
T1VMRA== 50246
SW50cm8= 50247
IHN0YWxscw== 50248
IEFCT1VU 50249
Y3RpY2FtZW50ZQ== 50250
IGRpbGlnZW50 50251
IG1hbmlmZXN0cw== 50252
IFBha2lzdGFuaQ== 50253
ICgn 50254
5Zy6 50255
= 50256

>>>> whisper/mlx_whisper/assets/gpt2.tiktoken
IQ== 0
Ig== 1
Iw== 2
JA== 3
JQ== 4
Jg== 5
Jw== 6
KA== 7
KQ== 8
Kg== 9
Kw== 10
LA== 11
LQ== 12
Lg== 13
Lw== 14
MA== 15
MQ== 16
Mg== 17
Mw== 18
NA== 19
NQ== 20
Ng== 21
Nw== 22
OA== 23
OQ== 24
Og== 25
Ow== 26
PA== 27
PQ== 28
Pg== 29
Pw== 30
QA== 31
QQ== 32
Qg== 33
Qw== 34
RA== 35
RQ== 36
Rg== 37
Rw== 38
SA== 39
SQ== 40
Sg== 41
Sw== 42
TA== 43
TQ== 44
Tg== 45
Tw== 46
UA== 47
UQ== 48
Ug== 49
Uw== 50
VA== 51
VQ== 52
Vg== 53
Vw== 54
WA== 55
WQ== 56
Wg== 57
Ww== 58
XA== 59
XQ== 60
Xg== 61
Xw== 62
YA== 63
YQ== 64
Yg== 65
Yw== 66
ZA== 67
ZQ== 68
Zg== 69
Zw== 70
aA== 71
aQ== 72
ag== 73
aw== 74
bA== 75
bQ== 76
bg== 77
bw== 78
cA== 79
cQ== 80
cg== 81
cw== 82
dA== 83
dQ== 84
dg== 85
dw== 86
eA== 87
eQ== 88
eg== 89
ew== 90
fA== 91
fQ== 92
fg== 93
oQ== 94
og== 95
ow== 96
pA== 97
pQ== 98
pg== 99
pw== 100
qA== 101
qQ== 102
qg== 103
qw== 104
rA== 105
rg== 106
rw== 107
sA== 108
sQ== 109
sg== 110
sw== 111
tA== 112
tQ== 113
tg== 114
tw== 115
uA== 116
uQ== 117
ug== 118
uw== 119
vA== 120
vQ== 121
vg== 122
vw== 123
wA== 124
wQ== 125
wg== 126
ww== 127
xA== 128
xQ== 129
xg== 130
xw== 131
yA== 132
yQ== 133
yg== 134
yw== 135
zA== 136
zQ== 137
zg== 138
zw== 139
0A== 140
0Q== 141
0g== 142
0w== 143
1A== 144
1Q== 145
1g== 146
1w== 147
2A== 148
2Q== 149
2g== 150
2w== 151
3A== 152
3Q== 153
3g== 154
3w== 155
4A== 156
4Q== 157
4g== 158
4w== 159
5A== 160
5Q== 161
5g== 162
5w== 163
6A== 164
6Q== 165
6g== 166
6w== 167
7A== 168
7Q== 169
7g== 170
7w== 171
8A== 172
8Q== 173
8g== 174
8w== 175
9A== 176
9Q== 177
9g== 178
9w== 179
+A== 180
+Q== 181
+g== 182
+w== 183
/A== 184
/Q== 185
/g== 186
/w== 187
AA== 188
AQ== 189
Ag== 190
Aw== 191
BA== 192
BQ== 193
Bg== 194
Bw== 195
CA== 196
CQ== 197
Cg== 198
Cw== 199
DA== 200
DQ== 201
Dg== 202
Dw== 203
EA== 204
EQ== 205
Eg== 206
Ew== 207
FA== 208
FQ== 209
Fg== 210
Fw== 211
GA== 212
GQ== 213
Gg== 214
Gw== 215
HA== 216
HQ== 217
Hg== 218
Hw== 219
IA== 220
fw== 221
gA== 222
gQ== 223
gg== 224
gw== 225
hA== 226
hQ== 227
hg== 228
hw== 229
iA== 230
iQ== 231
ig== 232
iw== 233
jA== 234
jQ== 235
jg== 236
jw== 237
kA== 238
kQ== 239
kg== 240
kw== 241
lA== 242
lQ== 243
lg== 244
lw== 245
mA== 246
mQ== 247
mg== 248
mw== 249
nA== 250
nQ== 251
ng== 252
nw== 253
oA== 254
rQ== 255
IHQ= 256
IGE= 257
aGU= 258
aW4= 259
cmU= 260
b24= 261
IHRoZQ== 262
ZXI= 263
IHM= 264
YXQ= 265
IHc= 266
IG8= 267
ZW4= 268
IGM= 269
aXQ= 270
aXM= 271
YW4= 272
b3I= 273
ZXM= 274
IGI= 275
ZWQ= 276
IGY= 277
aW5n 278
IHA= 279
b3U= 280
IGFu 281
YWw= 282
YXI= 283
IHRv 284
IG0= 285
IG9m 286
IGlu 287
IGQ= 288
IGg= 289
IGFuZA== 290
aWM= 291
YXM= 292
bGU= 293
IHRo 294
aW9u 295
b20= 296
bGw= 297
ZW50 298
IG4= 299
IGw= 300
c3Q= 301
IHJl 302
dmU= 303
IGU= 304
cm8= 305
bHk= 306
IGJl 307
IGc= 308
IFQ= 309
Y3Q= 310
IFM= 311
aWQ= 312
b3Q= 313
IEk= 314
dXQ= 315
ZXQ= 316
IEE= 317
IGlz 318
IG9u 319
aW0= 320
YW0= 321
b3c= 322
YXk= 323
YWQ= 324
c2U= 325
IHRoYXQ= 326
IEM= 327
aWc= 328
IGZvcg== 329
YWM= 330
IHk= 331
dmVy 332
dXI= 333
IHU= 334
bGQ= 335
IHN0 336
IE0= 337
J3M= 338
IGhl 339
IGl0 340
YXRpb24= 341
aXRo 342
aXI= 343
Y2U= 344
IHlvdQ== 345
aWw= 346
IEI= 347
IHdo 348
b2w= 349
IFA= 350
IHdpdGg= 351
IDE= 352
dGVy 353
Y2g= 354
IGFz 355
IHdl 356
ICg= 357
bmQ= 358
aWxs 359
IEQ= 360
aWY= 361
IDI= 362
YWc= 363
ZXJz 364
a2U= 365
ICI= 366
IEg= 367
ZW0= 368
IGNvbg== 369
IFc= 370
IFI= 371
aGVy 372
IHdhcw== 373
IHI= 374
b2Q= 375
IEY= 376
dWw= 377
YXRl 378
IGF0 379
cmk= 380
cHA= 381
b3Jl 382
IFRoZQ== 383
IHNl 384
dXM= 385
IHBybw== 386
IGhh 387
dW0= 388
IGFyZQ== 389
IGRl 390
YWlu 391
YW5k 392
IG9y 393
aWdo 394
ZXN0 395
aXN0 396
YWI= 397
cm9t 398
IE4= 399
dGg= 400
IGNvbQ== 401
IEc= 402
dW4= 403
b3A= 404
MDA= 405
IEw= 406
IG5vdA== 407
ZXNz 408
IGV4 409
IHY= 410
cmVz 411
IEU= 412
ZXc= 413
aXR5 414
YW50 415
IGJ5 416
ZWw= 417
b3M= 418
b3J0 419
b2M= 420
cXU= 421
IGZyb20= 422
IGhhdmU= 423
IHN1 424
aXZl 425
b3VsZA== 426
IHNo 427
IHRoaXM= 428
bnQ= 429
cmE= 430
cGU= 431
aWdodA== 432
YXJ0 433
bWVudA== 434
IGFs 435
dXN0 436
ZW5k 437
LS0= 438
YWxs 439
IE8= 440
YWNr 441
IGNo 442
IGxl 443
aWVz 444
cmVk 445
YXJk 446
4oA= 447
b3V0 448
IEo= 449
IGFi 450
ZWFy 451
aXY= 452
YWxseQ== 453
b3Vy 454
b3N0 455
Z2g= 456
cHQ= 457
IHBs 458
YXN0 459
IGNhbg== 460
YWs= 461
b21l 462
dWQ= 463
VGhl 464
IGhpcw== 465
IGRv 466
IGdv 467
IGhhcw== 468
Z2U= 469
J3Q= 470
IFU= 471
cm91 472
IHNh 473
IGo= 474
IGJ1dA== 475
IHdvcg== 476
IGFsbA== 477
ZWN0 478
IGs= 479
YW1l 480
IHdpbGw= 481
b2s= 482
IHdoZQ== 483
IHRoZXk= 484
aWRl 485
MDE= 486
ZmY= 487
aWNo 488
cGw= 489
dGhlcg== 490
IHRy 491
Li4= 492
IGludA== 493
aWU= 494
dXJl 495
YWdl 496
IG5l 497
aWFs 498
YXA= 499
aW5l 500
aWNl 501
IG1l 502
IG91dA== 503
YW5z 504
b25l 505
b25n 506
aW9ucw== 507
IHdobw== 508
IEs= 509
IHVw 510
IHRoZWly 511
IGFk 512
IDM= 513
IHVz 514
YXRlZA== 515
b3Vz 516
IG1vcmU= 517
dWU= 518
b2c= 519
IFN0 520
aW5k 521
aWtl 522
IHNv 523
aW1l 524
cGVy 525
LiI= 526
YmVy 527
aXo= 528
YWN0 529
IG9uZQ== 530
IHNhaWQ= 531
IC0= 532
YXJl 533
IHlvdXI= 534
Y2M= 535
IFRo 536
IGNs 537
ZXA= 538
YWtl 539
YWJsZQ== 540
aXA= 541
IGNvbnQ= 542
IHdoaWNo 543
aWE= 544
IGlt 545
IGFib3V0 546
IHdlcmU= 547
dmVyeQ== 548
dWI= 549
IGhhZA== 550
IGVu 551
IGNvbXA= 552
LCI= 553
IElu 554
IHVu 555
IGFn 556
aXJl 557
YWNl 558
YXU= 559
YXJ5 560
IHdvdWxk 561
YXNz 562
cnk= 563
IOKA 564
Y2w= 565
b29r 566
ZXJl 567
c28= 568
IFY= 569
aWdu 570
aWI= 571
IG9mZg== 572
IHRl 573
dmVu 574
IFk= 575
aWxl 576
b3Nl 577
aXRl 578
b3Jt 579
IDIwMQ== 580
IHJlcw== 581
IG1hbg== 582
IHBlcg== 583
IG90aGVy 584
b3Jk 585
dWx0 586
IGJlZW4= 587
IGxpa2U= 588
YXNl 589
YW5jZQ== 590
a3M= 591
YXlz 592
b3du 593
ZW5jZQ== 594
IGRpcw== 595
Y3Rpb24= 596
IGFueQ== 597
IGFwcA== 598
IHNw 599
aW50 600
cmVzcw== 601
YXRpb25z 602
YWls 603
IDQ= 604
aWNhbA== 605
IHRoZW0= 606
IGhlcg== 607
b3VudA== 608
IENo 609
IGFy 610
IGlm 611
IHRoZXJl 612
IHBl 613
IHllYXI= 614
YXY= 615
IG15 616
IHNvbWU= 617
IHdoZW4= 618
b3VnaA== 619
YWNo 620
IHRoYW4= 621
cnU= 622
b25k 623
aWNr 624
IG92ZXI= 625
dmVs 626
IHF1 627
Cgo= 628
IHNj 629
cmVhdA== 630
cmVl 631
IEl0 632
b3VuZA== 633
cG9ydA== 634
IGFsc28= 635
IHBhcnQ= 636
ZnRlcg== 637
IGtu 638
IGJlYw== 639
IHRpbWU= 640
ZW5z 641
IDU= 642
b3BsZQ== 643
IHdoYXQ= 644
IG5v 645
ZHU= 646
bWVy 647
YW5n 648
IG5ldw== 649
LS0tLQ== 650
IGdldA== 651
b3J5 652
aXRpb24= 653
aW5ncw== 654
IGp1c3Q= 655
IGludG8= 656
IDA= 657
ZW50cw== 658
b3Zl 659
dGU= 660
IHBlb3BsZQ== 661
IHByZQ== 662
IGl0cw== 663
IHJlYw== 664
IHR3 665
aWFu 666
aXJzdA== 667
YXJr 668
b3Jz 669
IHdvcms= 670
YWRl 671
b2I= 672
IHNoZQ== 673
IG91cg== 674
d24= 675
aW5r 676
bGlj 677
IDE5 678
IEhl 679
aXNo 680
bmRlcg== 681
YXVzZQ== 682
IGhpbQ== 683
b25z 684
IFs= 685
IHJv 686
Zm9ybQ== 687
aWxk 688
YXRlcw== 689
dmVycw== 690
IG9ubHk= 691
b2xs 692
IHNwZQ== 693
Y2s= 694
ZWxs 695
YW1w 696
IGFjYw== 697
IGJs 698
aW91cw== 699
dXJu 700
ZnQ= 701
b29k 702
IGhvdw== 703
aGVk 704
ICc= 705
IGFmdGVy 706
YXc= 707
IGF0dA== 708
b3Y= 709
bmU= 710
IHBsYXk= 711
ZXJ2 712
aWN0 713
IGNvdWxk 714
aXR0 715
IGFt 716
IGZpcnN0 717
IDY= 718
IGFjdA== 719
ICQ= 720
ZWM= 721
aGluZw== 722
dWFs 723
dWxs 724
IGNvbW0= 725
b3k= 726
b2xk 727
Y2Vz 728
YXRlcg== 729
IGZl 730
IGJldA== 731
d2U= 732
aWZm 733
IHR3bw== 734
b2Nr 735
IGJhY2s= 736
KS4= 737
aWRlbnQ= 738
IHVuZGVy 739
cm91Z2g= 740
c2Vs 741
eHQ= 742
IG1heQ== 743
cm91bmQ= 744
IHBv 745
cGg= 746
aXNz 747
IGRlcw== 748
IG1vc3Q= 749
IGRpZA== 750
IGFkZA== 751
amVjdA== 752
IGluYw== 753
Zm9yZQ== 754
IHBvbA== 755
b250 756
IGFnYWlu 757
Y2x1ZA== 758
dGVybg== 759
IGtub3c= 760
IG5lZWQ= 761
IGNvbnM= 762
IGNv 763
IC4= 764
IHdhbnQ= 765
IHNlZQ== 766
IDc= 767
bmluZw== 768
aWV3 769
IFRoaXM= 770
Y2Vk 771
IGV2ZW4= 772
IGluZA== 773
dHk= 774
IFdl 775
YXRo 776
IHRoZXNl 777
IHBy 778
IHVzZQ== 779
IGJlY2F1c2U= 780
IGZs 781
bmc= 782
IG5vdw== 783
IOKAkw== 784
Y29t 785
aXNl 786
IG1ha2U= 787
IHRoZW4= 788
b3dlcg== 789
IGV2ZXJ5 790
IFVu 791
IHNlYw== 792
b3Nz 793
dWNo 794
IGVt 795
ID0= 796
IFJl 797
aWVk 798
cml0 799
IGludg== 800
bGVjdA== 801
IHN1cHA= 802
YXRpbmc= 803
IGxvb2s= 804
bWFu 805
cGVjdA== 806
IDg= 807
cm93 808
IGJ1 809
IHdoZXJl 810
aWZpYw== 811
IHllYXJz 812
aWx5 813
IGRpZmY= 814
IHNob3VsZA== 815
IHJlbQ== 816
VGg= 817
SW4= 818
IGV2 819
ZGF5 820
J3Jl 821
cmli 822
IHJlbA== 823
c3M= 824
IGRlZg== 825
IHJpZ2h0 826
IHN5 827
KSw= 828
bGVz 829
MDAw 830
aGVu 831
IHRocm91Z2g= 832
IFRy 833
X18= 834
IHdheQ== 835
IGRvbg== 836
ICw= 837
IDEw 838
YXNlZA== 839
IGFzcw== 840
dWJsaWM= 841
IHJlZw== 842
IEFuZA== 843
aXg= 844
IHZlcnk= 845
IGluY2x1ZA== 846
b3RoZXI= 847
IGltcA== 848
b3Ro 849
IHN1Yg== 850
IOKAlA== 851
IGJlaW5n 852
YXJn 853
IFdo 854
PT0= 855
aWJsZQ== 856
IGRvZXM= 857
YW5nZQ== 858
cmFt 859
IDk= 860
ZXJ0 861
cHM= 862
aXRlZA== 863
YXRpb25hbA== 864
IGJy 865
IGRvd24= 866
IG1hbnk= 867
YWtpbmc= 868
IGNhbGw= 869
dXJpbmc= 870
aXRpZXM= 871
IHBo 872
aWNz 873
YWxz 874
IGRlYw== 875
YXRpdmU= 876
ZW5lcg== 877
IGJlZm9yZQ== 878
aWxpdHk= 879
IHdlbGw= 880
IG11Y2g= 881
ZXJzb24= 882
IHRob3Nl 883
IHN1Y2g= 884
IGtl 885
IGVuZA== 886
IEJ1dA== 887
YXNvbg== 888
dGluZw== 889
IGxvbmc= 890
ZWY= 891
IHRoaW5r 892
eXM= 893
IGJlbA== 894
IHNt 895
aXRz 896
YXg= 897
IG93bg== 898
IHByb3Y= 899
IHNldA== 900
aWZl 901
bWVudHM= 902
Ymxl 903
d2FyZA== 904
IHNob3c= 905
IHByZXM= 906
bXM= 907
b21ldA== 908
IG9i 909
IHNheQ== 910
IFNo 911
dHM= 912
ZnVs 913
IGVmZg== 914
IGd1 915
IGluc3Q= 916
dW5k 917
cmVu 918
Y2Vzcw== 919
IGVudA== 920
IFlvdQ== 921
IGdvb2Q= 922
IHN0YXJ0 923
aW5jZQ== 924
IG1hZGU= 925
dHQ= 926
c3RlbQ== 927
b2xvZw== 928
dXA= 929
IHw= 930
dW1w 931
IGhlbA== 932
dmVybg== 933
dWxhcg== 934
dWFsbHk= 935
IGFj 936
IG1vbg== 937
IGxhc3Q= 938
IDIwMA== 939
MTA= 940
IHN0dWQ= 941
dXJlcw== 942
IEFy 943
c2VsZg== 944
YXJz 945
bWVyaWM= 946
dWVz 947
Y3k= 948
IG1pbg== 949
b2xsb3c= 950
IGNvbA== 951
aW8= 952
IG1vZA== 953
IGNvdW50 954
IENvbQ== 955
aGVz 956
IGZpbg== 957
YWly 958
aWVy 959
4oCU 960
cmVhZA== 961
YW5r 962
YXRjaA== 963
ZXZlcg== 964
IHN0cg== 965
IHBvaW50 966
b3Jr 967
IE5ldw== 968
IHN1cg== 969
b29s 970
YWxr 971
ZW1lbnQ= 972
IHVzZWQ= 973
cmFjdA== 974
d2Vlbg== 975
IHNhbWU= 976
b3Vu 977
IEFs 978
Y2k= 979
IGRpZmZlcmU= 980
IHdoaWxl 981
LS0tLS0tLS0= 982
IGdhbWU= 983
Y2VwdA== 984
IHNpbQ== 985
Li4u 986
IGludGVy 987
ZWs= 988
IHJlcG9ydA== 989
IHByb2R1 990
IHN0aWxs 991
bGVk 992
YWg= 993
IGhlcmU= 994
IHdvcmxk 995
IHRob3VnaA== 996
IG51bQ== 997
YXJjaA== 998
aW1lcw== 999
YWxl 1000
IFNl 1001
IElm 1002
Ly8= 1003
IExl 1004
IHJldA== 1005
IHJlZg== 1006
IHRyYW5z 1007
bmVy 1008
dXRpb24= 1009
dGVycw== 1010
IHRha2U= 1011
IENs 1012
IGNvbmY= 1013
d2F5 1014
YXZl 1015
IGdvaW5n 1016
IHNs 1017
dWc= 1018
IEFtZXJpYw== 1019
IHNwZWM= 1020
IGhhbmQ= 1021
IGJldHdlZW4= 1022
aXN0cw== 1023
IERl 1024
b290 1025
SXQ= 1026
IGVhcg== 1027
IGFnYWluc3Q= 1028
IGhpZ2g= 1029
Z2Fu 1030
YXo= 1031
YXRoZXI= 1032
IGV4cA== 1033
IG9w 1034
IGlucw== 1035
IGdy 1036
IGhlbHA= 1037
IHJlcXU= 1038
ZXRz 1039
aW5z 1040
IFBybw== 1041
aXNt 1042
IGZvdW5k 1043
bGFuZA== 1044
YXRh 1045
dXNz 1046
YW1lcw== 1047
IHBlcnNvbg== 1048
IGdyZWF0 1049
cHI= 1050
IHNpZ24= 1051
IEFu 1052
J3Zl 1053
IHNvbWV0 1054
IHNlcg== 1055
aGlw 1056
IHJ1bg== 1057
IDo= 1058
IHRlcg== 1059
aXJlY3Q= 1060
IGZvbGxvdw== 1061
IGRldA== 1062
aWNlcw== 1063
IGZpbmQ= 1064
MTI= 1065
IG1lbQ== 1066
IGNy 1067
ZXJlZA== 1068
ZXg= 1069
IGV4dA== 1070
dXRo 1071
ZW5zZQ== 1072
Y28= 1073
IHRlYW0= 1074
dmluZw== 1075
b3VzZQ== 1076
YXNo 1077
YXR0 1078
dmVk 1079
IHN5c3RlbQ== 1080
IEFz 1081
ZGVy 1082
aXZlcw== 1083
bWlu 1084
IGxlYWQ= 1085
IEJs 1086
Y2VudA== 1087
IGFyb3VuZA== 1088
IGdvdmVybg== 1089
IGN1cg== 1090
dmVsb3A= 1091
YW55 1092
IGNvdXI= 1093
YWx0aA== 1094
YWdlcw== 1095
aXpl 1096
IGNhcg== 1097
b2Rl 1098
IGxhdw== 1099
IHJlYWQ= 1100
J20= 1101
Y29u 1102
IHJlYWw= 1103
IHN1cHBvcnQ= 1104
IDEy 1105
Li4uLg== 1106
IHJlYWxseQ== 1107
bmVzcw== 1108
IGZhY3Q= 1109
IGRheQ== 1110
IGJvdGg= 1111
eWluZw== 1112
IHNlcnY= 1113
IEZvcg== 1114
IHRocmVl 1115
IHdvbQ== 1116
IG1lZA== 1117
b2R5 1118
IFRoZXk= 1119
NTA= 1120
IGV4cGVy 1121
dG9u 1122
IGVhY2g= 1123
YWtlcw== 1124
IGNoZQ== 1125
IGNyZQ== 1126
aW5lcw== 1127
IHJlcA== 1128
MTk= 1129
Z2c= 1130
aWxsaW9u 1131
IGdyb3U= 1132
dXRl 1133
aWs= 1134
V2U= 1135
Z2V0 1136
RVI= 1137
IG1ldA== 1138
IHNheXM= 1139
b3g= 1140
IGR1cmluZw== 1141
ZXJu 1142
aXplZA== 1143
YXJlZA== 1144
IGZhbQ== 1145
aWNhbGx5 1146
IGhhcHA= 1147
IElz 1148
IGNoYXI= 1149
bWVk 1150
dmVudA== 1151
IGdlbmVy 1152
aWVudA== 1153
cGxl 1154
aWV0 1155
cmVudA== 1156
MTE= 1157
dmVz 1158
cHRpb24= 1159
IDIw 1160
Zm9ybWF0aW9u 1161
IGNvcg== 1162
IG9mZmlj 1163
aWVsZA== 1164
IHRvbw== 1165
aXNpb24= 1166
IGluZg== 1167
IFo= 1168
dGhl 1169
b2Fk 1170
IHB1YmxpYw== 1171
IHByb2c= 1172
cmlj 1173
Kio= 1174
IHdhcg== 1175
IHBvd2Vy 1176
dmlldw== 1177
IGZldw== 1178
IGxvYw== 1179
IGRpZmZlcmVudA== 1180
IHN0YXRl 1181
IGhlYWQ= 1182
J2xs 1183
IHBvc3M= 1184
IHN0YXQ= 1185
cmV0 1186
YW50cw== 1187
IHZhbA== 1188
IGlzcw== 1189
IGNsZQ== 1190
aXZlcnM= 1191
YW5j 1192
IGV4cGw= 1193
IGFub3RoZXI= 1194
IFE= 1195
IGF2 1196
dGhpbmc= 1197
bmNl 1198
V2g= 1199
IGNoaWxk 1200
IHNpbmNl 1201
aXJlZA== 1202
bGVzcw== 1203
IGxpZmU= 1204
IGRldmVsb3A= 1205
aXR0bGU= 1206
IGRlcA== 1207
IHBhc3M= 1208
44M= 1209
IHR1cm4= 1210
b3Ju 1211
VGhpcw== 1212
YmVycw== 1213
cm9zcw== 1214
IEFk 1215
IGZy 1216
IHJlc3A= 1217
IHNlY29uZA== 1218
b2g= 1219
IC8= 1220
IGRpc2M= 1221
ICY= 1222
IHNvbWV0aGluZw== 1223
IGNvbXBsZQ== 1224
IGVk 1225
IGZpbA== 1226
IG1vbnRo 1227
YWo= 1228
dWM= 1229
IGdvdmVybm1lbnQ= 1230
IHdpdGhvdXQ= 1231
IGxlZw== 1232
IGRpc3Q= 1233
IHB1dA== 1234
IHF1ZXN0 1235
YW5u 1236
IHByb3Q= 1237
MjA= 1238
IG5ldmVy 1239
aWVuY2U= 1240
IGxldmVs 1241
IGFydA== 1242
IHRoaW5ncw== 1243
IG1pZ2h0 1244
IGVmZmVjdA== 1245
IGNvbnRybw== 1246
IGNlbnQ= 1247
IDE4 1248
IGFsbG93 1249
IGJlbGll 1250
Y2hvb2w= 1251
b3R0 1252
IGluY3Jl 1253
IGZlZWw= 1254
IHJlc3VsdA== 1255
IGxvdA== 1256
IGZ1bg== 1257
b3Rl 1258
IHR5 1259
ZXJlc3Q= 1260
IGNvbnRpbg== 1261
IHVzaW5n 1262
IGJpZw== 1263
MjAx 1264
IGFzaw== 1265
IGJlc3Q= 1266
ICk= 1267
SU4= 1268
IG9wcA== 1269
MzA= 1270
IG51bWJlcg== 1271
aW5lc3M= 1272
U3Q= 1273
bGVhc2U= 1274
IGNh 1275
IG11c3Q= 1276
IGRpcmVjdA== 1277
IGds 1278
IDw= 1279
IG9wZW4= 1280
IHBvc3Q= 1281
IGNvbWU= 1282
IHNlZW0= 1283
b3JkaW5n 1284
IHdlZWs= 1285
YXRlbHk= 1286
aXRhbA== 1287
IGVs 1288
cmllbmQ= 1289
IGZhcg== 1290
IHRyYQ== 1291
aW5hbA== 1292
IHByaQ== 1293
IFVT 1294
IHBsYWNl 1295
IGZvcm0= 1296
IHRvbGQ= 1297
Ijo= 1298
YWlucw== 1299
YXR1cmU= 1300
IFRydW1w 1301
IHN0YW5k 1302
ICM= 1303
aWRlcg== 1304
IEZy 1305
IG5leHQ= 1306
IHNvYw== 1307
IHB1cg== 1308
IGxldA== 1309
IGxpdHRsZQ== 1310
IGh1bQ== 1311
IGk= 1312
cm9u 1313
MTU= 1314
IDE1 1315
IGNvbW11bg== 1316
IG1hcms= 1317
IFRoZXJl 1318
IHdy 1319
IFRoYXQ= 1320
IGluZm9ybWF0aW9u 1321
d2F5cw== 1322
IGJ1cw== 1323
YXBw 1324
IGludmVzdA== 1325
bWU= 1326
IGhhcmQ= 1327
YWluZWQ= 1328
ZWFk 1329
IGltcG9ydA== 1330
IGFwcHJv 1331
IHRlc3Q= 1332
IHRyaQ== 1333
IHJlc3Q= 1334
b3NlZA== 1335
IGZ1bGw= 1336
IGNhcmU= 1337
IFNw 1338
IGNhc2U= 1339
T04= 1340
IHNr 1341
IGxlc3M= 1342
ICs= 1343
IHBhcnRpYw== 1344
IFBs 1345
YWJseQ== 1346
dWNr 1347
aXNoZWQ= 1348
Y2hu 1349
YmU= 1350
IGxpc3Q= 1351
YXRvcg== 1352
IHRvcA== 1353
IGFkdg== 1354
IEJl 1355
cnVjdA== 1356
IGRlbQ== 1357
cmF0aW9u 1358
bGluZw== 1359
Z3k= 1360
cmVlbg== 1361
Z2Vy 1362
IGhvbWU= 1363
IGxlZnQ= 1364
IGJldHRlcg== 1365
IGRhdGE= 1366
IDEx 1367
IGF0dGFjaw== 1368
IHByb2JsZQ== 1369
bGluZQ== 1370
YXJkcw== 1371
IGJlaA== 1372
cmFs 1373
IEhvdw== 1374
IFNoZQ== 1375
YXJnZQ== 1376
IC0t 1377
Oi8v 1378
IGJybw== 1379
IFBo 1380
YXRz 1381
IGJ1aWxk 1382
d3c= 1383
aWRlZA== 1384
YWlt 1385
YXNlcw== 1386
ZW5jeQ== 1387
IG1haW4= 1388
aW5lZA== 1389
IGluY2x1ZGluZw== 1390
IHs= 1391
IGdvdA== 1392
IGludGVyZXN0 1393
IGtlZXA= 1394
IFg= 1395
IGVhcw== 1396
YWluaW5n 1397
IGNsYXNz 1398
4oCm 1399
IE5v 1400
IHZhcg== 1401
IHNtYWxs 1402
YW1wbGU= 1403
QVQ= 1404
IGlkZQ== 1405
IFNv 1406
IHJlY2U= 1407
IHBvbGl0 1408
IG1vdg== 1409
IHBsYW4= 1410
IHBlcmNlbnQ= 1411
aXZpbmc= 1412
IGNhbXA= 1413
IHBheQ== 1414
MTQ= 1415
c2M= 1416
aXNlZA== 1417
IHVudA== 1418
b25leQ== 1419
cGxveQ== 1420
PT09PQ== 1421
IGRpZG4= 1422
IEluZA== 1423
ZWxz 1424
ZXJ0YWlu 1425
IHBvcw== 1426
X19fXw== 1427
aXZlcg== 1428
IHByb2Nlc3M= 1429
IHByb2dyYW0= 1430
aWZpZWQ= 1431
IFJlcA== 1432
MTY= 1433
dXJv 1434
b2xvZ3k= 1435
YXR0ZXI= 1436
aW5h 1437
IG5hbWU= 1438
IEFsbA== 1439
IGZvdXI= 1440
IHJldHVybg== 1441
dmlvdXM= 1442
YnM= 1443
IGNhbGxlZA== 1444
IG1vdmU= 1445
IFNj 1446
aXJk 1447
IGdyb3Vw 1448
IGJyZQ== 1449
IG1lbg== 1450
IGNhcA== 1451
dGVu 1452
ZWU= 1453
IGRyaQ== 1454
bGVn 1455
aGVyZQ== 1456
dXRob3I= 1457
IHBhdA== 1458
IGN1cnJlbnQ= 1459
aWRlcw== 1460
IHBvcA== 1461
dG8= 1462
ZW50aW9u 1463
IGFsd2F5cw== 1464
IG1pbA== 1465
IHdvbWVu 1466
IDE2 1467
IG9sZA== 1468
aXZlbg== 1469
cmFwaA== 1470
IE9y 1471
cm9y 1472
ZW50bHk= 1473
IG5lYXI= 1474
IEV4 1475
cmVhbQ== 1476
c2g= 1477
IDE0 1478
IGZyZWU= 1479
aXNzaW9u 1480
c3RhbmQ= 1481
IENvbg== 1482
YWxpdHk= 1483
dXNlZA== 1484
MTM= 1485
IGRlc2lnbg== 1486
IGNoYW5nZQ== 1487
IGNoYW5n 1488
IGJv 1489
IHZpcw== 1490
ZW1iZXI= 1491
IGJvb2s= 1492
cmVhZHk= 1493
IGtpbGw= 1494
MjU= 1495
cHBlZA== 1496
IGF3YXk= 1497
IGFibGU= 1498
IGNvdW50cnk= 1499
IGNvbnN0 1500
YXJu 1501
IG9yZGVy 1502
QVI= 1503
aW9y 1504
aXVt 1505
b3J0aA== 1506
MTg= 1507
YWlsYWJsZQ== 1508
IHN3 1509
IG1pbGxpb24= 1510
IDEz 1511
YXRpYw== 1512
dGVk 1513
IEdv 1514
IG9wZXI= 1515
ZW5n 1516
IHRoaW5n 1517
YWpvcg== 1518
Y29ub20= 1519
IENvbW0= 1520
IHdoeQ== 1521
dXJlZA== 1522
dXJhbA== 1523
IHNjaG9vbA== 1524
Ynk= 1525
IE1hcg== 1526
IGFmZg== 1527
IGRheXM= 1528
IGFubg== 1529
dXNo 1530
YW5l 1531
SWY= 1532
ZWc= 1533
IHByb2Y= 1534
IGhlYWx0aA== 1535
b3V0aA== 1536
QnV0 1537
aW9uYWw= 1538
Liw= 1539
IHNvbA== 1540
IGFscmVhZHk= 1541
IDMw 1542
IGNoYXJhY3Q= 1543
SGU= 1544
IGZyaWVuZA== 1545
RVM= 1546
aWFucw== 1547
aWNsZQ== 1548
J2Q= 1549
IE9u 1550
IGxlYXN0 1551
IHByb20= 1552
IGRy 1553
IGhpc3Q= 1554
aXRoZXI= 1555
IGVzdA== 1556
aXF1 1557
MTc= 1558
c29u 1559
IHRlbGw= 1560
IHRhbGs= 1561
b2hu 1562
b2ludA== 1563
bGVjdGlvbg== 1564
QU4= 1565
IHVudGls 1566
YXVnaA== 1567
IGxhdGVy 1568
IHZl 1569
IHZpZXc= 1570
ZW5kaW5n 1571
aXZlZA== 1572
IHdvcmQ= 1573
d2FyZQ== 1574
IGNvc3Q= 1575
IGVub3VnaA== 1576
IGdpdmU= 1577
IFVuaXRlZA== 1578
IHRlY2hu 1579
YXJlbnQ= 1580
T1I= 1581
IHBhcg== 1582
IERy 1583
IDIwMTY= 1584
cmlzdA== 1585
ZXJpbmc= 1586
IMI= 1587
IGxhcmdl 1588
c2lkZQ== 1589
YWN5 1590
Y2Nlc3M= 1591
IHdpbg== 1592
IGltcG9ydGFudA== 1593
IDE5OQ== 1594
IGRvZXNu 1595
IDE3 1596
IGJ1c2luZXNz 1597
IGNsZWFy 1598
IHJlc2U= 1599
Iiw= 1600
dXJ5 1601
IGVxdQ== 1602
YXN0ZXI= 1603
YWxm 1604
IEFtZXJpY2Fu 1605
bmVjdA== 1606
IGV4cGVjdA== 1607
aXZlcnNpdHk= 1608
IG9jYw== 1609
IEZs 1610
IGtpbmQ= 1611
IG1lYW4= 1612
IHBhc3Q= 1613
IGRldg== 1614
IGJhcw== 1615
bGV0 1616
cmFmdA== 1617
IG9yZ2Fu 1618
IGRlbA== 1619
IHBlcmZvcm0= 1620
IHN0b3J5 1621
IHNlYXNvbg== 1622
IENvbA== 1623
IGNsYWlt 1624
IGNhbWU= 1625
IHdpdGhpbg== 1626
IGxpbmU= 1627
IHByb2plY3Q= 1628
IEF0 1629
IGNvbnRyb2w= 1630
ZW5kZWQ= 1631
IFN5 1632
IGFpcg== 1633
aXphdGlvbg== 1634
ICo= 1635
bGV5 1636
IG1vbmV5 1637
aWRk 1638
WW91 1639
Zm9y 1640
IGZhbWlseQ== 1641
IG1ha2luZw== 1642
IGJpdA== 1643
IHBvbGljZQ== 1644
IGhhcHBlbg== 1645
IHZlcnM= 1646
b255 1647
dWZm 1648
IFdoZW4= 1649
IHNpdA== 1650
aWRlbw== 1651
bGY= 1652
aXNvbg== 1653
IHN1cmU= 1654
Z2lu 1655
IGFwcGVhcg== 1656
IGxpZ2h0 1657
IGVz 1658
b2Y= 1659
IHdhdGVy 1660
IHRpbWVz 1661
bm90 1662
IGdyb3c= 1663
IGNvbXBhbnk= 1664
IFRl 1665
b3dz 1666
IG1hcg== 1667
b3VyY2U= 1668
aW9s 1669
YXJt 1670
YnI= 1671
IGV4YW1wbGU= 1672
IGNvbmM= 1673
IGZvcmU= 1674
IFRv 1675
cHJv 1676
RU4= 1677
cmllcw== 1678
IDI1 1679
IENhbg== 1680
bmV5 1681
IGFjdHVhbGx5 1682
IGV2ZXI= 1683
dXJpdHk= 1684
YWtlbg== 1685
YXBz 1686
IHRheA== 1687
IG1ham9y 1688
YW1h 1689
IG9mdGVu 1690
ZXJhbA== 1691
IGh1bWFu 1692
IGpvYg== 1693
aXN0ZXI= 1694
IGF2YWlsYWJsZQ== 1695
b2Ny 1696
ZW5u 1697
YWlk 1698
aXZpZA== 1699
IHJlY29yZA== 1700
PyI= 1701
IHNpbmc= 1702
IEFt 1703
aWRlbmNl 1704
IG5ld3M= 1705
c3Rlcg== 1706
IGVjb25vbQ== 1707
IGZvbGxvd2luZw== 1708
IEJy 1709
aXNpbmc= 1710
IGhvdXI= 1711
bW9zdA== 1712
dW1lbnQ= 1713
IHNleA== 1714
IGRlc2M= 1715
IGJlY29tZQ== 1716
IEVk 1717
IHRvb2s= 1718
IGhhdmluZw== 1719
IHByb2R1Y3Q= 1720
YXVsdA== 1721
QXM= 1722
YXJpbmc= 1723
IG1lYW5z 1724
IGhvcA== 1725
dW5l 1726
IGNobw== 1727
IGNlcnRhaW4= 1728
IG5vbg== 1729
IGRlYWw= 1730
MjQ= 1731
bGVtZW50 1732
b2Np 1733
ZW5l 1734
IHNpZGU= 1735
IFBy 1736
IE1heQ== 1737
IHJlYXNvbg== 1738
dWVk 1739
Y2hlZA== 1740
dWxhdGlvbg== 1741
IGVsZWN0 1742
IG9mZmljaWFs 1743
IHBvc3NpYmxl 1744
IGhvbGQ= 1745
YW5kcw== 1746
b3Rz 1747
IGNpdHk= 1748
b3JpZXM= 1749
IHNldmVy 1750
IGNoaWxkcmVu 1751
IG9uY2U= 1752
IGFjdGl2 1753
bGVy 1754
IG5pZ2h0 1755
aXRpb25z 1756
IEpvaG4= 1757
YXBl 1758
cGxheQ== 1759
IGRvbmU= 1760
IGxpbQ== 1761
IHdvcmtpbmc= 1762
IFByZXM= 1763
b3JsZA== 1764
ZWI= 1765
IENv 1766
IGJvZHk= 1767
YWlscw== 1768
dXRlcw== 1769
IE1y 1770
IHdoZXRoZXI= 1771
IGF1dGhvcg== 1772
cm9w 1773
IHByb3Blcg== 1774
IHNlZW4= 1775
KTs= 1776
IGZhYw== 1777
IFN1 1778
IGNvbmQ= 1779
aXRpbmc= 1780
IGNvdXJzZQ== 1781
IH0= 1782
LS0tLS0tLS0tLS0tLS0tLQ== 1783
YWlnbg== 1784
IGV2ZW50 1785
IGVuZw== 1786
IHBvdA== 1787
IGludGVybg== 1788
aWFt 1789
IHNob3J0 1790
ZW1wdA== 1791
44I= 1792
IEdvZA== 1793
aWxhcg== 1794
ODA= 1795
IG9yaWc= 1796
SVM= 1797
b3Vybg== 1798
YWJpbGl0eQ== 1799
aXRpdmU= 1800
IGRhbQ== 1801
IDEwMA== 1802
IHByZXNz 1803
IGRvaW5n 1804
IHByb3RlY3Q= 1805
cmluZw== 1806
IHRob3VnaHQ= 1807
IHF1ZXN0aW9u 1808
cmV3 1809
IFdhcg== 1810
IHNldmVyYWw= 1811
IFN0YXRl 1812
IGdpdmVu 1813
IGZ1bmQ= 1814
IFR3 1815
IHdlbnQ= 1816
YW5jZXM= 1817
d29yaw== 1818
cG9y 1819
bXk= 1820
NDA= 1821
IGFyZw== 1822
YXJ0bWVudA== 1823
dXN0b20= 1824
IHBvbGlj 1825
IG1lZXQ= 1826
IGNyZWF0 1827
MjI= 1828
IFN0YXRlcw== 1829
IGdhbWVz 1830
cmF3 1831
dXR1cmU= 1832
IHVuZGVyc3RhbmQ= 1833
dXJz 1834
IE9i 1835
bGlzaA== 1836
c3k= 1837
IG1ha2Vz 1838
IHdvbg== 1839
YWdvbg== 1840
IGh0dA== 1841
IGxvdmU= 1842
ZW50aWFs 1843
IGNvbXBsZXRl 1844
cGFy 1845
IElt 1846
QUw= 1847
IGFjY291bnQ= 1848
wqA= 1849
b3JlZA== 1850
dmVydA== 1851
IGlkZW50 1852
IDIwMTU= 1853
IG90aGVycw== 1854
IE1pbg== 1855
aWJlcg== 1856
dmVyYWdl 1857
VGhlcmU= 1858
aXRpb25hbA== 1859
ZGQ= 1860
IHByb2I= 1861
IHlvdW5n 1862
IGFsb25n 1863
IGFjY29yZGluZw== 1864
IHlldA== 1865
IG1lbWJlcnM= 1866
IFdoYXQ= 1867
b2lk 1868
IE1hbg== 1869
QW5k 1870
IGFtb25n 1871
YWk= 1872
IGVtcGxveQ== 1873
IFJlcw== 1874
ID4= 1875
IGludm9s 1876
IGxvdw== 1877
YWY= 1878
IENhcg== 1879
IGhpZw== 1880
IE9uZQ== 1881
IFNlYw== 1882
aW5hdGlvbg== 1883
IGxpa2VseQ== 1884
IGFudA== 1885
YWdlZA== 1886
IFJ1c3M= 1887
IGJlbg== 1888
IHJlbGU= 1889
Rm9y 1890
YmFjaw== 1891
IE5vdA== 1892
IHByZXNpZGVudA== 1893
YmFsbA== 1894
IGFjY2Vzcw== 1895
aXZpZHVhbA== 1896
IERlbQ== 1897
IEV1cm8= 1898
NjA= 1899
IGtub3du 1900
aXJs 1901
IEdy 1902
IGVhcmx5 1903
dXNl 1904
aWV0eQ== 1905
4oCT 1906
IGZpZ2h0 1907
IHNlbnQ= 1908
IHRvZGF5 1909
IG1hcmtldA== 1910
Ii4= 1911
IGJhc2Vk 1912
IHN0cm9uZw== 1913
dXJ0aGVy 1914
IGRlYg== 1915
bWJlcg== 1916
IHByb2JsZW0= 1917
IGRlYXRo 1918
IHNvY2lhbA== 1919
aW1hdGU= 1920
QVM= 1921
b3J0dW4= 1922
IGNhbXBhaWdu 1923
ZXJ5 1924
Q2g= 1925
IGV5 1926
aWFsbHk= 1927
IG11cw== 1928
d2g= 1929
cG9z 1930
IGVy 1931
IHNhZg== 1932
IG1vbnRocw== 1933
aXJvbg== 1934
IHZpb2w= 1935
IGZpdmU= 1936
IHN0cmU= 1937
IHBsYXllcnM= 1938
aW5j 1939
YWxk 1940
eWVhcg== 1941
YXVu 1942
IHN1Y2Nlc3M= 1943
IHByZXNlbnQ= 1944
ZXJlbmNl 1945
IDIwMTQ= 1946
IHN1Z2c= 1947
IHBhcnRpY3VsYXI= 1948
IHRyeQ== 1949
IHN1Z2dlc3Q= 1950
IENocmlzdA== 1951
b25lcw== 1952
IHByaXY= 1953
MjM= 1954
IGNyaXQ= 1955
IGxhbmQ= 1956
IGxvY2Fs 1957
aWZ5 1958
Mjk= 1959
IGF1dA== 1960
RUQ= 1961
IEd1 1962
IG11bHQ= 1963
IHBvbGl0aWNhbA== 1964
IGFza2Vk 1965
IGZvcm1lcg== 1966
aXR0ZXI= 1967
cmlwdA== 1968
IGNsb3Nl 1969
IHByYWN0 1970
IFlvcms= 1971
IGdldHRpbmc= 1972
IGFjcm9zcw== 1973
IGNvbWI= 1974
IGJlbGlldmU= 1975
IHo= 1976
IHRvZ2V0 1977
IHRvZ2V0aGVy 1978
IENlbnQ= 1979
aXJj 1980
IGluZGl2aWR1YWw= 1981
IE1j 1982
Mjc= 1983
aXNr 1984
IEVuZw== 1985
IGZhY2U= 1986
IDI0 1987
IHZhbHVl 1988
IGFyZWE= 1989
ZXY= 1990
IHdyaXQ= 1991
IFByZXNpZGVudA== 1992
IHZvdA== 1993
IGtleQ== 1994
IG1vbQ== 1995
cHV0 1996
IGFueXRoaW5n 1997
IGV4cGVyaWVuY2U= 1998
YXR0bGU= 1999
IG1pbmQ= 2000
YWZm 2001
b21t 2002
IGZ1dHVyZQ== 2003
Z2Vk 2004
IGN1dA== 2005
IHRvdA== 2006
aXRjaA== 2007
IHZpZGVv 2008
IGludmVzdGln 2009
IG5ldA== 2010
IE15 2011
cmljdA== 2012
aWVu 2013
Lik= 2014
IGltcHJv 2015
dGhvdWdo 2016
d2FyZHM= 2017
IGNvbm5lY3Q= 2018
IE1lZA== 2019
c2VsdmVz 2020
ZW5zaXZl 2021
bWI= 2022
b2Jlcg== 2023
YXRvcnM= 2024
QW4= 2025
IDUw 2026
IHJlZHU= 2027
cmVzZW50 2028
IGFib3Zl 2029
IGZyZQ== 2030
IEV1cm9wZQ== 2031
c3c= 2032
IGFtb3VudA== 2033
IEFwcA== 2034
IGVpdGhlcg== 2035
IG1pbGl0 2036
IGFuYWw= 2037
IGZhaWw= 2038
IEVu 2039
YWxlcw== 2040
IHNwZWNpYWw= 2041
IGJsYWNr 2042
SVQ= 2043
Y2hlcg== 2044
IGxvb2tpbmc= 2045
IGZpcmU= 2046
eW4= 2047
IGFsbW9zdA== 2048
b29u 2049
IHN0dWR5 2050
IG1pc3M= 2051
Y2hlcw== 2052
cm93bg== 2053
IHRyZQ== 2054
IGNvbW11bml0eQ== 2055
IG1lZGlh 2056
IGZvb2Q= 2057
IGNvbWVz 2058
IFVuaXZlcnNpdHk= 2059
IHNpbmdsZQ== 2060
V2hhdA== 2061
dWx5 2062
IGhhbGY= 2063
YWd1ZQ== 2064
aG9k 2065
IFJlcHVibGlj 2066
IHN0YXJ0ZWQ= 2067
IHF1aWNr 2068
b3Rv 2069
Ym9vaw== 2070
IGlzc3Vl 2071
aXRvcg== 2072
IGVsc2U= 2073
IGNvbnNpZGVy 2074
MjY= 2075
cm9kdQ== 2076
IHRha2Vu 2077
Mjg= 2078
OTk= 2079
IFdpdGg= 2080
IHRydWU= 2081
IHdh 2082
IHRyYWQ= 2083
IGFnbw== 2084
IG1lc3M= 2085
aWVm 2086
IGFkZGVk 2087
b2tl 2088
IGJhZA== 2089
IGZhdg== 2090
MzM= 2091
IHNpbWlsYXI= 2092
YXNr 2093
IERvbg== 2094
IGNoYXJhY3Rlcg== 2095
b3J0cw== 2096
IEhvdXNl 2097
IHJlcG9ydGVk 2098
IHR5cGU= 2099
dmFs 2100
aW9k 2101
IEhvd2V2ZXI= 2102
IHRhcmc= 2103
IGVudGlyZQ== 2104
cHBpbmc= 2105
IGhpc3Rvcnk= 2106
IGxpdmU= 2107
ZmZpYw== 2108
Li4uLi4uLi4= 2109
ZWRlcmFs 2110
IHRyeWluZw== 2111
IGRpc2N1c3M= 2112
IEhhcg== 2113
YWNlcw== 2114
bGlzaGVk 2115
IHNlbGY= 2116
b3Nw 2117
cmVzdA== 2118
IHJvb20= 2119
ZWx0 2120
IGZhbGw= 2121
b2x1dGlvbg== 2122
IGV0 2123
IHg= 2124
IGlzbg== 2125
IGlkZWE= 2126
Ym8= 2127
IHNvdW5k 2128
IERlcA== 2129
IHNvbWVvbmU= 2130
Y2lhbGx5 2131
dWxseQ== 2132
IGZvYw== 2133
IG9iamVjdA== 2134
aWZ0 2135
YXBlcg== 2136
IHBsYXllcg== 2137
IHJhdGhlcg== 2138
IHNlcnZpY2U= 2139
YXNoaW5n 2140
IERv 2141
IFBhcnQ= 2142
cnVn 2143
bW9u 2144
cGx5 2145
IG1vcg== 2146
IG5vdGhpbmc= 2147
IHByb3ZpZGU= 2148
SUM= 2149
dW5n 2150
IHBhcnR5 2151
IGV4aXN0 2152
IG1hZw== 2153
NzA= 2154
IHJ1bA== 2155
IGhvdXNl 2156
IGJlaGluZA== 2157
IGhvd2V2ZXI= 2158
IFdvcmxk 2159
IHN1bQ== 2160
IGFwcGxpYw== 2161
IDs= 2162
IGZ1bmN0aW9u 2163
Z3I= 2164
IFBvbA== 2165
IGZyb250 2166
MjAw 2167
IHNlcmllcw== 2168
IHRlbQ== 2169
IHR5cA== 2170
aWxscw== 2171
IG9wdA== 2172
IHBvaW50cw== 2173
IGJlbG93 2174
aXR0ZWQ= 2175
IHNwZWNpZmlj 2176
IDIwMTc= 2177
dW1i 2178
IHJh 2179
IHByZXZpb3Vz 2180
IHByZXQ= 2181
cmVtZQ== 2182
IGN1c3RvbQ== 2183
IGNvdXJ0 2184
IE1l 2185
IHJlcGw= 2186
IHdob2xl 2187
Z28= 2188
Y2Vy 2189
IHRyZWF0 2190
IEFjdA== 2191
IHByb2JhYmx5 2192
IGxlYXJu 2193
ZW5kZXI= 2194
IEFzcw== 2195
IHZlcnNpb24= 2196
bm93 2197
IGNoZWNr 2198
IENhbA== 2199
UkU= 2200
bWluaXN0 2201
T24= 2202
b3VyY2Vz 2203
IGJlbmVm 2204
IGRvYw== 2205
IGRldGVy 2206
IGVuYw== 2207
IHN1cGVy 2208
IGFkZHJlc3M= 2209
IHZpY3Q= 2210
IDIwMTM= 2211
IG1lYXM= 2212
dHI= 2213
IGZpZWxk 2214
V2hlbg== 2215
IHNpZ25pZmlj 2216
dWdl 2217
IGZlYXQ= 2218
IGNvbW1vbg== 2219
bG9hZA== 2220
IGJlZ2lu 2221
IGJyaW5n 2222
IGFjdGlvbg== 2223
ZXJtYW4= 2224
IGRlc2NyaWI= 2225
IGluZHVzdA== 2226
IHdhbnRlZA== 2227
cmllZA== 2228
bWluZw== 2229
IGF0dGVtcHQ= 2230
NDU= 2231
ZmVy 2232
IGR1ZQ== 2233
cmVzc2lvbg== 2234
IyM= 2235
IHNoYWxs 2236
IHNpeA== 2237
b28= 2238
IHN0ZXA= 2239
IHB1Yg== 2240
IGhpbXNlbGY= 2241
IDIz 2242
IGNvcA== 2243
IGRlc3Q= 2244
IHN0b3A= 2245
QUM= 2246
aWJpbGl0eQ== 2247
IGxhYg== 2248
aWN1bHQ= 2249
IGhvdXJz 2250
IGNyZWF0ZQ== 2251
IGZ1cnRoZXI= 2252
IEFtZXJpY2E= 2253
IENpdHk= 2254
IGRvdQ== 2255
aGVhZA== 2256
U1Q= 2257
IE5vcnRo 2258
Y2luZw== 2259
IG5hdGlvbmFs 2260
dWxl 2261
IEluc3Q= 2262
IHRha2luZw== 2263
IFF1 2264
aXJ0 2265
IHJlZA== 2266
IHJlc2VhcmNo 2267
dmlyb24= 2268
IEdl 2269
IGJyZWFr 2270
YW5h 2271
IHNwYWNl 2272
YXRlcmlhbA== 2273
IHJlY2VudA== 2274
IEFi 2275
IGdlbmVyYWw= 2276
IGhpdA== 2277
IHBlcmlvZA== 2278
IGV2ZXJ5dGhpbmc= 2279
aXZlbHk= 2280
IHBoeXM= 2281
IHNheWluZw== 2282
YW5rcw== 2283
IGNvdQ== 2284
IGN1bHQ= 2285
YWNlZA== 2286
ZWFs 2287
dWF0aW9u 2288
IGNvdW4= 2289
bHU= 2290
IGluY2x1ZGU= 2291
IHBvc2l0aW9u 2292
IEFmdGVy 2293
IENhbmFk 2294
IEVt 2295
IGltbQ== 2296
IFJlZA== 2297
IHBpY2s= 2298
IGNvbXBs 2299
IG1hdHRlcg== 2300
cmVn 2301
ZXh0 2302
YW5ndQ== 2303
aXNj 2304
b2xl 2305
YXV0 2306
IGNvbXBldA== 2307
ZWVk 2308
ZmVjdA== 2309
IDIx 2310
IFNlbg== 2311
IFRoZXNl 2312
YXNpbmc= 2313
IGNhbm5vdA== 2314
IGluaXQ= 2315
IHJlbGF0aW9ucw== 2316
YWNoZWQ= 2317
IGJhcg== 2318
IDQw 2319
IFRI 2320
IDIwMTI= 2321
IHZvbA== 2322
IGdyb3VuZA== 2323
IHNlY3VyaXR5 2324
IHVwZA== 2325
aWx0 2326
MzU= 2327
IGNvbmNlcm4= 2328
IEp1c3Q= 2329
IHdoaXRl 2330
IHNlZW1z 2331
IEhlcg== 2332
cGVjaWFsbHk= 2333
aWVudHM= 2334
IGFubm91bg== 2335
IGZpZw== 2336
aWdodHM= 2337
IHN0cmk= 2338
bGlrZQ== 2339
aWRz 2340
IHN1cw== 2341
IHdhdGNo 2342
IOI= 2343
IHdpbmQ= 2344
IENvbnQ= 2345
IGl0c2VsZg== 2346
IG1hc3M= 2347
QWw= 2348
eWxl 2349
aXF1ZQ== 2350
IE5hdGlvbmFs 2351
IGFicw== 2352
IHBhY2s= 2353
IG91dHNpZGU= 2354
IGFuaW0= 2355
IHBhaW4= 2356
ZXRlcg== 2357
IG1hbmFn 2358
ZHVjdA== 2359
b2du 2360
IF0= 2361
IFNlcHQ= 2362
c2Vj 2363
b2Zm 2364
IEphbg== 2365
IGZvb3Q= 2366
YWRlcw== 2367
IHRoaXJk 2368
IG1vdA== 2369
IGV2aWRlbmNl 2370
aW50b24= 2371
IHRocmVhdA== 2372
YXB0 2373
cGxlcw== 2374
Y2xl 2375
IGxv 2376
IGRlY2w= 2377
IGl0ZW0= 2378
bWVkaQ== 2379
IHJlcHJlc2VudA== 2380
b21i 2381
YW1lcg== 2382
IHNpZ25pZmljYW50 2383
b2dyYXBo 2384
c3U= 2385
IGNhbA== 2386
aXJlcw== 2387
MDAwMA== 2388
SUQ= 2389
QU0= 2390
IHNpbXBseQ== 2391
IGxvbmdlcg== 2392
IGZpbGU= 2393
T1Q= 2394
Y2hl 2395
U28= 2396
YXRlZw== 2397
b3Jn 2398
IEhpcw== 2399
IGVuZXI= 2400
IGRvbQ== 2401
IHVwb24= 2402
aWxp 2403
Ijoi 2404
IHRoZW1zZWx2ZXM= 2405
IGNvbWluZw== 2406
IHF1aXRl 2407
IGRpZmZpY3VsdA== 2408
IEJhcg== 2409
aWxpdGllcw== 2410
cmVs 2411
ZW5kcw== 2412
Y2lhbA== 2413
NjQ= 2414
IHdvbWFu 2415
cmFw 2416
eXI= 2417
IG5lY2Vzcw== 2418
aXBz 2419
IHRleHQ= 2420
IHJlcXVpcmU= 2421
IG1pbGl0YXJ5 2422
IHJldmlldw== 2423
IHJlc3BvbnM= 2424
NzU= 2425
IHN1YmplY3Q= 2426
IGluc3RlYWQ= 2427
IGlzc3Vlcw== 2428
IGdlbg== 2429
Iiwi 2430
IG1pbnV0ZXM= 2431
IHdlYXA= 2432
cmF5 2433
YW1lZA== 2434
dGltZQ== 2435
Ymw= 2436
SG93 2437
IGNvZGU= 2438
IFNt 2439
IGhpZ2hlcg== 2440
IFN0ZQ== 2441
cmlz 2442
IHBhZ2U= 2443
IHN0dWRlbnRz 2444
IEludGVybg== 2445
IG1ldGhvZA== 2446
IEF1Zw== 2447
IFBlcg== 2448
IEFn 2449
IHBvbGljeQ== 2450
IFN3 2451
IGV4ZWM= 2452
IGFjY2VwdA== 2453
dW1l 2454
cmlidXQ= 2455
IHdvcmRz 2456
IGZpbmFs 2457
IGNoYW5nZXM= 2458
IERlbW9jcg== 2459
IGZyaWVuZHM= 2460
IHJlc3BlY3Q= 2461
IGVw 2462
IGNvbXBhbg== 2463
aXZpbA== 2464
IGRhbWFnZQ== 2465
KioqKg== 2466
b2dsZQ== 2467
dmlyb25tZW50 2468
IG5lZw== 2469
ZW50YWw= 2470
IGFw 2471
IHRvdGFs 2472
aXZhbA== 2473
ISI= 2474
bGlt 2475
IG5lZWRz 2476
IGFncmU= 2477
IGRldmVsb3BtZW50 2478
IGFnZQ== 2479
aXBsZQ== 2480
MjE= 2481
IHJlc3VsdHM= 2482
IEFm 2483
U2g= 2484
IGd1bg== 2485
IE9iYW1h 2486
cm9sbA== 2487
IEA= 2488
IHJpZ2h0cw== 2489
IEJyaXQ= 2490
IHJ1bm5pbmc= 2491
IHdhc24= 2492
IHBvcnQ= 2493
IHJhdGU= 2494
IHByZXR0eQ== 2495
IHRhcmdldA== 2496
IHNhdw== 2497
IGNpcmM= 2498
IHdvcmtz 2499
aWNybw== 2500
YWx0 2501
b3Zlcg== 2502
d3d3 2503
VGhhdA== 2504
bGllcg== 2505
IGV2ZXJ5b25l 2506
dWRl 2507
IHBpZQ== 2508
aWRkbGU= 2509
cmFlbA== 2510
IHJhZA== 2511
IGJsb2Nr 2512
IHdhbGs= 2513
VG8= 2514
44E= 2515
bmVz 2516
IEF1c3Q= 2517
YXVs 2518
cm90ZQ== 2519
IFNvdXRo 2520
ZXNzaW9u 2521
b3Bo 2522
IHNob3dz 2523
IHNpdGU= 2524
IGpv 2525
IHJpc2s= 2526
Y2x1cw== 2527
bHQ= 2528
IGluag== 2529
aWRpbmc= 2530
IFNwZQ== 2531
IGNoYWxs 2532
aXJt 2533
IDIy 2534
aXR0aW5n 2535
c3Ry 2536
IGh5 2537
TEU= 2538
a2V5 2539
IGJlZ2Fu 2540
YXR1cg== 2541
YXNoaW5ndG9u 2542
bGFt 2543
IERhdg== 2544
Yml0 2545
IHNpemU= 2546
IFBhcg== 2547
Mzg= 2548
b3VybmFs 2549
ZmFjZQ== 2550
IGRlY2lzaW9u 2551
IGxhcmc= 2552
IGp1ZA== 2553
cmVjdA== 2554
IGNvbnRpbnVl 2555
IE9jdA== 2556
b3ZlcmVk 2557
IEludA== 2558
PT09PT09PT0= 2559
IHBhcmVudA== 2560
IFdpbGw= 2561
IGVhc3k= 2562
IGRydWc= 2563
YW5nZXI= 2564
IHNlbnNl 2565
IGRp 2566
aWRheQ== 2567
IGVuZXJneQ== 2568
aXN0aWM= 2569
IGFzc29jaQ== 2570
YXJ0ZXI= 2571
b2JhbA== 2572
ZWtz 2573
IEVs 2574
dXJjaA== 2575
IGdpcmw= 2576
b2U= 2577
aXRsZQ== 2578
IDI4 2579
IENoZQ== 2580
IHJlcXVlc3Q= 2581
IHNvb24= 2582
IGhvc3Q= 2583
a3k= 2584
IHN0YXRlcw== 2585
b21lcw== 2586
IG1hdGVyaWFs 2587
bGV4 2588
IG1vbWVudA== 2589
IGFuc3c= 2590
b25zZQ== 2591
IGVzcGVjaWFsbHk= 2592
IG5vcm0= 2593
IHNlcnZpY2Vz 2594
cGl0ZQ== 2595
cmFu 2596
IHJvbGU= 2597
NDQ= 2598
KTo= 2599
IGNyZWQ= 2600
Q2w= 2601
X19fX19fX18= 2602
IG1hdA== 2603
IGxvZw== 2604
IENsaW50b24= 2605
T1U= 2606
IG9mZmljZQ== 2607
IDI2 2608
IGNoYXJn 2609
IHRyYWNr 2610
bWE= 2611
IGhlYXJ0 2612
IGJhbGw= 2613
IHBlcnNvbmFs 2614
IGJ1aWxkaW5n 2615
bmE= 2616
c2V0 2617
Ym9keQ== 2618
IEJsYWNr 2619
IGluY3JlYXNl 2620
aXR0ZW4= 2621
IG5lZWRlZA== 2622
MzY= 2623
MzI= 2624
PSI= 2625
IGxvc3Q= 2626
IGJlY2FtZQ== 2627
IGdyb3Vwcw== 2628
IE11cw== 2629
IHdyb3Rl 2630
IFBl 2631
IHByb3A= 2632
am95 2633
w6k= 2634
IFdoaXRl 2635
IGRlYWQ= 2636
Lic= 2637
IGh0dHA= 2638
IHdlYnM= 2639
T1M= 2640
IGluc2lkZQ== 2641
IHdyb25n 2642
IHN0YXRlbWVudA== 2643
IC4uLg== 2644
eWw= 2645
IGZpbG0= 2646
IG11c2lj 2647
IHNoYXJl 2648
aWZpY2F0aW9u 2649
IHJlbGVhc2U= 2650
IGZvcndhcmQ= 2651
IHN0YXk= 2652
IGNvbXB1dA== 2653
aXR0ZQ== 2654
c2Vy 2655
IG9yaWdpbmFs 2656
IGNhcmQ= 2657
IGNhbmQ= 2658
IGRpdg== 2659
YXR1cmFs 2660
IGZhdm9y 2661
T00= 2662
IGNhc2Vz 2663
dXNlcw== 2664
IHNlY3Rpb24= 2665
IGxlYXZl 2666
Z2luZw== 2667
b3ZlZA== 2668
IFdhc2hpbmd0b24= 2669
Mzk= 2670
IEds 2671
IHJlcXVpcmVk 2672
YWN0aW9u 2673
YXBhbg== 2674
b29y 2675
aXRlcg== 2676
IEtpbmc= 2677
IGNvdW50cmllcw== 2678
IEdlcm1hbg== 2679
bGxpbmc= 2680
IDI3 2681
MzQ= 2682
IHF1ZXN0aW9ucw== 2683
IHByaW0= 2684
IGNlbGw= 2685
IHNob290 2686
IGFueW9uZQ== 2687
IFdlc3Q= 2688
IGFmZmVjdA== 2689
ZXBlbmQ= 2690
IG9ubGluZQ== 2691
IElzcmFlbA== 2692
IFNlcHRlbWJlcg== 2693
IGFiaWxpdHk= 2694
IGNvbnRlbnQ= 2695
aXNlcw== 2696
IHJldmU= 2697
IGxhdW4= 2698
IGluZGlj 2699
IGZvcmNl 2700
Y2FzdA== 2701
IHNvbGQ= 2702
YXZpbmc= 2703
Zmw= 2704
IHNvZnQ= 2705
IGNvbXBhbmllcw== 2706
Y2VlZA== 2707
IGFydGljbGU= 2708
IGF1ZA== 2709
IHJldg== 2710
IGVkdWM= 2711
IHBsYXlpbmc= 2712
MDU= 2713
IGhlbGQ= 2714
Y3Rvcg== 2715
IHJlbGVhc2Vk 2716
IGZlZGVyYWw= 2717
Mzc= 2718
IGFkbWluaXN0 2719
IGludGVydmlldw== 2720
IGluc3RhbGw= 2721
IHJlY2VpdmVk 2722
IHNvdXJjZQ== 2723
dWs= 2724
UGg= 2725
IHNlcmlvdXM= 2726
IGNyZWF0ZWQ= 2727
IGNhdXNl 2728
IGltbWVkaQ== 2729
IGRlZmlu 2730
dWVs 2731
IERlcGFydG1lbnQ= 2732
Y3Rpb25z 2733
IENvdXI= 2734
IE5vdw== 2735
emU= 2736
aXRlcw== 2737
aXR1dGlvbg== 2738
IGxhdGU= 2739
IHNwZWFr 2740
bmVycw== 2741
IGxlZ2Fs 2742
YXJp 2743
IENvcg== 2744
IHdlZWtz 2745
IG1vZGVs 2746
IHByZWQ= 2747
IGV4YWN0 2748
QkM= 2749
IEJ5 2750
SU5H 2751
b3Npbmc= 2752
IHRha2Vz 2753
IHJlZ2FyZA== 2754
IG9wcG9ydHVu 2755
IHByaWNl 2756
IDE5OA== 2757
IEFwcg== 2758
ZnVsbHk= 2759
IG9yZA== 2760
IHByb2JsZW1z 2761
cnVjdGlvbg== 2762
aGFt 2763
IENvdW50 2764
bGVnZQ== 2765
IGxlYWRlcnM= 2766
RVQ= 2767
bGV2 2768
IGRlZXA= 2769
b2xvZ2ljYWw= 2770
ZXNl 2771
aGFwcw== 2772
IFNvbWU= 2773
IHBlcnM= 2774
IGNvbnRyYWN0 2775
IHJlbGF0aW9uc2hpcA== 2776
c3A= 2777
b3Vk 2778
IGJhc2U= 2779
NDg= 2780
bWl0 2781
QWQ= 2782
YW5jaWFs 2783
IGNvbnN1bQ== 2784
IHBvdGVudGlhbA== 2785
IGxhbmd1 2786
cmVt 2787
ZXRo 2788
IHJlbGln 2789
cmVzc2Vk 2790
NjY= 2791
IGxpbms= 2792
IGxvd2Vy 2793
YXllcg== 2794
IEp1bmU= 2795
IGZlbQ== 2796
dW50 2797
ZXJj 2798
dXJk 2799
IGNvbnRhY3Q= 2800
IGlsbA== 2801
IG1vdGhlcg== 2802
IGVzdGFi 2803
aHR0 2804
IE1hcmNo 2805
IEJybw== 2806
IENoaW5h 2807
IDI5 2808
IHNxdQ== 2809
IHByb3ZpZGVk 2810
IGF2ZXJhZ2U= 2811
YXNvbnM= 2812
IDIwMTE= 2813
IGV4YW0= 2814
bGlu 2815
NTU= 2816
bmVk 2817
IHBlcmZlY3Q= 2818
IHRvdQ== 2819
YWxzZQ== 2820
dXg= 2821
IGJ1eQ== 2822
IHNob3Q= 2823
IGNvbGxlY3Q= 2824
IHBob3Q= 2825
IHBsYXllZA== 2826
IHN1cnBy 2827
IG9mZmljaWFscw== 2828
IHNpbXBsZQ== 2829
YXZ5 2830
IGluZHVzdHJ5 2831
IGhhbmRz 2832
Z3JvdW5k 2833
IHB1bGw= 2834
IHJvdW5k 2835
IHVzZXI= 2836
IHJhbmdl 2837
dWFyeQ== 2838
IHByaXZhdGU= 2839
b3Bz 2840
ZWVz 2841
IHdheXM= 2842
IE1pY2g= 2843
IHZlaA== 2844
IGV4Y2VwdA== 2845
IHRlcm1z 2846
aW11bQ== 2847
cHBlcg== 2848
SU9O 2849
b3Jlcw== 2850
IERyYWdvbg== 2851
b3Vs 2852
IGRlbg== 2853
IHBlcmZvcm1hbmNl 2854
IGJpbGw= 2855
Y2ls 2856
NDc= 2857
IGVudmlyb25tZW50 2858
IGV4Yw== 2859
YWRk 2860
IHdvcnRo 2861
IHBpY3Q= 2862
IGNoYW5jZQ== 2863
IDIwMTg= 2864
Ym9y 2865
IHNwZWVk 2866
aWN0aW9u 2867
IGFsbGVn 2868
IEphcGFu 2869
YXRvcnk= 2870
cmVldA== 2871
IG1hdGNo 2872
IElJ 2873
IHN0cnU= 2874
b3JkZXI= 2875
IHN0ZQ== 2876
IGxpdmluZw== 2877
IHN0cnVjdA== 2878
aW5v 2879
IHNlcGFy 2880
aGVybg== 2881
IHJlc3BvbnNl 2882
IGVuam95 2883
IHZpYQ== 2884
QUQ= 2885
dW1lbnRz 2886
YWNlYm9vaw== 2887
IG1lbWJlcg== 2888
aWJy 2889
aXppbmc= 2890
IHRvb2w= 2891
IE1vbg== 2892
IFdoaWxl 2893
aG9vZA== 2894
IEFuZw== 2895
IERlZg== 2896
IG9mZmVy 2897
VHI= 2898
YXVy 2899
IHR1cm5lZA== 2900
IEp1bHk= 2901
ZG93bg== 2902
YW5jZWQ= 2903
IHJlY2VudGx5 2904
IEVhcg== 2905
IGNl 2906
IFN0YXI= 2907
IENvbmc= 2908
cm91Z2h0 2909
IGJsb29k 2910
IGhvcGU= 2911
IGNvbW1lbnQ= 2912
YWludA== 2913
IGFycmk= 2914
aWxlcw== 2915
IHBhcnRpY2lw 2916
b3VnaHQ= 2917
cmlwdGlvbg== 2918
MDg= 2919
NDk= 2920
IGdhdmU= 2921
IHNlbGVjdA== 2922
IGtpbGxlZA== 2923
c3ljaA== 2924
IGdvZXM= 2925
aWo= 2926
IGNvbGw= 2927
IGltcGFjdA== 2928
YXRpdmVz 2929
IFNlcg== 2930
MDk= 2931
IEF1Z3VzdA== 2932
IGJveQ== 2933
ZGU= 2934
IERlcw== 2935
IGZlbHQ= 2936
VVM= 2937
IGV4cGVjdGVk 2938
IGltYWdl 2939
IE1hcms= 2940
Y2NvcmRpbmc= 2941
b2ljZQ== 2942
RUM= 2943
IE1hZw== 2944
ZW5lZA== 2945
aG9sZA== 2946
IFBvc3Q= 2947
IHByZXZlbnQ= 2948
Tm8= 2949
IGludm9sdmVk 2950
IGV5ZXM= 2951
IHF1aWNrbHk= 2952
QXQ= 2953
dW5r 2954
IGJlaGF2 2955
IHVy 2956
IGxlZA== 2957
Y29tZQ== 2958
ZXk= 2959
IGNhbmRpZA== 2960
IGVhcmxpZXI= 2961
IGZvY3Vz 2962
ZXR5 2963
UHJv 2964
bGVkZ2U= 2965
aXhlZA== 2966
aWxsZWQ= 2967
IHBvcHVsYXI= 2968
QVA= 2969
IHNldHQ= 2970
bGlnaHQ= 2971
IHZhcmlvdXM= 2972
aW5rcw== 2973
IGxldmVscw== 2974
IHJvYWQ= 2975
ZWxsaWc= 2976
YWJsZXM= 2977
aGVs 2978
aXR0ZWU= 2979
IEdlbmVy 2980
eXBl 2981
IGhlYXJk 2982
aWNsZXM= 2983
IG1pcw== 2984
IHVzZXJz 2985
IFNhbg== 2986
IGltcHJvdmU= 2987
IGZhdGhlcg== 2988
IHNlYXJjaA== 2989
VGhleQ== 2990
dmls 2991
IHByb2Zlc3M= 2992
IGtuZXc= 2993
IGxvc3M= 2994
IGV2ZW50cw== 2995
NjU= 2996
IGJpbGxpb24= 2997
MDc= 2998
MDI= 2999
IE5ld3M= 3000
IEFN 3001
IGNvdmVy 3002
d2hlcmU= 3003
ZW5zaW9u 3004
IGJvdHQ= 3005
IGFyZWFz 3006
ZW5jZXM= 3007
b3Bl 3008
IFR3aXR0ZXI= 3009
YWVs 3010
IGdldHM= 3011
IEdvb2dsZQ== 3012
IHNu 3013
aWFudA== 3014
IHZvdGU= 3015
IG5lYXJseQ== 3016
IGluY2x1ZGVk 3017
IHJlY29nbg== 3018
eno= 3019
bW0= 3020
YWxlZA== 3021
IGhhcHBlbmVk 3022
MDQ= 3023
IGhvdA== 3024
IHdob3Nl 3025
IGNpdmls 3026
IHN1ZmY= 3027
b2Vz 3028
aXRpeg== 3029
IFN5cmk= 3030
IHJlc3BvbmQ= 3031
IGhvbg== 3032
IGZlYXR1cmVz 3033
IGVjb25vbWlj 3034
IEFwcmls 3035
cmlt 3036
IHRlY2hub2xvZ3k= 3037
IG9wdGlvbg== 3038
YWdpbmc= 3039
IHB1cmNo 3040
UmU= 3041
IGxhdA== 3042
Y2hpZQ== 3043
aXNs 3044
IHJlY29tbQ== 3045
dWY= 3046
IHRyYWluaW5n 3047
IGVmZmVjdHM= 3048
IGZhc3Q= 3049
IDIwMTA= 3050
IG9jY3Vy 3051
IHdlYnNpdGU= 3052
IGVtYWls 3053
IHNlbnM= 3054
ZWNo 3055
IG9pbA== 3056
IGluZmx1 3057
IGN1cnJlbnRseQ== 3058
IFNjaA== 3059
IEFkZA== 3060
IGdvYWw= 3061
IHNjaWVudA== 3062
IGNvbnY= 3063
MTAw 3064
ZW15 3065
IGRlY2lkZWQ= 3066
IHRyYXZlbA== 3067
IG1lbnRpb24= 3068
TEw= 3069
MDM= 3070
IGVsZWN0aW9u 3071
IHBob25l 3072
IGxvb2tz 3073
IHNpdHVhdGlvbg== 3074
IGN5 3075
IGhvcg== 3076
YmVk 3077
IENvdXJ0 3078
YWlseQ== 3079
YXZlcw== 3080
IHF1YWxpdHk= 3081
IENvbXA= 3082
d2lzZQ== 3083
IHRhYmxl 3084
IHN0YWZm 3085
IFdpbmQ= 3086
ZXR0 3087
IHRyaWVk 3088
aWRlcmVk 3089
IGFkZGl0aW9u 3090
IGJveA== 3091
IGxhY2s= 3092
YXJpbHk= 3093
IHdpZGU= 3094
IG1pZA== 3095
IGJvYXJk 3096
eXNpcw== 3097
IGFudGk= 3098
aGE= 3099
IGRpZw== 3100
ZW5pbmc= 3101
IGRybw== 3102
Q29u 3103
Njg= 3104
IHNsb3c= 3105
YmFzZWQ= 3106
c2VxdQ== 3107
IHBhdGg= 3108
RXg= 3109
YWtlcg== 3110
IHdvcmtlZA== 3111
IHBlbg== 3112
IGVuZ2luZQ== 3113
IGxvb2tlZA== 3114
IFN1cGVy 3115
IFNlcnY= 3116
IHZpY3RpbQ== 3117
VW4= 3118
IHByb3BlcnR5 3119
IGludHJvZHU= 3120
IGV4ZWN1dA== 3121
IFBN 3122
TGU= 3123
IGNvbG9y 3124
IE1vcmU= 3125
IDYw 3126
IG5ldHdvcms= 3127
IGRhdGU= 3128
Y3Vs 3129
aWRnZQ== 3130
IGV4dHJh 3131
MzE= 3132
IHNsZQ== 3133
Njc= 3134
IHdvbmQ= 3135
IHJlcG9ydHM= 3136
anVzdA== 3137
IEF1c3RyYWw= 3138
IGNhcGl0YWw= 3139
IGVucw== 3140
IGNvbW1hbmQ= 3141
IGFsbG93ZWQ= 3142
IHByZXA= 3143
IGNhcHQ= 3144
aGli 3145
IG51bWJlcnM= 3146
Y2hhbg== 3147
IGZhaXI= 3148
bXA= 3149
b21z 3150
IHJlYWNo 3151
V2l0aA== 3152
dGFpbg== 3153
IGJyb2Fk 3154
IGNvdXBsZQ== 3155
ZWNhdXNl 3156
bHlpbmc= 3157
IEZlYg== 3158
IHNjcmVlbg== 3159
IGxpdmVz 3160
IHByaW9y 3161
IENvbmdyZXNz 3162
QXI= 3163
IGFwcHJvYWNo 3164
IGVtZXI= 3165
YXJpZXM= 3166
IERpcw== 3167
c2Vydg== 3168
IE5l 3169
IGJ1aWx0 3170
Y2llcw== 3171
IHJlcGU= 3172
IHJ1bGVz 3173
Zm9yY2U= 3174
IFBhbA== 3175
IGZpbmFuY2lhbA== 3176
IGNvbnNpZGVyZWQ= 3177
IENoYXI= 3178
bmNlcw== 3179
IElT 3180
IGJyb3VnaHQ= 3181
IGJp 3182
aWVycw== 3183
IFNpbQ== 3184
T1A= 3185
IHByb2R1Y3Rz 3186
IHZpc2l0 3187
IGRvY3VtZW50 3188
IGNvbmR1Y3Q= 3189
IGNvbXBsZXRlbHk= 3190
aW5pbmc= 3191
IENhbGlm 3192
aWJseQ== 3193
IHdyaXR0ZW4= 3194
IFRW 3195
ZW1lbnRz 3196
IGRyYXc= 3197
T25l 3198
IHB1Ymxpc2hlZA== 3199
IHNlY3JldA== 3200
cmFpbg== 3201
aGV0 3202
IEZhY2Vib29r 3203
b25kYXk= 3204
IFVw 3205
IHNleHVhbA== 3206
IHRob3Vz 3207
IFBhdA== 3208
IGVzcw== 3209
IHN0YW5kYXJk 3210
IGFybQ== 3211
Z2Vz 3212
ZWN0aW9u 3213
IGZlbGw= 3214
IGZvcmVpZ24= 3215
YW5p 3216
IEZyaWRheQ== 3217
IHJlZ3VsYXI= 3218
aW5hcnk= 3219
IGluY3JlYXNlZA== 3220
IHVzdWFsbHk= 3221
IGRlbW9u 3222
IGRhcms= 3223
IGFkZGl0aW9uYWw= 3224
cm9s 3225
IE9m 3226
IHByb2R1Y3Rpb24= 3227
ISE= 3228
dW5kcmVk 3229
IGludGVybmF0aW9uYWw= 3230
aWRlbnRz 3231
IEZyZWU= 3232
cm91cA== 3233
IHJhY2U= 3234
IG1hY2g= 3235
IGh1Z2U= 3236
QWxs 3237
bGVhcg== 3238
b3ZlbWJlcg== 3239
IHRvd24= 3240
IGF0dGVudGlvbg== 3241
IE9mZg== 3242
eW9uZA== 3243
IFRoZW4= 3244
ZmllbGQ= 3245
IHRlcnJvcg== 3246
cmF6 3247
IEJv 3248
IG1lZXRpbmc= 3249
IFBhcms= 3250
IGFycmVzdA== 3251
IGZlYXI= 3252
IGF3 3253
IFZhbA== 3254
b3Jpbmc= 3255
Jyw= 3256
IGV4dHJlbWU= 3257
YXJy 3258
IHdvcmtlcnM= 3259
QWZ0ZXI= 3260
IDMx 3261
bmV0 3262
YW1lbnQ= 3263
IGRpcmVjdGx5 3264
IHBvcHVsYXRpb24= 3265
dWJl 3266
IE9jdG9iZXI= 3267
IElO 3268
IEphbnVhcnk= 3269
NTk= 3270
IERhdmlk 3271
IGNyb3Nz 3272
Y2VtYmVy 3273
IEZpcnN0 3274
IG1lc3NhZ2U= 3275
aXJpdA== 3276
IG5hdGlvbg== 3277
IHBvbGw= 3278
aXNpb25z 3279
IGFuc3dlcg== 3280
bnk= 3281
aXNvZGU= 3282
IGNhcnJ5 3283
IFJ1c3NpYQ== 3284
IGhlYXI= 3285
ZW5ndGg= 3286
cm95 3287
IG5hdHVyYWw= 3288
aW5hbGx5 3289
IGRvZw== 3290
bWl0dGVk 3291
IHRyYWRl 3292
IHN1YnN0 3293
IG11bHRpcGxl 3294
IEFmcmlj 3295
IGZhbnM= 3296
IHNvcnQ= 3297
IGdsb2JhbA== 3298
aWNhdGlvbg== 3299
IFdlZA== 3300
YXJh 3301
IGFjaGll 3302
IGxhbmd1YWdl 3303
dmV5 3304
IHRhbA== 3305
IG5lY2Vzc2FyeQ== 3306
IGRldGFpbHM= 3307
IHNlbg== 3308
IFN1bmQ= 3309
IFJlZw== 3310
IFJlYw== 3311
MDY= 3312
IHNpbA== 3313
cmVzc2l2ZQ== 3314
IG1lZGljYWw= 3315
dW5jaA== 3316
b3JuaWE= 3317
IHVuZA== 3318
Zm9ydA== 3319
b2Nrcw== 3320
IE1vbmRheQ== 3321
dWVzZGF5 3322
Y3JhZnQ= 3323
Nzc= 3324
dXJ0 3325
IHZlcg== 3326
IEhpbGw= 3327
IHJlY2VpdmU= 3328
IG1vcm5pbmc= 3329
ZXN0ZXJu 3330
IGJhbms= 3331
IHNhdA== 3332
aXJ0aA== 3333
IEhpZ2g= 3334
IGRldmljZQ== 3335
IFRIRQ== 3336
IENlbnRlcg== 3337
IHNhZmU= 3338
IHBsZQ== 3339
IENhbmFkYQ== 3340
IHN5c3RlbXM= 3341
IGFzc2lzdA== 3342
IHN1cnY= 3343
IGJhdHRsZQ== 3344
IFNvYw== 3345
dmVydGlz 3346
U2hl 3347
IHBhcGVy 3348
IGdyb3d0aA== 3349
IGNhc3Q= 3350
U2M= 3351
IHBsYW5z 3352
bGxlZA== 3353
IHBhcnRz 3354
IHdhbGw= 3355
IG1vdmVtZW50 3356
IHByYWN0aWNl 3357
aW1hdGVseQ== 3358
IGRpc3BsYXk= 3359
IHNvbWV0aW1lcw== 3360
b21w 3361
IFBhdWw= 3362
IFllcw== 3363
a2luZw== 3364
NTg= 3365
b2x5 3366
IHNvbg== 3367
IGF2b2lk 3368
b2tlcw== 3369
IEpldw== 3370
IHRvd2FyZHM= 3371
YXNj 3372
IC8v 3373
IEtvcmU= 3374
IHRhbGtpbmc= 3375
IGNvcnJlY3Q= 3376
IHNwZW50 3377
aWNrcw== 3378
aWFibGU= 3379
ZWFyZWQ= 3380
IHRlcm0= 3381
IHdhbnRz 3382
b21pbmc= 3383
IHV0 3384
IGRvdWI= 3385
IGZvcmNlcw== 3386
IHBsZWFzZQ== 3387
Njk= 3388
IE5vdmVtYmVy 3389
YXRmb3Jt 3390
b25kb24= 3391
IG9uZXM= 3392
IGltbWVkaWF0ZWx5 3393
IFJ1c3NpYW4= 3394
IE1ldA== 3395
IGRlZw== 3396
IHBhcmVudHM= 3397
Q0g= 3398
IEFtZXJpY2Fucw== 3399
YWx5 3400
IE1vZA== 3401
IHNob3du 3402
IGNvbmRpdGlvbnM= 3403
IHN0dWZm 3404
IHJlYg== 3405
IFlvdXI= 3406
IGluY2x1ZGVz 3407
bm93bg== 3408
IFNhbQ== 3409
IGV4cGVyaWVu 3410
bWlzc2lvbg== 3411
IEV2ZW4= 3412
YXVnaHQ= 3413
IGFubm91bmNlZA== 3414
IFJlcHVibGljYW4= 3415
IGRldGVybWlu 3416
IGRlc2NyaWJlZA== 3417
IENvdW50eQ== 3418
KCk= 3419
IGRvb3I= 3420
IGNoYW5nZWQ= 3421
IG5laWdo 3422
IEhlcmU= 3423
IGNsZWFu 3424
IHBhbg== 3425
IERlY2VtYmVy 3426
IEV1cm9wZWFu 3427
aXJpbmc= 3428
YXB0ZXI= 3429
IGNsdWI= 3430
IFR1ZXNkYXk= 3431
IHBhaWQ= 3432
IE5ldA== 3433
IGF0dGFja3M= 3434
IGNoYXJhY3RlcnM= 3435
IGFsb25l 3436
IGRpcmVjdG9y 3437
ZG9t 3438
IDM1 3439
IGxvYWQ= 3440
IHJvdXQ= 3441
IENhbGlmb3JuaWE= 3442
IGZpbmFsbHk= 3443
IHJhYw== 3444
IGNvbnRy 3445
IGV4YWN0bHk= 3446
cmVzaA== 3447
cHJp 3448
IElzbGFt 3449
IG5hdHVyZQ== 3450
IGNhcmVlcg== 3451
IGxhdGVzdA== 3452
IGNvbnZlcnM= 3453
IFNs 3454
cG9zZQ== 3455
Y2llbnQ= 3456
IEluYw== 3457
aXZpdHk= 3458
ODg= 3459
IEF0dA== 3460
IE1vcg== 3461
bmVzZGF5 3462
IHdlaWdodA== 3463
a2Vu 3464
IG5vdGU= 3465
IHRlYW1z 3466
IFw= 3467
YWlycw== 3468
IEdyZWVu 3469
IGh1bmRyZWQ= 3470
b25lbnQ= 3471
IHN0cmVuZw== 3472
IGNvbnNpc3Q= 3473
aWNhdGVk 3474
IHJlZ3Vs 3475
IGxpYw== 3476
YXN0aWM= 3477
IHRlbg== 3478
dXJzZGF5 3479
ZWxsaWdlbmNl 3480
b3VzbHk= 3481
IFVL 3482
Qkk= 3483
IGNvc3Rz 3484
IGluZGVwZW5k 3485
IEFQ 3486
IG5vcm1hbA== 3487
IGhvbQ== 3488
IG9idmlvdXM= 3489
IHN3ZQ== 3490
IHN0YXI= 3491
IHJlYWR5 3492
YWNoZXI= 3493
IGltcGxlbWVudA== 3494
Z2VzdA== 3495
IHNvbmc= 3496
IEdldA== 3497
IExhYg== 3498
IGludGVyZXN0aW5n 3499
dXNpbmc= 3500
IGdpdmluZw== 3501
IFN1bmRheQ== 3502
IGV0Yw== 3503
IG1pZGRsZQ== 3504
IHJlbWVtYmVy 3505
cmlnaHQ= 3506
b3NpdGlvbg== 3507
dXRpb25z 3508
IG1heA== 3509
NDY= 3510
IHlvdXJzZWxm 3511
IGRlbWFuZA== 3512
IHRyZWF0bWVudA== 3513
IGRhbmdlcg== 3514
IENvbnM= 3515
IGd1eQ== 3516
IEJyaXRpc2g= 3517
IHBoeXNpY2Fs 3518
IHJlbGF0ZWQ= 3519
IHJlbWFpbg== 3520
IGNvdWxkbg== 3521
IHJlZmVy 3522
IGNpdGl6 3523
Ym94 3524
RU5U 3525
Ym9hcmQ= 3526
IGlubg== 3527
SUc= 3528
ZXJv 3529
IFN0cmVldA== 3530
b3NwaXRhbA== 3531
cmVuY2g= 3532
Y2hlcnM= 3533
IHN0cmE= 3534
T0w= 3535
YWdlcg== 3536
IEFO 3537
IGVhc2lseQ== 3538
SUE= 3539
ZW5nZQ== 3540
aW55 3541
IGNsb3M= 3542
b2NrZWQ= 3543
IHVzZXM= 3544
IENvdW4= 3545
SW0= 3546
dWlsZA== 3547
Pz8= 3548
bW9yZQ== 3549
IGFuZw== 3550
IHdyaXRl 3551
b2x1dGU= 3552
NTc= 3553
IGxlYWRlcg== 3554
IHJlYWRpbmc= 3555
PC8= 3556
IGF1dG9t 3557
ZXN0cw== 3558
NDM= 3559
IGxlZ2lzbA== 3560
IEdvbGQ= 3561
IGRlc2lnbmVk 3562
IFNU 3563
IExlZw== 3564
YXJlcw== 3565
IGJlYXV0 3566
IFRleA== 3567
IGFwcGVhcnM= 3568
IHN0cnVnZw== 3569
IFJvbQ== 3570
IDAw 3571
IGNob2ljZQ== 3572
IHBhcnRpY3VsYXJseQ== 3573
IEZyb20= 3574
b3Blcg== 3575
IExvbmRvbg== 3576
YW5uZWQ= 3577
IGFsbG93cw== 3578
b2JpbGU= 3579
IGRpZmZlcmVuY2U= 3580
4oCi 3581
IFZpZXc= 3582
IFdlZG5lc2RheQ== 3583
IGFsdGhvdWdo 3584
IHJlbGF0aXZl 3585
IGFwcGxpY2F0aW9u 3586
YXRldmVy 3587
IGFyZW4= 3588
IG15c2VsZg== 3589
IGltYWc= 3590
IGRpc2U= 3591
IHNvY2lldHk= 3592
IGZyZXF1 3593
IEVuZ2xpc2g= 3594
IHBvb3I= 3595
IERheQ== 3596
IHdyaXRpbmc= 3597
IHNldmVu 3598
IHN0YXJ0aW5n 3599
IGJ1ZA== 3600
IHByaW50 3601
IFRyYW5z 3602
dWZhY3Q= 3603
IFN0dWQ= 3604
bmV3 3605
IGNyaW0= 3606
IGdpdmVz 3607
IGNvb2w= 3608
YWU= 3609
aWFuY2U= 3610
IEdlbmVyYWw= 3611
IHRoaW5raW5n 3612
IHNhdmU= 3613
IGxpbWl0ZWQ= 3614
IFBhcnR5 3615
IG1lYW5pbmc= 3616
cGVu 3617
b3dlcnM= 3618
IEphY2s= 3619
RU0= 3620
IG5pY2U= 3621
cnVwdA== 3622
IGdhcw== 3623
IGVpZ2h0 3624
IGZlZXQ= 3625
IGVmZm9ydA== 3626
IGlnbg== 3627
aWNpdA== 3628
Qmw= 3629
Y29pbg== 3630
IG9waW4= 3631
IGJyYWlu 3632
V2hpbGU= 3633
aGVzdA== 3634
IFRodXJzZGF5 3635
IHdvdWxkbg== 3636
YXVnaHRlcg== 3637
IHRvdWNo 3638
bGVtZW50cw== 3639
IHN0dWRpZXM= 3640
IGNlbnRlcg== 3641
Y29udA== 3642
b3JnZQ== 3643
IGNvbXB1dGVy 3644
IGludmVzdGlnYXRpb24= 3645
UGw= 3646
b3Jrcw== 3647
IDIwMDg= 3648
IGluY3JlYXNpbmc= 3649
IHN0b3Jl 3650
IGNvbW1lbnRz 3651
IGJhbA== 3652
bWVu 3653
IGRvbGw= 3654
IGxpYmVy 3655
IHdpZmU= 3656
IGxhd3M= 3657
YXR1cmRheQ== 3658
aXRuZXNz 3659
IG1vZGVybg== 3660
IFNr 3661
IGFkbWluaXN0cmF0aW9u 3662
IG9wcG9ydHVuaXR5 3663
IHNhbA== 3664
IHBvd2VyZnVs 3665
TXk= 3666
IGNsYWltcw== 3667
IEVhcnRo 3668
b3Jkcw== 3669
IHRpdGxl 3670
IGVzYw== 3671
bmFtZQ== 3672
Tm90 3673
b21lbg== 3674
IGJleW9uZA== 3675
IGNhbWVy 3676
IHNlbGw= 3677
aXR1dGU= 3678
ZWFyY2g= 3679
IGFwcGw= 3680
aW1lbnQ= 3681
NDI= 3682
IEFydA== 3683
IHVuZg== 3684
IHZpb2xlbmNl 3685
dXJn 3686
IEVhc3Q= 3687
IGNvbXBhcmVk 3688
IG9wdGlvbnM= 3689
IHRocm91Z2hvdXQ= 3690
IHZz 3691
aWdy 3692
Lls= 3693
YWNoZXM= 3694
Nzg= 3695
IGZpbGVz 3696
Rkw= 3697
RUw= 3698
YXJpYW4= 3699
IEphbWVz 3700
IEFpcg== 3701
YW5jaA== 3702
IGRldGFpbA== 3703
IHBpZWNl 3704
UFM= 3705
IG5hbWVk 3706
IGVkdWNhdGlvbg== 3707
IGRyaXZl 3708
IGl0ZW1z 3709
IHN0dWRlbnQ= 3710
aWNlZA== 3711
Ojo= 3712
aWNv 3713
IHRocm93 3714
IHNjZW5l 3715
IGNvbXBsZXg= 3716
IDIwMDk= 3717
IHByZWM= 3718
IEJyZQ== 3719
Nzk= 3720
IGNvbmNlcHQ= 3721
IHN0YXR1cw== 3722
YW1pbmc= 3723
IGRpZWQ= 3724
IGtub3dsZWRnZQ== 3725
IGJlZ2lubmluZw== 3726
T0Q= 3727
cnVhcnk= 3728
IGNlcnRhaW5seQ== 3729
IGd1eXM= 3730
IHNsaWdodA== 3731
aW5u 3732
b3VuZHM= 3733
IGZpbmU= 3734
IGZhdA== 3735
aWNhdGlvbnM= 3736
IHBlcmhhcHM= 3737
IEFudA== 3738
IGluY29tZQ== 3739
IGh0dHBz 3740
IG1ham9yaXR5 3741
cG9ydHM= 3742
c3Rvbg== 3743
IGdyZWF0ZXI= 3744
IGZlZWQ= 3745
ZW50aWFsbHk= 3746
IHNhZmV0eQ== 3747
IHVuaXF1ZQ== 3748
YW5kb20= 3749
IGdvbmU= 3750
IHNob3dlZA== 3751
IGhpc3Rvcg== 3752
IGNvdW50ZXI= 3753
aXVz 3754
aWRh 3755
IGxlYWRpbmc= 3756
aXBl 3757
IHNlbmQ= 3758
IERvbmFsZA== 3759
ZXJ2ZQ== 3760
IGRlZmVuc2U= 3761
aW5lc2U= 3762
IHllcw== 3763
IEZpcmU= 3764
IE11c2xpbQ== 3765
cmFx 3766
IGNvbnRpbnVlZA== 3767
b3No 3768
IHByb3ZpZGVz 3769
IHByaXNvbg== 3770
IFByZQ== 3771
IGhhcHB5 3772
IGVjb25vbXk= 3773
IHRydXN0 3774
YWdz 3775
IEdhbWU= 3776
IHdlYXBvbnM= 3777
dW1hbg== 3778
IENsZQ== 3779
aXRhdGlvbg== 3780
IGFuYWx5c2lz 3781
IFRpbWVz 3782
IHNjaWVuY2U= 3783
LT4= 3784
IGZpZ3VyZQ== 3785
IGRpc2FwcA== 3786
ZW50eQ== 3787
IHNvZnR3YXJl 3788
IHVsdA== 3789
IG9mZmljZXJz 3790
TmV3 3791
SXM= 3792
IHJlbWFpbnM= 3793
IEluZGlh 3794
IHBzeWNo 3795
cmllZg== 3796
IGNhdA== 3797
ZXNj 3798
IG9ic2Vydg== 3799
IHN0YWdl 3800
IERhcms= 3801
IGVudGVy 3802
Y2hhbmdl 3803
IHBhc3NlZA== 3804
IGRlc3BpdGU= 3805
IE91dA== 3806
IG1vdmll 3807
cnM= 3808
IHZvaWNl 3809
bWluZQ== 3810
IFBsYXk= 3811
IHRvd2FyZA== 3812
IFRlcg== 3813
IHJlZ2lvbg== 3814
IHZhbHVlcw== 3815
b3J0ZXJz 3816
IG1vdW50 3817
IG9mZmljZXI= 3818
IE90aGVy 3819
YmFu 3820
IGhvdXM= 3821
d29vZA== 3822
cm9vbQ== 3823
SVY= 3824
IFN1bg== 3825
c2Vl 3826
IE92ZXI= 3827
cm9n 3828
OTA= 3829
IGxheQ== 3830
IFR1cg== 3831
YXdu 3832
IHByZXNzdXJl 3833
IFN1Yg== 3834
IGJvb2tz 3835
ZWRvbQ== 3836
IFNhbmQ= 3837
QUE= 3838
YWdv 3839
IHJlYXNvbnM= 3840
Zm9yZA== 3841
IGFjdGl2aXR5 3842
VVQ= 3843
Tm93 3844
IFNlbmF0ZQ== 3845
Y2VsbA== 3846
bmlnaHQ= 3847
IGNhbGxz 3848
aW50ZXI= 3849
IGxldHRlcg== 3850
IFJvYg== 3851
IEpl 3852
IGNob29zZQ== 3853
IExhdw== 3854
R2V0 3855
QmU= 3856
IHJvYg== 3857
IHR5cGVz 3858
IHBsYXRmb3Jt 3859
IHF1YXJ0ZXI= 3860
UkE= 3861
IFRpbWU= 3862
IG1heWJl 3863
IENy 3864
OTU= 3865
cHJl 3866
IG1vdmluZw== 3867
IGxpZg== 3868
IGdvbGQ= 3869
IHNvbQ== 3870
IHBhdGllbnRz 3871
IHRydXRo 3872
IEtl 3873
dXJhbmNl 3874
YW50bHk= 3875
bWFy 3876
IGNoYXJnZQ== 3877
IEdyZWF0 3878
IGNlbGU= 3879
LS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0= 3880
IHJvY2s= 3881
cm9pZA== 3882
YW5jeQ== 3883
IGNyZWRpdA== 3884
YXVk 3885
Qnk= 3886
IEV2ZXJ5 3887
IG1vdmVk 3888
aW5nZXI= 3889
cmlidXRpb24= 3890
IG5hbWVz 3891
IHN0cmFpZ2h0 3892
IEhlYWx0aA== 3893
IFdlbGw= 3894
IGZlYXR1cmU= 3895
IHJ1bGU= 3896
IHNjaGU= 3897
aW5hdGVk 3898
IE1pY2hhZWw= 3899
YmVyZw== 3900
NDE= 3901
aWxlZA== 3902
YmFuZA== 3903
IGNsaWNr 3904
IEFuZ2Vs 3905
b25lbnRz 3906
wq0= 3907
IElyYXE= 3908
IFNhdHVyZGF5 3909
IGF3YXJl 3910
cGFydA== 3911
IHBhdHRlcm4= 3912
T1c= 3913
IExldA== 3914
IGdyYWQ= 3915
aWduZWQ= 3916
IGFzc29jaWF0ZWQ= 3917
IHN0eWxl 3918
bm8= 3919
aWF0aW9u 3920
YWl0aA== 3921
aWxpZXM= 3922
IHN0b3JpZXM= 3923
dXJhdGlvbg== 3924
IGluZGl2aWR1YWxz 3925
IOKApg== 3926
bWlzcw== 3927
IEFzc29jaQ== 3928
aXNoaW5n 3929
YWJ5 3930
IHN1bW1lcg== 3931
IEJlbg== 3932
IDMy 3933
IGFyY2g= 3934
dXR5 3935
IFRleGFz 3936
aG9s 3937
IGZ1bGx5 3938
IG1pbGw= 3939
IGZvbGxvd2Vk 3940
IEJpbGw= 3941
IEluZGlhbg== 3942
IFNlY3JldA== 3943
IEJlbA== 3944
IEZlYnJ1YXJ5 3945
IGpvYnM= 3946
IHNlZW1lZA== 3947
IEdvdmVybg== 3948
aXBwZWQ= 3949
IHJlYWxpdHk= 3950
IGxpbmVz 3951
IHBhcms= 3952
IG1lYXN1cmU= 3953
IE91cg== 3954
SU0= 3955
IGJyb3RoZXI= 3956
IGdyb3dpbmc= 3957
IGJhbg== 3958
IGVzdGlt 3959
IGNyeQ== 3960
IFNjaG9vbA== 3961
IG1lY2hhbg== 3962
IE9G 3963
IFdpbmRvd3M= 3964
IHJhdGVz 3965
IE9o 3966
IHBvc2l0aXZl 3967
IGN1bHR1cmU= 3968
aXN0aWNz 3969
aWNh 3970
IGhhcg== 3971
eWE= 3972
aXRlbHk= 3973
aXBw 3974
IG1hcA== 3975
ZW5jaWVz 3976
IFdpbGxpYW0= 3977
SUk= 3978
YWtlcnM= 3979
NTY= 3980
IE1hcnQ= 3981
IFJlbQ== 3982
IGFsdGVybg== 3983
aXR1ZGU= 3984
IGNvYWNo 3985
cm93ZA== 3986
RG9u 3987
IGtpZHM= 3988
IGpvdXJuYWw= 3989
IGNvcnBvcg== 3990
IGZhbHNl 3991
IHdlYg== 3992
IHNsZWVw 3993
IGNvbnRhaW4= 3994
IHN0bw== 3995
IGJlZA== 3996
aXZlcnNl 3997
IFJpY2g= 3998
IENoaW5lc2U= 3999
IHB1bg== 4000
IG1lYW50 4001
a25vd24= 4002
IG5vdGljZQ== 4003
IGZhdm9yaXRl 4004
YXZlbg== 4005
IGNvbmRpdGlvbg== 4006
IHB1cnBvc2U= 4007
KSk= 4008
IG9yZ2FuaXphdGlvbg== 4009
IGNoYWxsZW5n 4010
IG1hbnVmYWN0 4011
IHN1c3A= 4012
IEFj 4013
IGNyaXRpYw== 4014
dW5lcw== 4015
dWNsZWFy 4016
IG1lcg== 4017
dmVudGlvbg== 4018
IDgw 4019
IG1pc3Q= 4020
IFVz 4021
IFRvcg== 4022
aHR0cA== 4023
b2xm 4024
IGxhcmdlcg== 4025
IGFkdmFudA== 4026
IHJlc2Vhcg== 4027
IGFjdGlvbnM= 4028
bWw= 4029
IGtlcHQ= 4030
IGFpbQ== 4031
LCc= 4032
Y29s 4033
IGJlbmVmaXRz 4034
aWZ5aW5n 4035
IGFjdHVhbA== 4036
IEludGVybmF0aW9uYWw= 4037
IHZlaGljbGU= 4038
IGNoaWVm 4039
IGVmZm9ydHM= 4040
IExlYWd1ZQ== 4041
IE1vc3Q= 4042
IHdhaXQ= 4043
IGFkdWx0 4044
IG92ZXJhbGw= 4045
IHNwZWVjaA== 4046
IGhpZ2hseQ== 4047
IGZlbWFsZQ== 4048
IGVycm9y 4049
IGVmZmVjdGl2ZQ== 4050
NTQ= 4051
IGVuY291cg== 4052
d2VsbA== 4053
IGZhaWxlZA== 4054
IGNvbnNlcnY= 4055
IHByb2dyYW1z 4056
IHRyb3U= 4057
IGFoZWFk 4058
NTAw 4059
dmVydGlzZW1lbnQ= 4060
SVA= 4061
IEZvdW5k 4062
cGly 4063
ICU= 4064
IGNyaW1l 4065
YW5kZXI= 4066
IGxvY2F0aW9u 4067
IElyYW4= 4068
IGJlaGF2aW9y 4069
YXppbmc= 4070
IHJhcmU= 4071
IGVtYg== 4072
IGNhdXNlZA== 4073
IHNoaXA= 4074
IGFjdGl2ZQ== 4075
IGNvbnRyaWJ1dA== 4076
IGdyZWVu 4077
IGFjcXU= 4078
IHJlZmxlY3Q= 4079
dmVudWU= 4080
IGZpcm0= 4081
IGJpcnRo 4082
XS4= 4083
IGNsZWFybHk= 4084
IGVtb3Q= 4085
IGFnZW5jeQ== 4086
cmlhZ2U= 4087
IG1lbW9yeQ== 4088
OTg= 4089
U0E= 4090
IFNlZQ== 4091
YWNpbmc= 4092
Q0M= 4093
IGJpZ2dlc3Q= 4094
IHJhcA== 4095
IGJhc2lj 4096
IGJhbmQ= 4097
ZWF0 4098
IHN1c3BlY3Q= 4099
IE1hYw== 4100
IDkw 4101
bWFyaw== 4102
aXN0YW4= 4103
IHNwcmVhZA== 4104
YW1z 4105
a2k= 4106
YXN5 4107
cmF2 4108
IFJvYmVy 4109
IGRlbW9uc3Ry 4110
cmF0ZWQ= 4111
IGFic29sdXRl 4112
IHBsYWNlcw== 4113
IGltcGw= 4114
aWJyYXJ5 4115
IGNhcmRz 4116
IGRlc3Ryb3k= 4117
IHZpcnQ= 4118
dmVyZQ== 4119
IGFwcGVhcmVk 4120
eWFu 4121
cG9pbnQ= 4122
IGJlZw== 4123
IHRlbXBlcg== 4124
c3Bl 4125
YW50ZWQ= 4126
ZWFycw== 4127
IERpcmVjdA== 4128
IGxlbmd0aA== 4129
IGJsb2c= 4130
YW1i 4131
IGludGVn 4132
IHJlc291cmNlcw== 4133
YWNj 4134
aWZ1bA== 4135
IHNwb3Q= 4136
IGZvcmNlZA== 4137
IHRob3VzYW5kcw== 4138
IE1pbmlzdGVy 4139
IHF1YWw= 4140
IEZyZW5jaA== 4141
YXRpY2FsbHk= 4142
IGdlbmVyYWxseQ== 4143
IGRyaW5r 4144
IHRodXM= 4145
SUw= 4146
b2Rlcw== 4147
IGFwcHJvcHJp 4148
IFJlYWQ= 4149
IHdob20= 4150
IGV5ZQ== 4151
IGNvbGxlZ2U= 4152
IDQ1 4153
aXJlY3Rpb24= 4154
IGVuc3VyZQ== 4155
IGFwcGFyZW50 4156
aWRlcnM= 4157
IHJlbGlnaW91cw== 4158
IG1pbm9y 4159
b2xpYw== 4160
IHRybw== 4161
IFdoeQ== 4162
cmlidXRl 4163
bWV0 4164
IHByaW1hcnk= 4165
IGRldmVsb3BlZA== 4166
IHBlYWNl 4167
IHNraW4= 4168
c3Rl 4169
YXZh 4170
IGJsdWU= 4171
IGZhbWlsaWVz 4172
IGly 4173
IGFwcGx5 4174
IGluZm9ybQ== 4175
IFNtaXRo 4176
Q1Q= 4177
aWk= 4178
IGxpbWl0 4179
IHJlc2lzdA== 4180
Li4uLi4uLi4uLi4uLi4uLg== 4181
dW1u 4182
IGNvbmZsaWM= 4183
IHR3ZQ== 4184
dWRk 4185
IFRvbQ== 4186
IGxpdGVy 4187
cXVl 4188
Ym9u 4189
IGhhaXI= 4190
IGV2ZW50dWFsbHk= 4191
IHB1cw== 4192
IGhlbHBlZA== 4193
IGFnZw== 4194
b3JuZXk= 4195
IEFwcGxl 4196
IGZpdA== 4197
IFN1cg== 4198
IHByZW0= 4199
IHNhbGVz 4200
IHNlY29uZHM= 4201
IHN0cmVuZ3Ro 4202
IGZlZWxpbmc= 4203
v70= 4204
IHRvdXI= 4205
IGtub3dz 4206
b29t 4207
IGV4ZXJj 4208
IHNvbWV3 4209
77+9 4210
Pj4= 4211
IHNwb2tlcw== 4212
IGlkZWFz 4213
IHJlZ2lzdA== 4214
c29mdA== 4215
IERlbA== 4216
IFBD 4217
IHByb3Bvcw== 4218
IGxhdW5jaA== 4219
IGJvdHRvbQ== 4220
VEg= 4221
IFBsZWFzZQ== 4222
dmVzdA== 4223
aXR6 4224
IEludGVy 4225
IHNjcmlwdA== 4226
IHJhdA== 4227
YXJuaW5n 4228
IGls 4229
IEplcg== 4230
IEFyZQ== 4231
IHdoYXRldmVy 4232
b2tlbg== 4233
Y2llbmNl 4234
IG1vZGU= 4235
IGFncmVl 4236
IHNvdXJjZXM= 4237
IGluaXRpYWw= 4238
IHJlc3RyaWN0 4239
IHdvbmRlcg== 4240
dXNpb24= 4241
IyMjIw== 4242
IFNpbA== 4243
dmlsbGU= 4244
IGJ1cm4= 4245
dHc= 4246
YXNpb24= 4247
IMKj 4248
IG5vcg== 4249
dWluZw== 4250
IHJlYWNoZWQ= 4251
IHN1bg== 4252
IGNhdGVn 4253
aWdyYXRpb24= 4254
IGNvb2s= 4255
IHByb21vdA== 4256
IG1hbGU= 4257
IGNsaW1hdGU= 4258
IGZpeA== 4259
IGFsbGVnZWQ= 4260
VVI= 4261
YWxsZWQ= 4262
IGltYWdlcw== 4263
Q29udA== 4264
b3Rh 4265
IHNjaG9vbHM= 4266
aW9z 4267
IGRyb3A= 4268
IHN0cmVhbQ== 4269
IE1v 4270
IHByZXZpb3VzbHk= 4271
YWxpbmc= 4272
IHBldA== 4273
IGRvdWJsZQ== 4274
IChA 4275
YW5uZWw= 4276
IGRlZmF1bHQ= 4277
dGllcw== 4278
IHJhbms= 4279
IERlYw== 4280
IENvdW5jaWw= 4281
IHdlYXBvbg== 4282
IHN0b2Nr 4283
IGFuYWx5 4284
IFN0cg== 4285
IHBpY3R1cmU= 4286
IFBvbGljZQ== 4287
ZmVyZW5jZQ== 4288
IGNlbnR1cnk= 4289
IGNpdGl6ZW5z 4290
IG9udG8= 4291
IGV4cGFuZA== 4292
IGhlcm8= 4293
IFNvbA== 4294
IHdpbGQ= 4295
IHVwZGF0ZQ== 4296
IGN1c3RvbWVycw== 4297
cm9udA== 4298
ZGVm 4299
IGxpaw== 4300
IGNyaW1pbmFs 4301
IENocmlzdGlhbg== 4302
U1A= 4303
NzY= 4304
IGxlYXZpbmc= 4305
IG90aGVyd2lzZQ== 4306
IERpc3Q= 4307
IGJhc2lz 4308
NTI= 4309
NTM= 4310
aWNpcA== 4311
IEJlcg== 4312
IHJlY29tbWVuZA== 4313
IGZsb29y 4314
IGNyb3dk 4315
b2xlcw== 4316
IDcw 4317
IGNlbnRyYWw= 4318
IEV2 4319
IGRyZWFt 4320
IGRvd25sb2Fk 4321
IGNvbmZpcg== 4322
IFRob20= 4323
IHdpbmRvdw== 4324
IGhhcHBlbnM= 4325
IHVuaXQ= 4326
IHRlbmQ= 4327
IHNwbA== 4328
IGJlY29tZXM= 4329
IGZpZ2h0aW5n 4330
IHByZWRpY3Q= 4331
IFByZXNz 4332
IFBvd2Vy 4333
IGhlYXZ5 4334
YWtlZA== 4335
IGZhbg== 4336
b3J0ZXI= 4337
YXRlZ3k= 4338
QkE= 4339
aXplcw== 4340
IHNwZW5k 4341
SGVyZQ== 4342
IDIwMDc= 4343
IGFkb3A= 4344
IEhhbQ== 4345
IGZvb3RiYWxs 4346
IFBvcnQ= 4347
b2RheQ== 4348
NTE= 4349
YW1waW9ucw== 4350
IHRyYW5zZmVy 4351
aHQ= 4352
IDM4 4353
dGVybQ== 4354
YWNpdHk= 4355
IGJ1cg== 4356
XSw= 4357
dGVybmFs 4358
cmln 4359
YnV0 4360
IHRoZXJlZm9yZQ== 4361
IEJlY2F1c2U= 4362
cmVzcA== 4363
cmV5 4364
IG1pc3Npb24= 4365
U29tZQ== 4366
IG5vdGVk 4367
IGFzc3Vt 4368
IGRpc2Vhc2U= 4369
IGVkaXQ= 4370
IHByb2dyZXNz 4371
cmQ= 4372
IEJyb3du 4373
b2NhbA== 4374
IGFkZGluZw== 4375
IHJhaXNlZA== 4376
IEFueQ== 4377
IHRpY2s= 4378
IHNlZWluZw== 4379
IFBlb3BsZQ== 4380
IGFncmVlbWVudA== 4381
IHNlcnZlcg== 4382
IHdhdA== 4383
IGRlYmF0ZQ== 4384
IHN1cHBvc2Vk 4385
aWxpbmc= 4386
IGxhcmdlc3Q= 4387
IHN1Y2Nlc3NmdWw= 4388
IFByaQ== 4389
IERlbW9jcmF0aWM= 4390
IGp1bXA= 4391
IFN5cmlh 4392
IG93bmVycw== 4393
IG9mZmVycw== 4394
IHNob290aW5n 4395
IGVmZmlj 4396
c2V5 4397
IGhhdmVu 4398
dmVyc2U= 4399
dGVyZWQ= 4400
IExpZ2h0 4401
aW1hbA== 4402
IEJpZw== 4403
IGRlZmVuZA== 4404
IGJlYXQ= 4405
IHJlY29yZHM= 4406
JSk= 4407
IHNjZW4= 4408
IGVtcGxveWVlcw== 4409
IGRldmljZXM= 4410
aGVt 4411
IGNvbW1lcg== 4412
IE1leA== 4413
IGJlbmVmaXQ= 4414
IFByb2Y= 4415
IGlsbGVn 4416
IHN1cmZhY2U= 4417
IEFsc28= 4418
IGhhcm0= 4419
aW5nbHk= 4420
d2lkZQ== 4421
IEFsZXg= 4422
IHNodXQ= 4423
IEN1cg== 4424
IGxvc2U= 4425
cG0= 4426
IGNoYWxsZW5nZQ== 4427
c2VtYg== 4428
IHN0YXRpb24= 4429
IGludGVsbGlnZW5jZQ== 4430
IGFjY3Vy 4431
IEZsb3I= 4432
IHJlcXVpcmVz 4433
IE1hbA== 4434
YnVt 4435
IGhvc3BpdGFs 4436
IHNwaXJpdA== 4437
IG9mZmVyZWQ= 4438
IHByb2R1Y2U= 4439
IENvbW11bg== 4440
IGNyZWF0aW5n 4441
IGNyaXM= 4442
c3BlY3Q= 4443
IGVuZGVk 4444
IGRhaWx5 4445
IHZvdGVycw== 4446
bGFuZHM= 4447
aWFz 4448
aWg= 4449
b25h 4450
IHNtYXJ0 4451
IE9mZmljZQ== 4452
IExvcmQ= 4453
cmlhbA== 4454
IEludGVybmV0 4455
IGNpcmN1bQ== 4456
IGV4dHJlbWVseQ== 4457
Jy4= 4458
IG9waW5pb24= 4459
IE1pbA== 4460
IGdhaW4= 4461
QlM= 4462
IEZpbg== 4463
eXA= 4464
IHVzZWZ1bA== 4465
IGJ1ZGdldA== 4466
IGNvbWZvcnQ= 4467
aXNm 4468
IGJhY2tncm91bmQ= 4469
ZWxpbmU= 4470
IGVwaXNvZGU= 4471
IGVuZW15 4472
IHRyaWFs 4473
IGVzdGFibGlzaA== 4474
ZGF0ZQ== 4475
IENhcA== 4476
IGNvbnRpbnVlcw== 4477
IHNob3dpbmc= 4478
IFVuaW9u 4479
d2l0aA== 4480
IHBvc3RlZA== 4481
IFN5c3RlbQ== 4482
IGVhdA== 4483
cmlhbg== 4484
IHJpc2U= 4485
IEdlcm1hbnk= 4486
aWxz 4487
IHNpZ25lZA== 4488
IHZpbGw= 4489
IGdyYW5k 4490
bW9y 4491
IEVuZ2xhbmQ= 4492
IHByb2plY3Rz 4493
dW1iZXI= 4494
IGNvbmZlcmVuY2U= 4495
emE= 4496
IHJlc3BvbnNpYmxl 4497
IEFyYWI= 4498
IGxlYXJuZWQ= 4499
4oCU4oCU 4500
aXBwaW5n 4501
IEdlb3JnZQ== 4502
T0M= 4503
IHJldHVybmVk 4504
IEF1c3RyYWxpYQ== 4505
IGJyaWVm 4506
UXU= 4507
IGJyYW5k 4508
aWxsaW5n 4509
YWJsZWQ= 4510
IGhpZ2hlc3Q= 4511
IHRyYWlu 4512
IENvbW1pc3Npb24= 4513
d2hpbGU= 4514
IG5vbQ== 4515
Y2VwdGlvbg== 4516
IG11dA== 4517
IEJsdWU= 4518
IGluY2lkZW50 4519
dmFudA== 4520
ODY= 4521
IElE 4522
IG51Y2xlYXI= 4523
NzQ= 4524
IExpa2U= 4525
IFJF 4526
IE1pY3Jv 4527
bGk= 4528
bWFpbA== 4529
IGNoYXJnZXM= 4530
ODk= 4531
IGFkanVzdA== 4532
YWRv 4533
IGVhcnRo 4534
TkE= 4535
IHByaWNlcw== 4536
UEE= 4537
IGRyYWZ0 4538
IHJ1bnM= 4539
IGNhbmRpZGF0ZQ== 4540
ZW5zZXM= 4541
IG1hbmFnZW1lbnQ= 4542
IFBoaWw= 4543
IE1pc3M= 4544
IHRlYWNo 4545
Z3JhbQ== 4546
IHVuZGVyc3RhbmRpbmc= 4547
YWl0 4548
aWNhZ28= 4549
QWRk 4550
IEVw 4551
c2VjdXQ= 4552
IHNlcGFyYXRl 4553
IGluc3RhbmNl 4554
IGV0aA== 4555
IHVubGVzcw== 4556
KioqKioqKio= 4557
IEZvcmU= 4558
aW5hdGU= 4559
IG9wZXJhdGlvbnM= 4560
U3A= 4561
IGZhaXRo 4562
Z2Fy 4563
IENodXJjaA== 4564
cm9uaWM= 4565
IGNvbmZpZw== 4566
b3N1cmU= 4567
IGFjdGl2aXRpZXM= 4568
IHRyYWRpdGlvbmFs 4569
IDM2 4570
IGRpcmVjdGlvbg== 4571
IG1hY2hpbmU= 4572
IHN1cnJvdW5k 4573
IHB1c2g= 4574
dW5jdGlvbg== 4575
IEVV 4576
IGVhc2llcg== 4577
IGFyZ3VtZW50 4578
R0I= 4579
IG1pY3Jv 4580
IHNwZW5kaW5n 4581
aXphdGlvbnM= 4582
IHRoZW9yeQ== 4583
YWRvdw== 4584
IGNhbGxpbmc= 4585
IExhc3Q= 4586
IGRlcg== 4587
IGluZmx1ZW5jZQ== 4588
IGNvbW1pdA== 4589
IHBob3Rv 4590
IHVuYw== 4591
aXN0cnk= 4592
Z24= 4593
YXN0ZQ== 4594
YWNrcw== 4595
IGRpc3A= 4596
YWR5 4597
ZG8= 4598
IEdvb2Q= 4599
IGA= 4600
IHdpc2g= 4601
IHJldmVhbGVk 4602
wqDCoA== 4603
bGln 4604
IGVuZm9yY2U= 4605
IENvbW1pdHRlZQ== 4606
IGNoZW0= 4607
IG1pbGVz 4608
IGludGVyZXN0ZWQ= 4609
IHNvbHV0aW9u 4610
aWN5 4611
aW5jdA== 4612
IC0+ 4613
IERldA== 4614
IHJlbW92ZWQ= 4615
IGNvbXBhcg== 4616
ZWFo 4617
IHBsYW50 4618
IFNpbmNl 4619
IGFjaGlldmU= 4620
IGFkdmFudGFnZQ== 4621
IHNsaWdodGx5 4622
YmluZw== 4623
IHBsYWNlZA== 4624
dW5kZXI= 4625
MjAxNQ== 4626
IE1hZA== 4627
IHRpbQ== 4628
b3Nlcw== 4629
IGNydQ== 4630
IFJvY2s= 4631
IG1vc3RseQ== 4632
IG5lZ2F0aXZl 4633
IHNldHRpbmc= 4634
IHByb2R1Y2Vk 4635
IG11cg== 4636
IGNvbm5lY3Rpb24= 4637
IE1lcg== 4638
IGRyaXZlcg== 4639
IGV4ZWN1dGl2ZQ== 4640
IGFzc2F1bHQ= 4641
IGJvcm4= 4642
IFZlcg== 4643
dGFpbmVk 4644
IHN0cnVjdHVyZQ== 4645
IHJlZHVjZQ== 4646
IGRlY2FkZXM= 4647
IGRlZA== 4648
dWtl 4649
IE1hbnk= 4650
aWRkZW4= 4651
IGxlYWd1ZQ== 4652
U2U= 4653
IGpvaW4= 4654
IGRpc2Nv 4655
IGRpZQ== 4656
Y2tz 4657
YWN0aW9ucw== 4658
IGFzc2Vzcw== 4659
YWdu 4660
IGdvYWxz 4661
b3Vycw== 4662
SVI= 4663
IHNlbmlvcg== 4664
aWxsZXI= 4665
bW9k 4666
aXBtZW50 4667
b2NvbA== 4668
dXk= 4669
IFF1ZQ== 4670
IHBhcnRpZXM= 4671
aXJnaW4= 4672
IGxlYXJuaW5n 4673
aXRhYmxl 4674
IHN0cmVldA== 4675
IGNhbWVyYQ== 4676
QXBw 4677
IHNraWxscw== 4678
YnJl 4679
Y2lvdXM= 4680
IGNlbGVicg== 4681
IEZyYW5j 4682
IGV4aXN0aW5n 4683
IHdpbGxpbmc= 4684
bG9y 4685
IGlk 4686
IFNwYWNl 4687
IGNyaXRpY2Fs 4688
IExh 4689
b3J0dW5hdGVseQ== 4690
IHNlcnZl 4691
IGNvbGQ= 4692
IHNwZWNpZXM= 4693
VFM= 4694
IGFuaW1hbHM= 4695
IEJheQ== 4696
IG9sZGVy 4697
IFVuZGVy 4698
ZXN0aWM= 4699
IFRyZQ== 4700
IHRlYWNoZXI= 4701
IHByZWZlcg== 4702
dmlz 4703
IHRocmVhZA== 4704
IE1hdHQ= 4705
IG1hbmFnZXI= 4706
44O7 4707
IHByb2Zlc3Npb25hbA== 4708
IFZvbA== 4709
IG5vdGVz 4710
VGhlc2U= 4711
dWxh 4712
IGZyZXNo 4713
ZW50ZWQ= 4714
dXp6 4715
ZWR5 4716
Y2x1c2lvbg== 4717
IFJlbA== 4718
IGRvdWJ0 4719
RU8= 4720
IG9wZW5lZA== 4721
IEJpdA== 4722
QWR2ZXJ0aXNlbWVudA== 4723
IGd1ZXNz 4724
IFVO 4725
IHNlcXU= 4726
IGV4cGxhaW4= 4727
b3R0ZW4= 4728
IGF0dHJhY3Q= 4729
YWtz 4730
IHN0cmluZw== 4731
IGNvbnRleHQ= 4732
b3NzaWJsZQ== 4733
IFJlcHVibGljYW5z 4734
IHNvbGlk 4735
IGNpdGllcw== 4736
IGFza2luZw== 4737
IHJhbmRvbQ== 4738
dXBz 4739
dXJpZXM= 4740
YXJhbnQ= 4741
ZGRlbg== 4742
Z2w= 4743
IEZsb3JpZGE= 4744
IGRlcGVuZA== 4745
IFNjb3R0 4746
IDMz 4747
IGlU 4748
aWNvbg== 4749
IG1lbnRpb25lZA== 4750
IDIwMDA= 4751
IGNsYWltZWQ= 4752
IGRlZmluaXRlbHk= 4753
dWxm 4754
IGNvcmU= 4755
IG9wZW5pbmc= 4756
IENvbnN0 4757
d2hpY2g= 4758
IFRyYQ== 4759
QUc= 4760
NzI= 4761
IGJlbGlldmVk 4762
YWRh 4763
IDQ4 4764
IFNlY3VyaXR5 4765
eXJpZ2h0 4766
IFBldA== 4767
IExvdQ== 4768
IGhvbGRpbmc= 4769
PT09PT09PT09PT09PT09PQ== 4770
IGljZQ== 4771
IGJyb3c= 4772
IGF1dGhvcml0aWVz 4773
aG9zdA== 4774
d29yZA== 4775
IHNjb3Jl 4776
IERpdg== 4777
IGNlbGxz 4778
IHRyYW5zbA== 4779
IG5laWdoYm9y 4780
IHJlbW92ZQ== 4781
dWN0 4782
IGRpc3RyaWN0 4783
IEFjY29yZGluZw== 4784
IHdvcnNl 4785
IGNvbmNlcm5z 4786
IHByZXNpZGVudGlhbA== 4787
IHBvbGljaWVz 4788
IEhhbGw= 4789
NzM= 4790
IGh1cw== 4791
QVk= 4792
IDIwMDY= 4793
IEp1ZA== 4794
IGluZGVwZW5kZW50 4795
IEp1c3RpY2U= 4796
aWxpYXI= 4797
cHJpbnQ= 4798
aWdodGVy 4799
IHByb3RlY3Rpb24= 4800
emVu 4801
IHN1ZGRlbg== 4802
aG91c2U= 4803
IEplcw== 4804
UFI= 4805
IEluZg== 4806
IGJ1bA== 4807
IF8= 4808
IFNlcnZpY2U= 4809
IFBS 4810
IHN0cmF0ZWd5 4811
ZmZlY3Q= 4812
IGdpcmxz 4813
IG1pc3Npbmc= 4814
b3lhbA== 4815
IFRlYW0= 4816
dWxhdGVk 4817
IGRhdA== 4818
IHBvbGl0aWNz 4819
YWJvcg== 4820
QWNjb3JkaW5n 4821
IHNwZWxs 4822
IGdyYXBo 4823
b3J0aGVybg== 4824
VEM= 4825
QWI= 4826
IGxhYm9y 4827
aXNoZXI= 4828
IGtpY2s= 4829
IGlUdW5lcw== 4830
IHN0ZXBz 4831
cG9zZXM= 4832
IHNtYWxsZXI= 4833
RW4= 4834
YmVydA== 4835
IHJvbGw= 4836
IHJlc2VhcmNoZXJz 4837
IGNsb3NlZA== 4838
IHRyYW5zcG9ydA== 4839
IGxhd3k= 4840
X19fX19fX19fX19fX19fXw== 4841
IENoaWNhZ28= 4842
IGFzcGVjdA== 4843
IG5vbmU= 4844
IG1hcnJpYWdl 4845
OTY= 4846
IGVsZW1lbnRz 4847
IEZyZQ== 4848
IFNhbA== 4849
IGRyYW0= 4850
RkM= 4851
dG9w 4852
ZXF1 4853
IGhlYXJpbmc= 4854
IHN1cHBvcnRlZA== 4855
IHRlc3Rpbmc= 4856
Y29ob2w= 4857
IG1hc3NpdmU= 4858
IHN0aWNr 4859
IGd1YXJk 4860
aXNjbw== 4861
cGhvbmU= 4862
RnJvbQ== 4863
SG93ZXZlcg== 4864
IGJvcmRlcg== 4865
IGNvcHk= 4866
b2dyYXBoeQ== 4867
bGlzdA== 4868
NzE= 4869
IG93bmVy 4870
Y2xhc3M= 4871
cnVpdA== 4872
cmF0ZQ== 4873
IE9uY2U= 4874
IGRpZ2l0YWw= 4875
IHRhc2s= 4876
RVJT 4877
IGluY3JlZA== 4878
dGVz 4879
Kys= 4880
IEZyYW5jZQ== 4881
IGJyZWF0 4882
b3ds 4883
IGlzc3VlZA== 4884
IFdlc3Rlcm4= 4885
IGRldGVjdA== 4886
IHBhcnRuZXJz 4887
IHNoYXJlZA== 4888
IENhbGw= 4889
IGNhbmNlcg== 4890
YWNoZQ== 4891
cmliZQ== 4892
IGV4cGxhaW5lZA== 4893
IGhlYXQ= 4894
eyI= 4895
IGludmVzdG1lbnQ= 4896
IEJvb2s= 4897
IHdvb2Q= 4898
IHRvb2xz 4899
IEFsdGhvdWdo 4900
IGJlbGllZg== 4901
IGNyaXNpcw== 4902
IGdl 4903
IE1Q 4904
IG9wZXJhdGlvbg== 4905
dHlwZQ== 4906
fn4= 4907
Z2E= 4908
IGNvbnRhaW5z 4909
YW50YQ== 4910
IGV4cHJlc3M= 4911
IEdyb3Vw 4912
IEpvdXJuYWw= 4913
a2E= 4914
IGFtYg== 4915
IFVTQQ== 4916
IGZpbmRpbmc= 4917
IGZ1bmRpbmc= 4918
aG93 4919
IGVzdGFibGlzaGVk 4920
aWRlb3M= 4921
IGRlZ3JlZQ== 4922
IGRhbmdlcm91cw== 4923
YW5naW5n 4924
IGZyZWVkb20= 4925
cHBvcnQ= 4926
b3V0aGVybg== 4927
IGNodXJjaA== 4928
IGNhdGNo 4929
IFR3bw== 4930
IHByZXNlbmNl 4931
IEd1YXJk 4932
VXA= 4933
IGF1dGhvcml0eQ== 4934
IFByb2plY3Q= 4935
IGJ1dHRvbg== 4936
IGNvbnNlcXU= 4937
IHZhbGlk 4938
IHdlYWs= 4939
IHN0YXJ0cw== 4940
IHJlZmVyZW5jZQ== 4941
IE1lbQ== 4942
Iik= 4943
VU4= 4944
b3JhZ2U= 4945
IE9wZW4= 4946
IGNvbGxlY3Rpb24= 4947
eW0= 4948
Z2VuY3k= 4949
IGJlYXV0aWZ1bA== 4950
cm9z 4951
IHRlbGxz 4952
IHdhaXRpbmc= 4953
bmVs 4954
IHByb3ZpZGluZw== 4955
IERlbW9jcmF0cw== 4956
IGRhdWdodGVy 4957
IG1hc3Rlcg== 4958
IHB1cnBvc2Vz 4959
IEphcGFuZXNl 4960
IGVxdWFs 4961
IHR1cm5z 4962
IGRvY3VtZW50cw== 4963
IHdhdGNoaW5n 4964
UmVz 4965
IHJhbg== 4966
MjAxNA== 4967
IHJlamVjdA== 4968
IEtvcmVh 4969
IHZpY3RpbXM= 4970
TGV2ZWw= 4971
ZXJlbmNlcw== 4972
IHdpdG5lc3M= 4973
IDM0 4974
IHJlZm9ybQ== 4975
Y29taW5n 4976
IG9jY3Vw 4977
IGNhdWdodA== 4978
IHRyYWZmaWM= 4979
YWRpbmc= 4980
IG1vZGVscw== 4981
YXJpbw== 4982
IHNlcnZlZA== 4983
IGJhdHRlcg== 4984
dWF0ZQ== 4985
IFNlY3JldGFyeQ== 4986
IGFncmVlZA== 4987
IHRydWx5 4988
eW5hbQ== 4989
IFJldA== 4990
IHVuaXRz 4991
IFJlc2VhcmNo 4992
aGFuZA== 4993
YXppbmU= 4994
IE1pa2U= 4995
IHZhcmlldHk= 4996
b3RhbA== 4997
IGFtYXppbmc= 4998
IGNvbmZpcm1lZA== 4999
IGVudGlyZWx5 5000
IHB1cmNoYXNl 5001
IGVsZW1lbnQ= 5002
IGNhc2g= 5003
IGRldGVybWluZQ== 5004
RGU= 5005
IGNhcnM= 5006
IFdhbGw= 5007
4pY= 5008
IHZpZXdz 5009
IGRydWdz 5010
IGRlcGFydG1lbnQ= 5011
IFN0ZXA= 5012
dWl0 5013
IDM5 5014
YXN1cmU= 5015
IENsYXNz 5016
IGNvdmVyZWQ= 5017
IEJhbms= 5018
IG1lcmU= 5019
dWFuYQ== 5020
IG11bHRp 5021
IG1peA== 5022
IHVubGlrZQ== 5023
bGV2aXNpb24= 5024
IHN0b3BwZWQ= 5025
IHNlbQ== 5026
IEdhbA== 5027
dWxlcw== 5028
IHdlbA== 5029
IEpvaG5zb24= 5030
bGE= 5031
IHNraWxs 5032
IGJlY29taW5n 5033
cmll 5034
IGFwcHJvcHJpYXRl 5035
ZmU= 5036
ZWxsb3c= 5037
IFByb3Q= 5038
dWxhdGU= 5039
b2NhdGlvbg== 5040
IHdlZWtlbmQ= 5041
b2RpZXM= 5042
IHNpdGVz 5043
IGFuaW1hbA== 5044
IFRpbQ== 5045
IHNjYWxl 5046
IGNoYXJnZWQ= 5047
IGluc3RydWN0 5048
aWxsYQ== 5049
IG1ldGhvZHM= 5050
IGNlcnQ= 5051
IGp1ZGdl 5052
IEhlbA== 5053
IGRvbGxhcnM= 5054
IHN0YW5kaW5n 5055
IFNxdQ== 5056
IGRlYnQ= 5057
bGlhbQ== 5058
IGRyaXZpbmc= 5059
IFN1bQ== 5060
IEVkaXRpb24= 5061
IGFsYnVt 5062
YW5kb24= 5063
SUY= 5064
IFVr 5065
NjM= 5066
YWRlcg== 5067
IGNvbW1lcmNpYWw= 5068
ZXNo 5069
IEdvdmVybm1lbnQ= 5070
IGRpc2NvdmVyZWQ= 5071
IG91dHB1dA== 5072
IEhpbGxhcnk= 5073
IENhcm9s 5074
IDIwMDU= 5075
IGFidXNl 5076
YW5jaW5n 5077
IHN3aXRjaA== 5078
IGFubnVhbA== 5079
VHc= 5080
IHN0YXRlZA== 5081
YWdlbWVudA== 5082
aW5uZXI= 5083
IGRlbW9jcg== 5084
IHJlc2lkZW50cw== 5085
IGFsbG93aW5n 5086
IGZhY3RvcnM= 5087
b2Rk 5088
IGZ1Y2s= 5089
ZW1pZXM= 5090
IG9jY3VycmVk 5091
b3Rp 5092
IG5vcnRo 5093
IFB1YmxpYw== 5094
IGluanVyeQ== 5095
IGluc3VyYW5jZQ== 5096
Q0w= 5097
b2xseQ== 5098
44A= 5099
IHJlcGVhdGVk 5100
IGFybXM= 5101
YW5nZWQ= 5102
IGNvbnN0cnVjdGlvbg== 5103
IGZsZQ== 5104
UFU= 5105
aWNpYW5z 5106
IGZvcm1z 5107
IE1jQw== 5108
YW50aWM= 5109
IG1lbnRhbA== 5110
cGlyZQ== 5111
IGVxdWlwbWVudA== 5112
IGZhbnQ= 5113
IGRpc2N1c3Npb24= 5114
IHJlZ2FyZGluZw== 5115
a2lu 5116
YXJw 5117
IGNoYWly 5118
b2d1ZQ== 5119
IHByb2NlZWQ= 5120
IElk 5121
T3Vy 5122
IG11cmRlcg== 5123
TWFu 5124
IDQ5 5125
YXNw 5126
IHN1cHBseQ== 5127
IGlucHV0 5128
IHdlYWx0aA== 5129
bGlhbWVudA== 5130
IHByb2NlZA== 5131
b3JpYWw= 5132
IFN0YXQ= 5133
IE5GTA== 5134
aGVucw== 5135
IEluc3RpdHV0ZQ== 5136
IHB1dHRpbmc= 5137
b3VybmFtZW50 5138
ZXRpYw== 5139
IGxvY2F0ZWQ= 5140
IGtpZA== 5141
ZXJpYQ== 5142
cnVu 5143
IHByaW5j 5144
ICE= 5145
Z29pbmc= 5146
IEJldA== 5147
IGNsb3Q= 5148
IHRlbGxpbmc= 5149
IHByb3Bvc2Vk 5150
aW90 5151
b3JyeQ== 5152
IGZ1bmRz 5153
Z21lbnQ= 5154
IExpZmU= 5155
IGJhYnk= 5156
IEJhY2s= 5157
IHNwb2tl 5158
SW1hZ2U= 5159
IGVhcm4= 5160
IEFU 5161
Z3U= 5162
IGV4Y2hhbmdl 5163
IExpbg== 5164
b3Zpbmc= 5165
IHBhaXI= 5166
TW9yZQ== 5167
YXpvbg== 5168
IGFycmVzdGVk 5169
IGtpbGxpbmc= 5170
Y2Fu 5171
IENhcmQ= 5172
eWQ= 5173
IGlkZW50aWZpZWQ= 5174
IG1vYmlsZQ== 5175
IHRoYW5rcw== 5176
b255bQ== 5177
IEZvcm0= 5178
IGh1bmRyZWRz 5179
IENocmlz 5180
IENhdA== 5181
IHRyZW5k 5182
aGF0 5183
IEF2 5184
b21hbg== 5185
IGVsZWN0cmlj 5186
IFdpbA== 5187
U0U= 5188
T2Y= 5189
IHJlc3RhdXI= 5190
b3RlZA== 5191
IHRyaWc= 5192
IG5pbmU= 5193
IGJvbWI= 5194
V2h5 5195
wq8= 5196
IGNvdmVyYWdl 5197
IGFwcGVhbA== 5198
IFJvYmVydA== 5199
IFN1cA== 5200
IGZpbmlzaGVk 5201
IGZsb3c= 5202
IGRlbGl2ZXI= 5203
IGNhbGN1bA== 5204
IHBob3Rvcw== 5205
IHBoaWw= 5206
IHBpZWNlcw== 5207
IGFwcHJl 5208
a2Vz 5209
IHJvdWdo 5210
RG8= 5211
IHBhcnRuZXI= 5212
IGNvbmNlcm5lZA== 5213
IDM3 5214
IEdlbg== 5215
Q29s 5216
Y3RvcnM= 5217
ID0+ 5218
c3RhdGU= 5219
IHN1Z2dlc3RlZA== 5220
IEZvcmNl 5221
Q0U= 5222
IGhlcnNlbGY= 5223
IFBsYW4= 5224
d29ya3M= 5225
b290aA== 5226
cmVuY3k= 5227
IGNvcm5lcg== 5228
IGh1c2JhbmQ= 5229
IGludGVybmV0 5230
IEF1dA== 5231
ZW1z 5232
b3Nlbg== 5233
IEF0bA== 5234
Z2Vu 5235
IGJhbGFuY2U= 5236
NjI= 5237
IHNvdW5kcw== 5238
dGV4dA== 5239
IGFycg== 5240
b3Zlcw== 5241
IG1pbGxpb25z 5242
IHJhZGlv 5243
IHNhdGlzZg== 5244
IERhbQ== 5245
TXI= 5246
R28= 5247
U3Bl 5248
IGNvbWJhdA== 5249
cmFudA== 5250
IEdyZWU= 5251
IGZ1ZWw= 5252
IGRpc3RhbmNl 5253
IHRlc3Rz 5254
IGRlY3Jl 5255
IEVy 5256
IG1hbmFnZWQ= 5257
RFM= 5258
IHRpdA== 5259
IG1lYXN1cmVz 5260
IExpYmVy 5261
IGF0dGVuZA== 5262
YXNoZWQ= 5263
IEpvc2U= 5264
IE5pZ2h0 5265
ZGl0 5266
IE5vdg== 5267
IEVuZA== 5268
b3V0cw== 5269
IGdlbmVyYXRpb24= 5270
IGFkdm9j 5271
eXRo 5272
IGNvbnZlcnNhdGlvbg== 5273
IFNreQ== 5274
YWN0aXZl 5275
Y2Vs 5276
cmllcg== 5277
IEZyYW5r 5278
IGdlbmRlcg== 5279
IGNvbmNlbnQ= 5280
IGNhcnJpZWQ= 5281
YW5kYQ== 5282
IFZpcmdpbg== 5283
IGFycml2ZWQ= 5284
aWNpZGU= 5285
YWRlZA== 5286
IGZhaWx1cmU= 5287
IG1pbmltdW0= 5288
bGV0cw== 5289
IHdvcnN0 5290
IGtlZXBpbmc= 5291
IGludGVuZGVk 5292
IGlsbGVnYWw= 5293
IHN1YnNj 5294
IGRldGVybWluZWQ= 5295
IHRyaXA= 5296
WWVz 5297
IHJhaXNl 5298
IH4= 5299
IGZlZWxz 5300
IHBhY2thZ2U= 5301
IEpv 5302
aGk= 5303
MjAxNg== 5304
cmVhbA== 5305
IGZyYQ== 5306
IHN5bWI= 5307
TWU= 5308
dWNreQ== 5309
cHJldA== 5310
IEto 5311
IEVkaXQ= 5312
IFdlYg== 5313
ZW1pYw== 5314
IENvbG9y 5315
IGp1c3RpY2U= 5316
SW50 5317
IGZhcm0= 5318
Y2tub3c= 5319
Ij4= 5320
ZWxlc3M= 5321
IHJlZHVjZWQ= 5322
IDUwMA== 5323
eHg= 5324
IFJhZA== 5325
IFdvb2Q= 5326
IGNsaW4= 5327
IGh5cA== 5328
aWxlcg== 5329
dXJh 5330
a2lucw== 5331
ODU= 5332
NjE= 5333
IFRoZWly 5334
IE1hcnk= 5335
IHNhbg== 5336
IG5vdmVs 5337
IFdobw== 5338
IGNhcGFjaXR5 5339
IGltcG9zc2libGU= 5340
IHBsYXlz 5341
IG1pbmlzdGVy 5342
aWp1YW5h 5343
aWNhdGU= 5344
IFNldA== 5345
IGZyYW0= 5346
IGluZw== 5347
IGNvbW11bml0aWVz 5348
IEZCSQ== 5349
aXRh 5350
IGJvbg== 5351
IHN0cmF0ZWc= 5352
IGludGVyZXN0cw== 5353
bG9jaw== 5354
Z2Vycw== 5355
bWFz 5356
IEFORA== 5357
IGNvbmZsaWN0 5358
IHJlcXVpcmVtZW50cw== 5359
IHNhYw== 5360
IG9wZXJhdGluZw== 5361
aW5p 5362
cmVsYXRlZA== 5363
IGNvbW1pdHRlZA== 5364
IHJlbGF0aXZlbHk= 5365
IHNvdXRo 5366
wq/Crw== 5367
IGFmZm9yZA== 5368
IGlkZW50aXR5 5369
IGRlY2lzaW9ucw== 5370
IGFjY3VzZWQ= 5371
cGxhY2U= 5372
IHZpY3Rvcnk= 5373
b2No 5374
aWF0 5375
TmFtZQ== 5376
Q29t 5377
dGlvbg== 5378
ZWRz 5379
IHNlZWs= 5380
IHRpZ2h0 5381
IEltYWdlcw== 5382
IGluaXRp 5383
IGh1bWFucw== 5384
IGZhbWlsaWFy 5385
IGF1ZGllbmNl 5386
IGludGVybmFs 5387
dmVudHVyZQ== 5388
IHNpZGVz 5389
IFRP 5390
IGRpbQ== 5391
IGNvbmNsdWQ= 5392
IGFwcG9pbnQ= 5393
IGVuZm9yY2VtZW50 5394
IEppbQ== 5395
IEFzc29jaWF0aW9u 5396
IGNpcmN1bXN0 5397
IENhbmFkaWFu 5398
IGpvaW5lZA== 5399
IGRpZmZlcmVuY2Vz 5400
IExvcw== 5401
IHByb3Rlc3Q= 5402
IHR3aWNl 5403
d2lu 5404
IGdsYXNz 5405
YXJzaA== 5406
IEFybXk= 5407
IGV4cHJlc3Npb24= 5408
IGRlY2lkZQ== 5409
IHBsYW5uaW5n 5410
YW5pYQ== 5411
IGhhbmRsZQ== 5412
IE1pY3Jvc29mdA== 5413
IE5vcg== 5414
IG1heGltdW0= 5415
IFJldg== 5416
IHNlYQ== 5417
IGV2YWw= 5418
IGhlbHBz 5419
cmVm 5420
IGJvdW5k 5421
IG1vdXRo 5422
IHN0YW5kYXJkcw== 5423
IGNsaW0= 5424
IENhbXA= 5425
IEZveA== 5426
Y2xlcw== 5427
IGFybXk= 5428
IFRlY2hu 5429
YWNraW5n 5430
eHk= 5431
U1M= 5432
IDQy 5433
IGJ1Zw== 5434
IFVrcmFpbg== 5435
IE1heA== 5436
IEpvbmVz 5437
IFNob3c= 5438
bG8= 5439
IHBsYW5ldA== 5440
IDc1 5441
IHdpbm5pbmc= 5442
IGZhc3Rlcg== 5443
IHNwZWN0 5444
IGJyb2tlbg== 5445
VFI= 5446
IGRlZmluZWQ= 5447
IGhlYWx0aHk= 5448
IGNvbXBldGl0aW9u 5449
aHR0cHM= 5450
IElzbGFuZA== 5451
IEZl 5452
IGFubm91bmNl 5453
IEN1cA== 5454
IEluc3RlYWQ= 5455
IGNsaWVudA== 5456
IHBvc3NpYmx5 5457
c2VjdGlvbg== 5458
b2NrZXQ= 5459
bG9vaw== 5460
IGZpbmlzaA== 5461
IGNyZXc= 5462
IHJlc2Vydg== 5463
IGVkaXRvcg== 5464
IGhhdGU= 5465
IHNhbGU= 5466
IGNvbnRyb3ZlcnM= 5467
IHBhZ2Vz 5468
d2luZw== 5469
IG51bWVy 5470
IG9wcG9zaXRpb24= 5471
IDIwMDQ= 5472
IHJlZnVnZQ== 5473
IGZsaWdodA== 5474
IGFwYXJ0 5475
IExhdA== 5476
QW1lcmlj 5477
IEFmcmljYQ== 5478
IGFwcGxpY2F0aW9ucw== 5479
IFBhbGVzdA== 5480
IEJ1cg== 5481
IGdhcg== 5482
IFNvY2lhbA== 5483
IHVwZ3I= 5484
IHNoYXBl 5485
IHNwZWFraW5n 5486
YW5zaW9u 5487
YW8= 5488
IFNu 5489
IHdvcnJ5 5490
IEJyaXRhaW4= 5491
UGxlYXNl 5492
cm91ZA== 5493
IGh1bg== 5494
IGludHJvZHVjZWQ= 5495
IGRpZXQ= 5496
SW5k 5497
IFNlY29uZA== 5498
IGZ1bmN0aW9ucw== 5499
dXRz 5500
IEVhY2g= 5501
IEplZmY= 5502
IHN0cmVzcw== 5503
IGFjY291bnRz 5504
IGd1YXJhbnQ= 5505
IEFubg== 5506
ZWRpYQ== 5507
IGhvbmVzdA== 5508
IHRyZWU= 5509
IEFmcmljYW4= 5510
IEJ1c2g= 5511
fSw= 5512
IHNjaA== 5513
IE9ubHk= 5514
IGZpZg== 5515
aWdhbg== 5516
IGV4ZXJjaXNl 5517
IEV4cA== 5518
IHNjaWVudGlzdHM= 5519
IGxlZ2lzbGF0aW9u 5520
IFdvcms= 5521
IFNwcg== 5522
w4I= 5523
IEh1bWFu 5524
IOg= 5525
IHN1cnZleQ== 5526
IHJpY2g= 5527
cmlw 5528
IG1haW50YWlu 5529
IGZsbw== 5530
IGxlYWRlcnNoaXA= 5531
c3RyZWFt 5532
IElzbGFtaWM= 5533
IDAx 5534
IENvbGxlZ2U= 5535
IG1hZ2lj 5536
IFByaW1l 5537
IGZpZ3VyZXM= 5538
MjAxNw== 5539
aW5kZXI= 5540
eHVhbA== 5541
IERlYWQ= 5542
IGFic29sdXRlbHk= 5543
IGZvdXJ0aA== 5544
IHByZXNlbnRlZA== 5545
cmVzcG9uZA== 5546
cmlibGU= 5547
IGFsY29ob2w= 5548
YXRv 5549
IERF 5550
cG9yYXJ5 5551
IGdyYWI= 5552
IHZhcmk= 5553
IHF1YW50 5554
IFBob3Rv 5555
IHBsdXM= 5556
cmljaw== 5557
YXJrcw== 5558
IGFsdGVybmF0aXZl 5559
IHBpbA== 5560
IGFwcHJveA== 5561
dGhhdA== 5562
IG9iamVjdHM= 5563
IFJv 5564
IEFuZHJvaWQ= 5565
IHNpZ25pZmljYW50bHk= 5566
IFJvYWQ= 5567
a2F5 5568
UmVhZA== 5569
YXZvcg== 5570
IGFja25vdw== 5571
IEhE 5572
IFNpbmc= 5573
T3I= 5574
IE1vbnQ= 5575
IHVucw== 5576
cHJvZg== 5577
IG5lZ290aQ== 5578
IEFyY2g= 5579
aWtp 5580
IHRlbGV2aXNpb24= 5581
IEpld2lzaA== 5582
IGNvbW1pdHRlZQ== 5583
IG1vdG9y 5584
IGFwcGVhcmFuY2U= 5585
IHNpdHRpbmc= 5586
IHN0cmlrZQ== 5587
IERvd24= 5588
Y29tcA== 5589
IEhpc3Q= 5590
IGZvbGQ= 5591
YWNlbWVudA== 5592
IExvdWlz 5593
IGJlbG9uZw== 5594
IOKAog== 5595
IG1vcnQ= 5596
IHByZXBhcmVk 5597
IDY0 5598
IE1hc3Rlcg== 5599
IGluZGVlZA== 5600
IERlbg== 5601
IHJlbnQ= 5602
VEE= 5603
b3VybmV5 5604
YXJj 5605
U3U= 5606
OTc= 5607
IGFkdmljZQ== 5608
IGNoYW5naW5n 5609
IGxpc3RlZA== 5610
IGxhdW5jaGVk 5611
aXNhdGlvbg== 5612
IFBldGVy 5613
aXNoZXM= 5614
IGxpdmVk 5615
IE1lbA== 5616
IFN1cHJlbWU= 5617
IEZlZGVyYWw= 5618
ICk7 5619
cnVjdHVyZQ== 5620
IHNldHM= 5621
IHBoaWxvcw== 5622
dW91cw== 5623
IMKg 5624
IGFwcGxpZWQ= 5625
IE5PVA== 5626
IGhvdXNpbmc= 5627
IE1vdW50 5628
IG9kZA== 5629
IHN1c3Q= 5630
REE= 5631
ZmZpY2llbnQ= 5632
ID8= 5633
b2x2ZWQ= 5634
IHBvd2Vycw== 5635
IHRocg== 5636
IHJlbWFpbmluZw== 5637
IFdhdGVy 5638
TEM= 5639
IGNhdXNlcw== 5640
44Gu 5641
IG1hbm5lcg== 5642
YWRz 5643
IHN1Z2dlc3Rz 5644
IGVuZHM= 5645
c3RhbmRpbmc= 5646
Zmln 5647
IER1bg== 5648
aWR0aA== 5649
IGdheQ== 5650
IHRlcm1pbg== 5651
IEFuZ2VsZXM= 5652
TVM= 5653
IHNjaWVudGlmaWM= 5654
IGNvYWw= 5655
YXBlcnM= 5656
YmFy 5657
IFRob21hcw== 5658
IHN5bQ== 5659
IFJ1bg== 5660
dGhpcw== 5661
UEM= 5662
aWdyYW50cw== 5663
IG1pbnV0ZQ== 5664
IERpc3RyaWN0 5665
Y2VsbGVudA== 5666
IGxlYXZlcw== 5667
IGNvbXBsZXRlZA== 5668
YW1pbg== 5669
IGZvY3VzZWQ= 5670
IG1vbml0b3I= 5671
IHZlaGljbGVz 5672
TUE= 5673
IE1hc3M= 5674
IEdyYW5k 5675
IGFmZmVjdGVk 5676
aXR1dGlvbmFs 5677
IGNvbnN0cnVjdA== 5678
IGZvbGxvd3M= 5679
IHRvbg== 5680
cmVlbnM= 5681
IGhvbWVz 5682
IEV4dA== 5683
IExldmVs 5684
cmFzdA== 5685
IEly 5686
IGVsaW0= 5687
IGxhcmdlbHk= 5688
IEpvZQ== 5689
IHZvdGVz 5690
YWxscw== 5691
IGJ1c2luZXNzZXM= 5692
IEZvdW5kYXRpb24= 5693
IENlbnRyYWw= 5694
IHlhcmRz 5695
IG1hdGVyaWFscw== 5696
dWxuZXI= 5697
IGd1aWRl 5698
IGNsb3Nlcg== 5699
dW1z 5700
IHNwb3J0cw== 5701
ZWRlcg== 5702
SnVzdA== 5703
IHRheGVz 5704
ODQ= 5705
IE9sZA== 5706
IGRlY2FkZQ== 5707
b2xh 5708
IHZpcg== 5709
IGRyb3BwZWQ= 5710
IGRlbGF5 5711
aXRlY3Q= 5712
IHNlY3VyZQ== 5713
c3RlaW4= 5714
bGV2ZWw= 5715
IHRyZWF0ZWQ= 5716
IGZpbGVk 5717
YWluZQ== 5718
IHZhbg== 5719
IG1pcg== 5720
IGNvbHVtbg== 5721
aWN0ZWQ= 5722
ZXBlcg== 5723
IHJvdA== 5724
IGNvbnN1bHQ= 5725
IGVudHJ5 5726
IG1hcmlqdWFuYQ== 5727
IERvdQ== 5728
IGFwcGFyZW50bHk= 5729
b2tpbmc= 5730
Y2x1c2l2ZQ== 5731
IGluY3JlYXNlcw== 5732
YW5v 5733
IHNwZWNpZmljYWxseQ== 5734
IHRlbGU= 5735
ZW5zaW9ucw== 5736
IHJlbGlnaW9u 5737
YWJpbGl0aWVz 5738
IGZyYW1l 5739
IE5vdGU= 5740
IExlZQ== 5741
IGhlbHBpbmc= 5742
IGVkZ2U= 5743
b3N0b24= 5744
IG9yZ2FuaXphdGlvbnM= 5745
w4M= 5746
IEJvdGg= 5747
aGlwcw== 5748
IGJpZ2dlcg== 5749
IGJvb3N0 5750
IFN0YW5k 5751
IHJvdw== 5752
dWxz 5753
YWJhc2U= 5754
IHJpZA== 5755
TGV0 5756
YXJlbg== 5757
cmF2ZQ== 5758
IHN0cmV0 5759
UEQ= 5760
IHZpc2lvbg== 5761
IHdlYXJpbmc= 5762
IGFwcHJlY2k= 5763
IGF3YXJk 5764
IFVzZQ== 5765
IGZhY3Rvcg== 5766
d2Fy 5767
dWxhdGlvbnM= 5768
KSg= 5769
IGdvZA== 5770
IHRlcnJpdA== 5771
IHBhcmFt 5772
YXN0cw== 5773
ODc= 5774
IGVuZW1pZXM= 5775
IEdhbWVz 5776
RkY= 5777
IGFjY2lkZW50 5778
V2VsbA== 5779
IE1hcnRpbg== 5780
VEVS 5781
IGF0aA== 5782
IEhlbGw= 5783
IGZvcmc= 5784
IHZldGVy 5785
IE1lZGlj 5786
ZnJlZQ== 5787
IHN0YXJz 5788
IGV4cGVuc2l2ZQ== 5789
IGFjYWQ= 5790
cmF3bg== 5791
IFdoZQ== 5792
IGxvY2s= 5793
IGZvcm1hdA== 5794
IHNvbGRpZXJz 5795
c20= 5796
IGFnZW50 5797
IHJlc3BvbnNpYmlsaXR5 5798
b3Jh 5799
IFNjaWVuY2U= 5800
IHJhcGlk 5801
IHRvdWdo 5802
IEplc3Vz 5803
IGJlbGlldmVz 5804
TUw= 5805
IHdlYXI= 5806
bGV0ZQ== 5807
w4PDgg== 5808
IERyaQ== 5809
IGNvbW1pc3Npb24= 5810
IEJvYg== 5811
T2g= 5812
YXBlZA== 5813
IHdhcm0= 5814
w4PDgsODw4I= 5815
IDIwMDM= 5816
b3J0aW9u 5817
IGhhc24= 5818
dXN0ZXI= 5819
IHVuaXZlcnM= 5820
IElsbA== 5821
IGtpbmc= 5822
b2xvZ2llcw== 5823
OTQ= 5824
IFRlbQ== 5825
IE1vcw== 5826
IHBhdGllbnQ= 5827
IE1leGljbw== 5828
Y2Vhbg== 5829
IERlYXRo 5830
IFNhbmRlcnM= 5831
eW91 5832
IENhc3Q= 5833
IENvbXBhbnk= 5834
cHR5 5835
IGhhcHBlbmluZw== 5836
RlA= 5837
IEJhdHRsZQ== 5838
IGJvdWdodA== 5839
QW0= 5840
TW9k 5841
VXM= 5842
dXRlcnM= 5843
IENyZQ== 5844
IFRob3Nl 5845
IDQ0 5846
aXNlcg== 5847
IHNvdWw= 5848
IFRvcA== 5849
IEhhcnJ5 5850
IEF3 5851
IHNlYXQ= 5852
ZmZlZQ== 5853
IHJldm9sdXRpb24= 5854
ICgi 5855
IER1cmluZw== 5856
ZXR0ZQ== 5857
IHJpbmc= 5858
IG9mZmVuc2l2ZQ== 5859
IHJldHVybnM= 5860
IHZpZGVvcw== 5861
IGRpc2Ns 5862
IGZhbW91cw== 5863
ZW5jZWQ= 5864
IFNpZ24= 5865
IFJpdmVy 5866
IDMwMA== 5867
UE0= 5868
IEJ1cw== 5869
IENI 5870
IGNhbmRpZGF0ZXM= 5871
YXJkZW4= 5872
IHBlcmNlbnRhZ2U= 5873
IHZpc3VhbA== 5874
IHRoYW5r 5875
IHRyb3VibGU= 5876
bmVyZ3k= 5877
IDIwMDE= 5878
IHByb3Zl 5879
YXNoaW9u 5880
IGVuaA== 5881
IExvbmc= 5882
VU0= 5883
IGNvbm5lY3RlZA== 5884
IHBvc3NpYmlsaXR5 5885
T3Zlcg== 5886
IGV4cGVydA== 5887
IGxpYnJhcnk= 5888
YXJ0cw== 5889
IERpcmVjdG9y 5890
IGZlbGxvdw== 5891
OTI= 5892
aXJ0eQ== 5893
IGRyeQ== 5894
IHNpZ25z 5895
IExvdmU= 5896
IHF1aWV0 5897
Zm9vdA== 5898
IHB1cmU= 5899
IEh1bg== 5900
IGZpbGxlZA== 5901
cGhhcw== 5902
IEVsZWN0 5903
ZW5kbWVudA== 5904
IEV4cGw= 5905
IHVuYWJsZQ== 5906
bnM= 5907
bW8= 5908
IHZhc3Q= 5909
b2Jl 5910
IGlkZW50aWZ5 5911
YXBwaW5n 5912
IENhcm9saW5h 5913
Z3Jlc3M= 5914
IHByb3Rl 5915
IGZpc2g= 5916
IGNpcmN1bXN0YW5jZXM= 5917
cmF6eQ== 5918
IFBob3Q= 5919
IGJvZGllcw== 5920
IE11cg== 5921
IGRldmVsb3Bpbmc= 5922
IEFS 5923
IGV4cGVyaWVuY2Vk 5924
IHN1YnN0YW50 5925
IEJvYXJk 5926
ZXNvbWU= 5927
IGRvbWVzdGlj 5928
IGNvbWJpbmVk 5929
IFB1dA== 5930
IGNoZW1pY2Fs 5931
IENoaWxk 5932
IHBvb2w= 5933
IEN5 5934
IGVnZw== 5935
Y29ucw== 5936
c3RlcnM= 5937
IGh1cnQ= 5938
IG1hcmtldHM= 5939
IGNvbnNlcnZhdGl2ZQ== 5940
IHN1cHBvcnRlcnM= 5941
IGFnZW5jaWVz 5942
aWRlbA== 5943
T2I= 5944
dXJi 5945
IDQz 5946
IERlZmVuc2U= 5947
eWU= 5948
IEFw 5949
ZHVsZQ== 5950
IHRlbXBlcmF0dXJl 5951
IGNvbmR1Y3RlZA== 5952
IENoaWVm 5953
IHB1bGxlZA== 5954
IGZvbA== 5955
TGFzdA== 5956
b250bw== 5957
b3Npcw== 5958
VkVS 5959
RGVz 5960
IFBhbg== 5961
Rmlyc3Q= 5962
IGFkdmFuY2U= 5963
IGxpY2Vuc2U= 5964
cm9ycw== 5965
IEpvbg== 5966
IGltYWdpbmU= 5967
IGhlbGw= 5968
IGZpeGVk 5969
IGluY29y 5970
b3NpdGU= 5971
IExvZw== 5972
aWNrZW4= 5973
XTo= 5974
IHN1cnByaXNl 5975
aGFi 5976
IGNyYWZ0 5977
b2x0 5978
IEp1bA== 5979
IGRpYWw= 5980
IHJlbGV2YW50 5981
IGVudGVyZWQ= 5982
IGxlYWRz 5983
IEFE 5984
IENsZWFu 5985
IHBpY3R1cmVz 5986
ZXNzb3I= 5987
IGFsdA== 5988
IHBheWluZw== 5989
UGVy 5990
IE1hcmtldA== 5991
IHVwZGF0ZXM= 5992
YW1pbHk= 5993
IFR5cGU= 5994
IEhvbWU= 5995
IDU1 5996
c2VtYmx5 5997
cm9tZQ== 5998
ODM= 5999
IGdyZWF0ZXN0 6000
IGhlaWdodA== 6001
IGhlYXY= 6002
YWludHM= 6003
IGxpc3Rlbg== 6004
YXNlcg== 6005
IFNI 6006
IGNhcGFibGU= 6007
YWNsZQ== 6008
IHBlcnNwZWN0 6009
aW5hdGluZw== 6010
IG9mZmVyaW5n 6011
cnlwdA== 6012
IERldmVsb3A= 6013
YWJpbg== 6014
cmM= 6015
IGJyaWdodA== 6016
YWx0eQ== 6017
YXJyb3c= 6018
IHN1cHBs 6019
aW5kaW5n 6020
YWNrZWQ= 6021
Z3lwdA== 6022
IEFub3RoZXI= 6023
cGc= 6024
IFZpcmdpbmlh 6025
IEx1 6026
IHBsYW5uZWQ= 6027
IHBpdA== 6028
IHN3ZWV0 6029
VHlwZQ== 6030
IERp 6031
IHR5cGljYWxseQ== 6032
IEZyYW5jaXNjbw== 6033
IHByb3NwZWN0 6034
IERhbg== 6035
IHRlZW4= 6036
cmVlcw== 6037
IHNjaGVk 6038
IGhvbA== 6039
IHNjcg== 6040
IGxvdHM= 6041
bGlmZQ== 6042
IG5ld3Nw 6043
IGZvcmdldA== 6044
IE5vbmU= 6045
IE1pZGRsZQ== 6046
IFJ5YW4= 6047
ZWRk 6048
IHNldmVyZQ== 6049
IHN1aXQ= 6050
bGxlcg== 6051
OTM= 6052
IGNvcnJlc3BvbmQ= 6053
IGV4cGxvcw== 6054
dWF0aW9ucw== 6055
IGZsYWc= 6056
Z2FtZQ== 6057
cmlk 6058
IHByaW4= 6059
IERhdGE= 6060
IGRlcGxveQ== 6061
IEVudGVy 6062
c3VpdA== 6063
Z2hhbg== 6064
IE1lbg== 6065
IHRob3VnaHRz 6066
IG1hdHRlcnM= 6067
IGFkYXB0 6068
IEFyaQ== 6069
IGZpbGw= 6070
IGZvcnRo 6071
IHNhbQ== 6072
IDQx 6073
IHBheW1lbnQ= 6074
IEhvcg== 6075
IHNwcmluZw== 6076
ZHVj 6077
IGxvc2luZw== 6078
IGJyaW5naW5n 6079
Rk8= 6080
YWxh 6081
IGRpc3RyaWJ1dGlvbg== 6082
aGVyZWQ= 6083
Ym91cg== 6084
IElzcmFlbGk= 6085
b21h 6086
IGNvbWJpbmF0aW9u 6087
IHBsZW50eQ== 6088
VkU= 6089
Q2Fu 6090
IEhhdw== 6091
IHBlcm1hbg== 6092
IFNwZWNpYWw= 6093
IHRvdw== 6094
IHNlZWtpbmc= 6095
IGV4YW1wbGVz 6096
IGNsYXNzZXM= 6097
Y3I= 6098
IGJlZXI= 6099
IG1vdmVz 6100
IElQ 6101
IEtu 6102
IHBhbmVs 6103
RXZlbg== 6104
IHByb3Blcmx5 6105
IHJpcw== 6106
IHBsdWc= 6107
IGVzdGltYXRlZA== 6108
RXZlcnk= 6109
IGRlZmVuc2l2ZQ== 6110
YWdyYXBo 6111
IHByZWdu 6112
IGluc3RpdA== 6113
IFZpY3Q= 6114
IHZvbHVtZQ== 6115
IHBvc2l0aW9ucw== 6116
IGxpbmtz 6117
IFByb2dyYW0= 6118
IFdlZWs= 6119
YWd1ZXM= 6120
IHRyYW5zZm9ybQ== 6121
a2Vy 6122
IENFTw== 6123
IGNhcw== 6124
IG9wcG9uZW50 6125
IHR3ZWV0 6126
IENvZGU= 6127
IHNob3A= 6128
IGZseQ== 6129
IHRhbGtz 6130
IGJhZw== 6131
UGhvbmU= 6132
IGFpZA== 6133
IHBsYW50cw== 6134
IDY1 6135
IGF0dG9ybmV5 6136
YXJ0ZXJz 6137
cXVlc3Q= 6138
IE1hZ2lj 6139
IGJlZ2lucw== 6140
IG15c3Rlcg== 6141
IGVudmlyb25tZW50YWw= 6142
IHN0b3JhZ2U= 6143
Tk4= 6144
IG1hcmc= 6145
IHNrZQ== 6146
IG1ldGFs 6147
ZWxseQ== 6148
IG9yZGVyZWQ= 6149
IHJlbWFpbmVk 6150
IGxvdmVk 6151
IHByb21wdA== 6152
IHVwZGF0ZWQ= 6153
IGV4cGVydHM= 6154
IHdhbGtpbmc= 6155
IGFuY2llbnQ= 6156
IHBlcmZvcm1lZA== 6157
QVRF 6158
IG5laXRoZXI= 6159
aWVuY3k= 6160
IG1hbnVmYWN0dXJl 6161
IFBhaw== 6162
IHNlbGVjdGVk 6163
IG1pbmU= 6164
IHVsdGltYXRlbHk= 6165
IGV4cGxhbg== 6166
IGxhYmVs 6167
IFNlcnZpY2Vz 6168
cmlidXRlZA== 6169
VHJ1bXA= 6170
IHN5bg== 6171
IFVsdA== 6172
U0M= 6173
IG1lYXQ= 6174
IGdpYW50 6175
IFdhcnM= 6176
IE9O 6177
IGFkbQ== 6178
IGludGVycHJldA== 6179
IGV2ZW5pbmc= 6180
IGV2aWw= 6181
IEJvc3Rvbg== 6182
IFdpbGQ= 6183
IMM= 6184
IEJpdGNvaW4= 6185
IEFtYXpvbg== 6186
RHI= 6187
IEluZm9ybWF0aW9u 6188
IG9idmlvdXNseQ== 6189
IGFkdmFuY2Vk 6190
UGhvdG8= 6191
b2xhcg== 6192
IHdlYXRoZXI= 6193
IHN5bWJvbA== 6194
IHNvbGU= 6195
IHBvdGVudGlhbGx5 6196
b3N0ZXI= 6197
IG9yaWdpbmFsbHk= 6198
bXVu 6199
MzAw 6200
YXpl 6201
ZXNzaW9ucw== 6202
IGRlY2s= 6203
IHN0b29k 6204
IHlvdXRo 6205
IEJlcm4= 6206
UmVw 6207
IFRlc3Q= 6208
IGJhc2ljYWxseQ== 6209
b3RpYw== 6210
IGludm9sdmU= 6211
b2xpdA== 6212
bHlu 6213
U2Vl 6214
IGFpcmNyYWZ0 6215
IGNvbmZpcm0= 6216
RVc= 6217
IG1lc3NhZ2Vz 6218
IFJpY2hhcmQ= 6219
IGtpdA== 6220
IHByb2hpYg== 6221
IHZ1bG5lcg== 6222
aXN0ZXJz 6223
IGV4aXN0ZW5jZQ== 6224
IHR1cm5pbmc= 6225
IFNQ 6226
IGRlc2lyZQ== 6227
IGZsYXQ= 6228
IG1lbnQ= 6229
c2Vhc29u 6230
YW5nZXM= 6231
IG5laWdoYm9yaG9vZA== 6232
IExha2U= 6233
QVRJT04= 6234
IHBvaW50ZWQ= 6235
YnVy 6236
IGlubm92 6237
dWNrcw== 6238
VUw= 6239
IHByb2Zlc3Nvcg== 6240
IGV4cHJlc3NlZA== 6241
QUI= 6242
aWNpb3Vz 6243
IDIwMDI= 6244
IERldg== 6245
IHNlc3Npb24= 6246
IGJhcmU= 6247
c2Vu 6248
IGRpc3M= 6249
IENhdGg= 6250
IFBhc3M= 6251
IFBvaW50 6252
IGRvY3Rvcg== 6253
b3Jyb3c= 6254
YWlsZWQ= 6255
IFJ1Yg== 6256
IERD 6257
IENoYXJs 6258
cGVyc29u 6259
IHdyaXRlcg== 6260
aWdodGVycw== 6261
dXJlYXU= 6262
IG9ibGln 6263
IHJlY29yZGVk 6264
IGJyb2tl 6265
IG9yZGVycw== 6266
aWx0eQ== 6267
IG1vdGlvbg== 6268
aW5pdHk= 6269
bGF3 6270
YWRpdW0= 6271
IGltbWlncmF0aW9u 6272
IGNvbnRyYXN0 6273
IGJhdHQ= 6274
IGV4Y2VsbGVudA== 6275
IHRlY2huaWNhbA== 6276
YW1p 6277
IHR1bg== 6278
IGNsb3Vk 6279
IFllYXI= 6280
Z2Vvbg== 6281
IGNyZWF0aW9u 6282
IHN0cmFuZ2U= 6283
IGF1dGg= 6284
IGZvcnQ= 6285
Ym9ybg== 6286
IGV4dGVudA== 6287
IFRvZGF5 6288
IENsdWI= 6289
IHJhaW4= 6290
IHNhbXBsZQ== 6291
IGFjY2VwdGVk 6292
IHRhY3Q= 6293
IGZpcmVk 6294
IFNvbg== 6295
IHN0YW5kcw== 6296
IGJvb3Q= 6297
IDQ3 6298
IHN0YXRlbWVudHM= 6299
IHZlcnNpb25z 6300
IHNlbGxpbmc= 6301
b3VuZGVk 6302
IDE5OTA= 6303
IHdlcmVu 6304
IFdhdGNo 6305
IGV4cGVyaW1lbnQ= 6306
UG9zdA== 6307
IHJldGFpbA== 6308
dWxlZA== 6309
SW5zdA== 6310
dW50ZQ== 6311
44O8 6312
IGRlcGFydA== 6313
IGJvbmQ= 6314
aXZlcnk= 6315
b21wbA== 6316
IHJlYWN0aW9u 6317
IFN5cmlhbg== 6318
IFBhYw== 6319
YXBwZWQ= 6320
YW5pZWw= 6321
RFA= 6322
IHJlc29sdXRpb24= 6323
IHJlYWN0 6324
IGFwcHJvdmVk 6325
b25vbQ== 6326
bW9uZA== 6327
IE9mZmlj 6328
LS0t 6329
IHJlcGxhY2U= 6330
IHRhY2s= 6331
IHNwb3J0 6332
IGNoYWlu 6333
IGVtZXJnZW5jeQ== 6334
cmFk 6335
IFBhbGVzdGlu 6336
IDQ2 6337
IGF1dG9tYXRpY2FsbHk= 6338
IHJvdXRl 6339
IHBhbA== 6340
IGJhbmtz 6341
IFBhcmlz 6342
IE1lZGlh 6343
cm9hZA== 6344
aWNpbmc= 6345
aXh0 6346
aXN0ZWQ= 6347
IGdyZXc= 6348
IGNvb3Jk 6349
IFdoZXJl 6350
b21pbg== 6351
IHN1YnM= 6352
77+977+9 6353
IMKx 6354
IGNvcnBvcmF0ZQ== 6355
IHNlbGVjdGlvbg== 6356
bm9vbg== 6357
IFJlcG9ydA== 6358
Y3M= 6359
Y2x1ZGluZw== 6360
b3JkZXJz 6361
YW5jaGU= 6362
IEl0cw== 6363
IHNsb3dseQ== 6364
IEVneXB0 6365
IEFjYw== 6366
IGNvbGxl 6367
aXF1ZXM= 6368
RVg= 6369
IGF0dGVtcHRz 6370
dXJs 6371
IENyb3Nz 6372
IGZpbmRpbmdz 6373
IFND 6374
IE9S 6375
IGluZGV4 6376
ZW5zaXR5 6377
IFdheQ== 6378
IExhbmQ= 6379
IHNob2Nr 6380
ZGlz 6381
IGR5bmFt 6382
IGNhcnQ= 6383
bW9zcA== 6384
U2luY2U= 6385
aWVzdA== 6386
IEJveQ== 6387
IHN0b3Jt 6388
IENvbnRpbg== 6389
MjAxMw== 6390
aGV3 6391
aWxpdA== 6392
IGVzc2VudGlhbA== 6393
aXF1aWQ= 6394
T3RoZXI= 6395
aXZlcmVk 6396
IHJlYXNvbmFibGU= 6397
QWN0 6398
IHN1YnNlcXU= 6399
IFBhY2s= 6400
IEZvcnQ= 6401
IGNvbnNpZGVyaW5n 6402
IHVuaXZlcnNpdHk= 6403
bG9n 6404
IG1hcnJpZWQ= 6405
IGlsbHVzdA== 6406
IFRydWU= 6407
o48= 6408
IG51bWVyb3Vz 6409
cmFzdHJ1Y3R1cmU= 6410
IHNlcmlvdXNseQ== 6411
IHJlZmVycmVk 6412
dWE= 6413
IGNvbnNpc3RlbnQ= 6414
b25uYQ== 6415
IFJlYWw= 6416
cnVwdGlvbg== 6417
Y2lwbGVz 6418
IGZhY3Rz 6419
OTE= 6420
b3Rlcw== 6421
ZXJn 6422
VGhlbg== 6423
IGFjY29tcGw= 6424
Tm90ZQ== 6425
IHJldmVudWU= 6426
IHBhc3Npbmc= 6427
IG1hbA== 6428
ZWVu 6429
IFlldA== 6430
IGdhdGhlcg== 6431
dGVyZGF5 6432
ZXdvcms= 6433
IEF1dGhvcg== 6434
UGU= 6435
IG9wdGlt 6436
IHJ1Yg== 6437
IOijjw== 6438
IHVua25vd24= 6439
c3RvbmU= 6440
IHVuaW9u 6441
b2x2ZQ== 6442
IG9wcG9ydHVuaXRpZXM= 6443
IGJyb3dzZXI= 6444
IFdhbA== 6445
IENvc3Q= 6446
IHJlcG9ydGluZw== 6447
c3Rz 6448
cGV0 6449
IHNhbmQ= 6450
IHN1ZGRlbmx5 6451
IHN1cnByaXNpbmc= 6452
IFZS 6453
IHNvbWV3aGF0 6454
IEJhcw== 6455
dWx0dXJl 6456
aXp6 6457
IENE 6458
IGNoYWxsZW5nZXM= 6459
IHNldHRpbmdz 6460
IGV4cGVyaWVuY2Vz 6461
IEZ1bGw= 6462
IGNhbm4= 6463
IHJlY2VpdmluZw== 6464
RVNU 6465
IGpvaW50 6466
IGN1bHR1cmFs 6467
IGFzdA== 6468
ODI= 6469
YXN0ZXJu 6470
Y2VpdmVk 6471
IENydQ== 6472
IGJ1bGw= 6473
cGlyZWQ= 6474
YW1t 6475
IGZhY2luZw== 6476
cG93ZXI= 6477
IGJvc3M= 6478
IEhvbA== 6479
IGluc3Ry 6480
IGluY3JlYXNpbmdseQ== 6481
IHNoaWZ0 6482
IHN0cmVldHM= 6483
IFdpbGxpYW1z 6484
YWJi 6485
IGxpZQ== 6486
IGxhdWdo 6487
IENh 6488
UEw= 6489
IGFkdWx0cw== 6490
IGN1c3RvbWVy 6491
IG9idGFpbmVk 6492
IHN1cHBvcnRpbmc= 6493
aHRtbA== 6494
ZmlyZQ== 6495
IGRldGFpbGVk 6496
IHBpY2tlZA== 6497
IFJpZ2h0 6498
bGRlcg== 6499
RUU= 6500
c3Rvb2Q= 6501
IEtpbQ== 6502
IHdpcmU= 6503
IHNpZ2h0 6504
IGRldmVsb3BlcnM= 6505
IHBlcnNvbnM= 6506
IHNhZA== 6507
IGN1cA== 6508
IHdhcm5pbmc= 6509
IGJveXM= 6510
bG9uZw== 6511
IGJpcmQ= 6512
Zm8= 6513
IHdhbA== 6514
IG9ic2VydmVk 6515
IHpvbmU= 6516
aXZlbmVzcw== 6517
IGNoYW5uZWw= 6518
Y3JpcHQ= 6519
IHJlZnVzZWQ= 6520
IEFnYWlu 6521
IHN1Yw== 6522
IHNwb2tlc21hbg== 6523
IFJlZg== 6524
cml0ZQ== 6525
b3VzdG9u 6526
44Oz 6527
IFNoZXI= 6528
IGFjdHM= 6529
IE5hbWU= 6530
IHN0cnVnZ2xl 6531
YXJyeQ== 6532
b21ldGltZXM= 6533
IGRpc2NyaW0= 6534
SFQ= 6535
IGNhdGVnb3J5 6536
IHJlYWxpemU= 6537
IGVtcGxveWVl 6538
IEFmZ2hhbg== 6539
ZW5nZXI= 6540
IGd1bnM= 6541
IFN0ZXZl 6542
IE1vdA== 6543
IE9s 6544
b2tlZA== 6545
IHRoaWNr 6546
IGZhaXJseQ== 6547
aWxseQ== 6548
IHN1cnZl 6549
IE1hdA== 6550
d2VpZ2h0 6551
4pQ= 6552
IHRyb29wcw== 6553
IGFnZW50cw== 6554
IGJhdHRlcnk= 6555
IG1vdGl2 6556
w6E= 6557
U2Vj 6558
ZGVu 6559
b3Zlcnk= 6560
TFM= 6561
IGZsdQ== 6562
IGNvbmZpZGVudA== 6563
IE9wZXI= 6564
IGVtcHR5 6565
IHBoZW4= 6566
IHNlY3Rvcg== 6567
IGV4Y2l0ZWQ= 6568
IHJlbW90ZQ== 6569
YXBo 6570
b2Vu 6571
IGRlc3Ryb3llZA== 6572
IG1vcmFs 6573
IEhQ 6574
IFJvbg== 6575
IGRyZXNz 6576
IEJhdA== 6577
IGxpdA== 6578
IE1T 6579
IGFm 6580
SEw= 6581
cnVt 6582
aXNtcw== 6583
IHNob3VsZG4= 6584
IHN5bXB0 6585
IFRvcm9udG8= 6586
aGV0aWM= 6587
IGNhcmJvbg== 6588
IGluc3RhbGxlZA== 6589
IHZpb2xlbnQ= 6590
IHNvbGFy 6591
amE= 6592
IHByYWN0aWNlcw== 6593
IHJpZGU= 6594
IFBlbm4= 6595
IGltcHJvdmVk 6596
IGF1ZGlv 6597
IGJlaGF2aQ== 6598
IFBT 6599
IGVhdGluZw== 6600
RGF0YQ== 6601
IFJldmlldw== 6602
cGFzcw== 6603
Y2xhaW0= 6604
dWF0ZWQ= 6605
YW5nZXJz 6606
Y2hlbg== 6607
IHByb3BlcnRpZXM= 6608
IGFueXdoZXJl 6609
QW5vdGhlcg== 6610
IGJsb3c= 6611
IEphY2tzb24= 6612
IHByb3Vk 6613
IHBsYW5l 6614
bGluZXM= 6615
IHNxdWFyZQ== 6616
IHByb29m 6617
YW5zYXM= 6618
IHRhbGtlZA== 6619
bWFrZXJz 6620
IHNpc3Rlcg== 6621
IGhvbGRz 6622
IHJlc2lkZW50 6623
ID09 6624
IHJlc2lzdGFuY2U= 6625
IHNwbGl0 6626
IHByb3NlY3V0 6627
IGNvbmZpZGVuY2U= 6628
cmVzZW50cw== 6629
IGN1dHM= 6630
IGV4Y2VwdGlvbg== 6631
IHplcm8= 6632
R2V0dHk= 6633
IGNvcHlyaWdodA== 6634
IHRvdGFsbHk= 6635
b3JtYWw= 6636
aWZpY2F0aW9ucw== 6637
IEF1c3RyYWxpYW4= 6638
IHNpY2s= 6639
IDE1MA== 6640
IGhvdXNlaG9sZA== 6641
IGZlZXM= 6642
IGRyaXZlcnM= 6643
b2dlbg== 6644
IE5Z 6645
IG5lY2Vzc2FyaWx5 6646
IHJlZ3VsYXRpb25z 6647
ZWFyaW5n 6648
c2w= 6649
IHBlcnNwZWN0aXZl 6650
Y2FyZQ== 6651
aWNpYWw= 6652
SGlz 6653
IGVzY2FwZQ== 6654
IHN1cnByaXNlZA== 6655
IFZhbg== 6656
dXJyZW50 6657
IHZhYw== 6658
ODE= 6659
IFRodXM= 6660
IGVtcGhhcw== 6661
IENoYW1waW9ucw== 6662
IEljZQ== 6663
IG5hcnI= 6664
IGhlYWRz 6665
IGNhdXNpbmc= 6666
YmVs 6667
Zm9ydHVuYXRlbHk= 6668
IE1h 6669
IHRhcmdldHM= 6670
Y2lwbA== 6671
IGFmdGVybm9vbg== 6672
IGFkZHM= 6673
IE1heWJl 6674
IEZvdXI= 6675
ZXNzZWQ= 6676
cGxldGU= 6677
IHVzdWFs 6678
Y2hv 6679
aW5ndQ== 6680
IHdpdGhk 6681
IEVuZXJneQ== 6682
IEVjb25vbQ== 6683
T08= 6684
IGFydGljbGVz 6685
IGluanVyZWQ= 6686
IG1hbmFnZQ== 6687
IGV4cGxhaW5z 6688
IGRpYWdu 6689
UmVj 6690
YXR1cmVz 6691
IGxpbmtlZA== 6692
IGRpc2N1c3NlZA== 6693
IGV4cGxv 6694
IG9jY2FzaW9u 6695
YXRoYW4= 6696
IG9wcG9zaXRl 6697
IGZhY2Vz 6698
IGRlbmllZA== 6699
IEtuaWdodA== 6700
IG51dA== 6701
IGFwcHJveGltYXRlbHk= 6702
IGRpc2FwcG9pbnQ= 6703
b255bW91cw== 6704
IEJlc3Q= 6705
IExv 6706
IEh5 6707
IEFmZg== 6708
IHZvdGluZw== 6709
YW53aGlsZQ== 6710
IElJSQ== 6711
IGluc3RpdHV0aW9ucw== 6712
YWdyYW0= 6713
IERhaWx5 6714
IGRyYWc= 6715
IG5lYXJieQ== 6716
IGd1aWx0eQ== 6717
IGNvbnZlcg== 6718
UHJl 6719
c2hpcA== 6720
IHJld2FyZA== 6721
IHBoaWxvc29waA== 6722
IFNT 6723
dWdo 6724
IGFwcHM= 6725
ZnJpZW5k 6726
IHVwcGVy 6727
IGFkdmVydA== 6728
IHNub3c= 6729
IGZydXN0 6730
IG91cnNlbHZlcw== 6731
RnI= 6732
IERpZQ== 6733
YW1waW9u 6734
IGRpc21pc3M= 6735
IGNlcmU= 6736
IHNpZ25hbA== 6737
ZnJvbQ== 6738
ICku 6739
IDUy 6740
IGNyaW1lcw== 6741
aXRvcnM= 6742
ZXN0aXZhbA== 6743
dXNldW0= 6744
IGNvdW5jaWw= 6745
IFNhdWQ= 6746
TWF5 6747
IEd1bg== 6748
aWNpYW4= 6749
ZXRoZXI= 6750
IHN1ZmZpY2llbnQ= 6751
IEhlbg== 6752
c29sZQ== 6753
IGhpc3RvcmljYWw= 6754
IEZhcg== 6755
IFR1cm4= 6756
IHBpbg== 6757
IHN1Y2NlZWQ= 6758
bWF0 6759
bHltcA== 6760
IHRyYWRpdGlvbg== 6761
IE9r 6762
IGNybw== 6763
IGRlc2NyaXB0aW9u 6764
YWxsZQ== 6765
IHNreQ== 6766
VGU= 6767
IHdpZGVseQ== 6768
IHdhdmU= 6769
IGRlZmluaXRpb24= 6770
IEpld3M= 6771
IGN5Y2xl 6772
IHJlZmVyZQ== 6773
IGJyaW5ncw== 6774
dXNhbA== 6775
IGFsaXZl 6776
IGZyZXF1ZW50bHk= 6777
IGludGVudGlvbg== 6778
IENvbnRyb2w= 6779
bHY= 6780
eXN0ZW0= 6781
IHByaXZhY3k= 6782
Z2VudA== 6783
cmVuY2U= 6784
IFF1ZXN0 6785
IENocmlzdG1hcw== 6786
IHJhaWw= 6787
IGNvb3Blcg== 6788
IHRlc3RlZA== 6789
IENhcHQ= 6790
YXNrcw== 6791
IGNvbWZvcnRhYmxl 6792
IGRlbGl2ZXJlZA== 6793
c2NhcGU= 6794
IGRlcHRo 6795
IEdPUA== 6796
IHdyaXRlcw== 6797
IGFzc2V0cw== 6798
IHNhdg== 6799
aW1lbnRz 6800
IHRyYW5zaXRpb24= 6801
IGFydGlzdA== 6802
IExvb2s= 6803
IGxvYg== 6804
IGNvbXBvbmVudHM= 6805
YXJpdHk= 6806
IHdhbGtlZA== 6807
IHJvb3Q= 6808
IHBhcnRpY2lwYW50cw== 6809
IG5vdGljZWQ= 6810
IHJlc2M= 6811
IG5hdg== 6812
IEFkbWluaXN0 6813
ZGE= 6814
dXRyYWw= 6815
cGxhdGU= 6816
IGltcG9ydGFuY2U= 6817
IGFzc2VydA== 6818
aW91c2x5 6819
Y3JpcHRpb24= 6820
IGluanVyaWVz 6821
IENoZWNr 6822
IHJlZ2lzdGVyZWQ= 6823
IGludGVudA== 6824
IG1pc3NlZA== 6825
b2dyYXBoaWM= 6826
IHNlbnRlbmNl 6827
b3VudGVy 6828
IGFzc2lzdGFuY2U= 6829
ZXZpbg== 6830
IGRhdGFiYXNl 6831
IGJ1aWxkaW5ncw== 6832
IGNsYXNzaWM= 6833
IHRoaW5rcw== 6834
IE9oaW8= 6835
UHI= 6836
dWdn 6837
IGZlZQ== 6838
cGFu 6839
IGVmZmVjdGl2ZWx5 6840
IGZhY2lsaXR5 6841
IGJlYXI= 6842
IGNoYXB0ZXI= 6843
IGRvZ3M= 6844
IENvbHVtYg== 6845
IGxhdHRlcg== 6846
aXRpYWw= 6847
IGFkbWl0dGVk 6848
VFY= 6849
IEdlb3Jn 6850
IHBvc3Rz 6851
XFw= 6852
IGxhd3llcg== 6853
IGVxdWl2YWw= 6854
IG1hbmQ= 6855
IGNvbnRyb2xsZWQ= 6856
IFdhbGs= 6857
IEFuZHJldw== 6858
IG1lbnU= 6859
YW1lbnRhbA== 6860
IHByb3RlY3RlZA== 6861
dmE= 6862
IGFkbWluaXN0cg== 6863
b3JhbA== 6864
IHJlaW4= 6865
IFNhcg== 6866
IGFtb3VudHM= 6867
IG5hdGl2ZQ== 6868
IE1vb24= 6869
IHJlcHJlc2VudHM= 6870
IGFiYW5kb24= 6871
IGNhcnJ5aW5n 6872
IHRhbms= 6873
bWFyeQ== 6874
IGRlY2xhcmVk 6875
VHViZQ== 6876
IGhhdA== 6877
IHB1bmlzaA== 6878
ZWxsZWN0 6879
bWVz 6880
IHVuaXZlcnNl 6881
IFJvZA== 6882
cGh5 6883
IGluZnJhc3RydWN0dXJl 6884
IDUx 6885
IG9wcG9zZWQ= 6886
b3dudA== 6887
Y2E= 6888
IE1ha2U= 6889
IGhhcmR3YXJl 6890
IGNvZmZlZQ== 6891
UmVs 6892
YmFs 6893
d29ybGQ= 6894
IFNhZg== 6895
IFNlYQ== 6896
aW5hbHM= 6897
IG93bmVk 6898
IGhhbGw= 6899
ZXJzaW9u 6900
IGRlc2NyaWJl 6901
IFBvdA== 6902
IHBvcnRpb24= 6903
IGF0bW9zcA== 6904
IGdvdmVybm1lbnRz 6905
IGRlcGVuZGluZw== 6906
IG9mZmVuc2U= 6907
IHRyaWNr 6908
YXdh 6909
IExpbmU= 6910
IFZpcw== 6911
IEhhcmQ= 6912
IE9yaWc= 6913
IENsaWNr 6914
IGRlc2s= 6915
IFZhbGxleQ== 6916
IFNvdg== 6917
IG1vdmllcw== 6918
IHJlbWFyaw== 6919
IG1haWw= 6920
IGNvbnNjaW91cw== 6921
IHJ1bGluZw== 6922
IFJpZ2h0cw== 6923
IG1lZGlj 6924
aGVudA== 6925
IFdvbWVu 6926
Pjw= 6927
IHJlcGxhY2Vk 6928
IFByZW0= 6929
IFRoYW5rcw== 6930
IHJlbmV3 6931
IEJhbGw= 6932
aWZvcm0= 6933
IHNob3Rz 6934
Q29tbQ== 6935
IGFybWVk 6936
IGNvbnN0YW50 6937
IHRhc3Rl 6938
IHJlYWxpemVk 6939
IGJ1ZmY= 6940
IG1v 6941
IGVmZmljaWVudA== 6942
TW9zdA== 6943
b3JhdGlvbg== 6944
aWZpZXM= 6945
IGNvbW11bmljYXRpb24= 6946
IGZsb29k 6947
IGNvbnNlcXVlbmNlcw== 6948
IGFueXdheQ== 6949
aWdn 6950
IEdN 6951
IFRoYW5r 6952
IGlyb24= 6953
IGV2b2x1dGlvbg== 6954
IENvcA== 6955
dHdpdHRlcg== 6956
IDk1 6957
IHJlbGF0aW9uc2hpcHM= 6958
YWRlbA== 6959
IFlvdW5n 6960
IHByb3Bvc2Fs 6961
YXllcnM= 6962
dWlsZGluZw== 6963
IEhvdA== 6964
T1JF 6965
Y29z 6966
IGNvbGxhYm9y 6967
UEc= 6968
YXh5 6969
IGtub3dpbmc= 6970
IHN1cHBvcnRz 6971
b3dlZA== 6972
IGNvbnRyb2xz 6973
IG1lcmVseQ== 6974
dW1lcg== 6975
IGF0aGxldA== 6976
IGZhc2hpb24= 6977
cGF0aA== 6978
IGdpZnQ= 6979
IGVyYQ== 6980
QU5E 6981
IGtpbmRz 6982
IEtvcmVhbg== 6983
IGxlZ2l0 6984
dWxvdXM= 6985
IGVzc2VudGlhbGx5 6986
IHRoZXJhcA== 6987
bmlj 6988
IHN1ZmZlcmVk 6989
IGh1cg== 6990
IHByb21pc2U= 6991
IGV4Y2Vzcw== 6992
IG92ZXJ3 6993
IHByaW1l 6994
IEhvdXN0b24= 6995
ZXJyeQ== 6996
IE1z 6997
UlM= 6998
MjAxMg== 6999
IHN0b3Jlcw== 7000
IE9seW1w 7001
IGpvdXJuZXk= 7002
QWx0aG91Z2g= 7003
U3Vi 7004
IEVkdWM= 7005
IENoYXB0ZXI= 7006
IHJlcXVlc3Rz 7007
IGNvbnN1bWVycw== 7008
IHRpbnk= 7009
IGlzb2w= 7010
IEZhaXI= 7011
YmE= 7012
IFlPVQ== 7013
IGNyYXNo 7014
Y2VsZXI= 7015
IGVtb3Rpb25hbA== 7016
IGdvb2Rz 7017
IGVsZWN0ZWQ= 7018
IG1vZGVy 7019
IExpbnV4 7020
IGJsb2Nrcw== 7021
IGlzbGFuZA== 7022
IFNvY2lldHk= 7023
IGVsZWN0aW9ucw== 7024
IGJyb2FkY2FzdA== 7025
IGNoZWFw 7026
IG5hdGlvbnM= 7027
IHNlYXNvbnM= 7028
NDAw 7029
IHdhc3Rl 7030
IFNhdA== 7031
IGZpZWxkcw== 7032
ZW1wbG95 7033
IHByb2ZpbGU= 7034
IGF1dGhvcnM= 7035
QUxM 7036
IEdyYQ== 7037
d2VzdA== 7038
IFR5 7039
IGRlYXRocw== 7040
IHZhY2M= 7041
IGZvcm1lZA== 7042
IGR1 7043
IG9uZ29pbmc= 7044
IE11c2xpbXM= 7045
ZWxm 7046
aWd1cmU= 7047
IGFzc3VtZQ== 7048
IFVrcmFpbmU= 7049
d2F0ZXI= 7050
IGNvYXN0 7051
IHZvdGVk 7052
Z29y 7053
IEFT 7054
IE1pY2hpZ2Fu 7055
YXph 7056
IEFybQ== 7057
aXJv 7058
IGZsZXg= 7059
YXN0ZXJz 7060
Jyc= 7061
IHdlbGNvbWU= 7062
YXJs 7063
IGxvY2F0aW9ucw== 7064
aWdhdGlvbg== 7065
IEZpbA== 7066
IGJ1eWluZw== 7067
IGFyY2hpdGVjdA== 7068
IGhhcmRlcg== 7069
IEN1Yg== 7070
IGludGVyZmFjZQ== 7071
IHJlc3RhdXJhbnQ= 7072
IGRpc2NvdmVy 7073
IGV4Y2VlZA== 7074
IGZhdm91cg== 7075
Z2VyeQ== 7076
IGR1dHk= 7077
IHBpdGNo 7078
YWRvcg== 7079
IE1hY2g= 7080
Ym95 7081
IHJlc3BvbmRlZA== 7082
IGV4dGVuZGVk 7083
aGVycw== 7084
TWFueQ== 7085
cmFpZA== 7086
aWZlcg== 7087
IElucw== 7088
U2Vy 7089
IG1lZGl1bQ== 7090
c2hl 7091
IFNwb3J0cw== 7092
IG1hZ2F6aW5l 7093
dXRhdGlvbg== 7094
IGxpbWl0cw== 7095
IEdhbGw= 7096
IGV4dGVybmFs 7097
cmF6aWw= 7098
IHlvdW5nZXI= 7099
dGxl 7100
IHJlbWluZA== 7101
IENPTg== 7102
IGltbWVkaWF0ZQ== 7103
IGhpZGRlbg== 7104
IHZvbHVudGU= 7105
IHNpbXBs 7106
b2RjYXN0 7107
IHBoYXNl 7108
ZHI= 7109
IHBsb3Q= 7110
IGV4cG9zdXJl 7111
Ukk= 7112
b2dyYXA= 7113
dmlu 7114
YW5pc2g= 7115
IEFjYWQ= 7116
IEVuZ2luZQ== 7117
IGV4cGFuc2lvbg== 7118
IFBheQ== 7119
WW91cg== 7120
IHB1c2hlZA== 7121
IEVsbA== 7122
IEhlYWQ= 7123
IG1hcmtldGluZw== 7124
IEFD 7125
a2V0 7126
IGhpdHM= 7127
IGdybw== 7128
IEFnZQ== 7129
IFNjb3Q= 7130
XVs= 7131
IHN0aW0= 7132
IGlQaG9uZQ== 7133
iJI= 7134
IG5hcnJvdw== 7135
IEdldHR5 7136
IFR1cmtleQ== 7137
IHBlcmZlY3RseQ== 7138
IGVuYWJsZQ== 7139
dXRjaA== 7140
IHByZWNpc2U= 7141
IHJlZ2ltZQ== 7142
IHNoaWY= 7143
IGNvbXBlbnM= 7144
Z3Vu 7145
ZGl2 7146
IGNob3Nlbg== 7147
IEtlbg== 7148
QW55 7149
IHRyZWVz 7150
IHJlY29tbWVuZGVk 7151
IFJlbg== 7152
dWFibGU= 7153
IEhU 7154
Rm9sbG93 7155
RUc= 7156
IEhhbmQ= 7157
IEtlbm4= 7158
IGFyZ3VtZW50cw== 7159
IGV4aXN0cw== 7160
IGJpa2U= 7161
IENvbnNlcnY= 7162
IGJyZWFraW5n 7163
IEdhcg== 7164
IGNyYXp5 7165
IHZpcnR1YWw= 7166
YXlsb3I= 7167
aXhlbA== 7168
IDE5ODA= 7169
IHBlcm1pc3Npb24= 7170
IFNlcmllcw== 7171
IGNvbnN1bWVy 7172
IGNsb3NlbHk= 7173
Y2FsbGVk 7174
IDU0 7175
IGhvcGVz 7176
IGFycmF5 7177
IFdpbg== 7178
IExhYm91cg== 7179
IHNwb25z 7180
IElyZQ== 7181
IHBvdw== 7182
IHJlYWRlcnM= 7183
IGVtcGxveW1lbnQ= 7184
IGNyZWF0dXJl 7185
IHJlc3VsdGluZw== 7186
IGFjY3VyYXRl 7187
IG1vbWVudHM= 7188
IGFyZ3VlZA== 7189
IHBlZA== 7190
RHVyaW5n 7191
IDUz 7192
IFRhbA== 7193
IHNvdWdodA== 7194
IHN1ZmZlcmluZw== 7195
IGljb24= 7196
bGVl 7197
ICgk 7198
YWxpYW4= 7199
wrA= 7200
IHByYQ== 7201
IGJvbnVz 7202
KCI= 7203
a28= 7204
IGFjdGluZw== 7205
REU= 7206
ZmFsbA== 7207
IGNvbXBhcmlzb24= 7208
IHNtb290aA== 7209
IE5BUw== 7210
dXBw 7211
IEpvc2VwaA== 7212
ZXBpbmc= 7213
IFRha2U= 7214
IE1pZA== 7215
IHNlbmRpbmc= 7216
ZmFzdA== 7217
IEZhbGw= 7218
IGRlYWxpbmc= 7219
dXNlcg== 7220
IE9yZ2Fu 7221
Q28= 7222
IGF0dGFjaGVk 7223
IHNlZXM= 7224
JS4= 7225
IHR5cGljYWw= 7226
QVJU 7227
IGZpbmRz 7228
IEFzaWE= 7229
dW1pbg== 7230
IENvcmU= 7231
IEVudA== 7232
aW5lbnQ= 7233
dWNl 7234
IEJsb29k 7235
IE5ldmVy 7236
IGVtYWlscw== 7237
IGhpZ2hsaWdodA== 7238
IGNvbmZyb250 7239
YXR1cw== 7240
dXRlZA== 7241
IHVudXM= 7242
IHRvcGlj 7243
IEFkYW0= 7244
IGJsZQ== 7245
YXRp 7246
IHVuZGVyc3Rvb2Q= 7247
U2V0 7248
c3RydWN0 7249
VFA= 7250
IG1vYg== 7251
YWE= 7252
IFN0YXJ0 7253
cGVjdGVk 7254
c2VsbA== 7255
IGRlZGljYXRlZA== 7256
IENB 7257
dWFu 7258
IHNvbmdz 7259
ZXNjcmlwdGlvbg== 7260
IHRlY2g= 7261
IHJhcGU= 7262
IGFzaWRl 7263
IGdyYW50 7264
IDU2 7265
c3Vi 7266
IGFyZ3Vl 7267
IGNvbnRhaW5pbmc= 7268
IHNjaGVkdWxl 7269
IGxpYmVyYWw= 7270
IHB1YmxpY2x5 7271
IGhlYXZpbHk= 7272
IFV0 7273
aW5lcg== 7274
IFNlY3Rpb24= 7275
IENhcmU= 7276
d2VldA== 7277
bHM= 7278
RGlz 7279
4pSA 7280
IEZvbGxvdw== 7281
QmFjaw== 7282
IElU 7283
IGJlcw== 7284
amk= 7285
IEhpdA== 7286
ZXN0ZWQ= 7287
IGV2ZXJ5Ym9keQ== 7288
IFN3ZWQ= 7289
IGZlbWlu 7290
IGZhY2lsaXRpZXM= 7291
IGNvbnZlbg== 7292
Q29tcA== 7293
IE9T 7294
Y29yZQ== 7295
IGFueA== 7296
IGRpdmlzaW9u 7297
IENhbQ== 7298
IFN0YW4= 7299
bWF0ZXM= 7300
IGV4cGxvcmU= 7301
cGxvbQ== 7302
IHNoYXJlcw== 7303
cGxvYWQ= 7304
YW5lcw== 7305
IGlkZWFs 7306
ZXRlcnM= 7307
IEJhc2U= 7308
IHBsYXN0aWM= 7309
IGRpc3RpbmN0 7310
IE5ldHdvcms= 7311
IFNlYXR0bGU= 7312
IHRyYWRpbmc= 7313
ZW5zdXM= 7314
aW50ZW5k 7315
IGV4aGli 7316
IGluaXRpYWxseQ== 7317
IEZvb2Q= 7318
IHRob3VzYW5k 7319
IEJ1c2luZXNz 7320
YWN0ZXI= 7321
IHBhcmFncmFwaA== 7322
IHJvdWdobHk= 7323
IHd3dw== 7324
IGNyZWF0aXZl 7325
IENvbmY= 7326
IGNvbnN1bXB0aW9u 7327
IGZpbG1z 7328
YWdhbg== 7329
IG9idGFpbg== 7330
IHRhbGw= 7331
IHRvcg== 7332
IGFja25vd2xlZA== 7333
IGdyb3du 7334
YWxv 7335
S0U= 7336
IDQwMA== 7337
ZW5kZXJz 7338
dGFpbmluZw== 7339
VUc= 7340
IHN1aWNpZGU= 7341
IHdhdGNoZWQ= 7342
IExpc3Q= 7343
YWxp 7344
cmVoZW5z 7345
IHN1cnJvdW5kaW5n 7346
IHBpcA== 7347
IGZseWluZw== 7348
IEphdmE= 7349
b3JkYW4= 7350
IHNlcnZpbmc= 7351
aW5hdGlvbnM= 7352
cG9zdA== 7353
IHNobw== 7354
QXY= 7355
IGphaWw= 7356
enk= 7357
IDE5OTk= 7358
IDwv 7359
IGxpdGVyYWxseQ== 7360
IFNpcg== 7361
IGV4cG9zZWQ= 7362
IGxpZXM= 7363
c3Rhcg== 7364
IGJhdA== 7365
IGVhcm5lZA== 7366
IERpZw== 7367
IHNwZWNpZmllZA== 7368
IFNlYXNvbg== 7369
IGRlZ3JlZXM= 7370
RG9uYWxk 7371
IGNlbnRyZQ== 7372
IHNoYXJpbmc= 7373
IHdpbnRlcg== 7374
IENP 7375
Q2hl 7376
IM4= 7377
TVA= 7378
IHVudw== 7379
IGZld2Vy 7380
IE1pcg== 7381
IHNvbWV3aGVyZQ== 7382
IEtleQ== 7383
IGF0dGFja2Vk 7384
IEtpcg== 7385
IGRvbWFpbg== 7386
IHN0cm9uZ2Vy 7387
IDk5 7388
IHBlbmFsdHk= 7389
SWQ= 7390
U2NyaXB0 7391
IGRlY2xpbmVk 7392
IG5lY2s= 7393
IGZyYXVk 7394
IGN1cnJlbmN5 7395
IHJpc2luZw== 7396
UkM= 7397
4oCm4oCm 7398
SHo= 7399
IHRhYg== 7400
IHRhbGVudA== 7401
bmFt 7402
IE5CQQ== 7403
IHZpbGxhZ2U= 7404
IGxlZ3M= 7405
IE5leHQ= 7406
RWQ= 7407
IGFjaWQ= 7408
IGh5ZA== 7409
ODAw 7410
IGludm9sdmluZw== 7411
IEltYWdl 7412
IEJlZm9yZQ== 7413
Rmw= 7414
IHllc3RlcmRheQ== 7415
U291cmNl 7416
IHRlcnJvcmlzdA== 7417
IHN1cA== 7418
IHN5bnQ= 7419
IFNhdWRp 7420
IHdlc3Q= 7421
IHJ1 7422
YnVyZw== 7423
IHZpc2libGU= 7424
IHN0cnVjaw== 7425
cmlzb24= 7426
IGF3ZXNvbWU= 7427
IGRyYXdu 7428
IGFuc3dlcnM= 7429
IEdpcmw= 7430
IFJhbQ== 7431
IHRocmVhdHM= 7432
IGRlZmVhdA== 7433
b3NpdA== 7434
IHZlbnQ= 7435
YXR1cmFsbHk= 7436
QW1lcmljYW4= 7437
ZW5kYQ== 7438
IEhvbHk= 7439
IHJ1bQ== 7440
JSw= 7441
Y2FzZQ== 7442
IEhpc3Rvcnk= 7443
IFlvdVR1YmU= 7444
IHNpdHVhdGlvbnM= 7445
IEROQQ== 7446
U3Rl 7447
IHNhdmVk 7448
SXRlbQ== 7449
IHJlY2lw 7450
b2xvZ2lzdA== 7451
IGZhY2Vk 7452
IGVsaWc= 7453
T25jZQ== 7454
IExp 7455
dWg= 7456
IG1pc3Rha2U= 7457
IERpdmlzaW9u 7458
IEJlbGw= 7459
IHN5bXB0b21z 7460
wq4= 7461
IGRvbWlu 7462
IGZhbGxpbmc= 7463
IGVuZGluZw== 7464
YXNoZXM= 7465
IG1hdGNoZXM= 7466
IE9ubGluZQ== 7467
IGV4cGxhbmF0aW9u 7468
RGVm 7469
cmVkaXQ= 7470
IGFueW1vcmU= 7471
IFRvdGFs 7472
IEZPUg== 7473
dXNoZWQ= 7474
IGxldHRlcnM= 7475
IHJpc2tz 7476
IE9L 7477
IHJlcG9ydGVkbHk= 7478
Olw= 7479
IHBsYXRl 7480
IHN1YmplY3Rz 7481
IGF0dGVtcHRlZA== 7482
aWZpZXI= 7483
aWFuYQ== 7484
IHVubGlrZWx5 7485
IFRob3VnaA== 7486
dW1h 7487
IEludmVzdA== 7488
IFByaW4= 7489
aWNhbg== 7490
IERhcg== 7491
IENvbG9yYWRv 7492
YXVn 7493
IHZlZ2V0 7494
YW9z 7495
cmlh 7496
IHNoZWw= 7497
IG1hcmtlZA== 7498
ICgp 7499
IHNwcg== 7500
cG8= 7501
IExpbms= 7502
IGRlZmU= 7503
IEpy 7504
IHRoZW1l 7505
IHBhc3Npb24= 7506
IFBlbg== 7507
IGluZm8= 7508
aXplcg== 7509
IHNoaXQ= 7510
IENpdmls 7511
YXBzZQ== 7512
Y3Jl 7513
IHBvbHk= 7514
IGNvbXBvbmVudA== 7515
IENoYXJsZXM= 7516
IElyZWxhbmQ= 7517
IFByb3Y= 7518
IGRvY3RvcnM= 7519
IGdyYW50ZWQ= 7520
IHBhaW50 7521
IGhvbm9y 7522
IHNtb2tl 7523
IHBheW1lbnRz 7524
IHByaW1hcmlseQ== 7525
IEtpbmdkb20= 7526
cmljaA== 7527
YXRlbGw= 7528
IGRlYWxz 7529
IHNjaGVkdWxlZA== 7530
IGZ1bmRhbWVudGFs 7531
IHByb3RlaW4= 7532
IG5ld3NwYXBlcg== 7533
IGNsaWVudHM= 7534
eXRob24= 7535
IERhdGU= 7536
aHVz 7537
IGZlZWRiYWNr 7538
IHN0cmV0Y2g= 7539
IGNvY2s= 7540
IGhvdGVs 7541
IFF1ZWVu 7542
IHN1Z2Fy 7543
IGp1 7544
IG1pbGs= 7545
IGFwcHJvdmFs 7546
IExpdmU= 7547
IGVxdWl2YWxlbnQ= 7548
ZWZ1bGx5 7549
IGluc2VydA== 7550
em9uYQ== 7551
IGV4dGVuc2lvbg== 7552
ZHJp 7553
Sm9obg== 7554
IGFjY29tcA== 7555
U20= 7556
IEZ1bmQ= 7557
IGNvbnN0YW50bHk= 7558
IGBg 7559
IGdlbmVyYXRlZA== 7560
IEFjdGlvbg== 7561
IFBzeWNo 7562
IFRyaQ== 7563
IHJlY29nbml6ZQ== 7564
IHZhcnk= 7565
cGhh 7566
IFJh 7567
ZGY= 7568
ZXRjaA== 7569
IFNvdmlldA== 7570
VHdv 7571
IHBhdHRlcm5z 7572
IHByb2Zlc3Npb24= 7573
YW5pbmc= 7574
VGltZQ== 7575
IExpbQ== 7576
IGNvbG9ycw== 7577
IEF6 7578
IFRS 7579
IGluZmVjdA== 7580
IHBoZW5vbWVu 7581
IHNoZWxs 7582
QWxzbw== 7583
IHB1dHM= 7584
IGRlbGl2ZXJ5 7585
IGJyb3du 7586
IHByb2Nlc3Npbmc= 7587
IGxpZ2h0cw== 7588
ZXNzYWdl 7589
IEJyb29r 7590
IEF1ZA== 7591
bGF0aW9u 7592
IGluZHVzdHJpYWw= 7593
TGlrZQ== 7594
IEJyYXppbA== 7595
cm91cw== 7596
RVNT 7597
IEx1Yw== 7598
IHNvbWVob3c= 7599
IDg1 7600
IHByb3BvcnQ= 7601
IHBvbGl0aWNpYW5z 7602
IGluZGljYXRl 7603
IGhvbGU= 7604
IHRlY2huaXF1ZXM= 7605
IGNvbXBldGl0aXZl 7606
IHBocg== 7607
IHZv 7608
aXN0ZW50 7609
IERyZWFt 7610
IGNhbXB1cw== 7611
IGFzcGVjdHM= 7612
IGhlbHBmdWw= 7613
IHNoaWVsZA== 7614
b3JzZQ== 7615
IHRyaWdnZXI= 7616
bWFs 7617
IDU4 7618
IHRvcnQ= 7619
IHBlcnNvbmFsbHk= 7620
IHRhZw== 7621
IGtlZXBz 7622
IFZpZGVv 7623
IGJlbmNo 7624
IGdhcA== 7625
YWlyZQ== 7626
IGVhc3Q= 7627
IHJlY292ZXJ5 7628
cGVyaWFs 7629
IHByb2ZpdA== 7630
IE1pYw== 7631
IDU3 7632
IGNvbG9u 7633
IHN0cm9uZ2x5 7634
c3R5bGU= 7635
IGFsbGVnYXRpb25z 7636
aGFu 7637
IHJlcG9ydGVycw== 7638
am8= 7639
cmluZQ== 7640
YXJnZXQ= 7641
YW5kYWw= 7642
IDAz 7643
IGZsYXNo 7644
dHJhbnM= 7645
IHN0cmljdA== 7646
IHBhcmtpbmc= 7647
IFBha2lzdGFu 7648
IGxp 7649
IHdlaXJk 7650
IEVyaWM= 7651
IHJlZ2lvbnM= 7652
IEp1bg== 7653
IGludGVsbGVjdA== 7654
IFdI 7655
b2Rpbmc= 7656
cmlidXRlcw== 7657
dXBpZA== 7658
IFRpdA== 7659
IGZpbmdlcg== 7660
b3JpYQ== 7661
IGVsZXY= 7662
IEZpZWxk 7663
IGNvbmNsdXNpb24= 7664
Ozs= 7665
IGZlZWxpbmdz 7666
IGV4dGVuc2l2ZQ== 7667
IG1peGVk 7668
IG5ldXJv 7669
dnk= 7670
IGhhcmFzcw== 7671
IENpcmM= 7672
b3VjaA== 7673
IHRlcnJpdG9yeQ== 7674
IHN1Y2Nlc3NmdWxseQ== 7675
TWFy 7676
IGluZ3JlZA== 7677
IG92ZXJ3aGVs 7678
IGxheWVy 7679
Vmlldw== 7680
IGFsbGllcw== 7681
aWxsYW5jZQ== 7682
IFRocmVl 7683
IGJ1bmNo 7684
IG5vcm1hbGx5 7685
IG5ldHdvcmtz 7686
IHNhY3I= 7687
IENJQQ== 7688
Ymxlcw== 7689
IGNob3Nl 7690
IG9wcG9uZW50cw== 7691
IHJlZ2FyZGxlc3M= 7692
IGZyYW5jaA== 7693
IHByZWY= 7694
IFBv 7695
IGJyaWRnZQ== 7696
YW5uYQ== 7697
IFNpbHZlcg== 7698
IHdhZ2U= 7699
cGFnZQ== 7700
cmlvcg== 7701
IHJhZGljYWw= 7702
IExpdHRsZQ== 7703
IG1hbmlw 7704
IHNlY3JldGFyeQ== 7705
IGdhbmc= 7706
RFI= 7707
RkE= 7708
IGRlY2VudA== 7709
IFNwaXJpdA== 7710
IHVuY2xl 7711
IERldmVsb3BtZW50 7712
IGludmVzdG9ycw== 7713
IHdhbGxz 7714
IHB1Ymxpc2g= 7715
IGdlbmVyYXRl 7716
aXNzaW9ucw== 7717
Y2Fy 7718
IHByb21vdGU= 7719
IGN1dHRpbmc= 7720
IGNoZXN0 7721
IGRyaW5raW5n 7722
IGNvbGxlY3RlZA== 7723
IDcy 7724
IGhvcGluZw== 7725
IGVtYnI= 7726
Z29yaXRo 7727
IHdhcm5lZA== 7728
IGluc3RydWN0aW9ucw== 7729
T0c= 7730
IERpZA== 7731
IEFnZW5jeQ== 7732
IGdlYXI= 7733
IGNyaXRpY2lzbQ== 7734
IEZ1cnRoZXI= 7735
IHV0aWw= 7736
YW5ueQ== 7737
UmVk 7738
IGNvdW5zZWw= 7739
IEFzaWFu 7740
IHJlZHVjdGlvbg== 7741
cG9vbA== 7742
IHRlYWNoaW5n 7743
IGRlZXBseQ== 7744
aXk= 7745
IGVzdGltYXRlcw== 7746
IGNob2ljZXM= 7747
IHBlcm1hbmVudA== 7748
aW5lbQ== 7749
a2Vs 7750
IGZhc2M= 7751
cHNl 7752
ZmlsZQ== 7753
IExvdw== 7754
IFBlcnNvbg== 7755
IHRvdXJuYW1lbnQ= 7756
c3RhbA== 7757
IG1lbA== 7758
VVNU 7759
IFJheQ== 7760
YXpp 7761
VmFs 7762
IGNvbnRhaW5lZA== 7763
IEhvbGx5 7764
IHdha2U= 7765
IHJldmVhbA== 7766
IHByb2Nlc3Nlcw== 7767
IElTSVM= 7768
IDA5 7769
IGJsaW5k 7770
IHN0ZWVs 7771
IEJhZA== 7772
IGNhcmVmdWxseQ== 7773
YXBweQ== 7774
cm9pdA== 7775
IGdhbWluZw== 7776
IGhvdXNlcw== 7777
IENvbGw= 7778
IHRydWNr 7779
ZXJt 7780
IHNjb3JlZA== 7781
IG9jY2Fz 7782
cmV0dXJu 7783
Ym91bmQ= 7784
dmFy 7785
IHNoYXJw 7786
IGFmcmFpZA== 7787
IEVY 7788
YW1iZXI= 7789
Y2lmaWM= 7790
IHNjaGVtZQ== 7791
TkM= 7792
IFBvbGl0 7793
IGRlY2xpbmU= 7794
IDE5OTg= 7795
IHB1c2hpbmc= 7796
IHBvc3Nlc3Npb24= 7797
IHByaXZpbGU= 7798
IHRlYWNoZXJz 7799
IHlpZWxk 7800
SEE= 7801
IERhdmlz 7802
aXRsZWQ= 7803
IyMjIyMjIyM= 7804
IHJpZw== 7805
IERhbmllbA== 7806
YWNvbg== 7807
IGhpZGU= 7808
dXRlbg== 7809
IGNvbGxlYWd1ZXM= 7810
IHByaW5jaXBsZXM= 7811
IGxvdWQ= 7812
IHNpbg== 7813
IERlbW9u 7814
IHN0b25l 7815
IDAy 7816
IHRhdWdodA== 7817
IHRlcnJpYmxl 7818
IHN0dWNr 7819
IFBvbGljeQ== 7820
dGVlbg== 7821
IGltcGxlbWVudGF0aW9u 7822
IEJCQw== 7823
IEFQSQ== 7824
IHdoZWVs 7825
YWxsYXM= 7826
IGNoYW1waW9ucw== 7827
b2xhcnM= 7828
cGxheWVy 7829
IHJlcGVhdGVkbHk= 7830
IFN0aWxs 7831
IGxpa2Vz 7832
YXN0eQ== 7833
ZXN0ZXI= 7834
IENhdGhvbGlj 7835
Ukw= 7836
IGJhdGg= 7837
IG5vaXNl 7838
dGl0bGU= 7839
IG5vcnRoZXJu 7840
UGFydA== 7841
IG1hZ24= 7842
IGZhYg== 7843
IEFzaA== 7844
IGRpc3Bs 7845
IHRpY2tldA== 7846
IG11cmQ= 7847
IGFsb25nc2lkZQ== 7848
IE11c2lj 7849
IHJpdmVy 7850
IFN0ZWVs 7851
IENM 7852
IFBsYXllcg== 7853
IE11bHQ= 7854
b3dpbmc= 7855
cmVw 7856
c2l6ZQ== 7857
IHR1cg== 7858
IEdlb3JnaWE= 7859
aXNjYWw= 7860
cmFjdGlvbg== 7861
IGNhYmxl 7862
IDU5 7863
IHdpbnM= 7864
IHVwY29taW5n 7865
IHN1cnZpdmU= 7866
IGluc3BpcmVk 7867
IEVkdWNhdGlvbg== 7868
IHN0YXRpc3RpY3M= 7869
IEZvb3Q= 7870
aWFtaQ== 7871
IHllbGxvdw== 7872
IFBhZ2U= 7873
Li0= 7874
IEhhcw== 7875
IHVyYmFu 7876
IGF4 7877
ZXNzZWw= 7878
XCI= 7879
IHF1YXJ0ZXJiYWNr 7880
IHJlZ2lzdGVy 7881
IExhYm9y 7882
IGFiaWxpdGllcw== 7883
IEZhbWlseQ== 7884
IHZhcmlhYmxl 7885
IFByaWNl 7886
IGNvbnRlbQ== 7887
IHRoaW4= 7888
IEVxdQ== 7889
ZGF0YQ== 7890
IGdvdHRlbg== 7891
IGNvbnN0aXQ= 7892
IGFza3M= 7893
IHRhaWw= 7894
IGV4Y2l0aW5n 7895
IEVmZmVjdA== 7896
IFNwYW5pc2g= 7897
IGVuY291cmFnZQ== 7898
aW5zb24= 7899
IEFo 7900
IGNvbW1pdG1lbnQ= 7901
Q1M= 7902
IHJhbGx5 7903
IDo6 7904
IHN1YnNpZA== 7905
IHNwaW4= 7906
IGNhcHR1cmVk 7907
MjAxOA== 7908
IGlubm9j 7909
IGFsbGVnZWRseQ== 7910
IENvbWU= 7911
IGFydGlzdHM= 7912
IE51bWJlcg== 7913
IGVsZWN0cm9uaWM= 7914
IHJlZ2lvbmFs 7915
YXBlcw== 7916
IHdyYQ== 7917
IG15dGg= 7918
cHJpc2U= 7919
IE1pbGxlcg== 7920
IENyZWF0 7921
IEVwaXNvZGU= 7922
YmVsbA== 7923
IGRpcmVjdGVk 7924
IGV4dHJhY3Q= 7925
IHNvcnJ5 7926
IHZpY2U= 7927
YWdnZXI= 7928
IFN1cHBvcnQ= 7929
IDY2 7930
IElyb24= 7931
IHdvbmRlcmZ1bA== 7932
IGdyYQ== 7933
TmV0 7934
aW9uZQ== 7935
RW5n 7936
IHNoaXBz 7937
aWtlcw== 7938
IEtldmlu 7939
aXRhcg== 7940
IGFjdGl2aXN0cw== 7941
dHJ1ZQ== 7942
IEFyaXpvbmE= 7943
ZW50aA== 7944
IERlc3BpdGU= 7945
IFNF 7946
IGhhYml0 7947
ZXJuZWw= 7948
IGlucXU= 7949
IGFib3J0aW9u 7950
IHZvaWQ= 7951
IGV4cGxpY2l0 7952
IGVuZ2FnZWQ= 7953
IGFuZ3J5 7954
IHJhdGluZw== 7955
IGZyYWc= 7956
YnJv 7957
aWNraW5n 7958
ZGV2 7959
IHdvcnJpZWQ= 7960
IG9ic2Vy 7961
IGFwYXJ0bWVudA== 7962
IEdU 7963
IGVzdGF0ZQ== 7964
IENvbnN0aXR1dGlvbg== 7965
ZW1vbg== 7966
IFNub3c= 7967
IGNvdW50eQ== 7968
IGRpc2Fn 7969
IFN0ZXBoZW4= 7970
IGltbWlncmFudHM= 7971
d2luZA== 7972
IE5hdGlvbnM= 7973
IGZvbGtz 7974
T3V0 7975
IGdhbGw= 7976
IHRhcmdldGVk 7977
IHN0ZWFk 7978
IEJvbg== 7979
IExpYg== 7980
IGluZm9ybWVk 7981
IDEyMA== 7982
Y2hhaW4= 7983
aWRlbGluZXM= 7984
b3JvdWdo 7985
IGRyaXZlbg== 7986
IHJlZ3VsYXJseQ== 7987
IGJhc2tldA== 7988
IHByaW5jaXBsZQ== 7989
b2N1bWVudA== 7990
IHN0dW4= 7991
aWJpbGl0aWVz 7992
IFJvbWFu 7993
IEFib3V0 7994
IGFsZXJ0 7995
IGRlbW9jcmFjeQ== 7996
IHJlcHJlc2VudGVk 7997
SFM= 7998
Y2Vycw== 7999
cGFyZW50 8000
QXJ0 8001
cGFjaw== 8002
IGRpcGxvbQ== 8003
cmV0cw== 8004
IE5P 8005
IGNhcHR1cmU= 8006
IEFkdg== 8007
hKI= 8008
IGFubm91bmNlbWVudA== 8009
IExlYXI= 8010
IGhvb2s= 8011
IHB1cnM= 8012
IFN1Y2g= 8013
IENhbWVy 8014
IHJlZnVnZWVz 8015
IFZl 8016
UG9s 8017
IHJlY29nbml6ZWQ= 8018
bGli 8019
IGhhZG4= 8020
QXNz 8021
IHBpbG90 8022
dXNoaW5n 8023
IHJldHVybmluZw== 8024
IHRyYWls 8025
IFN0b25l 8026
IHJvdXRpbmU= 8027
IGNvdXJ0cw== 8028
IGRlc3Blcg== 8029
IGZyaWVuZGx5 8030
IEl0YWx5 8031
IHBsZWQ= 8032
IGJyZWF0aA== 8033
IHN0dWRpbw== 8034
TlM= 8035
IGltcHJlc3NpdmU= 8036
IEFmZ2hhbmlzdGFu 8037
IGZpbmc= 8038
IGRvd250 8039
aW5raW5n 8040
IFJvZw== 8041
aWFyeQ== 8042
Y29sb3I= 8043
c2V4 8044
YXJvbg== 8045
IGZhdWx0 8046
IE5pY2s= 8047
RG93bg== 8048
IFJvc2U= 8049
IFNvdXRoZXJu 8050
WFg= 8051
aXNvZGVz 8052
TGlzdA== 8053
NjAw 8054
IG91dGNvbWU= 8055
ZXJy 8056
IGVsc2V3aGVyZQ== 8057
IHJldGlyZQ== 8058
IHBvdW5kcw== 8059
IEdsb2JhbA== 8060
UGVvcGxl 8061
IGNvbW11bmljYXRpb25z 8062
IGxvYW4= 8063
IHJhdGlv 8064
IEVtcGlyZQ== 8065
IGdvbm5h 8066
IGludmVudA== 8067
REY= 8068
IDE5NzA= 8069
IENvbW1vbg== 8070
cGF0 8071
IHByb21pc2Vk 8072
IGRpbm5lcg== 8073
IEhvbQ== 8074
IGNyZWF0ZXM= 8075
IG9wZXJhdGU= 8076
dmVydHk= 8077
IEpvcmRhbg== 8078
ZXRpbWU= 8079
IHN1c3RhaW4= 8080
UmVn 8081
IGluY3JlZGlibGU= 8082
aW1h 8083
IHdhcnJhbnQ= 8084
IG1t 8085
QXR0 8086
IGxhd3N1aXQ= 8087
IHJldmlld3M= 8088
aXR1cmU= 8089
IFNvdXJjZQ== 8090
bGlnaHRz 8091
IEZvcmQ= 8092
IDYz 8093
Z3JvdXA= 8094
c3RvcmU= 8095
IGZlYXR1cmVk 8096
IGZvcmV2ZXI= 8097
IHBvdmVydHk= 8098
IFBvcA== 8099
IENOTg== 8100
YXp6 8101
YWJpcw== 8102
YWNoaW5n 8103
IGxhaWQ= 8104
IFN1cHA= 8105
IGZpbHRlcg== 8106
ZW5h 8107
IENvbW11bml0eQ== 8108
IGNyZWF0dXJlcw== 8109
dWN0aW9u 8110
IFJveWFs 8111
IGFzc29jaWF0aW9u 8112
IENvbm5lY3Q= 8113
IEJyYWQ= 8114
4paI 8115
bGVycw== 8116
dGhlcmU= 8117
IEdp 8118
IHZhbHVhYmxl 8119
QUNL 8120
IFRheWxvcg== 8121
IGxpcXVpZA== 8122
IEF0dG9ybmV5 8123
IENhcmw= 8124
IEZpbmFs 8125
YWdh 8126
IFdpbHNvbg== 8127
QmVjYXVzZQ== 8128
IFByb2Zlc3Nvcg== 8129
YWth 8130
IGluY3JlZGlibHk= 8131
cmFuY2U= 8132
ISk= 8133
UmVm 8134
c2s= 8135
IHNvbHV0aW9ucw== 8136
IGF0bW9zcGhlcmU= 8137
IGJsYW1l 8138
dW1lcw== 8139
IE5vYg== 8140
Q0E= 8141
dW1wcw== 8142
cmljYWw= 8143
IFB1dGlu 8144
IERlc3Q= 8145
b3JpYw== 8146
IFBB 8147
IHJlc3BlY3RpdmVseQ== 8148
d2Fu 8149
IGZpZnRo 8150
4oSi 8151
IENyeQ== 8152
IGdvdmVybm9y 8153
cmVzaWRlbnQ= 8154
IHB1cmNoYXNlZA== 8155
IGhhY2s= 8156
IGludGVuc2U= 8157
b2Jz 8158
IG9yaWdpbg== 8159
IGRlZmluZQ== 8160
IGNhcmVmdWw= 8161
Kioq 8162
IHNob3VsZGVy 8163
Q2xpY2s= 8164
IHRpZWQ= 8165
IGRlc3RydWN0aW9u 8166
b3VyZWQ= 8167
IG5vYm9keQ== 8168
IGhv 8169
IEV4cGVy 8170
IHRpcA== 8171
Ijs= 8172
IHRlY2huaXF1ZQ== 8173
IGp1cg== 8174
IFBvaw== 8175
Ym93 8176
IGxlZ2VuZA== 8177
IGFjY29yZA== 8178
IGJ1c3k= 8179
IEludGVs 8180
IGhhbmc= 8181
YWtp 8182
Ll0= 8183
4oCU4oCU4oCU4oCU 8184
IHN1cmdlcnk= 8185
IHJlcHJvZHU= 8186
IHVuaWZvcm0= 8187
IHNjZW5lcw== 8188
Y29kZQ== 8189
IDYy 8190
bGlzaGVy 8191
IEhhdmU= 8192
cGhpYQ== 8193
IGNyeXB0 8194
IHJlY29u 8195
IHNjcmVhbQ== 8196
IGFkb3B0ZWQ= 8197
IHNjb3Jlcw== 8198
TmU= 8199
IEl0YWxpYW4= 8200
aW5jbHVkaW5n 8201
Qk8= 8202
IGluZGljYXRlZA== 8203
IGVudGVydGFpbg== 8204
R3U= 8205
VGV4dA== 8206
aWVs 8207
IHR3ZW50eQ== 8208
IGVuZ2FnZQ== 8209
b2Zmcw== 8210
IFBhY2lmaWM= 8211
IHNtaWxl 8212
IHBlcnNvbm5lbA== 8213
IHRvbGVy 8214
IGRvb3Jz 8215
IHRvbmU= 8216
IG1hY2hpbmVz 8217
IGVudGVyaW5n 8218
dGVuYW5jZQ== 8219
Q08= 8220
IEplcnNleQ== 8221
IGZvcmVzdA== 8222
IGhvcnNl 8223
IGNvbXBsYWludA== 8224
IFNwcmluZw== 8225
eW8= 8226
IFBsdXM= 8227
ZWRpbmc= 8228
IFJldHVybg== 8229
cXVhcnRlcnM= 8230
aWFscw== 8231
Y293 8232
IGFjYWRlbWlj 8233
IGZydWl0 8234
IDE5OTY= 8235
b2dldGhlcg== 8236
IHdpbmU= 8237
IHB1cnN1 8238
IFN0ZXZlbg== 8239
IGxpY2Vucw== 8240
V2hv 8241
IGNsb3RoZXM= 8242
cmVjdGlvbg== 8243
IHNxdWFk 8244
IHN0YWJsZQ== 8245
IHJhdw== 8246
emVucw== 8247
U3Rhcg== 8248
dXRpZXM= 8249
YW5jZXI= 8250
IGtleXM= 8251
IE11 8252
IGNvbXBsaWNhdGVk 8253
aWdlcg== 8254
IFRleHQ= 8255
IGFic29y 8256
IDY4 8257
IGZ1bm55 8258
IHJlbGllZg== 8259
IExldw== 8260
IENvb2s= 8261
IGNoYXJ0 8262
IGRyYXdpbmc= 8263
R0U= 8264
IG1vZHVsZQ== 8265
IEJ1bGw= 8266
SUxM 8267
IHNhbHQ= 8268
MDAwMDAwMDA= 8269
aWxsZQ== 8270
IHJlc291cmNl 8271
YXdheQ== 8272
YWRlbHBoaWE= 8273
IEJydQ== 8274
IDY3 8275
IHNvbWVib2R5 8276
IHBhcnRpY2lwYXRl 8277
IHJvc2U= 8278
d2VyZWQ= 8279
IG11c2NsZQ== 8280
IGNvbnNlbnQ= 8281
IGNvbnRpbnVpbmc= 8282
IEd1YXJkaWFu 8283
IE9yZGVy 8284
cmVnb24= 8285
IHJlYXI= 8286
IHByb3Zpc2lvbg== 8287
IGxpa2Vk 8288
cmllbnQ= 8289
IGJyYQ== 8290
VHJhbnM= 8291
IG1lZXRpbmdz 8292
IHRveA== 8293
IGNvbnZlbnQ= 8294
IGF1dG8= 8295
IHJlY29yZGluZw== 8296
IFNvZnQ= 8297
MDAx 8298
IFJvbGw= 8299
IHByb2dyYW1taW5n 8300
IHBpYw== 8301
IHByb3ZlZA== 8302
IHN0YWI= 8303
IEFzdA== 8304
IGNhcHRpb24= 8305
dWxhdGluZw== 8306
IEF0dGFjaw== 8307
IG5ld2x5 8308
IDE5OTc= 8309
ZnI= 8310
IGRpc2NpcGw= 8311
IEdyZWVr 8312
IGVkaXRpb24= 8313
IERvZXM= 8314
IEJveA== 8315
aWZsZQ== 8316
YWNrZXQ= 8317
IHBhc3Nlcw== 8318
IGd1ZXN0 8319
IGFjY2VsZXI= 8320
aXRhbHM= 8321
VUQ= 8322
IGF1dGhlbnQ= 8323
IFJlc3Q= 8324
b3ZhbA== 8325
dGE= 8326
dWluZQ== 8327
IGFybW9y 8328
IFRvd24= 8329
IGNvbXBhdA== 8330
IGluY2hlcw== 8331
RGVzcGl0ZQ== 8332
IGFzc2lnbg== 8333
aGVyZW50 8334
IHByZXBhcmU= 8335
IE1lZw== 8336
b2NrZXk= 8337
IGRlcGVuZHM= 8338
IHRyYWNrcw== 8339
d2F0Y2g= 8340
IGxpc3Rz 8341
IE5vcnRoZXJu 8342
IGFsdGVy 8343
cmVj 8344
IEVhc3Rlcm4= 8345
IGNvbmRlbQ== 8346
IGV2ZXJ5d2hlcmU= 8347
Pyc= 8348
IGFmZmlsaQ== 8349
IGZvdWdodA== 8350
Ijp7Ig== 8351
IG1hYw== 8352
aXRhcmlhbg== 8353
IHNjb3Bl 8354
IEFM 8355
YXdz 8356
YXJtcw== 8357
IHF1ZQ== 8358
IGVuam95ZWQ= 8359
bmVzb3Rh 8360
IGFnZ3Jlc3NpdmU= 8361
IFN0b3J5 8362
IElW 8363
IHJlY2lwZQ== 8364
IHJhcmVseQ== 8365
IE1lZGljYWw= 8366
dmFsdWU= 8367
YW5nZWw= 8368
YXlpbmc= 8369
b21ldGhpbmc= 8370
IHN1YnNlY3Rpb24= 8371
IHNvdXRoZXJu 8372
IGZyZXF1ZW5jeQ== 8373
cmV0ZQ== 8374
cm9sbGVk 8375
dWx0cw== 8376
IE5pYw== 8377
IGJlaGFsZg== 8378
IHNlcXVlbmNl 8379
YWJldA== 8380
IGNvbnRyb3ZlcnNpYWw= 8381
IGNvbXByb20= 8382
IHdvcmtlcg== 8383
IG1haW5seQ== 8384
IGFsZ29yaXRo 8385
IE1ham9y 8386
b3JjZQ== 8387
Z2VuZGVy 8388
IG9yZ2FuaXplZA== 8389
IGZha2U= 8390
IGNvbmNsdWRlZA== 8391
IEVE 8392
IEV4ZWM= 8393
cmFnZQ== 8394
IGNoYW5jZXM= 8395
YmVycnk= 8396
IFRyYWQ= 8397
IGNvbmZpZ3VyYXRpb24= 8398
IHdpdGhkcmF3 8399
IGZybw== 8400
dWRlcw== 8401
IEJyb3RoZXI= 8402
IEJyaWFu 8403
IHRyaWVz 8404
IHNhbXBsZXM= 8405
IGJpZA== 8406
IEdvbGRlbg== 8407
IHBob3RvZ3JhcGg= 8408
aWZlc3Q= 8409
IERP 8410
IFBhcmxpYW1lbnQ= 8411
KioqKioqKioqKioqKioqKg== 8412
UmVt 8413
IGNvbnRlc3Q= 8414
IHNpZ25pbmc= 8415
cHg= 8416
IFplYWw= 8417
4pSA4pSA 8418
RWFy 8419
IGV4aXQ= 8420
QmVmb3Jl 8421
IENvcnBvcg== 8422
bnVsbA== 8423
bW9udGg= 8424
IHJhY2lhbA== 8425
b3R0ZWQ= 8426
IFZlZw== 8427
IFJldXRlcnM= 8428
IHN3b3Jk 8429
cHNvbg== 8430
IFJvbW5leQ== 8431
YWVk 8432
IHRyaWI= 8433
IGlubmVy 8434
IHByb3RvY29s 8435
IEJp 8436
IE1pYW1p 8437
ZXZlcmFs 8438
cHJlc3M= 8439
IHNoaXBwaW5n 8440
IEFtZW5kbWVudA== 8441
IEhvd2FyZA== 8442
Y29ubmVjdA== 8443
IERpc2M= 8444
IEphYw== 8445
aWFtb25k 8446
IFRoZXJlZm9yZQ== 8447
c2Vz 8448
IFByaW5jZXNz 8449
IFVTQg== 8450
IEFudGg= 8451
IHN1cnZlaWxsYW5jZQ== 8452
IGFwb2xvZw== 8453
IDYx 8454
b3dh 8455
IGZ1bGY= 8456
anM= 8457
IGx1Y2s= 8458
dXN0ZWQ= 8459
IMKn 8460
bmk= 8461
IGFudGljaXA= 8462
ZW1hbg== 8463
IHdpbm5lcg== 8464
IHNpbHZlcg== 8465
bGxh 8466
aWNpdHk= 8467
IHVudXN1YWw= 8468
IGNyYWNr 8469
IHRpZXM= 8470
ZXo= 8471
IHByYWN0aWNhbA== 8472
IHByb3ZpbmNl 8473
IFBsYWNl 8474
IHByaW9yaXR5 8475
SUNF 8476
IGRlc2NyaWJlcw== 8477
IGJyYW5jaA== 8478
Rm9ybQ== 8479
YXNrYQ== 8480
bWlzc2lvbnM= 8481
Ymk= 8482
IHBvcm4= 8483
IFR1cms= 8484
IGVudGh1cw== 8485
IGZpZ2h0ZXJz 8486
IDA4 8487
IERldHJvaXQ= 8488
IGZvdW5kYXRpb24= 8489
YXZpZA== 8490
QXJl 8491
IGp1ZGdtZW50 8492
Y2xpbmc= 8493
IHNvbHZl 8494
IERlc2lnbg== 8495
V2hlcmU= 8496
aGVzaXM= 8497
IFRybw== 8498
YWZ0ZXI= 8499
IG5ldXRyYWw= 8500
IFBhbGVzdGluaWFu 8501
IEhvbGx5d29vZA== 8502
IGFkdmlz 8503
IE5vbg== 8504
eWVz 8505
b2xpcw== 8506
IHJlcHV0YXRpb24= 8507
IHNtZWxs 8508
IGJyZWFk 8509
IEJ1bA== 8510
IEJlYWNo 8511
IGNsYWltaW5n 8512
IGdlbmV0aWM= 8513
IHRlY2hub2xvZ2llcw== 8514
IHVwZ3JhZGU= 8515
cm93cw== 8516
IGRldmVsb3Blcg== 8517
IEpvc2g= 8518
IERpc25leQ== 8519
ZXJ2ZWQ= 8520
aXBhbA== 8521
IHVuZXg= 8522
IGJhcmVseQ== 8523
dGhlbg== 8524
IFB1Yg== 8525
IGlsbG5lc3M= 8526
ZXRhcnk= 8527
IEJhbA== 8528
IHBhdGNo 8529
IGJ1dHQ= 8530
IHN0dXBpZA== 8531
IERvZw== 8532
IERhbGxhcw== 8533
ZnJvbnQ= 8534
aWVjZQ== 8535
IHByb3Rlc3Rz 8536
IGNoYXQ= 8537
b2VuaXg= 8538
IHdpbmc= 8539
IHBhcmxpYW1lbnQ= 8540
IDc3 8541
b3NleHVhbA== 8542
IHJlbmRlcg== 8543
cHRpb25z 8544
IENvYXN0 8545
b3Nh 8546
IEdyZWc= 8547
aG9w 8548
IE1hbmFnZW1lbnQ= 8549
IGJpdGNvaW4= 8550
IHJlY292ZXI= 8551
IGluY29ycG9y 8552
b3JuZQ== 8553
IFVzaW5n 8554
IHByZWNlZA== 8555
IHRocmVhdGVuZWQ= 8556
IHNwaXJpdHVhbA== 8557
IEV2ZW50 8558
IEZyZWQ= 8559
IGFkdmVydGlzaW5n 8560
IGltcHJvdmVtZW50cw== 8561
IEN1c3RvbQ== 8562
IGVycm9ycw== 8563
IHNlbnNpdGl2ZQ== 8564
IE5hdnk= 8565
IGNyZWFt 8566
TG9vaw== 8567
IGV4Y2x1c2l2ZQ== 8568
IGNvbXByZWhlbnM= 8569
IGRlbGVn 8570
IGNvbmNl 8571
IHJlbWVt 8572
IHN0cnVjdHVyZXM= 8573
IHN0b3JlZA== 8574
TkQ= 8575
IDEwMDA= 8576
VVA= 8577
IEJ1ZGQ= 8578
QUY= 8579
d29tYW4= 8580
IEFjYWRlbXk= 8581
8J8= 8582
c2Vh 8583
IHRlbXBvcmFyeQ== 8584
QWJvdXQ= 8585
ZXN0ZXJz 8586
IHRpY2tldHM= 8587
IHBvc3Nlc3M= 8588
aW5jaA== 8589
b3o= 8590
IGxh 8591
IGNvbnRyYWN0cw== 8592
IHVucA== 8593
IGNpZw== 8594
IEthdA== 8595
dWx0dXJhbA== 8596
YXNt 8597
IG1vdW50YWlu 8598
IENhcHRhaW4= 8599
U3RlcA== 8600
bWFraW5n 8601
IFNwYWlu 8602
IGVxdWFsbHk= 8603
IGxhbmRz 8604
YXRlcnM= 8605
IHJlamVjdGVk 8606
ZXJh 8607
aW1t 8608
cml4 8609
Q0Q= 8610
IHRyYW5zYWN0aW9u 8611
Z2VuZXI= 8612
bGVzc2x5 8613
IHx8 8614
IGNvcw== 8615
IEhlbnJ5 8616
IHByb3Zpc2lvbnM= 8617
IGdhaW5lZA== 8618
IGRpcmVjdG9yeQ== 8619
IHJhaXNpbmc= 8620
IFNlcA== 8621
b2xlbg== 8622
b25kZXI= 8623
IGNvbnNvbGU= 8624
aW5zdA== 8625
IGJvbQ== 8626
IHVuY2VydGFpbg== 8627
MTUw 8628
b2NraW5n 8629
IG1lYXN1cmVk 8630
IHBsYWlu 8631
IHNlYXRz 8632
IGRpY3Q= 8633
U0w= 8634
YWZl 8635
IGVzdGltYXRl 8636
aXpvbg== 8637
YXRoZXJlZA== 8638
IGNvbnRyaWJ1dGVk 8639
IGVwaXNvZGVz 8640
b21tb2Q= 8641
R3I= 8642
QU5U 8643
IDY5 8644
R2VuZXI= 8645
IDI1MA== 8646
dmlvdXNseQ== 8647
cm9nZW4= 8648
IHRlcnJvcmlzbQ== 8649
IG1vdmVtZW50cw== 8650
ZW50bGU= 8651
b3VuY2U= 8652
IFNvdWw= 8653
IHByZXY= 8654
IFRhYmxl 8655
YWN0cw== 8656
cmlvcnM= 8657
dGFi 8658
IHN1ZmZlcg== 8659
IG5lcnY= 8660
IG1haW5zdHJlYW0= 8661
IFdvbGY= 8662
IGZyYW5jaGlzZQ== 8663
YmF0 8664
IGRlbWFuZHM= 8665
IGFnZW5kYQ== 8666
IGRvemVu 8667
IGNsaW5pY2Fs 8668
aXphcmQ= 8669
IE9w 8670
dGQ= 8671
IHZpc2l0ZWQ= 8672
IFBlcmhhcHM= 8673
IGFjdG9y 8674
IGRlbGlj 8675
IGNvbnRyaWJ1dGU= 8676
IGluamVjdA== 8677
IEVz 8678
YWNjbw== 8679
IGxpc3RlbmluZw== 8680
IGNvbmdyZXNz 8681
ZXBlbmRlbnQ= 8682
IHByZW1pdW0= 8683
IDc2 8684
IElyaXNo 8685
IGFzc2lnbmVk 8686
IFBoeXM= 8687
IHdvcmxkd2lkZQ== 8688
IG5hcnJhdGl2ZQ== 8689
b3R5cGU= 8690
bW9udA== 8691
YmFzZQ== 8692
IEJvd2w= 8693
IEFkbWluaXN0cmF0aW9u 8694
IHJlbGF0aW9u 8695
IEVW 8696
Q1A= 8697
IGNvdmVycw== 8698
IDc4 8699
IGNlcnRpZmlj 8700
IGdyYXNz 8701
IDA0 8702
cGlyYWN5 8703
aXJh 8704
IGVuZ2luZWVyaW5n 8705
IE1hcnM= 8706
IHVuZW1wbG95 8707
IEZvcmVpZ24= 8708
c3RyYWN0 8709
IHZlbg== 8710
IHN0ZWFs 8711
IHJlcGxpZWQ= 8712
IHVsdGltYXRl 8713
IHRpdGxlcw== 8714
ZGF0ZWQ= 8715
IGpveQ== 8716
YXVz 8717
IGh5cGVy 8718
YWt1 8719
IG9mZmljaWFsbHk= 8720
IFByb2R1Y3Q= 8721
IGRpZmZpY3VsdHk= 8722
cGVyb3I= 8723
IHJlc3VsdGVk 8724
cmliZWQ= 8725
bGluaw== 8726
d2hv 8727
fn5+fg== 8728
IFNwZWVk 8729
IFZpZXQ= 8730
V2luZA== 8731
IEJhcmFjaw== 8732
IHJlc3RyaWN0aW9ucw== 8733
IFNoYXJl 8734
IDE5OTU= 8735
aXRpb25hbGx5 8736
IGJlYXV0eQ== 8737
b3B0 8738
IG1hcHM= 8739
IENS 8740
IE5hdGlvbg== 8741
IENydXo= 8742
V2lsbA== 8743
IGVsZWN0cmljaXR5 8744
IG9yZw== 8745
IGJ1cmQ= 8746
IHZpb2xhdGlvbg== 8747
IHVzYWdl 8748
IHBlcm1pdA== 8749
IENocm9u 8750
IEZhbnQ= 8751
IG5hdHVyYWxseQ== 8752
IDA3 8753
IHRocm93bg== 8754
IEF3b2tlbg== 8755
IGFsaWVu 8756
IEhlcm8= 8757
IEtlbnQ= 8758
IFJpY2s= 8759
cmlrZQ== 8760
IHBhY2U= 8761
fSx7Ig== 8762
R0w= 8763
IHBvaXNvbg== 8764
IFRvd2Vy 8765
IGZvcm1hbA== 8766
YWx5c2lz 8767
IGdlbnVpbmU= 8768
IGtpbA== 8769
YXZlcg== 8770
IHByb2NlZHVyZQ== 8771
IFByb3A= 8772
aW50ZW5kbw== 8773
IE1haW4= 8774
YXNhbnQ= 8775
IHRyYWluZWQ= 8776
R2FtZQ== 8777
IExvYWQ= 8778
IE1B 8779
IGNydWNpYWw= 8780
IGxldHM= 8781
IEZS 8782
IGNoYW1waW9u 8783
MTAx 8784
IENvbmZlcmVuY2U= 8785
IHdyaXRlcnM= 8786
IGNvbm5lY3Rpb25z 8787
IG9rYXk= 8788
aXJtcw== 8789
IFJhbmQ= 8790
IGVuY291bnRlcg== 8791
IEJ1ZmY= 8792
IGFjaGlldmVk 8793
IGNoZWNrcw== 8794
aXNjb25z 8795
IGFzc2lzdGFudA== 8796
IHdoZW5ldmVy 8797
IEFjY2Vzcw== 8798
IFVy 8799
Ymlu 8800
IGNsb2Nr 8801
aXNw 8802
b3BoZXI= 8803
IGJvcnJvdw== 8804
IG1hZA== 8805
IHBlcnNvbmFsaXR5 8806
b25seQ== 8807
SVNU 8808
YWJhbWE= 8809
IGdhaW5z 8810
IGNvbW1vbmx5 8811
IHRlcnI= 8812
IGh5cG90 8813
IHJlbHk= 8814
IHRpc3M= 8815
aXNjb25zaW4= 8816
IHJpZGlj 8817
ZnVuY3Rpb24= 8818
IE9yZWdvbg== 8819
IHVuY29t 8820
cmF0aW5n 8821
ZWxhbmQ= 8822
IE5D 8823
IG1vb24= 8824
YW5ub24= 8825
IHZ1bG5lcmFibGU= 8826
dXRpdmU= 8827
wqDCoMKgwqA= 8828
IFJhZGlv 8829
IHdlc3Rlcm4= 8830
c2VjdA== 8831
IFRvbnk= 8832
IG9jY3Vycw== 8833
IE9z 8834
IEhvbg== 8835
w60= 8836
IHZlc3NlbA== 8837
IFNjb3RsYW5k 8838
IGRpc2NyaW1pbmF0aW9u 8839
IHN1YnNlcXVlbnQ= 8840
c3RyaW5n 8841
IGZhbnRhc3k= 8842
IFNoYWRvdw== 8843
IHRlc3RpbQ== 8844
V0U= 8845
aXRp 8846
cmFz 8847
IGJvYXQ= 8848
IG1hcmtz 8849
IG9yZGluYXJ5 8850
IHJlbg== 8851
IHJlcHJlc2VudGF0aXZl 8852
IHBldGl0aW9u 8853
IDcz 8854
IGFkdmVudHVyZQ== 8855
IGlnbm9yZQ== 8856
IFBoaWxhZGVscGhpYQ== 8857
IFNhdg== 8858
VlA= 8859
IGZhY3Rvcnk= 8860
IHRhc2tz 8861
IGRlcHJlc3Npb24= 8862
emVk 8863
Li4uLi4uLi4uLi4uLi4uLi4uLi4uLi4uLi4uLi4uLi4= 8864
IFN0b3Jt 8865
IGNvZ24= 8866
IGVsaWdpYmxl 8867
IHJlZHVjaW5n 8868
dmlh 8869
IDA1 8870
IHN0cmlraW5n 8871
IGRvbGxhcg== 8872
aG8= 8873
T1Y= 8874
IGluc3RydW1lbnQ= 8875
IHBoaWxvc29waHk= 8876
IE1vb3Jl 8877
IEF2ZW51ZQ== 8878
IHJ1bGVk 8879
IEZyb250 8880
SU5F 8881
IE1haA== 8882
IHNjZW5hcmlv 8883
IE5BU0E= 8884
IGVub3Jt 8885
IGRlYnV0 8886
IHRlYQ== 8887
VG9kYXk= 8888
IGFic2VuY2U= 8889
U2lt 8890
IGhhbQ== 8891
bGVlcA== 8892
IHRhYmxlcw== 8893
IEhlYXJ0 8894
TUk= 8895
S2U= 8896
cmVxdQ== 8897
VkQ= 8898
bWFw 8899
IGNoYWlybWFu 8900
IHB1bXA= 8901
IHJhcGlkbHk= 8902
dmk= 8903
IHN1YnN0YW50aWFs 8904
RVA= 8905
ZGVz 8906
Y2hhbnQ= 8907
aWxpcHA= 8908
IFNhbnRh 8909
cmllcnM= 8910
YW5jaGVzdGVy 8911
TG9hZA== 8912
IENhc2U= 8913
IHNhdmluZw== 8914
IDc0 8915
IEFGUA== 8916
ZXJuaW5n 8917
b3VuY2Vk 8918
IE1pbm5lc290YQ== 8919
IFdhcw== 8920
IHJlY3J1 8921
IGFzc2Vzc21lbnQ= 8922
IEJyb24= 8923
VUU= 8924
IGR5bmFtaWM= 8925
IGZ1cm4= 8926
dWxhdG9y 8927
IHByb3BhZw== 8928
aGlnaA== 8929
IGFjY29tbW9k 8930
IHN0YWNr 8931
IFN1cw== 8932
d3JpdA== 8933
IHJldmVu 8934
IEdvZGQ= 8935
IFplYWxhbmQ= 8936
YWJz 8937
IGJydXQ= 8938
IHBlcnBldA== 8939
aG90 8940
IGhhcmRseQ== 8941
IEJ1cm4= 8942
44K5 8943
IHN0eQ== 8944
IHRyYW5zYWN0aW9ucw== 8945
IGdhdGU= 8946
IHNjcmVlbnM= 8947
IHN1Ym1pdHRlZA== 8948
IDEwMQ== 8949
IGxhbmd1YWdlcw== 8950
dWdodA== 8951
ZW1lbg== 8952
IGZhbGxz 8953
IGNvYw== 8954
gqw= 8955
IHN0cmlrZXM= 8956
cGE= 8957
IGRlbGliZXI= 8958
IElN 8959
IHJlbGF4 8960
YW5uZWxz 8961
IFNlbmF0b3I= 8962
IGV4dHJlbQ== 8963
IH0s 8964
IERlYg== 8965
IGJlbGw= 8966
IGRpc29yZGVy 8967
Y3V0 8968
IGlPUw== 8969
IGxvY2tlZA== 8970
IGVtaXNzaW9ucw== 8971
IHNob3J0bHk= 8972
Il0= 8973
IEp1ZGdl 8974
IFNvbWV0aW1lcw== 8975
IHJpdmFs 8976
IGR1c3Q= 8977
IHJlYWNoaW5n 8978
RmlsZQ== 8979
wq/Cr8Kvwq8= 8980
aW5vaXM= 8981
IEphc29u 8982
IHNhdGVsbA== 8983
YXJldA== 8984
IHN0YXRpb25z 8985
IGFncmlj 8986
IFRlY2hub2xvZ3k= 8987
Y29tZXM= 8988
IFVuZm9ydHVuYXRlbHk= 8989
IENoaWxkcmVu 8990
IGFwcGxpZXM= 8991
YXN0ZWQ= 8992
IGFuZ2Vy 8993
YWlsYWJpbGl0eQ== 8994
IERhbWFnZQ== 8995
IGNvbXBhcmU= 8996
IFN0YW5kYXJk 8997
IGFpbWVk 8998
IEJh 8999
YW5ndWFnZQ== 9000
IHJlZ3VsYXRpb24= 9001
IGp1cnk= 9002
IGFpcnBvcnQ= 9003
IHNlY3Rpb25z 9004
IFByaW5jZQ== 9005
ZW1lZA== 9006
IG1lZGljaW5l 9007
IGhpdHRpbmc= 9008
IHNwYXJr 9009
b2x2ZXM= 9010
IGFkcw== 9011
U3RhdGU= 9012
IGZvb2Rz 9013
IHJlcGxhY2VtZW50 9014
IGNoaWNrZW4= 9015
IGxvd2VzdA== 9016
IG1pbmRz 9017
IGludm9sdmVz 9018
dWk= 9019
IGFycmFuZw== 9020
IHByb2NlZHVyZXM= 9021
IFdoaWNo 9022
aXZlcnNhcnk= 9023
IGJpbGxz 9024
IGltcHJvdmVtZW50 9025
IGluZXY= 9026
IGV4cGVjdGF0aW9ucw== 9027
IGludGVsbGVjdHVhbA== 9028
IHNwYWNlcw== 9029
IG1lY2hhbmlzbQ== 9030
MjUw 9031
YnJlYWs= 9032
IFpl 9033
IFRlbm4= 9034
IEJhbHQ= 9035
IGJhcnJlbA== 9036
IHN0YXRpYw== 9037
bWFubg== 9038
UG9saWNl 9039
IHRpcHM= 9040
IGhhbmRsaW5n 9041
Y3Vz 9042
b2RlZA== 9043
aWx0b24= 9044
aXJ5 9045
IGpvdXJuYWxpc3Rz 9046
b3Vyc2U= 9047
IGNvbWlj 9048
IG5vbWluZQ== 9049
SVRZ 9050
IHZlcnN1cw== 9051
IGxvb3A= 9052
IHN1cmY= 9053
IEluZHVzdA== 9054
IEh1bnRlcg== 9055
IGJlbGllZnM= 9056
aXNhbg== 9057
IHNldHVw 9058
IGJyZXc= 9059
aW1hZ2U= 9060
IGNvbXB1dGVycw== 9061
Zm9s 9062
fSwi 9063
IE1lZGFs 9064
IHRheHA= 9065
IGRpc3BsYXllZA== 9066
IGdyYXY= 9067
IGZpc2NhbA== 9068
TW9u 9069
IE1vc2Nvdw== 9070
IEtvbmc= 9071
IENlbnRyZQ== 9072
IGNhbWVyYXM= 9073
IE1ycw== 9074
IEhheQ== 9075
IGF2ZXI= 9076
IEtlbGx5 9077
cHk= 9078
IHJlcXVpcmVtZW50 9079
IGVudGl0bGVk 9080
b21iaWU= 9081
IHNoYWRvdw== 9082
YWdpYw== 9083
IEFr 9084
IGVsaXRl 9085
IGRpdmlkZWQ= 9086
IGhlYWRpbmc= 9087
IGNvcGllcw== 9088
IGxvc3Nlcw== 9089
IHZpdA== 9090
a2Vk 9091
IEJyeQ== 9092
IGFucw== 9093
IFN0ZWFt 9094
IHJlcG9ydGVy 9095
aGVpbQ== 9096
IEl0ZW0= 9097
IHN1cGVyaW9y 9098
ZG9u 9099
ZXJlbnQ= 9100
w7Y= 9101
IHRoZXJhcHk= 9102
IHBlYWs= 9103
IE1vZGVs 9104
IGx5aW5n 9105
IGdhbQ== 9106
emVy 9107
cml0dGVu 9108
IHJlc3BvbnNlcw== 9109
IGNvbnNpZGVyYXRpb24= 9110
IEJpYmxl 9111
IGxveWFs 9112
IGluc3RhbnQ= 9113
IHBt 9114
IEZvcmVzdA== 9115
w7w= 9116
IGV4dGVuZA== 9117
IGNvbnZpY3RlZA== 9118
IGZvdW5kZXI= 9119
IGNvbnZpbg== 9120
IE9haw== 9121
Y2hlY2s= 9122
IHNjaG9sYXJz 9123
cGVk 9124
IG92ZXJzZQ== 9125
VG9w 9126
Y291bnQ= 9127
IEFyaw== 9128
wrc= 9129
IDA2 9130
IExB 9131
bWQ= 9132
IExhdGlu 9133
aW1lbnRhbA== 9134
IENQVQ== 9135
IHN1YnN0YW5jZQ== 9136
IG1pbm9yaXR5 9137
IG1hbnVmYWN0dXJpbmc= 9138
RXI= 9139
b2NvbGF0ZQ== 9140
IGF0dGVuZGVk 9141
IE1hbmFnZXI= 9142
cmF0aW9ucw== 9143
IGFwcHJlY2lhdGU= 9144
b215 9145
R0JU 9146
aWRlbmN5 9147
Qkw= 9148
IGd1YXJhbnRlZQ== 9149
cG9zaXRpb24= 9150
IG9jZWFu 9151
Y2x1ZGU= 9152
IGhlYWRlZA== 9153
IHRhcGU= 9154
IGxvb3Nl 9155
IGxvZ2lj 9156
IHByb3Zlbg== 9157
IHNwaXI= 9158
IGFkbWl0 9159
aXNh 9160
IGludmVzdGlnYXRl 9161
IDE5OTQ= 9162
c3lsdg== 9163
IExvc3Q= 9164
Y2VzdA== 9165
IDcx 9166
IHJlcXVlc3RlZA== 9167
IHdpbmRvd3M= 9168
IFBva8Op 9169
IFdpdGhvdXQ= 9170
TWV0 9171
IGJlaGF2aW91cg== 9172
IHJlYWRlcg== 9173
IGh1bmc= 9174
IEtlZXA= 9175
IHJvbGVz 9176
IGltcGxlbWVudGVk 9177
IGJsYW5r 9178
IHNlcnZlcw== 9179
IEpheQ== 9180
IGNpdGVk 9181
IEZyaWVuZA== 9182
cHJvZml0 9183
YXBvbg== 9184
IHJlcGFpcg== 9185
aXRlbQ== 9186
YXJyYXNz 9187
IGNyaXRpY3M= 9188
YWRp 9189
IEZhdGhlcg== 9190
IHNob3V0 9191
IGZvb2w= 9192
IDg4 9193
IHByb2R1Y2luZw== 9194
IGxpYg== 9195
IHJvdW5kcw== 9196
IGNpcmNsZQ== 9197
IHByZXBhcg== 9198
IHN1Ym1pdA== 9199
IG5pYw== 9200
bW9ycm93 9201
44Or 9202
VW5kZXI= 9203
IHZpdGFs 9204
YXRlcm4= 9205
IHBhc3N3b3Jk 9206
IHB1YmxpY2F0aW9u 9207
IHByb21pbmVudA== 9208
IHNwZWFrcw== 9209
IGJhcnM= 9210
IGRlZXBlcg== 9211
IE1pbGw= 9212
cG9ydGVk 9213
IHdpZA== 9214
IGJ1dHRlcg== 9215
IHNtb2tpbmc= 9216
IGluZGljYXRlcw== 9217
S2V5 9218
cm9wcmk= 9219
IEZpbGU= 9220
YWxsaW5n 9221
YXN0aW5n 9222
IFJ1cw== 9223
IGFkag== 9224
IDc5 9225
YXZhbA== 9226
IHByZXN1bQ== 9227
YnVyZ2g= 9228
b25pYw== 9229
IGZ1cg== 9230
IHBvbGxz 9231
aWth 9232
IHNlY29uZGFyeQ== 9233
IG1vbnN0ZXI= 9234
aWdz 9235
IEN1cnJlbnQ= 9236
RXZlbnQ= 9237
IG93bmVyc2hpcA== 9238
ZW5kYXI= 9239
IGFycml2ZQ== 9240
IFRheA== 9241
IG51bGw= 9242
IFByaXY= 9243
IHRocm8= 9244
IGtpc3M= 9245
Y2F0 9246
IHVwc2V0 9247
YW5nbGU= 9248
aXRjaGVz 9249
ZWN0b3I= 9250
b2xvZ2lzdHM= 9251
IEdhbGF4eQ== 9252
IGNvcnJ1cHRpb24= 9253
IGhpbnQ= 9254
ZW50ZXI= 9255
IEhvc3BpdGFs 9256
IGdyZWF0bHk= 9257
IGJlZ3Vu 9258
ZXN5 9259
IHNvaWw= 9260
IEFudG9u 9261
IG1haW50ZW5hbmNl 9262
44Op 9263
IGRvemVucw== 9264
IGh1bWFuaXR5 9265
IEFsYWJhbWE= 9266
IHJvbQ== 9267
d29ydGg= 9268
YXBpbmc= 9269
c3lsdmFuaWE= 9270
bGFo 9271
IGdhdGhlcmVk 9272
R0E= 9273
IGF0dGFja2luZw== 9274
Zm91bmQ= 9275
IFNxdWFyZQ== 9276
IGFyYml0 9277
aWN0aW9ucw== 9278
IFdpc2NvbnNpbg== 9279
IGRhbmNl 9280
IFNhaW50 9281
YXJjaHk= 9282
IGJhc2ViYWxs 9283
IGNvbnRyaWJ1dGlvbnM= 9284
IGxpdGVyYXR1cmU= 9285
IGV4aGE= 9286
cGVydHk= 9287
dGVzdA== 9288
IGJhYg== 9289
IGNvbnRhaW5lcg== 9290
bGV0dGVy 9291
IGZhbGxlbg== 9292
IHdlYnNpdGVz 9293
IGJvdHRsZQ== 9294
IFNhYw== 9295
IGJyZWFzdA== 9296
IFBM 9297
IHZldGVyYW4= 9298
IGludGVydmlld3M= 9299
IEFsZQ== 9300
IGJhbm5lZA== 9301
ZW5nZXJz 9302
IFJldm9sdXRpb24= 9303
aW50aA== 9304
IGNvbmNlcm5pbmc= 9305
SVZF 9306
IGV4cGVuc2Vz 9307
IE1hdHRoZXc= 9308
IENvbHVtYmlh 9309
ZHM= 9310
aXN0YW5jZQ== 9311
IGVudGl0eQ== 9312
Li4uIg== 9313
IHJlbGlhYmxl 9314
IHBhcmFsbGU= 9315
IENocmlzdGlhbnM= 9316
IG9waW5pb25z 9317
IGluZHU= 9318
bG93 9319
IGNvbXBldGU= 9320
IHRob3JvdWdo 9321
IGVtcGxveWVk 9322
IGVzdGFibGlzaG1lbnQ= 9323
aWdlbg== 9324
IENybw== 9325
IGxhd3llcnM= 9326
IFN0YXRpb24= 9327
VEU= 9328
IExpbmQ= 9329
IFB1cg== 9330
aXRhcnk= 9331
IGVmZmljaWVuY3k= 9332
4oCQ 9333
IEx5 9334
IG1hc2s= 9335
IGRpc2FzdGVy 9336
IGFnZXM= 9337
RVJF 9338
ZXNpcw== 9339
IEhvbGQ= 9340
IGNhc3VhbA== 9341
YmxlZA== 9342
IGVuYWJsZWQ= 9343
IEVudmlyb25tZW50 9344
IEludGVsbGlnZW5jZQ== 9345
aXBlcg== 9346
IE1hcA== 9347
IEJF 9348
IGVtZXJnZWQ= 9349
aXNkb20= 9350
IGNhYmlu 9351
IHJlZ2lzdHJhdGlvbg== 9352
IGZpbmdlcnM= 9353
IHJvc3Rlcg== 9354
IGZyYW1ld29yaw== 9355
IERvY3Rvcg== 9356
ZXR0cw== 9357
IHRyYW5zcG9ydGF0aW9u 9358
IGF3YXJlbmVzcw== 9359
SGVy 9360
IGF0dGVtcHRpbmc= 9361
T2Zm 9362
IFN0b3Jl 9363
w4PDgsODw4LDg8OCw4PDgg== 9364
IEtub3c= 9365
IGRlZmVuY2U= 9366
IHNjYW4= 9367
IFRlbg== 9368
IENoYWly 9369
IFBI 9370
IEF0bGFudGE= 9371
IGZ1Y2tpbmc= 9372
IGFuc3dlcmVk 9373
Ym4= 9374
IEthcg== 9375
IGNhdGVnb3JpZXM= 9376
IHJhdGlvbmFs 9377
IGN1c3Q= 9378
IHJvYm90 9379
IGNvcnJlY3RseQ== 9380
IGdpZg== 9381
IGdyYXBoaWNz 9382
bWlj 9383
IGdyb3VuZHM= 9384
IE9wcA== 9385
aWF0ZQ== 9386
IGRpc3RyaWJ1dGVk 9387
IHNhbmN0aW9ucw== 9388
IGNoYWxsZW5naW5n 9389
dXRv 9390
IGluZ3JlZGllbnRz 9391
IGludml0ZWQ= 9392
IGZvdW5kZWQ= 9393
IFJlcXU= 9394
ZGVk 9395
IGJvd2w= 9396
IGJyb3RoZXJz 9397
IEhh 9398
SU8= 9399
IHdhZ2Vz 9400
aW1vcmU= 9401
b2NpYWw= 9402
IHNlZWQ= 9403
YXRpdmVseQ== 9404
IGFkZHJlc3Nlcw== 9405
IElvd2E= 9406
YWJldGg= 9407
IGF0dGl0dWRl 9408
aXNk 9409
Y2hpbGQ= 9410
IG1vbGU= 9411
IGRpc2NvdmVyeQ== 9412
eWFyZA== 9413
QnI= 9414
IDgy 9415
IHN1cHBsaWVz 9416
ZWxsaW5n 9417
IGRpc3Rpbmd1 9418
Q1I= 9419
IHJlY2VwdA== 9420
IHZlcnQ= 9421
IHN3aW0= 9422
YmVj 9423
ZG9vcg== 9424
IFllYWg= 9425
IGdhbA== 9426
IGludGVyYWN0 9427
IEVTUA== 9428
IENT 9429
YW1wcw== 9430
IGNvbnZpbmNlZA== 9431
IG9iamVjdGl2ZQ== 9432
IGRpc2g= 9433
IFBob3Rvcw== 9434
bGFk 9435
IGRvd250b3du 9436
b2ls 9437
aW5jdGlvbg== 9438
IHRvbW9ycm93 9439
IENPTQ== 9440
IHN1cnZpdmFs 9441
c2hvdA== 9442
IHNldHRsZW1lbnQ= 9443
Q29ucw== 9444
IFhib3g= 9445
aW50ZXJlc3Q= 9446
IFNN 9447
YXJnbw== 9448
ZW5lc3M= 9449
IGV0aG5pYw== 9450
YmVyZWQ= 9451
TWlu 9452
IFRvaw== 9453
IGluY2VudA== 9454
IENvbW1hbmQ= 9455
IG1haW50YWluZWQ= 9456
IGJyZWFrcw== 9457
YnJpZGdl 9458
YXRhcg== 9459
YWdn 9460
IEZpbmFsbHk= 9461
dW5pY2lw 9462
IE9udA== 9463
bGVmdA== 9464
IHJlY29nbml0aW9u 9465
ICov 9466
IFBlcnM= 9467
IHdlbGY= 9468
IGFkZHJlc3NlZA== 9469
IEthbnNhcw== 9470
IHZpcnVz 9471
IHdoZXJlYXM= 9472
IHBhcGVycw== 9473
cmFtcw== 9474
IE1pbmlzdHJ5 9475
IHBsZWFzdXJl 9476
IGFjcXVpcmVk 9477
IGR1cmF0aW9u 9478
anBn 9479
IGNhbG0= 9480
IE5ITA== 9481
IGJ1cm5pbmc= 9482
IGZvbGRlcg== 9483
aWNrZWQ= 9484
IFB5 9485
IElsbGlub2lz 9486
Q2xhc3M= 9487
IEdvZGRlc3M= 9488
IHBlcmZvcm1pbmc= 9489
IHdlbGZhcmU= 9490
amFy 9491
SW50ZXI= 9492
IGxpbg== 9493
IGVuaGFuY2U= 9494
IG5vdGlvbg== 9495
ZmFyZQ== 9496
eXBlcw== 9497
IEFyZWE= 9498
IGNhbm5hYmlz 9499
IERpZWdv 9500
ZnM= 9501
IE1hbmNoZXN0ZXI= 9502
Y29tbQ== 9503
aW5pdGU= 9504
IGNvdmVyaW5n 9505
IFNvdW5k 9506
IDE5NjA= 9507
IDg0 9508
ZWxlY3Q= 9509
emluZw== 9510
IGNpdGl6ZW4= 9511
IHBob25lcw== 9512
IHJhaWQ= 9513
IGlnbm9yZWQ= 9514
IE9iamVjdA== 9515
IHVwbG9hZA== 9516
Y2FyZA== 9517
IG1vZGlmaWVk 9518
IHJvb21z 9519
aWFo 9520
cmFuZ2U= 9521
aGVhc3Q= 9522
YWNodXM= 9523
IHN1Z2dlc3Rpbmc= 9524
4oCL 9525
Z3JhZGU= 9526
RWw= 9527
IGNsb3RoaW5n 9528
IHJo 9529
IEhhbg== 9530
dW5pdHk= 9531
ZW5jaW5n 9532
IEF1c3Rpbg== 9533
c2VjdXRpb24= 9534
dHJh 9535
ZGVt 9536
IFF1YWw= 9537
IGhlYXZlbg== 9538
IHN0YWdlcw== 9539
IHdlZGQ= 9540
cGx1cw== 9541
aWZpY2lhbA== 9542
IEltbQ== 9543
IEhv 9544
aWV0aWVz 9545
IHBocmFzZQ== 9546
IGJyaWxs 9547
YWN0b3J5 9548
IHByb3ZpZGVycw== 9549
IHNpbGVuY2U= 9550
IGFlcg== 9551
IEFJ 9552
IEFkdmVudHVyZQ== 9553
IHBsYXRmb3Jtcw== 9554
IGRlbW9uc3RyYXRlZA== 9555
IGludGVyZg== 9556
aW5ndG9u 9557
IHJhY2Vz 9558
IGdyYWRl 9559
dWx0YW5l 9560
IFRocm91Z2g= 9561
ZmFsc2U= 9562
IGJvdw== 9563
IEFC 9564
IGZsYXZvcg== 9565
IGhpc3Rvcmlj 9566
Z292 9567
IGNvbG91cg== 9568
IHZpZXdlZA== 9569
IEVtYWls 9570
ZWxjb21l 9571
IGludGVydmVudGlvbg== 9572
IGRpdmVyc2l0eQ== 9573
IHBlcmlvZHM= 9574
IHJldmVyc2U= 9575
IFZlcnk= 9576
IHF1b3Rl 9577
IExlZnQ= 9578
dGhyb3VnaA== 9579
IHNjcmV3 9580
IGxhbmRpbmc= 9581
IHBpbGw= 9582
IHdldA== 9583
IHByb3Rlc3RlcnM= 9584
IHJlcGVhdA== 9585
YXZlZA== 9586
ZXJr 9587
IHNhbGFyeQ== 9588
IFBlbm5zeWx2YW5pYQ== 9589
U3RpbGw= 9590
IG1heW9y 9591
IGtpdGNoZW4= 9592
IGZlYXR1cmluZw== 9593
IE11c2V1bQ== 9594
IFRvdXJuYW1lbnQ= 9595
IEZhbA== 9596
IHNlcnZlcnM= 9597
VUM= 9598
IGFueWJvZHk= 9599
aW1n 9600
IFRyYWRl 9601
aXh0dXJl 9602
dGhlbGVzcw== 9603
IGZpbmFuY2U= 9604
IGNsb3Npbmc= 9605
IFBhdHJp 9606
aWFj 9607
YWJlbA== 9608
ID4+ 9609
b3JvdXM= 9610
IGZpcm1z 9611
c2NyZWVu 9612
dW5h 9613
IGVtYmFycmFzcw== 9614
dWxzZQ== 9615
IGxldHRpbmc= 9616
IHRocmV3 9617
aWxleQ== 9618
IGNoYW5uZWxz 9619
bGFu 9620
IFZlZ2Fz 9621
IHNlYXI= 9622
IGZhbnRhc3RpYw== 9623
YXJyZQ== 9624
dXp6bGU= 9625
IERlcg== 9626
VGhvc2U= 9627
IHN3aW5n 9628
IHNoZWV0 9629
aW5kZXg= 9630
Y292ZXI= 9631
b2dhbg== 9632
IHZhcmlhYmxlcw== 9633
IFRlY2g= 9634
IHNwb2tlbg== 9635
YWNoZWw= 9636
IERh 9637
IE1vdW50YWlu 9638
IGxvYWRlZA== 9639
IGZvb3RhZ2U= 9640
dmVyc2lvbg== 9641
IHVubA== 9642
IFBob2VuaXg= 9643
IHRocm93aW5n 9644
IGZpcmluZw== 9645
IHRyYWNraW5n 9646
IHdpZHRo 9647
IHN0cnVnZ2xpbmc= 9648
cm9vbXM= 9649
b3Rpb24= 9650
IG1vbnRobHk= 9651
IFNlcnZlcg== 9652
IGVnZ3M= 9653
b3Blbg== 9654
TUM= 9655
IDE5OTM= 9656
IGhpcmVk 9657
IHN0YXllZA== 9658
IEFsbGVu 9659
IHN0cm8= 9660
IDk4 9661
c3RlcA== 9662
IFR1cmtpc2g= 9663
IGZhYnJpYw== 9664
aXN0aW5n 9665
IERvbQ== 9666
IGRhdGVz 9667
IHByb24= 9668
IGJhc2tldGJhbGw= 9669
IGx1Y2t5 9670
IEFyYWJpYQ== 9671
IGFzc3VtZWQ= 9672
ZXN0eQ== 9673
IGFmZmFpcnM= 9674
IGdsYWQ= 9675
IEluZGVlZA== 9676
IEZB 9677
IFdvcmQ= 9678
IGpvaW5pbmc= 9679
aWZpY2U= 9680
cHJlYWQ= 9681
aXJ0cw== 9682
IFNlbGVjdA== 9683
IHBvcHVsYXRpb25z 9684
YXdhcmU= 9685
IG5vc2U= 9686
IGNvbXBsYWludHM= 9687
c3RhcnQ= 9688
IHNjb3Jpbmc= 9689
VGhhbmtz 9690
IG1pbmluZw== 9691
IHZpc2l0b3Jz 9692
U0g= 9693
IGRhbWFnZWQ= 9694
IGNoYXJhY3RlcmlzdGljcw== 9695
IFBlbnQ= 9696
REM= 9697
IDgz 9698
IFNpeA== 9699
cmF0ZXM= 9700
IGZsYWdz 9701
IEJyZXc= 9702
ZG9n 9703
TWFyaw== 9704
Ly8vLw== 9705
IGV4ZWN1dGlvbg== 9706
IGpva2U= 9707
cGhvbmVz 9708
IHRlc3RpbW9ueQ== 9709
IG9ic3Q= 9710
UUw= 9711
IEN1dA== 9712
IHN0dWRpZWQ= 9713
IE5pbnRlbmRv 9714
aWNrZXQ= 9715
IE5CQw== 9716
IGxhZA== 9717
IEJyYQ== 9718
IE1vaA== 9719
IGtlcm5lbA== 9720
IG92ZXJ3aGVsbWluZw== 9721
IGFnZWQ= 9722
IGFwcGxpY2FibGU= 9723
IENvbmQ= 9724
IHJvYWRz 9725
IEJsb2Nr 9726
bWFkZQ== 9727
b2RnZQ== 9728
IGNvbW1hbmRz 9729
IG9mZmljZXM= 9730
dmVsYW5k 9731
IHR1dA== 9732
IHJlY2VpdmVy 9733
IEZybw== 9734
IHNob3BwaW5n 9735
IGlQ 9736
IFN0cmU= 9737
IEFCQw== 9738
IGVudGVydGFpbm1lbnQ= 9739
IEJvdw== 9740
b3J0ZWQ= 9741
TWM= 9742
IHJlYWRz 9743
Z3JhZA== 9744
IENvbGxlY3Q= 9745
IOKIkg== 9746
IENhcGl0YWw= 9747
ZWRlcmF0aW9u 9748
IGVtcGxveWVy 9749
IGludm9sdmVtZW50 9750
IGFueGlldHk= 9751
YWxpYQ== 9752
IHJvb2Y= 9753
IEFtb25n 9754
IERlbW9jcmF0 9755
IHN0YXRz 9756
IFZpbGw= 9757
IGNvbnN0aXR1dGlvbmFs 9758
IHJlZmVycmluZw== 9759
aXR0eQ== 9760
IHRhY2tsZQ== 9761
b3V0dWJl 9762
IGJhY2tlZA== 9763
IEhvbmc= 9764
IEJyb2Fk 9765
IGVsZQ== 9766
IE90dA== 9767
IDE5OTI= 9768
aG91cg== 9769
YWNodXNldHRz 9770
Q2Fs 9771
IGRlZmVhdGVk 9772
IDgx 9773
ZXNw 9774
IHNlZW1pbmdseQ== 9775
d2Fz 9776
IEplbm4= 9777
IEt1cmQ= 9778
IGdlbmU= 9779
IGRpc2NvdW50 9780
UmV0 9781
RUNU 9782
KCk7 9783
IGNsdWJz 9784
IHNpZA== 9785
IE1hcnNo 9786
Q2hlY2s= 9787
IHBw 9788
IEVhZw== 9789
aWRlc3ByZWFk 9790
IGJlaW5ncw== 9791
RlQ= 9792
IGludHJvZHVjdGlvbg== 9793
IENoYW5nZQ== 9794
QVJE 9795
IDExMA== 9796
YWRvd3M= 9797
aWVyY2U= 9798
IG1lYWw= 9799
YXV0aG9y 9800
IEJhbmc= 9801
bGFob21h 9802
IHJhbmtz 9803
MjAxMQ== 9804
Pz8/Pw== 9805
bWF4 9806
IGNvbGxhcHNl 9807
IG9wZW5z 9808
IGVjaG8= 9809
IHNvcGg= 9810
IHJhY2lzdA== 9811
IGVub3Jtb3Vz 9812
IHdhdmVz 9813
IHRhcA== 9814
IGNvbXByZWhlbnNpdmU= 9815
Li0t 9816
IFJveQ== 9817
IGZhcm1lcnM= 9818
UmVsYXRlZA== 9819
YWlyZWQ= 9820
cm9uZXM= 9821
IENyaW0= 9822
IHByb3BvcnRpb24= 9823
IGRlc2lnbnM= 9824
IG5lZ290aWF0aW9ucw== 9825
IHZpcnR1YWxseQ== 9826
IEJhdG1hbg== 9827
IHdhcm4= 9828
IGxlZ2l0aW1hdGU= 9829
bWF0ZQ== 9830
IGNvbnZlbnRpb24= 9831
LCw= 9832
bmV0aWM= 9833
IFNE 9834
IGNvbnNpc3RlbnRseQ== 9835
IGNvbXBlbnNhdGlvbg== 9836
IHB1bmlzaG1lbnQ= 9837
IHll 9838
IHRpZQ== 9839
IEJ1cmVhdQ== 9840
aXJsZg== 9841
IEJ1 9842
IEFyZW4= 9843
IFBoaWxpcHA= 9844
IGtuaWZl 9845
IG1lbW9yaWVz 9846
IFJvc3M= 9847
IGFuZ2xl 9848
IDg2 9849
IFRodW5kZXI= 9850
IHJlbmQ= 9851
IFRvdXI= 9852
IGNvdW50cw== 9853
c3VuZw== 9854
IEltcA== 9855
IGVkdWNhdGlvbmFs 9856
IGFjY2Vzc2libGU= 9857
Q09N 9858
IGRyZXc= 9859
eWVy 9860
R2w= 9861
YW1pbmU= 9862
T1JU 9863
T0I= 9864
SUI= 9865
bWFzdGVy 9866
IHRyaWFscw== 9867
b2d5 9868
aGFy 9869
IFRydXN0 9870
IHByZWZlcnJlZA== 9871
aXJsZnJpZW5k 9872
IE5ldg== 9873
IGJpbg== 9874
IGNvdw== 9875
UGFnZQ== 9876
IHNpZ25hdHVyZQ== 9877
IEJM 9878
NzAw 9879
IHJldGlyZWQ= 9880
IGJ5dGVz 9881
IG5laWdoYg== 9882
IExlZ2VuZA== 9883
IGRldmFzdA== 9884
IHN1c3BlY3RlZA== 9885
aXNvbnM= 9886
IFBva8OpbW9u 9887
c2NhbGU= 9888
IGNhcGFiaWxpdGllcw== 9889
IHJldmVs 9890
IGNoZWVzZQ== 9891
ZHk= 9892
aWdyYW50 9893
IGZhaWxpbmc= 9894
Yml0cw== 9895
IEhlcm9lcw== 9896
IEdob3N0 9897
IFNjaWVudA== 9898
IGFwcG9pbnRlZA== 9899
dXJp 9900
IGluc3RpdHV0aW9u 9901
IGV4cGFuZGVk 9902
Z3JlZw== 9903
IG1vbml0b3Jpbmc= 9904
IHBvZGNhc3Q= 9905
IGNvYWxpdGlvbg== 9906
IDk2 9907
Sm8= 9908
IHN0b2xlbg== 9909
IFNhYg== 9910
IHN0b3Bz 9911
IGhvbGlkYXk= 9912
IGludHI= 9913
Q2Fy 9914
QmxhY2s= 9915
IExHQlQ= 9916
IHdhcm1pbmc= 9917
IEFuZGVyc29u 9918
IDg5 9919
IHByb2R1Y2Vy 9920
TWVk 9921
IGFjY3VyYWN5 9922
IE1hcnZlbA== 9923
aXphYmV0aA== 9924
IFBhdHJpY2s= 9925
bW9ueQ== 9926
IG1pbmk= 9927
YWNsZXM= 9928
IG92ZXJ0 9929
dGhleQ== 9930
IG1lbWJlcnNoaXA= 9931
IFZlbg== 9932
IGV4Y2g= 9933
IHJlbW92YWw= 9934
IERhdmU= 9935
VFk= 9936
bWFk 9937
IEZpbmQ= 9938
IGFkZXF1 9939
IGVj 9940
IHRlZXRo 9941
IGVtb3Rpb24= 9942
IHBlcm0= 9943
IHNvbGVseQ== 9944
ZGI= 9945
IGV4dHJhb3Jk 9946
SUdIVA== 9947
Y2Fs 9948
IGd1aWRlbGluZXM= 9949
IGR5aW5n 9950
IHN1c3BlbmRlZA== 9951
IFByZW1pZXI= 9952
IEFudGhvbnk= 9953
ZWx2ZQ== 9954
IGRhZA== 9955
IEV0aA== 9956
IEZvb3RiYWxs 9957
IGFiYW5kb25lZA== 9958
IDw8 9959
IG1hcmNo 9960
IGhvcnJvcg== 9961
4oCmIg== 9962
IGNoaWxkaG9vZA== 9963
IGNhbXBhaWducw== 9964
IGx1bmNo 9965
IEFsYmVydA== 9966
YmxvY2s= 9967
4paI4paI 9968
b3VuZGluZw== 9969
IGJvbmU= 9970
b3JnYW4= 9971
YWRlcnM= 9972
IEZsYXNo 9973
IERyaXZl 9974
IHRvbmlnaHQ= 9975
IHdhcnM= 9976
IEZM 9977
IGZvcm1hdGlvbg== 9978
Y29uc3Q= 9979
TmV3cw== 9980
IGNvbXBl 9981
b3Jpb3Vz 9982
IFN0YWZm 9983
IGRpc2N1c3Npb25z 9984
IFByb3RlY3Rpb24= 9985
IEphbQ== 9986
IGNyaXRlcmlh 9987
IGluc3RhbGxhdGlvbg== 9988
IGFjY29tcGxpc2g= 9989
aXp6YQ== 9990
IHB1Ymxpc2hlcg== 9991
IHJlc2N1ZQ== 9992
IFRyeQ== 9993
VUxM 9994
IFNvbQ== 9995
IEhvcA== 9996
b3JldA== 9997
dGhz 9998
b3Jkb24= 9999
IHBvY2tldA== 10000
IEludg== 10001
RG93bmxvYWQ= 10002
IENyaW1l 10003
IGJlbmU= 10004
IEd1aWRl 10005
IEFzc2VtYmx5 10006
IHBhcmFtZXRlcnM= 10007
SUU= 10008
IEFsZXhhbmRlcg== 10009
IGNvbmNlcnQ= 10010
IFNjaGU= 10011
IHNob2Vz 10012
IHZpc2l0aW5n 10013
IHJlY2FsbA== 10014
IGJ1Yg== 10015
IHJ1cmFs 10016
IGNvbmNyZXRl 10017
IFJvcw== 10018
TmV4dA== 10019
UnVzcw== 10020
IGxvYW5z 10021
IFNoaWVsZA== 10022
IHRyZW0= 10023
aGVtYXQ= 10024
a2c= 10025
IEhhcnJpcw== 10026
aXNpdGlvbg== 10027
IE1vdmU= 10028
IEZD 10029
IGZhdGU= 10030
IENobw== 10031
IHRpcmVk 10032
IHByaW5jaXBhbA== 10033
aGlzdA== 10034
aWVuY2Vz 10035
YXRoeQ== 10036
IHNldmVudA== 10037
IG1vb2Q= 10038
IHN0cmF0ZWdpYw== 10039
IGRpc2Vhc2Vz 10040
IGZvcnVt 10041
IHRlbXBvcg== 10042
IGhlYWRxdWFydGVycw== 10043
UGFy 10044
aWdl 10045
ZmxpeA== 10046
IGd1aXRhcg== 10047
IDk0 10048
T25seQ== 10049
IHJlbGVhc2Vz 10050
cm9waA== 10051
PT09PT09PT09PT09PT09PT09PT09PT09PT09PT09PT0= 10052
IDYwMA== 10053
IENvbnRpbnVl 10054
aWdhdGU= 10055
IENyaXQ= 10056
c3lzdGVt 10057
IGRpc2FibGVk 10058
IHVuZXhwZWN0ZWQ= 10059
aXRodWI= 10060
IHVuY2xlYXI= 10061
IEVzdA== 10062
IGNvbnRyYWQ= 10063
IHN0cmF0ZWdpZXM= 10064
dmVudHVyZXM= 10065
IHBhc3NhZ2U= 10066
QU1F 10067
IGltcHJvdmluZw== 10068
IHJldmVhbHM= 10069
IGRlY3JlYXNl 10070
b3Zh 10071
IGFubm95 10072
IFNob3J0 10073
IExpYnJhcnk= 10074
IGN5YmVy 10075
bmVsbA== 10076
IEh1cg== 10077
IENC 10078
IHBob3RvZ3JhcA== 10079
VUk= 10080
IHNlZA== 10081
R2U= 10082
IDg3 10083
IGRpdmVyc2U= 10084
IGVuY291cmFnZWQ= 10085
IGNvbnNwaXJhY3k= 10086
IGJpcmRz 10087
IG9wZXJhdG9y 10088
IGhhbmRmdWw= 10089
IGNsYXNzaWZpZWQ= 10090
Pyk= 10091
IGRyYW1hdGlj 10092
IGludmVzdGlnYXRvcnM= 10093
aXRv 10094
IHdpZGVzcHJlYWQ= 10095
IFJvb20= 10096
LS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLQ== 10097
IGNvbGxlY3RpdmU= 10098
IGpvdXJuYWxpc3Q= 10099
U3RyaW5n 10100
IHRlbXBlcmF0dXJlcw== 10101
aWxh 10102
IGd1aWQ= 10103
IGluc3BlY3Q= 10104
IG1pc3NpbGU= 10105
IE1heW9y 10106
IG1hbnVhbA== 10107
IHNpbXVsdGFuZQ== 10108
IHJhdGluZ3M= 10109
IHN1Y2s= 10110
IDk3 10111
IHVuaXZlcnNhbA== 10112
IHBoYXJt 10113
IGRpc3J1cHQ= 10114
aWFubw== 10115
QVY= 10116
IGZ0 10117
IHN0YXRpc3Q= 10118
b2xkcw== 10119
IFdhbGtlcg== 10120
cGhw 10121
IHVuZGVydA== 10122
IExhcw== 10123
aXNob3A= 10124
bnRpbA== 10125
cmVzaG9sZA== 10126
IFdoZXRoZXI= 10127
TXM= 10128
IGRlbnk= 10129
IENsb3Vk 10130
IHByb3ZpZGVy 10131
IHN1cnZpdg== 10132
IFVwZGF0ZQ== 10133
aGFz 10134
IG1pc3Rha2Vz 10135
Y2hhcmdl 10136
cGxlZA== 10137
cml0eQ== 10138
IG5vZGU= 10139
IE1hc3NhY2h1c2V0dHM= 10140
b29scw== 10141
bGljYXRpb24= 10142
IGZhaWxz 10143
ZW1hbGU= 10144
b3Jp 10145
YmFja3M= 10146
IHNoaXJ0 10147
ICcn 10148
IE5BVA== 10149
IHdhdGVycw== 10150
ZWxzb24= 10151
IGVhc2U= 10152
IHNjYXI= 10153
IGNvbnRlbnRz 10154
bWluZA== 10155
IGNvbnRyaWJ1dGlvbg== 10156
IHNocg== 10157
IGhhbmRlZA== 10158
IHN0YWJpbGl0eQ== 10159
IHRyYXZl 10160
RW0= 10161
IG1pcnJvcg== 10162
MTIz 10163
IHdlaWdo 10164
IGZpY3Rpb24= 10165
b3V2ZXI= 10166
aXN0YW50 10167
cml0aW9u 10168
IEZlZA== 10169
IHBoeXNpY2FsbHk= 10170
IHN0YWtl 10171
IEFydGljbGU= 10172
IEFyYw== 10173
IExld2lz 10174
IE1pbmQ= 10175
IGRlbW9uc3RyYXRl 10176
IHByb2ZpdHM= 10177
dmlzaW9u 10178
b21pYw== 10179
b2xpZA== 10180
IGJhdHRsZXM= 10181
IGRyaXZlcw== 10182
IGVhc3Rlcm4= 10183
IFNvbnk= 10184
ISEh 10185
YXJhdGlvbg== 10186
dmFyZA== 10187
IEdM 10188
cG9ydGF0aW9u 10189
IDky 10190
IGxhd21ha2Vycw== 10191
IHByb3RlY3Rpbmc= 10192
IEVQQQ== 10193
IHllYWg= 10194
IHNoYW1l 10195
b2xwaA== 10196
ZXZlbg== 10197
eGl0 10198
IGF0dGFjaA== 10199
IHJlcHJlc2VudGluZw== 10200
IG9icw== 10201
IFV0YWg= 10202
aWZmcw== 10203
IEZyZWVkb20= 10204
w7M= 10205
QUs= 10206
IGluY2lkZW50cw== 10207
aXRhZ2U= 10208
IHZpZXdlcnM= 10209
Y2Q= 10210
IG1vdXNl 10211
IGNsYXI= 10212
IGFjY29yZGFuY2U= 10213
IGJvdA== 10214
Y29y 10215
IFN1bW1lcg== 10216
aGVsZA== 10217
IGlubm9jZW50 10218
IGluaXRpYXRpdmU= 10219
b2xz 10220
X19fX19fX19fX19fX19fX19fX19fX19fX19fX19fX18= 10221
IHNwb3Rz 10222
cGFjZQ== 10223
IGNvbnZlbnRpb25hbA== 10224
IGNvcnBvcmF0aW9ucw== 10225
IGJsb2NrZWQ= 10226
SEQ= 10227
YXR0ZXJlZA== 10228
IHJlZmVycw== 10229
IGJ1Y2s= 10230
IERpZ2l0YWw= 10231
MTIw 10232
IHRvcGljcw== 10233
VEY= 10234
xIE= 10235
YnJpZA== 10236
cmVlbWVudA== 10237
IHVuZGVybHlpbmc= 10238
IE1lbWJlcg== 10239
IGludmVzdGlnYXRpbmc= 10240
IHByZWduYW5jeQ== 10241
IHRvdWNoZG93bg== 10242
IEJhbmQ= 10243
IENhbGxlcg== 10244
IGluc3RhbmNlcw== 10245
UFA= 10246
d2E= 10247
R29vZA== 10248
IDE5OTE= 10249
IENvbGQ= 10250
IGZlYXJz 10251
IHJlbWFya3M= 10252
hpI= 10253
YXRhbA== 10254
IG1pdA== 10255
IGV4cGVyaW1lbnRz 10256
aXB0 10257
Q29sb3I= 10258
aW5kdQ== 10259
VXBkYXRl 10260
IDkz 10261
QWc= 10262
IOU= 10263
YW5jb3V2ZXI= 10264
Qm90aA== 10265
IGp1ZGdlcw== 10266
T2JqZWN0 10267
IHN0ZXJl 10268
dW1ibg== 10269
IHBhcnRpY2lwYXRpb24= 10270
IFN0YXJz 10271
IEplcmU= 10272
IHdlZWtseQ== 10273
IEJhbg== 10274
IGNvbnZlcnNhdGlvbnM= 10275
IFBpdHQ= 10276
dXo= 10277
IEluZGlhbmE= 10278
IEtpY2s= 10279
IGluZmVjdGlvbg== 10280
IGhlcm9lcw== 10281
IHNldHRsZWQ= 10282
IHN0cmlw 10283
IGhhbA== 10284
IGR1bXA= 10285
IFNjaQ== 10286
IGxlcw== 10287
IHJlZmVyZW5jZXM= 10288
IFVSTA== 10289
IEJyaWRnZQ== 10290
IHdhbnRpbmc= 10291
Rm9yY2U= 10292
IGV4Y2x1cw== 10293
TWVhbndoaWxl 10294
bW4= 10295
IGdlbnRsZQ== 10296
bWFrZXI= 10297
c2VuYWw= 10298
IEdybw== 10299
b3VyaQ== 10300
IFJhaW4= 10301
IEFsbGlhbmNl 10302
IGxpZnQ= 10303
ZWxh 10304
U0Q= 10305
IENsZXZlbGFuZA== 10306
IHJhbmtlZA== 10307
IHN0YWRpdW0= 10308
IGRlYWRseQ== 10309
5Lg= 10310
IHJpZGluZw== 10311
YXJpYQ== 10312
IEFybW9y 10313
IGRvY3VtZW50YXRpb24= 10314
IEdyZWVjZQ== 10315
cmVlaw== 10316
IGxlbnM= 10317
IFNh 10318
IGdyb3Nz 10319
IEVtZXI= 10320
YWdlcnM= 10321
IER1Yg== 10322
IFJo 10323
IEFNRA== 10324
IGFycml2YWw= 10325
IGRlc2VydA== 10326
IHN1cHBsZW1lbnQ= 10327
IFJlc3A= 10328
IGtuZWU= 10329
IG1hcmdpbg== 10330
Zm9udA== 10331
b2dn 10332
MjAxMA== 10333
IFBpcg== 10334
IFByb20= 10335
aXZhbHM= 10336
IGludGFrZQ== 10337
IGRpZmZlcmVudGx5 10338
dWdz 10339
IGJpdHM= 10340
Y2x1ZGVk 10341
IHNlYXJjaGluZw== 10342
IER1 10343
dW1ibGU= 10344
IGZ1bmN0aW9uYWw= 10345
IEJhbHRpbW9yZQ== 10346
IENvdWxk 10347
IGRlc2lyZWQ= 10348
IGNpcmN1aXQ= 10349
IEx5bg== 10350
IEdP 10351
IEZhbHNl 10352
cmVwcmU= 10353
Jzo= 10354
YWx0aWVz 10355
IG1pbmlt 10356
IGRyb3Zl 10357
IFNob3VsZA== 10358
IGhpcA== 10359
IHByb3M= 10360
IHV0aWxpdHk= 10361
IE5hdHVyZQ== 10362
IE1vZGU= 10363
UHJlc2lkZW50 10364
b3Bw 10365
cmF0 10366
Zm9ybWFuY2U= 10367
IGNvbmNlbnRyYXRpb24= 10368
IGZvbnQ= 10369
IEJ1ZA== 10370
IGFtaWQ= 10371
IHJldmVycw== 10372
IE1M 10373
QmFy 10374
IGludGVyYWN0aW9u 10375
IGp1cmlzZA== 10376
IHNwZWxscw== 10377
ZGVw 10378
Zmls 10379
IGNpdmlsaWFucw== 10380
dXR0ZXI= 10381
IENvb3Blcg== 10382
IEJlbG93 10383
IGVudHJhbmNl 10384
IGNvbnZlcnQ= 10385
IGNvbnRyb3ZlcnN5 10386
b3dlcmVk 10387
IGNvbnRyYXJ5 10388
IGFyYw== 10389
IEV4ZWN1dGl2ZQ== 10390
IE9mZmljZXI= 10391
IHBhY2thZ2Vz 10392
IHByb2dyZXNzaXZl 10393
d2lkdGg= 10394
IHJlc2VydmVk 10395
dm9s 10396
IFNhbXN1bmc= 10397
IHByaW50ZWQ= 10398
IGNlbnRlcnM= 10399
IGludHJvZHVjZQ== 10400
IEtlbm5lZHk= 10401
IG9kZHM= 10402
IHN1cmVseQ== 10403
IGluZGVwZW5kZW5jZQ== 10404
IHBhc3NlbmdlcnM= 10405
cmVwcmVuZQ== 10406
IEJlaA== 10407
IGxvdmVz 10408
IEVTUE4= 10409
IGZhY2lsaXQ= 10410
IGlkZW50aWNhbA== 10411
IGRvY3Q= 10412
IHBhcnRuZXJzaGlw 10413
Y29uZg== 10414
IEhpZGU= 10415
IGNvbmZ1c2Vk 10416
IENvdw== 10417
TWVu 10418
IHdyZXN0 10419
IElyYXFp 10420
IGhvbGVz 10421
IFN0dWRpZXM= 10422
IHByZWduYW50 10423
aGFyZA== 10424
IHNpZ25hbHM= 10425
SVg= 10426
IHB1bGxpbmc= 10427
IGdyYWR1YXRl 10428
IG5vbWluZWU= 10429
RGF0ZQ== 10430
IHBlcm1pdHRlZA== 10431
IOKCrA== 10432
IE9rbGFob21h 10433
U3RhcnQ= 10434
IGF1dGhvcml6ZWQ= 10435
IGFsYXJt 10436
IENvcw== 10437
dmFu 10438
IGdlbmVyYXRpb25z 10439
Y3VsYXI= 10440
IGRyYWdvbg== 10441
IFNvZnR3YXJl 10442
IEVkd2FyZA== 10443
IGNvbnRyb2xsZXI= 10444
U2Vu 10445
Z2VyZWQ= 10446
IFZpaw== 10447
IGFwcHJvYWNoZWQ= 10448
VGhhbms= 10449
IGNhbmNl 10450
IGZvcm11bGE= 10451
IFNtYWxs 10452
IHdlYWtuZXNz 10453
IHJhbXA= 10454
aXR1ZGVz 10455
anVk 10456
IGJyaWxsaWFudA== 10457
IGFjY3Vz 10458
c291cmNl 10459
IDgwMA== 10460
IEV2aWw= 10461
U3c= 10462
IGhvbWVsZXNz 10463
d2Vlaw== 10464
aWVucw== 10465
cmljcw== 10466
IFRoaXJk 10467
VE8= 10468
IG9yZ2FuaWM= 10469
IHByZXNlbnRhdGlvbg== 10470
YWdo 10471
IERvd25sb2Fk 10472
dmF0aW9u 10473
IGFzc2VtYmx5 10474
b3JhYmxl 10475
aG9sZGVycw== 10476
IEJlcm5pZQ== 10477
IEhlbHA= 10478
IHRvbmc= 10479
IEZpZ2h0 10480
IGJlYWNo 10481
Qm9vaw== 10482
IExpYw== 10483
IHJ1c2g= 10484
IFJvdW5k 10485
b3Vw 10486
IE1hcng= 10487
IGNhbGN1bGF0ZWQ= 10488
IERldmls 10489
IFNhcmFo 10490
IG9jY2FzaW9uYWxseQ== 10491
IGJ1bGxldA== 10492
QXZhaWxhYmxl 10493
Z2F0ZQ== 10494
IDkx 10495
IGhvc3A= 10496
IHByb21pc2Vz 10497
IEhJVg== 10498
IFN0YWRpdW0= 10499
IFN0b2Nr 10500
IENvcnBvcmF0aW9u 10501
Z2FnZQ== 10502
Tkc= 10503
IENyZWRpdA== 10504
IHNuZQ== 10505
aWJs 10506
IGFjY3Vt 10507
c3VjaA== 10508
IHRlcnJvcmlzdHM= 10509
IGNvbnNjaW91c25lc3M= 10510
IFpo 10511
IGRyYW1h 10512
b29sYQ== 10513
cGlyYXRpb24= 10514
IGxhYm91cg== 10515
IE5pbg== 10516
IHV0dGVy 10517
IGRlbW9jcmF0aWM= 10518
IGFzc2Fzcw== 10519
aWxhdGlvbg== 10520
IGdlc3Q= 10521
IGFicm9hZA== 10522
IG1ldGFi 10523
IHNvcnRz 10524
IGZsYXY= 10525
VUI= 10526
IG1n 10527
IE5vdGhpbmc= 10528
IE9k 10529
IG11c2ljYWw= 10530
MjAwOQ== 10531
IGRyb3Bz 10532
b2NhdGVk 10533
YXRlcmFs 10534
MDAwMDAw 10535
IGdyZQ== 10536
IGVxdWFsaXR5 10537
IGJ1cmRlbg== 10538
IHZpZw== 10539
IExlYWRlcg== 10540
LS0tLS0tLS0tLS0t 10541
IGNlcmVtb255 10542
IGZpZ2h0ZXI= 10543
IGFjdG9ycw== 10544
IOY= 10545
YW1hbg== 10546
Rmk= 10547
IGFsaWdu 10548
cHV0ZXI= 10549
IGVsZGVy 10550
IE5TQQ== 10551
IHJlcHJlc2VudGF0aW9u 10552
IE9udGFyaW8= 10553
SVRI 10554
dXNhbGVt 10555
IGhhcmFzc21lbnQ= 10556
aXR6ZXI= 10557
IHN5bXA= 10558
IGJveGVz 10559
IERS 10560
IG1hbmlmZXN0 10561
YXRyZQ== 10562
IF4= 10563
IGRpZXM= 10564
bGV0b24= 10565
IG1pc3Npb25z 10566
ZXRoZQ== 10567
IHJlc29sdmU= 10568
IGZvbGxvd2Vycw== 10569
IGFzYw== 10570
IGtt 10571
bG9yZA== 10572
YW1tZWQ= 10573
IHNpbGVudA== 10574
IEFzc29jaWF0ZWQ= 10575
IHRpbWluZw== 10576
IHByaXNvbmVycw== 10577
IEtpbmdz 10578
IEZpdmU= 10579
IHRvd2Vy 10580
IGFwcHJvYWNoZXM= 10581
IHByZWNpc2VseQ== 10582
IGJ1cmVhdQ== 10583
IE1vdGhlcg== 10584
IElzcw== 10585
IGtleWJvYXJk 10586
aXR1YWw= 10587
IGZ1bmRlZA== 10588
IHN0YXlpbmc= 10589
IHBzeWNob2xvZ2ljYWw= 10590
IG1pbGU= 10591
IExlb24= 10592
IEJhcmI= 10593
d2lsbA== 10594
IHdpZGVy 10595
IEF0bGFudGlj 10596
IHRpbGw= 10597
IFJvbWU= 10598
cm90 10599
IGFjY29tcGFu 10600
IGZsb3Vy 10601
YWNv 10602
V29ybGQ= 10603
IEV4cHJlc3M= 10604
IFl1 10605
Q29y 10606
IHBsZWFzZWQ= 10607
cGFydHk= 10608
IHBvaW50aW5n 10609
IGluZmxhdGlvbg== 10610
IHJveQ== 10611
ICks 10612
YWluZXI= 10613
IHdlZGRpbmc= 10614
b3Jtb24= 10615
IHJlcXVpcmluZw== 10616
IHF1YWxpZmllZA== 10617
IHNlZ21lbnQ= 10618
RU5E 10619
IHNpemVz 10620
ZWFscw== 10621
IGNvcnJ1cHQ= 10622
YXNzYWRvcg== 10623
IGNlbGVi 10624
IGRyZWFtcw== 10625
IE1lc3M= 10626
IGNoZWNraW5n 10627
IFZlcnNpb24= 10628
IHByZXBhcmluZw== 10629
IGFjdGl2ZWx5 10630
IERpZmY= 10631
IGx1eA== 10632
IFdpbnRlcg== 10633
YWN0ZXJpYQ== 10634
IE5F 10635
IGRlcHV0eQ== 10636
IHRyYW5zZ2VuZGVy 10637
IHN1bW1hcnk= 10638
IGluaGVy 10639
ZXJpZXM= 10640
Y2hhcg== 10641
IFlhbg== 10642
IGtub2Nr 10643
IFBhdGg= 10644
IGxpcA== 10645
cm9sbGVy 10646
IGltcHJlc3Npb24= 10647
IGNlbGVicmF0ZQ== 10648
IHNsaWRl 10649
IGd1ZXN0cw== 10650
IGNsaXA= 10651
RlM= 10652
IHNhdmluZ3M= 10653
IGNhcHRhaW4= 10654
IGxlZ2FjeQ== 10655
IERlbnZlcg== 10656
IHdvdW5kZWQ= 10657
dGFib29sYQ== 10658
QUNU 10659
IHB1cnN1ZQ== 10660
IG94eQ== 10661
IHE= 10662
IHNlbWk= 10663
IE5lZWQ= 10664
IEFmZmFpcnM= 10665
IG9ic2M= 10666
IGNoZWNrZWQ= 10667
IGR1YWw= 10668
Q29kZQ== 10669
IE1E 10670
bGVt 10671
dWx0eQ== 10672
IMKp 10673
IEVsaXphYmV0aA== 10674
IGNlbnR1cmllcw== 10675
YXJkZWQ= 10676
c3Jj 10677
IGV2aWRlbnQ= 10678
ZW5uaXM= 10679
YXRpbg== 10680
IHVuZW1wbG95bWVudA== 10681
IE1hcmlv 10682
IGludGlt 10683
Q2hyaXN0 10684
IGJpb2xvZ2ljYWw= 10685
IHNvbGRpZXI= 10686
IEFkZGVk 10687
IG1hdGg= 10688
IEdpbA== 10689
IGJpYXM= 10690
IGRhdGluZw== 10691
IE9jZWFu 10692
IG1pY2U= 10693
TXVz 10694
aGlyZQ== 10695
IFRlcw== 10696
U2VydmVy 10697
bGltaXRlZA== 10698
U2l6ZQ== 10699
IG1ldGVycw== 10700
IHJvY2tldA== 10701
ZXNzZWU= 10702
IGNlcnRpZmljYXRl 10703
IElyYW5pYW4= 10704
QVNT 10705
IGdyaWQ= 10706
RGVj 10707
IHJvbGxpbmc= 10708
Y29tbXVu 10709
IFN3ZWRlbg== 10710
YnVyeQ== 10711
IHRpc3N1ZQ== 10712
IHJhY2lzbQ== 10713
IExvY2Fs 10714
IG15c3Rlcnk= 10715
IGV4YW1pbmU= 10716
IHN0ZW0= 10717
IHNpdHM= 10718
IGhvcGVk 10719
b3Rpbmc= 10720
IGRpYWxvZ3Vl 10721
IHBlcnN1 10722
V2F0Y2g= 10723
bGF5 10724
TUFO 10725
IGNocm9uaWM= 10726
IFBvcnRsYW5k 10727
bWFya2V0 10728
IFNFQw== 10729
IHBhcmFsbGVs 10730
IHNjYW5kYWw= 10731
IGNhcnJpZXM= 10732
IHBoZW5vbWVub24= 10733
aHVtYW4= 10734
YWNrZXI= 10735
IE94 10736
IHJldGlyZW1lbnQ= 10737
dGFpbm1lbnQ= 10738
b3ZpZQ== 10739
IEdlYXI= 10740
IGR1dGllcw== 10741
IGRvc2U= 10742
IHNjcm9sbA== 10743
TUI= 10744
aW5m 10745
IHNhdWNl 10746
IGxhbmRzY2FwZQ== 10747
cmVkZGl0 10748
IENoYW1waW9uc2hpcA== 10749
IFJlZGRpdA== 10750
YWxpZA== 10751
IGNvaW4= 10752
IG92ZXJz 10753
IHBvc3Rpbmc= 10754
YWJvdXQ= 10755
IGZlbA== 10756
YW5keQ== 10757
IGJvbGQ= 10758
IGZvY3VzaW5n 10759
ZWZmZWN0 10760
R1I= 10761
IGRlZW1lZA== 10762
IHJlY29tbWVuZGF0aW9ucw== 10763
IHN0ZXBwZWQ= 10764
IHZvdGVy 10765
IERlZXA= 10766
IEluc3RhZ3JhbQ== 10767
IG1vZGVyYXRl 10768
IE1hcnlsYW5k 10769
IHJlc3RyaWN0ZWQ= 10770
IE1C 10771
IENoYWxs 10772
IHRvYg== 10773
IGNpcg== 10774
IE9jYw== 10775
IEV2ZXI= 10776
IGNvbGxhcHM= 10777
SU5GTw== 10778
PS0= 10779
IFBpY3Q= 10780
IEFjY291bnQ= 10781
bmM= 10782
IG91Z2h0 10783
IGV4cG9ydA== 10784
IGRydW5r 10785
KCc= 10786
IHdpc2U= 10787
IE1vcnQ= 10788
bmVjZXNz 10789
IGFuY2VzdA== 10790
IEluY3Jl 10791
IGZyZXF1ZW50 10792
bWly 10793
IGludGVycHJldGF0aW9u 10794
IGRlcGVuZGVudA== 10795
IGNvaW5z 10796
IEJvbA== 10797
VmlkZW8= 10798
IEp1c3Rpbg== 10799
IGZhdGFs 10800
IGNvb2tpbmc= 10801
IGNvbmZ1c2lvbg== 10802
aXBoZXI= 10803
IGN1c3RvZHk= 10804
IE1vcmdhbg== 10805
b21hY2g= 10806
IEdvdmVybm9y 10807
IHJlc3RhdXJhbnRz 10808
ZWxpbmc= 10809
IGFja25vd2xlZGdlZA== 10810
IHRoZXI= 10811
IGdlbmVz 10812
Y2hpbmc= 10813
SGV5 10814
IHRhY3RpY3M= 10815
IE1leGljYW4= 10816
IHZlbmQ= 10817
IGhlcw== 10818
cXVlcg== 10819
IG5vdGluZw== 10820
IENhbWVyb24= 10821
IHRhcmdldGluZw== 10822
cm9jaw== 10823
IGNyZWRpdHM= 10824
IGVtb3Rpb25z 10825
IHJlcHJlc2VudGF0aXZlcw== 10826
bmV3cw== 10827
IGxlZ2lzbGF0aXZl 10828
IHJlbW92aW5n 10829
IHR3ZWV0ZWQ= 10830
IENhcnRlcg== 10831
IEZpeGVk 10832
IGZvcmNpbmc= 10833
IHNwZWFrZXI= 10834
IG1hbGVz 10835
IFZpZXRuYW0= 10836
bGluZWQ= 10837
IGNvbmNlcHRz 10838
IHZvaWNlcw== 10839
b2ly 10840
IFRyaWI= 10841
V2hl 10842
IEplcnVzYWxlbQ== 10843
IFNhbnQ= 10844
IGN1bA== 10845
IGxhZHk= 10846
IEhhd2Fp 10847
IGFydHM= 10848
IElubg== 10849
IE1hY2hpbmU= 10850
IEVtcGVyb3I= 10851
IHNsb3Q= 10852
Z2x5 10853
IFByb2Nlc3M= 10854
SUlJ 10855
IGF0aGxldGVz 10856
IFRlbXBsZQ== 10857
IFJlcHJlc2VudA== 10858
IHByZXNj 10859
IHRvbnM= 10860
IGdvbGRlbg== 10861
IHB1bmNo 10862
IEdS 10863
aXZlcnBvb2w= 10864
IGVuYWN0 10865
IGxvYmJ5 10866
IG1vcw== 10867
IHBpY2tpbmc= 10868
IGxpZmV0aW1l 10869
IGNvZ25pdGl2ZQ== 10870
RWFjaA== 10871
em8= 10872
IGR1Yg== 10873
IGNvbnNpc3Rz 10874
b2xu 10875
IGZlc3RpdmFs 10876
YW1vdXM= 10877
IGludGVsbGln 10878
d29yZHM= 10879
IFNtYXJ0 10880
IGRlbGU= 10881
IGxhcHQ= 10882
IG1hZ2ljYWw= 10883
IFNpbg== 10884
YnVz 10885
dXJpdGllcw== 10886
aWdodGg= 10887
IFJ1Ynk= 10888
IFN1cmU= 10889
b2x2aW5n 10890
IGp1bg== 10891
T1NU 10892
IGltcG9zZWQ= 10893
IGFzdHJvbg== 10894
IGNvcnJlbA== 10895
IE5T 10896
IEtpdA== 10897
IEZ1dHVyZQ== 10898
YnVybg== 10899
IGltbXVuZQ== 10900
b2N1cw== 10901
IGNvdXJzZXM= 10902
IFN0cmluZw== 10903
IGxlYW4= 10904
IGdob3N0 10905
IG91dGNvbWVz 10906
IGV4cGVuc2U= 10907
IGV2ZXJ5ZGF5 10908
IGFjY2VwdGFibGU= 10909
QWg= 10910
IGVxdWlwcGVk 10911
IG9yYW5nZQ== 10912
RlI= 10913
IER1dGNo 10914
VGhvdWdo 10915
IFJhbms= 10916
UVU= 10917
IFJvYmVydHM= 10918
d2hhdA== 10919
cmVuZA== 10920
IGRpc2FwcGVhcg== 10921
IHNwYXdu 10922
IExhbQ== 10923
b2lz 10924
IGRlc2VydmU= 10925
IG1pbmltYWw= 10926
IG5lcnZvdXM= 10927
IFdvdWxk 10928
IHJvb2s= 10929
IFZhbmNvdXZlcg== 10930
IHJlc2lnbg== 10931
c2hpcmU= 10932
IFdvcmtz 10933
IEJ1aWxk 10934
IGFmZm9yZGFibGU= 10935
IEdhcnk= 10936
IEFyZW5h 10937
IGhhbmdpbmc= 10938
IGltcGxpY2F0aW9ucw== 10939
IFNvbmc= 10940
IG1haW50YWluaW5n 10941
IGd1YXJkcw== 10942
Q09O 10943
IGRlcml2ZWQ= 10944
IGV4ZWN1dGVk 10945
IHRoZW9yaWVz 10946
IHF1b3RlZA== 10947
IEFuZHJl 10948
b2dh 10949
c2VsZXNz 10950
aW5mbw== 10951
IEJlbGc= 10952
IHRlYXJz 10953
IFN1cnY= 10954
IGJpcnRoZGF5 10955
aWdpb3Vz 10956
aW1tZXI= 10957
IHNwZWN0cnVt 10958
IGFyY2hpdGVjdHVyZQ== 10959
IHJlY3J1aXQ= 10960
YXJtYQ== 10961
VGFibGU= 10962
IG1vbnN0ZXJz 10963
IEdvdg== 10964
IGRlc3RpbmF0aW9u 10965
IGF0dHJhY3RpdmU= 10966
IGZvc3M= 10967
IE1vcmVvdmVy 10968
IHByZXNlbnRz 10969
VEhF 10970
IHJlcGx5 10971
cHRvbg== 10972
IGN1bQ== 10973
IGRlbGlnaHQ= 10974
IGFmZmVjdHM= 10975
IGRvbmF0aW9ucw== 10976
IFRveQ== 10977
IEhpbQ== 10978
TUVOVA== 10979
IG92ZXJjb21l 10980
aXRjaGVk 10981
IEZhbnRhc3k= 10982
IEhhdA== 10983
IEJlYXN0 10984
Ym90dA== 10985
IGludmVzdGlnYXRpb25z 10986
UnVu 10987
IGh1bnRpbmc= 10988
ZGk= 10989
ZnVuZA== 10990
IHNlc3Npb25z 10991
ZXN0eWxl 10992
IHBvcnRyYXk= 10993
b2lkcw== 10994
WWVhaA== 10995
IGNvbW11bmljYXRl 10996
IGNvbWVkeQ== 10997
IFlhbmc= 10998
IGJlbHQ= 10999
IE1hcmluZQ== 11000
IHByZWRpY3RlZA== 11001
UGxheQ== 11002
IGltcG9ydGFudGx5 11003
IHJlbWFya2FibGU= 11004
IGVsaW1pbmF0ZQ== 11005
RGF2aWQ= 11006
IGJpbmQ= 11007
VklE 11008
IGFkdm9jYXRlcw== 11009
IEdhemE= 11010
aW1w 11011
REI= 11012
IE5h 11013
IFNpbWlsYXI= 11014
SUVT 11015
IGNoYXJpdHk= 11016
dmFz 11017
bWF0aA== 11018
IOKW 11019
b2tlcg== 11020
bmR1bQ== 11021
IGNhcHM= 11022
IEhhbA== 11023
MjAwMA== 11024
ZWFu 11025
IGZsZWV0 11026
IHJlY3Jl 11027
UmlnaHQ= 11028
IHNsZWVwaW5n 11029
aWppbmc= 11030
a2luZA== 11031
IGRlc2lnbmF0ZWQ= 11032
w6Q= 11033
IGFuaW1hdGlvbg== 11034
a2Vl 11035
IEludHJvZHU= 11036
IC8+ 11037
IGRlbGF5ZWQ= 11038
IHRyZW1lbmQ= 11039
IGN1cmlvdXM= 11040
VXNl 11041
IGxlY3Q= 11042
ZGFt 11043
IGlubm92YXRpb24= 11044
IFBvaW50cw== 11045
IGxvYWRpbmc= 11046
IGRpc3B1dGU= 11047
Y3RpYw== 11048
aXJkcw== 11049
IEJZ 11050
IG51cnM= 11051
IFZhbHVl 11052
SU9OUw== 11053
IEh1bQ== 11054
IHRlbXBsYXRl 11055
bWVycw== 11056
IGFwcGVhcmFuY2Vz 11057
IEVudGVydGFpbm1lbnQ= 11058
IHRyYW5zbGF0aW9u 11059
IHNha2U= 11060
IGJlbmVhdGg= 11061
IGluaGli 11062
IGV1cm8= 11063
YWJldGVz 11064
IHN0dWR5aW5n 11065
IE1hcw== 11066
IHBlcmNlaXZlZA== 11067
IGV4YW1pbmVk 11068
IGVhZ2Vy 11069
IGNvYWNoZXM= 11070
IGltcGVy 11071
Y2hp 11072
IHByb2R1Y2Vz 11073
Iiku 11074
IEV2ZXJ5b25l 11075
IG11bmljaXA= 11076
IGdpcmxmcmllbmQ= 11077
IGhpcmU= 11078
IFZpY2U= 11079
IHN1aXRhYmxl 11080
b3B5 11081
IGluZXF1 11082
IER1a2U= 11083
ZmlzaA== 11084
Zmlyc3Q= 11085
IE9icw== 11086
IGludGVyaW9y 11087
IEJydWNl 11088
IFJ5 11089
IGFuYWx5cw== 11090
IGNvbnNpZGVyYWJsZQ== 11091
IGZvcmVjYXN0 11092
IGZlcnQ= 11093
b3JzaGlw 11094
IERydWc= 11095
IEFMTA== 11096
OiI= 11097
dGh1cg== 11098
IE1haWw= 11099
IGJhbGxvdA== 11100
IGluc3RhbnRseQ== 11101
IENoYW5uZWw= 11102
IHBpY2tz 11103
IDE5ODk= 11104
IHRlbnQ= 11105
b2xp 11106
IGNpdmlsaWFu 11107
Ymxpbmc= 11108
ZWxsbw== 11109
YnU= 11110
IGluY2g= 11111
IGxvZ28= 11112
IGNvb3BlcmF0aW9u 11113
IHdhbGtz 11114
IGludmVzdG1lbnRz 11115
IGltcHJpc29u 11116
IEZlc3RpdmFs 11117
IEt5 11118
IGxlZ2FsbHk= 11119
IGdyaQ== 11120
Y2hhcmc= 11121
U2w= 11122
IHRocmVhdGVuaW5n 11123
ZHVjdGlvbg== 11124
Zmxvdw== 11125
IGRpc21pc3NlZA== 11126
aWJyYXJpZXM= 11127
Y2Fw 11128
ZWxl 11129
IE1jRw== 11130
IEhhcnZhcmQ= 11131
IENvbnNlcnZhdGl2ZQ== 11132
IENCUw== 11133
cG5n 11134
IHJvb3Rz 11135
IEhhdmluZw== 11136
dW1ibGVk 11137
IEZ1bg== 11138
XC8= 11139
IFNlYXJjaA== 11140
cGxleA== 11141
IGRpc2N1c3Npbmc= 11142
IGNvbnRpbnU= 11143
IFRhaQ== 11144
IFdpaw== 11145
RnJlZQ== 11146
Zml0 11147
IHJlZnVzZQ== 11148
IG1hbmFnaW5n 11149
IHN5bmQ= 11150
aXBlZGlh 11151
d2Fsaw== 11152
IHByb2Zlc3Npb25hbHM= 11153
IGd1aWRhbmNl 11154
IHVuaXZlcnNpdGllcw== 11155
IGFzc2VtYg== 11156
dW50dQ== 11157
RmluYWxseQ== 11158
QVNF 11159
IEF1dG8= 11160
IEhhZA== 11161
IGFubml2ZXJzYXJ5 11162
TEQ= 11163
IER1cg== 11164
IFVsdGltYXRl 11165
aWhhZA== 11166
cHJvZHVjdA== 11167
IHRyYW5zaXQ= 11168
IHJlc3RvcmU= 11169
IGV4cGxhaW5pbmc= 11170
IGFzc2V0 11171
IHRyYW5zZmVycmVk 11172
IGJ1cnN0 11173
YXBvbGlz 11174
IE1hZ2F6aW5l 11175
IENyYQ== 11176
IEJS 11177
Z2dlZA== 11178
IEhF 11179
TWljaA== 11180
YmV0 11181
IExhZHk= 11182
eWx1bQ== 11183
ZXJ2ZXM= 11184
IG1lZXRz 11185
d2hpdGU= 11186
TG9n 11187
IGNvcnJlc3BvbmRpbmc= 11188
IGluc2lzdGVk 11189
R0c= 11190
IHN1cnJvdW5kZWQ= 11191
IHRlbnM= 11192
IGxhbmU= 11193
IGNvaW5j 11194
aG9tZQ== 11195
IGV4aXN0ZWQ= 11196
ZWN0ZWQ= 11197
IERvdWJsZQ== 11198
bGFtbQ== 11199
IHNrZXB0 11200
ZXhw 11201
IHBlcmNlcHRpb24= 11202
aWV2 11203
IEJlaW5n 11204
b2Z0 11205
IGFkb3B0 11206
Ljo= 11207
XTs= 11208
V2luZG93cw== 11209
IHNhdGVsbGl0ZQ== 11210
QVNI 11211
IGluZmFudA== 11212
ZGVzY3JpcHRpb24= 11213
IE1lYW53aGlsZQ== 11214
Y20= 11215
b2Nh 11216
IFRyZWF0 11217
YWN0b3I= 11218
IHRvYmFjY28= 11219
IE5vcm0= 11220
ZW1wdGlvbg== 11221
IGZsZXNo 11222
IGpl 11223
b29w 11224
IEhlYXZlbg== 11225
IGJlYXRpbmc= 11226
YW5pbQ== 11227
IGdhdGhlcmluZw== 11228
IGN1bHRpdg== 11229
R08= 11230
YWJl 11231
IEpvbmF0aGFu 11232
IFNhZmV0eQ== 11233
IGJhZGx5 11234
cHJvdA== 11235
IGNob29zaW5n 11236
IGNvbnRhY3RlZA== 11237
IHF1aXQ= 11238
IGRpc3R1cg== 11239
IHN0aXI= 11240
IHRva2Vu 11241
RGV0 11242
IFBh 11243
IGZ1bmN0aW9uYWxpdHk= 11244
MDAz 11245
c29tZQ== 11246
IGxpbWl0YXRpb25z 11247
IG1ldGg= 11248
YnVpbGQ= 11249
Y29uZmln 11250
TlQ= 11251
cmVsbA== 11252
YmxlbQ== 11253
IE1vbQ== 11254
IHZldGVyYW5z 11255
IEh1 11256
IHRyZW5kcw== 11257
YXJlcg== 11258
IEdpdmVu 11259
IENhcHRpb24= 11260
bWF5 11261
QVNU 11262
IHdvbmRlcmluZw== 11263
IENsYXJr 11264
bm9ybWFs 11265
IHNlcGFyYXRlZA== 11266
IGRlc3A= 11267
c3RpYw== 11268
YnJldw== 11269
IHJlbGF0aW5n 11270
IE5paw== 11271
IEZhcm0= 11272
IGVudGh1c2k= 11273
Z29vZA== 11274
ZGVi 11275
IGFjdGl2aXN0 11276
IG1hcnQ= 11277
IGV4cGxvc2lvbg== 11278
IEVjb25vbWlj 11279
TGluaw== 11280
IGluc2lnaHQ= 11281
IGNvbnZlbmllbnQ= 11282
IGNvdW50ZXJwYXJ0 11283
c3VwcG9ydA== 11284
IFZpcnQ= 11285
YWdlbg== 11286
IFRlbm5lc3NlZQ== 11287
IFNpbW9u 11288
IEF3YXJk 11289
T0NL 11290
IEZpZ3VyZQ== 11291
IG92ZXJzZWFz 11292
IHByaWRl 11293
IENhcw== 11294
bm90ZQ== 11295
bWc= 11296
Q3VycmVudA== 11297
IGRpc3BsYXlz 11298
Y29udGVudA== 11299
IHRyYXZlbGluZw== 11300
IGhvc3BpdGFscw== 11301
IEZpbmFuY2lhbA== 11302
IFBhc3Q= 11303
IGRlZmVuZGFudA== 11304
IHN0cmVhbWluZw== 11305
bWJsZQ== 11306
IEJlcmxpbg== 11307
dWtp 11308
IGRpc3RyaWJ1dA== 11309
IGFudGli 11310
IGNob2NvbGF0ZQ== 11311
IENhc3RsZQ== 11312
IGludGVycnVwdA== 11313
IFJvdw== 11314
IGNvbnZlcnNpb24= 11315
IGJ1Z3M= 11316
IFJhdGhlcg== 11317
bGllc3Q= 11318
TFk= 11319
IEplYW4= 11320
Y29tbW9u 11321
YWto 11322
IDEzMA== 11323
b3R0b24= 11324
IERlYW4= 11325
IGFtZW5kbWVudA== 11326
IGdhbWVwbGF5 11327
IFdhcnJlbg== 11328
b2Rh 11329
IGhpZ2hsaWdodHM= 11330
IGlycmU= 11331
IE5BVE8= 11332
IGJhbGxz 11333
IGRlbWFuZGluZw== 11334
VVJF 11335
IEx1a2U= 11336
RmlndXJl 11337
c3RvcA== 11338
b25pYQ== 11339
em9uZQ== 11340
aXplcnM= 11341
IFdS 11342
IGF3YXJkZWQ= 11343
IHJlZ3VsYXRvcnk= 11344
IEhhcnQ= 11345
IFNO 11346
cGxpbmc= 11347
IHNvdXI= 11348
IFBpeGVs 11349
dXNpdmU= 11350
IGZldA== 11351
IFNlbnQ= 11352
IGF1dG9tYXRpYw== 11353
IGZlcg== 11354
dmVybm1lbnQ= 11355
IEtoYW4= 11356
VE9O 11357
ZmF0aGVy 11358
IGV4dHJhb3JkaW5hcnk= 11359
dGhyb3A= 11360
IFB5dGhvbg== 11361
IEdQVQ== 11362
IHNleHVhbGx5 11363
IGRlc2t0b3A= 11364
aXRpdml0eQ== 11365
IEFudG9uaW8= 11366
IG9yaWVudA== 11367
IGVhcnM= 11368
b2JieQ== 11369
b3VzZXM= 11370
dmVydGlzZW1lbnRz 11371
IG1hbnVmYWN0dXJlcnM= 11372
aWNpZW50 11373
bWludXRl 11374
IGNvbnZpY3Rpb24= 11375
IGdhcmRlbg== 11376
cHVibGlj 11377
IHNhdGlzZmllZA== 11378
Zm9sZA== 11379
T0s= 11380
IGluaGFi 11381
IFRoaW5r 11382
IHByb2dyYW1tZQ== 11383
IHN0b21hY2g= 11384
IGNvb3JkaW4= 11385
IGhvbHk= 11386
IHRocmVzaG9sZA== 11387
IHJoZXQ= 11388
IHNlcmlhbA== 11389
IGVtcGxveWVycw== 11390
IEV2ZXJ5dGhpbmc= 11391
cmFo 11392
IGJvdGhlcg== 11393
IGJyYW5kcw== 11394
VmFsdWU= 11395
IFRlZA== 11396
IFBsYW5ldA== 11397
IHBpbms= 11398
IEZ1cnRoZXJtb3Jl 11399
c2E= 11400
UEU= 11401
cmVjaw== 11402
IFVTRA== 11403
b3R0ZQ== 11404
ICYm 11405
IGxhbmRlZA== 11406
Z2V0cw== 11407
IHByb2R1Y2Vycw== 11408
IGhlYWx0aGNhcmU= 11409
IGRvbWluYW50 11410
IGRlc3Rybw== 11411
IGFtZW5kZWQ= 11412
Y2hyb24= 11413
IGZpdHM= 11414
IFN5ZA== 11415
IEF1dGhvcml0eQ== 11416
QVRDSA== 11417
IGZpZ2h0cw== 11418
IExMQw== 11419
IC0tLQ== 11420
IENvcnA= 11421
IHRveGlj 11422
c3BlY2lmaWM= 11423
IENvcm4= 11424
IENoZWw= 11425
IHRlbGVwaG9uZQ== 11426
IFBhbnQ= 11427
IG15c3RlcmlvdXM= 11428
YXVuY2g= 11429
b2RveA== 11430
bWVkaWE= 11431
IHdpdG5lc3Nlcw== 11432
YWd1 11433
IHF1ZXN0aW9uZWQ= 11434
IEJyZXhpdA== 11435
IFJlbWVtYmVy 11436
ZW5leg== 11437
IGVuZG9yc2U= 11438
aWF0cmlj 11439
IElkZW50 11440
IHJpZGljdWxvdXM= 11441
MTEw 11442
IHByYXllcg== 11443
IHNjaWVudGlzdA== 11444
IDE5NTA= 11445
IEFxdQ== 11446
IHVuZGVyZ3JvdW5k 11447
IFVGQw== 11448
bWFyZQ== 11449
IExhdGVy 11450
d2ljaA== 11451
IHN1YnNjcmli 11452
IGhvc3Rz 11453
IGVycg== 11454
IGdyYW50cw== 11455
YW50b20= 11456
IHN1bW1vbg== 11457
ZWFybHk= 11458
IENsZWFy 11459
IFByaW0= 11460
IHN1c3BlbnNpb24= 11461
IGd1YXJhbnRlZWQ= 11462
YXBwZXI= 11463
IHJpY2U= 11464
IFNlYW4= 11465
IFNoaW4= 11466
IHJlZmVyZW5kdW0= 11467
IGZsZWQ= 11468
cnVzdA== 11469
IDM2MA== 11470
dGVyeQ== 11471
IHNob2NrZWQ= 11472
QlI= 11473
IE9pbA== 11474
IEFsbGFo 11475
IHBhcnRseQ== 11476
IGlnbm9y 11477
IHRyYW5zbWlzc2lvbg== 11478
IGhvbW9zZXh1YWw= 11479
aXZlcnNhbA== 11480
IGhvcGVmdWxseQ== 11481
44Kk 11482
IGxlc3Nvbg== 11483
TGVn 11484
IC4u 11485
WWV0 11486
dGFibGU= 11487
YXBwcm9wcmk= 11488
cmV0dA== 11489
IGJvYXJkcw== 11490
IGluY29ycmVjdA== 11491
IGJhY3Rlcmlh 11492
YXJ1 11493
YW1hYw== 11494
IHNuYXA= 11495
Lici 11496
IHBhcmFk 11497
dGVt 11498
aGVhcnQ= 11499
IGF2YWlsYWJpbGl0eQ== 11500
IHdpc2RvbQ== 11501
ICgr 11502
IHByaWVzdA== 11503
IMKgIMKg 11504
T3Blbg== 11505
IHNwYW4= 11506
IHBhcmFtZXRlcg== 11507
IGNvbnZpbmNl 11508
ICglKQ== 11509
cmFj 11510
IGZv 11511
IHNhZmVseQ== 11512
IGNvbnZlcnRlZA== 11513
IE9seW1waWM= 11514
IHJlc2VydmU= 11515
IGhlYWxpbmc= 11516
IE1pbmU= 11517
TWF4 11518
IGluaGVyZW50 11519
IEdyYWhhbQ== 11520
IGludGVncmF0ZWQ= 11521
RGVt 11522
IHBpcGVsaW5l 11523
IGFwcGx5aW5n 11524
IGVtYmVk 11525
IENoYXJsaWU= 11526
IGNhdmU= 11527
MjAwOA== 11528
IGNvbnNlbnN1cw== 11529
IHJld2FyZHM= 11530
UGFs 11531
IEhUTUw= 11532
IHBvcHVsYXJpdHk= 11533
bG9va2luZw== 11534
IFN3b3Jk 11535
IEFydHM= 11536
Jyk= 11537
IGVsZWN0cm9u 11538
Y2x1c2lvbnM= 11539
IGludGVncml0eQ== 11540
IGV4Y2x1c2l2ZWx5 11541
IGdyYWNl 11542
IHRvcnR1cmU= 11543
IGJ1cm5lZA== 11544
dHdv 11545
IDE4MA== 11546
UHJvZHU= 11547
IGVudHJlcHJlbmU= 11548
cmFwaGljcw== 11549
IGd5bQ== 11550
cmljYW5l 11551
IFRhbQ== 11552
IGFkbWluaXN0cmF0aXZl 11553
IG1hbnVmYWN0dXJlcg== 11554
IHZlbA== 11555
IE5p 11556
IGlzb2xhdGVk 11557
IE1lZGljaW5l 11558
IGJhY2t1cA== 11559
IHByb21vdGluZw== 11560
IGNvbW1hbmRlcg== 11561
IGZsZWU= 11562
IFJ1c3NlbGw= 11563
IGZvcmdvdHRlbg== 11564
IE1pc3NvdXJp 11565
IHJlc2lkZW5jZQ== 11566
bW9ucw== 11567
IHJlc2VtYg== 11568
IHdhbmQ= 11569
IG1lYW5pbmdmdWw= 11570
UFQ= 11571
IGJvbA== 11572
IGhlbGlj 11573
IHdlYWx0aHk= 11574
IHJpZmxl 11575
c3Ryb25n 11576
cm93aW5n 11577
cGxhbg== 11578
YXN1cnk= 11579
4oCmLg== 11580
IGV4cGFuZGluZw== 11581
IEhhbWlsdG9u 11582
IHJlY2VpdmVz 11583
U0k= 11584
ZWF0dXJlcw== 11585
IEFuaW0= 11586
UkVF 11587
UHV0 11588
IGJyaWVmbHk= 11589
cml2ZQ== 11590
IHN0aW11bA== 11591
IGBgKA== 11592
IF9f 11593
IGNoaXA= 11594
IGhheg== 11595
IHByaXpl 11596
IFRoaW5ncw== 11597
QUNF 11598
dWxpbg== 11599
ZGljdA== 11600
b2t1 11601
IGFzc29jaWF0ZQ== 11602
b2NrZXRz 11603
eW91dHViZQ== 11604
U3Rvcnk= 11605
YXRlZ29yeQ== 11606
IG1pbGQ= 11607
YWlsaW5n 11608
IFll 11609
T3JpZw== 11610
IEth 11611
b3JpZw== 11612
IHByb3BhZ2FuZGE= 11613
IGFub255bW91cw== 11614
IHN0cnVnZ2xlZA== 11615
IG91dHJhZ2U= 11616
QVRFRA== 11617
IEJlaWppbmc= 11618
cmFyeQ== 11619
IGxlYXRoZXI= 11620
IHdvcmxkcw== 11621
IGJyb2FkZXI= 11622
MTI1 11623
aWRhbA== 11624
IEJldHRlcg== 11625
IHRlYXI= 11626
RXh0 11627
IHByb3Bvc2Fscw== 11628
IGl0ZXI= 11629
IFNxdWFk 11630
IHZvbHVudA== 11631
bWk= 11632
RGlk 11633
IFB1 11634
cGlu 11635
IHNwZWFrZXJz 11636
IGJvcmRlcnM= 11637
IGZpZ3VyZWQ= 11638
PSc= 11639
IHNpbXVsdGFuZW91c2x5 11640
YWVkYQ== 11641
IGNoYXJnaW5n 11642
IHVyZ2Vk 11643
IGNvbmo= 11644
MjU2 11645
IEdvcmRvbg== 11646
bWVyY2U= 11647
IGRvY3VtZW50YXJ5 11648
U2hhcmU= 11649
aXRvbA== 11650
T05F 11651
IEdhcmRlbg== 11652
aGF0dA== 11653
IFRob21wc29u 11654
YW5lb3Vz 11655
YXBvcmU= 11656
IHRhbmtz 11657
IGxlc3NvbnM= 11658
dHJhY2s= 11659
IG91dHN0YW5kaW5n 11660
IHZvbHVudGVlcnM= 11661
IHNwcmF5 11662
IG1hbmFnZXJz 11663
bGFyZ2U= 11664
IGNhbXBz 11665
IGFydGlmaWNpYWw= 11666
IFJ1 11667
IGJhZ3M= 11668
dGhhbA== 11669
IGNvbXBhdGlibGU= 11670
IEJsYWRl 11671
IGZlZA== 11672
IGFyZ3Vlcw== 11673
Rkk= 11674
IHVuZmFpcg== 11675
IGNvcm4= 11676
IG9mZnNldA== 11677
IGRpcmVjdGlvbnM= 11678
IGRpc2FwcG9pbnRlZA== 11679
IENvbnZlbnRpb24= 11680
IHZpZXdpbmc= 11681
TUU= 11682
b2NpdHk= 11683
IHRvd25z 11684
IGxheWVycw== 11685
IHJvbGxlZA== 11686
IGp1bXBlZA== 11687
IGF0dHJpYnV0ZQ== 11688
IHVubmVjZXNz 11689
aW5jb2xu 11690
IHN1cHBvc2U= 11691
IE5ldGhlcg== 11692
Y2hh 11693
IGJ1cmllZA== 11694
IHNpeHRo 11695
QmVu 11696
cmVzc2luZw== 11697
T1VS 11698
IHdvdW5k 11699
IGN5Y2w= 11700
IG1lY2hhbmlzbXM= 11701
IGNvbmdyZXNzaW9uYWw= 11702
IEVsZW1lbnQ= 11703
IGFncmVlbWVudHM= 11704
IGRlY29y 11705
IGNsb3Nlc3Q= 11706
IE1pdA== 11707
R29vZ2xl 11708
fX0= 11709
IG1peHR1cmU= 11710
IGZsdWlk 11711
U2lnbg== 11712
IFNjaG9sYXI= 11713
IHBpc3Q= 11714
YXNrZXQ= 11715
YWJsaW5n 11716
IHJhY2luZw== 11717
aGVybw== 11718
cmllbA== 11719
YXNzeQ== 11720
IGNoZWFwZXI= 11721
YmVu 11722
IHZlcnRpY2Fs 11723
YW1hY2FyZQ== 11724
IFJlYWRpbmc= 11725
Z21lbnRz 11726
IGhlbGljb3A= 11727
IHNhY3JpZmljZQ== 11728
YXlh 11729
cGFyZW4= 11730
VkE= 11731
IExlcw== 11732
IFN0dWRpbw== 11733
IHZpb2xhdGlvbnM= 11734
IEFubmE= 11735
YWNlcg== 11736
6b4= 11737
IFJhdA== 11738
IEJlY2s= 11739
IERpY2s= 11740
IEFDVA== 11741
IGNvbXBvc2l0aW9u 11742
IHRleHR1cmU= 11743
IE93bg== 11744
IHNtYXJ0cGhvbmU= 11745
IE5B 11746
IGZvcmI= 11747
aW1wb3J0 11748
IGRlZmVuZGluZw== 11749
aWxzdA== 11750
cmVy 11751
IG9o 11752
IEplcmVteQ== 11753
IGJhbmtpbmc= 11754
Y2VwdGlvbnM= 11755
IHJlc3BlY3RpdmU= 11756
Ly4= 11757
IGRyaW5rcw== 11758
IFdp 11759
IGJhbmRz 11760
IExpdmVycG9vbA== 11761
IGdyaXA= 11762
IEJ1eQ== 11763
IG9wZW5seQ== 11764
IHJldmlld2Vk 11765
cGVydA== 11766
IHZlcmlmeQ== 11767
IENvbGU= 11768
IFdhbGVz 11769
TU8= 11770
IHVucHJl 11771
IHNoZWx0ZXI= 11772
IEltcGVyaWFs 11773
IGd1aQ== 11774
IERhaw== 11775
IHN1Z2dlc3Rpb25z 11776
IGV4cGxpY2l0bHk= 11777
IHNsYXZl 11778
IGJsb2NrY2hhaW4= 11779
IGNvbXBldGluZw== 11780
IHByb21pc2luZw== 11781
U09O 11782
IHNvY2Nlcg== 11783
IGNvbnN0aXR1dGlvbg== 11784
NDI5 11785
IGRpc3RyYWN0 11786
IFVzZXI= 11787
ZXNpZGVz 11788
IE1ldGhvZA== 11789
IFRva3lv 11790
IGFjY29tcGFuaWVk 11791
Q2xpZW50 11792
c3Vy 11793
YWxvZw== 11794
IGlkZW50aWZpY2F0aW9u 11795
IGludmFzaW9u 11796
YXNtYQ== 11797
IGluZHVzdHJpZXM= 11798
cHBlcnM= 11799
IHN1YnRsZQ== 11800
IFVuaXQ= 11801
bmF0dXJhbA== 11802
IHN1cnZpdmVk 11803
IGZsYXc= 11804
mIU= 11805
IEhvbGw= 11806
IGRlZmljaXQ= 11807
IHR1dG9yaWFs 11808
IENoYW5jZQ== 11809
IGFyZ3Vpbmc= 11810
IGNvbnRlbXBvcmFyeQ== 11811
IGludGVncmF0aW9u 11812
Zm9yd2FyZA== 11813
IHR1bQ== 11814
aXRpcw== 11815
IGhpZGluZw== 11816
IERvbWlu 11817
IFRhbg== 11818
IEJ1aWxkaW5n 11819
IFZpbg== 11820
IHNwb2tlc3BlcnNvbg== 11821
IE5vdGVz 11822
IGVtZXJnaW5n 11823
IHByZXBhcmF0aW9u 11824
IHByb3N0 11825
IHN1c3BlY3Rz 11826
IGF1dG9ub20= 11827
RGVzY3JpcHRpb24= 11828
IGRlYWx0 11829
IFBlYXI= 11830
IHN0ZWFkeQ== 11831
IGRlY3JlYXNlZA== 11832
IHNvdmVyZQ== 11833
IENsaW4= 11834
IGdyYWR1YWxseQ== 11835
b3JzZXM= 11836
IFdBUg== 11837
U2Vydg== 11838
44Ki 11839
aHI= 11840
IGRpcnR5 11841
IEJhcm4= 11842
IEJD 11843
IGRpbA== 11844
IGNhbGVuZGFy 11845
IGNvbXBsaWFuY2U= 11846
IGNoYW1iZXI= 11847
YmI= 11848
IHBhc3Nlbmdlcg== 11849
YXRlZnVs 11850
IFRpdGxl 11851
IFN5ZG5leQ== 11852
IEdvdA== 11853
IGRhcmtuZXNz 11854
IGRlZmVjdA== 11855
IHBhY2tlZA== 11856
YXNzaW9u 11857
IGdvZHM= 11858
IGhhcnNo 11859
SUNL 11860
bGVhbnM= 11861
IGFsZ29yaXRobQ== 11862
IG94eWdlbg== 11863
IHZpc2l0cw== 11864
IGJsYWRl 11865
IGtpbG9tZXQ= 11866
IEtlbnR1Y2t5 11867
IGtpbGxlcg== 11868
UGFjaw== 11869
ZW5ueQ== 11870
IGRpdmluZQ== 11871
IG5vbWluYXRpb24= 11872
YmVpbmc= 11873
IGVuZ2luZXM= 11874
IGNhdHM= 11875
IGJ1ZmZlcg== 11876
IFBoaWxs 11877
IHRyYWZm 11878
QUdF 11879
IHRvbmd1ZQ== 11880
IHJhZGlhdGlvbg== 11881
ZXJlcg== 11882
bWVt 11883
IEV4cGxpY2l0 11884
6b6N 11885
IGNvdXBsZXM= 11886
IHBoeXNpY3M= 11887
IE1jSw== 11888
IHBvbGl0aWNhbGx5 11889
YXdrcw== 11890
IEJsb29t 11891
IHdvcnNoaXA= 11892
ZWdlcg== 11893
dXRlcg== 11894
IEZP 11895
IG1hdGhlbWF0 11896
IHNlbnRlbmNlZA== 11897
IGRpc2s= 11898
IE1hcmc= 11899
IC8q 11900
UEk= 11901
IG9wdGlvbmFs 11902
IGJhYmllcw== 11903
IHNlZWRz 11904
IFNjb3R0aXNo 11905
IHRoeQ== 11906
XV0= 11907
IEhpdGxlcg== 11908
UEg= 11909
bmd0aA== 11910
IHJlY292ZXJlZA== 11911
aW5nZQ== 11912
IHBvd2Rlcg== 11913
IGxpcHM= 11914
IGRlc2lnbmVy 11915
IGRpc29yZGVycw== 11916
IGNvdXJhZ2U= 11917
IGNoYW9z 11918
In0seyI= 11919
IGNhcnJpZXI= 11920
YmFibHk= 11921
SGlnaA== 11922
IFJU 11923
ZXNpdHk= 11924
bGVu 11925
IHJvdXRlcw== 11926
dWF0aW5n 11927
Rmls 11928
Tk9U 11929
d2FsbA== 11930
c2J1cmdo 11931
IGVuZ2FnaW5n 11932
IEphdmFTY3JpcHQ= 11933
b3Jlcg== 11934
bGlob29k 11935
IHVuaW9ucw== 11936
IEZlZGVyYXRpb24= 11937
IFRlc2xh 11938
IGNvbXBsZXRpb24= 11939
IFRh 11940
IHByaXZpbGVnZQ== 11941
IE9yYW5nZQ== 11942
IG5ldXI= 11943
cGFyZW5jeQ== 11944
IGJvbmVz 11945
IHRpdGxlZA== 11946
IHByb3NlY3V0b3Jz 11947
IE1F 11948
IGVuZ2luZWVy 11949
IFVuaXZlcnNl 11950
IEhpZw== 11951
bmll 11952
b2FyZA== 11953
IGhlYXJ0cw== 11954
IEdyZQ== 11955
dXNzaW9u 11956
IG1pbmlzdHJ5 11957
IHBlbmV0 11958
IE51dA== 11959
IE93 11960
IFhQ 11961
aW5zdGVpbg== 11962
IGJ1bGs= 11963
U3lzdGVt 11964
aWNpc20= 11965
IE1hcmtldGFibGU= 11966
IHByZXZhbA== 11967
IHBvc3Rlcg== 11968
IGF0dGVuZGluZw== 11969
dXJhYmxl 11970
IGxpY2Vuc2Vk 11971
IEdo 11972
ZXRyeQ== 11973
IFRyYWRhYmxl 11974
IGJsYXN0 11975
4KQ= 11976
IFRpdGFu 11977
ZWxsZWQ= 11978
ZGll 11979
SGF2ZQ== 11980
IEZsYW1l 11981
IHByb2ZvdW5k 11982
IHBhcnRpY2lwYXRpbmc= 11983
IGFuaW1l 11984
IEVzcw== 11985
IHNwZWNpZnk= 11986
IHJlZ2FyZGVk 11987
IFNwZWxs 11988
IHNvbnM= 11989
b3duZWQ= 11990
IG1lcmM= 11991
IGV4cGVyaW1lbnRhbA== 11992
bGFuZG8= 11993
aHM= 11994
IER1bmdlb24= 11995
aW5vcw== 11996
IGNvbXBseQ== 11997
IFN5c3RlbXM= 11998
YXJ0aA== 11999
IHNlaXplZA== 12000
bG9jYWw= 12001
IEdpcmxz 12002
dWRv 12003
b25lZA== 12004
IEZsZQ== 12005
IGNvbnN0cnVjdGVk 12006
IGhvc3RlZA== 12007
IHNjYXJlZA== 12008
YWN0aWM= 12009
IElzbGFuZHM= 12010
IE1PUkU= 12011
IGJsZXNz 12012
IGJsb2NraW5n 12013
IGNoaXBz 12014
IGV2YWM= 12015
UHM= 12016
IGNvcnBvcmF0aW9u 12017
IG94 12018
IGxpZ2h0aW5n 12019
IG5laWdoYm9ycw== 12020
IFVi 12021
YXJv 12022
IGJlZWY= 12023
IFViZXI= 12024
RmFjZWJvb2s= 12025
YXJtZWQ= 12026
aXRhdGU= 12027
IFJhdGluZw== 12028
IFF1aWNr 12029
IG9jY3VwaWVk 12030
IGFpbXM= 12031
IEFkZGl0aW9uYWxseQ== 12032
IEludGVyZXN0 12033
IGRyYW1hdGljYWxseQ== 12034
IGhlYWw= 12035
IHBhaW50aW5n 12036
IGVuZ2luZWVycw== 12037
TU0= 12038
IE11c3Q= 12039
IHF1YW50aXR5 12040
UGF1bA== 12041
IGVhcm5pbmdz 12042
IFBvc3Rz 12043
c3RyYQ== 12044
44O844M= 12045
IHN0YW5jZQ== 12046
IGRyb3BwaW5n 12047
c2NyaXB0 12048
IGRyZXNzZWQ= 12049
TWFrZQ== 12050
IGp1c3RpZnk= 12051
IEx0ZA== 12052
IHByb21wdGVk 12053
IHNjcnV0 12054
IHNwZWVkcw== 12055
IEdpYW50cw== 12056
b21lcg== 12057
IEVkaXRvcg== 12058
IGRlc2NyaWJpbmc= 12059
IExpZQ== 12060
bWVudGVk 12061
IG5vd2hlcmU= 12062
b2NhbHk= 12063
IGluc3RydWN0aW9u 12064
Zm9ydGFibGU= 12065
IGVudGl0aWVz 12066
IGNt 12067
IE5hdHVyYWw= 12068
IGlucXVpcnk= 12069
IHByZXNzZWQ= 12070
aXpvbnQ= 12071
Zm9yY2Vk 12072
IHJhaXNlcw== 12073
IE5ldGZsaXg= 12074
IFNpZGU= 12075
IG91dGVy 12076
IGFtb25nc3Q= 12077
aW1z 12078
b3dza2k= 12079
IGNsaW1i 12080
bmV2ZXI= 12081
IGNvbWJpbmU= 12082
ZGluZw== 12083
IGNvbXBy 12084
IHNpZ25pZmljYW5jZQ== 12085
IHJlbWVtYmVyZWQ= 12086
IE5ldmFkYQ== 12087
IFRlbA== 12088
IFNjYXI= 12089
IFdhcnJpb3Jz 12090
IEphbmU= 12091
IGNvdXA= 12092
YmFz 12093
IHRlcm1pbmFs 12094
LC0= 12095
T0g= 12096
IHRlbnNpb24= 12097
IHdpbmdz 12098
IE15c3Rlcg== 12099
77+977+977+977+9 12100
IFVubGlrZQ== 12101
dmFsaWQ= 12102
dmlyb25tZW50cw== 12103
IEFsaQ== 12104
IG5ha2Vk 12105
Ym9va3M= 12106
IE11bg== 12107
IEd1bGY= 12108
IGRlbnNpdHk= 12109
IGRpbWlu 12110
IGRlc3BlcmF0ZQ== 12111
IHByZXNpZGVuY3k= 12112
IDE5ODY= 12113
aHk= 12114
SU5E 12115
IHVubG9jaw== 12116
aW1lbnM= 12117
IGhhbmRsZWQ= 12118
IEVi 12119
IGRpc2FwcGVhcmVk 12120
IGdlbnJl 12121
IDE5ODg= 12122
IGRldGVybWluYXRpb24= 12123
U3RyZWFt 12124
aWtv 12125
YXB0ZXJz 12126
IGFja25vd2xlZGdl 12127
SmFu 12128
IGNhcGl0YWxpc20= 12129
UGF0 12130
IDIwMjA= 12131
IHBhaW5mdWw= 12132
IGN1cnZl 12133
IGJvbWJz 12134
c3Rvcm0= 12135
IE1ldGFs 12136
ZW5jZXI= 12137
IEZpZw== 12138
IEFhcm9u 12139
YW5jaGVz 12140
IGluc3BpcmF0aW9u 12141
IGV4aGF1c3Q= 12142
dGFpbnM= 12143
YXNoaQ== 12144
IGRlc2NyaXB0 12145
IHJpdHVhbA== 12146
IENoZWxzZWE= 12147
IHByb21vdGlvbg== 12148
IEh1bmc= 12149
IFdhcmQ= 12150
aXZh 12151
IEVU 12152
IHRvc3M= 12153
YWxsb3c= 12154
IEZyYW5jaXM= 12155
RGVw 12156
IGhhcHBpbmVzcw== 12157
IEdsYXNz 12158
IGJldGE= 12159
IHN0cmVuZ3RoZW4= 12160
TkU= 12161
b2E= 12162
IGJ1dHRvbnM= 12163
IE11cnJheQ== 12164
IGtpY2tlZA== 12165
UXVlc3Q= 12166
IFRhbGs= 12167
IFNldmVyYWw= 12168
IFplcm8= 12169
IGRyb25l 12170
dWxr 12171
IGNhbQ== 12172
IE1vYmlsZQ== 12173
IHByZXZlbnRpbmc= 12174
IHJldHJv 12175
IEF4 12176
IGNydWVs 12177
IGZsb2F0 12178
Liks 12179
IGZpbGluZw== 12180
IEdyYW50 12181
IEJvcg== 12182
IHJpYg== 12183
IGNoYW1waW9uc2hpcA== 12184
IE1lcmM= 12185
IHN0eWxlcw== 12186
IGNha2U= 12187
IGJ1aWxkcw== 12188
IFNlbGY= 12189
aW94 12190
IGVwaWM= 12191
b3lk 12192
QmVs 12193
IFN0ZXc= 12194
Lig= 12195
YWh1 12196
IEJleW9uZA== 12197
IG91dHM= 12198
IHNvbG8= 12199
IFRyZWU= 12200
IHByZXNlcnZl 12201
IHR1Yg== 12202
QVJF 12203
cm9j 12204
IEltcHJv 12205
IFdyaWdodA== 12206
IGJ1bmQ= 12207
IHRyYWdlZA== 12208
IG9jY2FzaW9uYWw= 12209
Ymlhbg== 12210
U2Vjb25k 12211
cm9ucw== 12212
IGludGVyYWN0aW9ucw== 12213
Zm9ybWVk 12214
c2luZw== 12215
IG93bnM= 12216
IGhvY2tleQ== 12217
R2VuZXJhbA== 12218
IGxvZ2ljYWw= 12219
IGV4cGVuZA== 12220
IGVzY2Fs 12221
IEdyaWZm 12222
IENyb3du 12223
IFJlc2VydmU= 12224
IHN0b3BwaW5n 12225
IGV4Y3VzZQ== 12226
c2Vjb25k 12227
IG9wZXJhdGVk 12228
IHJlYWNoZXM= 12229
IE1hbGF5cw== 12230
IHBvbGx1dGlvbg== 12231
IEJyb29rbHlu 12232
IGRlbGV0ZQ== 12233
IGhhc2g= 12234
QmxvY2s= 12235
YWhh 12236
4oCz 12237
IHNob3J0ZXI= 12238
cGllY2U= 12239
Pjwv 12240
IGhvcm0= 12241
IFdhdA== 12242
IEJyZWFr 12243
IHByb2hpYml0ZWQ= 12244
IGludGVuc2l0eQ== 12245
IEFsYW4= 12246
IGxpYWJpbGl0eQ== 12247
PyE= 12248
YW5kZWQ= 12249
IG5laWdoYm91cg== 12250
IENvbGxlY3Rpb24= 12251
IGZpcmVz 12252
IHJldm9sdXRpb25hcnk= 12253
Zmx5 12254
IE9ybGVhbnM= 12255
V2hpdGU= 12256
IFdyaXQ= 12257
IERhd24= 12258
IHNldHRsZQ== 12259
IGV4ZWN1dGU= 12260
Qk0= 12261
IHNwb2tlc3dvbWFu 12262
IGxpZmVzdHlsZQ== 12263
IGNsaWNraW5n 12264
IEtpbGw= 12265
IExpYmVyYWw= 12266
IE5hemk= 12267
IHRyYWlsZXI= 12268
IG1vdW50YWlucw== 12269
IGRhbW4= 12270
emVz 12271
cGVz 12272
IHByZXNzaW5n 12273
IGJhaWw= 12274
IE9yZ2FuaXphdGlvbg== 12275
IHBpcg== 12276
IHRoaXJ0eQ== 12277
IGVsZWN0cmljYWw= 12278
IDExNQ== 12279
IFBvbHk= 12280
IFJhcA== 12281
IFN0cmlrZQ== 12282
IENhbm4= 12283
IGRlbWFuZGVk 12284
IGJhY2tpbmc= 12285
ZGVmYXVsdA== 12286
c3BlZWQ= 12287
IExlZ2lzbA== 12288
IG1vdGhlcnM= 12289
IEJvZHk= 12290
IHZhcmlhdGlvbg== 12291
Y2VkZW50ZWQ= 12292
cG93ZXJlZA== 12293
bGVhZGluZw== 12294
TmV2ZXI= 12295
IGdyYXZl 12296
IEFudGk= 12297
QVc= 12298
IGludGVydmlld2Vk 12299
IEdhYg== 12300
IEZhdA== 12301
IHJvb2tpZQ== 12302
dXU= 12303
IGRlcG9z 12304
aXhvbg== 12305
IGFtcGw= 12306
cmV0aW9u 12307
IEhlYXQ= 12308
IHBlYWNlZnVs 12309
U00= 12310
aWV2ZQ== 12311
IGRpdmVy 12312
IFZpY3Rvcmlh 12313
IG1pYw== 12314
cGRm 12315
IHN0YXRpbmc= 12316
IGx1bmc= 12317
IGNyaXRpY2l6ZWQ= 12318
IHZhY2NpbmU= 12319
IExvYWRpbmc= 12320
dXJzZQ== 12321
VGFrZQ== 12322
IEZyYW4= 12323
IFNvbGQ= 12324
IFJvYmlu 12325
IGRldGVjdGVk 12326
IFNjcmlwdA== 12327
IGFkanVzdGVk 12328
IHNlbmF0b3I= 12329
IG9wcG9zaW5n 12330
RXJyb3I= 12331
Q291bnQ= 12332
IGNvbmZsaWN0cw== 12333
IG93 12334
IEFyZ2VudA== 12335
IG1hdGNoaW5n 12336
aGg= 12337
IFRyZWs= 12338
c3RhcnRlcg== 12339
Iiks 12340
IEFG 12341
b2Rlcg== 12342
eHh4eA== 12343
IEFsdA== 12344
YWNyZQ== 12345
IFBpY2s= 12346
IFNvbGFy 12347
IERhbA== 12348
T2N0 12349
IEJhdHQ= 12350
IHNyYw== 12351
IGVuZ2FnZW1lbnQ= 12352
IGV4ZWN1dGl2ZXM= 12353
IGxpYmVydHk= 12354
amF2YQ== 12355
IHRhbGVudGVk 12356
aWdlbm91cw== 12357
IGNvbnNlY3V0 12358
Li4uLi4= 12359
SW5mbw== 12360
IGhvcnJpYmxl 12361
IHN1cnByaXNpbmdseQ== 12362
ZmVlZA== 12363
aWNhdGluZw== 12364
IExFRA== 12365
IGZlbWFsZXM= 12366
U3RhdGlvbg== 12367
ZWxsZXI= 12368
IE9ha2xhbmQ= 12369
IG1lY2hhbmljYWw= 12370
aW9sb2d5 12371
IFZhcg== 12372
IHJvYnVzdA== 12373
ZXR0aW5ncw== 12374
b3R0YQ== 12375
IHRoZW9yZXQ= 12376
IHJldGFpbg== 12377
a3dhcmQ= 12378
IGRh 12379
IGRlcGxveWVk 12380
ZGVs 12381
IEFuZHk= 12382
IHN1YnNjcmliZQ== 12383
d2Vi 12384
IG5h 12385
IE1pY2hlbA== 12386
IHBhcnRpYWxseQ== 12387
IENvbWV5 12388
IGNyb3du 12389
IE1hag== 12390
IEJsdQ== 12391
cmF0b3I= 12392
RGF5 12393
SU5U 12394
IGRvY3VtZW50ZWQ= 12395
IEdEUA== 12396
Z2k= 12397
Y2hlbGw= 12398
IGJydXRhbA== 12399
IEJhYg== 12400
c3RyYXRpb24= 12401
IHRoZWZ0 12402
IHR1YmU= 12403
QEA= 12404
IHF1ZXJ5 12405
IExpbmNvbG4= 12406
IHB1Ymxpc2hpbmc= 12407
IHdvcmU= 12408
b3JpY2Fs 12409
IHJpYw== 12410
IG5vdGFibGU= 12411
IHN1YnNlcXVlbnRseQ== 12412
bmV4 12413
IG9ic2VydmU= 12414
IEJvZQ== 12415
IGNvZGVz 12416
bWFpbg== 12417
V0g= 12418
IFNM 12419
IHJlc2lkZW50aWFs 12420
YXZhbg== 12421
IG1hcw== 12422
YXJlc3Q= 12423
YWRlb24= 12424
T1VU 12425
IHNvcGhpc3RpYw== 12426
YW50ZQ== 12427
IGNlbnM= 12428
ICoq 12429
IG1vcnRhbGl0eQ== 12430
IHlvdXJz 12431
IG9jY2FzaW9ucw== 12432
IHJlY2FsbGVk 12433
IERyaXZlcg== 12434
IHZvY2Fs 12435
IGJhdGhyb29t 12436
IHNob3Bz 12437
IGNvbGxhYm9yYXRpb24= 12438
IE9iYW1hY2FyZQ== 12439
IENlbGw= 12440
Q2hhcg== 12441
U3VwZXI= 12442
Q3Jl 12443
IHRlbmRz 12444
IHRvcm4= 12445
IGVjb25vbWljcw== 12446
YXZlcnk= 12447
IFJhaWQ= 12448
IFNlbQ== 12449
IHNob3VsZGVycw== 12450
IGV4cGVjdGluZw== 12451
IGV4YW1pbmF0aW9u 12452
ZW5hbWU= 12453
IFVJ 12454
aWFiaWxpdHk= 12455
b2xhcw== 12456
IEFtYg== 12457
IERyYQ== 12458
IG1pZGZpZWxk 12459
IElD 12460
IGxheW91dA== 12461
IGZsb2F0aW5n 12462
Zmk= 12463
aXRhdGl2ZQ== 12464
IHRyZW1lbmRvdXM= 12465
INA= 12466
IGFidW5k 12467
V29yaw== 12468
IExpZ2h0bmluZw== 12469
IHNpbWlsYXJseQ== 12470
IGNvbnNlcnZhdGl2ZXM= 12471
IHByYXk= 12472
QkU= 12473
aXphcnJl 12474
IHRlbXB0 12475
IGVtcGhhc2lz 12476
IE1ldHJv 12477
IGZpc2hpbmc= 12478
IG1hcnJ5 12479
bmVn 12480
IFN0dWR5 12481
IHJlY2s= 12482
IGRpc3Bvcw== 12483
b25pbmc= 12484
YnNpdGU= 12485
IHN1c3BpYw== 12486
IG1lcmNo 12487
IEdpYg== 12488
IERlc2NyaXB0aW9u 12489
IERWRA== 12490
d2hl 12491
IFllbWVu 12492
IGVudmlyb25tZW50cw== 12493
b290aW5n 12494
IE1vZGVybg== 12495
ZXU= 12496
IHJlZmxlY3Rz 12497
IGhvbmV5 12498
IGFuYWx5c3Q= 12499
IGd1dA== 12500
ZGVj 12501
QWN0aW9u 12502
IGhvdXNlaG9sZHM= 12503
IHN0ZXI= 12504
IHRlbXBsZQ== 12505
IHJlZm9ybXM= 12506
IGZhdm91cml0ZQ== 12507
IGRlYWRsaW5l 12508
IExF 12509
VGhyZWU= 12510
IFdpdGhpbg== 12511
QXVn 12512
IG5pZ2h0cw== 12513
ZWx0YQ== 12514
IGludmFsaWQ= 12515
IEV4Y2hhbmdl 12516
IERlbGhp 12517
d2hlbg== 12518
aW5jb21l 12519
IPCf 12520
IHdpcmVsZXNz 12521
c2NyaWJl 12522
aXN0YQ== 12523
IGhvc3RpbGU= 12524
IGFsbHk= 12525
IGdpZw== 12526
IG91dGxldHM= 12527
IERvcg== 12528
RU1FTlQ= 12529
IGFzaA== 12530
IGFic3RyYWN0 12531
T1JE 12532
IE1vdG9y 12533
IGFkdmlzZXI= 12534
aXN0bGU= 12535
IGJhc2Vz 12536
IGNvdXJ0ZXN5 12537
IGNyb3NzaW5n 12538
IGNsZWFyZWQ= 12539
IHJlZnVnZWU= 12540
Y29zeXN0ZW0= 12541
IHRocm93cw== 12542
ZnVu 12543
Ym91cm5l 12544
ZGF5cw== 12545
IGRpc2FncmVl 12546
IE5hdGl2ZQ== 12547
IHJlZmxlY3RlZA== 12548
IEZhc3Q= 12549
IFllbGxvdw== 12550
IFNpbmdhcG9yZQ== 12551
IFJhdmVu 12552
IGVtYnJhY2U= 12553
IEt1 12554
IENoZW4= 12555
IEVhcmx5 12556
IGFwcG9pbnRtZW50 12557
IE1pbmk= 12558
aXRlbWVudA== 12559
IHBsYWNpbmc= 12560
IGJpY3k= 12561
U1I= 12562
IHdoaXM= 12563
U1U= 12564
IGludmVzdGlnYXRlZA== 12565
IHBob3RvZ3JhcGhz 12566
Z2l0aHVi 12567
IEJlYXQ= 12568
IFJpbmc= 12569
aWdoZWQ= 12570
aWFy 12571
IGV2b2x2ZWQ= 12572
ZXJhbGQ= 12573
IGR1bg== 12574
IGh1Yg== 12575
SUFM 12576
IGVuY291cmFnaW5n 12577
IFByaW50 12578
IERheXM= 12579
IHByb3NlY3V0aW9u 12580
IHBhbnRz 12581
YXp5 12582
bGl2ZQ== 12583
IGZvc3NpbA== 12584
IEp1 12585
IHJvY2tz 12586
dWRnZQ== 12587
IFJhY2U= 12588
IGdyZWV0 12589
Ymll 12590
IGZpbGxpbmc= 12591
IExlbg== 12592
IGRpYWJldGVz 12593
IGZpcmVhcm1z 12594
dW1pbmc= 12595
ZW5lenVlbA== 12596
IEJC 12597
IGFjY2VwdGluZw== 12598
QVRI 12599
IHJlc29ydA== 12600
IGh1bnQ= 12601
cmlr 12602
dWNrZXI= 12603
YW1lbnRz 12604
IHN1c3RhaW5lZA== 12605
IGNyb3NzZWQ= 12606
IGJyZWFrZmFzdA== 12607
IGF0dHJpYnV0ZXM= 12608
bGVjdGVk 12609
YXRpbGU= 12610
IHZpYnI= 12611
IEthbA== 12612
YXJzb24= 12613
b3BsZXM= 12614
IHRvdWNoZWQ= 12615
IGRhbWFnZXM= 12616
IGltcHJlc3NlZA== 12617
cnVw 12618
IGFuY2g= 12619
IEFkYW1z 12620
SGVs 12621
IFZpY3Rvcg== 12622
IG1vdW50ZWQ= 12623
IEND 12624
IGRlbGljaW91cw== 12625
c3Bhbg== 12626
ZWxsYQ== 12627
IGVsYWJvcg== 12628
YW1wbGVz 12629
IGRlZmlj 12630
IGNvbnN0aXR1 12631
dWF0ZXM= 12632
IE1pc3Npb24= 12633
IFRoZXI= 12634
IE1vbnN0ZXI= 12635
YmVz 12636
UmV1dGVycw== 12637
IEluZG9uZXM= 12638
aGlsbA== 12639
bXVuaXRpb24= 12640
IGNvbmZpcm1hdGlvbg== 12641
IENvbnNpZGVy 12642
YWNlbnQ= 12643
IGpldA== 12644
IEVtcGxveQ== 12645
IEdUWA== 12646
bmFu 12647
IFNwaWRlcg== 12648
IHByb2Nlc3Nvcg== 12649
IHBhdHJp 12650
IFBlbnRhZ29u 12651
IFJvYmluc29u 12652
IHJlYWxpc3RpYw== 12653
w7E= 12654
IGFwcGVhcmluZw== 12655
IHBpcGU= 12656
b21lZA== 12657
IGZydQ== 12658
IGF3ZnVs 12659
IGV2YWx1YXRpb24= 12660
IGludGVsbGlnZW50 12661
IENpdGl6 12662
IGZ1bmRyYQ== 12663
b2RpdW0= 12664
IHR3ZWV0cw== 12665
IHdvcm4= 12666
cHJpbmc= 12667
IGtpZG4= 12668
IHJlYmVscw== 12669
IEthbQ== 12670
IE5ldGhlcmxhbmRz 12671
IFNX 12672
IGFjcXVpc2l0aW9u 12673
IE1hbGU= 12674
44Oq 12675
b21iaWVz 12676
IHRyYWRlbQ== 12677
IFN0YXR1cw== 12678
QnJl 12679
IFRISVM= 12680
IGFkdmVyc2U= 12681
IE5FVw== 12682
c2lnbg== 12683
IG9yZ2FuaXNhdGlvbg== 12684
ZW5j 12685
IEhhcnBlcg== 12686
YXBvcg== 12687
IE1lbWJlcnM= 12688
IFBlYWNl 12689
IEFpcnBvcnQ= 12690
IE90aGVycw== 12691
IHNjcmF0Y2g= 12692
IFBpbA== 12693
IHNlbnNvcg== 12694
IGFkb3B0aW9u 12695
IEhvdGVs 12696
IERyYWc= 12697
IGhvbmVzdGx5 12698
IHlhcmQ= 12699
IEZvcmNlcw== 12700
IHBhdGVudA== 12701
IGJhc3M= 12702
IHF1aWV0bHk= 12703
IGJyZWF0aGluZw== 12704
IHBvc2U= 12705
aW9ycw== 12706
IEplc3M= 12707
c3RhdGlj 12708
SVRF 12709
T2ZmaWM= 12710
IGpldw== 12711
d2Nz 12712
IDE0MA== 12713
IHByZXZpZXc= 12714
aXBwaQ== 12715
IHVuZm9ydHVuYXRlbHk= 12716
b2tlbW9u 12717
IGhvcm4= 12718
IHJlYXNz 12719
IHBlZXI= 12720
b2NrZXI= 12721
IHVudG8= 12722
IEdyYXk= 12723
IGNsZWFuaW5n 12724
IGF0dHJhY3RlZA== 12725
MjAwNw== 12726
UG9pbnQ= 12727
a2lsbA== 12728
IEFncmVlbWVudA== 12729
dXJjaGVz 12730
IGhvcnI= 12731
IE1pc3Npc3M= 12732
IHdvcnRoeQ== 12733
IGZsb3dlcnM= 12734
dG93bg== 12735
ZGxs 12736
IHJlYWN0aW9ucw== 12737
IGRlY2U= 12738
IGluZGljYXRpbmc= 12739
TUQ= 12740
IHByZWZlcmVuY2U= 12741
IE1WUA== 12742
ZXNzaW9uYWw= 12743
IFRhcmdldA== 12744
Z2VuY2U= 12745
IEluZGlhbnM= 12746
IG1pc2M= 12747
IGZyZWVseQ== 12748
IG11c2NsZXM= 12749
IGxpbmV1cA== 12750
IGltcGFjdHM= 12751
b3VzaW5n 12752
b21p 12753
YWN1bGFy 12754
IGNvbnRyb2xsaW5n 12755
YWdpbmU= 12756
Y2VyeQ== 12757
aGVsbA== 12758
IHJhbmtpbmc= 12759
IE5pY2g= 12760
IEF2ZQ== 12761
MTI4 12762
IGhpZ2h3YXk= 12763
IGluY29ucw== 12764
IGJpbmRpbmc= 12765
IHN0cnVnZ2xlcw== 12766
IFBpdHRzYnVyZ2g= 12767
IGdyYXk= 12768
cmlu 12769
IGNvbWljcw== 12770
IFNwb3J0 12771
IHJlbGF0aXZlcw== 12772
IGZyaWdodA== 12773
IHByb2Jl 12774
IFBvcnR1Zw== 12775
IHZvYw== 12776
IHR1 12777
IENvcnBz 12778
IHBvc3NpYmlsaXRpZXM= 12779
IHF1YWxpZnk= 12780
d2Nzc3RvcmU= 12781
IGxpYnJhcmllcw== 12782
IG1pZ3JhbnRz 12783
IGVudHJpZXM= 12784
IGNvbnNlY3V0aXZl 12785
dmFscw== 12786
IENoYWlybWFu 12787
IGhpbGw= 12788
SU1F 12789
IEdhcmQ= 12790
IGluZXF1YWxpdHk= 12791
Zm94 12792
IFNhdmU= 12793
IGNvcnQ= 12794
Y2xhaW1lZA== 12795
IHRyYWl0cw== 12796
IHBvdXI= 12797
IG1pc3NpbGVz 12798
IGVzc2VuY2U= 12799
IHNlbmRz 12800
IGFsbGlhbmNl 12801
IHdpc2hlcw== 12802
IENocmlzdG9waGVy 12803
Qmln 12804
Tlk= 12805
IEphY29i 12806
c2Fu 12807
dXJyZWQ= 12808
IFNP 12809
bGx5 12810
IGFkdm9jYXRl 12811
IEJvbmQ= 12812
ICIv 12813
VXNpbmc= 12814
IGRpc3RyaWN0cw== 12815
IEdhdGU= 12816
IEJpcg== 12817
cmlkZ2U= 12818
IE5heg== 12819
IFJz 12820
Ym9hcmRz 12821
IEdh 12822
IFJlYWdhbg== 12823
IGluZmx1ZW5jZWQ= 12824
MTAwMA== 12825
YXB5 12826
IGNoYWxsZW5nZWQ= 12827
IGJhcmc= 12828
IGZhY3VsdHk= 12829
IEZpZg== 12830
IGFjcXVpcmU= 12831
QWM= 12832
IGluc2VjdA== 12833
IGluc3RydW1lbnRz 12834
IGxlYWY= 12835
dGhvZG94 12836
TWVzc2FnZQ== 12837
IHRhbGU= 12838
IHRoZXJlYnk= 12839
IHRyYXA= 12840
IHN0cm9uZ2VzdA== 12841
IE1pbGl0YXJ5 12842
aXNpYmxl 12843
IDE5ODQ= 12844
ZXRoZWxlc3M= 12845
IGZsZXhpYmxl 12846
IGtpbGxz 12847
IGZpbmlzaGluZw== 12848
IFNpemU= 12849
IHJlZHVjZXM= 12850
IGVwaWQ= 12851
IG9yaWVudGF0aW9u 12852
ZnVsbA== 12853
IHRyYWNl 12854
IGxhc2Vy 12855
IG9wcG9zZQ== 12856
IGVkaXRpbmc= 12857
IG1vbWVudHVt 12858
5Lo= 12859
c2hvdw== 12860
Vkk= 12861
IExhZA== 12862
IDE5ODU= 12863
IG11cmRlcmVk 12864
OTAw 12865
dXRoZXI= 12866
IHByb2JhYmlsaXR5 12867
IFBvbGw= 12868
IHJlbHVjdA== 12869
IENoZW0= 12870
IE1vbnRyZWFs 12871
IGFkZXF1YXRl 12872
IFBvbGFuZA== 12873
IFNoZXJpZmY= 12874
dW1waA== 12875
IG9r 12876
IDAwMA== 12877
ICJb 12878
IG9wZXJhdG9ycw== 12879
IEZlcg== 12880
IG1vZGVz 12881
IEV2ZQ== 12882
IGRpc2NpcGxpbmU= 12883
TkVU 12884
SGFuZA== 12885
IG9yYWw= 12886
IFdF 12887
ZW1haWw= 12888
SlA= 12889
IFBhbGVzdGluaWFucw== 12890
IGhlbmNl 12891
IExlc3M= 12892
IG92ZXJs 12893
ZGln 12894
IGludGltaWQ= 12895
IENvYWw= 12896
IHJhbmdpbmc= 12897
dGhh 12898
IGRpc3RhbnQ= 12899
IGZpYg== 12900
IEluZGV4 12901
IFdvbmRlcg== 12902
IFBlbA== 12903
aGF0dGFu 12904
IEh1Zw== 12905
w5c= 12906
cmFpdA== 12907
IHdyYXBwZWQ= 12908
IFJQRw== 12909
IGNoZW1pY2Fscw== 12910
IE1vbmV5 12911
IGZyb3plbg== 12912
IGluZGlyZWN0 12913
IEFnYWluc3Q= 12914
RW5k 12915
IHVuY29tZm9ydGFibGU= 12916
IEdhbGxlcnk= 12917
IFBvc3RlZA== 12918
2Kc= 12919
b25kdWN0 12920
IGNvbnNlcXVlbmNl 12921
IGJpdHRlcg== 12922
IDE5ODc= 12923
cG9w 12924
IGNvdW50bGVzcw== 12925
IEFsYXNrYQ== 12926
ZmZmZg== 12927
IGRlcGFydHVyZQ== 12928
IHJlZnVuZA== 12929
IElhbg== 12930
aWF0ZWQ= 12931
IHNlZWtz 12932
IG1lY2hhbmljcw== 12933
IGp1cmlzZGljdGlvbg== 12934
bHlubg== 12935
IGFsaWtl 12936
IEh1bnQ= 12937
YXRob24= 12938
IHJlc29sdmVk 12939
IGNhY2hl 12940
IGRpc3RpbmN0aW9u 12941
ZGlyZWN0 12942
IGVuY291bnQ= 12943
b3Vi 12944
YmVhdA== 12945
IENvdW50cnk= 12946
c2VhcmNo 12947
IGNvbnRpbnVvdXM= 12948
IG1vZGVzdA== 12949
IFJhaWw= 12950
dGhvb2Q= 12951
MTMw 12952
QlVH 12953
IGNyaW1pbmFscw== 12954
IGluZGljYXRpb24= 12955
IGVuY291bnRlcmVk 12956
bGFzdA== 12957
IFd5 12958
IGlkZW9sb2d5 12959
IFBERg== 12960
c2VjdXJpdHk= 12961
XSk= 12962
IEppbW15 12963
IEVO 12964
IGhpcmluZw== 12965
VGVt 12966
IHBpZw== 12967
YXVudA== 12968
IENyeXN0YWw= 12969
IHBlbmFsdGllcw== 12970
IGNhcGFiaWxpdHk= 12971
IHB5 12972
IHByb2R1Y3RpdmU= 12973
IGJhbGFuY2Vk 12974
IEdlRm9yY2U= 12975
Y2xpY2s= 12976
b2xpdGFu 12977
b2Rz 12978
IGFmdGVyd2FyZHM= 12979
IHBsYXlvZmZz 12980
IEdpbGw= 12981
VXNlcg== 12982
IGJhY2tz 12983
cHVi 12984
dGFn 12985
IGFic3VyZA== 12986
cGlyaW5n 12987
IGNpdGluZw== 12988
IHRyaWxsaW9u 12989
IG9ibGlnYXRpb24= 12990
IG1heGlt 12991
YWhvbw== 12992
Y2Y= 12993
dW1p 12994
IEFscGhh 12995
IE5lbHNvbg== 12996
IHB1cnN1YW50 12997
aW5pdGVseQ== 12998
IGZyYWN0 12999
ZW50cnk= 13000
YmVyeQ== 13001
IFRob3I= 13002
QWRkZWQ= 13003
IERK 13004
IEdlbmU= 13005
IGF3a3dhcmQ= 13006
U3R1ZA== 13007
IHdhbGxldA== 13008
IERpdmluZQ== 13009
YXJpb3M= 13010
IHJlbGVhc2luZw== 13011
IGVkaXRlZA== 13012
IGFjY29tcGxpc2hlZA== 13013
QmVzdA== 13014
IGVkZ2Vz 13015
IHBsYW5lcw== 13016
IGZlZWRpbmc= 13017
In0sIg== 13018
IGRpc2Nsb3N1cmU= 13019
IGdyYWlu 13020
YWlyeQ== 13021
b29ucw== 13022
ZXJuYW5k 13023
VlI= 13024
IHJlYXNvbmFibHk= 13025
IGRydW0= 13026
IHBhcnRpYWw= 13027
IGdyYXBoaWM= 13028
IHVucHJlY2VkZW50ZWQ= 13029
IGFkdmlzZWQ= 13030
TWljcm8= 13031
IEFzc2Fk 13032
cG9pbnRz 13033
c2Nhcg== 13034
IFpvbmU= 13035
dHRlcw== 13036
IDcwMA== 13037
dm8= 13038
IEhhbXA= 13039
IGZpeGVz 13040
IGNhdXRpb24= 13041
IHN0cmluZ3M= 13042
IHBhbmVscw== 13043
IGxlYWs= 13044
IHByaWNpbmc= 13045
cm93dGg= 13046
IEVycm9y 13047
IFNhaW50cw== 13048
Zml4 13049
IG9ic2VydmF0aW9ucw== 13050
IEFicw== 13051
IHN1Z2dlc3Rpb24= 13052
IFVrcmFpbmlhbg== 13053
IGJhcnJpZXI= 13054
IHBhaW50ZWQ= 13055
QmV0 13056
aW1pcg== 13057
IFNwZWN0 13058
cG90 13059
b3JuZXlz 13060
IGNvbXBvdW5k 13061
IGJlYXJz 13062
IFJ1c2g= 13063
IGx1eHVyeQ== 13064
U3Vt 13065
IG9yYml0 13066
IE1hcmM= 13067
IGV4ZW1wdA== 13068
IFRyYWls 13069
IE1P 13070
IEhhbnM= 13071
IFdlYXBvbg== 13072
b2N1c2Vk 13073
dW1pbnVt 13074
IEplcnJ5 13075
IGJ1c3Q= 13076
IEFH 13077
IFdpa2k= 13078
IGVuZGxlc3M= 13079
IFZsYWQ= 13080
IEJhaA== 13081
IFJhZGVvbg== 13082
a2V5cw== 13083
IFN1cnZleQ== 13084
IFZpb2w= 13085
ZGVmaW5l 13086
bGVhbg== 13087
IGNvbW1vZA== 13088
IHJldmVudWVz 13089
xY0= 13090
IGZ1cm5pdHVyZQ== 13091
IGNhc3Rpbmc= 13092
IGRpcGxvbWF0aWM= 13093
IFBsYXllcnM= 13094
IEtpbGxlZA== 13095
IG1vZGlmeQ== 13096
IGlubm92YXRpdmU= 13097
IEFidQ== 13098
bm9y 13099
IGJvbmRz 13100
IGNvYWNoaW5n 13101
TWVy 13102
IG1vZHVsZXM= 13103
IFBhdHJpb3Rz 13104
IGVuaGFuY2Vk 13105
IHByb2NlZWRpbmdz 13106
IHRlYW1tYXRlcw== 13107
IDEyOA== 13108
YXJkbw== 13109
IGNvbXByb21pc2U= 13110
IE11Y2g= 13111
IGZsZXc= 13112
IEVkZ2U= 13113
IHVubmVjZXNzYXJ5 13114
IGRvY3RyaW5l 13115
cmVwb3J0 13116
IE9ybGFuZG8= 13117
IFByb2ZpbGU= 13118
IHBsYXlvZmY= 13119
ZnJpZW5kbHk= 13120
IGNvbXBsYWlu 13121
IE1D 13122
IE9wdA== 13123
IEdC 13124
IGJlYXRlbg== 13125
IGdvbGY= 13126
IHBsYWNlbWVudA== 13127
Qml0 13128
IG5ld3NsZXR0ZXI= 13129
IDIwMTk= 13130
dmlzb3I= 13131
cmF3bA== 13132
IGlQYWQ= 13133
IGFjdGVk 13134
IGp1aWNl 13135
IGRlY2tz 13136
UE4= 13137
c3VjY2Vzcw== 13138
IEhhbGY= 13139
IGRlbGV0ZWQ= 13140
IHNlY3JldHM= 13141
IGFzeWx1bQ== 13142
TWFydA== 13143
IEFjdGl2 13144
IEd1eQ== 13145
IFRz 13146
IGR5cw== 13147
IGFzc3VtaW5n 13148
IG1hbmE= 13149
IHN1YnVy 13150
IDEyNQ== 13151
TWVkaWE= 13152
QVJZ 13153
cmlkZQ== 13154
Y3A= 13155
IGRpZmZpY3VsdGllcw== 13156
IGNvbGxlY3Rpbmc= 13157
IGJhbmtydXB0 13158
bm9u 13159
IGNvbXBvc2Vk 13160
IHZvbHQ= 13161
IG1pbGl0YW50cw== 13162
ID4+Pg== 13163
IE1vcm1vbg== 13164
dG9y 13165
IHBhcnRpY2xlcw== 13166
IEJhcnQ= 13167
cnlwdGlvbg== 13168
IGFkbWlu 13169
IHNxdWVl 13170
VklESUE= 13171
IGNyZWF0b3I= 13172
aWFtZXRlcg== 13173
aWN1bGFy 13174
TkJD 13175
IGdyYWJiZWQ= 13176
IG5vZGQ= 13177
IHJhdGVk 13178
IHJvdGF0aW9u 13179
IGdyYXNw 13180
IGV4Y2Vzc2l2ZQ== 13181
IEVD 13182
IFdoaXQ= 13183
IGludmVudG9yeQ== 13184
YXVsdHM= 13185
IEZC 13186
IGVjb3N5c3RlbQ== 13187
IGJpbGxpb25z 13188
IHZlbnR1cmU= 13189
bmFtZWQ= 13190
IGRlZmVuZGVy 13191
b3V0ZQ== 13192
SW5zdGVhZA== 13193
aXJhYmxl 13194
V2Fy 13195
IGFzc3VtcHRpb24= 13196
IGJpdGU= 13197
IGVhcnRocXU= 13198
dGFpbA== 13199
c3BhY2U= 13200
IGdpZnRz 13201
Ym95cw== 13202
IGluZXZpdGFibGU= 13203
IHN0cnVjdHVyYWw= 13204
IGJlbmVmaWNpYWw= 13205
IGNvbXBlbGxpbmc= 13206
aG9sZQ== 13207
ZXJ2YXRpb24= 13208
IGNvYXQ= 13209
b2o= 13210
aW5jYXJu 13211
IFllYXJz 13212
IGRldGVybWluaW5n 13213
IHJoZXRvcmlj 13214
IGJvdW5kYXJpZXM= 13215
IHdoaXRlcw== 13216
QW50 13217
YWRkeQ== 13218
KS0= 13219
cmFoYW0= 13220
ZXRlcm1pbg== 13221
IGhhcnZlc3Q= 13222
IENvbmM= 13223
IGxhcHRvcA== 13224
IE1hdGNo 13225
IGVuam95aW5n 13226
Y2Nh 13227
b2xsYXI= 13228
IHRyaXBz 13229
IGFkZGljdGlvbg== 13230
IFNhaw== 13231
IHBvd2VyZWQ= 13232
IGNvdXM= 13233
IFJ1c3NpYW5z 13234
aWVyZQ== 13235
IHJldHJpZQ== 13236
cXVhbGl0eQ== 13237
IGRpZmZlcg== 13238
IGtpbmdkb20= 13239
IExhdXI= 13240
IENhcGl0b2w= 13241
IGNvbmNsdXNpb25z 13242
IEFsdGVybg== 13243
IE5hdg== 13244
IHRyYW5zcGFyZW50 13245
QkVS 13246
R3JvdXA= 13247
IENvbXBsZXRl 13248
IGluZmVy 13249
IGludHJpZw== 13250
IGluc2FuZQ== 13251
Uk8= 13252
b3Bob2I= 13253
aXNlbg== 13254
cXVhbA== 13255
TWljaGFlbA== 13256
IG11c2V1bQ== 13257
IFBvcGU= 13258
IHJlc2V0 13259
cmF0aXZl 13260
Zml2ZQ== 13261
IGFnZ3JlZw== 13262
aXR0ZWVz 13263
b3NpdG9yeQ== 13264
IGNhcmI= 13265
IFJlY29yZA== 13266
IGRlY2lkZXM= 13267
IEZpeA== 13268
IGV4Y2VwdGlvbnM= 13269
IENvbW1pc3Npb25lcg== 13270
dW5z 13271
IEVudmlyb25tZW50YWw= 13272
IGxlZ2VuZGFyeQ== 13273
aXN0ZW5jZQ== 13274
IHR1bm5lbA== 13275
a20= 13276
IGluc3VsdA== 13277
IHRyb2xs 13278
IHNoYWtl 13279
IGRldGVudGlvbg== 13280
cXVlcw== 13281
IENocm9tZQ== 13282
IEZpbGVz 13283
IHN1YnQ= 13284
IHByb3NwZWN0cw== 13285
IHByb2w= 13286
cmVuZGVy 13287
cHJvb2Y= 13288
IHBlcmZvcm1hbmNlcw== 13289
U3Ry 13290
IGhyZWY= 13291
ZXJuYW1l 13292
IGFjaGlldmVtZW50 13293
IGZ1dA== 13294
RnVsbA== 13295
IExlYmFu 13296
Z29vZ2xl 13297
44OI 13298
YW1wYQ== 13299
TWF5YmU= 13300
IHByb2plY3RlZA== 13301
IEVtYg== 13302
IGNvbGxlZw== 13303
IGF3YXJkcw== 13304
IOKU 13305
R29sZA== 13306
IEJsYWtl 13307
IFJhag== 13308
aWZ0aW5n 13309
IHBlbmRpbmc= 13310
IGluc3RpbmN0 13311
IGRldmVsb3BtZW50cw== 13312
Q29ubmVjdA== 13313
IE1hbmQ= 13314
IFdJVEg= 13315
IFBoaWxpcHBpbmVz 13316
cHJvZmlsZQ== 13317
IGFsdG9nZXRoZXI= 13318
IEJ1bmQ= 13319
IFRE 13320
b29vbw== 13321
YW1wZWQ= 13322
aXBo 13323
IHN0ZWFt 13324
IG9sZGVzdA== 13325
IGRldGVjdGlvbg== 13326
dWxwdA== 13327
IOc= 13328
IFdheW5l 13329
MjAwNg== 13330
ZmE= 13331
IGNpcmNsZXM= 13332
IEZ1 13333
IGRvbm9ycw== 13334
YXBwcm9wcmlhdGU= 13335
IERha290YQ== 13336
amFtaW4= 13337
IG1vdGl2YXRlZA== 13338
IHB1cmNoYXNlcw== 13339
IExvdWlzaWFuYQ== 13340
IFNwbA== 13341
IGdsb2Jl 13342
IDEwNQ== 13343
emlw 13344
Y2FsbA== 13345
IGRlcGFydG1lbnRz 13346
IHN1c3RhaW5hYmxl 13347
MTA1 13348
IE9Q 13349
aWZpZXJz 13350
IHByZXZlbnRlZA== 13351
IGluY29tcA== 13352
IENvbW1hbmRlcg== 13353
IGRvbWluYXRlZA== 13354
IMK7 13355
IGludmVzdGVk 13356
IGNvbXBsZXhpdHk= 13357
IGluY2w= 13358
IGVuc3VyaW5n 13359
IHJlYWxt 13360
eW5j 13361
IEluZGVwZW5kZW50 13362
cmFpbmVk 13363
IEplbg== 13364
IEZsaWdodA== 13365
IGF0aGU= 13366
IHNwZWN1bGF0aW9u 13367
IFRF 13368
b2NhdGU= 13369
dGlj 13370
IHBsYWludA== 13371
aGVycnk= 13372
IHRveQ== 13373
IDExMQ== 13374
IHBsYXRlcw== 13375
c3RhdHVz 13376
IElzYQ== 13377
IGRldm90ZWQ= 13378
Q29w 13379
IEVT 13380
MjU1 13381
dXJyZW5jeQ== 13382
TWFpbg== 13383
IHNsYXZlcw== 13384
IHBlcHBlcg== 13385
IHF1b3Rlcw== 13386
IGNlaWxpbmc= 13387
IEZpc2g= 13388
IHRyYW5zZm9ybWF0aW9u 13389
IGZyYWN0aW9u 13390
IGFkdmFudGFnZXM= 13391
IHRvaWxl 13392
IHN0dW5uaW5n 13393
IG1vaXN0 13394
YnJlYWtpbmc= 13395
c2k= 13396
IExvY2F0aW9u 13397
IE1lZGl1bQ== 13398
IHRleHRz 13399
IHVnbHk= 13400
IGJpbw== 13401
LuKAlA== 13402
IEJhc2Vk 13403
IHRyYWlucw== 13404
IFdpbmc= 13405
IEFuY2llbnQ= 13406
IFJlY29yZHM= 13407
IEhvcGU= 13408
U3BlY2lhbA== 13409
YWRlc2g= 13410
b2Jp 13411
Wy8= 13412
IHRlbXBvcmFyaWx5 13413
VmVy 13414
aHU= 13415
b3Nlcg== 13416
IG92ZXJuaWdodA== 13417
IG1hbW0= 13418
IFRyZWFzdXJ5 13419
IFZlbmV6dWVs 13420
IE1lZ2E= 13421
IHRhcg== 13422
IGV4cGVjdHM= 13423
YmxhY2s= 13424
b3JwaA== 13425
XFxcXA== 13426
IGFjY2VwdGFuY2U= 13427
IHJhZGFy 13428
c2lz 13429
IGp1bmlvcg== 13430
IGZyYW1lcw== 13431
IG9ic2VydmF0aW9u 13432
YWNpZXM= 13433
UG93ZXI= 13434
IEFkdmFuY2Vk 13435
TWFn 13436
b2xvZ2ljYWxseQ== 13437
IE1lY2hhbg== 13438
IHNlbnRlbmNlcw== 13439
IGFuYWx5c3Rz 13440
YXVnaHRlcnM= 13441
Zm9yY2VtZW50 13442
IHZhZ3Vl 13443
IGNsYXVzZQ== 13444
IGRpcmVjdG9ycw== 13445
IGV2YWx1YXRl 13446
IGNhYmluZXQ= 13447
TWF0dA== 13448
IENsYXNzaWM= 13449
QW5n 13450
IGNsZXI= 13451
IEJ1Y2s= 13452
IHJlc2VhcmNoZXI= 13453
IDE2MA== 13454
IHBvb3JseQ== 13455
IGV4cGVyaWVuY2luZw== 13456
IFBlZA== 13457
IE1hbmhhdHRhbg== 13458
IGZyZWVk 13459
IHRoZW1lcw== 13460
YWR2YW50 13461
IG5pbg== 13462
IHByYWlzZQ== 13463
MTA0 13464
IExpYnlh 13465
YmVzdA== 13466
IHRydXN0ZWQ= 13467
IGNlYXNl 13468
IGRpZ24= 13469
RGlyZWN0 13470
IGJvbWJpbmc= 13471
IG1pZ3JhdGlvbg== 13472
IFNjaWVuY2Vz 13473
IG11bmljaXBhbA== 13474
IEF2ZXJhZ2U= 13475
IGdsb3J5 13476
IHJldmVhbGluZw== 13477
IGFyZW5h 13478
IHVuY2VydGFpbnR5 13479
IGJhdHRsZWZpZWxk 13480
aWFv 13481
R29k 13482
IGNpbmVt 13483
cmFwZQ== 13484
ZWxsZQ== 13485
YXBvbnM= 13486
IGxpc3Rpbmc= 13487
IHdhaXRlZA== 13488
IHNwb3R0ZWQ= 13489
a2VsZXk= 13490
IEF1ZGlv 13491
ZW9y 13492
YXJkaW5n 13493
aWRkaW5n 13494
aWdtYQ== 13495
IE5lZw== 13496
IGxvbmU= 13497
IC0tLS0= 13498
ZXhl 13499
ZGVn 13500
IHRyYW5zZg== 13501
IHdhc2g= 13502
IHNsYXZlcnk= 13503
IGV4cGxvcmluZw== 13504
IFdX 13505
YXRzb24= 13506
IGVuY2w= 13507
bGllcw== 13508
IENyZWVr 13509
IHdvb2Rlbg== 13510
TWFuYWdlcg== 13511
IEJyYW5k 13512
dW1teQ== 13513
IEFydGh1cg== 13514
IGJ1cmVhdWNy 13515
IGJsZW5k 13516
YXJpYW5z 13517
RnVydGhlcg== 13518
IHN1cHBvc2VkbHk= 13519
IHdpbmRz 13520
IDE5Nzk= 13521
IGdyYXZpdHk= 13522
IGFuYWx5c2Vz 13523
IFRyYXZlbA== 13524
IFZldGVy 13525
IGR1bWI= 13526
IGFsdGVybmF0ZQ== 13527
Z2Fs 13528
IGNvbnN1bWVk 13529
IGVmZmVjdGl2ZW5lc3M= 13530
Licn 13531
IHBhdGhz 13532
b25kYQ== 13533
TEE= 13534
IFN0cm9uZw== 13535
IGVuYWJsZXM= 13536
IGVzY2FwZWQ= 13537
ICIi 13538
IDExMg== 13539
IDE5ODM= 13540
IHNtaWxlZA== 13541
IHRlbmRlbmN5 13542
RmlyZQ== 13543
IHBhcnM= 13544
IFJvYw== 13545
IGxha2U= 13546
IGZpdG5lc3M= 13547
IEF0aA== 13548
IEhvcm4= 13549
IGhpZXI= 13550
IGltcG9zZQ== 13551
bW90aGVy 13552
IHBlbnNpb24= 13553
aWN1dA== 13554
Ym9ybmU= 13555
aWNpYXJ5 13556
Ll8= 13557
IFNV 13558
IHBvbGFy 13559
aXN5 13560
ZW5ndQ== 13561
aXRpYWxpemVk 13562
QVRB 13563
d3JpdGU= 13564
IGV4ZXJjaXNlcw== 13565
IERpYW1vbmQ= 13566
b3R5cGVz 13567
IGhhcm1mdWw= 13568
b256 13569
IHByaW50aW5n 13570
c3Rvcnk= 13571
IGV4cGVydGlzZQ== 13572
IEdlcg== 13573
IHRyYWdlZHk= 13574
IEZseQ== 13575
IGRpdmlk 13576
YW1waXJl 13577
c3RvY2s= 13578
TWVt 13579
IHJlaWdu 13580
IHVudmU= 13581
IGFtZW5k 13582
IFByb3BoZXQ= 13583
IG11dHVhbA== 13584
IEZhYw== 13585
IHJlcGxhY2luZw== 13586
SGFy 13587
IENpcmN1aXQ= 13588
IHRocm9hdA== 13589
IFNob3Q= 13590
IGJhdHRlcmllcw== 13591
IHRvbGw= 13592
IGFkZHJlc3Npbmc= 13593
IE1lZGljYWlk 13594
IHB1cHA= 13595
IE5hcg== 13596
b2xr 13597
IGVxdWl0eQ== 13598
TVI= 13599
IEhpc3Bhbg== 13600
IExhcmdl 13601
bWlk 13602
RGV2 13603
IGV4cGVk 13604
IGRlbW8= 13605
IE1hcnNoYWxs 13606
ZXJndXM= 13607
IGZpYmVy 13608
IGRpdm9yY2U= 13609
IENyZWF0ZQ== 13610
IHNsb3dlcg== 13611
IFBhcmtlcg== 13612
IFN0dWRlbnQ= 13613
IFRyYWluaW5n 13614
UmV0dXJu 13615
IFRydQ== 13616
IGN1Yg== 13617
IFJlYWNoZWQ= 13618
IHBhbmlj 13619
IHF1YXJ0ZXJz 13620
IHJlY3Q= 13621
IHRyZWF0aW5n 13622
IHJhdHM= 13623
IENocmlzdGlhbml0eQ== 13624
b2xlcg== 13625
IHNhY3JlZA== 13626
IGRlY2xhcmU= 13627
dWxhdGl2ZQ== 13628
ZXRpbmc= 13629
IGRlbGl2ZXJpbmc= 13630
ZXN0b25l 13631
IHRlbA== 13632
IExhcnJ5 13633
IG1ldGE= 13634
YWNjZXB0 13635
YXJ0eg== 13636
IFJvZ2Vy 13637
aGFuZGVk 13638
IGhlYWRlcg== 13639
IHRyYXBwZWQ= 13640
IENlbnR1cnk= 13641
IGtub2NrZWQ= 13642
IE94Zm9yZA== 13643
IHN1cnZpdm9ycw== 13644
Ym90 13645
IGRlbW9uc3RyYXRpb24= 13646
IGRpcnQ= 13647
IGFzc2lzdHM= 13648
T01F 13649
IERyYWZ0 13650
b3J0dW5hdGU= 13651
Zm9saW8= 13652
cGVyZWQ= 13653
dXN0ZXJz 13654
Z3Q= 13655
IExvY2s= 13656
IGp1ZGljaWFs 13657
dmVydGVk 13658
IHNlY3VyZWQ= 13659
b3V0aW5n 13660
IEJvb2tz 13661
IGhvc3Rpbmc= 13662
IGxpZnRlZA== 13663
bGVuZ3Ro 13664
IGplcg== 13665
IHdoZWVscw== 13666
IFJhbmdl 13667
dW1ibmFpbHM= 13668
IGRpYWdub3Npcw== 13669
dGVjaA== 13670
IFN0ZXdhcnQ= 13671
IFByYWN0 13672
IG5hdGlvbndpZGU= 13673
IGRlYXI= 13674
IG9ibGlnYXRpb25z 13675
IGdyb3dz 13676
IG1hbmRhdG9yeQ== 13677
IHN1c3BpY2lvdXM= 13678
ISc= 13679
QXBy 13680
R3JlYXQ= 13681
IG1vcnRnYWdl 13682
IHByb3NlY3V0b3I= 13683
IGVkaXRvcmlhbA== 13684
IEty 13685
IHByb2Nlc3NlZA== 13686
dW5nbGU= 13687
IGZsZXhpYmlsaXR5 13688
RWFybGllcg== 13689
IENhcnQ= 13690
IFN1Zw== 13691
IGZvY3VzZXM= 13692
IHN0YXJ0dXA= 13693
IGJyZWFjaA== 13694
IFRvYg== 13695
Y3ljbGU= 13696
44CM 13697
cm9zZQ== 13698
IGJpemFycmU= 13699
44CN 13700
IHZlZ2V0YWJsZXM= 13701
JCQ= 13702
IHJldHJlYXQ= 13703
b3NoaQ== 13704
IFNob3A= 13705
IEdyb3VuZA== 13706
IFN0b3A= 13707
IEhhd2FpaQ== 13708
IEF5 13709
UGVyaGFwcw== 13710
IEJlYXV0 13711
dWZmZXI= 13712
ZW5uYQ== 13713
IHByb2R1Y3Rpdml0eQ== 13714
Rml4ZWQ= 13715
Y29udHJvbA== 13716
IGFic2VudA== 13717
IENhbXBhaWdu 13718
R3JlZW4= 13719
IGlkZW50aWZ5aW5n 13720
IHJlZ3JldA== 13721
IHByb21vdGVk 13722
IFNldmVu 13723
IGVydQ== 13724
bmVhdGg= 13725
YXVnaGVk 13726
IFBpbg== 13727
IExpdmluZw== 13728
Q29zdA== 13729
b21hdGlj 13730
bWVnYQ== 13731
IE5pZw== 13732
b2N5 13733
IGluYm94 13734
IGVtcGlyZQ== 13735
IGhvcml6b250 13736
IGJyYW5jaGVz 13737
IG1ldGFwaA== 13738
QWN0aXZl 13739
ZWRp 13740
IEZpbG0= 13741
IFNvbWV0aGluZw== 13742
IG1vZHM= 13743
aW5jaWFs 13744
IE9yaWdpbmFs 13745
R2Vu 13746
IHNwaXJpdHM= 13747
IGVhcm5pbmc= 13748
SGlzdA== 13749
IHJpZGVycw== 13750
IHNhY3JpZmlj 13751
TVQ= 13752
IFZB 13753
IFNhbHQ= 13754
IG9jY3VwYXRpb24= 13755
IE1p 13756
IGRpc2c= 13757
bGljdA== 13758
IG5pdA== 13759
IG5vZGVz 13760
ZWVt 13761
IFBpZXI= 13762
IGhhdHJlZA== 13763
cHN5 13764
44OJ 13765
IHRoZWF0ZXI= 13766
IHNvcGhpc3RpY2F0ZWQ= 13767
IGRlZmVuZGVk 13768
IGJlc2lkZXM= 13769
IHRob3JvdWdobHk= 13770
IE1lZGljYXJl 13771
IGJsYW1lZA== 13772
YXJlbnRseQ== 13773
IGNyeWluZw== 13774
Rk9S 13775
cHJpdg== 13776
IHNpbmdpbmc= 13777
IEls 13778
IGN1dGU= 13779
b2lkZWQ= 13780
b2xpdGljYWw= 13781
IE5ldXJv 13782
5aQ= 13783
IGRvbmF0aW9u 13784
IEVhZ2xlcw== 13785
IEdpdmU= 13786
VG9t 13787
IHN1YnN0YW50aWFsbHk= 13788
IExpY2Vuc2U= 13789
IEph 13790
IGdyZXk= 13791
IEFuaW1hbA== 13792
IEVS 13793
IFVuZA== 13794
IGtlZW4= 13795
IGNvbmNsdWRl 13796
IE1pc3Npc3NpcHBp 13797
RW5naW5l 13798
IFN0dWRpb3M= 13799
UHJlc3M= 13800
b3ZlcnM= 13801
bGxlcnM= 13802
IDM1MA== 13803
IFJhbmdlcnM= 13804
IHJvdQ== 13805
ZXJ0bw== 13806
RXA= 13807
aXNzYQ== 13808
aXZhbg== 13809
IHNlYWw= 13810
IFJlZ2lzdA== 13811
ZGlzcGxheQ== 13812
IHdlYWtlbg== 13813
dXVt 13814
IENvbW1vbnM= 13815
IFNheQ== 13816
IGN1bHR1cmVz 13817
IGxhdWdoZWQ= 13818
IHNsaXA= 13819
IHRyZWF0bWVudHM= 13820
aXphYmxl 13821
bWFydA== 13822
IFJpY2U= 13823
IGJlYXN0 13824
IG9iZXNpdHk= 13825
IExhdXJl 13826
aWdh 13827
V2hpY2g= 13828
aG9sZGVy 13829
IGVsZGVybHk= 13830
IHBheXM= 13831
IGNvbXBsYWluZWQ= 13832
IGNyb3A= 13833
IHByb2M= 13834
IGV4cGxvc2l2ZQ== 13835
IEZhbg== 13836
IEFyc2VuYWw= 13837
QXV0aG9y 13838
ZWZ1bA== 13839
IG1lYWxz 13840
ICgt 13841
aWRheXM= 13842
IGltYWdpbmF0aW9u 13843
IGFubnVhbGx5 13844
IG1z 13845
YXN1cmVz 13846
SGVhZA== 13847
aWto 13848
bWF0aWM= 13849
IGJveWZyaWVuZA== 13850
IENvbXB1dGVy 13851
IGJ1bXA= 13852
IHN1cmdl 13853
IENyYWln 13854
IEtpcms= 13855
RGVs 13856
bWVkaWF0ZQ== 13857
IHNjZW5hcmlvcw== 13858
IE11dA== 13859
IFN0cmVhbQ== 13860
IGNvbXBldGl0b3Jz 13861
2YQ= 13862
IFN0YW5mb3Jk 13863
IFJlc291cmNlcw== 13864
YXplZA== 13865
YmFnZQ== 13866
IG9yZ2FuaXM= 13867
IFJlbGVhc2U= 13868
IHNlcGFyYXRlbHk= 13869
IGhhYml0cw== 13870
IG1lYXN1cmVtZW50cw== 13871
IENsb3Nl 13872
IGFjY29tcGFueQ== 13873
IGdseQ== 13874
IHRhbmc= 13875
IFJvdQ== 13876
IHBsdWdpbg== 13877
IGNvbnZleQ== 13878
IENoYWxsZW5nZQ== 13879
b290cw== 13880
amFu 13881
IGN1cnM= 13882
IFJlbGF0aW9ucw== 13883
a2VlcGVy 13884
IGFwcHJvYWNoaW5n 13885
cGluZw== 13886
U3BlYWtpbmc= 13887
IGFycmFuZ2VtZW50 13888
IFZJ 13889
YXJldHRlcw== 13890
IGFmZmVjdGluZw== 13891
IHBlcm1pdHM= 13892
YmVjYXVzZQ== 13893
IHVzZWxlc3M= 13894
IEh1cw== 13895
ISEhIQ== 13896
IGRlc3Ryb3lpbmc= 13897
VW5mb3J0dW5hdGVseQ== 13898
IGZhc2NpbmF0aW5n 13899
U2Vt 13900
IGVsZWN0b3JhbA== 13901
IHRyYW5zcGFyZW5jeQ== 13902
IENoYW9z 13903
IHZvbHVudGVlcg== 13904
IHN0YXRpc3RpY2Fs 13905
IGFjdGl2YXRlZA== 13906
cm94 13907
V2Vi 13908
SEU= 13909
IEhhbXBzaGlyZQ== 13910
aXNpdmU= 13911
TWFw 13912
IHRyYXNo 13913
IExhd3JlbmNl 13914
c3RpY2s= 13915
Q3I= 13916
IHJpbmdz 13917
RVhU 13918
IG9wZXJhdGlvbmFs 13919
b3Blcw== 13920
RG9lcw== 13921
IEV2YW5z 13922
IHdpdG5lc3NlZA== 13923
UG9ydA== 13924
IGxhdW5jaGluZw== 13925
ZWNvbm9t 13926
d2Vhcg== 13927
IFBhcnRpY2lw 13928
dW1t 13929
Y3VsZXM= 13930
IFJBTQ== 13931
IFR1bg== 13932
IGFzc3VyZWQ= 13933
IGJpbmFyeQ== 13934
IGJldHJheQ== 13935
IGV4cGxvcmF0aW9u 13936
IEZlbA== 13937
IGFkbWlzc2lvbg== 13938
aXRhdGVk 13939
U3k= 13940
IGF2b2lkZWQ= 13941
IFNpbXVsYXRvcg== 13942
IGNlbGVicmF0ZWQ= 13943
IEVsZWN0cmlj 13944
pZ4= 13945
IGNsdXN0ZXI= 13946
aXR6ZXJsYW5k 13947
aGVhbHRo 13948
TGluZQ== 13949
IE5hc2g= 13950
YXRvbg== 13951
IHNwYXJl 13952
IGVudGVycHJpc2U= 13953
IERJUw== 13954
Y2x1ZGVz 13955
IGZsaWdodHM= 13956
IHJlZ2FyZHM= 13957
IMOX 13958
aGFsZg== 13959
IHRydWNrcw== 13960
IGNvbnRhY3Rz 13961
IHVuY29ucw== 13962
IENsaW1hdGU= 13963
IGltbWVuc2U= 13964
TkVX 13965
b2Nj 13966
ZWN0aXZl 13967
IGVtYm9k 13968
IHBhdHJvbA== 13969
IGJlc2lkZQ== 13970
IHZpYWJsZQ== 13971
IGNyZWVw 13972
IHRyaWdnZXJlZA== 13973
dmVybmluZw== 13974
IGNvbXBhcmFibGU= 13975
cWw= 13976
IGdhaW5pbmc= 13977
YXNzZXM= 13978
ICgpOw== 13979
IEdyZXk= 13980
IE1MUw== 13981
c2l6ZWQ= 13982
IHByb3NwZXI= 13983
Ij8= 13984
IHBvbGxpbmc= 13985
IHNoYXI= 13986
IFJD 13987
IGZpcmVhcm0= 13988
b3JpZW50 13989
IGZlbmNl 13990
IHZhcmlhdGlvbnM= 13991
Z2l2aW5n 13992
IFBp 13993
b3NwZWw= 13994
IHBsZWRnZQ== 13995
IGN1cmU= 13996
IHNweQ== 13997
IHZpb2xhdGVk 13998
IHJ1c2hlZA== 13999
IHN0cm9rZQ== 14000
IEJsb2c= 14001
c2Vscw== 14002
IEVj 14003
LCcn 14004
IHBhbGU= 14005
IENvbGxpbnM= 14006
dGVycm9y 14007
IENhbmFkaWFucw== 14008
IHR1bmU= 14009
IGxhYm9yYXRvcnk= 14010
IG5vbnM= 14011
dGFyaWFu 14012
IGRpc2FiaWxpdHk= 14013
IEdhbQ== 14014
IHNpbmdlcg== 14015
YWxn 14016
IFNlbmlvcg== 14017
IHRyYWRlZA== 14018
IFdhcnJpb3I= 14019
IGluZnJpbmc= 14020
IEZyYW5rbGlu 14021
IHN0cmFpbg== 14022
IFN3ZWRpc2g= 14023
IHNldmVudGg= 14024
IEJlbm4= 14025
IFRlbGw= 14026
IHN5bmRyb21l 14027
IHdvbmRlcmVk 14028
aWRlbg== 14029
KysrKw== 14030
aWdv 14031
IHB1cnBsZQ== 14032
IGpvdXJuYWxpc20= 14033
IHJlYmVs 14034
IGZ1 14035
YmxvZw== 14036
IGludml0ZQ== 14037
cmVuY2llcw== 14038
IENvbnRhY3Q= 14039
SXNyYWVs 14040
IENvbnRlbnQ= 14041
IGNoZWVy 14042
IGJlZHJvb20= 14043
IEVuZ2luZWVyaW5n 14044
IFF1ZWVucw== 14045
IGR3ZWxs 14046
IFBsYXlTdGF0aW9u 14047
IERpbQ== 14048
IENvbG9u 14049
bHI= 14050
IG9wZXJhdGVz 14051
IG1vdGl2YXRpb24= 14052
VVNB 14053
YXN0ZXJlZA== 14054
Q29yZQ== 14055
IFRydXRo 14056
b2xv 14057
T1NF 14058
IE1lbW9yeQ== 14059
IHByZWRlYw== 14060
IGFuYXJjaA== 14061
IDE5MjA= 14062
IFlhbQ== 14063
w6g= 14064
Ymlk 14065
IGdyYXRlZnVs 14066
IGV4Y2l0ZW1lbnQ= 14067
IHRyZWFzdXJl 14068
IGxvbmdlc3Q= 14069
Y3RpdmU= 14070
IGRlc2VydmVz 14071
IHJlc2VydmVz 14072
IGNvcHM= 14073
IE90dGF3YQ== 14074
IEVneXB0aWFu 14075
YW5rZWQ= 14076
IGFydGlm 14077
IGh5cG90aGVzaXM= 14078
Oi8= 14079
IHB1cmNoYXNpbmc= 14080
IGxvdmVseQ== 14081
SFA= 14082
IGRpdmlkZQ== 14083
IHN0cmljdGx5 14084
IHF1ZXN0aW9uaW5n 14085
IHRheHBheWVycw== 14086
IEpveQ== 14087
IHJvbGxz 14088
IEhlYXZ5 14089
IHBvcnRz 14090
IG1hZ25ldGlj 14091
IGluZmxhbW0= 14092
IGJydXNo 14093
dGljcw== 14094
4oiS 14095
IGJvdHRsZXM= 14096
cHB5 14097
IHBhZGQ= 14098
44Kv 14099
bWlsbGlvbg== 14100
IGRldmFzdGF0aW5n 14101
IGNvbXBpbGVk 14102
IG1lZGljYXRpb24= 14103
IHR3ZWx2ZQ== 14104
IFBlcnJ5 14105
U3BhY2U= 14106
aW1i 14107
eW91cg== 14108
IGxlYWtlZA== 14109
IFRhcg== 14110
IHVuaXR5 14111
IGluZmVjdGVk 14112
IHRyYXZlbGVk 14113
SURF 14114
IE1jRG9uYWxk 14115
dHh0 14116
IFByaW5j 14117
IGludGVydmVu 14118
IFRhaXdhbg== 14119
IFBvdw== 14120
IGJlYXJpbmc= 14121
IFRocmVhZA== 14122
IHpvbmVz 14123
aXphcmRz 14124
dW5rcw== 14125
Q2hhcHRlcg== 14126
bGxvcg== 14127
IMK3 14128
IHdvdW5kcw== 14129
IGRpc2NyZXRpb24= 14130
IHN1Y2NlZWRlZA== 14131
aWtpbmc= 14132
IGljb25pYw== 14133
Q2FsbA== 14134
IHNjcmVlbmluZw== 14135
IE1pcw== 14136
aWN0cw== 14137
IG1pbmlzdGVycw== 14138
IHNlcGFyYXRpb24= 14139
UGxheWVy 14140
IGJpcA== 14141
IGJlbG92ZWQ= 14142
IGNvdW50aW5n 14143
IEV5ZQ== 14144
YXJvdW5k 14145
aW5naW5n 14146
IHRhYmxldA== 14147
IG9mZmVuY2U= 14148
aW5hbmNl 14149
aGF2ZQ== 14150
IEluZm8= 14151
IE5pbmph 14152
IHByb3RlY3RpdmU= 14153
IENhc3M= 14154
TWFj 14155
IFF1YWxpdHk= 14156
Tm9ydGg= 14157
IGlj 14158
IEN1YmE= 14159
IENocm9uaWNsZQ== 14160
IFByb3BlcnR5 14161
IGZhc3Rlc3Q= 14162
b3Rvcw== 14163
IEdlcm0= 14164
T1dO 14165
IGJvb20= 14166
IFN0YW5sZXk= 14167
ZXJndXNvbg== 14168
IGNsZXZlcg== 14169
IGVudGVycw== 14170
bW9kZQ== 14171
dGVyaW9y 14172
IFNlbnM= 14173
IGxpbmVhcg== 14174
QVJL 14175
IGNvbXBhcmluZw== 14176
IHB1cmVseQ== 14177
IHNhZmVy 14178
IFBvdHRlcg== 14179
IGN1cHM= 14180
UlQ= 14181
IGdsdWM= 14182
IGF0dHJpYnV0ZWQ= 14183
IGR1cGw= 14184
IFBhcA== 14185
IHByZWNpb3Vz 14186
IHBh 14187
aWN0aW9uYXJ5 14188
IFRpZw== 14189
IFRvbw== 14190
b2x1dGlvbnM= 14191
c3Rhbg== 14192
IHJvYm90cw== 14193
IGxvYmI= 14194
IHN0YXR1dGU= 14195
IHByZXZlbnRpb24= 14196
d2VzdGVybg== 14197
MTYw 14198
IEFjdGl2ZQ== 14199
IE1hcmlh 14200
aGFs 14201
Tm9uZQ== 14202
ZWxsYXI= 14203
IEtC 14204
IFBhcnRuZXJz 14205
IFNpbmdsZQ== 14206
IEZvbGxvd2luZw== 14207
YW5nbw== 14208
YWNpb3Vz 14209
IHRob3U= 14210
IGtn 14211
IGluZmx1ZW50aWFs 14212
IEZyaWVuZHM= 14213
U3Vy 14214
YWludGVk 14215
IGZvcnVtcw== 14216
IHN0YXJ0ZXI= 14217
IGNpdGl6ZW5zaGlw 14218
IEVsZWN0aW9u 14219
b25nZQ== 14220
b3RhdGlvbg== 14221
b3NwaA== 14222
Ozs7Ow== 14223
dXRpY2Fs 14224
cHVy 14225
ZXJlbg== 14226
IGFjY3VzYXRpb25z 14227
Yml0aW91cw== 14228
YWJiaXQ= 14229
IE9yZA== 14230
UG9zdGVk 14231
aXJr 14232
IHNlbnNpdGl2aXR5 14233
aWNoZQ== 14234
IEFteQ== 14235
IEZhYg== 14236
IHN1bW1pdA== 14237
IHBlZGVzdA== 14238
IHJ1YmJlcg== 14239
IGFncmljdWx0dXJhbA== 14240
IGNhbmNlbA== 14241
QUU= 14242
IGluYXVn 14243
IGNvbnRhbQ== 14244
IGZpcm1seQ== 14245
aXc= 14246
c3RhZ2U= 14247
IEthbg== 14248
IHRpZXI= 14249
IGludmVudGlvbg== 14250
IHRyYW5zbGF0ZWQ= 14251
IFJ1bGVz 14252
Qm94 14253
VHdpdHRlcg== 14254
SURT 14255
IHBpenph 14256
IGRlYnVn 14257
IERyb3A= 14258
dnM= 14259
IGhvcnNlcw== 14260
Ymln 14261
IGJvcmluZw== 14262
IGhvb2Q= 14263
IE1jQ2Fpbg== 14264
YXRjaGVk 14265
IEJyb3M= 14266
IHNraXA= 14267
IGVzc2F5 14268
c3RhdA== 14269
IExlZ2VuZHM= 14270
IGFtbXVuaXRpb24= 14271
YXVj 14272
IHNob290ZXI= 14273
IHVuaA== 14274
IHN1cHBsaWVk 14275
IGdlbmVyaWM= 14276
IFNL 14277
aWJhbg== 14278
eXJpY3M= 14279
IDI1NQ== 14280
IGNsaW1iaW5n 14281
Rm9ybWVy 14282
IGZsaXA= 14283
IGp1bXBpbmc= 14284
IGZydXN0cmF0aW9u 14285
IFRlcnJ5 14286
IG5laWdoYm9yaG9vZHM= 14287
IG1lZGlhbg== 14288
YmVhbg== 14289
IGJyYWlucw== 14290
Rm9sbG93aW5n 14291
IHNoYXBlZA== 14292
IGRyYXdz 14293
IGFsdGVyZWQ= 14294
SmFjaw== 14295
IHJlY2lwZXM= 14296
IHNraWxsZWQ= 14297
d2VhbHRo 14298
YWNoaQ== 14299
ZWxlY3Rpb24= 14300
IGJlaGF2aW9ycw== 14301
ZGVhbHM= 14302
IFVudGls 14303
RmU= 14304
IGRlY2xhcmF0aW9u 14305
bWFya3M= 14306
IEJldHdlZW4= 14307
Y2Vsb25h 14308
IHJlc29u 14309
IGJ1YmJsZQ== 14310
QW1vbmc= 14311
IGltcGVyaWFs 14312
R1M= 14313
IGZlbWluaXN0 14314
MjAwNQ== 14315
IEt5bGU= 14316
IGFjY291bnRpbmc= 14317
IFRlbGU= 14318
IFR5cg== 14319
IGNvbm5lY3Rpbmc= 14320
IHJlaGFi 14321
IFByZWQ= 14322
c2lt 14323
IG1lYW50aW1l 14324
IHBoeXNpY2lhbg== 14325
TVc= 14326
IENhbXBiZWxs 14327
IEJyYW5kb24= 14328
IGNvbnRyaWJ1dGluZw== 14329
IFJ1bGU= 14330
IFdlaWdodA== 14331
IE5hcA== 14332
IGludGVyYWN0aXZl 14333
IHZhZw== 14334
IGhlbG1ldA== 14335
IENvbWI= 14336
Zm91cg== 14337
IHNoaXBwZWQ= 14338
IGNvbXBsZXRpbmc= 14339
IFBE 14340
UERBVEU= 14341
IHNwcmVhZGluZw== 14342
IHNjYXJ5 14343
ZXJ2aW5n 14344
IEdhcw== 14345
IGZyYW5r 14346
c2Nob29s 14347
IHJvbWFudGlj 14348
IHN0YWJpbA== 14349
Um9i 14350
IGFjY3VyYXRlbHk= 14351
IGFjdXRl 14352
IEhhbm4= 14353
IHN5bWJvbHM= 14354
IGNpdmlsaXphdGlvbg== 14355
IEFX 14356
IGxpZ2h0bmluZw== 14357
IGNvbnNpZGVycw== 14358
IHZlbnVl 14359
INc= 14360
IG92ZW4= 14361
IFNG 14362
aGlz 14363
IG51 14364
IExlYXJu 14365
IHBlb3BsZXM= 14366
IHN0ZA== 14367
IHNsZWU= 14368
IHNsaWM= 14369
IFN0YXRpc3RpY3M= 14370
IGNvcm5lcnM= 14371
IEJha2Vy 14372
IDop 14373
bWVudGF0aW9u 14374
b2x2ZXI= 14375
IGxhdWdoaW5n 14376
IFRvZGQ= 14377
b25kZQ== 14378
IEhpbGxz 14379
IG51dHM= 14380
IFdvbWFu 14381
cGxhbmU= 14382
IGxpdmVy 14383
IEluc2lkZQ== 14384
U29ycnk= 14385
IGFncmVlcw== 14386
IGZ1bmRhbWVudA== 14387
IEZpc2hlcg== 14388
IGF1Y3Rpb24= 14389
IHRocmVhZHM= 14390
Z2xhcw== 14391
IEJhc2lj 14392
IE5hdA== 14393
IGxhY2tpbmc= 14394
IGNlbGVicmF0aW9u 14395
anU= 14396
IHNpbGx5 14397
RXVybw== 14398
IHRhdHQ= 14399
aWdodHk= 14400
Y29udHJvbGxlZA== 14401
VGVzdA== 14402
IFNpbmdo 14403
IHJhZ2U= 14404
IHJoeXRo 14405
b2ZmaWM= 14406
IFBoYW50b20= 14407
IGhlYWRsaW5lcw== 14408
IHJlc3BvbmRpbmc= 14409
IE1vcm5pbmc= 14410
IHZpdGFtaW4= 14411
IGJvb3Rz 14412
IFNpdGU= 14413
YWxpbg== 14414
cGk= 14415
IHZpcmFs 14416
IFVD 14417
REVS 14418
IFNleA== 14419
IHN0b2Nrcw== 14420
Y3VycmVudA== 14421
IGNodXJjaGVz 14422
IFJhcmU= 14423
IE11cnBoeQ== 14424
IGRlbmlhbA== 14425
IEdhbWluZw== 14426
IHRvdWc= 14427
IG5pY2s= 14428
IG1ha2Vycw== 14429
IFJvbmFsZA== 14430
IGdlbmVyb3Vz 14431
IERvYw== 14432
IE1vcnJpcw== 14433
IHRyYW5zZm9ybWVk 14434
IE5vcm1hbA== 14435
IDEwNA== 14436
IEtpY2tzdGFydGVy 14437
IFVwb24= 14438
T25saW5l 14439
IElSUw== 14440
IHdyYXA= 14441
IGxvdmluZw== 14442
IGFycml2ZXM= 14443
IER1ZQ== 14444
IGhldGVy 14445
IE1hZGU= 14446
IHJlbnRhbA== 14447
IGJlbG9uZ3M= 14448
IGF0dG9ybmV5cw== 14449
IGNyb3Bz 14450
IG1hdGNoZWQ= 14451
dWx1bQ== 14452
b2xpbmU= 14453
MTA5 14454
IGRpc3Bhcg== 14455
IGJ1eWVycw== 14456
IENhbWJyaWRnZQ== 14457
IGV0aGljcw== 14458
cm91cHM= 14459
IGp1c3RpZmllZA== 14460
IG1hcmdpbmFs 14461
IHJlc3BlY3RlZA== 14462
d2lubmluZw== 14463
IG5vZGRlZA== 14464
IFNlcmdl 14465
IEZvcm1lcg== 14466
Q3JhZnQ= 14467
IyMjIyMjIyMjIyMjIyMjIw== 14468
IFdhcm5lcg== 14469
IGRhc2g= 14470
ZXRl 14471
IGVudGVydA== 14472
IEVzY2FwZQ== 14473
b3V0aGVhc3Q= 14474
IGtuZWVz 14475
IEJvbWI= 14476
IHJ1Zw== 14477
UGFzcw== 14478
IGF0dGl0dWRlcw== 14479
Z292ZXJubWVudA== 14480
IFByaW9y 14481
IHF1YWxpdGllcw== 14482
IG5vdGlmaWNhdGlvbg== 14483
IFBob25l 14484
bGll 14485
IGFudGljaXBhdGVk 14486
IENvbWJhdA== 14487
IEJhcnJ5 14488
IDE5ODI= 14489
VXNlcnM= 14490
b25lcg== 14491
IGNvbXB1dGluZw== 14492
IENvbm5lY3RpY3V0 14493
IGxlc3Nlcg== 14494
IHBlZXJz 14495
IEN1 14496
IHRlY2huaWNhbGx5 14497
IHN1Ym1pc3Npb24= 14498
IFVuaXZlcnNhbA== 14499
IG1hbnVhbGx5 14500
b3VyZ2U= 14501
IHJlc3BvbmRlbnRz 14502
IEJUQw== 14503
IEhvc3Q= 14504
IGZhcmU= 14505
IEJpcmQ= 14506
IHJlY2VpcHQ= 14507
YWxzbw== 14508
IGphY2s= 14509
IGFncmljdWx0dXJl 14510
IHNrdWxs 14511
ICE9 14512
IHBhc3NpdmU= 14513
IENJ 14514
IHNvY2lldGllcw== 14515
IHJlbWluZGVk 14516
IGludGVyZmVyZW5jZQ== 14517
QnV5 14518
IOKc 14519
Z29u 14520
IHNjcnV0aW55 14521
IFdpdGNo 14522
IGNvbmR1Y3Rpbmc= 14523
IOOD 14524
IGV4Y2hhbmdlcw== 14525
IE1pdGNoZWxs 14526
IGluaGFiaXQ= 14527
IHR3aXN0 14528
QkQ= 14529
IHdoZXJldmVy 14530
Z3JvdXBvbg== 14531
IGpva2Vz 14532
IEJlbmphbWlu 14533
IFJhbmRvbQ== 14534
ZnJhbWU= 14535
IExpb25z 14536
IGhpZ2hsaWdodGVk 14537
IEFya2Fuc2Fz 14538
RW50 14539
IHBpbGU= 14540
IHByZWxpbQ== 14541
Z3M= 14542
bWluZGVk 14543
IGZlbG9ueQ== 14544
IEdB 14545
IEx1Y2s= 14546
IHByYWN0aWNhbGx5 14547
IEJvcw== 14548
IGFjdHJlc3M= 14549
RGFt 14550
IEJvdQ== 14551
IHZpc2E= 14552
IGVtYmVkZGVk 14553
IGh5YnJpZA== 14554
IGVhcmxpZXN0 14555
IHNvb25lcg== 14556
c29jaWFs 14557
IEhB 14558
IHN0ZWVw 14559
IGRpc2FkdmFudA== 14560
IGV4cGxvaXQ= 14561
IEVnZw== 14562
IFVsdHJh 14563
IG5lY2Vzc2l0eQ== 14564
TG9jYWw= 14565
aWVnZQ== 14566
IGRhdGVk 14567
IG1hc3Nlcw== 14568
IHN1YnNjcmlwdGlvbg== 14569
cGxlc3M= 14570
IGFub255bQ== 14571
IHByZXN1bWFibHk= 14572
Qmx1ZQ== 14573
VGhlaXI= 14574
YXNrZXRiYWxs 14575
IFBoaWxpcA== 14576
IGNvbWVk 14577
bG9hZGVk 14578
cmFuZQ== 14579
IHJlZmxlY3Rpb24= 14580
Q2hpbmE= 14581
IGV4dGVuZHM= 14582
IGZvcm1pbmc= 14583
IHVuZGVycw== 14584
MjAwMQ== 14585
IGdyYXQ= 14586
IGNvbmNlbnRyYXRpb25z 14587
IGluc3VsaW4= 14588
IHNlY3VsYXI= 14589
IHdoaWxzdA== 14590
IHdpbm5lcnM= 14591
QWR2ZXJ0aXNlbWVudHM= 14592
IGRlbGliZXJhdGVseQ== 14593
IFdvcmtpbmc= 14594
IHNpbms= 14595
ZXRpY3M= 14596
ZGFsZQ== 14597
IG1hbmRhdGU= 14598
IGdyYW0= 14599
IHZhY2F0aW9u 14600
IHdhcm5pbmdz 14601
cmlwcA== 14602
IFRIQVQ= 14603
IGNvbW1lbnRhcnk= 14604
IGludHU= 14605
IGFlc3Q= 14606
IHJlYXNvbmluZw== 14607
IGJyZWFrZG93bg== 14608
IFpvbWJpZQ== 14609
IC0tPg== 14610
IFBvbGl0aWNhbA== 14611
Y290dA== 14612
IHRocnVzdA== 14613
IHRlY2hub2xvZ2ljYWw= 14614
IGRlY2lkaW5n 14615
IHRyYWZmaWNraW5n 14616
TG9uZw== 14617
V2VsY29tZQ== 14618
cHJpc2luZw== 14619
IENvbW11bmljYXRpb25z 14620
IGVuZG9ycw== 14621
IHN3aWZ0 14622
IG1ldGFib2w= 14623
Y29pbnM= 14624
cmVzYQ== 14625
IEhUVFA= 14626
IGVucm9sbA== 14627
IEhhcHB5 14628
dXNy 14629
aW50YWdl 14630
IFsi 14631
dWFibHk= 14632
IE1hdGVyaWFs 14633
IHJlcGVhbA== 14634
U2VwdA== 14635
a2g= 14636
IE1vZGk= 14637
IHVuZGVybmVhdGg= 14638
IElM 14639
c2hvcmU= 14640
IGRpYWdub3NlZA== 14641
YWNldXRpY2Fs 14642
IHNob3dlcg== 14643
YXV4 14644
IFN3aXRjaA== 14645
IFN0cmVuZ3Ro 14646
IGppaGFk 14647
bmF0aW9uYWw= 14648
IHRyYXVtYQ== 14649
dXNzeQ== 14650
b25p 14651
IGNvbnNvbGlk 14652
IGNhbG9yaWVz 14653
IEZseW5u 14654
YWdnZWQ= 14655
MTY4 14656
IFBpbms= 14657
IGZ1bGZpbGw= 14658
IGNoYWlucw== 14659
IG5vdGFibHk= 14660
IEFW 14661
TGlmZQ== 14662
IENodWNr 14663
bXVz 14664
IFVyYmFu 14665
IEhlbmQ= 14666
IGRlcG9zaXQ= 14667
IFNhZA== 14668
IGFmZmFpcg== 14669
T1JL 14670
aWV2YWw= 14671
IEZEQQ== 14672
IHRyb3A= 14673
IE92ZXJhbGw= 14674
IHZpcnR1ZQ== 14675
IHNhdGlzZmFjdGlvbg== 14676
YXVuZA== 14677
IGx1bg== 14678
IFN3aXR6ZXJsYW5k 14679
IE9wZXJhdGlvbg== 14680
cHJvY2Vzcw== 14681
IHNob29r 14682
IGNvdW50aWVz 14683
bGVhc2Vk 14684
IENoYXJsb3R0ZQ== 14685
MTEy 14686
IHRyYW5zY3JpcHQ= 14687
IHJlZGQ= 14688
cHVzaA== 14689
IEhleQ== 14690
IEFuYWx5c2lz 14691
WyI= 14692
IGFsdGVybmF0aXZlcw== 14693
YXJkbGVzcw== 14694
IGVsZXBo 14695
IHByZWp1ZA== 14696
IExlYWY= 14697
SGF2aW5n 14698
IEh1Yg== 14699
IGV4cHJlc3Npb25z 14700
IFZvbHVtZQ== 14701
IHNob2NraW5n 14702
IFJlZHM= 14703
IHJlYWRpbHk= 14704
IHBsYW5ldHM= 14705
YWRhdGE= 14706
IGNvbGxhcHNlZA== 14707
IE1hZHJpZA== 14708
IGlycml0 14709
aXBwZXI= 14710
IEVuYw== 14711
IFdpcmU= 14712
IGJ1eno= 14713
IEdQ 14714
YXNoYQ== 14715
IGFjY2lkZW50YWxseQ== 14716
dXJ1 14717
IGZydXN0cmF0ZWQ= 14718
IFNB 14719
IGh1bmdyeQ== 14720
IEh1ZmY= 14721
IGxhYmVscw== 14722
YW50bw== 14723
IEVQ 14724
IGJhcnJpZXJz 14725
KXw= 14726
IEJlcmtlbGV5 14727
IEpldHM= 14728
IHBhaXJz 14729
IExhbg== 14730
SmFtZXM= 14731
IEJlYXI= 14732
IGh1bW9y 14733
IExpYmVydHk= 14734
IG1hZ25pdHVkZQ== 14735
IGFnaW5n 14736
IE1hc29u 14737
IGZyaWVuZHNoaXA= 14738
dW1ibGluZw== 14739
IGVtZXJnZQ== 14740
IG5ld3NwYXBlcnM= 14741
IGFtYml0aW91cw== 14742
IFJpY2hhcmRz 14743
YXRlcm5hbA== 14744
IDE5ODE= 14745
IGNvb2tpZXM= 14746
IHNjdWxwdA== 14747
IHB1cnN1aXQ= 14748
TG9jYXRpb24= 14749
IHNjcmlwdHM= 14750
cGM= 14751
IGFycmFuZ2VtZW50cw== 14752
IGRpYW1ldGVy 14753
IGxvc2Vz 14754
YW1hdGlvbg== 14755
IGxpcXU= 14756
IEpha2U= 14757
YXJldHRl 14758
IHVuZGVyc3RhbmRz 14759
IFplbg== 14760
dm0= 14761
IGFwcHJvdmU= 14762
IHdpcA== 14763
IHVsdHJh 14764
IGludGVuZA== 14765
IERJ 14766
YXNjdWxhcg== 14767
IHN0YXlz 14768
IEtvcg== 14769
IEts 14770
IGludmVzdGluZw== 14771
TGE= 14772
IGJlbGlldmluZw== 14773
YmFk 14774
bW91dGg= 14775
IHRheHBheWVy 14776
44OD 14777
IFF1ZWJlYw== 14778
IGxhcA== 14779
IFN3aXNz 14780
ZHJvcA== 14781
IGRyYWlu 14782
aXJp 14783
ZXRj 14784
ZnRlbg== 14785
IE5leA== 14786
IHN0cmF3 14787
IHNjcmVhbWluZw== 14788
IGNvdW50ZWQ= 14789
IGRhbWFnaW5n 14790
IGFtYmFzc2Fkb3I= 14791
Y2VudHVyeQ== 14792
IHByb3g= 14793
IGFycmVzdHM= 14794
dXY= 14795
aWxhdGVyYWw= 14796
IENoYXJn 14797
IHByZXNjcmliZWQ= 14798
IGluZGVwZW5kZW50bHk= 14799
IGZpZXJjZQ== 14800
IEJhYnk= 14801
IGJyYXZl 14802
IHN1aXRz 14803
PT4= 14804
IGJhc2VsaW5l 14805
IFJhdGU= 14806
IGlzbGFuZHM= 14807
ICgo 14808
Z3JlZW4= 14809
aXhlbHM= 14810
IG5hbWVseQ== 14811
IFZpbGxhZ2U= 14812
dGhhbg== 14813
YW15 14814
VmVyc2lvbg== 14815
Z21haWw= 14816
ZW50aWFscw== 14817
IFN1ZA== 14818
IE1lbGJvdXJuZQ== 14819
IGFycml2aW5n 14820
IHF1YW50dW0= 14821
ZWZm 14822
cm9wb2xpdGFu 14823
VHJp 14824
IGZ1bmVyYWw= 14825
IElS 14826
w4PDgsODw4LDg8OCw4PDgsODw4LDg8OCw4PDgsODw4I= 14827
IENvYg== 14828
aXRhYmx5 14829
IHR1cmI= 14830
IGNvbWJv 14831
UmV2aWV3 14832
IGRlcGxveW1lbnQ= 14833
dWl0eQ== 14834
IEJvdHQ= 14835
IGludmlzaWJsZQ== 14836
IHJlbmRlcmluZw== 14837
IHVubG9ja2Vk 14838
IGFxdQ== 14839
IFZsYWRpbWly 14840
IHBhZA== 14841
IEJyYWlu 14842
IExlZ2FjeQ== 14843
ZHJhZ29u 14844
IEt1cmRpc2g= 14845
IHNvdW5kZWQ= 14846
IGRldGFpbmVk 14847
IERN 14848
Z2FyeQ== 14849
IGRhdWdodGVycw== 14850
IGRpc3R1cmJpbmc= 14851
dWth 14852
IFBhcmFk 14853
IHRhc3Q= 14854
IHVuZm9ydHVuYXRl 14855
IHVs 14856
ZW1pbg== 14857
IGF0dGVuZGFuY2U= 14858
dHJs 14859
IHBhcmtz 14860
IE1lbW9yaWFs 14861
IEFsaWNl 14862
b3RoeQ== 14863
Z3VhcmQ= 14864
IERpc2U= 14865
IFNoYW4= 14866
IEZvcnVt 14867
UmljaA== 14868
IHNoaWZ0ZWQ= 14869
dWV6 14870
IGxpZ2h0ZXI= 14871
IE1hZ24= 14872
IGNvZA== 14873
U2No 14874
aGFtbWFk 14875
UHVi 14876
MzUw 14877
IFBva2Vtb24= 14878
IHByb3RvdHlwZQ== 14879
IHVucmU= 14880
QmFzZQ== 14881
IFN0dWRlbnRz 14882
IFJlcGx5 14883
IENvbW11bmlzdA== 14884
IGdhdQ== 14885
IFR5bGVy 14886
SVo= 14887
IHBhcnRpY2lwYXRlZA== 14888
IHN1cHJlbQ== 14889
IERldGFpbHM= 14890
IHZlc3NlbHM= 14891
cm9k 14892
IHRyaWJl 14893
a2VlcA== 14894
IGFzc3VtcHRpb25z 14895
IHBvdW5k 14896
IGNydWRl 14897
IEF2YWlsYWJsZQ== 14898
IHN3aW1taW5n 14899
IGluY2x1c2lvbg== 14900
IGFkdmFuY2Vz 14901
Y3VsYXRpb24= 14902
IGNvbnNlcnZhdGlvbg== 14903
IG92ZXJk 14904
IEJ1ZmZhbG8= 14905
QXJ0aWNsZQ== 14906
ZWRnZQ== 14907
IGF3YQ== 14908
IE1hZGlzb24= 14909
IHNpZGV3 14910
IGNhdGFzdA== 14911
IEtyaXN0 14912
dWNsZQ== 14913
IEhpZ2h3YXk= 14914
IFRlcnJvcg== 14915
IGFjdGl2YXRpb24= 14916
IHVuY29uc2Npb3Vz 14917
IFNhdGFu 14918
IFN1c2Fu 14919
aWxsZXJ5 14920
IGFycmFuZ2Vk 14921
aW9w 14922
IHJ1bW9ycw== 14923
dXJyaW5n 14924
dGhpbms= 14925
IEtlaXRo 14926
IEtpbmQ= 14927
IGF2b2lkaW5n 14928
Ynlu 14929
bnV0 14930
IFNwZWFrZXI= 14931
cnVz 14932
bmFtZXM= 14933
IGd1aWx0 14934
IE9seW1waWNz 14935
IHNhaWw= 14936
IE1lcw== 14937
bGV2YW50 14938
IENvbHVtYnVz 14939
YWZ0 14940
Q2l0eQ== 14941
U291dGg= 14942
IEhhcnZleQ== 14943
IFB1bg== 14944
U2V2ZXJhbA== 14945
IG1lbnRhbGx5 14946
IGltcHJlc3M= 14947
bW91bnQ= 14948
IFVidW50dQ== 14949
4oCU4oCU4oCU4oCU4oCU4oCU4oCU4oCU 14950
IFN1cGVybWFu 14951
IE1Qcw== 14952
IGludGVudGlvbnM= 14953
IFJhY2luZw== 14954
IGxpa2VsaWhvb2Q= 14955
IDI0MA== 14956
VG90YWw= 14957
IHRveXM= 14958
IFdhdHNvbg== 14959
IHVyZ2U= 14960
TGVhcg== 14961
IFBhcGVy 14962
IG9jY3VycmluZw== 14963
IEJlbmc= 14964
IENlcnQ= 14965
IHN0b25lcw== 14966
VGlt 14967
IFR3aW4= 14968
emI= 14969
IER5bmFt 14970
IHBvbGl0aWNpYW4= 14971
a2Vucw== 14972
IEVudGVycHJpc2U= 14973
VVRFUlM= 14974
IGFib2w= 14975
IHJlZnJlc2g= 14976
IGFyYml0cmFyeQ== 14977
cGVjdGlvbg== 14978
IHRyb3VibGVz 14979
IH0pOw== 14980
dHY= 14981
IHBpbG90cw== 14982
IGRpc3RyaWJ1dGU= 14983
IGF1ZGl0 14984
IHBhdXNl 14985
b3JpZ2luYWw= 14986
IHJpdmFscw== 14987
wqM= 14988
Rmln 14989
VEw= 14990
YWJpbA== 14991
cnlpbmc= 14992
TGlu 14993
aW9uZWQ= 14994
bG9u 14995
IGZhbmN5 14996
IGNyYXNoZWQ= 14997
IHRyYWN0 14998
IHNoZWQ= 14999
IGNvbnN1bWU= 15000
QmFzZWQ= 15001
ZG93bmxvYWQ= 15002
aW5pdA== 15003
IHZvbHRhZ2U= 15004
SW50cm9kdQ== 15005
IGNvbmRlbW5lZA== 15006
IEZpbmFuY2U= 15007
cmVzcGVjdA== 15008
IGV4Y2x1ZGVk 15009
IGVzdGFibGlzaGluZw== 15010
aGVyaWM= 15011
IGhlcml0YWdl 15012
IHNwZWN0YWN1bGFy 15013
IHVuc3Q= 15014
IFNub3dkZW4= 15015
IExhbmU= 15016
U2Fu 15017
IHByb3RlY3Rpb25z 15018
c3RydWN0aW9u 15019
aW5jaW5u 15020
IG1hY3Jv 15021
Q3VzdG9t 15022
aW9zaXR5 15023
IGVzcA== 15024
IGZ1bmN0aW9uaW5n 15025
IG11c2g= 15026
IHB1enpsZQ== 15027
IGV0aGljYWw= 15028
TWFs 15029
IGdvdmVybmluZw== 15030
IEZlcmd1c29u 15031
IHJlc3RvcmVk 15032
IHN0cmVzc2Vk 15033
IENvdW50ZXI= 15034
IEthcw== 15035
Y2xpcA== 15036
QU5T 15037
IHNlaXo= 15038
VUs= 15039
Ynlzcw== 15040
b2xkb3du 15041
YXBp 15042
IHBlcm1hbmVudGx5 15043
b3VudGVycw== 15044
V2VzdA== 15045
VGhyb3VnaA== 15046
TGlnaHQ= 15047
YXRvZXM= 15048
IG5lYXQ= 15049
IGNvcmQ= 15050
dXJlcg== 15051
IHNldmVyZWx5 15052
IEF2ZW4= 15053
IGludGVycm9n 15054
IHRyaXBsZQ== 15055
R2l2ZW4= 15056
TnVtYmVy 15057
IGFyaXNl 15058
IHNoZXI= 15059
cGxhbnQ= 15060
IGZsb3dlcg== 15061
IENvdQ== 15062
IGF0ZQ== 15063
IG5ld2Vy 15064
YnVs 15065
IG1lYW53aGlsZQ== 15066
IExhaXI= 15067
IGFkanVzdG1lbnQ= 15068
IENvcHlyaWdodA== 15069
IGRpdmVycw== 15070
aW9sb2dpY2Fs 15071
IGdhbWVycw== 15072
b2F0 15073
IGhpc3RvcmljYWxseQ== 15074
IGFuYWxvZw== 15075
IGxvbmd0aW1l 15076
IHByZXNjcmlwdGlvbg== 15077
IE1pc3Q= 15078
IEh5cGVy 15079
IE1haW5l 15080
IERlaXR5 15081
IG11bHRpcGw= 15082
IFJlaW5jYXJu 15083
IEh5ZA== 15084
IFBpYw== 15085
U2ls 15086
cmFudHM= 15087
IENyaXM= 15088
Ljs= 15089
KHs= 15090
ZXBlbmRlbmNl 15091
IHJlY3k= 15092
YXRldXI= 15093
IHF1YWQ= 15094
IGdsb2I= 15095
IGNvbmNlZA== 15096
dGVhbQ== 15097
IGNhcGl0YWxpc3Q= 15098
IExvdA== 15099
IHJveWFs 15100
IEN5YmVy 15101
IGJsYWNrcw== 15102
bWV0aWM= 15103
cml2 15104
IERhbm55 15105
IHNwbw== 15106
IFJP 15107
IGFuaW1hdGVk 15108
cnlwdGVk 15109
IERlcHV0eQ== 15110
IHJlbmRlcmVk 15111
RkU= 15112
IHN0cmVhaw== 15113
IGNsb3Vkcw== 15114
IERvdWc= 15115
fn5+fn5+fn4= 15116
IGRpc2NvdXI= 15117
IFZlaA== 15118
IHBzeWNob2xvZ3k= 15119
IEpvdXJuZXk= 15120
IGNyeXN0YWw= 15121
IEZyb3N0 15122
IHN1c3BpY2lvbg== 15123
IHJlbGF0ZQ== 15124
b3J1cw== 15125
IENyeXB0 15126
IE5WSURJQQ== 15127
Y29tZWQ= 15128
dXRpbmc= 15129
aW5jaW5uYXRp 15130
IHZ1bG5lcmFiaWxpdHk= 15131
b3N0aWM= 15132
IGlzb2xhdGlvbg== 15133
IGNvb2xpbmc= 15134
IENvYWxpdGlvbg== 15135
IDExOQ== 15136
Rm91cg== 15137
IERlYWw= 15138
IOKJ 15139
c2VtYmxl 15140
cmFtZW50 15141
IEJhcmNlbG9uYQ== 15142
IDEwMg== 15143
IGNvY2FpbmU= 15144
b2NhbHlwc2U= 15145
RmVi 15146
b2dlbmlj 15147
IG11dGF0aW9u 15148
IGNyeXB0b2M= 15149
IEtlbA== 15150
IEdpdA== 15151
YWlz 15152
IHNpc3RlcnM= 15153
QU5L 15154
IGFjdGl2YXRl 15155
VGVy 15156
IGRyZWFk 15157
eWxvbg== 15158
IHByb3ByaQ== 15159
QXVzdA== 15160
IERlZmF1bHQ= 15161
IG91dGRvb3I= 15162
IHNoZWVy 15163
Y2VpdmU= 15164
IGdlbnRseQ== 15165
0L4= 15166
UHJvZ3JhbQ== 15167
IOKGkg== 15168
IHZlZ2Fu 15169
IENydXM= 15170
IHJlc3BvbnNpYmlsaXRpZXM= 15171
IEhS 15172
T0xE 15173
IHByZXZlbnRz 15174
IHN0aWZm 15175
IFdlcmU= 15176
IGF0aGxldGlj 15177
IFNjb3Jl 15178
ICk6 15179
IGNvbHVtbnM= 15180
IExvYw== 15181
YXZhaWxhYmxl 15182
IEZyYW0= 15183
IFNlc3Npb25z 15184
IGNvbXBhbmlvbg== 15185
IHBhY2tz 15186
MTQw 15187
IEtuaWdodHM= 15188
IGZhcnQ= 15189
IHN0cmVhbXM= 15190
IHNob3Jl 15191
IGFwcGVhbHM= 15192
IFBlcmZvcm1hbmNl 15193
aGF1bA== 15194
IFN0cmE= 15195
IE5hZw== 15196
MTAz 15197
IFRyYW5zcG9ydGF0aW9u 15198
QkI= 15199
RXY= 15200
emFu 15201
UHVibGlj 15202
IHR3aW4= 15203
dWxzaW9u 15204
TXVsdA== 15205
IGVsZWN0cm8= 15206
IHN0YXR1ZQ== 15207
YXRpb25hbGx5 15208
IE5vcnQ= 15209
IGluc3BlY3Rpb24= 15210
Lyo= 15211
aWd1ZQ== 15212
IGNvbXBhc3Npb24= 15213
IFRhbGVz 15214
IFN0ZWlu 15215
IFNjcmVlbg== 15216
IEJ1Zw== 15217
IExpb24= 15218
Z2lybA== 15219
IHdpdGhkcmF3YWw= 15220
IG9iamVjdGl2ZXM= 15221
IGJsb29keQ== 15222
IHByZWxpbWluYXJ5 15223
IGphY2tldA== 15224
IGRpbWVuc2lvbnM= 15225
IENvb2w= 15226
IE9jY3Vw 15227
IHdyZWNr 15228
IGRvdWJsZWQ= 15229
YW5raW5n 15230
IDE5NzU= 15231
IGdsYXNzZXM= 15232
IFdhbmc= 15233
cHJvdg== 15234
UGF0aA== 15235
Y29ubmVjdGVk 15236
IE11bHRp 15237
IE5vcndheQ== 15238
YWdvbmlzdA== 15239
IGZlYXJlZA== 15240
IHRvdWNoaW5n 15241
IGFyZ3VhYmx5 15242
wq/Cr8Kvwq/Cr8Kvwq/Crw== 15243
IE5DQUE= 15244
Y2hlbQ== 15245
IHNwYXQ= 15246
IFdXRQ== 15247
IENlbA== 15248
aWdnZXI= 15249
IGF0dGFja2Vy 15250
IEpvaW4= 15251
b2JqZWN0 15252
ZXR0YQ== 15253
IGVsaW1pbmF0ZWQ= 15254
ZGV0 15255
IGRlc3RydWN0 15256
IEx1Y2Fz 15257
Y3R1YXJ5 15258
MTgw 15259
IEJyYWR5 15260
IEJsdWVz 15261
QmF5 15262
YXVrZWU= 15263
IHRpbWVsaW5l 15264
IGRlbGVnYXRlcw== 15265
d3JpdHRlbg== 15266
dWZmaWNpZW50 15267
IHNoYXBlcw== 15268
Q29weXJpZ2h0 15269
b3VibGU= 15270
c2VydmljZQ== 15271
IHBpb25l 15272
IGNvbGxlZ2Vz 15273
IHJvd3M= 15274
IHNwaXRl 15275
IGFzc2Vzc2Vk 15276
MzYw 15277
IGxlYXNl 15278
IGNvbmZpZGVudGlhbA== 15279
Y2tlcg== 15280
IE1hbm5pbmc= 15281
IFZvaWNl 15282
IHNlYWxlZA== 15283
IGNhbGN1bGF0ZQ== 15284
Tk8= 15285
IEFzc2lzdGFudA== 15286
IHRlZW5hZ2Vy 15287
dWxlbnQ= 15288
YXRoZXJpbmU= 15289
IG1vY2s= 15290
IGRpYW1vbmQ= 15291
IGZlc3Q= 15292
IHN3aXRjaGVk 15293
IHJlc3VtZQ== 15294
IFB1ZXJ0bw== 15295
IGxhbmVz 15296
aXJhdGlvbg== 15297
IFNpbWlsYXJseQ== 15298
IHJvZA== 15299
IFNlbA== 15300
IFBhbGFjZQ== 15301
IExpbWl0ZWQ= 15302
ZW91cw== 15303
IHZhcmlhbnQ= 15304
IHdhcmQ= 15305
ICkp 15306
U2hvdw== 15307
T09L 15308
QWxleA== 15309
IE5lcA== 15310
YnJpcw== 15311
IFdpa2lwZWRpYQ== 15312
IGV4Y2VwdGlvbmFs 15313
IG1hbmFnZXM= 15314
IERyYXc= 15315
QWdhaW4= 15316
IGNvcHBlcg== 15317
dXR0 15318
IGV4cG9ydHM= 15319
IHBvcnRmb2xpbw== 15320
IGVsZXZhdGVk 15321
UmF0ZWQ= 15322
IE90aGVyd2lzZQ== 15323
IFRhY3Q= 15324
IFNoZWw= 15325
IFRY 15326
IuKAlA== 15327
IHJlc3Vy 15328
IFdh 15329
dmVuYW50 15330
IG1vbmV0YXJ5 15331
cGVvcGxl 15332
RW1haWw= 15333
IGZpZnR5 15334
IFN3ZWV0 15335
IE1hbGF5c2lh 15336
IGNvbmZ1c2luZw== 15337
IFJpbw== 15338
dWRh 15339
dXRlbmFudA== 15340
Iik7 15341
IHByYWlzZWQ= 15342
IHZvbHVtZXM= 15343
dHVybg== 15344
IG1hdHVyZQ== 15345
IG5vbnByb2ZpdA== 15346
IHBhc3Npb25hdGU= 15347
IFByaXZhdGU= 15348
IDEwMw== 15349
IGRlc2NlbmQ= 15350
56We 15351
dWZmeQ== 15352
aGVhZGVk 15353
V2hldGhlcg== 15354
cmllbg== 15355
emVjaA== 15356
YmVpdA== 15357
IGNocm9t 15358
IE1jTQ== 15359
IGRhbmNpbmc= 15360
IGVsZWc= 15361
IE5vdGljZWQ= 15362
MTE1 15363
IGFkdm9jYWN5 15364
RU5UUw== 15365
YW1ibGluZw== 15366
IE1pbm9y 15367
IEZpbm4= 15368
IHByaW9yaXRpZXM= 15369
IHRoZXJlb2Y= 15370
IFN0YWdl 15371
IFJvZ2Vycw== 15372
IHN1YnN0aXR1dGU= 15373
IEphcg== 15374
IEplZmZlcnNvbg== 15375
IGxpZ2h0bHk= 15376
MTAy 15377
IExpc2E= 15378
dWl0cw== 15379
eXNpY2Fs 15380
IHNoaWZ0cw== 15381
IGRyb25lcw== 15382
IHdvcmtwbGFjZQ== 15383
IHJlc2lk 15384
ZW5zZWQ= 15385
YWhu 15386
IHByZWZlcmVuY2Vz 15387
c2VydmVy 15388
IGRlYmF0ZXM= 15389
ZG9j 15390
IEdvZHM= 15391
IGhlbGljb3B0ZXI= 15392
IGhvbm91cg== 15393
IGNvbnNpZGVyYWJseQ== 15394
ZWRlZA== 15395
IEZlbWFsZQ== 15396
IEFubmU= 15397
IHJldW4= 15398
IEZhY2U= 15399
IEhhbGxvdw== 15400
IEJ1ZGdldA== 15401
IGNvbmRlbW4= 15402
IHRlbmRlcg== 15403
UHJvZg== 15404
b2NyYXRpYw== 15405
IFR1cm5lcg== 15406
IEFncmlj 15407
IDE5NzY= 15408
IGFwdA== 15409
ZGlzYw== 15410
IEZpZ2h0ZXI= 15411
IEF1cg== 15412
IGdhcmJhZ2U= 15413
aW5wdXQ= 15414
IEthcmw= 15415
IE9saXZlcg== 15416
IExhbmd1YWdl 15417
a24= 15418
Tm9u 15419
IENsYXI= 15420
IHRyYWRpdGlvbnM= 15421
IGFkdmVydGlzZW1lbnQ= 15422
IFNvcg== 15423
IGFyY2hpdmU= 15424
IHZpbGxhZ2Vz 15425
NzUw 15426
IGltcGxlbWVudGluZw== 15427
d2F1a2Vl 15428
IGRpZXRhcnk= 15429
IHN3aXRjaGluZw== 15430
UmVwdWJsaWM= 15431
IHZlbG9jaXR5 15432
IGNpdA== 15433
IEF3YXJkcw== 15434
IGZpbmFuY2luZw== 15435
IGxhc3RlZA== 15436
KV0= 15437
IHJlbWluZGVy 15438
UGVyc29u 15439
IHByZWNpc2lvbg== 15440
IGRlc2lnbmVycw== 15441
IEZyaWVk 15442
IEJvcmRlcg== 15443
IHRyYWdpYw== 15444
IHdpZWxk 15445
IGluaXRpYXRpdmVz 15446
IFRhbms= 15447
d2Vy 15448
IGpvaW5z 15449
Um8= 15450
aW5lcnk= 15451
IGFycm93 15452
IGdlbmVyYXRpbmc= 15453
Zm91bmRlcg== 15454
IHNlYXJjaGVz 15455
IHJhbmRvbWx5 15456
QWNjZXNz 15457
IGJhdGNo 15458
IHBvc2Vk 15459
bGF0 15460
IHB1cnN1aW5n 15461
YXNh 15462
IHRlc3RpZmllZA== 15463
Zm9ybWluZw== 15464
IFNoYXI= 15465
d2lraQ== 15466
IEVpdGhlcg== 15467
U29tZXRpbWVz 15468
IHNlbmF0b3Jz 15469
IEpvaG5ueQ== 15470
IFRhbGliYW4= 15471
IEdQUw== 15472
IjoiLw== 15473
44Gu5Q== 15474
IGFuYWx5emVk 15475
IFJ1Ymlv 15476
IE1vdmVtZW50 15477
b3BhcmQ= 15478
aWlp 15479
U3RhbmQ= 15480
ZmlnaHQ= 15481
IGlnbm9yaW5n 15482
aWFuZw== 15483
IEdO 15484
c29ldmVy 15485
IFNUQVQ= 15486
IHJlZnVzaW5n 15487
IHN3ZWF0 15488
IGJheQ== 15489
UE9SVA== 15490
aXJtZWQ= 15491
YWt5 15492
IGRpc3Bybw== 15493
IGxhYmVsZWQ= 15494
IDEwOA== 15495
SGVsbG8= 15496
IHBsZWFzYW50 15497
YWJh 15498
IHRyaXVtcGg= 15499
IGFib2FyZA== 15500
IGluY29t 15501
IENyb3c= 15502
bGV0dA== 15503
IGZvbGs= 15504
IGNoYXNl 15505
YGA= 15506
IEJydXM= 15507
IHRlZW5z 15508
Y3Vl 15509
IHRlcnJhaW4= 15510
aHlk 15511
aWxpZ2h0 15512
T1JZ 15513
U3VwcG9ydA== 15514
ZXdz 15515
bGxp 15516
cmFpbnRz 15517
IENhbmQ= 15518
IGFidXNlZA== 15519
YWNobWVudA== 15520
bGFyZw== 15521
QmFz 15522
IENhbmNlcg== 15523
IDE5Nzg= 15524
IHN1cHBvcnRlcg== 15525
YWNjZXNz 15526
IFRlcm1pbg== 15527
IFRhbXBh 15528
IEFOWQ== 15529
IG5ld2VzdA== 15530
IENyaW1pbmFs 15531
ZWR1 15532
IDE5MzA= 15533
IGFkbWl0cw== 15534
IGVuZGU= 15535
IGZhaWx1cmVz 15536
dXJhdGU= 15537
ZnVsbmVzcw== 15538
Y3ljbA== 15539
IFN1YmplY3Q= 15540
IGluZmluaXRl 15541
dGhyZWU= 15542
V0E= 15543
cGl0 15544
IEluc3RhbGw= 15545
UmFk 15546
aWxpYXRpb24= 15547
R00= 15548
IGNvbnRpbmVudA== 15549
IGFjY29tbW9kYXRl 15550
IENsYXk= 15551
IHB1cA== 15552
IEZ1bmN0aW9u 15553
IGhhbW1lcg== 15554
IEFsYmVydGE= 15555
IHJldmlzZWQ= 15556
IG1pbm9yaXRpZXM= 15557
IG1lYXN1cmVtZW50 15558
Q29ubmVsbA== 15559
IGRpc2FibGU= 15560
IE1peA== 15561
SW5jcmU= 15562
IGZvcms= 15563
IFJvc2Vu 15564
IGltcGxpZXM= 15565
dW1ibHI= 15566
QU5H 15567
IHByb3RlaW5z 15568
IGFnZ3Jlc3Npb24= 15569
IGZhY2lsaXRhdGU= 15570
U04= 15571
IGlsbGVnYWxseQ== 15572
dWVy 15573
IGFjYWRlbQ== 15574
IHB1eno= 15575
IFNoaWZ0 15576
cGF5 15577
b2xsbw== 15578
IGF1ZGllbmNlcw== 15579
QnVpbGQ= 15580
IG5vYmxl 15581
IHN5bnRheA== 15582
4piF 15583
IGJlYW0= 15584
IEJlZA== 15585
IEFsZA== 15586
IG9yaWdpbnM= 15587
dmlkZW8= 15588
IDE5Nzc= 15589
IEFzc2F1bHQ= 15590
IGdhcmFnZQ== 15591
VGVhbQ== 15592
IHZlcmRpY3Q= 15593
IGR3YXI= 15594
IFZpcnR1YWw= 15595
ZXZlbnQ= 15596
S2VlcA== 15597
IHNlbnRpbWVudA== 15598
IHdpbGRsaWZl 15599
c2hpcnQ= 15600
IGJ1cmc= 15601
IHJlY29tbWVuZGF0aW9u 15602
cmVwcmVzZW50 15603
IGdhbGxlcnk= 15604
b3duZXJz 15605
IHNjaG9sYXI= 15606
IGNvbnZlbmllbmNl 15607
IFN3aWZ0 15608
IGNvbnZpbmM= 15609
Q2Fw 15610
IHdhcmZhcmU= 15611
IFZpc3VhbA== 15612
IGNvbnN0aXR1dGU= 15613
IGFib3J0 15614
IFdlYXRoZXI= 15615
IExvb2tpbmc= 15616
IEhlbQ== 15617
IG1hcnRpYWw= 15618
IGluY29taW5n 15619
ZXRpdGlvbg== 15620
IHRvbGVyYW5jZQ== 15621
IENyZWF0ZWQ= 15622
IGZsb3dz 15623
IEVsZGVy 15624
IHNvdWxz 15625
IGZvdWw= 15626
IFBhaW4= 15627
IENBTg== 15628
IDIyMA== 15629
YmM= 15630
aGVuZA== 15631
IGdlbml1cw== 15632
UmVhbA== 15633
IFdy 15634
b21ldGVy 15635
cGFk 15636
IGxpbWl0aW5n 15637
IFNp 15638
IExvcmU= 15639
IEFkdmVudHVyZXM= 15640
IHZhcmllZA== 15641
RGlzYw== 15642
Zmlu 15643
IFBlcnNvbmFs 15644
Q2hyaXM= 15645
IGludmVudGVk 15646
IGRpdmU= 15647
IFJpc2U= 15648
IG96 15649
IENvbWljcw== 15650
IGV4cG9zZQ== 15651
IFJlYg== 15652
bGV0dGVycw== 15653
c2l0ZQ== 15654
aW1hdGVk 15655
IGhhY2tpbmc= 15656
IGVkdWNhdGVk 15657
IE5vYm9keQ== 15658
IGRlcHJp 15659
IGluY2VudGl2ZQ== 15660
44K3 15661
IG92ZXJzaWdodA== 15662
IHRyaWJlcw== 15663
IEJlbGdpdW0= 15664
IGxpY2Vuc2luZw== 15665
b3VydA== 15666
UHJvZHVjdA== 15667
YWhs 15668
IEdlbQ== 15669
IHNwZWNpYWxpc3Q= 15670
IGNyYQ== 15671
YW5uZXJz 15672
IENvcmJ5bg== 15673
IDE5NzM= 15674
UkVBRA== 15675
IHN1bW1hcg== 15676
IG92ZXJsb29r 15677
IEFwcGxpY2F0aW9u 15678
IGluYXBwcm9wcmlhdGU= 15679
IGRvd25sb2FkZWQ= 15680
UXVl 15681
IEJlYXJz 15682
IHRodW1i 15683
IENoYXJhY3Rlcg== 15684
IFJlaW5jYXJuYXRlZA== 15685
IFNpZA== 15686
IGRlbW9uc3RyYXRlcw== 15687
c2t5 15688
IEJsb29tYmVyZw== 15689
IEFycmF5 15690
IFJlc3VsdHM= 15691
IEZvdXJ0aA== 15692
IEVEVA== 15693
IE9zY2Fy 15694
Y2VuZA== 15695
IDEwNg== 15696
IE5VTEw= 15697
IEhFUkU= 15698
bWF0Y2g= 15699
IEJydW4= 15700
IGdsdWNvc2U= 15701
aWVn 15702
ZWd1 15703
IGNlcnRpZmllZA== 15704
IHJlbGll 15705
IGh1bWFuaXRhcmlhbg== 15706
IHByYXllcnM= 15707
S2luZw== 15708
IG5hbg== 15709
aG91 15710
MTA4 15711
dWx1 15712
IHJlbmV3YWJsZQ== 15713
IGRpc3Rpbmd1aXNo 15714
IGRlbnNl 15715
IFZlbnQ= 15716
IFBhY2thZ2U= 15717
IEJvc3M= 15718
IGVkaXRvcnM= 15719
IG1pZ3I= 15720
VHJh 15721
IFBldGVycw== 15722
IEFyY3RpYw== 15723
MjAwNA== 15724
IENhcGU= 15725
IGxvY2FsbHk= 15726
IGxhc3Rpbmc= 15727
IGhhbmR5 15728
Liku 15729
UGFu 15730
IFJFUw== 15731
SW5kZXg= 15732
IHRlbnNpb25z 15733
IGZvcm1lcmx5 15734
IGlkZW9sb2dpY2Fs 15735
IHNlbnNvcnM= 15736
IGRlYWxlcnM= 15737
IGRlZmluZXM= 15738
U2s= 15739
IHByb2NlZWRz 15740
IHByb3h5 15741
YXppbmVz 15742
IEJhc2g= 15743
IFBhZA== 15744
IENyYWZ0 15745
ZWFsb3Vz 15746
IHNoZWV0cw== 15747
b21ldHJ5 15748
SnVuZQ== 15749
Y2xvY2s= 15750
VFQ= 15751
IFRoZWF0cmU= 15752
IEJ1eno= 15753
IGNoYXB0ZXJz 15754
IG1pbGxlbm4= 15755
IGRvdWdo 15756
IENvbmdyZXNzaW9uYWw= 15757
IGltYWdpbmVk 15758
YXZpb3I= 15759
IGNsaW5pYw== 15760
IDE5NDU= 15761
IGhvbGRlcg== 15762
cm9vdA== 15763
b2xlc3Rlcg== 15764
IHJlc3RhcnQ= 15765
Qk4= 15766
IEhhbWFz 15767
IEpvYg== 15768
IG9yYg== 15769
IHJhbQ== 15770
IGRpc2Nsb3Nl 15771
IHRyYW5zbGF0ZQ== 15772
IGltbWlncmFudA== 15773
IGFubm95aW5n 15774
IHRyZWF0eQ== 15775
YW5pdW0= 15776
IFRlYQ== 15777
IExlZ2lvbg== 15778
IGNyb3dkcw== 15779
IEJlYw== 15780
IEFlcg== 15781
b2h5ZA== 15782
QnJv 15783
TG9va2luZw== 15784
IGxicw== 15785
IGFnZ3Jlc3M= 15786
IHNlYW0= 15787
IGludGVyY2VwdA== 15788
IE1J 15789
bWVyY2lhbA== 15790
YWN0aXY= 15791
IENpdA== 15792
IGRpbWVuc2lvbg== 15793
IGNvbnNpc3RlbmN5 15794
IHJ1c2hpbmc= 15795
IERvdWdsYXM= 15796
IHRyaW0= 15797
SW5zdGFsbA== 15798
aWNrZXI= 15799
IHNoeQ== 15800
MTA2 15801
IG1lbnRpb25z 15802
cGVsbGVk 15803
IFRhaw== 15804
Y29zdA== 15805
IGNsYXNzcm9vbQ== 15806
IGZvcnR1bmU= 15807
ZHJpdmVu 15808
IHVubGU= 15809
IFdoZWVs 15810
IGludmVzdG9y 15811
IE1hc3RlcnM= 15812
a2l0 15813
IGFzc29jaWF0aW9ucw== 15814
IEV2b2x1dGlvbg== 15815
b3Bpbmc= 15816
dXNjcmlwdA== 15817
IHByb3ZpbmNpYWw= 15818
IFdhbHRlcg== 15819
YXZp 15820
U08= 15821
IHVubGltaXRlZA== 15822
RW5nbGlzaA== 15823
IENhcmRz 15824
IEVib2xh 15825
bmVyZWQ= 15826
IHJldmVuZ2U= 15827
IG91dHJpZ2h0 15828
dW1wZXI= 15829
IGZpdHRpbmc= 15830
IFNvbGlk 15831
IGZvcm1hbGx5 15832
IHByb2JsZW1hdGlj 15833
IGhhemFyZA== 15834
IGVuY3J5cHRpb24= 15835
IHN0cmFpZ2h0Zm9yd2FyZA== 15836
IEFL 15837
IHBzZQ== 15838
IE9yYg== 15839
IENoYW1iZXI= 15840
IE1haw== 15841
Q29udGVudHM= 15842
IGxveWFsdHk= 15843
IGx5cmljcw== 15844
IFN5bQ== 15845
IHdlbGNvbWVk 15846
IGNvb2tlZA== 15847
IG1vbm9w 15848
IG51cnNl 15849
IG1pc2xlYWRpbmc= 15850
IGV0ZXJuYWw= 15851
IHNoaWZ0aW5n 15852
ICs9 15853
Vmlz 15854
IGluc3RpdHV0aW9uYWw= 15855
aWxsYXJ5 15856
IHBhbnQ= 15857
VkVSVA== 15858
IEFDQw== 15859
IEVuaA== 15860
IGluY29u 15861
IFJFVVRFUlM= 15862
IGRvbmF0ZWQ= 15863
4oCm4oCm4oCm4oCm 15864
SW50ZXJu 15865
IGV4aGliaXQ= 15866
IHRpcmU= 15867
IFJpYw== 15868
IENoYW1waW9u 15869
IE11aGFtbWFk 15870
TklORw== 15871
IFNvY2Nlcg== 15872
IG1vYmlsaXR5 15873
IHZhcnlpbmc= 15874
IE1vdmll 15875
IGxvcmQ= 15876
b2Fr 15877
RmllbGQ= 15878
IHZlY3Rvcg== 15879
dXNpb25z 15880
IHNjcmFw 15881
IGVuYWJsaW5n 15882
bWFrZQ== 15883
VG9y 15884
Lio= 15885
fHw= 15886
IFdlYnNpdGU= 15887
IE5QQw== 15888
IHNvY2lhbGlzdA== 15889
IEJpbGx5 15890
IEFkZGl0aW9uYWw= 15891
IGNhcmdv 15892
IGZhcm1z 15893
IFNvb24= 15894
IFByaXpl 15895
IG1pZG5pZ2h0 15896
IDkwMA== 15897
c2Vlbg== 15898
IFNwb3Q= 15899
IHNoZWVw 15900
IHNwb25zb3JlZA== 15901
IEhp 15902
IEp1bXA= 15903
IDE5Njc= 15904
TWljcm9zb2Z0 15905
IEFnZW50 15906
IGNoYXJ0cw== 15907
ZGly 15908
IGFkamFjZW50 15909
IHRyaWNrcw== 15910
IG1hbmdh 15911
IGV4YWdnZXI= 15912
Lz4= 15913
Zm9vdGJhbGw= 15914
IEZDQw== 15915
R0M= 15916
IFRpZXI= 15917
YW5kcmE= 15918
T1VORA== 15919
JSks 15920
IGZydWl0cw== 15921
VkM= 15922
IEFB 15923
Um9iZXI= 15924
IG1pZHN0 15925
4pc= 15926
YW5rYQ== 15927
IGxlZ2lzbGF0dXJl 15928
IE5laWw= 15929
IHRvdXJpc3Rz 15930
IiI= 15931
IFdhcm5pbmc= 15932
IE5ldmVydGhlbGVzcw== 15933
IE9mZmljaWFs 15934
IFdoYXRldmVy 15935
IG1vbGQ= 15936
IGRyYWZ0ZWQ= 15937
IHN1YnN0YW5jZXM= 15938
IGJyZWVk 15939
IHRhZ3M= 15940
IFRhc2s= 15941
IHZlcmI= 15942
IG1hbnVmYWN0dXJlZA== 15943
Y29tbWVudHM= 15944
IFBvbGlzaA== 15945
UHJvdg== 15946
IGRldGVybWluZXM= 15947
T2JhbWE= 15948
a2Vycw== 15949
IHV0dGVybHk= 15950
IHNlY3Q= 15951
c2NoZQ== 15952
IEdhdGVz 15953
IENoYXA= 15954
IGFsdW1pbnVt 15955
IHpvbWJpZQ== 15956
IFRvdWNo 15957
IFVQ 15958
IHNhdGlzZnk= 15959
IHByZWRvbWlu 15960
YXNjcmlwdA== 15961
IGVsYWJvcmF0ZQ== 15962
IDE5Njg= 15963
IG1lYXN1cmluZw== 15964
IFZhcmk= 15965
YW55YWh1 15966
IHNpcg== 15967
dWxhdGVz 15968
aWRnZXM= 15969
aWNrZXRz 15970
IFNwZW5jZXI= 15971
VE0= 15972
b3VidGVk 15973
IHByZXk= 15974
IGluc3RhbGxpbmc= 15975
IENhYg== 15976
cmVlZA== 15977
cmVhdGVk 15978
U3VwcA== 15979
IHdyaXN0 15980
IEtlcnJ5 15981
MTA3 15982
IEtsZQ== 15983
IFJhY2hlbA== 15984
IGNvdHRvbg== 15985
IEFSRQ== 15986
IEVsZQ== 15987
Q29udHJvbA== 15988
IGxvYWRz 15989
IERvZA== 15990
YW5hcw== 15991
Ym9uZQ== 15992
IGNsYXNzaWNhbA== 15993
IFJlZ2lvbmFs 15994
IEludGVn 15995
Vk0= 15996
IGRlc2lyZXM= 15997
IGF1dGlzbQ== 15998
c3VwcG9ydGVk 15999
IE1lc3NhZ2U= 16000
IGNvbXBhY3Q= 16001
d3JpdGVy 16002
IDEwOQ== 16003
IEh1cnJpY2FuZQ== 16004
Y2lzaW9u 16005
IGN5Y2xlcw== 16006
IGRyaWxs 16007
IGNvbGxlYWd1ZQ== 16008
IG1ha2Vy 16009
R2VybWFu 16010
IG1pc3Rha2Vu 16011
U3Vu 16012
IEdheQ== 16013
IHdoYXRzb2V2ZXI= 16014
IHNlbGxz 16015
IEFpcmw= 16016
bGl2 16017
IE9wdGlvbg== 16018
IHNvbHZlZA== 16019
IHNlY3RvcnM= 16020
IGhvcml6b250YWw= 16021
IGVxdWF0aW9u 16022
IFNraWxs 16023
IEJpbw== 16024
Z2VtZW50 16025
IFNuYXA= 16026
IExlZ2Fs 16027
IHRyYWRlbWFyaw== 16028
IG1ha2V1cA== 16029
IGFzc2VtYmxlZA== 16030
IHNhdmVz 16031
IEhhbGxvd2Vlbg== 16032
IFZlcm1vbnQ= 16033
IEZST00= 16034
IGZhcm1pbmc= 16035
IFBvZGNhc3Q= 16036
YWNjZXB0YWJsZQ== 16037
IEhpZ2hlcg== 16038
IGFzbGVlcA== 16039
dWxsaXZhbg== 16040
IHJlZmVyZW4= 16041
IExldg== 16042
IGJ1bGxldHM= 16043
b2tv 16044
SEM= 16045
IHN0YWlycw== 16046
IG1haW50YWlucw== 16047
IExvd2Vy 16048
IFZp 16049
IG1hcmluZQ== 16050
IGFjcmVz 16051
IGNvb3JkaW5hdG9y 16052
IEpvaA== 16053
IGNvdW50ZXJwYXJ0cw== 16054
IEJyb3RoZXJz 16055
IGluZGljdA== 16056
YnJh 16057
IGNodW5r 16058
IGNlbnRz 16059
SG9tZQ== 16060
IE1vbnRo 16061
IGFjY29yZGluZ2x5 16062
aWZsZXM= 16063
IEdlcm1hbnM= 16064
IFN5bg== 16065
SHVi 16066
IGV5ZWI= 16067
4pSA4pSA4pSA4pSA 16068
IHJhbmdlcw== 16069
IEhvbGxhbmQ= 16070
IFJvYm90 16071
ZmM= 16072
TWlrZQ== 16073
IHBsYXNtYQ== 16074
IHN3YXA= 16075
IGF0aGxldGU= 16076
IFJhbXM= 16077
LCci 16078
IGluZmVjdGlvbnM= 16079
IGNvcnJpZA== 16080
IHZpYg== 16081
IHBhdGNoZXM= 16082
IHRyYWRpdGlvbmFsbHk= 16083
IHJldmVsYXRpb24= 16084
IHN3ZWVw 16085
IGdsYW5jZQ== 16086
IGluZXg= 16087
MjAwMw== 16088
IFJhdw== 16089
d29ya2luZw== 16090
b3N1cmVz 16091
IERhdA== 16092
IEx5bmNo 16093
IGxldmVyYWdl 16094
IFJlaWQ= 16095
IGNvcnJlbGF0aW9u 16096
aWFuY2Vz 16097
YXZhc2NyaXB0 16098
IHJlcG9zaXRvcnk= 16099
cmV0dHk= 16100
IDE5NzI= 16101
MjQw 16102
IG91bg== 16103
cG9s 16104
IFJlZWQ= 16105
IHRhY3RpY2Fs 16106
aXNpdGU= 16107
QXBwbGU= 16108
IFF1aW5u 16109
IHJhcGVk 16110
aWxsbw== 16111
RXVyb3Bl 16112
IGFsZ29yaXRobXM= 16113
IFJvZHJpZw== 16114
aXU= 16115
IGlsbHVt 16116
IGZhbWU= 16117
IGludHJvZHVjaW5n 16118
IGRlbGF5cw== 16119
IFJhaWRlcnM= 16120
IHdoaXN0bGU= 16121
IG5vdmVscw== 16122
IFJlYWxseQ== 16123
IGRlcml2 16124
IHB1YmxpY2F0aW9ucw== 16125
IE5laXRoZXI= 16126
IENvbW1lcmNl 16127
IGFzdG9u 16128
bGFuZ3VhZ2U= 16129
Tm90ZXM= 16130
IFJvdGg= 16131
IEZlYXI= 16132
IG1hdGU= 16133
IHBhcmFkZQ== 16134
IFFC 16135
IG1hbmV1 16136
IENpbmNpbm5hdGk= 16137
bWl0dGluZw== 16138
IHdhaXN0 16139
IFJldw== 16140
IGRpc2NvbnQ= 16141
0LA= 16142
IHN0YXJpbmc= 16143
IGFsaWFz 16144
IHNlY3VyaXRpZXM= 16145
IHRvaWxldA== 16146
IEplZGk= 16147
IHVubGF3 16148
dmlzZWQ= 16149
Ly8vLy8vLy8= 16150
XSg= 16151
IFdlaXNz 16152
IHByZXN0 16153
IENvbXBhbg== 16154
IG1lbW8= 16155
IEdyYWNl 16156
SnVseQ== 16157
IEVsaXRl 16158
Y2VudGVy 16159
IFN0YXk= 16160
IGdhbGF4eQ== 16161
IHRvb3Ro 16162
IFNldHRpbmdz 16163
IHN1YmplY3RlZA== 16164
44Km 16165
IGxpbmViYWNr 16166
IHJldGFpbGVycw== 16167
IFdhbnQ= 16168
IGRhbmdlcnM= 16169
QWly 16170
IHZvbHVudGFyeQ== 16171
ZXdheQ== 16172
IGludGVycHJldGVk 16173
b3RpbmU= 16174
w6c= 16175
IHBlbA== 16176
U2VydmljZQ== 16177
IEV2ZW50dWFsbHk= 16178
IGNhcmVlcnM= 16179
IHRocmVhdGVu 16180
IG1lbW9y 16181
IEJyYWRsZXk= 16182
YW5jaWVz 16183
c24= 16184
IFVua25vd24= 16185
TmF0aW9uYWw= 16186
IHNoYWRvd3M= 16187
YWlsYW5k 16188
IERhc2g= 16189
RXZlcnlvbmU= 16190
aXp6YXJk 16191
TWFyY2g= 16192
PSg= 16193
IHB1bGxz 16194
IHN0cmFuZ2Vy 16195
IGJhY2t3YXJkcw== 16196
IEJlcm5hcmQ= 16197
aW1lbnNpb25hbA== 16198
IGNocm9u 16199
IHRoZW9yZXRpY2Fs 16200
a3RvcA== 16201
IHdhcmU= 16202
IEludmVzdGln 16203
IEluaXRp 16204
IE9wZXJhdGlvbnM= 16205
b3Zlbg== 16206
b2NpZGU= 16207
Ki8= 16208
IGZsYW1lcw== 16209
IENhc2g= 16210
c2hpdA== 16211
IGNhYg== 16212
IEFuYWx5 16213
IFNlYWg= 16214
IGRlZmluaW5n 16215
IG9yZGVyaW5n 16216
IGltbXVu 16217
IHBlcnNpc3RlbnQ= 16218
QUNI 16219
UnVzc2lhbg== 16220
bWFucw== 16221
IGhpbmQ= 16222
IHBob3RvZ3JhcGh5 16223
wqk= 16224
IGh1Zw== 16225
IDEwNw== 16226
IEhlbmNl 16227
aW90cw== 16228
dWRlYXU= 16229
IHN1YnNpZGllcw== 16230
IHJvdXRpbmVseQ== 16231
IERldmljZQ== 16232
aXRpYw== 16233
IGRpc2d1c3Q= 16234
bGFuZGVy 16235
IDE5NDA= 16236
IGFzc2lnbm1lbnQ= 16237
IEJlc2lkZXM= 16238
d2ljaw== 16239
IER1c3Q= 16240
dXNj 16241
c3RydWN0ZWQ= 16242
MTEx 16243
ZGV2ZWxvcA== 16244
IGZvbmQ= 16245
IGludGVyc2VjdGlvbg== 16246
IGRpZ25pdHk= 16247
IGNvbW1pc3Npb25lcg== 16248
V2l0aG91dA== 16249
cmVhY2g= 16250
IGNhcnRvb24= 16251
IHNjYWxlcw== 16252
44Ot 16253
RklH 16254
IHN1cnZleXM= 16255
IEluZG9uZXNpYQ== 16256
IGFydHdvcms= 16257
IHVuY2g= 16258
IGN5Y2xpbmc= 16259
dW5jdA== 16260
YXVlcg== 16261
b3JhdGU= 16262
IE9idmlvdXNseQ== 16263
IGNoYXJhY3Rlcml6ZWQ= 16264
ZmVsZA== 16265
IGFmZmlybQ== 16266
IGlubmluZ3M= 16267
IOk= 16268
IGFsaWVucw== 16269
IGNsb3Ro 16270
ZXRvb3Ro 16271
IENlcnRhaW4= 16272
wqc= 16273
IGRpZ2VzdA== 16274
a25vdw== 16275
IFhM 16276
IHByZWRpY3Rpb25z 16277
IGRpbg== 16278
V0FS 16279
IGFmdGVybWF0aA== 16280
RXhhbXBsZQ== 16281
IFN1Y2Nlc3M= 16282
IFRocg== 16283
SUdO 16284
IG1pbmVy 16285
QnVz 16286
IGNsYXJpdHk= 16287
aGVpbWVy 16288
IE9VVA== 16289
IFNlbmQ= 16290
IENpcmNsZQ== 16291
IERpZXQ= 16292
IHByb25vdW5jZWQ= 16293
IGNyZWF0b3Jz 16294
IGVhcnRocXVha2U= 16295
YXR0ZXJ5 16296
Z2VvbnM= 16297
IG9k 16298
IGxheWluZw== 16299
b3Jw 16300
VWx0 16301
cHJvamVjdA== 16302
IHVuZGVybWlu 16303
IHNlcXVlbA== 16304
U2Ft 16305
IERhcmtuZXNz 16306
IHJlY2VwdGlvbg== 16307
YnVsbA== 16308
WVM= 16309
IFZpcg== 16310
IHNlcXVlbmNlcw== 16311
IENvaW4= 16312
IG91dGZpdA== 16313
IFdhaXQ= 16314
MTE5 16315
IGRlbGl2ZXJz 16316
Li4uLi4u 16317
IGJsb3du 16318
IEVzYw== 16319
IE1hdGg= 16320
cGVybQ== 16321
IFVs 16322
IGdsaW0= 16323
IGZhY2lhbA== 16324
IGdyZWVuaG91c2U= 16325
IHRva2Vucw== 16326
Ly0= 16327
IEFubnVhbA== 16328
IE9ORQ== 16329
IHRlZW5hZ2U= 16330
IFBoeXNpY2Fs 16331
IExhbmc= 16332
IENlbHQ= 16333
IHN1ZWQ= 16334
aXZpZHVhbGx5 16335
IHBhdGllbmNl 16336
Y2hhaXI= 16337
cmVndWxhcg== 16338
IGF1Zw== 16339
aW52 16340
ZXhjZXB0 16341
IExpbA== 16342
IG5lc3Q= 16343
ZmQ= 16344
c3Vt 16345
IENoYXNl 16346
UnVzc2lh 16347
IEplbm5pZmVy 16348
IG9mZnNlYXNvbg== 16349
T3ZlcmFsbA== 16350
Rm9yZQ== 16351
IHJpb3Q= 16352
QXVk 16353
Zm9ybWVy 16354
IGRlZmVuZGVycw== 16355
IENU 16356
aW90aWM= 16357
cmlibHk= 16358
IGF1dG9tYXRlZA== 16359
IHBlbmlz 16360
IGluc2lzdA== 16361
IGRpYWdyYW0= 16362
IFNRTA== 16363
IEdhcmM= 16364
IHdpdGNo 16365
Y2xpZW50 16366
aWVycmE= 16367
YW1iZXJz 16368
IHJlY291bnQ= 16369
ZmFy 16370
VmVyeQ== 16371
b3N0ZXJvbmU= 16372
IGFwcHJlY2lhdGVk 16373
IFBlcmZlY3Q= 16374
U2VjdGlvbg== 16375
IGRvc2Vz 16376
b2NhdXN0 16377
IGNvc3RseQ== 16378
IGdyYW1z 16379
IFNoaQ== 16380
IHdyZXN0bGluZw== 16381
IDE5NzE= 16382
IHRyb3BoeQ== 16383
IG5lcnZl 16384
IEtheg== 16385
IEV4cGVyaWVuY2U= 16386
IHBsZWRnZWQ= 16387
IHBsYXliYWNr 16388
IGNyZWF0aXZpdHk= 16389
Ynll 16390
IGF0dGFja2Vycw== 16391
IGhvbGRlcnM= 16392
IENvYWNo 16393
IFBoRA== 16394
IHRyYW5zZmVycw== 16395
IGNvbG9yZWQ= 16396
IEhpbmR1 16397
IGRyb3du 16398
IGxpc3RlbmVk 16399
IFdB 16400
aWFzbQ== 16401
UE8= 16402
IGFwcGVhbGluZw== 16403
IGRpc2Nsb3NlZA== 16404
IENoaWNrZW4= 16405
YWdnaW5n 16406
IHBsZWFkZWQ= 16407
IG5hdmlnYXRpb24= 16408
IFJldHVybnM= 16409
IFtb 16410
Uk9S 16411
RUE= 16412
IHBob3RvZ3JhcGhlcg== 16413
IFJpZGVy 16414
aXBwZXJz 16415
IHNsaWNl 16416
IGVyZWN0 16417
IGhlZA== 16418
aXNzYW5jZQ== 16419
IFZpa2luZ3M= 16420
dXJpb3Vz 16421
IGFwcGV0 16422
b3VidGVkbHk= 16423
Q2hpbGQ= 16424
IGF1dGhlbnRpYw== 16425
b29z 16426
IE1ha2luZw== 16427
IGFubm91bmNpbmc= 16428
IGJvZA== 16429
IG1ldGVy 16430
IE5pbmU= 16431
IFJvZ3Vl 16432
IHdvcmtmb3JjZQ== 16433
IHJlbmV3ZWQ= 16434
IG9yZ2FuaXNhdGlvbnM= 16435
YWNz 16436
UExF 16437
U2hvcnQ= 16438
IGNvbXBvdW5kcw== 16439
IFZpc2l0 16440
IGVudmVsb3A= 16441
ZWFydGg= 16442
IHN1cHBvcnRpdmU= 16443
Z2dsZQ== 16444
IEJydXNzZWxz 16445
IEd1aWxk 16446
Q3JlYXRl 16447
UkVM 16448
IGF2ZXJhZ2Vk 16449
IDE5Njk= 16450
cmlhZ2Vz 16451
IGxlbmd0aHk= 16452
IGZvcmdvdA== 16453
T2theQ== 16454
IEVyZA== 16455
IGRlYWxlcg== 16456
IHJlY2Vzc2lvbg== 16457
REQ= 16458
IGRlc3BlcmF0ZWx5 16459
IGh1bmdlcg== 16460
IHN0aWNrcw== 16461
IG1waA== 16462
IEZhaXRo 16463
IGludGVudGlvbmFsbHk= 16464
IGRlbW9s 16465
dWVsbGVy 16466
IFNhbGU= 16467
IGRlYnJpcw== 16468
c3ByaW5n 16469
IGxlYXA= 16470
Pj4+Pg== 16471
IGNvbnRhaW5lcnM= 16472
c2VsbGluZw== 16473
cmFuZWFu 16474
YXR0ZXJpbmc= 16475
IGNvbW1lbnRlZA== 16476
IENN 16477
b251dA== 16478
IHdvb2Rz 16479
ZXNwZWNpYWxseQ== 16480
IG9yZ2FuaXpl 16481
aXZpYw== 16482
IFdvb2Rz 16483
YW5nYQ== 16484
c3F1 16485
IG1hag== 16486
YW1vbg== 16487
IGF4aXM= 16488
IDE5NzQ= 16489
IERlbm1hcms= 16490
IHdhcnJpb3I= 16491
IFBhbmQ= 16492
IG91dGxpbmVk 16493
IEJP 16494
aW5zdWxh 16495
emlsbGE= 16496
ZWJvb2s= 16497
IGRhcmU= 16498
IHNlYXJjaGVk 16499
IG5hdmlnYXRl 16500
U24= 16501
d3JpdGluZw== 16502
IHVuaXRlZA== 16503
SmFwYW4= 16504
IEhlYnJldw== 16505
IGZsYW1l 16506
IHJlbGllcw== 16507
IGNhdGNoaW5n 16508
IFNobw== 16509
IGltcHJpc29ubWVudA== 16510
IHBvY2tldHM= 16511
IGNsb3N1cmU= 16512
IEZhbQ== 16513
dGlt 16514
YWRlcXU= 16515
QWN0aXZpdHk= 16516
IHJlY3J1aXRpbmc= 16517
IFdBVENI 16518
IEFyZ2VudGluYQ== 16519
ZGVzdA== 16520
IGFwb2xvZ2l6ZQ== 16521
b3Jv 16522
IGxhY2tz 16523
IHR1bmVk 16524
IEdyaWZmaW4= 16525
IGluZmFtb3Vz 16526
IGNlbGVicml0eQ== 16527
c3Nvbg== 16528
IC0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0= 16529
IElzaXM= 16530
IERpc3BsYXk= 16531
IGNyZWRpYmlsaXR5 16532
IGVjb25vbWllcw== 16533
IGhlYWRsaW5l 16534
IENvd2JveXM= 16535
IGluZGVm 16536
IGxhdGVseQ== 16537
IGluY2VudGl2ZXM= 16538
YnV0dG9u 16539
IE1vYg== 16540
QXV0 16541
IHJlc2lnbmVk 16542
IE9t 16543
Y2FtcA== 16544
IHByb2ZpbGVz 16545
IHNjaGVtZXM= 16546
b2xwaGlucw== 16547
YXllZA== 16548
Q2xpbnRvbg== 16549
ZW5o 16550
IFlhaG9v 16551
IGFic3Q= 16552
IGFuaw== 16553
c3VpdHM= 16554
IHdpc2hlZA== 16555
IE1hcmNv 16556
dWRkZW4= 16557
IHNwaGVyZQ== 16558
IEJpc2hvcA== 16559
IGluY29ycG9yYXRlZA== 16560
IFBsYW50 16561
MTE0 16562
IGhhdGVk 16563
cGlj 16564
IGRvbmF0ZQ== 16565
IGxpbmVk 16566
IGJlYW5z 16567
IHN0ZWFsaW5n 16568
IGNvc3R1bWU= 16569
IHNoZXJpZmY= 16570
IGZvcnR5 16571
IGludGFjdA== 16572
IGFkYXB0ZWQ= 16573
IHRyYXZlbGxpbmc= 16574
YmFydA== 16575
IG5pY2VseQ== 16576
IGRyaWVk 16577
IHNjYWw= 16578
b3NpdHk= 16579
Tk9URQ== 16580
IEJo 16581
IEJyb25jb3M= 16582
IElnbg== 16583
IGludGltYXRl 16584
IGNoZW1pc3RyeQ== 16585
IG9wdGltYWw= 16586
RGVi 16587
IEdlbmVyYXRpb24= 16588
IF0s 16589
aWNoaQ== 16590
IFdpaQ== 16591
IFlPVVI= 16592
dmVudGlvbnM= 16593
V3JpdGU= 16594
IHBvcHVs 16595
dW5uaW5n 16596
IFdvcg== 16597
Vm9s 16598
IHF1ZWVu 16599
aGVhZHM= 16600
S0s= 16601
IGFuYWx5emU= 16602
b3BpYw== 16603
ZWFyY2hlcnM= 16604
IGRvdA== 16605
bGVncmFwaA== 16606
YXN0aWNhbGx5 16607
IHVwZ3JhZGVz 16608
IGNhcmVz 16609
IGV4dGVuZGluZw== 16610
IGZyZWV6ZQ== 16611
IGluYWJpbGl0eQ== 16612
IG9yZ2Fucw== 16613
IHByZXRlbmQ= 16614
IG91dGxldA== 16615
MTEz 16616
b2xhbg== 16617
IE1hbGw= 16618
dWxpbmc= 16619
dGFsaw== 16620
IGV4cHJlc3Npbmc= 16621
IEFsd2F5cw== 16622
IEJlZ2lu 16623
ZmlsZXM= 16624
IGxpY2Vuc2Vz 16625
JSU= 16626
IE1pdHQ= 16627
IGZpbHRlcnM= 16628
IE1pbHdhdWtlZQ== 16629
R04= 16630
IHVuZm9sZA== 16631
TW8= 16632
IG51dHJpdGlvbg== 16633
cHBv 16634
Qm8= 16635
IGZvdW5kaW5n 16636
IHVuZGVybWluZQ== 16637
IGVhc2llc3Q= 16638
IEN6ZWNo 16639
IE1hY2s= 16640
IHNleHVhbGl0eQ== 16641
IE5peG9u 16642
V2lu 16643
IEFybg== 16644
IEtpbg== 16645
44Kj 16646
aWNlcg== 16647
IGZvcnR1bg== 16648
IHN1cmZhY2Vz 16649
YWdoZA== 16650
IGNhcnJpZXJz 16651
IFBBUlQ= 16652
IFRpYg== 16653
IGludGVydmFs 16654
IGZydXN0cmF0aW5n 16655
IFNoaXA= 16656
IEFybWVk 16657
ZmZl 16658
IGJvYXRz 16659
IEFicmFoYW0= 16660
aW5pcw== 16661
IHN1aXRlZA== 16662
dGhyZWFk 16663
aW92 16664
YWJ1bA== 16665
IFZlbmV6dWVsYQ== 16666
IHRvbQ== 16667
c3VwZXI= 16668
IGNhc3RsZQ== 16669
YWx0aG91Z2g= 16670
aW94aWRl 16671
ZWNoZXM= 16672
IGV2b2x1dGlvbmFyeQ== 16673
IG5lZ290aWF0ZQ== 16674
IGNvbmZyb250ZWQ= 16675
UmVtZW1iZXI= 16676
IDE3MA== 16677
U3VjaA== 16678
IDkxMQ== 16679
bXVsdA== 16680
IEFieXNz 16681
dXJyeQ== 16682
a2Vlcw== 16683
c3BlYw== 16684
IEJhcmJhcmE= 16685
IGJlbG9uZ2luZw== 16686
IHZpbGxhaW4= 16687
aXN0YW5p 16688
IGFjY291bnRhYmxl 16689
IHBvcnRpb25z 16690
IERlY2w= 16691
VXI= 16692
IEthdGU= 16693
Z3Jl 16694
IG1hZ2F6aW5lcw== 16695
VUNL 16696
IHJlZ3VsYXRl 16697
b21vbg== 16698
IEFsbW9zdA== 16699
IG92ZXJ2aWV3 16700
IHNjcmFt 16701
IGxvb3Q= 16702
IEZpdHo= 16703
IGNoYXJhY3RlcmlzdGlj 16704
IFNuYWtl 16705
c2F5 16706
IFJpY28= 16707
IHRyYWl0 16708
IEpvaW5lZA== 16709
YXVjdXM= 16710
IGFkYXB0YXRpb24= 16711
IEFpcmxpbmVz 16712
IGFyY2hhZQ== 16713
IElkZQ== 16714
IGJpa2Vz 16715
IGxpdGVyYXJ5 16716
IGluZmx1ZW5jZXM= 16717
IFVzZWQ= 16718
Q3JlYXQ= 16719
IHBsZWE= 16720
IERlZmVuY2U= 16721
IEFzc2Fzcw== 16722
IHBvbmQ= 16723
VUxU 16724
KSI= 16725
IGV2YWx1YXRlZA== 16726
IG9idGFpbmluZw== 16727
IGRlbW9ncmFwaGlj 16728
IHZpZ2ls 16729
YWxleQ== 16730
IHNwb3VzZQ== 16731
IFNlYWhhd2tz 16732
cmVzcG9ucw== 16733
IEJlbHQ= 16734
dW1hdGlj 16735
IHJpc2Vz 16736
cnVubmVy 16737
IE1pY2hlbGxl 16738
IHBvdGVudA== 16739
cmFjZQ== 16740
IFBBQw== 16741
RmluZA== 16742
b2xlc3Rlcm9s 16743
SVNT 16744
IEludHJvZHVjZWQ= 16745
cmVzc2Vz 16746
aWdubWVudA== 16747
T3M= 16748
IFR1 16749
IERleA== 16750
aWNpZGVz 16751
IHNwYXJrZWQ= 16752
IExhdXJh 16753
IEJyeWFudA== 16754
IHNtaWxpbmc= 16755
IE5leHVz 16756
IGRlZmVuZGFudHM= 16757
IENhdGFs 16758
IGRpc2hlcw== 16759
c2hhcGVk 16760
IHByb2xvbmc= 16761
bXQ= 16762
KCQ= 16763
44CC 16764
IGNhbGN1bGF0aW9ucw== 16765
IFNhbWU= 16766
IHBpdg== 16767
SEg= 16768
IGNhbmNlbGxlZA== 16769
IGdyaW4= 16770
IHRlcnJpdG9yaWVz 16771
aXN0aWNhbGx5 16772
Q29tZQ== 16773
IFBhcmVudA== 16774
UHJvamVjdA== 16775
IG5lZ2xpZw== 16776
IFByaXZhY3k= 16777
IGFtbW8= 16778
TEVDVA== 16779
b2x1dGVseQ== 16780
IEVwaWM= 16781
IG1pc3VuZGVy 16782
d2Fs 16783
QXByaWw= 16784
bW9z 16785
cGF0aHk= 16786
IENhcnNvbg== 16787
IGFsYnVtcw== 16788
IEVhc3k= 16789
IHBpc3RvbA== 16790
PDw= 16791
IFwo 16792
dGFyZ2V0 16793
aGVscA== 16794
IGludGVycHJl 16795
Y29uc2Npb3Vz 16796
IEhvdXNpbmc= 16797
IEpvaW50 16798
MTI3 16799
IGJlZXJz 16800
c2NpZW5jZQ== 16801
IEZpcmVmb3g= 16802
ZWZmZWN0aXZl 16803
IENhYmlu 16804
IE9rYXk= 16805
IEFwcGxpYw== 16806
IHNwYWNlY3JhZnQ= 16807
IFNS 16808
dmV0 16809
IFN0cmFuZ2U= 16810
U0I= 16811
IGNvcnBz 16812
aWJlcmFs 16813
ZWZmaWNpZW50 16814
IHByZXZhbGVuY2U= 16815
IGVjb25vbWlzdHM= 16816
MTE4 16817
VGhyZWFk 16818
b3JkYWJsZQ== 16819
T0RF 16820
IENhbnQ= 16821
PS09LQ== 16822
aWZpYWJsZQ== 16823
IEFyb3VuZA== 16824
IHBvbGU= 16825
IHdpbGxpbmduZXNz 16826
Q0xB 16827
IEtpZA== 16828
IGNvbXBsZW1lbnQ= 16829
IHNjYXR0ZXJlZA== 16830
IGlubWF0ZXM= 16831
IGJsZWVkaW5n 16832
ZXZlcnk= 16833
IHF1ZXVl 16834
IFRyYWlu 16835
IGhpag== 16836
IG1lbGVl 16837
cGxldGVk 16838
IGRpZ2l0 16839
IGdlbQ== 16840
b2ZmaWNpYWw= 16841
IGxpZnRpbmc= 16842
0LU= 16843
UmVxdQ== 16844
aXR1dGVz 16845
IHBhY2thZ2luZw== 16846
IFdvcmtlcnM= 16847
aHJhbg== 16848
IExlYmFub24= 16849
b2xlc2M= 16850
IHB1bmlzaGVk 16851
IEp1YW4= 16852
IGphbQ== 16853
IERvY3VtZW50 16854
IG1hcHBpbmc= 16855
aWNhdGVz 16856
IGluZXZpdGFibHk= 16857
IHZhbmlsbGE= 16858
IFRvbg== 16859
IHdhdGNoZXM= 16860
IGxlYWd1ZXM= 16861
IGluaXRpYXRlZA== 16862
ZGVncmVl 16863
cG9ydGlvbg== 16864
IHJlY2FsbHM= 16865
IHJ1aW4= 16866
IG1lbHQ= 16867
SUFO 16868
IGhlbQ== 16869
RXhw 16870
IGJha2luZw== 16871
IENvbG9tYg== 16872
YXRpYmxl 16873
IHJhZGl1cw== 16874
cGx1Zw== 16875
IElG 16876
ZXRpY2FsbHk= 16877
IGZpY3Q= 16878
SEVS 16879
IFRhcA== 16880
YXRpbnVt 16881
IGluaw== 16882
IGNvaA== 16883
IFdpemFyZA== 16884
Ym90aA== 16885
dGV4 16886
IHNwZW5kcw== 16887
IEN1cnJlbnRseQ== 16888
IFBpdA== 16889
IG5ldXJvbnM= 16890
aWdudA== 16891
IHJhbGw= 16892
IGJ1c2Vz 16893
YnVpbGRpbmc= 16894
IGFkanVzdG1lbnRz 16895
IGNyaWVk 16896
aWJsaWNhbA== 16897
YXR0ZWQ= 16898
IFppb24= 16899
IE1hdHRlcg== 16900
IG1lZGl0YXRpb24= 16901
IERlbm5pcw== 16902
IG91cnM= 16903
IFRhYg== 16904
IHJhbmtpbmdz 16905
b3J0YWw= 16906
IGFkdmVycw== 16907
IHN1cnJlbmRlcg== 16908
IEdvYg== 16909
Y2l1bQ== 16910
b21hcw== 16911
aW1ldGVy 16912
IG11bHRpcGxheWVy 16913
IGhlcm9pbg== 16914
IG9wdGltaXN0aWM= 16915
IGluZGljYXRvcg== 16916
IEJyaWc= 16917
IGdyb2Nlcnk= 16918
IGFwcGxpY2FudA== 16919
IFJvY2tldA== 16920
dmlk 16921
RXhjZXB0aW9u 16922
cGVudA== 16923
IG9yZ2FuaXppbmc= 16924
IGVuY291bnRlcnM= 16925
IFRPRA== 16926
IGpld2Vs 16927
U2F2ZQ== 16928
IENocmlzdGll 16929
IGhlYXRpbmc= 16930
IGxhenk= 16931
IENQ 16932
IGNvdXNpbg== 16933
Q29uZmln 16934
IHJlZ2VuZXI= 16935
IG5lYXJlc3Q= 16936
IGFjaGlldmluZw== 16937
RU5T 16938
dGhyb3c= 16939
IFJpY2htb25k 16940
YW50bGU= 16941
MjAwMg== 16942
IGFudGVu 16943
YmlyZA== 16944
MTMz 16945
IG5hcmM= 16946
cmFpbnQ= 16947
dW5ueQ== 16948
IEhpc3Bhbmlj 16949
b3VybmFtZW50cw== 16950
IHByb3BoZQ== 16951
IFRoYWlsYW5k 16952
IFRp 16953
IGluamVjdGlvbg== 16954
IGluaGVyaXQ= 16955
cmF2aXM= 16956
IG1lZGk= 16957
IHdob2V2ZXI= 16958
IERFQlVH 16959
R1A= 16960
IEh1ZA== 16961
Q2FyZA== 16962
cHJvbQ== 16963
IHBvcg== 16964
IG92ZXJoZWFk 16965
TGF3 16966
IHZpb2xhdGU= 16967
IGhlYXRlZA== 16968
IGRlc2NyaXB0aW9ucw== 16969
IGFjaGlldmVtZW50cw== 16970
IEJlZXI= 16971
IFF1YW50 16972
V2Fz 16973
IGVpZ2h0aA== 16974
IEl2 16975
IHNwZWNpYWxpemVk 16976
VVBEQVRF 16977
IERlbHRh 16978
UG9w 16979
SnVs 16980
IEFzaw== 16981
b3BoeQ== 16982
IG5ld3NsZXR0ZXJz 16983
IFRvb2w= 16984
IGdhcmQ= 16985
IENvbmZlZGVy 16986
IEdNVA== 16987
IEFiYm90dA== 16988
IGltbXVuaXR5 16989
IFZN 16990
SXNsYW0= 16991
IGltcGxpY2l0 16992
d2Q= 16993
IDE5NDQ= 16994
cmF2aXR5 16995
b21ldHJpYw== 16996
IHN1cnZpdmluZw== 16997
dXJhaQ== 16998
IFByaXNvbg== 16999
IHJ1c3Q= 17000
IFNrZXRjaA== 17001
IGJlZXM= 17002
IFRoZW9yeQ== 17003
IG1lcml0 17004
VGV4 17005
Y2hhdA== 17006
IG1pbQ== 17007
IHBhc3Rl 17008
IEtvY2g= 17009
IGlnbm9yYW5jZQ== 17010
IFNob290 17011
IGJhc2VtZW50 17012
VW5pdGVk 17013
IEFkdmlz 17014
aGVpZ2h0 17015
IGZvc3Rlcg== 17016
IGRldGFpbg== 17017
aW5mb3JtYXRpb24= 17018
IG5ldXJhbA== 17019
Jzs= 17020
IHByb3Zlcw== 17021
YWxsZXJ5 17022
IGludml0YXRpb24= 17023
dW1iZXJz 17024
IGNhdHRsZQ== 17025
IGJpY3ljbGU= 17026
emk= 17027
IGNvbnN1bHRhbnQ= 17028
IGFwb2xvZ3k= 17029
IFRpZ2Vy 17030
IDEyMw== 17031
OTk5 17032
IGluZGl2aWR1YWxseQ== 17033
cnQ= 17034
aWdpb24= 17035
IEJyYXppbGlhbg== 17036
IGRpc3R1cmI= 17037
IGVudHJlcHJlbmV1cnM= 17038
IGZvcmVzdHM= 17039
Y2VycHQ= 17040
cGxhdGVz 17041
cGhlcg== 17042
Y2xpcHNl 17043
IHR3aXR0ZXI= 17044
IGFjaWRz 17045
b2dyYXBoaWNhbA== 17046
aHVt 17047
IEJhbGQ= 17048
aWZ1bGx5 17049
IGNvbXBpbGVy 17050
IERB 17051
IGRvbm9y 17052
YXNp 17053
IHRyaWJhbA== 17054
bGFzaA== 17055
IENvbmZpZw== 17056
IGFwcGxpY2FudHM= 17057
IHNhbGFyaWVz 17058
MTM1 17059
UHV0aW4= 17060
IEZvY3Vz 17061
aXJz 17062
IG1pc2NvbmR1Y3Q= 17063
IEhheg== 17064
IGVhdGVu 17065
TW9iaWxl 17066
TXVzbGlt 17067
IE1hcmN1cw== 17068
dmlvbA== 17069
IGZhdm9yYWJsZQ== 17070
IHN0dWI= 17071
YWRpbg== 17072
IEhvYg== 17073
IGZhaXRoZnVs 17074
IGVsZWN0cm9uaWNz 17075
IHZhY3V1bQ== 17076
d2FpdA== 17077
YmFja2Vk 17078
ZWNvbm9taWM= 17079
ZGlzdA== 17080
IHRlbnVyZQ== 17081
IHNpbmNlcmU= 17082
IFRvZ2V0aGVy 17083
IFdhdmU= 17084
IHByb2dyZXNzaW9u 17085
IGRlbnlpbmc= 17086
IGRpc3RyZXNz 17087
YnJhc2th 17088
dGhpcmQ= 17089
IG1peGluZw== 17090
IGNvbG9uaWFs 17091
IHByaXZhdGVseQ== 17092
IHVucmVzdA== 17093
YXRlcm5pdHk= 17094
IHByZW1pc2Vz 17095
YW50aQ== 17096
Z3JlZ2F0aW9u 17097
IGxpY2VuY2U= 17098
IEhpbmQ= 17099
IFNhbXVlbA== 17100
IGNvbnZpbmNpbmc= 17101
IEFjZQ== 17102
IFJ1c3Q= 17103
IE5ldGFueWFodQ== 17104
IGhhbmRsZXM= 17105
IFBhdGNo 17106
b3JpZW50ZWQ= 17107
YWhv 17108
IEdvbno= 17109
IGhhY2tlcnM= 17110
Y2xhaW1lcg== 17111
IGN1c3RvbXM= 17112
IEdyYW4= 17113
ZmlnaHRlcnM= 17114
IGx1Yw== 17115
IG1hbnVzY3JpcHQ= 17116
YXJlbnRob29k 17117
IGRldmls 17118
IHdhcnJpb3Jz 17119
IG9mZmVuZGVycw== 17120
V2lsbGlhbQ== 17121
IGhvbGlkYXlz 17122
IG5pZ2h0bWFyZQ== 17123
IGxldmVy 17124
aWZmZXJlbnQ= 17125
U3RhdA== 17126
IGV4aGliaXRpb24= 17127
cHV0ZWQ= 17128
IFB1cmU= 17129
IGFscGhh 17130
IGVudGh1c2lhc20= 17131
IFJlcHJlc2VudGF0aXZlcw== 17132
RUFS 17133
IFR5cA== 17134
IHdoZWF0 17135
IEFsZg== 17136
IGNvcnJlY3Rpb24= 17137
IGV2YW5nZWw= 17138
QVRU 17139
TWlzcw== 17140
IHNvdXA= 17141
IGltcGxpZWQ= 17142
cGFyYW0= 17143
IHNleHk= 17144
IEx1eA== 17145
IHJlcHVibGlj 17146
cGF0Y2g= 17147
YWJsaXNo 17148
IGljb25z 17149
IGZhdGhlcnM= 17150
IEdFVA== 17151
IENhcmli 17152
IHJlZ3VsYXRlZA== 17153
IENvaGVu 17154
IEJvYmJ5 17155
IG5lcg== 17156
IGJlbnQ= 17157
dmVudG9yeQ== 17158
IEFsb25n 17159
IEVTVA== 17160
IFdhbGxhY2U= 17161
IG11cmRlcnM= 17162
cmlzZQ== 17163
a2VsbA== 17164
IENvbW1vbndlYWx0aA== 17165
IG5hc3R5 17166
ZXRh 17167
IE1JVA== 17168
IGFkbWluaXN0ZXJlZA== 17169
IGdlbnVpbmVseQ== 17170
RWRpdG9y 17171
bmljaw== 17172
IGh5ZHJv 17173
KioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKio= 17174
IEJsZQ== 17175
IGZpbmVz 17176
IGdvcmdl 17177
YXVzaWJsZQ== 17178
cmg= 17179
IGFwcGxl 17180
bWVudGlvbmVk 17181
IHJvcGU= 17182
b3R5cA== 17183
SFI= 17184
IGRpc2FwcG9pbnRpbmc= 17185
IGNhZ2U= 17186
bmlr 17187
IGRvdWJ0cw== 17188
IEZSRUU= 17189
cHJpbnRz 17190
IE1VU1Q= 17191
IHZlbmRvcnM= 17192
IElucXU= 17193
IGxpYmVyYWxz 17194
IGNvbnRyYWN0b3I= 17195
IHVwc2lkZQ== 17196
Y2hpbGRyZW4= 17197
IHRyaWNreQ== 17198
IHJlZ3VsYXRvcnM= 17199
Y2hhcmdlZA== 17200
bGl0ZXI= 17201
ICoqKg== 17202
IHJlYmVsbA== 17203
bGFuZw== 17204
IGxvY2Fscw== 17205
IHBoeXNpY2lhbnM= 17206
IGhleQ== 17207
YXJzZQ== 17208
dG0= 17209
IExleA== 17210
IGJlaGF2aW9yYWw= 17211
c3VjY2Vzc2Z1bA== 17212
Rlg= 17213
IGJyaWNr 17214
b3ZpYw== 17215
IGNvbmZvcm0= 17216
IHJldmlld2luZw== 17217
IGluc2lnaHRz 17218
IGJpb2xvZ3k= 17219
IFJlbW92ZQ== 17220
IEV4dHJh 17221
IGNvbW1pdHRpbmc= 17222
aW5kdWNlZA== 17223
aWdudHk= 17224
aWdt 17225
IGF0b21pYw== 17226
Q29tbW9u 17227
IEVN 17228
IFBlcmU= 17229
IEl0ZW1z 17230
ZWg= 17231
IHByZXNlcnZlZA== 17232
IEhvb2Q= 17233
IHByaXNvbmVy 17234
IGJhbmtydXB0Y3k= 17235
IGdyZW4= 17236
dXNoZXM= 17237
IGV4cGxvaXRhdGlvbg== 17238
IHNpZ25hdHVyZXM= 17239
IGZpbmFu 17240
XSwi 17241
IE1S 17242
IG1lZw== 17243
cmVtbGlu 17244
IG11c2ljaWFucw== 17245
IHNlbGVjdGluZw== 17246
IGV4YW1pbmluZw== 17247
SU5L 17248
bGF0ZWQ= 17249
SGk= 17250
IGFydGlj 17251
IHBldHM= 17252
IGltcGFpcg== 17253
IE1BTg== 17254
IHRhYmxldHM= 17255
aW5jbHVkZQ== 17256
UmFuZ2U= 17257
IGNhdXQ= 17258
IGxvZ3M= 17259
IG1vdW50aW5n 17260
IHVuYXdhcmU= 17261
IGR5bmFtaWNz 17262
IFBhbGVzdGluZQ== 17263
IFF1YXJ0ZXI= 17264
IFB1cnBsZQ== 17265
IG1h 17266
IEltcG9ydA== 17267
IGNvbGxlY3Rpb25z 17268
Y2lhdGlvbg== 17269
IHN1Y2Nlc3Nvcg== 17270
IGNsb25l 17271
IGFpbWluZw== 17272
IHBvc3Nlc3NlZA== 17273
IHN0aWNraW5n 17274
IHNoYWtpbmc= 17275
IGxvY2F0ZQ== 17276
IEhvY2tleQ== 17277
VHVybg== 17278
MTcw 17279
IGZpZnRlZW4= 17280
IEhhcnJpc29u 17281
IGNvbnRpbnVvdXNseQ== 17282
IFRD 17283
IFZhbGVudA== 17284
IFJlc2N1ZQ== 17285
IGJ5cGFzcw== 17286
YW1vdW50 17287
IG1hc3Q= 17288
IHByb3RlY3Rz 17289
IGFydGlzdGlj 17290
IHNvbWV0aW1l 17291
IHNob2U= 17292
IHNob3V0ZWQ= 17293
aWZpY2FudA== 17294
ZXRpdGl2ZQ== 17295
IFJlZ2lzdGVy 17296
IEppbg== 17297
IGNvbmNlbnRyYXRlZA== 17298
bGluZ3Rvbg== 17299
b25pZXM= 17300
IGdlbmVyYXRvcg== 17301
eXJpbQ== 17302
IEFybWVu 17303
IGNsZWFyaW5n 17304
aWRv 17305
IFRX 17306
YWxwaA== 17307
IGxhZGllcw== 17308
SGFyZA== 17309
IGRpYWxvZw== 17310
IGlucHV0cw== 17311
5pw= 17312
IHBvc2Vz 17313
IHNsb3Rz 17314
IFByZW1pdW0= 17315
IGxlYWtz 17316
IGJvc3Nlcw== 17317
IDExMw== 17318
Y291cnNl 17319
QWNj 17320
IE5ld3Rvbg== 17321
IEF1c3RyaWE= 17322
IE1hZ2U= 17323
IHRlYWNoZXM= 17324
YWJhZA== 17325
IHdlYXJz 17326
IGN5bA== 17327
IGN1cnNl 17328
IFNhbGVz 17329
IFdpbmdz 17330
IHBzeQ== 17331
IGdhcHM= 17332
IEljZWxhbmQ= 17333
IFBpbnRlcmVzdA== 17334
IGxhbmRsb3Jk 17335
IGRlZmluaXRpb25z 17336
IEtlcg== 17337
IHN1ZmZpY2llbnRseQ== 17338
IFBlbmNl 17339
IEFyY2hpdGVjdA== 17340
IHN1cnBhc3M= 17341
IDExNA== 17342
IHN1cGVyaGVybw== 17343
IERpc2Vhc2U= 17344
IHByaWVzdHM= 17345
IEN1bHR1cmU= 17346
IGRlZmluaXRpdmU= 17347
IHNlY3JldGx5 17348
IERhbmNl 17349
aW5zdGFsbA== 17350
Y2hpZWY= 17351
IEplc3NpY2E= 17352
V291bGQ= 17353
VXBkYXRlZA== 17354
IGxvY2tlcg== 17355
IEtheQ== 17356
IG1lbW9yaWFs 17357
6KY= 17358
ZmF0 17359
IGRpc2d1 17360
IGZsYXZvcnM= 17361
IEJhc2ViYWxs 17362
IFJlc2lzdGFuY2U= 17363
IGtpY2tz 17364
IGVudg== 17365
IHRlZW5hZ2Vycw== 17366
RGFyaw== 17367
IENBUg== 17368
IGhhbHQ= 17369
IExH 17370
IEdhYnJpZWw= 17371
IGZldmVy 17372
IHNhdHVy 17373
IG1hbGw= 17374
IGFmZmlsaWF0ZQ== 17375
IFNsZWVw 17376
IFNwZWNpZmlj 17377
IFZlbA== 17378
IGphcg== 17379
IFNhY3JlZA== 17380
IEVkd2FyZHM= 17381
IEFDTA== 17382
IHJldGFpbmVk 17383
IEdpYW50 17384
IGxpbWl0YXRpb24= 17385
aW5jZXM= 17386
IHJlZnVzYWw= 17387
IFRhbGU= 17388
IEJ1dGxlcg== 17389
IGFjY2lkZW50cw== 17390
IENTUw== 17391
IGltcG9ydGVk 17392
IENvcHk= 17393
zrE= 17394
RVJU 17395
emVs 17396
IGRpdmlzaW9ucw== 17397
aG90cw== 17398
IEFsYg== 17399
IERT 17400
TG9hZGVy 17401
V2FzaGluZ3Rvbg== 17402
YXRpc2Y= 17403
IENyZWF0aXZl 17404
XC4= 17405
IEF1dG9t 17406
cmVkaWN0 17407
IHJlY2VwdG9y 17408
IENhcmxvcw== 17409
TWV0aG9k 17410
b2th 17411
IG1hbGljaW91cw== 17412
IHN0ZXBwaW5n 17413
LFs= 17414
IERhZA== 17415
IGF0dHJhY3Rpb24= 17416
IEVmZmVjdHM= 17417
IFBpcmF0ZQ== 17418
IENlcg== 17419
IEluZHVzdHJ5 17420
IFJ1ZA== 17421
IGNoYXJ0ZXI= 17422
IGRpbmluZw== 17423
IGluc2lzdHM= 17424
IGNvbmZpZ3VyZQ== 17425
ICgj 17426
IFNpbXBsZQ== 17427
IFNjcm9sbA== 17428
VVRD 17429
MTc1 17430
IEtvbg== 17431
IG1hcmtldHBsYWNl 17432
IOOC 17433
IHJlZnJlcw== 17434
IGdhdGVz 17435
ZXJyZWQ= 17436
IFBvZA== 17437
IGJlaGF2ZQ== 17438
RnJhbms= 17439
bm9kZQ== 17440
IGVuZG9yc2Vk 17441
aGV0dA== 17442
YXNpdmU= 17443
IEhvbWVsYW5k 17444
IHJpZGVz 17445
IExlYXZl 17446
ZXJuZXNz 17447
IGZsb29kaW5n 17448
QUZQ 17449
IHJpc2Vu 17450
IGNvbnRpbnVhbGx5 17451
IHVuYW5pbQ== 17452
IENvbnRyYWN0 17453
IFBhcw== 17454
IGd1aWRlZA== 17455
IENoaWxl 17456
YmQ= 17457
IHN1Y2M= 17458
cHRpYw== 17459
IGNvbW1pdHRlZXM= 17460
IEx1dGhlcg== 17461
IEFueW9uZQ== 17462
IHNhYg== 17463
MTI0 17464
IHBpeGVs 17465
IEJhaw== 17466
IFRhZw== 17467
IEJlbm5ldHQ= 17468
RW50ZXI= 17469
c21hbGw= 17470
IFByZXNpZGVudGlhbA== 17471
IHB1bA== 17472
IGNvbnRyYWNl 17473
YXJjaGl2ZQ== 17474
IGNvYXN0YWw= 17475
IEtpZHM= 17476
MTky 17477
4oCy 17478
aWNreQ== 17479
SU5HVE9O 17480
IHdvbGY= 17481
IFN0YWxpbg== 17482
VHVy 17483
aWRnZXQ= 17484
YW1hcw== 17485
IFVubGVzcw== 17486
IHNwb25zb3I= 17487
IG1vcnBo 17488
IENob29zZQ== 17489
IHJ1bm5lcg== 17490
IHVuYmVs 17491
IG11ZA== 17492
IE1hbmE= 17493
IGR1YmJlZA== 17494
IGdvZGQ= 17495
dXJlcnM= 17496
d2luZG93 17497
IHJlbGllZA== 17498
IGNlbGVicmF0aW5n 17499
b3Nj 17500
IDEzNQ== 17501
IGxvYmJ5aW5n 17502
IGluY29tcGxldGU= 17503
IHJlc3RyaWN0aW9u 17504
IGluY2Fw 17505
aXR1cw== 17506
IGV4cGVjdGF0aW9u 17507
IEFwb2xsbw== 17508
IGludGVucw== 17509
IHN5bmM= 17510
R0g= 17511
IG1hbmlwdWxhdGlvbg== 17512
Qlk= 17513
IHNwZWFy 17514
IGJyZWFzdHM= 17515
IHZvbGNhbg== 17516
aWxpYQ== 17517
TWF0ZXJpYWw= 17518
IGZvcm1hdHM= 17519
IEJhc3Q= 17520
IHBhcmxpYW1lbnRhcnk= 17521
IHNuYWtl 17522
IHNlcnZhbnRz 17523
IFRydWRlYXU= 17524
IEdyaW0= 17525
IEFyYWJpYw== 17526
IFNDUA== 17527
IEJveXM= 17528
c3RhdGlvbg== 17529
IHByb3NwZWN0aXZl 17530
b3JkZQ== 17531
aW5pdGlhbGl6ZWQ= 17532
IGJvcmVk 17533
QUJMRQ== 17534
IGFjY2Vzc2Vk 17535
IHRheGk= 17536
IFNoZWxs 17537
YWlkZW4= 17538
dXJzZWQ= 17539
aW5hdGVz 17540
IEluc3VyYW5jZQ== 17541
IFBldGU= 17542
U2VwdGVtYmVy 17543
NjUw 17544
IGFkdmVudHVyZXM= 17545
IENvdmVy 17546
IHRyaWJ1dGU= 17547
IHNrZXRjaA== 17548
IGVtcG93ZXI= 17549
INg= 17550
IEdsZW5u 17551
IERhdw== 17552
PVwi 17553
IFBvbGl0aWNz 17554
IGd1aWRlcw== 17555
IGRpb3hpZGU= 17556
IEdvcmU= 17557
IEJyaWdodA== 17558
IFNpZXJyYQ== 17559
IHZhbHVlZA== 17560
Y29uZA== 17561
IHBvaW50ZXI= 17562
U2VsZWN0 17563
IHJpc2t5 17564
IGFic29yYg== 17565
aW1hZ2Vz 17566
IHJlZnVzZXM= 17567
IGJvbnVzZXM= 17568
X19f 17569
IGhpbGFy 17570
IEZlYXR1cmVz 17571
MjIw 17572
IENvbGxlY3Rvcg== 17573
Rm9vdA== 17574
IDE5NjQ= 17575
Y3VsdXM= 17576
IGRhd24= 17577
IHdvcmtvdXQ= 17578
IExP 17579
IHBoaWxvc29waGljYWw= 17580
IFNhbmR5 17581
IFlvdXRo 17582
IGxpYWJsZQ== 17583
QWY= 17584
Ymx1ZQ== 17585
IG92ZXJ0dXJu 17586
bGVzc25lc3M= 17587
IFRyaWJ1bmU= 17588
IEluZw== 17589
IGZhY3Rvcmllcw== 17590
IGNhdGNoZXM= 17591
IHByb25l 17592
IG1hdHJpeA== 17593
IGxvZ2lu 17594
IGluYWNj 17595
IGV4ZXJ0 17596
c3lz 17597
IG5lZWRsZQ== 17598
IFF1cg== 17599
IG5vdGlmaWVk 17600
b3VsZGVy 17601
dHg= 17602
IHJlbWluZHM= 17603
IHB1Ymxpc2hlcnM= 17604
IG5vcnQ= 17605
IGdpdA== 17606
IGZsaWVz 17607
IEVtaWx5 17608
IGZsb3dpbmc= 17609
IEFsaWVu 17610
IFN0cmF0ZWc= 17611
IGhhcmRlc3Q= 17612
IG1vZGlmaWNhdGlvbg== 17613
QVBJ 17614
IE1Z 17615
IGNyYXNoZXM= 17616
c3RhaXJz 17617
bnVtYmVy 17618
IHVyZ2luZw== 17619
Y2hhbm5lbA== 17620
IEZhbGNvbg== 17621
IGluaGFiaXRhbnRz 17622
IHRlcnJpZnlpbmc= 17623
IHV0aWxpemU= 17624
IGJhbm5lcg== 17625
IGNpZ2FyZXR0ZXM= 17626
IHNlbnNlcw== 17627
IEhvbG1lcw== 17628
IHByYWN0aXRpb24= 17629
IFBoaWxsaXBz 17630
b3R0bw== 17631
IGNvbXBpbGU= 17632
TW9kZWw= 17633
IEtv 17634
IFtd 17635
QW1lcmljYW5z 17636
IFRlcm1z 17637
IG1lZGljYXRpb25z 17638
IEFuYQ== 17639
IGZ1bmRhbWVudGFsbHk= 17640
IE5vdGljZQ== 17641
IHdlYWtlcg== 17642
IDAwMDA= 17643
IGdhcmxpYw== 17644
IG91dGJyZWFr 17645
IGVjb25vbWlzdA== 17646
IEJpcnRo 17647
IG9ic3RhY2xlcw== 17648
YXJjZXI= 17649
IE9ydGhvZG94 17650
IHBsYWNlYm8= 17651
IENyZXc= 17652
YXNwYmVycnk= 17653
IEFuZ2Vscw== 17654
IGRpc2NoYXJnZQ== 17655
IGRlc3RydWN0aXZl 17656
MTE3 17657
IFJpc2luZw== 17658
IGRhaXJ5 17659
bGF0ZQ== 17660
IGNvbGxpc2lvbg== 17661
IFRpZ2Vycw== 17662
ZWFub3I= 17663
b2N1bWVudGVk 17664
IEludmFsaWQ= 17665
IGRvbnQ= 17666
IExpdGVy 17667
IFZh 17668
IGh5ZHJvZ2Vu 17669
IHZhcmlhbnRz 17670
IEJyb3ducw== 17671
IDE5NjU= 17672
IGluZGlnZW5vdXM= 17673
IHRyYWRlcw== 17674
IHJlbWFpbmRlcg== 17675
IHN3ZXB0 17676
IEltcGFjdA== 17677
IHJlZGlzdA== 17678
IHVuaW50 17679
Z3JhZHVhdGU= 17680
44OV 17681
IFdJTEw= 17682
44Gu5w== 17683
IENyaXRpY2Fs 17684
IGZpc2hlcg== 17685
IHZpY2lvdXM= 17686
IHJldmVyc2Vk 17687
WWVhcg== 17688
IFNveA== 17689
IHNob290aW5ncw== 17690
IGZpbG1pbmc= 17691
IHRvdWNoZG93bnM= 17692
YWlyZXM= 17693
bWVs 17694
IGdyYW5kZmF0aGVy 17695
IGFmZmVjdGlvbg== 17696
aW5nbGU= 17697
IG92ZXJseQ== 17698
QWRkaXRpb25hbA== 17699
IHN1cHJlbWU= 17700
IEdyYWQ= 17701
IHNwb3J0aW5n 17702
IG1lcmN5 17703
IEJyb29rcw== 17704
b3VudHk= 17705
IHBlcmZvcm1z 17706
IHRpZ2h0bHk= 17707
IGRlbW9ucw== 17708
IGtpbGxpbmdz 17709
IGZhY3Rpb24= 17710
IE5vdmE= 17711
YXV0cw== 17712
IHVuZG91YnRlZGx5 17713
YXJpbg== 17714
IHVuZGVyd2F5 17715
cmFr 17716
IGxpdg== 17717
IFJlZ2lvbg== 17718
IGJyaWVmaW5n 17719
c2Vycw== 17720
Y2xvdWQ= 17721
IE1paw== 17722
dXNw 17723
IHByZWRpY3Rpb24= 17724
YXpvcg== 17725
IHBvcnRhYmxl 17726
IEdhbmQ= 17727
IHByZXNlbnRpbmc= 17728
IDEwODA= 17729
wrs= 17730
dXNoaQ== 17731
IFNwYXJr 17732
dGhlcmV1bQ== 17733
IGp1c3RpZmljYXRpb24= 17734
IE55 17735
IGNvbnRyYWN0b3Jz 17736
bWluZ2hhbQ== 17737
IFN0eWxl 17738
5YU= 17739
IENocm9uaWNsZXM= 17740
IFBpY3R1cmU= 17741
IHByb3Zpbmc= 17742
IHdpdmVz 17743
c2V0dA== 17744
IG1vbGVjdWxlcw== 17745
IEZhaXJ5 17746
IGNvbnNpc3Rpbmc= 17747
IHBpZXI= 17748
YWxvbmU= 17749
aW5pdGlvbg== 17750
IG51Y2xl 17751
anNvbg== 17752
IGdvdHRh 17753
IG1vYmls 17754
IHZlcmJhbA== 17755
YXJpdW0= 17756
IG1vbnVtZW50 17757
dWNrZWQ= 17758
IDI1Ng== 17759
VGVjaA== 17760
bWluZWNyYWZ0 17761
IFRyYWNr 17762
IHRpbGU= 17763
IGNvbXBhdGliaWxpdHk= 17764
YXNpcw== 17765
IHNhZGQ= 17766
IGluc3RydWN0ZWQ= 17767
IE11ZWxsZXI= 17768
IGxldGhhbA== 17769
IGhvcm1vbmU= 17770
IG9yY2hl 17771
ZWxzZQ== 17772
IHNrZWxldA== 17773
IGVudGVydGFpbmluZw== 17774
IG1pbmltaXpl 17775
YWdhaW4= 17776
IHVuZGVyZ28= 17777
IGNvbnN0cmFpbnRz 17778
IGNpZ2FyZXR0ZQ== 17779
IElzbGFtaXN0 17780
IHRyYXZlbHM= 17781
IFBhbnRoZXJz 17782
bGluZ3M= 17783
Q2FyZQ== 17784
IGxhd3N1aXRz 17785
dXJhcw== 17786
IGNyeXN0 17787
IGxvd2VyZWQ= 17788
IGFlcmlhbA== 17789
IGNvbWJpbmF0aW9ucw== 17790
IGhhdW4= 17791
IGNoYQ== 17792
IHZpbmU= 17793
IHF1YW50aXRpZXM= 17794
IGxpbmtpbmc= 17795
YmFuaw== 17796
IHNveQ== 17797
QmlsbA== 17798
IEFuZ2VsYQ== 17799
IHJlY2lwaWVudA== 17800
IFByb3Rlc3Q= 17801
IHNvY2tldA== 17802
IHNvbGlkYXJpdHk= 17803
IOKG 17804
bWlsbA== 17805
IHZhcmllcw== 17806
IFBha2lzdGFuaQ== 17807
RHJhZ29u 17808
IHVuZQ== 17809
IGhvcml6b24= 17810
wqDCoMKgwqDCoMKgwqDCoA== 17811
IHByb3ZpbmNlcw== 17812
IGZyYW5rbHk= 17813
IGVuYWN0ZWQ= 17814
bm90ZXM= 17815
Wyc= 17816
IDE5Mg== 17817
b2NyYWN5 17818
IGVuZG9yc2VtZW50 17819
IG92ZXJ0aW1l 17820
VHJ1ZQ== 17821
TGFi 17822
bGljdGVk 17823
IEROQw== 17824
IGJlYXRz 17825
IEphbWll 17826
MTUy 17827
IElOVA== 17828
Q29udGFjdA== 17829
IGFjY291bnRlZA== 17830
aGFzaA== 17831
IFBhY2tlcnM= 17832
cGlyZXM= 17833
IGxlc2JpYW4= 17834
IGFtZW5kbWVudHM= 17835
IGhvcGVmdWw= 17836
IEZpbmxhbmQ= 17837
IHNwb3RsaWdodA== 17838
IGNvbmZpZ3VyZWQ= 17839
IHRyb3VibGVk 17840
IGdhemU= 17841
IENhbGdhcnk= 17842
IHJlbGlhYmlsaXR5 17843
IGluc3VyZw== 17844
c3dlcg== 17845
YnV5 17846
IFNraW4= 17847
IHBpeGVscw== 17848
IGhhbmRndW4= 17849
IHBhcmFz 17850
IGNhdGVnb3I= 17851
IEVM 17852
IFJleA== 17853
SW5kZWVk 17854
IGtpbmRh 17855
IGNvbmp1bmN0aW9u 17856
IEJyeWFu 17857
IE1hbnVmYWN0 17858
eWFuZw== 17859
UGx1cw== 17860
U1FM 17861
aXNobWVudA== 17862
IGRvbWluYXRl 17863
IG5haWw= 17864
IG9hdGg= 17865
IGVydXB0 17866
IEZpbmU= 17867
aXRiYXJ0 17868
IENoaXA= 17869
IEFiZA== 17870
IE5hbQ== 17871
IGJ1eWVy 17872
IGRpc3NlbnQ= 17873
TGVha3M= 17874
Q29udGlu 17875
IHJpZGVy 17876
IFNvbWVvbmU= 17877
IGlsbHVzaW9u 17878
Y2lu 17879
IEJvZWluZw== 17880
IGluYWRlcXU= 17881
b3ZhdGlvbg== 17882
aWFudHM= 17883
IHJlYnVpbGQ= 17884
NDUw 17885
IERlc3Rpbnk= 17886
U1c= 17887
IFRpbGw= 17888
SGl0 17889
aWF6 17890
IEJhbmds 17891
YWNoZXJz 17892
IFJlZm9ybQ== 17893
IHNlZ21lbnRz 17894
IHN5c3RlbWF0aWM= 17895
ZGM= 17896
IENvbnNlcnZhdGl2ZXM= 17897
IHBvcnRhbA== 17898
aG9y 17899
IERyYWdvbmJvdW5k 17900
IGRyYWdnZWQ= 17901
b21v 17902
IHRoZWU= 17903
YWR2ZXJ0 17904
IFJlcG9ydHM= 17905
IEV0 17906
IGJhcnJlbHM= 17907
QXVndXN0 17908
IGNvbXBhcmlzb25z 17909
IGhleA== 17910
IGFudGhyb3A= 17911
Ils= 17912
Ym9yb3VnaA== 17913
YWJp 17914
IHBpY3R1cmVk 17915
cGxheWluZw== 17916
IEFkZHJlc3M= 17917
IE1pcnJvcg== 17918
U21pdGg= 17919
IHRpcmVz 17920
IE5QUg== 17921
QUFBQQ== 17922
IGNsYXNzaWZpY2F0aW9u 17923
IFRoYW4= 17924
IEhhcm0= 17925
IFJB 17926
IHJlamVjdGlvbg== 17927
bWluYXRpb24= 17928
IHJhbmdlZA== 17929
IEZhbGxz 17930
REk= 17931
SG9zdA== 17932
44K0 17933
IEV4YW1wbGU= 17934
bGlzdGVk 17935
dGhpcmRz 17936
IHNhZmVndQ== 17937
YnJhbmQ= 17938
IHByb2JhYmxl 17939
Q2FuYWRh 17940
SVRJT04= 17941
IFFhZWRh 17942
IGNoaWNr 17943
IGltcG9ydHM= 17944
aGl0 17945
bG9j 17946
V1c= 17947
IGJsZXc= 17948
IGFueXRpbWU= 17949
IHdob2xlcw== 17950
aWtlZA== 17951
IGNhbGN1bGF0aW9u 17952
Y3JlYXRl 17953
IE9yaQ== 17954
IHVwZ3JhZGVk 17955
IGFwcGFy 17956
dXRvcnk= 17957
IE1vbA== 17958
QnJpdA== 17959
IEpvbmc= 17960
SU5BTA== 17961
IFN0YXJ0aW5n 17962
IGRpY2U= 17963
dXJ0bGU= 17964
IHJlbHlpbmc= 17965
Y2xvc3VyZQ== 17966
IHByb2ZpdGFibGU= 17967
IHNsYXVnaHRlcg== 17968
IE1hbnVhbA== 17969
Y2FzdGVy 17970
ICIk 17971
IGZlYXRoZXI= 17972
IFNpbXBseQ== 17973
aWV2ZXM= 17974
IGRldGVyaW9y 17975
IFBDSQ== 17976
IHN0YW1w 17977
IGZsYXdz 17978
IHNoYWRl 17979
aGFtbWVy 17980
IHBhc3Nwb3J0 17981
IGNvbnRpbmc= 17982
YW1lbA== 17983
IG9ic2VydmVycw== 17984
IG5lZ2xlY3Q= 17985
IFJC 17986
IEJyb3RoZXJob29k 17987
IHNrZXB0aWNhbA== 17988
ZmFtaWx5 17989
dXNr 17990
IGVtb3Rpb25hbGx5 17991
4pk= 17992
IEJldGE= 17993
YXNvbmFibGU= 17994
aWRpdHk= 17995
IE11bA== 17996
IGtpY2tpbmc= 17997
IENhcm0= 17998
b2xsYWg= 17999
VkVSVElT 18000
IEF0aGVu 18001
IGxhZGRlcg== 18002
IEJ1bGxldA== 18003
5aM= 18004
MDAwMQ== 18005
IFdpbGRsaWZl 18006
IE1hc2s= 18007
IE5hbg== 18008
UmV2 18009
IHVuYWNjZXB0YWJsZQ== 18010
bGVnYWw= 18011
IGNyb3dkZWQ= 18012
YWdp 18013
IENveA== 18014
amU= 18015
IG1vcmFsaXR5 18016
IGZ1ZWxz 18017
IGNhYmxlcw== 18018
IG1hbmtpbmQ= 18019
IENhcmliYmVhbg== 18020
IGFuY2hvcg== 18021
IGJ5dGU= 18022
IE9mdGVu 18023
IE96 18024
IGNyYWZ0ZWQ= 18025
IGhpc3Rvcmlhbg== 18026
IFd1 18027
IHRvd2Vycw== 18028
IENpdGl6ZW5z 18029
IGhlbG0= 18030
IGNyZWRlbnRpYWxz 18031
IHNpbmd1bGFy 18032
IEplc3Nl 18033
IHRhY2tsZXM= 18034
IGNvbnRlbXB0 18035
IGFmb3Jl 18036
IFNoYWRvd3M= 18037
IG5pbA== 18038
IHVyZ2VudA== 18039
YXBwbGU= 18040
Ymxvb2Q= 18041
IHZvbg== 18042
IG9mZmxpbmU= 18043
IGJyZWF0aGU= 18044
IGp1bXBz 18045
IGlycmVsZXZhbnQ= 18046
b3hpYw== 18047
b21hbA== 18048
aW1wb3J0YW50 18049
Smlt 18050
IGdsb3Zlcw== 18051
YXJtaW5n 18052
ZGVwdGg= 18053
IHRhbGVudHM= 18054
b29raWU= 18055
IFNC 18056
IHBhbG0= 18057
dWZmcw== 18058
ZXN0YQ== 18059
SUdI 18060
IGNhbm9u 18061
IFZlcml6b24= 18062
IFBsZQ== 18063
IGNvdXBsZWQ= 18064
dmVsdA== 18065
IGZ1bmRyYWlzaW5n 18066
IEdldHRpbmc= 18067
IERMQw== 18068
IG1hdGhlbWF0aWNhbA== 18069
IEhT 18070
IENhcmRpbmFscw== 18071
dGVsbGluZw== 18072
IHNwb25zb3Jz 18073
IM8= 18074
IEJ1bGxz 18075
b3B0aW9u 18076
IHByb3Bvc2U= 18077
IG1lbW9yYWJsZQ== 18078
IGVtYnJhY2Vk 18079
IGRlY2xpbmluZw== 18080
SGVhbHRo 18081
ZWRh 18082
IH07 18083
IHNwYW0= 18084
bWlsZQ== 18085
IHBpdGNoZXI= 18086
IEVpZ2h0 18087
IGNhcmluZw== 18088
dXRpYw== 18089
cm9sZQ== 18090
IGFpcmxpbmU= 18091
ZXJuYW5kZXo= 18092
IEF0aGxldA== 18093
IGNlcnRpZmljYXRpb24= 18094
dXhl 18095
cmlnZXI= 18096
IGVtcGly 18097
IHNlbnNhdGlvbg== 18098
IGRpc20= 18099
IGJvbHQ= 18100
IGV2b2x2ZQ== 18101
SG91c2U= 18102
IGNvbnN1bHRhdGlvbg== 18103
IER1dHk= 18104
IHRvdWNoZXM= 18105
IE5hdGhhbg== 18106
IGZhaW50 18107
aGFk 18108
Iig= 18109
IENvbnN1bWVy 18110
IEV4dHJlbWU= 18111
IDEyNw== 18112
IEhlcm0= 18113
IFNhY3JhbWVudA== 18114
aXpvcGg= 18115
IGFueGlvdXM= 18116
dWxvdXNseQ== 18117
IHNvY2lhbGx5 18118
IFVUQw== 18119
IHNvbHZpbmc= 18120
IExldHRlcg== 18121
SGlzdG9yeQ== 18122
ZWR1Yw== 18123
UHJpY2U= 18124
KSk7 18125
IHJlbG9hZA== 18126
YW1pYw== 18127
IHBvcms= 18128
IGRpc2NvdXJzZQ== 18129
IHRvdXJuYW1lbnRz 18130
YWlybw== 18131
IEt1cg== 18132
IENvc3Rh 18133
IHZpb2xhdGluZw== 18134
IGludGVyZmVyZQ== 18135
IHJlY3JlYXRpb25hbA== 18136
dWZmbGU= 18137
IHNwZWVjaGVz 18138
IG5lZWRpbmc= 18139
IHJlbWVtYmVycw== 18140
IGNyZWRpdGVk 18141
bmlh 18142
Zm9jdXNlZA== 18143
YW1lcmE= 18144
IGJydQ== 18145
dW1icw== 18146
IEN1YmFu 18147
IHByZWNlZGluZw== 18148
IG5vbnNlbnNl 18149
YWNpYWw= 18150
IHNtYXJ0cGhvbmVz 18151
IFN0b3JpZXM= 18152
U3BvcnRz 18153
IEVtZXJnZW5jeQ== 18154
b3VuY2luZw== 18155
ZWZpbmVk 18156
IGJlcg== 18157
IGNvbnN1bHRpbmc= 18158
IG1hc3RlcnM= 18159
aGVhc3Rlcm4= 18160
LiJb 18161
IFJ1bm5pbmc= 18162
IHN1c2NlcHQ= 18163
IEZlbmc= 18164
QW1lcmljYQ== 18165
cHJpc2Vz 18166
c3RpdGlhbA== 18167
IFdlZWtseQ== 18168
IEdyZWF0ZXI= 18169
bW9kdWxlcw== 18170
aWZ0ZXI= 18171
R3JhcGhpY3M= 18172
dWxlcg== 18173
IHdob2xseQ== 18174
IHN1cHByZXNz 18175
IGNvbmNlYWxlZA== 18176
IGhhcHBpbHk= 18177
IGFjY2VwdHM= 18178
IEVuam95 18179
IHJpdmVycw== 18180
IEV4Y2VwdA== 18181
MjI1 18182
IE5IUw== 18183
IE1jQ29ubmVsbA== 18184
IHB1c3N5 18185
ZmVycmVk 18186
dXRhYmxl 18187
IGF0dGFpbg== 18188
ID49 18189
IGRlcG9zaXRz 18190
cm9waGlj 18191
IG5vdG9yaW91cw== 18192
IFNoYXc= 18193
aWxpdGF0aW9u 18194
IGVwaWRlbWlj 18195
YWxsaWM= 18196
IHNtYWxsZXN0 18197
b3ZpY2g= 18198
IGFjY2Vzc29yaWVz 18199
cGVydGllcw== 18200
IHN1cnBsdXM= 18201
IE1lY2g= 18202
IGFtYmln 18203
IEltbWlncmF0aW9u 18204
IGNoaW0= 18205
ZXZhbA== 18206
IHByYWN0aWNpbmc= 18207
IE15c3Rlcnk= 18208
IGRvbWFpbnM= 18209
IFNpbGljb24= 18210
YXBwcw== 18211
IGtpbG9tZXRlcnM= 18212
ZWE= 18213
IFNtYXNo 18214
IHdhcnJhbnR5 18215
IG5vc3Q= 18216
c2ls 18217
cmV2 18218
Sm9u 18219
IER1Ymxpbg== 18220
IHRhc3Rlcw== 18221
IGJvdXQ= 18222
Z3JlYXQ= 18223
ZXJyb3I= 18224
IHN3aXRjaGVz 18225
IEJhcHQ= 18226
RE8= 18227
b2tp 18228
IHNvdXJjZWQ= 18229
cHJvZHU= 18230
IGF0dGFjaG1lbnQ= 18231
IElzc3Vl 18232
IFF1ZXN0aW9u 18233
Sm9pbg== 18234
IGZpdHRlZA== 18235
IHVubGF3ZnVs 18236
Xl4= 18237
ZXJlaw== 18238
IGF1dGhlbnRpY2F0aW9u 18239
IHN0b2xl 18240
IGFjY291bnRhYmlsaXR5 18241
bGFiZWw= 18242
U2VhcmNo 18243
IGFsYmVpdA== 18244
YXRpY2Fu 18245
ZnVuZGVk 18246
IEFkZGluZw== 18247
IElR 18248
IHN1Ym1hcg== 18249
bGl0 18250
YXF1ZQ== 18251
IExlYXJuaW5n 18252
IGludGVnZXI= 18253
TWFzdGVy 18254
IENocm9t 18255
IHByZW1pZXI= 18256
T3A= 18257
IExpdQ== 18258
IGJsZXNzZWQ= 18259
IEdsb2Jl 18260
IFJlc3BvbnNl 18261
IGxlZ2l0aW0= 18262
IE1lcmtlbA== 18263
IGRpc3Bvc2Fs 18264
wrQ= 18265
IGdhdWdl 18266
cGVhdA== 18267
IGluZHVjZWQ= 18268
IHF1ZXN0aW9uYWJsZQ== 18269
YXJ0aHk= 18270
IFZpdA== 18271
IEZlZWQ= 18272
VW50aWw= 18273
VXQ= 18274
d29ydGh5 18275
Ulk= 18276
IEhlcmFsZA== 18277
IEhhbW1lcg== 18278
IG1lZGFs 18279
IFJpdmVycw== 18280
IEhhY2s= 18281
IGNsYXJpZnk= 18282
IHRyYWNrZWQ= 18283
IGF1dG9ub21vdXM= 18284
IHRlbmFudA== 18285
IFFhdGFy 18286
ZXJpZQ== 18287
IGdyaW0= 18288
IE1vbml0b3I= 18289
IHJlc2lzdGFudA== 18290
IFNwZWM= 18291
IFdlbGxz 18292
TkFT 18293
MTQ4 18294
IG1pbmVycw== 18295
aW90aWNz 18296
IG1pc3Nlcw== 18297
MTE2 18298
Z2lhbg== 18299
Z2l0 18300
IEV5ZXM= 18301
cHJlcw== 18302
IGdyYWR1YXRlZA== 18303
IGFuZ2Vs 18304
IHN5bmNocm9u 18305
IGVmZmljaWVudGx5 18306
IHRyYW5zbWl0dGVk 18307
SGFycnk= 18308
IGdsb2JhbGx5 18309
RU5DRQ== 18310
IE1vbnRhbmE= 18311
cmFnZWQ= 18312
IFByZXZlbnRpb24= 18313
IHBpc3M= 18314
IExs 18315
IHNoZWxm 18316
IEJKUA== 18317
IFRlc3RhbWVudA== 18318
IExhdGU= 18319
aWtlcg== 18320
IEhhcHA= 18321
IEp1bGlhbg== 18322
aGFsbA== 18323
IHNwb250 18324
IHNodXRkb3du 18325
IGluY29uc2lzdGVudA== 18326
IHN1YnNjcmliZXJz 18327
IHNrZWxldG9u 18328
IE5lYnJhc2th 18329
IGluc3BpcmU= 18330
IFZvaWQ= 18331
RmVlZA== 18332
IGFuZ2xlcw== 18333
IFNwcmluZ3M= 18334
IGJlbmNobWFyaw== 18335
IHZhY2NpbmVz 18336
aXpvcGhyZW4= 18337
c2V4dWFs 18338
dWZmZWQ= 18339
IHNoaW5l 18340
IEthdGg= 18341
IGdlc3R1cmU= 18342
aW5lYQ== 18343
IHJpcA== 18344
IG9wcHJlc3Npb24= 18345
IGNvbnNjaWVuY2U= 18346
YnQ= 18347
IEx1bQ== 18348
IGluY2lkZW5jZQ== 18349
IEZh 18350
d3I= 18351
IG1pbmVyYWw= 18352
IFNwdXJz 18353
YWxreQ== 18354
IHRodW5kZXI= 18355
IG9waW8= 18356
QmVpbmc= 18357
IFBhbG0= 18358
IHdhc3RlZA== 18359
IGxi 18360
aWFyaWVz 18361
IEluaXRpYXRpdmU= 18362
IGN1cnJpYw== 18363
IG1hcmtlcg== 18364
IE1jTA== 18365
IGV4dGVuc2lvbnM= 18366
IFB2 18367
IEFybXM= 18368
IG9mZmVyaW5ncw== 18369
IGRlZmVuc2Vz 18370
IHZlbmRvcg== 18371
IGNvbnRyYWRpY3Q= 18372
IENvbGlu 18373
IHJlZGRpdA== 18374
IHBlcmlwaGVy 18375
MTIy 18376
IHNpbnM= 18377
RWRpdA== 18378
SUNU 18379
U29mdA== 18380
IFNoYWg= 18381
IGFkbWluaXN0cmF0b3I= 18382
IFRyaXA= 18383
IHBvcm5vZ3JhcGh5 18384
IHR1aXRpb24= 18385
aW5lbmNl 18386
IFByb2dyZXNz 18387
IGNhdGFsb2c= 18388
IHN1aXRl 18389
IGhpa2U= 18390
IHJlcHJvZHVjdGl2ZQ== 18391
ZW5naW5l 18392
IGRyb3VnaHQ= 18393
IE5vYWg= 18394
IDIzMA== 18395
IGR1ZGU= 18396
IHJlbGF4ZWQ= 18397
IHBhcnRpdGlvbg== 18398
IHBhcnRpY2lwYW50 18399
IHRlbGVzYw== 18400
IGZlYXM= 18401
IEZG 18402
b3duZXI= 18403
IHN3ZWVwaW5n 18404
IGxlbnNlcw== 18405
IG1hdGNodXA= 18406
IFJlcGw= 18407
b3VybmFscw== 18408
IGNyZWRpYmxl 18409
IGdyYW5kbW90aGVy 18410
IHRoZXJtYWw= 18411
IHN1YnNjcmliaW5n 18412
IGlkZW50aXRpZXM= 18413
Y29sbQ== 18414
VUNU 18415
IHJlbHVjdGFudA== 18416
dXNlcnM= 18417
IENvcnQ= 18418
IGFzc2lzdGVk 18419
T1NT 18420
QVRJT05T 18421
SVNI 18422
IHBoYXJtYWNldXRpY2Fs 18423
aWNhYmxl 18424
YWRpYW4= 18425
IFNvbmlj 18426
IEZ1cnk= 18427
IE1vbmc= 18428
QUg= 18429
IFBzeWNob2xvZ3k= 18430
IHBob3NwaA== 18431
IHRyZWF0cw== 18432
rZQ= 18433
IHN0ZWFkaWx5 18434
IEhlbGxv 18435
IHJlbGF0ZXM= 18436
IGNsdWU= 18437
RXhwbA== 18438
YXV0aA== 18439
IHJldmlzaW9u 18440
IGVsZA== 18441
b3Npb24= 18442
IGJyb24= 18443
MTQ0 18444
cmlrZXM= 18445
IG1pbmVz 18446
IGJsYW5rZXQ= 18447
IEZhaWw= 18448
ZWxlZA== 18449
IEltYWdpbmU= 18450
IFBsYW5uZWQ= 18451
YWlj 18452
UmVxdWVzdA== 18453
TWFk 18454
IEhvcnNl 18455
IEVhZ2xl 18456
IGNhcGFj 18457
MTU3 18458
IGxpbmc= 18459
IE5pY2U= 18460
IFBhcmVudGhvb2Q= 18461
bWluc3Rlcg== 18462
b2dz 18463
ZW5zaXRpdmU= 18464
Tm90aGluZw== 18465
IGNhcm4= 18466
Rmlu 18467
IFBF 18468
IHJpZmxlcw== 18469
IExQ 18470
U2FuZA== 18471
IGd1aUFjdGl2ZQ== 18472
IHRvdXJpc3Q= 18473
Q05O 18474
IHVudmVpbGVk 18475
IHByZWRlY2Vzc29y 18476
fXs= 18477
dWJlcg== 18478
IG9mZnNob3Jl 18479
IG9wdGljYWw= 18480
IFJvdA== 18481
IFBlYXJs 18482
ZXRvbg== 18483
IHN0YXJlZA== 18484
IGZhcnRoZXI= 18485
YXRpbGl0eQ== 18486
Y29udGlu 18487
IEd5 18488
IEZvc3Rlcg== 18489
IENvYw== 18490
cmllbnRz 18491
IGRlc2lnbmluZw== 18492
IEVjb25vbXk= 18493
T05H 18494
V29tZW4= 18495
IE5hbmN5 18496
ZXJ2ZXI= 18497
IG1hc2N1bA== 18498
IGNhc3VhbHRpZXM= 18499
IDIyNQ== 18500
IFN1bGxpdmFu 18501
IENob2ljZQ== 18502
IGFzdGVy 18503
d3M= 18504
IGhvdGVscw== 18505
IGNvbnNpZGVyYXRpb25z 18506
IGNvdWNo 18507
IFN0cmlw 18508
IEdu 18509
IG1hbmlwdWxhdGU= 18510
bGllZA== 18511
IHN5bnRoZXRpYw== 18512
IGFzc2F1bHRlZA== 18513
IG9mZmVuc2Vz 18514
IERyYWtl 18515
IGltcGU= 18516
T2N0b2Jlcg== 18517
IEhlcml0YWdl 18518
aGw= 18519
IEJsYWly 18520
VW5saWtl 18521
IGdyaWVm 18522
IDQ1MA== 18523
IG9wdGVk 18524
IHJlc2lnbmF0aW9u 18525
aWxv 18526
IHZlcnNl 18527
IFRvbWI= 18528
IHVwdA== 18529
IGFpcmVk 18530
IEhvb2s= 18531
IE1MQg== 18532
IGFzc3VtZXM= 18533
b3V0ZWQ= 18534
IFZlcnM= 18535
IGluZmVyaW9y 18536
IGJ1bmRsZQ== 18537
IEROUw== 18538
b2dyYXBoZXI= 18539
IG11bHRpcA== 18540
IFNvdWxz 18541
IGlsbHVzdHJhdGVk 18542
IHRhY3RpYw== 18543
IGRyZXNzaW5n 18544
IGR1bw== 18545
Q29uZg== 18546
IHJlbGVudA== 18547
IGNhbnQ= 18548
IHNjYXJjZQ== 18549
IGNhbmR5 18550
IENG 18551
IGFmZmlsaWF0ZWQ= 18552
IHNwcmludA== 18553
eWxhbg== 18554
IEdhcmNpYQ== 18555
IGp1bms= 18556
UHJpbnQ= 18557
ZXhlYw== 18558
Q3JpdA== 18559
IHBvcnRyYWl0 18560
aXJpZXM= 18561
IE9GRg== 18562
IGRpc3B1dGVz 18563
V1I= 18564
TG92ZQ== 18565
44GE 18566
IFJleW4= 18567
IGhpcHA= 18568
b3BhdGg= 18569
IGZsb29ycw== 18570
IEZlZWw= 18571
IHdvcnJpZXM= 18572
IHNldHRsZW1lbnRz 18573
IFBvcw== 18574
IG1vc3F1ZQ== 18575
IGZpbmFscw== 18576
IGNydXNoZWQ= 18577
IFByb2JhYmx5 18578
IEJvdA== 18579
IE1hbnM= 18580
IFBlcmlvZA== 18581
IHNvdmVyZWlnbnR5 18582
IHNlbGxlcg== 18583
IGFwb3N0 18584
IGFtYXRldXI= 18585
IGRvcm0= 18586
IGNvbnN1bWluZw== 18587
IGFybW91cg== 18588
IFJvb3Nl 18589
IGludGVuc2l2ZQ== 18590
IGVsaW1pbmF0aW5n 18591
IFN1bm5p 18592
IEFsZXBwbw== 18593
amlu 18594
IGFkdmlzZQ== 18595
cGFs 18596
IEhhbG8= 18597
IGRlc2NlbnQ= 18598
IHNpbXBsZXI= 18599
IGJvb3Ro 18600
U1RS 18601
TGF0ZXI= 18602
IENhdmU= 18603
PT09 18604
IG1vbA== 18605
IGZpc3Q= 18606
IHNob3RndW4= 18607
c3VwcA== 18608
IHJvYmJlcnk= 18609
RWZmZWN0 18610
IG9ic2N1cmU= 18611
IFByb2Zlc3Npb25hbA== 18612
IGVtYmFzc3k= 18613
IG1pbGl0YW50 18614
IGluY2FyY2Vy 18615
IGdlbmVyYXRlcw== 18616
IGxhdW5jaGVz 18617
IGFkbWluaXN0cmF0b3Jz 18618
IHNoYWZ0 18619
IGNpcmN1bGFy 18620
IGZyZXNobWFu 18621
IFdlcw== 18622
IEpvZWw= 18623
IERyZXc= 18624
IER1bmNhbg== 18625
IEFwcGFyZW50bHk= 18626
c2lnaHQ= 18627
IEludGVybmFs 18628
IEluZGl2aWR1YWw= 18629
IEZF 18630
IGJvcmU= 18631
IE10 18632
IGJyb2FkbHk= 18633
IE9wdGlvbnM= 18634
b3VudGFpbg== 18635
aXBlcw== 18636
IFZpZGVvcw== 18637
MjA0 18638
IGhpbGxz 18639
IHNpbXVsYXRpb24= 18640
IGRpc2FwcG9pbnRtZW50 18641
aXRhbg== 18642
IExhYm9yYXRvcnk= 18643
IHVwd2FyZA== 18644
IGJvdW5kYXJ5 18645
IGRhcmtlcg== 18646
aGFydA== 18647
IGRvbWluYW5jZQ== 18648
Q29uZw== 18649
IE9yYWNsZQ== 18650
IExvcmRz 18651
IHNjaG9sYXJzaGlw 18652
IFZpbmNlbnQ= 18653
ZWRl 18654
IFJhaA== 18655
IGVuY291cmFnZXM= 18656
cm92 18657
IHF1bw== 18658
IHByZW1pc2U= 18659
IENyaXNpcw== 18660
IEhvbG9jYXVzdA== 18661
IHJoeXRobQ== 18662
IG1ldHJpYw== 18663
Y2x1Yg== 18664
IHRyYW5zcG9ydGVk 18665
IG5vZA== 18666
IFBpc3Q= 18667
IGFuY2VzdG9ycw== 18668
IEZyZWRlcg== 18669
dGh1bWJuYWlscw== 18670
IENF 18671
T05E 18672
UGhpbA== 18673
dmVuZ2U= 18674
IFByb2R1Y3Rz 18675
Y2FzdGxl 18676
IHF1YWxpZnlpbmc= 18677
IEthcmVu 18678
VkVSVElTRU1FTlQ= 18679
IG1pZ2h0eQ== 18680
IGV4cGxhbmF0aW9ucw== 18681
IGZpeGluZw== 18682
RGk= 18683
IGRlY2xhcmluZw== 18684
IGFub255bWl0eQ== 18685
IGp1dmVu 18686
IE5vcmQ= 18687
IERvb20= 18688
IEFjdHVhbGx5 18689
T2s= 18690
cGhpcw== 18691
IERlc2VydA== 18692
IDExNg== 18693
SUs= 18694
IEZN 18695
IGluY29tZXM= 18696
VkVM 18697
b2tlcnM= 18698
IHBlY3Vs 18699
IGxpZ2h0d2VpZ2h0 18700
Z3Vl 18701
IGFjY2VudA== 18702
IGluY3JlbWVudA== 18703
IENoYW4= 18704
IGNvbXBsYWluaW5n 18705
IEJhZ2hk 18706
IG1pZGZpZWxkZXI= 18707
IG92ZXJoYXVs 18708
UHJvY2Vzcw== 18709
IEhvbGxvdw== 18710
IFRpdGFucw== 18711
U21hbGw= 18712
bWFudWVs 18713
IFVuaXR5 18714
IEV2ZW50cw== 18715
U3R5 18716
IGRpc3Byb3BvcnRpb24= 18717
bmVzdHk= 18718
ZW5lcw== 18719
IENvZA== 18720
IGRlbW9uc3RyYXRpb25z 18721
IENyaW1zb24= 18722
IE9I 18723
IGVucm9sbGVk 18724
IGNlbA== 18725
IEJyZXR0 18726
IGFpZGU= 18727
IGhlZWxz 18728
IGJyb2FkYmFuZA== 18729
IG1hcmtpbmc= 18730
IHdpemFyZA== 18731
IE5K 18732
IENoaWVmcw== 18733
IGluZ3JlZGllbnQ= 18734
IGR1Zw== 18735
IFNodXQ= 18736
dXJjaGFzZQ== 18737
ZW5kb3I= 18738
IGZhcm1lcg== 18739
IEdvbGRtYW4= 18740
MTI5 18741
MTU1 18742
T3JkZXI= 18743
IGxpb24= 18744
aWFibHk= 18745
IHN0YWlu 18746
YXJyYXk= 18747
aWxpdGFyeQ== 18748
IEZBUQ== 18749
IGV4cGxvZGVk 18750
IE1jQ2FydGh5 18751
IFR3ZWV0 18752
IEdyZWVucw== 18753
ZWtpbmc= 18754
bG4= 18755
ZW5zZW4= 18756
IG1vdG9yY3ljbGU= 18757
IHBhcnRpY2xl 18758
IGNob2xlc3Rlcm9s 18759
QnJvbg== 18760
IHN0YWly 18761
IG94aWQ= 18762
IGRlc2lyYWJsZQ== 18763
aWJsZXM= 18764
IHRoZW9y 18765
Zm9yY2luZw== 18766
IHByb21vdGlvbmFs 18767
b3Zv 18768
Ym9vdA== 18769
IEJvbnVz 18770
cmF3bGluZw== 18771
IHNob3J0YWdl 18772
IFBzeQ== 18773
IHJlY3J1aXRlZA== 18774
IGluZmFudHM= 18775
IHRlc3Rvc3Rlcm9uZQ== 18776
IGRlZHVjdA== 18777
IGRpc3RpbmN0aXZl 18778
IGZpcm13YXJl 18779
YnVpbHQ= 18780
MTQ1 18781
IGV4cGxvcmVk 18782
IGZhY3Rpb25z 18783
IHZpZGU= 18784
IHRhdHRvbw== 18785
IGZpbmFuY2lhbGx5 18786
IGZhdGlndWU= 18787
IHByb2NlZWRpbmc= 18788
Y29uc3RpdHV0aW9uYWw= 18789
IG1pc2Vy 18790
IGNoYWlycw== 18791
Z2dpbmc= 18792
aXBwbGU= 18793
IGRlbnQ= 18794
IGRpc3JlZw== 18795
55Q= 18796
c3RhbnQ= 18797
bGxv 18798
YnBz 18799
YWtlbmluZw== 18800
IGFibm9ybWFs 18801
IEVSQQ== 18802
5aOr 18803
IEhCTw== 18804
IE1BUg== 18805
IGNvbmNlc3M= 18806
IHNlcnZhbnQ= 18807
IGFzcGly 18808
bGF2 18809
IFBhbmVs 18810
YW1v 18811
IHByZWNpcA== 18812
IHJlY29yZGluZ3M= 18813
IHByb2NlZWRlZA== 18814
IGNvbG9ueQ== 18815
IFRhbmc= 18816
YWJsbw== 18817
IHN0cmlwcGVk 18818
TGVmdA== 18819
dG9v 18820
IHBvdGF0b2Vz 18821
IGZpbmVzdA== 18822
JSku 18823
IGNyYXA= 18824
IFphY2g= 18825
YWJhc2Vz 18826
IEdvdGg= 18827
IGJpbGxpb25haXJl 18828
d29sZg== 18829
IHNhbmN0aW9u 18830
U0s= 18831
IGxvZ2dlZA== 18832
UG8= 18833
ZXllZA== 18834
dW5hbA== 18835
IGNyaWNrZXQ= 18836
IGFybWllcw== 18837
IHVuY292ZXJlZA== 18838
Q2xvdWQ= 18839
w7Nu 18840
IHJlYm91bmRz 18841
IG1lcw== 18842
T3Blcg== 18843
UGFj 18844
IG5hdGlvbmFsbHk= 18845
IGluc2VydGVk 18846
cGljdA== 18847
IGdvdmVybmFuY2U= 18848
0Lg= 18849
IHByaXZpbGVnZXM= 18850
R0VU 18851
IGZhdm9yaXRlcw== 18852
aW1pdHk= 18853
IGxvdmVy 18854
dGhlbQ== 18855
ZW1wbA== 18856
IGdvcmdlb3Vz 18857
QW5u 18858
IHNsaXBwZWQ= 18859
IHZldG8= 18860
Qm9i 18861
IHNsaW0= 18862
dWNj 18863
IEZhbWU= 18864
dWRkZW5seQ== 18865
IGRlbmllcw== 18866
IE1hdXI= 18867
IGRpc3RhbmNlcw== 18868
IHdhbm5h 18869
dGFy 18870
IFNFUg== 18871
IOKI 18872
IGxlbW9u 18873
YXRoZXRpYw== 18874
IGxpdGVyYWw= 18875
IGRpc3Rpbmd1aXNoZWQ= 18876
IGFuc3dlcmluZw== 18877
R0k= 18878
IHJlbGlnaW9ucw== 18879
IFBoaWxvcw== 18880
IExheQ== 18881
IGNvbXBvcw== 18882
aXJlbWVudHM= 18883
IEtvcw== 18884
aW5leg== 18885
cm9sbGluZw== 18886
IHlvdW5nZXN0 18887
YW5kaXNl 18888
IEJvcm4= 18889
IGFsdGFy 18890
YW1pbmE= 18891
IEJvb3Q= 18892
dm9j 18893
IGRpZ2dpbmc= 18894
IHByZXNzdXJlcw== 18895
IGxlbg== 18896
MjY0 18897
IGFzc2Fzc2luYXRpb24= 18898
IEJpcm1pbmdoYW0= 18899
IE15dGg= 18900
IHNvdmVyZWlnbg== 18901
IEFydGlzdA== 18902
IFBob3RvZ3JhcGg= 18903
IGRlcGljdGVk 18904
IGRpc3BlbnM= 18905
b3J0aHk= 18906
IGFtYnVs 18907
aW50ZWc= 18908
IENlbGU= 18909
IFRpYmV0 18910
IGhpZXJhcmNoeQ== 18911
IGN1 18912
IHByZXNlYXNvbg== 18913
IFBldGVyc29u 18914
IGNvbG91cnM= 18915
IHdvcnJ5aW5n 18916
IGJhY2tlcnM= 18917
IFBhbG1lcg== 18918
IM68 18919
IGNvbnRyaWJ1dG9y 18920
IGhlYXJpbmdz 18921
IHVyaW5l 18922
INk= 18923
b3VyZ2VvaXM= 18924
U2ltaWxhcg== 18925
IFppbW1lcg== 18926
c29tZXRoaW5n 18927
IFVTQw== 18928
IHN0cmVuZ3Rocw== 18929
IEZJ 18930
IGxvZ2dpbmc= 18931
QXNrZWQ= 18932
IFRoYWk= 18933
aW5xdQ== 18934
IFdhbHQ= 18935
IGNyZXdz 18936
aXRpc20= 18937
MzAx 18938
IHNoYXJwbHk= 18939
dW1lZA== 18940
IHJlZGlyZWN0 18941
cmF0b3Jz 18942
SW5m 18943
IFdlYXBvbnM= 18944
IHRlYXNw 18945
MTk5OQ== 18946
TGl2ZQ== 18947
IEVzcGVjaWFsbHk= 18948
IFN0ZXI= 18949
IFZldGVyYW5z 18950
IGludHJv 18951
b3RoZXJhcHk= 18952
IG1hbHdhcmU= 18953
IGJyZWVkaW5n 18954
IG1vbGVjdWxhcg== 18955
IFJvdXRl 18956
IENvbW1lbnQ= 18957
b2NoZW0= 18958
IGFpbg== 18959
U2Vhc29u 18960
IGxpbmViYWNrZXI= 18961
xKs= 18962
IEVjb25vbWljcw== 18963
ZXNhcg== 18964
IExpdmVz 18965
IEVtbWE= 18966
IGtpbg== 18967
IFRlcnJpdA== 18968
IHBsYW50ZWQ= 18969
b3Rvbg== 18970
IEJ1dHRlcg== 18971
IFNwb25z 18972
UEVS 18973
IGR1bmdlb24= 18974
IHN5bWJvbGlj 18975
IGZpbG1lZA== 18976
IGRpZXRz 18977
IGNvbmNsdWRlcw== 18978
IGNlcnRhaW50eQ== 18979
IEZvcm1hdA== 18980
IHN0cmFuZ2Vycw== 18981
Zm9ybWF0 18982
IFBoYXNl 18983
IGNvcGllZA== 18984
IG1ldHJlcw== 18985
bGRh 18986
IFVzZXJz 18987
IGRlbGliZXJhdGU= 18988
IHdhc2hlZA== 18989
IExhbmNl 18990
aW1hdGlvbg== 18991
IGltcHJvcGVy 18992
IEdlbmVzaXM= 18993
aWNrcg== 18994
IEt1c2g= 18995
IHJlYWxpc2U= 18996
IGVtYmFycmFzc2luZw== 18997
YWxraW5n 18998
YnVja3M= 18999
IHZlcmlmaWVk 19000
IG91dGxpbmU= 19001
eWVhcnM= 19002
IEluY29tZQ== 19003
MjAy 19004
IHpvbWJpZXM= 19005
RmluYWw= 19006
IE1pbGxlbm4= 19007
IG1vZGlmaWNhdGlvbnM= 19008
IFZpc2lvbg== 19009
IE1vc2Vz 19010
dmVyYg== 19011
aXRlcnJhbmVhbg== 19012
IEpldA== 19013
IG5hdmFs 19014
IEFnZw== 19015
IHVybA== 19016
IHZpY3Rvcmllcw== 19017
IG5vbmV0aGVsZXNz 19018
IGluanVzdA== 19019
IEZhY3Q= 19020
55o= 19021
IGluc3VmZmljaWVudA== 19022
cmV2aWV3 19023
ZmFjZWJvb2s= 19024
IG5lZ290aWF0aW5n 19025
IGd1YXJhbnRlZXM= 19026
aW1lbg== 19027
dXRlbmJlcmc= 19028
IGdhbWJsaW5n 19029
IGNvbmdy 19030
TG9hZGluZw== 19031
IG5ldmVydGhlbGVzcw== 19032
IHByZXNpZGVudHM= 19033
IEluZHVzdHJpYWw= 19034
IDExOA== 19035
IHBvdXJlZA== 19036
IFRvcnk= 19037
IDE3NQ== 19038
IDo9 19039
U2NvdHQ= 19040
YW5nZXJlZA== 19041
VG9r 19042
IG9yZ2FuaXplcnM= 19043
TWF0 19044
IEdyb3d0aA== 19045
IGFkdWw= 19046
IGVuc3VyZXM= 19047
IDExNw== 19048
6b6N5Q== 19049
IG1hc3NhY3Jl 19050
IGdyYWRlcw== 19051
YmVmb3Jl 19052
QURWRVJUSVNFTUVOVA== 19053
IFNsb3c= 19054
IE1NQQ== 19055
4oCUIg== 19056
IFZhdGljYW4= 19057
UWFlZGE= 19058
IG93ZQ== 19059
NjY2Ng== 19060
IFNvcnJ5 19061
IEdyYXNz 19062
IGJhY2tncm91bmRz 19063
IGV4aGF1c3RlZA== 19064
IGNsYW4= 19065
IGNvbXByb21pc2Vk 19066
IEVsZg== 19067
IElzYWFj 19068
ZW5zb24= 19069
SW52ZXN0 19070
SUZB 19071
IGludGVycnVwdGVk 19072
44OJ44Op 19073
IHR3aXN0ZWQ= 19074
IERyYWdvbnM= 19075
TW9kZQ== 19076
IEtyZW1saW4= 19077
IGZlcnRpbA== 19078
aGVyZXM= 19079
cGhhbg== 19080
IE5vZGU= 19081
ZmVk 19082
IE9yYw== 19083
IHVud2lsbGluZw== 19084
Q2VudA== 19085
IHByaW9yaXQ= 19086
IGdyYWR1YXRlcw== 19087
IHN1YmplY3RpdmU= 19088
IGlzc3Vpbmc= 19089
IEx0 19090
IHZpZXdlcg== 19091
IHdva2U= 19092
VGh1cw== 19093
YnJvb2s= 19094
IGRlcHJlc3NlZA== 19095
IGJyYWNrZXQ= 19096
IEdvcg== 19097
IEZpZ2h0aW5n 19098
IHN0cmlrZXI= 19099
UmVwb3J0 19100
IFBvcnR1Z2Fs 19101
IG5lbw== 19102
d2Vk 19103
MTk5 19104
IGZsZWVpbmc= 19105
c2hhZG93 19106
aWRlbnRpZmllZA== 19107
VVNF 19108
U3RlYW0= 19109
IHN0cmV0Y2hlZA== 19110
IHJldmVsYXRpb25z 19111
YXJ0ZWQ= 19112
IER3 19113
IGFsaWdubWVudA== 19114
ZXN0b24= 19115
IEphcmVk 19116
U2Vw 19117
IGJsb2dz 19118
dXBkYXRl 19119
Z29t 19120
cmlzaw== 19121
IGNsYXNo 19122
IEhvdXI= 19123
IHJ1bnRpbWU= 19124
IHVud2FudGVk 19125
IHNjYW0= 19126
IHJhY2s= 19127
IGVubGlnaHQ= 19128
b25lc3Q= 19129
IEZlcnI= 19130
IGNvbnZpY3Rpb25z 19131
IHBpYW5v 19132
IGNpcmN1bGF0aW9u 19133
IFdlbGNvbWU= 19134
IGJhY2tsYXNo 19135
IFdhZGU= 19136
IHJlY2VpdmVycw== 19137
b3RpdmU= 19138
SmVmZg== 19139
IG5ldHdvcmtpbmc= 19140
IFByZXA= 19141
IEV4cGxvcmVy 19142
IGxlY3R1cmU= 19143
IHVwbG9hZGVk 19144
IE1lYXQ= 19145
QkxF 19146
IE5hemlz 19147
IFN5bmQ= 19148
c3R1ZA== 19149
cm9vdHM= 19150
cmlhbnM= 19151
IHBvcnRyYXllZA== 19152
ID8/ 19153
IEJ1ZGRoYQ== 19154
c3Vu 19155
Um9iZXJ0 19156
IENvbXBsZXg= 19157
IG92ZXJzZWU= 19158
IHN0ZWFsdGg= 19159
VGl0bGU= 19160
IEpvYnM= 19161
IEt1bQ== 19162
IGFwcHJlY2lhdGlvbg== 19163
IE1PRA== 19164
IGJhc2ljcw== 19165
IGNsaXBz 19166
IG51cnNpbmc= 19167
IHByb3Bvc2l0aW9u 19168
IHJlYWxpc2Vk 19169
IE5ZQw== 19170
IGFsbG9jYXRlZA== 19171
cml1bQ== 19172
YXJhbg== 19173
IFByb2R1Y3Rpb24= 19174
IFZvdGU= 19175
IHNtdWdn 19176
IGh1bnRlcg== 19177
YXplcg== 19178
IENoYW5nZXM= 19179
IGZsdWN0 19180
eW9u 19181
QXJyYXk= 19182
IGtpdHM= 19183
V2F0ZXI= 19184
IHVuY29tbW9u 19185
IHJlc3Rpbmc= 19186
ZWxscw== 19187
d291bGQ= 19188
IHB1cnN1ZWQ= 19189
IGFzc2VydGlvbg== 19190
b21ldG93bg== 19191
IE1vc3Vs 19192
IFBsYXRmb3Jt 19193
aW9sZXQ= 19194
IHNoYXJlaG9sZGVycw== 19195
IHRyYWlscw== 19196
UGF5 19197
IEVuZm9yY2VtZW50 19198
dHlwZXM= 19199
IEFub255bW91cw== 19200
IHNhdGlzZnlpbmc= 19201
aWxvZ3k= 19202
ICgn 19203
d2F2ZQ== 19204
Y2l0eQ== 19205
U3RldmU= 19206
IGNvbmZyb250YXRpb24= 19207
IEVsZA== 19208
Q2FwdA== 19209
YWhhbg== 19210
aHRt 19211
IEN0cmw= 19212
T05T 19213
MjMw 19214
aWZh 19215
aG9sZGluZw== 19216
IGRlbGljYXRl 19217
IGphdw== 19218
IEdvaW5n 19219
b3J1bQ== 19220
U2Fs 19221
IGR1bGw= 19222
IEJldGg= 19223
IHByaXNvbnM= 19224
IGVnbw== 19225
IEVsc2E= 19226
YXZvcml0ZQ== 19227
IEdhbmc= 19228
IE51Y2xlYXI= 19229
IHNwaWRlcg== 19230
YXRzdQ== 19231
IHNhbXBsaW5n 19232
IGFic29yYmVk 19233
IFBoYXJt 19234
aWV0aA== 19235
IGJ1Y2tldA== 19236
IFJlY29tbQ== 19237
T0Y= 19238
IEZhY3Rvcnk= 19239
QU5DRQ== 19240
IGJhY3Rlcg== 19241
SGFz 19242
IE9ic2Vydg== 19243
MTIx 19244
IHByZW1pZXJl 19245
RGV2ZWxvcA== 19246
IGN1cnJlbmNpZXM= 19247
Q2FzdA== 19248
IGFjY29tcGFueWluZw== 19249
IE5hc2h2aWxsZQ== 19250
IGZhdHR5 19251
IEJyZW5k 19252
IGxvY2tz 19253
IGNlbnRlcmVk 19254
IFVU 19255
YXVnaHM= 19256
b3JpZQ== 19257
IEFmZm9yZGFibGU= 19258
dmFuY2U= 19259
REw= 19260
ZW1ldA== 19261
IHRocm9uZQ== 19262
IEJsdWV0b290aA== 19263
IG5hbWluZw== 19264
aWZ0cw== 19265
QURF 19266
IGNvcnJlY3RlZA== 19267
IHByb21wdGx5 19268
IFNUUg== 19269
IGdlbm9tZQ== 19270
IGNvcGU= 19271
IHZhbGxleQ== 19272
IHJvdW5kZWQ= 19273
IEtlbmQ= 19274
YWxpb24= 19275
cGVycw== 19276
IHRvdXJpc20= 19277
IHN0YXJr 19278
dmw= 19279
IGJsb3dpbmc= 19280
IFNjaGVkdWxl 19281
c3Rk 19282
IHVuaGFwcHk= 19283
IGxpdGlnYXRpb24= 19284
Y2VkZXM= 19285
IGFuZHJvaWQ= 19286
IGludGVncmFs 19287
ZXJlcnM= 19288
dWRlZA== 19289
dGF4 19290
IHJlaXRlcg== 19291
IE1vdG9ycw== 19292
b2NpYXRlZA== 19293
IHdvbmRlcnM= 19294
IEFwb3N0 19295
dWNraW5n 19296
IFJvb3NldmVsdA== 19297
ZnJhbQ== 19298
IHlpZWxkcw== 19299
IGNvbnN0aXR1dGVz 19300
YXdr 19301
SW50ZXJlc3Q= 19302
IGludGVyaW0= 19303
IGJyZWFrdGhyb3VnaA== 19304
IENoZXI= 19305
IHByb3NlYw== 19306
IERq 19307
IE1U 19308
UmVzcA== 19309
IFBU 19310
IHNwZXJt 19311
ZWRpdA== 19312
QlQ= 19313
TGludXg= 19314
Y291bnRyeQ== 19315
bGVhZ3Vl 19316
IGRpY2s= 19317
IG9jdA== 19318
IGluc2VydGluZw== 19319
IHNjcmE= 19320
IEJyZXdpbmc= 19321
IDE5NjY= 19322
IHJ1bm5lcnM= 19323
IHBsdW4= 19324
aWR5 19325
IERpYW4= 19326
IGR5c2Z1bmN0aW9u 19327
IGV4Y2x1c2lvbg== 19328
IGRpc2dy 19329
IGluY29ycG9yYXRl 19330
IHJlY29uYw== 19331
IG5vbWluYXRlZA== 19332
IEFyY2hlcg== 19333
ZHJhdw== 19334
YWNoZWxvcg== 19335
IHdyaXRpbmdz 19336
IHNoYWxsb3c= 19337
IGhhc3Q= 19338
IEJNVw== 19339
IFJT 19340
IHRoaWdo 19341
IDE5NjM= 19342
IGxhbWI= 19343
IGZhdm9yZWQ= 19344
YWdsZQ== 19345
IGNvb2xlcg== 19346
IEhvdXJz 19347
IEdV 19348
IE9yaWdpbg== 19349
IGdsaW1wc2U= 19350
LS0tLS0tLS0tLS0tLS0tLS0tLS0= 19351
TGlt 19352
IGNoZWVr 19353
IGplYWxvdXM= 19354
LSc= 19355
IGhhcm5lc3M= 19356
IFBvaXNvbg== 19357
IGRpc2FiaWxpdGllcw== 19358
bmVhcG9saXM= 19359
IG91dGxvb2s= 19360
IG5vdGlmeQ== 19361
IEluZGlhbmFwb2xpcw== 19362
IGFicnVwdA== 19363
bnNpYw== 19364
IGVuY3J5cHRlZA== 19365
IGZvcmZl 19366
cmVhdGg= 19367
IHJhYmI= 19368
IGZvdW5kYXRpb25z 19369
IGNvbXBsaW1lbnQ= 19370
IEludGVydmlldw== 19371
IFN3ZQ== 19372
IGFkb2xlc2M= 19373
IG1vbml0b3Jz 19374
IFNhY3JhbWVudG8= 19375
IHRpbWVseQ== 19376
IGNvbnRlbXBs 19377
IHBvc2l0aW9uZWQ= 19378
IHBvc3RlcnM= 19379
cGhpZXM= 19380
aW92YXNjdWxhcg== 19381
dm9pZA== 19382
IEZpZnRo 19383
IGludmVzdGlnYXRpdmU= 19384
T1VO 19385
IGludGVncmF0ZQ== 19386
IElOQw== 19387
aXNoYQ== 19388
aWJsaW5ncw== 19389
IFJlcXVlc3Q= 19390
IFJvZHJpZ3Vleg== 19391
IHNsaWRlcw== 19392
IERY 19393
IGZlbWluaXNt 19394
IGRhdGFz 19395
IGJlbmQ= 19396
aXJ1cw== 19397
IE5pZ2VyaWE= 19398
Rm94 19399
Q2hhbmdl 19400
IGFpcnBsYW5l 19401
IExhZGVu 19402
IHB1YmxpY2l0eQ== 19403
aXh0eQ== 19404
IGNvbW1pdG1lbnRz 19405
IGFnZ3JlZ2F0ZQ== 19406
IGRpc3BsYXlpbmc= 19407
IEFycm93 19408
IDEyMg== 19409
IHJlc3BlY3Rz 19410
YW5kcm9pZA== 19411
c2l4 19412
IFNoYQ== 19413
IHJlc3RvcmF0aW9u 19414
KVw= 19415
V1M= 19416
b3lz 19417
IGlsbHVzdHJhdGU= 19418
d2l0aG91dA== 19419
MTI2 19420
IOKUgg== 19421
IHBpY2t1cA== 19422
bmVscw== 19423
IC4uLi4= 19424
Zm9vZA== 19425
IEZlbg== 19426
KT8= 19427
IHBoZW5vbWVuYQ== 19428
IGNvbXBhbmlvbnM= 19429
IFdyaXRl 19430
IHNwaWxs 19431
IGJyaWRnZXM= 19432
IFVwZGF0ZWQ= 19433
IEZv 19434
IGluc2VjdHM= 19435
QVNISU5HVE9O 19436
IHNjYXJl 19437
aWx0cg== 19438
IFpoYW5n 19439
IHNldmVyaXR5 19440
IGluZHVs 19441
MTQ5 19442
IENvZmZlZQ== 19443
IG5vcm1z 19444
IHB1bHNl 19445
IEZU 19446
IGhvcnJpZmlj 19447
IERlc3Ryb3k= 19448
IEpTT04= 19449
IG9saXZl 19450
IGRpc2N1c3Nlcw== 19451
UmVzdA== 19452
RWxlY3Q= 19453
IFdpbm4= 19454
IFN1cnZpdg== 19455
IEhhaXQ= 19456
U3VyZQ== 19457
b3BlZA== 19458
IHJvb3RlZA== 19459
IFNrZQ== 19460
IEJyb256ZQ== 19461
IGxvbA== 19462
RGVmYXVsdA== 19463
IGNvbW1vZGl0eQ== 19464
cmVkaXRlZA== 19465
IGxpYmVydGFyaWFu 19466
IGZvcmJpZGRlbg== 19467
IGdyYW4= 19468
4Kg= 19469
IGxhZw== 19470
ZW56 19471
ZHJpdmU= 19472
IG1hdGhlbWF0aWNz 19473
IHdpcmVz 19474
IGNyaXRpY2FsbHk= 19475
IGNhcmJvaHlk 19476
IENoYW5jZWxsb3I= 19477
IEVkZGll 19478
IGJhbm5pbmc= 19479
IEZyaQ== 19480
IGNvbXBsaWNhdGlvbnM= 19481
ZXRyaWM= 19482
IEJhbmdsYWRlc2g= 19483
IGJhbmR3aWR0aA== 19484
U3RvcA== 19485
IE9yaWdpbmFsbHk= 19486
IGhhbGZ3YXk= 19487
eW5hc3R5 19488
c2hpbmU= 19489
IHRhbGVz 19490
cml0aWVz 19491
YXZpZXI= 19492
IHNwaW5uaW5n 19493
IFdITw== 19494
IG5laWdoYm91cmhvb2Q= 19495
YmFjaA== 19496
IGNvbW1lcmNl 19497
IFNsZQ== 19498
QlU= 19499
IGVudHJlcHJlbmV1cg== 19500
IHBlY3VsaWFy 19501
IENvbW1lbnRz 19502
ZnJl 19503
MzIw 19504
SUNT 19505
IGltYWdlcnk= 19506
IENhbm9u 19507
IEVsZWN0cm9uaWM= 19508
c2hvcnQ= 19509
KCg= 19510
RGln 19511
IGNvbW1lbQ== 19512
dWNlZA== 19513
IGluY2xpbmVk 19514
IFN1bW1vbg== 19515
IGNsaWZm 19516
IE1lZGl0ZXJyYW5lYW4= 19517
IHBvZXRyeQ== 19518
IHByb3NwZXJpdHk= 19519
IFJlY2U= 19520
IHBpbGxz 19521
bWVtYmVy 19522
IGZpbmFsZQ== 19523
dW5j 19524
IEdpZw== 19525
5L0= 19526
IGxvZA== 19527
IGJhY2t3YXJk 19528
LSs= 19529
IEZvcndhcmQ= 19530
IHRocmk= 19531
c3VyZQ== 19532
IHNvYXA= 19533
IEZY 19534
UkVT 19535
IFNleHVhbA== 19536
b3Vsb3M= 19537
IGZvb2xpc2g= 19538
IHJpZ2h0ZW91cw== 19539
IGNvZmY= 19540
dGVycm9yaXNt 19541
dXN0YWlu 19542
b3Rlcg== 19543
IGFidXNlcw== 19544
bmV4dA== 19545
IGFidXNpdmU= 19546
IHRoZXJlYWZ0ZXI= 19547
IHByb2hpYml0aW9u 19548
IFNVUA== 19549
IGRpcA== 19550
IHJpcHBlZA== 19551
IGluaGVyaXRlZA== 19552
IGJhdHM= 19553
c3RydQ== 19554
R1Q= 19555
IGZsYXdlZA== 19556
cGhhYmV0 19557
IGZvZw== 19558
ZG9vcnM= 19559
IGltYWdpbmc= 19560
IGRpZ2l0cw== 19561
IEh1bmdhcnk= 19562
IGFycm9n 19563
IHRlYWNoaW5ncw== 19564
IHByb3RvY29scw== 19565
IEJhbmtz 19566
4Lg= 19567
cG91bmQ= 19568
IEN1cnQ= 19569
LiIp 19570
Li8= 19571
IGV4ZW1wdGlvbg== 19572
ZW5kaXg= 19573
IE11bGw= 19574
IGltcHJvdmVz 19575
IEdhbWVy 19576
ZGltZW5zaW9uYWw= 19577
SWNvbg== 19578
IE1hcmdhcmV0 19579
U3RhdHVz 19580
ZGF0ZXM= 19581
IGludGVuZHM= 19582
IGRlcGljdA== 19583
IHBhcmtlZA== 19584
Sm9l 19585
IE1hcmluZXM= 19586
Y2hub2xvZ3k= 19587
ISku 19588
IGp1ZGdlZA== 19589
IHdlaWdodHM= 19590
UmF5 19591
IGFwYXJ0bWVudHM= 19592
aGVzdGVy 19593
IHJlaW5mb3JjZQ== 19594
IG9mZmVuZGVy 19595
b2NjdXA= 19596
IHNvcmU= 19597
ZXB0 19598
IFBIUA== 19599
IEJyb3c= 19600
IGF1dGhvcml6YXRpb24= 19601
IFJpc2s= 19602
IERlbGF3YXJl 19603
IFFV 19604
IG5vdGlmaWNhdGlvbnM= 19605
IHN1bmxpZ2h0 19606
IGV4Y2x1ZGU= 19607
ZGF0 19608
IG1lc2g= 19609
IFN1ZGFu 19610
IGJlbG9uZ2Vk 19611
IHN1YndheQ== 19612
IG5vb24= 19613
IEludGVyaW9y 19614
b2xpY3M= 19615
IExha2Vycw== 19616
IGNvZGluZw== 19617
RGlzY2xhaW1lcg== 19618
Q2FsaWY= 19619
T2xk 19620
IGRpc2w= 19621
Pz8/Pz8= 19622
IGNvbmZpcm1z 19623
IHJlY3J1aXRtZW50 19624
IGhvbWljaWRl 19625
Q29uc2lkZXI= 19626
IEplZmZyZXk= 19627
ZnR5 19628
fTs= 19629
IG9iamVjdGlvbg== 19630
ZG9pbmc= 19631
IExlbw== 19632
V2FudA== 19633
IGdsb3c= 19634
IENsYXJrZQ== 19635
IE5vcm1hbg== 19636
IHZlcmlmaWNhdGlvbg== 19637
IHBhY2tldA== 19638
IEZvcm11bGE= 19639
IHBsYWc= 19640
ZXN2aWxsZQ== 19641
IHNob3V0aW5n 19642
IG92 19643
IFJFQw== 19644
IEJ1Yg== 19645
IG5pbnRo 19646
IGVuZXJn 19647
IHZhbGlkaXR5 19648
IHVwcw== 19649
amFjaw== 19650
IG5laWdoYm9yaW5n 19651
IE5lYw== 19652
ZXdvcmtz 19653
IEhhYg== 19654
YXJleg== 19655
IHNwaW5l 19656
IGV2ZW50dWFs 19657
IExlYWRlcnM= 19658
IENhcm4= 19659
IHByb2JhdGlvbg== 19660
IHJvbWFuY2U= 19661
bXNn 19662
IE1lY2hhbmljYWw= 19663
RVJZ 19664
Um9jaw== 19665
IHBhcnRpc2Fu 19666
Tm9kZQ== 19667
YXNzZXRz 19668
bWluZW50 19669
IGZvcmVpZ25lcnM= 19670
IHRlc3RpZnk= 19671
IFVzdWFsbHk= 19672
bG9yZHM= 19673
IEdyZW4= 19674
IFBvd2VsbA== 19675
QklM 19676
IHNy 19677
IGFkZGljdA== 19678
IHNoZWxscw== 19679
IHNpZ2g= 19680
IFlhbGU= 19681
dGVybml0eQ== 19682
IDc1MA== 19683
RVU= 19684
IFJpZmxl 19685
IHBhdHJvbg== 19686
ZW1h 19687
IEJhbm5vbg== 19688
YW5pdHk= 19689
IHRyb3BpY2Fs 19690
IFZJSQ== 19691
Y3Jvc3M= 19692
RXZlcnl0aGluZw== 19693
IElTTw== 19694
IGh1bWJsZQ== 19695
YXNzaW5n 19696
IEZJRw== 19697
IHVwZGF0aW5n 19698
eXNvbg== 19699
IGNhbGNpdW0= 19700
IGNvbXBldGVudA== 19701
IHN0ZWVyaW5n 19702
UHJvdA== 19703
IFNZ 19704
IEZpbmFscw== 19705
IFJ1Zw== 19706
MTU5 19707
MTM3 19708
IEdvbGY= 19709
IDEyNg== 19710
IGFjY29tbW9kYXRpb24= 19711
IEh1Z2hlcw== 19712
IGFlc3RoZXRpYw== 19713
YXJ0aXNhbg== 19714
IFR3aWxpZ2h0 19715
IHByaW5jZQ== 19716
IEFncmljdWx0dXJl 19717
IERpc2Nv 19718
IHByZWNlZGVudA== 19719
IHR5cGluZw== 19720
YXV0aG9yaXplZA== 19721
T3B0aW9u 19722
IEF1Yg== 19723
bGlzaGVz 19724
YWNodA== 19725
bWFn 19726
UGV0ZXI= 19727
IFVGTw== 19728
bW9udG9u 19729
IExpdGg= 19730
IGFyb20= 19731
IHNlY3VyaW5n 19732
IGNvbmZpbmVk 19733
cHJpdmF0ZQ== 19734
IHN3b3Jkcw== 19735
IG1hcmtlcnM= 19736
IG1ldGFib2xpYw== 19737
c2VsZWN0 19738
IEN1cnNl 19739
IE90 19740
Z3Jlc3NpdmU= 19741
IGluY3VtYg== 19742
IFNhZ2E= 19743
IHByaWNlZA== 19744
IGNsZWFyYW5jZQ== 19745
Q29udGVudA== 19746
IGRyaWxsaW5n 19747
IG5vdGljZXM= 19748
IGJvdXJnZW9pcw== 19749
IHZlc3Q= 19750
IGNvb2tpZQ== 19751
IEd1YXJkaWFucw== 19752
cnlz 19753
aW55bA== 19754
IDEyNA== 19755
IHBsYXVzaWJsZQ== 19756
b25naA== 19757
IE9kaW4= 19758
IGNvbmNlcHRpb24= 19759
IFl1aw== 19760
IEJhZ2hkYWQ= 19761
IEZsYWc= 19762
QXVzdHJhbA== 19763
IElCTQ== 19764
IGludGVybmF0aW9uYWxseQ== 19765
IFdpa2lMZWFrcw== 19766
SUVE 19767
IGN5bg== 19768
IGNob29zZXM= 19769
IFBpbGw= 19770
IGNvbWJpbmluZw== 19771
IHJhZGk= 19772
IE1vaGFtbWVk 19773
ZGVmZW5zZQ== 19774
YXRjaGluZw== 19775
U3ViamVjdA== 19776
aWNpZW5jeQ== 19777
RnJhbWU= 19778
IHsi 19779
IGNoZXNz 19780
IHRpbWVy 19781
MTkw 19782
IHRpbg== 19783
IG9yZGluYW5jZQ== 19784
ZW1ldGVyeQ== 19785
IGFjY3VzaW5n 19786
IG5vdGljZWFibGU= 19787
IGNlbnRyZXM= 19788
IGxpZA== 19789
IE1pbGxz 19790
aW1ndXI= 19791
IHpvb20= 19792
ZXJnaWM= 19793
IGNvbXByZXNzaW9u 19794
cHJpbQ== 19795
ZmluZA== 19796
IHN1cmc= 19797
IHBhbmQ= 19798
IEtlZQ== 19799
IENoYWQ= 19800
Y2VsbGVuY2U= 19801
b3lsZQ== 19802
IHNvY2lhbGlzbQ== 19803
IFRyYXZpcw== 19804
IE1Ieg== 19805
IGd1aWxk 19806
QUxMWQ== 19807
IFN1YnNjcmliZQ== 19808
IFJlbGF0ZWQ= 19809
IG9jY3VycmVuY2U= 19810
aXRjaGluZw== 19811
IGZpY3Rpb25hbA== 19812
IGNydXNo 19813
IEVB 19814
Y29k 19815
bWl4 19816
IFRyaXBsZQ== 19817
IHJldHJpZXZl 19818
IHN0aW11bHVz 19819
IHBzeWNoaWF0 19820
IERvb3I= 19821
IGhvbW9zZXh1YWxpdHk= 19822
IGVsZW1lbnRhcnk= 19823
IGNlbGx1bGFy 19824
aWRpYW4= 19825
IExhdW4= 19826
IGludHJpZ3Vpbmc= 19827
IGZvYW0= 19828
IEJhc3M= 19829
aWRp 19830
aXRzdQ== 19831
IGFzc3VyZQ== 19832
IGNvbmdyYXQ= 19833
IGJ1c2luZXNzbWFu 19834
IEJvb3N0 19835
Y2xvc2U= 19836
IGxpZWQ= 19837
IHNjaWVuY2Vz 19838
IE9tZWdh 19839
IEdyYXBoaWNz 19840
IDw9 19841
c3Bva2Vu 19842
IGNvbm5lY3Rpdml0eQ== 19843
U2F0dXJkYXk= 19844
IEF2ZW5nZXJz 19845
IHRvZ2dsZQ== 19846
IGFua2xl 19847
IG5hdGlvbmFsaXN0 19848
bW9kZWw= 19849
IFBvb2w= 19850
b3Bob2JpYQ== 19851
VmFy 19852
IE1vbnM= 19853
YXRvcmllcw== 19854
IGFnZ3Jlc3NpdmVseQ== 19855
Q2xlYXI= 19856
Rm9yZ2U= 19857
YWN0ZXJz 19858
IGhlZGdl 19859
IHBpcGVz 19860
IGJsdW50 19861
IHNx 19862
IHJlbW90ZWx5 19863
V2Vk 19864
YXNlcnM= 19865
IHJlZnJpZ2Vy 19866
IHRpbGVz 19867
IHJlc2N1ZWQ= 19868
IGNvbXByaXNlZA== 19869
aW5za3k= 19870
IG1hbmlm 19871
YXZhbmF1Z2g= 19872
IHByb2xpZmVy 19873
IGFsaWduZWQ= 19874
eG1s 19875
IHRyaXY= 19876
IGNvb3JkaW5hdGlvbg== 19877
IFBFUg== 19878
IFF1b3Rl 19879
MTM0 19880
YmY= 19881
IFNhdw== 19882
IHRlcm1pbmF0aW9u 19883
IDE5MA== 19884
IGFkZGl0aW9ucw== 19885
IHRyaW8= 19886
IHByb2plY3Rpb25z 19887
IHBvc2l0aXZlbHk= 19888
IGluY2x1c2l2ZQ== 19889
IG1lbWJy 19890
MTk5MA== 19891
b2xkZXI= 19892
IHByYWN0aWNlZA== 19893
aW5rbGU= 19894
QXJjaA== 19895
IHN0YXJ0ZXJz 19896
YXJpdXM= 19897
IGludGVybWVkaWF0ZQ== 19898
IEJlbmVm 19899
IEtpbGxlcg== 19900
IGludGVydmVudGlvbnM= 19901
IEtpbA== 19902
IEZseWluZw== 19903
SW52 19904
IHByZW1hdHVyZQ== 19905
IHBzeWNoaWF0cmlj 19906
IGluZGll 19907
IGNvbGxhcg== 19908
IFJhaW5ib3c= 19909
YWZp 19910
IGRpc3J1cHRpb24= 19911
IEZPWA== 19912
Y2FzdGluZw== 19913
IG1pc2RlbQ== 19914
Y3Jv 19915
IHdpcGU= 19916
YXJkb24= 19917
IGJhc3Q= 19918
IFRvbW15 19919
IFJlcHJlc2VudGF0aXZl 19920
IGJlbGx5 19921
IFBP 19922
IEJyZWl0YmFydA== 19923
MTMy 19924
IG1lc3NhZ2luZw== 19925
U2hvdWxk 19926
UmVmZXJlbmNlcw== 19927
IEdSRQ== 19928
aXN0aWNhbA== 19929
TFA= 19930
IENhdg== 19931
IENyYXp5 19932
IGludHVpdGl2ZQ== 19933
a2VlcGluZw== 19934
IE1vc3M= 19935
IGRpc2NvbnRpbg== 19936
IE1vZHVsZQ== 19937
IHVucmVsYXRlZA== 19938
IFByYWN0aWNl 19939
IFRyYW5zcG9ydA== 19940
IHN0YXRpc3RpY2FsbHk= 19941
b3Jucw== 19942
IHNpemVk 19943
cHU= 19944
IGNhZg== 19945
IFdvcmxkcw== 19946
IFJvZGdlcnM= 19947
IEx1bg== 19948
IENvbWlj 19949
bGl2aW5n 19950
IGNhcmVk 19951
IGNsaW1iZWQ= 19952
KXs= 19953
IGNvbnNpc3RlZA== 19954
IG1lZGlldmFs 19955
Zm9saw== 19956
IGhhY2tlZA== 19957
IGRpcmU= 19958
IEhlcm1pb25l 19959
IHRlbmRlZA== 19960
Y2VhbnM= 19961
RGFuaWVs 19962
d2VudA== 19963
IGxlZ2lzbGF0b3Jz 19964
IHJlZGVz 19965
Z2FtZXM= 19966
IGdu 19967
YW1pbGlhcg== 19968
ICsr 19969
Z2d5 19970
dGhyZWF0 19971
IG1hZ25ldA== 19972
IHBlcmNlaXZl 19973
IHppcA== 19974
IGluZGljdG1lbnQ= 19975
IGNyaXRpcXVl 19976
Z2FyZA== 19977
IFNhZmU= 19978
IENyZWFt 19979
IGFkdmVudA== 19980
b2Jh 19981
IHZvd2Vk 19982
b3VzYW5kcw== 19983
IHNraQ== 19984
IGFib3J0aW9ucw== 19985
dWFydA== 19986
IHN0dW5uZWQ= 19987
IGFkdmFuY2luZw== 19988
IGxhY2tlZA== 19989
IFwi 19990
IHNjaGl6b3BocmVu 19991
IGVsZWdhbnQ= 19992
IGNvbmZlcmVuY2Vz 19993
IGNhbmNlbGVk 19994
IEh1ZHNvbg== 19995
IEhvcGVmdWxseQ== 19996
IHRydW1w 19997
IGZyZXF1ZW5jaWVz 19998
IG1ldGVvcg== 19999
IEp1bmlvcg== 20000
IEZsZWV0 20001
IE1hbGNvbG0= 20002
IFRvb2xz 20003
IC4uLi4uLi4u 20004
IGhvYmJ5 20005
IEV1cm9wZWFucw== 20006
IDE1MDA= 20007
IEludG8= 20008
IHN3YXk= 20009
IEFwcHJv 20010
IENvbXBs 20011
Q29tbXVuaXR5 20012
IHRpZGU= 20013
IFN1bW1pdA== 20014
5Ls= 20015
IGludGVydmFscw== 20016
IEV0aGVy 20017
IGhhYml0YXQ= 20018
IFN0ZXZlbnM= 20019
bGlzaGluZw== 20020
IERvbWFpbg== 20021
IHRyaWdnZXJz 20022
IGNoYXNpbmc= 20023
IGNoYXJt 20024
IEZsb3dlcg== 20025
aXRvcmVk 20026
IGJsZXNzaW5n 20027
IHRleHR1cmVz 20028
Rml2ZQ== 20029
IGxpcXVvcg== 20030
UlA= 20031
RklO 20032
IDE5NjI= 20033
Q0FS 20034
VW5rbm93bg== 20035
IHJlc2ls 20036
IExpbHk= 20037
IGFidW5kYW5jZQ== 20038
IHByZWRpY3RhYmxl 20039
cmFy 20040
IGJ1bGxzaGl0 20041
bGVlbg== 20042
Y2hldA== 20043
TW9y 20044
TXVjaA== 20045
5Lk= 20046
IGVtcGhhc2l6ZWQ= 20047
IGNydXN0 20048
IHByaW1pdGl2ZQ== 20049
IGVuam95YWJsZQ== 20050
IFBpY3R1cmVz 20051
IHRlYW1tYXRl 20052
cGxlcg== 20053
IFRvbA== 20054
IEthbmU= 20055
IHN1bW1vbmVk 20056
dGh5 20057
cmFtYQ== 20058
IEhvbmRh 20059
IHJlYWxpemluZw== 20060
IHF1aWNrZXI= 20061
IGNvbmNlbnRyYXRl 20062
Y2xlYXI= 20063
IDIxMA== 20064
IEVyZG9nYW4= 20065
YXJpcw== 20066
IHJlc3BvbmRz 20067
IEJJ 20068
IGVsaWdpYmlsaXR5 20069
IHB1c2hlcw== 20070
IElkYWhv 20071
IGFnZ3Jhdg== 20072
IHJ1aW5z 20073
dXJhdGlvbnM= 20074
IGJhbnM= 20075
IGFuYXQ= 20076
c2hhcmU= 20077
IGdyaW5k 20078
aGlu 20079
dW1lbg== 20080
IHV0aWxpdGllcw== 20081
IFlhbmtlZXM= 20082
IGRhdGFiYXNlcw== 20083
IERE 20084
IGRpc3BsYWNlZA== 20085
IGRlcGVuZGVuY2llcw== 20086
IHN0aW11bGF0aW9u 20087
aHVu 20088
aG91c2Vz 20089
IFByZXR0eQ== 20090
IFJhdmVucw== 20091
IFRPREFZ 20092
IGFzc29jaWF0ZXM= 20093
IHRoZXJhcGU= 20094
Y2xlZA== 20095
IGRlZXI= 20096
IHJlcGFpcnM= 20097
cmVudGljZQ== 20098
IHJlY2VwdG9ycw== 20099
IHJlbWVk 20100
IENl 20101
IG1hcnJpYWdlcw== 20102
IGJhbGxvdHM= 20103
IFNvbGRpZXI= 20104
IGhpbGFyaW91cw== 20105
b3Bs 20106
MTM4 20107
IGluaGVyZW50bHk= 20108
IGlnbm9yYW50 20109
IGJvdW5jZQ== 20110
IEVhc3Rlcg== 20111
UkVMQVRFRA== 20112
IEN1cnJlbmN5 20113
RVY= 20114
44Oe 20115
IExlYWQ= 20116
IGRlY2Vhc2Vk 20117
QnJpZW4= 20118
IE11c2s= 20119
SlM= 20120
IG1lcmdl 20121
aGVhcnRlZA== 20122
Y3JlYXQ= 20123
bWl0dA== 20124
bXVuZA== 20125
IOKAiw== 20126
IEJhZw== 20127
IHByb2plY3Rpb24= 20128
IGphdmE= 20129
IFN0YW5kYXJkcw== 20130
IExlb25hcmQ= 20131
IGNvY29udXQ= 20132
IFBvcHVsYXRpb24= 20133
IHRyYWplY3Q= 20134
IGltcGx5 20135
IGN1cmlvc2l0eQ== 20136
IERC 20137
IEZyZXNo 20138
IFBvcg== 20139
IGhlYXZpZXI= 20140
bmV5cw== 20141
Z29tZXJ5 20142
IGRlc2VydmVk 20143
IHBocmFzZXM= 20144
IEdD 20145
IHllYXN0 20146
ZGVzYw== 20147
RGVhdGg= 20148
IHJlYm9vdA== 20149
IG1ldGFkYXRh 20150
SUNBTA== 20151
IHJlcGF5 20152
IEluZGVwZW5kZW5jZQ== 20153
IHN1YnVyYmFu 20154
aWNhbHM= 20155
IGF0b3A= 20156
IGFsbG9jYXRpb24= 20157
Z2VuZXJhdGlvbg== 20158
IEdyYW0= 20159
IG1vaXN0dXJl 20160
IHBpbmU= 20161
IExpYmVyYWxz 20162
IGFpZGVz 20163
IHVuZGVyZXN0 20164
IEJlcnJ5 20165
IGNlcmVtb24= 20166
Mzcw 20167
YXN0cm91cw== 20168
IFBpcmF0ZXM= 20169
IHRlbnNl 20170
IEluZHVzdHJpZXM= 20171
IEFwcGVhbHM= 20172
IE5lYXI= 20173
IOijj+c= 20174
IGxvdmVycw== 20175
IENBUA== 20176
IENyYXc= 20177
IGdpYW50cw== 20178
IGVmZmljYWN5 20179
RWxlbWVudA== 20180
IEJlaGF2aW9y 20181
IFRveW90YQ== 20182
IGludGVzdA== 20183
UHJpdg== 20184
QUk= 20185
IG1hbmV1dmVy 20186
IHBlcmZlY3Rpb24= 20187
IGJhbmc= 20188
cGFwZXI= 20189
cmlsbA== 20190
R2Vvcmdl 20191
Ym9yZGVy 20192
aW50ZXJz 20193
IFNldGg= 20194
IGNsdWVz 20195
IExldmk= 20196
IFJldmVudWU= 20197
MTQ3 20198
IHZhcG9y 20199
IGZvcnR1bmF0ZQ== 20200
IHRocmVhdGVucw== 20201
IHZldA== 20202
IGRlcGVuZGVuY3k= 20203
ZXJzZWQ= 20204
YXJ0aWNsZQ== 20205
IEJsaXp6YXJk 20206
IGNobG9y 20207
IG1pbnVz 20208
IEJpbGxz 20209
IGNyeXB0b2N1cnJlbmN5 20210
IG1ldGFib2xpc20= 20211
dGVyaW5n 20212
IHBlc3RpYw== 20213
c3RlcHM= 20214
IFRyZWFzdXJl 20215
cmFjdGVk 20216
IENvbnN0YW50 20217
IHRlbXA= 20218
MTM5 20219
IERldGVjdGl2ZQ== 20220
dXJhbGx5 20221
IHJlY292ZXJpbmc= 20222
IGNvcnRleA== 20223
IDE0NA== 20224
Y2xvc2Vk 20225
IHByZWp1ZGljZQ== 20226
YXVudGVk 20227
IHN0b3Jtcw== 20228
IE5PVw== 20229
IG1hY2hpbmVyeQ== 20230
QWRkcmVzcw== 20231
IGNvbXBlbGxlZA== 20232
Mjcw 20233
IGRlc3BhaXI= 20234
YmFuZQ== 20235
IHZlZ2V0YWJsZQ== 20236
IGJlZHM= 20237
TGVhcm4= 20238
IGNvbG9yZnVs 20239
IHNwaWtl 20240
IG1hcmdpbnM= 20241
IHN5bXBhdGh5 20242
IHdvcmtzaG9w 20243
IENCQw== 20244
U2F0 20245
IGJ1cm5z 20246
IEdlbmRlcg== 20247
IDEyOQ== 20248
IENhYmxl 20249
IGRlYnRz 20250
IFRoZXJlc2E= 20251
IHJlZmxlY3Rpbmc= 20252
IGFpcnN0 20253
IHJpbQ== 20254
cmFtaWQ= 20255
IHdlYWtuZXNzZXM= 20256
V3JpdA== 20257
b2dnbGU= 20258
dGk= 20259
IENoYXJnZQ== 20260
IHdlaWdoZWQ= 20261
ICgu 20262
IGxhdWdodGVy 20263
IHJvdXRlcg== 20264
IERlbW9jcmFjeQ== 20265
RGVhcg== 20266
IGhhc2h0 20267
IGR5 20268
IGhpbnRz 20269
cnVubmluZw== 20270
IGZpbmlzaGVz 20271
YXJ1cw== 20272
TWFzcw== 20273
cmVzdWx0 20274
YXNjdXM= 20275
IHZpbnRhZ2U= 20276
IGNvbnF1 20277
IHdpbGRseQ== 20278
YWNpc3Q= 20279
IGxpbmd1 20280
IHByb3RhZ29uaXN0 20281
c3Ryb20= 20282
dGVlbnRo 20283
IFNvbG8= 20284
bWFj 20285
ZmlsbGVk 20286
IHJlbm93bg== 20287
aXRpdmVz 20288
IG1vdGl2ZQ== 20289
IEFudGFy 20290
IE1hbm4= 20291
IEFkanVzdA== 20292
IHJvY2tldHM= 20293
IHRyb3VibGluZw== 20294
ZWk= 20295
IG9yZ2FuaXNtcw== 20296
YXNzaXM= 20297
Q2hyaXN0aWFu 20298
IDE0NQ== 20299
IEhhc3M= 20300
IHN3YWxs 20301
IHdheA== 20302
IFN1cnZpdmFs 20303
VlM= 20304
IE11cmQ= 20305
dmQ= 20306
c3RhbmRhcmQ= 20307
IGRyYWdvbnM= 20308
IGFjY2VsZXJhdGlvbg== 20309
cmF0aW9uYWw= 20310
ZmluYWw= 20311
IHBhaXJlZA== 20312
IEV0aGVyZXVt 20313
IGludGVyZmFjZXM= 20314
IHJlc2VudA== 20315
IGFydGlmYWN0cw== 20316
xas= 20317
YXJlbA== 20318
IGNvbXBldGl0b3I= 20319
IE5pY2hvbGFz 20320
IFN1cmZhY2U= 20321
Y3Bw 20322
IFRvdA== 20323
IGVjb25vbWljYWxseQ== 20324
IG9yZ2FuaXNlZA== 20325
IGVuZm9yY2Vk 20326
aW5obw== 20327
IHZhcmlldGllcw== 20328
IGFiZG9t 20329
IEJhaWxleQ== 20330
aWRhdg== 20331
IFNhbHY= 20332
cGFpZA== 20333
IGFsdGl0dWRl 20334
ZXNzZXJ0 20335
IEd1dGVuYmVyZw== 20336
YXJlYQ== 20337
b3BvdWxvcw== 20338
IHByb2Zlc3NvcnM= 20339
aWdncw== 20340
IEZhdGU= 20341
aGV5 20342
IDMwMDA= 20343
RGlzdA== 20344
IHR3aW5z 20345
Y2lsbA== 20346
IE1hcHM= 20347
IHRyYXBz 20348
IHdlZWQ= 20349
IEtpc3M= 20350
IHlvZ2E= 20351
IHJlY2lwaWVudHM= 20352
IFdlc3RtaW5zdGVy 20353
IHBvb2xz 20354
IFdhbG1hcnQ= 20355
MTg4 20356
IFNjaG9vbHM= 20357
YXR0YWNr 20358
IEFSTQ== 20359
cGFyYWdyYXBo 20360
V2FybmluZw== 20361
amw= 20362
IHNlbGZpc2g= 20363
YW5jaGV6 20364
IEhlaWdodHM= 20365
RnJl 20366
IFNvcGg= 20367
IC0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0t 20368
dG1s 20369
MzMz 20370
IHJhaWRz 20371
IHNhdGVsbGl0ZXM= 20372
S0VZ 20373
IGxhc3Rz 20374
0YI= 20375
SW5z 20376
IERhbWU= 20377
IHVucHJlZGljdA== 20378
Ly8v 20379
Z2hhaQ== 20380
IGFydGlsbGVyeQ== 20381
IGNydWlzZQ== 20382
IGdlbA== 20383
IENhYmluZXQ= 20384
IGJsb3dz 20385
IEVzcA== 20386
IHByb3hpbWl0eQ== 20387
b3RoZQ== 20388
IFNraWxscw== 20389
IFVwcGVy 20390
b2Jv 20391
IE5EUA== 20392
IGVuam95cw== 20393
IHJlcGVhdGluZw== 20394
IENvbnN0cnVjdGlvbg== 20395
IFF1ZXN0aW9ucw== 20396
SGlsbGFyeQ== 20397
IHVpbnQ= 20398
IHByb2Nlc3NvcnM= 20399
IEdpYnNvbg== 20400
IE11bHRpcGxl 20401
cWE= 20402
IEJvbQ== 20403
IE1pbGVz 20404
dmVudGlvbmFs 20405
IGh1cnRz 20406
c2tpbg== 20407
IEFJRFM= 20408
IGFkdmlzZXJz 20409
IFJvb3Q= 20410
IG1ldGhvZG9sb2d5 20411
IERhbGU= 20412
IGRldG9u 20413
IEtub3dsZWRnZQ== 20414
c2VxdWVudGx5 20415
IDEyMQ== 20416
IGNvbm5lY3Rz 20417
Q3k= 20418
IERhbmdlcg== 20419
IGNvbnRyaWJ1dG9ycw== 20420
IEJlbnQ= 20421
IGJyYXNz 20422
IEd1bnM= 20423
aW50bw== 20424
IEZvcnR1bmU= 20425
IGJyb2tlcg== 20426
YmFsYW5jZQ== 20427
IGxlbmd0aHM= 20428
IHZpYw== 20429
IGF2ZXJhZ2luZw== 20430
IGFwcHJvcHJpYXRlbHk= 20431
IENhbWVyYQ== 20432
IHNhbmR3aWNo 20433
IENEQw== 20434
IGNvb3JkaW5hdGU= 20435
IG5hdmln 20436
IGdvb2RuZXNz 20437
bGFpbQ== 20438
IGJyYWtl 20439
IGV4dHJlbWlzdA== 20440
IFdha2U= 20441
IE1lbmQ= 20442
IFRpbnk= 20443
IENPTA== 20444
IFJG 20445
IER1YWw= 20446
IFdpbmU= 20447
Q2FzZQ== 20448
IHJlZmluZWQ= 20449
IGxhbXA= 20450
TGVhZA== 20451
IGJhcHQ= 20452
IENhcmI= 20453
IFNhZGQ= 20454
IE1pbm5lYXBvbGlz 20455
UERG 20456
RWFybHk= 20457
IEhpZGRlbg== 20458
SXRz 20459
IFRJTUU= 20460
IHBhcA== 20461
IGNvbW1pc3Npb25lZA== 20462
IEZldw== 20463
IENvbHRz 20464
IEJyZW4= 20465
IGJvdGhlcmVk 20466
IGxpa2V3aXNl 20467
RXhwZXI= 20468
IFNjaHc= 20469
Y3J5 20470
bm4= 20471
IE1pdGNo 20472
aW1vbg== 20473
TUc= 20474
Ym0= 20475
VU1Q 20476
cmF5cw== 20477
IHJlZ2lzdHJ5 20478
IDI3MA== 20479
YWNoaW5l 20480
cmVsbGE= 20481
YW50aW5n 20482
MDAwMDA= 20483
IHJ1aW5lZA== 20484
c3BvdA== 20485
IHRh 20486
IG1heGltaXpl 20487
IGluY29udmVu 20488
RGVhZA== 20489
SHVtYW4= 20490
RW5hYmxlZA== 20491
IE1hcmll 20492
IGNoaWxs 20493
IFBhcmFkaXNl 20494
IHN0YXJyaW5n 20495
IExhdGlubw== 20496
IFByb3RvY29s 20497
IEVWRVI= 20498
IHN1cHBsaWVycw== 20499
bWVzc2FnZQ== 20500
IEJyb2Nr 20501
IHNlcnVt 20502
4paI4paI4paI4paI 20503
IGVuY29tcA== 20504
IGFtYml0aW9u 20505
dWVzZQ== 20506
IGFycm93cw== 20507
QW5kcmV3 20508
IGFudGVubmE= 20509
IDE5NjE= 20510
IEJhcms= 20511
IGJvb2w= 20512
44Kq 20513
IFN0b3JhZ2U= 20514
IHJhaWx3YXk= 20515
IHRvdWdoZXI= 20516
IENhZA== 20517
IHdhc2hpbmc= 20518
UHk= 20519
J10= 20520
ZW1iZWQ= 20521
IE1lbXBoaXM= 20522
YWNrbGU= 20523
IGZhbW91c2x5 20524
IEZvcnR1bmF0ZWx5 20525
b3ZpZXM= 20526
IG1pbmRzZXQ= 20527
IHNuZWFr 20528
IERo 20529
UkFX 20530
IFNpbXBzb24= 20531
IGxpdmVzdA== 20532
IGxhbmRtYXJr 20533
IGNlbWVudA== 20534
TG93 20535
IHRocmlsbGVk 20536
IENvdXJzZQ== 20537
aW5lbA== 20538
IGNodWNr 20539
aWRhdGU= 20540
Z2xvYmFs 20541
IHdoaXQ= 20542
IO+/vQ== 20543
YWRheXM= 20544
c2tp 20545
IFNW 20546
IHZpcnVzZXM= 20547
MzA2 20548
IFJlc3BvbnM= 20549
IHRoZWF0ZXJz 20550
IEJyYW5jaA== 20551
IEdlbmV2YQ== 20552
IE1L 20553
IHVuYmVsaWV2 20554
IGNvbW11bmlzdA== 20555
T3JpZ2luYWw= 20556
IFJlY2VpdmVk 20557
IFRyYW5zZmVy 20558
IEFyZw== 20559
SW5wdXQ= 20560
IFN0cmF0ZWd5 20561
IHBhbGFjZQ== 20562
dGhlbmluZw== 20563
RHJp 20564
IHNlbnRlbmNpbmc= 20565
dW1ibmFpbA== 20566
IHBpbnM= 20567
cmVjeQ== 20568
IHNpYmxpbmdz 20569
R2V0dGluZw== 20570
IEJV 20571
IE5vcnRod2VzdA== 20572
IHByb2xvbmdlZA== 20573
IFNha3VyYQ== 20574
Q29tYg== 20575
IEJvdXI= 20576
IGluYWRlcXVhdGU= 20577
IEthc2g= 20578
IHVzZXJuYW1l 20579
IEltcHJvdmU= 20580
IGJhdHRsaW5n 20581
IE1BQw== 20582
IGN1cnJpY3VsdW0= 20583
IHNvZGE= 20584
IENhbm5vbg== 20585
IHNlbnNpYmxl 20586
c3BvbnM= 20587
RGVjZW1iZXI= 20588
IHdpY2tlZA== 20589
IFBlbmd1 20590
IGRpY3RhdG9ycw== 20591
IEhlYXJ0cw== 20592
b2d5bg== 20593
IHNpbWlsYXJpdGllcw== 20594
IFN0YXRz 20595
IGhvbGxvdw== 20596
aXRhdGlvbnM= 20597
Ijpb 20598
IGhvdmVy 20599
IExpc3Rlbg== 20600
c2No 20601
U3VuZA== 20602
IGNhZA== 20603
IFBhcmtz 20604
IGx1cg== 20605
IGh5cGU= 20606
IExlbQ== 20607
TkFNRQ== 20608
aXN1cmU= 20609
RnJpZGF5 20610
IHNob290cw== 20611
IGNsb3Nlcw== 20612
IGRi 20613
IFJpZGdl 20614
IERpZmZlcmVudA== 20615
IHJlcGxpZXM= 20616
IEJyb2Fkd2F5 20617
b3BlcnM= 20618
IGludG9sZXI= 20619
IFpldXM= 20620
YWtlc3Bl 20621
IHByb3ByaWV0YXJ5 20622
IHJlcXVlc3Rpbmc= 20623
IGNvbnRyb2xsZXJz 20624
IE1JTg== 20625
aW1lZGlh 20626
YmVjY2E= 20627
IGV4cGFucw== 20628
IG9pbHM= 20629
Qm90 20630
IENoYW5k 20631
IHByaW50ZXI= 20632
IHRvcHBlZA== 20633
IFBPTA== 20634
IEVhcmxpZXI= 20635
U29jaWFs 20636
YXZpbg== 20637
IGRlY3JlYXNlcw== 20638
IFNlYg== 20639
IHNwZWNpZmljYXRpb25z 20640
IEJsYXN0 20641
IEt1cnQ= 20642
IGZyZWVs 20643
QnJvd24= 20644
IGRpbGln 20645
cm9l 20646
IFByb2JsZW0= 20647
IFF1YWQ= 20648
IGRlY2VudHJhbA== 20649
IFZlY3Rvcg== 20650
YW51dA== 20651
IHBsdWdpbnM= 20652
IEdyZWdvcnk= 20653
IGZ1Y2tlZA== 20654
ZWxpbmVz 20655
IEFtYmFzc2Fkb3I= 20656
dGFrZQ== 20657
IGNsZWFucw== 20658
b25neWFuZw== 20659
QW5vbnltb3Vz 20660
c3Rybw== 20661
In0= 20662
YWxpbmU= 20663
IE9kZA== 20664
IEV1Zw== 20665
MjE2 20666
IGJvaWw= 20667
IFBvd2Vycw== 20668
IG51cnNlcw== 20669
T2J2aW91c2x5 20670
IFRlY2huaWNhbA== 20671
IGV4Y2VlZGVk 20672
T1JT 20673
IGV4dHJlbWlzdHM= 20674
IHRyYWNlcw== 20675
ZXhwbA== 20676
IGNvbXI= 20677
IFNhY2g= 20678
KS8= 20679
IG1hc2tz 20680
IHNjaQ== 20681
Qm9u 20682
IHJlZ3Jlc3Npb24= 20683
d2VnaWFu 20684
IGFkdmlzb3I= 20685
aXR1cmVz 20686
IFZv 20687
ZXhhbXBsZQ== 20688
IEluc3RydWN0 20689
IHNpZWdl 20690
IHJlZHVjdGlvbnM= 20691
cHRy 20692
IHN0YXR1dG9yeQ== 20693
IHJlbW92ZXM= 20694
IHB1Y2s= 20695
cmVkaXRz 20696
IGJlZQ== 20697
IHNhbGFk 20698
IHByb21vdGlvbnM= 20699
IEpvc2h1YQ== 20700
d2l0aHN0YW5kaW5n 20701
RVRI 20702
IENoYQ== 20703
aW11cw== 20704
IGV4cGVuZGl0dXJl 20705
YXVudGluZw== 20706
IGRlbGlnaHRlZA== 20707
IDE1NQ== 20708
YmVo 20709
IGNhcnBldA== 20710
IFNwYXJ0 20711
IGp1bmdsZQ== 20712
bGlzdHM= 20713
IGJ1bGx5aW5n 20714
IE5vYmVs 20715
IEdsZW4= 20716
IHJlZmVyZW5jZWQ= 20717
IGludHJvZHVjZXM= 20718
c2Vpbg== 20719
IGNob3BwZWQ= 20720
Z2xhc3M= 20721
IFdyZXN0 20722
IG5ldXRyYWxpdHk= 20723
IOKZ 20724
IGludmVzdGlnYXRvcg== 20725
IHNoZWx2ZXM= 20726
IHVuY29uc3RpdHV0aW9uYWw= 20727
IHJlcHJvZHVjdGlvbg== 20728
IG1lcmNoYW50 20729
bWlh 20730
IG1ldHJpY3M= 20731
IGV4cGxvc2l2ZXM= 20732
IFNvbmlh 20733
IGJvZGlseQ== 20734
IHRoaWNrbmVzcw== 20735
IHByZWRvbWluYW50bHk= 20736
IEFiaWxpdHk= 20737
IG1vbml0b3JlZA== 20738
SUNI 20739
IF0u 20740
IE1hcnRpbmV6 20741
IHZpc2liaWxpdHk= 20742
IHF1ZXJpZXM= 20743
IGdlbm9jaWRl 20744
IFdhcmZhcmU= 20745
UXVlcnk= 20746
IHN0dWRpb3M= 20747
IGVtYnJ5 20748
IGNvcnJpZG9y 20749
IGNsZWFuZWQ= 20750
Y29tcGxldGU= 20751
IE1I 20752
IGVucm9sbG1lbnQ= 20753
SU5HUw== 20754
IGltcGFjdGVk 20755
IGRpc2FzdHJvdXM= 20756
IFl1bg== 20757
IENsYWlyZQ== 20758
IEJhc2ljYWxseQ== 20759
eXQ= 20760
dXN0ZXJpdHk= 20761
IGluZGlyZWN0bHk= 20762
d2lr 20763
IGRvZA== 20764
IENhcnI= 20765
IGFtcA== 20766
IHByb2hpYml0 20767
IEluaXRpYWw= 20768
IFJk 20769
aWpp 20770
IGVkdWNhdGU= 20771
Y29ybg== 20772
aW90dA== 20773
IEJlYXV0eQ== 20774
IGRldGVjdGl2ZQ== 20775
IENvbm4= 20776
c2luY2U= 20777
IHN0YWdnZXI= 20778
IG9iZXNl 20779
IGJyZWU= 20780
b2xvZ2lj 20781
aXNzZQ== 20782
d2Fsa2Vy 20783
IGJsYWRlcw== 20784
IGxhd2Z1bA== 20785
ZnVuYw== 20786
IEJlaGluZA== 20787
IGFwcGV0aXRl 20788
ICgq 20789
IHRlbm5pcw== 20790
IG9mZnNwcmluZw== 20791
IGpldHM= 20792
IHN0cnVjdHVyZWQ= 20793
IGFmb3JlbWVudGlvbmVk 20794
Tm92 20795
IHNjYWxpbmc= 20796
ZmlsbA== 20797
IHN0ZXc= 20798
IGN1cmI= 20799
IFN0ZXBoYW4= 20800
ZWRJbg== 20801
U0Y= 20802
b2JpYw== 20803
6a2U 20804
b3Vn 20805
IE1N 20806
IGdlbmV0aWNhbGx5 20807
b3Bleg== 20808
MTM2 20809
IHVtYg== 20810
YW5jZXJz 20811
IGNvaG9ydA== 20812
IG1lcmNoYW5kaXNl 20813
IGltcG9zaW5n 20814
IExlZ2lzbGF0dXJl 20815
IEFyY2hpdmU= 20816
aXZpYQ== 20817
IE5hdmFs 20818
IG9mZmVuY2Vz 20819
IG1pcmFjbGU= 20820
IHNuYXBwZWQ= 20821
IGZvZXM= 20822
IGV4dGVuc2l2ZWx5 20823
IFJhZg== 20824
IGNhdGVy 20825
ZWRpZW5jZQ== 20826
S2l0 20827
IEJpbg== 20828
IHJlY29tbWVuZHM= 20829
IENpdGllcw== 20830
IHJpZ2lk 20831
IFJFQUQ= 20832
IE5vYmxl 20833
IFRpYW4= 20834
IGNlcnRpZmljYXRlcw== 20835
YW50aXM= 20836
b2lsZXI= 20837
IEJ1ZGRoaXN0 20838
ZGlk 20839
IHN1cnZleWVk 20840
IGRvd253YXJk 20841
IHByaW50cw== 20842
IE1vdGlvbg== 20843
cm9uaWNz 20844
IFNhbnM= 20845
b3NzaWJseQ== 20846
dWN0aW9ucw== 20847
IGNvbG9uaWVz 20848
IERhbmlzaA== 20849
dW5pdA== 20850
IHNwb2ls 20851
IGFkdmlzb3J5 20852
YmVycmllcw== 20853
UGxhbg== 20854
IHNwZWNpZmljYXRpb24= 20855
b3BoZXJz 20856
IFJlc291cmNl 20857
IHNoaXJ0cw== 20858
cHJpc2luZ2x5 20859
Y29tbXVuaWNhdGlvbnM= 20860
IHRyaXZpYWw= 20861
IG1lbnRpb25pbmc= 20862
aXNleHVhbA== 20863
IHN1cHBsZW1lbnRz 20864
IHN1cGVydmlzaW9u 20865
QlA= 20866
dm9y 20867
IHdpdA== 20868
IGNvb2xkb3du 20869
IHBsYWludGlmZg== 20870
IFJldmlld3M= 20871
IFNyaQ== 20872
IE1pbnQ= 20873
IFN1Z2Fy 20874
IGFmdGVyd2FyZA== 20875
IFByaWVzdA== 20876
IEludmVzdG1lbnQ= 20877
b2dlbmU= 20878
IFRha2luZw== 20879
IHN0cmV0Y2hpbmc= 20880
IGluZmxhbW1hdGlvbg== 20881
IFRlaHJhbg== 20882
IGxpbmluZw== 20883
IGZyZWV6aW5n 20884
IEVudGl0eQ== 20885
IGluc3BpcmluZw== 20886
c3BlY2lhbA== 20887
cHJpY2U= 20888
IHN1ZQ== 20889
IFBvcnRlcg== 20890
b3VuZ2U= 20891
RVRB 20892
IERlcmVr 20893
IEx1aXM= 20894
dW8= 20895
eW1waA== 20896
IGV4dGVyaW9y 20897
aWhpbA== 20898
IEFzaGxleQ== 20899
aW5hdG9y 20900
IG51dHJpZW50cw== 20901
IFRocm9uZXM= 20902
IGZpbmFuY2Vz 20903
IEluc3BlY3Q= 20904
IHNwZWNpYWxseQ== 20905
IFJlcXVpcmVk 20906
IFBUUw== 20907
IFZpb2xlbmNl 20908
b2ludGVk 20909
c2hvdHM= 20910
IGV4Y2VycHQ= 20911
Y29vbg== 20912
SU5T 20913
IEdyaQ== 20914
IHJlY29nbmlzZWQ= 20915
V2Vlaw== 20916
WW91bmc= 20917
IHZvbQ== 20918
aXNsZQ== 20919
IEN1cnJ5 20920
IEJ1ZGRo 20921
IG5vdGVib29r 20922
IGR1cmFibGU= 20923
Lz8= 20924
IEdhZA== 20925
IFB1cHA= 20926
IGZvcmdpdmU= 20927
cGFyaw== 20928
IHBlcnNvbmFsaXRpZXM= 20929
YW5hbHlzaXM= 20930
Y2xhbWF0aW9u 20931
IGVsZXZhdG9y 20932
IHdhcmVob3VzZQ== 20933
IFJvbGU= 20934
dW5u 20935
IGlsbHVzdHJhdGlvbg== 20936
IFNjYW4= 20937
IGF0bW9zcGhlcmlj 20938
SW1wb3J0 20939
QU5D 20940
cmljdGVk 20941
ZnU= 20942
MDEw 20943
IGFyY2hl 20944
IHJld2FyZGVk 20945
YWtlc3BlYXJl 20946
IGludGVybmFsbHk= 20947
IFJCSQ== 20948
YWxrZXI= 20949
IGVsZXBoYW50 20950
b3dpdHo= 20951
IFBpenph 20952
IGJpcGFydGlzYW4= 20953
w6lz 20954
IHNsb3dlZA== 20955
IFN0YXJr 20956
IG92ZXJyaWRl 20957
T1VT 20958
IDMyMA== 20959
dW5kcmVkcw== 20960
IERlY2s= 20961
IENlbnN1cw== 20962
YmVl 20963
MTQ2 20964
b3Rvcg== 20965
IGlw 20966
IHVi 20967
b2NhdGlvbnM= 20968
IEJ1dHRvbg== 20969
cmljZQ== 20970
IGNyaXBw 20971
ZmZm 20972
IG9yaWdpbmF0ZWQ= 20973
IG92ZXJ3aGVsbWVk 20974
YXBwYQ== 20975
IGZvcmVtb3N0 20976
4oCR 20977
IExFRw== 20978
cmVsZWFzZQ== 20979
ZWF0dXJlZA== 20980
YXRjaGVz 20981
IHJlcHM= 20982
IGxlbmRpbmc= 20983
IFJlZmVyZW5jZQ== 20984
IENsaWVudA== 20985
MTY1 20986
dmVudGg= 20987
Q29tcGxldGU= 20988
IFBhdHJvbA== 20989
IHN3b3Ju 20990
Y2Ft 20991
IHNodXR0bGU= 20992
IFJhbHBo 20993
IGhvbWV0b3du 20994
LSw= 20995
b25hbA== 20996
IEJQ 20997
5Y8= 20998
IHBlcnN1YWRl 20999
IEFsZXhhbmQ= 21000
IGNvbWJpbmVz 21001
IHZpdmlk 21002
IExhZw== 21003
IGVuY29kaW5n 21004
IHNhbHZhdGlvbg== 21005
d2Vu 21006
IFJlY292ZXJ5 21007
aXlh 21008
VW5pdmVyc2l0eQ== 21009
IEJpZGVu 21010
IGJ1ZGdldHM= 21011
IFRleGFucw== 21012
Zml0cw== 21013
IGhvbm9yZWQ= 21014
IHB5dGhvbg== 21015
VEQ= 21016
IyMj 21017
Y2xvbmU= 21018
IGJsaW5r 21019
IExpcXVpZA== 21020
IHVuZW1wbG95ZWQ= 21021
IGNsYXNoZXM= 21022
IENvdW5zZWw= 21023
IGRpcmVjdGluZw== 21024
IHB1bmN0 21025
IEZhbGNvbnM= 21026
IHNoYXJr 21027
IERhbWFzY3Vz 21028
IGplYW5z 21029
IGVtYmFyaw== 21030
IHNlaXpl 21031
IHVwd2FyZHM= 21032
Mjgw 21033
IEV6 21034
IEFueXRoaW5n 21035
IGV4b3RpYw== 21036
bG93ZXI= 21037
IENyZWF0b3I= 21038
IFVt 21039
IHN1YnVyYnM= 21040
YmVyZ2Vy 21041
IFdlbmQ= 21042
IG1pbnQ= 21043
IFhY 21044
IERybw== 21045
IHN1ZmZlcnM= 21046
IGhlcmI= 21047
dHJlZQ== 21048
IGZyYWdpbGU= 21049
IGZsb29kZWQ= 21050
IEFsY29ob2w= 21051
b2xlYW4= 21052
bnlkZXI= 21053
IEtP 21054
RnJhbQ== 21055
IDEzNg== 21056
IG93ZWQ= 21057
IE1lbGVl 21058
IEhhc2g= 21059
IHdoaXNr 21060
IHN1ZG8= 21061
cnI= 21062
UXVpY2s= 21063
YXBwcm8= 21064
IGlp 21065
IEV4YW1wbGVz 21066
aGVl 21067
IHByb21vdGVz 21068
cGVyYXR1cmU= 21069
a2Fy 21070
IEhvbm9y 21071
IHNvZGl1bQ== 21072
IExpZg== 21073
cm9zc28= 21074
aW50ZW5kZW50 21075
IGNvcnJlc3BvbmRlbnQ= 21076
Rm91bmQ= 21077
c2VjcmV0 21078
IGlkZW50aWZpZXM= 21079
YWduZQ== 21080
IGxvdQ== 21081
IFBQ 21082
IGNvaW5jaWRlbmNl 21083
bW92ZQ== 21084
IG1pbGl0aWE= 21085
IGluZmlsdHI= 21086
IFByaW1hcnk= 21087
IHBpdGNoaW5n 21088
IEli 21089
IEdPT0Q= 21090
44K4 21091
IFdpemFyZHM= 21092
aXJhbA== 21093
IFZlbnVz 21094
UlI= 21095
IOKAlQ== 21096
IENhc2V5 21097
IHNhZGx5 21098
IGFkbWlyZQ== 21099
IGVtYmFycmFzc2Vk 21100
Y2I= 21101
TWVs 21102
IHR1YmVz 21103
IGJlYXV0aWZ1bGx5 21104
IFF1ZWVuc2xhbmQ= 21105
QmVsb3c= 21106
cmV6 21107
cXVldA== 21108
cGxlYXNhbnQ= 21109
IMKr 21110
Q2FtcA== 21111
IGRlY2lzaXZl 21112
MTk5OA== 21113
IExhbWI= 21114
dXR0b24= 21115
aG4= 21116
IEphZ3U= 21117
YXVuZGVy 21118
IENvcmQ= 21119
IGNsZXJr 21120
IGNhZmZl 21121
IHdpcGVk 21122
IHJlaW0= 21123
IE1vdW50YWlucw== 21124
IGltcHJpc29uZWQ= 21125
IGRldmVsb3Bz 21126
IFByYQ== 21127
IG1vZGVsaW5n 21128
QW55b25l 21129
YW5jZWw= 21130
IFNpdA== 21131
IHNoaWVsZHM= 21132
IGxhd24= 21133
IGNhcmRpb3Zhc2N1bGFy 21134
IGRlbW9uc3RyYXRpbmc= 21135
IHBhcnNl 21136
IElzcmFlbGlz 21137
IGV1cm9z 21138
MTQz 21139
IGdsb3Jpb3Vz 21140
aW5za2k= 21141
ZWNk 21142
IGNvbmRpdGlvbmluZw== 21143
IGhlbHBsZXNz 21144
IG1pY3Jvc2M= 21145
IEhhcmJvcg== 21146
IHN0YWtlcw== 21147
IDI2MA== 21148
IHVuZXF1 21149
IEZsb3lk 21150
IGRhbXA= 21151
IGFwcGFyYXR1cw== 21152
IExhd3M= 21153
IGNvdW50ZXJz 21154
IGluZHVjZQ== 21155
YXRhYmxl 21156
IEFobWVk 21157
IHNsYW0= 21158
Tm92ZW1iZXI= 21159
IHBlcnNpc3Q= 21160
IGltbWluZW50 21161
w6Fu 21162
IHNocmVk 21163
IHBoYXNlcw== 21164
IEVkbW9udG9u 21165
IEFybXN0cm9uZw== 21166
IE1lZXQ= 21167
IEtpdHR5 21168
0YA= 21169
Y2lyYw== 21170
IEFkdWx0 21171
IGFyb3Nl 21172
IFhlbg== 21173
RGFu 21174
Z293 21175
IHN1cGVyZg== 21176
IEFkbWly 21177
IGVuZHVyZQ== 21178
IGtleXdvcmQ= 21179
eXJ1cw== 21180
IHlhcm4= 21181
IHBhdGh3YXk= 21182
IEhvcGtpbnM= 21183
bWlkdA== 21184
IGNlbnNvcnNoaXA= 21185
ZGVwZW5kZW50 21186
IGluc3RydWN0b3I= 21187
U291cmNlcw== 21188
IHRvZQ== 21189
IGJhbGxvb24= 21190
Tm9i 21191
IHN3ZWFy 21192
IENhc3Rybw== 21193
IGdsb3Nz 21194
IEthdmFuYXVnaA== 21195
IHJlbWFya2FibHk= 21196
UGhvdG9z 21197
IE5vbQ== 21198
IFNvdXRoZWFzdA== 21199
eWVycw== 21200
IHZhbGlkYXRpb24= 21201
IGNhbm5vbg== 21202
IFZpY3Rvcnk= 21203
IFBpZXJyZQ== 21204
IGNhdXRpb3Vz 21205
QXVkaW8= 21206
IGZldGNo 21207
IEdpZnQ= 21208
IEh5cA== 21209
IHJlbWVkeQ== 21210
WkU= 21211
IHNjZW50 21212
IGJlYXJk 21213
IFJ1dA== 21214
LSI= 21215
IHBhdGVudHM= 21216
SHk= 21217
IHVuanVzdA== 21218
IHBvdGF0bw== 21219
IGZvcnRoY29taW5n 21220
IGNoZWY= 21221
IFJpZnQ= 21222
YWZmZQ== 21223
IFJPTQ== 21224
IExhdW5jaA== 21225
IHBhZHM= 21226
IE5lbw== 21227
IG9uc2V0 21228
IHNxdWVlemU= 21229
c2FmZQ== 21230
IHByZWZpeA== 21231
IFRN 21232
IE5lYXJseQ== 21233
IENsaW5pY2Fs 21234
IE1lbnRhbA== 21235
b3RpYXRpb24= 21236
IFVuaWM= 21237
YW50cnk= 21238
IENpcg== 21239
IGVwaXQ= 21240
w6Y= 21241
IGV4dHJhY3RlZA== 21242
dmVyc2VseQ== 21243
cmlhZA== 21244
IHN0cmFpbnM= 21245
IHRvcHM= 21246
IHBvZW0= 21247
IFJhbmR5 21248
IE1hcGxl 21249
VEhFUg== 21250
dXBpdGVy 21251
IFNTRA== 21252
muk= 21253
IHVuY29u 21254
cGVyaW5n 21255
IHNsZXB0 21256
aW5lcnM= 21257
IHVuZGVyd2F0ZXI= 21258
IEV2aWRlbmNl 21259
Z29uZQ== 21260
MjA1 21261
IGhpc3RvcmlhbnM= 21262
IHN5bnRoZXNpcw== 21263
IGZyb2c= 21264
YmFza2V0YmFsbA== 21265
IHZpYnJhbnQ= 21266
IHN1Ym9yZA== 21267
IDM2NQ== 21268
IERpYWw= 21269
IGNvb3BlcmF0ZQ== 21270
SEFIQQ== 21271
IGdyZWV0ZWQ= 21272
MTU4 21273
IGpheno= 21274
IGludG94 21275
IFdhbGtpbmc= 21276
IHN1cGVydmlzb3I= 21277
IEZ1c2lvbg== 21278
IE1lcmNlZGVz 21279
c2VuZA== 21280
SGFt 21281
c2Q= 21282
bmw= 21283
IHRvdXJz 21284
IEZJRkE= 21285
IGN1bHA= 21286
Z2Q= 21287
MzA0 21288
IHBsZWFz 21289
IGlsbHVzdHJhdGVz 21290
IENvbG9tYmlh 21291
IGhpZ2hsaWdodGluZw== 21292
IFN1bW1hcnk= 21293
IGV4cG9zaW5n 21294
IERydQ== 21295
IGlyb255 21296
cml0aW9uYWw= 21297
IENhcnJvbGw= 21298
IEVsbGlz 21299
UGljdA== 21300
IFJhcHQ= 21301
IGFkYXB0ZXI= 21302
IHVubQ== 21303
IGNvcnBzZQ== 21304
IGNlbGVicml0aWVz 21305
RGVu 21306
YXR1bQ== 21307
IEFwb2NhbHlwc2U= 21308
IFdhZw== 21309
bGluaW5n 21310
IGhvcm1vbmVz 21311
UnVi 21312
IFhp 21313
IFZhdWx0cw== 21314
MjA4 21315
YWxreXJpZQ== 21316
aW5vc2F1cg== 21317
IGZlZWRz 21318
dml0eQ== 21319
IGRlZmVhdGluZw== 21320
V2FpdA== 21321
IGVtcGhhc2l6ZQ== 21322
IFN0ZWVsZXJz 21323
eXJpbnRo 21324
bGV5cw== 21325
IFdoZW5ldmVy 21326
Q3VycmVudGx5 21327
IENsb2Nr 21328
IGNvbGxlY3RpdmVseQ== 21329
YW55b24= 21330
IEpQ 21331
IG1lbnRhbGl0eQ== 21332
IGRvd25sb2Fkcw== 21333
IHN1cnJvdW5kaW5ncw== 21334
IEJhcm5lcw== 21335
IGZsYWdzaGlw 21336
IGluZGljYXRvcnM= 21337
IGdyYXBw 21338
SmFudWFyeQ== 21339
IEVsZW1lbnRhbA== 21340
IEF0aGVuYQ== 21341
aWJhbA== 21342
IHNpZ2h0cw== 21343
IGNhcGl0YQ== 21344
IFRyZWF0eQ== 21345
IHZvaWNlZA== 21346
IEdheg== 21347
bGV0dGU= 21348
IHlh 21349
IGV4cGlyZWQ= 21350
TGVnZW5k 21351
SG90 21352
bmF0dXJl 21353
IHVuc3RhYmxl 21354
IDI4MA== 21355
w7o= 21356
Q29tbWVudA== 21357
QUxF 21358
IHF1ZXN0cw== 21359
IGhhbmRsZXI= 21360
bmlz 21361
IHZlcnNhdGlsZQ== 21362
IGNvbmNlYWw= 21363
ZW5nZWFuY2U= 21364
IEludGVyYWN0aXZl 21365
IG9ic2Vzc2Vk 21366
IERvZ3M= 21367
IGNyYWNrZWQ= 21368
U291bmQ= 21369
c3Y= 21370
IER5bGFu 21371
cm9hZHM= 21372
Zng= 21373
IENhdGhvbGljcw== 21374
IEhhZw== 21375
IHNsYW1tZWQ= 21376
IGdsb3dpbmc= 21377
c2FsZQ== 21378
IHRpc3N1ZXM= 21379
IENoaQ== 21380
bmVl 21381
IGNoZXI= 21382
c2lj 21383
dXJyZWN0aW9u 21384
IGJhY29u 21385
dWxhdG9yeQ== 21386
KS4i 21387
IGlycmVndWxhcg== 21388
Rk9STQ== 21389
YXNzZWQ= 21390
IGludGVudGlvbmFs 21391
IGNvbXBlbnNhdGU= 21392
IFNwZWFraW5n 21393
IFNldHM= 21394
MTUz 21395
IGNvbnZlbnRpb25z 21396
YmFuZHM= 21397
ZW1hZGU= 21398
IGVjYw== 21399
IFdpbnN0b24= 21400
IEFzc2Fzc2lu 21401
IEJlbGdpYW4= 21402
IGRlcGVuZGVuY2U= 21403
IG5pY2hl 21404
IGJhcms= 21405
IEpheno= 21406
IGRpc2FkdmFudGFnZQ== 21407
IGdhc29saW5l 21408
IDE2NQ== 21409
55qE 21410
ZXNzYQ== 21411
bW9kdWxl 21412
YW5ndWxhcg== 21413
T1k= 21414
IFRyZWF0bWVudA== 21415
aXRhcw== 21416
b2xhdGlvbg== 21417
IEFybm9sZA== 21418
IGZldWQ= 21419
IE5lc3Q= 21420
IHRoZWF0cmU= 21421
ZXdhdGVy 21422
IG1pbm9ycw== 21423
b2xpY3k= 21424
IEhhdmVu 21425
ZGl2aXNpb24= 21426
IHRydW5r 21427
RmFy 21428
IFB1bGw= 21429
IGNhcHR1cmluZw== 21430
IDE4MDA= 21431
IFRlZW4= 21432
IGV4ZW1wbA== 21433
IGNsaW5pY3M= 21434
IEJ1cmc= 21435
IHN1YnN0aXQ= 21436
IHBheWxvYWQ= 21437
IExhdg== 21438
IFRyb3k= 21439
IFdpdG5lc3M= 21440
IGZyYWdtZW50cw== 21441
IHBhc3N3b3Jkcw== 21442
IGdvc3BlbA== 21443
IEdpbg== 21444
IHRlbmFudHM= 21445
b2xpdGg= 21446
U2l4 21447
UHJldmlvdXM= 21448
IEFnZXM= 21449
IERhcndpbg== 21450
IGJsYXQ= 21451
IGVtcGF0aHk= 21452
c21pdGg= 21453
YmFn 21454
IEVjaG8= 21455
IENhbWI= 21456
IE1hZGQ= 21457
IEJvbw== 21458
IHJlZGU= 21459
IEJ1cm5pbmc= 21460
IHNtb290aGx5 21461
IEFkcmlhbg== 21462
IFZhbXBpcmU= 21463
IE1vbnN0ZXJz 21464
c3RlYW0= 21465
U3R5bGU= 21466
TWE= 21467
cmVh 21468
IER3YXI= 21469
YWx5c3Q= 21470
dXJzb3I= 21471
IGVsaW1pbmF0aW9u 21472
IGNyeXB0bw== 21473
Y2h0 21474
IEV0ZXJuYWw= 21475
4oCmXQ== 21476
IFNvcmNl 21477
SWxs 21478
TkVS 21479
IHVo 21480
Q29uY2x1c2lvbg== 21481
d2FnZQ== 21482
IHJlc3Bpcg== 21483
IHJlbWluaXM= 21484
aGV0aWNhbA== 21485
IGd5 21486
IHV0aWxpemVk 21487
aWNpZGFs 21488
IDE5MDA= 21489
IGh1bnRlcnM= 21490
IFN3YW4= 21491
IFJlYWN0 21492
IHZpc2l0b3I= 21493
IFRoYW5rc2dpdmluZw== 21494
MzA4 21495
UG9zdHM= 21496
IGhpcHM= 21497
MTk5Nw== 21498
b21lcnM= 21499
IGtub2NraW5n 21500
IFZlaGljbGU= 21501
IHRpbA== 21502
IDEzOA== 21503
IG1p 21504
IEludmVzdGlnYXRpb24= 21505
IEtlbnlh 21506
IGNhc2lubw== 21507
IG1vdGl2ZXM= 21508
IHJlZ2Fpbg== 21509
cmV4 21510
IHdlZWtlbmRz 21511
IHN0YWJiZWQ= 21512
Ym9ybw== 21513
IGV4cGxvaXRlZA== 21514
IEhBVkU= 21515
IFRlbGV2aXNpb24= 21516
Y29jaw== 21517
IHByZXBhcmF0aW9ucw== 21518
IGVuZGVhdg== 21519
IFJlbW90ZQ== 21520
IE1ha2Vy 21521
IFByb2R1 21522
IEV2YW4= 21523
IGluZm9ybWF0aW9uYWw= 21524
IExvdWlzdmlsbGU= 21525
MTU0 21526
IERyZWFtcw== 21527
IHBsb3Rz 21528
IFJ1bm5lcg== 21529
IGh1cnRpbmc= 21530
IGFjYWRlbXk= 21531
IE1vbnRnb21lcnk= 21532
bm0= 21533
IExhbmM= 21534
IEFseg== 21535
MjEw 21536
ZWxvbmc= 21537
IHJldGFpbGVy 21538
IGFyaXNpbmc= 21539
IHJlYmVsbGlvbg== 21540
IGJsb25kZQ== 21541
cGxheWVk 21542
IGluc3RydW1lbnRhbA== 21543
Q3Jvc3M= 21544
IHJldGVudGlvbg== 21545
IHRoZXJhcGV1dGlj 21546
IHNlYXM= 21547
IGluZmFudHJ5 21548
IENsaW50 21549
IHByb21wdGluZw== 21550
IGJpdGNo 21551
IHN0ZW1z 21552
IEtyYQ== 21553
IHRoZXNpcw== 21554
IEJvZw== 21555
cnVlZA== 21556
IGtpbmdz 21557
IGNsYXk= 21558
aWZpY2VudA== 21559
IFlFUw== 21560
IFRoaW5n 21561
IEN1YnM= 21562
dmV5YXJk 21563
ZWxzaA== 21564
aW5hcmlseQ== 21565
IEV5 21566
IFJvbGxpbmc= 21567
IGV2b2x2aW5n 21568
SW5kaWE= 21569
IHJlY29nbml6ZXM= 21570
IGdyYWR1YXRpb24= 21571
aXNlcnM= 21572
IGZlcnRpbGl0eQ== 21573
IE1pbGFu 21574
Q29tbWFuZA== 21575
IGJveGluZw== 21576
IDE5NDM= 21577
IGdsdXRlbg== 21578
IEVtaXI= 21579
IGlkb2w= 21580
IGNvbmNlaXZlZA== 21581
IENyZWF0aW9u 21582
TWVyaXQ= 21583
dWRkeQ== 21584
dXNzaW9ucw== 21585
IExpZXV0ZW5hbnQ= 21586
aWV0YWw= 21587
IHVuY2hhbmdlZA== 21588
IFNjYWxl 21589
IENyaW1lYQ== 21590
YmFsbHM= 21591
YXRvcmlhbA== 21592
IGRlcHRocw== 21593
IGVtcGlyaWNhbA== 21594
IHRyYW5zbQ== 21595
IHVuc2FmZQ== 21596
bWlzc2libGU= 21597
Y29tZm9ydA== 21598
MTU2 21599
IG1lY2hhbmlj 21600
MDAy 21601
bGlucw== 21602
IHNtb2tlZA== 21603
UG9z 21604
IHNsb3dpbmc= 21605
IGxhdg== 21606
VGV4YXM= 21607
IGNoZWF0aW5n 21608
IE1ldHJvcG9saXRhbg== 21609
ZXRoeWw= 21610
IGRpc2NvdmVyaW5n 21611
YXNzZQ== 21612
IHBlbmNpbA== 21613
IFB5b25neWFuZw== 21614
IGNsb3NldA== 21615
IFNoZWV0 21616
IEVudHJ5 21617
b3VzdGlj 21618
IG15c3Q= 21619
ZXJhdGU= 21620
YXJpYXQ= 21621
IG1pbmVyYWxz 21622
IG11c2ljaWFu 21623
IFB1bA== 21624
IE1heg== 21625
MjQ5 21626
IHBlcm1pc3Npb25z 21627
IGl2 21628
ZW5hcnk= 21629
aWNrZXJz 21630
IEJpbmc= 21631
aGVh 21632
ZW5hYmxl 21633
IGdyaWV2 21634
IGFzc2VydGVk 21635
IENvbG9uZWw= 21636
IGFmZmlkYXY= 21637
d28= 21638
IHNlYXRlZA== 21639
IFJpZGU= 21640
IHBhaW50aW5ncw== 21641
IFBpeA== 21642
IDEzNw== 21643
aXNoaQ== 21644
dW1iYWk= 21645
Z290dGVu 21646
IEVhcmw= 21647
IGlubmluZw== 21648
IGNlbnN1cw== 21649
IHRyYXZlbGxlZA== 21650
IENvbnN1bHQ= 21651
MTg1 21652
YmluZA== 21653
IHNpbXBsaWNpdHk= 21654
IG92ZXJsb29rZWQ= 21655
IEhlbHBmdWw= 21656
IG1vbmtleQ== 21657
IG92ZXJ3aGVsbWluZ2x5 21658
Qmxvb2Q= 21659
IEZsaW50 21660
IEphbWE= 21661
IFByZXNlbnQ= 21662
IFJhZ2U= 21663
IFRB 21664
cHRpdmU= 21665
IHR1cm5vdXQ= 21666
d2FsZA== 21667
IERvbHBoaW5z 21668
IFZQTg== 21669
IG9uaW9u 21670
IGNyYWZ0aW5n 21671
bW1h 21672
IE1lcmN1cnk= 21673
IGFycmFuZ2U= 21674
IGFsZXJ0cw== 21675
IE9U 21676
emJvbGxhaA== 21677
IGdhc2Vz 21678
IFJpY2hhcmRzb24= 21679
c2Fs 21680
bGFy 21681
IGZyb3N0 21682
IGxvd2VyaW5n 21683
IGFjY2xhaW0= 21684
IHN0YXJ0dXBz 21685
IEdhaW4= 21686
ZXNzbWVudA== 21687
IGd1YXJkaWFu 21688
5Lq6 21689
IFBpZQ== 21690
IExpbmtz 21691
IG1lcml0cw== 21692
IGF3YWtl 21693
IHBhcmVudGFs 21694
IGV4Y2VlZHM= 21695
IGlkbGU= 21696
IFBpbG90 21697
IGVCYXk= 21698
IEFjY2VwdA== 21699
aXBlZw== 21700
Q2Ft 21701
IEtvdA== 21702
IHRyYWRlcnM= 21703
b2xpdGljcw== 21704
dW5rZXI= 21705
IFBhbGU= 21706
b3Np 21707
YW5tYXI= 21708
IDE5NDc= 21709
IEZlbGw= 21710
ZXN0aWFs 21711
aXRhdGluZw== 21712
R0Y= 21713
IFNy 21714
aWZ0ZWQ= 21715
IGNvbm5lY3Rvcg== 21716
IEJvbmU= 21717
aWxsZXM= 21718
MjYw 21719
aG1h 21720
IG92ZXJsYXA= 21721
IEdpdEh1Yg== 21722
IGNsZWFuZXI= 21723
IEJhcHRpc3Q= 21724
IFdBUw== 21725
IGx1bmdz 21726
0YE= 21727
IEJVVA== 21728
IGNpdGU= 21729
IHBpdGNoZWQ= 21730
cmVhdG1lbnQ= 21731
IHRyb3BoaWVz 21732
IE51 21733
Mzg2 21734
IFByaWRl 21735
IGF0dGVuZGVlcw== 21736
W10= 21737
MTc5 21738
IHNwYXRpYWw= 21739
IHByaXplcw== 21740
IFJlbGlnaW9u 21741
IHNob3djYXNl 21742
IENhdGVnb3J5 21743
dmlkaWE= 21744
VGFyZ2V0 21745
UHJvcGVydHk= 21746
Pyw= 21747
IGZ1c2lvbg== 21748
cGll 21749
IFVDTEE= 21750
IHNvdW5kdHJhY2s= 21751
IHByaW5jZXNz 21752
IENhdmFs 21753
c2hvdWxk 21754
IGxpbWJz 21755
QmFja2dyb3VuZA== 21756
IGxvbmVseQ== 21757
IGNvcmVz 21758
IFRhaWw= 21759
c2hlZXQ= 21760
IDEzMg== 21761
UmE= 21762
44Kr 21763
IEJvbHQ= 21764
IGJvb2tlZA== 21765
IGFkbWluaXN0ZXI= 21766
IGVxdWFscw== 21767
d3k= 21768
IG9ic2VydmluZw== 21769
IEJhcm9u 21770
IEFkb2Jl 21771
IHZpcmdpbg== 21772
IFNvY2lhbGlzdA== 21773
TW92ZQ== 21774
Z2hhemk= 21775
IExpbmRh 21776
MjEy 21777
IGJyZXdpbmc= 21778
IG1lcmNoYW50cw== 21779
YnVyc2U= 21780
IGRpdm9y 21781
IG1ldGFscw== 21782
IE5lcg== 21783
IHN1bXM= 21784
IEVuZW15 21785
IGVudmlzaW9u 21786
IGdyYW50aW5n 21787
IEhvbmV5 21788
IFNreXJpbQ== 21789
IHNvY2lv 21790
Z3JhZGVk 21791
IHNlbGVjdGl2ZQ== 21792
V0FTSElOR1RPTg== 21793
IDE5NDg= 21794
IFNpcml1cw== 21795
IEdyb3Nz 21796
YWN0aXZpdHk= 21797
IEl2YW4= 21798
IGZ1cmlvdXM= 21799
QlNE 21800
IFByZXZpb3Vz 21801
IHJlc3BvbnNpdmU= 21802
IGNoYXJpdGFibGU= 21803
IGxlYW5pbmc= 21804
IFBldw== 21805
IHZpb2xhdGVz 21806
XFxcXFxcXFw= 21807
IENvbWluZw== 21808
d2lyZQ== 21809
IHBvZXQ= 21810
IHJlc29sdXRpb25z 21811
Y29tbWFuZA== 21812
IFBvcnR1Z3Vlc2U= 21813
IG5pY2tuYW1l 21814
IGRlYWY= 21815
RmVicnVhcnk= 21816
IHJlY29nbmlzZQ== 21817
IGVudGlyZXR5 21818
IHNlYXNvbmFs 21819
cGxhY2Vk 21820
IFRlbGVncmFwaA== 21821
IG1pY3JvcGhvbmU= 21822
b3VyaW5n 21823
IGdyYWlucw== 21824
IGdvdmVybmVk 21825
IHBvc3Rw 21826
IFdhdGVycw== 21827
aW5lbWVudA== 21828
IHVuZG9jdW1lbnRlZA== 21829
IENvbWNhc3Q= 21830
IGZveA== 21831
IGFzc2F1bHRz 21832
cmVvbg== 21833
bWFueQ== 21834
IEplbmtpbnM= 21835
IEFueXdheQ== 21836
IGFzc2Vzc21lbnRz 21837
IGRvd25z 21838
IE1vdXNl 21839
IHN1cGVyYg== 21840
a3Q= 21841
IERvdw== 21842
IHRheGF0aW9u 21843
NDAx 21844
IHNtaWxlcw== 21845
IHVuZGVydGFrZW4= 21846
IGV4aA== 21847
IGVudGh1c2lhc3RpYw== 21848
IHR3ZW50 21849
IGdvdmVybm1lbnRhbA== 21850
IGF1dG9ub215 21851
IFRlY2hub2xvZ2llcw== 21852
IENoYWlu 21853
IHByZXZhbGVudA== 21854
ZmI= 21855
IG5pY290aW5l 21856
b2dyYW0= 21857
am9i 21858
IGF3YWl0aW5n 21859
IE1lbnU= 21860
IGRlcHV0aWVz 21861
a292 21862
aXNob3Bz 21863
QnV0dG9u 21864
IFNoYW5naGFp 21865
IGRpZXNlbA== 21866
IER1Y2s= 21867
Unlhbg== 21868
IFBDcw== 21869
TkY= 21870
anVyeQ== 21871
ZW50ZQ== 21872
IGluYWNjdXJhdGU= 21873
ZWRkeQ== 21874
V2hhdGV2ZXI= 21875
IHNob3dj 21876
IE5hZA== 21877
b2R1cw== 21878
ZXRy 21879
IHBsYWludGlmZnM= 21880
IFdPUg== 21881
IEFzc2FuZ2U= 21882
IHByaXZhdA== 21883
IHByZW1pdW1z 21884
IHRhbQ== 21885
VVJM 21886
IGVsaXRlcw== 21887
IFJhbmdlcg== 21888
b3R0ZW5oYW0= 21889
IEhvZmY= 21890
IEF0aGVucw== 21891
IGRlZmluaXRl 21892
IHNpZ2hlZA== 21893
IGV2ZW5seQ== 21894
MjEx 21895
IEFtYmVy 21896
YWtpYQ== 21897
IG1haWxpbmc= 21898
IGNyYXNoaW5n 21899
IENvbmZlZGVyYXRl 21900
cnVnZ2Vk 21901
V2Fs 21902
IERlcHRocw== 21903
IGp1dmVuaWxl 21904
IHJlYWN0b3I= 21905
SW50cm9kdWN0aW9u 21906
IERlbHV4ZQ== 21907
MTk5NQ== 21908
IFNhbmNoZXo= 21909
IE1lYWQ= 21910
aXZhYmxl 21911
Oi0= 21912
IFBsYW5uaW5n 21913
IFRyYXA= 21914
cXVpbg== 21915
IFByb3RlY3Q= 21916
dmVyZWQ= 21917
SW5mb3JtYXRpb24= 21918
IGtpZG5leQ== 21919
aW5uYW1vbg== 21920
bGFz 21921
IHBvbGljaW5n 21922
IHRvbGVyYXRl 21923
IFFp 21924
IGJpYXNlZA== 21925
Rm9ydA== 21926
IEtp 21927
c2F2ZQ== 21928
IHByaXZpbGVnZWQ= 21929
IGJlYXN0cw== 21930
IEdsYXM= 21931
IENpbmVt 21932
IGNvbWViYWNr 21933
U3VuZGF5 21934
IGV4dGluY3Rpb24= 21935
aG9wcw== 21936
IHRyYW5zbWl0 21937
IGRvdWJsZXM= 21938
IEZsYXQ= 21939
MTY3 21940
IGRpc3B1dGVk 21941
IGluanVzdGljZQ== 21942
Zm9v 21943
VmljdA== 21944
cm9sZXVt 21945
IEp1bGll 21946
Q29udGV4dA== 21947
IFJhcml0eQ== 21948
aXNzdWU= 21949
Q29tcG9uZW50 21950
IGNvdW5zZWxpbmc= 21951
YW5uZQ== 21952
ZGFyaw== 21953
IG9iamVjdGlvbnM= 21954
dWlsdA== 21955
IGdhc3Q= 21956
IHBsYWM= 21957
IHVudXNlZA== 21958
44OH 21959
IFRyaWFs 21960
IEphcw== 21961
aGVkcmFs 21962
b2Ji 21963
IHRlbXBvcmFs 21964
IFBSTw== 21965
IE5X 21966
IEFubml2ZXJzYXJ5 21967
TGFyZ2U= 21968
IHRoZXJt 21969
IGRhdmlk 21970
IHN5c3RlbWlj 21971
IFNoaXI= 21972
bXV0 21973
IE5lcHQ= 21974
YWRkcmVzcw== 21975
IHNjYW5uaW5n 21976
IHVuZGVyc3RhbmRhYmxl 21977
IGNhbnZhcw== 21978
Q2F0 21979
IFpvbw== 21980
IGFuZ2Vscw== 21981
TE8= 21982
IFN0YXRlbWVudA== 21983
IFNpZw== 21984
b3ZhYmxl 21985
IEF3YXk= 21986
c2hhcmluZw== 21987
b2NyYXRz 21988
c3RhdGVk 21989
IHdlaWdoaW5n 21990
Tm9y 21991
d2lsZA== 21992
QmV5 21993
IGFzdG9uaXNoaW5n 21994
IFJleW5vbGRz 21995
IG9wZW5lcg== 21996
IHRyYWluZXI= 21997
IHN1cmdpY2Fs 21998
cG4= 21999
IGFkanVzdGluZw== 22000
d2hlZWw= 22001
IGZyb3du 22002
ZXJ2YXRpdmU= 22003
IHN1c3BlbmQ= 22004
V2l0aGlu 22005
dGVpbg== 22006
IG9ic3RhY2xl 22007
IGxpYmVydGllcw== 22008
eW1lcw== 22009
IHVyYW5pdW0= 22010
YW5zb20= 22011
YW5vbA== 22012
dWJh 22013
IExvc3M= 22014
IGFyb3Vz 22015
IEhlbmRlcnNvbg== 22016
V293 22017
c3Bs 22018
Y3Vy 22019
IMKt 22020
IHRoZWlycw== 22021
RGFtYWdl 22022
IGRvd25sb2FkaW5n 22023
IGRpc2Nlcm4= 22024
IFN0bw== 22025
IEZsYQ== 22026
IGhhdGg= 22027
IEFq 22028
IHVucGxlYXNhbnQ= 22029
RXVyb3BlYW4= 22030
ZXhwZW5zaXZl 22031
IHNjcmVlbnNob3Q= 22032
IFVW 22033
IGFsbGllZA== 22034
IFBlcnNpYW4= 22035
IG1vbm9wb2x5 22036
IGF0b20= 22037
IFJlZHNraW5z 22038
Ij48 22039
IGNhbmNlbGw= 22040
IGNpbmVtYQ== 22041
MTMx 22042
ZmFpcg== 22043
IEFsZnJlZA== 22044
IGR1Y2s= 22045
YXJncw== 22046
MjIz 22047
IElTSQ== 22048
IHNpZ25hbGluZw== 22049
aW5hcg== 22050
IGxhdWdocw== 22051
IGZvcndhcmRz 22052
IHJlY2tsZXNz 22053
IGxpc3RlbmVycw== 22054
YXRpdml0eQ== 22055
IHZhc3RseQ== 22056
bmFudA== 22057
TGVzcw== 22058
IEh1bnRpbmc= 22059
IFNjaWVudGlmaWM= 22060
SVRFRA== 22061
IGtuaWdodA== 22062
IEhUQw== 22063
dXNh 22064
dG1w 22065
IHJ1ZGU= 22066
IExlZ2VuZGFyeQ== 22067
IGFyaXNlcw== 22068
QmFk 22069
IENsYWlt 22070
cGVn 22071
IHJlYWxpdGllcw== 22072
VGhpbms= 22073
IMKw 22074
IHJvZGU= 22075
IHN0cml2ZQ== 22076
IGFuZWNk 22077
IHNob3J0cw== 22078
IGh5cG90aGVz 22079
IGNvb3JkaW5hdGVk 22080
IEdhbmRoaQ== 22081
IEZQUw== 22082
UkVE 22083
IHN1c2NlcHRpYmxl 22084
IHNocmluaw== 22085
IENoYXJ0 22086
SGVscA== 22087
IGlvbg== 22088
ZGVlcA== 22089
cmliZXM= 22090
IEthaQ== 22091
IEN1c3RvbWVy 22092
U3VtbWFyeQ== 22093
IGNvdWdo 22094
d2lmZQ== 22095
IGxlbmQ= 22096
IHBvc2l0aW9uaW5n 22097
IGxvdHRlcnk= 22098
IENhbnlvbg== 22099
IGZhZGU= 22100
IGJyb256ZQ== 22101
IEtlbm55 22102
IGJvYXN0cw== 22103
IEVuaGFuY2Vk 22104
cmVjb3Jk 22105
IGVtZXJnZW5jZQ== 22106
IGFraW4= 22107
IEJlcnQ= 22108
aXRvdXM= 22109
4paR 22110
IHN0aXA= 22111
IGV4Y2hhbmdlZA== 22112
b21vcmU= 22113
YWxzaA== 22114
IHJlc2Vydm9pcg== 22115
IHN0YW5kcG9pbnQ= 22116
V00= 22117
IGluaXRpYXRl 22118
IGRlY2F5 22119
IGJyZXdlcnk= 22120
IHRlcnJpYmx5 22121
IG1vcnRhbA== 22122
bGV2YXJk 22123
IHJldmlz 22124
Tkk= 22125
ZWxv 22126
IGNvbmZlc3M= 22127
IE1TTkJD 22128
IHN1Ym1pc3Npb25z 22129
Q29udHJvbGxlcg== 22130
IDIwMg== 22131
IFJ1dGg= 22132
fSk7 22133
IEF6dXJl 22134
IC4i 22135
MjA2 22136
IE1hcmtldGluZw== 22137
IGxhdW5k 22138
aWVuY2llcw== 22139
IHJlbm93bmVk 22140
IFRyb3U= 22141
IE5HTw== 22142
YmxlbXM= 22143
IHRlcnJpZmllZA== 22144
IHdhcm5z 22145
IHBlcnQ= 22146
IHVuc3VyZQ== 22147
NDgw 22148
YWxleg== 22149
dWx0eg== 22150
IE91dHNpZGU= 22151
IHN0eWw= 22152
IFVuZGVyZ3JvdW5k 22153
IHBhbmM= 22154
IGRpY3Rpb25hcnk= 22155
IGZvZQ== 22156
cmltaW5hbA== 22157
IE5vcndlZ2lhbg== 22158
IGphaWxlZA== 22159
IG1hdGVybmFs 22160
w6ll 22161
IEx1Y3k= 22162
Y29w 22163
Q2hv 22164
IHVuc2lnbmVk 22165
IFplbGRh 22166
IEluc2lkZXI= 22167
IENvbnRpbnVlZA== 22168
IDEzMw== 22169
IE5hcnV0bw== 22170
IE1ham9yaXR5 22171
MTY5 22172
IFdv 22173
44KT 22174
IHBhc3Rvcg== 22175
IGluZm9ybWFs 22176
0L0= 22177
YW50aHJvcA== 22178
am9pbg== 22179
44GX 22180
aXRhdGlvbmFs 22181
TlA= 22182
IFdyaXRpbmc= 22183
Zm4= 22184
IEJldmVy 22185
MTk1 22186
IHllbGxpbmc= 22187
IGRyYXN0aWNhbGx5 22188
IGVqZWN0 22189
IG5ldXQ= 22190
IHRocml2ZQ== 22191
IEZyZXF1 22192
b3V4 22193
IHBvc3Nlc3Nlcw== 22194
IFNlbmF0b3Jz 22195
IERFUw== 22196
IFNoYWtlc3BlYXJl 22197
IEZyYW5jbw== 22198
IExC 22199
dWNoaQ== 22200
IGluY2Fybg== 22201
IGZvdW5kZXJz 22202
RnVuY3Rpb24= 22203
IGJyaWdodG5lc3M= 22204
IEJU 22205
IHdoYWxl 22206
IFRoZWF0ZXI= 22207
bWFzcw== 22208
IERvbGw= 22209
U29tZXRoaW5n 22210
IGVjaG9lZA== 22211
IEhleA== 22212
Y3JpdA== 22213
YWZpYQ== 22214
IGdvZGRlc3M= 22215
IGVsZXZlbg== 22216
IFByZXZpZXc= 22217
IEF1cm9yYQ== 22218
IDQwMQ== 22219
dWxzaXZl 22220
IExvZ2Fu 22221
aW5idXJnaA== 22222
IENlbnRlcnM= 22223
IE9OTFk= 22224
IEFpZA== 22225
IHBhcmFkb3g= 22226
IGh1cmQ= 22227
IExD 22228
RHVl 22229
Y291cnQ= 22230
IG9mZmVuZGVk 22231
IGV2YWx1YXRpbmc= 22232
IE1hdHRoZXdz 22233
IHRvbWI= 22234
IHBheXJvbGw= 22235
IGV4dHJhY3Rpb24= 22236
IEhhbmRz 22237
aWZp 22238
IHN1cGVybmF0dXJhbA== 22239
IENPTU0= 22240
XT0= 22241
ZG9ncw== 22242
IDUxMg== 22243
IE1lZXRpbmc= 22244
UmljaGFyZA== 22245
IE1heGltdW0= 22246
IGlkZWFscw== 22247
VGhpbmdz 22248
bWFuZA== 22249
IFJlZ2FyZGxlc3M= 22250
IGh1bWlsaQ== 22251
YnVmZmVy 22252
TGl0dGxl 22253
IERhbmk= 22254
IE5haw== 22255
IGxpYmVyYXRpb24= 22256
IEFiZQ== 22257
IE9M 22258
IHN0dWZmZWQ= 22259
YWNh 22260
aW5kYQ== 22261
cmFwaGlj 22262
IG1vc3F1 22263
IGNhbXBhaWduaW5n 22264
IG9jY3VweQ== 22265
U3F1 22266
cmluYQ== 22267
IFdlbA== 22268
IFZT 22269
IHBoeXNpYw== 22270
IHB1bHM= 22271
cmludA== 22272
b2FkZWQ= 22273
RVRG 22274
IEFyY2hpdmVz 22275
IHZlbnVlcw== 22276
aG5lcg== 22277
IFR1cmJv 22278
IGx1c3Q= 22279
IGFwcGVhbGVk 22280
cXVleg== 22281
aWxpYg== 22282
IFRpbW90aHk= 22283
IG9tbg== 22284
ZHJv 22285
IG9ic2Vzc2lvbg== 22286
IFNhdmFnZQ== 22287
MTk5Ng== 22288
R2xvYmFs 22289
SmVz 22290
MjE0 22291
IHNsaWRpbmc= 22292
IGRpc2FwcHJv 22293
IE1hZ2ljYWw= 22294
IHZvbHVudGFyaWx5 22295
Z2I= 22296
YW5leQ== 22297
IHByb3BoZXQ= 22298
IFJlaW4= 22299
IEp1bGlh 22300
IFdvcnRo 22301
YXVydXM= 22302
IGJvdW5kcw== 22303
aWV1 22304
KSkp 22305
IGNyb3Jl 22306
IENpdGl6ZW4= 22307
U2t5 22308
IGNvbHVtbmlzdA== 22309
IHNlZWtlcnM= 22310
b25kbw== 22311
SVNB 22312
IExlbmd0aA== 22313
IG5vc3RhbGc= 22314
IG5ld2NvbQ== 22315
IGRldHJpbQ== 22316
ZW50cmlj 22317
Mzc1 22318
IEdF 22319
IGF1dG9w 22320
IGFjYWRlbWljcw== 22321
QXBwRGF0YQ== 22322
IFNoZW4= 22323
IGlkaW90 22324
IFRyYW5zaXQ= 22325
IHRlYXNwb29u 22326
V2ls 22327
S08= 22328
IENvbWVkeQ== 22329
Piw= 22330
IHBvcHVsYXRlZA== 22331
V0Q= 22332
IHBpZ3M= 22333
IE9jdWx1cw== 22334
IHN5bXBhdGhldGlj 22335
IG1hcmF0aG9u 22336
MTk4 22337
IHNlaXp1cmU= 22338
c2lkZWQ= 22339
IGRvcA== 22340
aXJ0dWFs 22341
TGFuZA== 22342
IEZsb29y 22343
b3NhdXJz 22344
Li4uXQ== 22345
IGxvcw== 22346
IHN1YnNpZGlhcnk= 22347
RVk= 22348
IFBhcnRz 22349
IFN0ZWY= 22350
IEp1ZGljaWFyeQ== 22351
IDEzNA== 22352
IG1pcnJvcnM= 22353
IGtldA== 22354
dGltZXM= 22355
IG5ldXJvbG9n 22356
IGNhdg== 22357
IEd1ZXN0 22358
IHR1bW9y 22359
c2NpbGw= 22360
IExsb3lk 22361
RXN0 22362
IGNsZWFyZXI= 22363
IHN0ZXJlb3R5cGVz 22364
IGR1cg== 22365
bm90aGluZw== 22366
UmVkZGl0 22367
IG5lZ290aWF0ZWQ= 22368
LS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0t 22369
MjM1 22370
IGZsb3du 22371
IFNlb3Vs 22372
IFJlc2lkZW50 22373
IFNDSA== 22374
IGRpc2FwcGVhcmFuY2U= 22375
IFZpbmNl 22376
Z3Jvd24= 22377
IGdyYWJz 22378
cmls 22379
IEluZmluaXRl 22380
IFR3ZW50eQ== 22381
IHBlZGVzdHJpYW4= 22382
IGplcnNleQ== 22383
IEZ1cg== 22384
IEluZmluaXR5 22385
IEVsbGlvdHQ= 22386
IG1lbnRvcg== 22387
IG1vcmFsbHk= 22388
IG9iZXk= 22389
c2VjdXJl 22390
aWZmZQ== 22391
IGFudGliaW90aWNz 22392
YW5nbGVk 22393
IEZyZWVtYW4= 22394
IEludHJvZHVjdGlvbg== 22395
SnVu 22396
IG1hcnNo 22397
aWNhbnM= 22398
IEVWRU5UUw== 22399
b2Nob25k 22400
V2FsbA== 22401
aWN1bHR5 22402
IG1pc2RlbWVhbm9y 22403
IGx5 22404
VGhvbWFz 22405
IFJlc29sdXRpb24= 22406
IGFuaW1hdGlvbnM= 22407
IERyeQ== 22408
IGludGVyY291cnNl 22409
IE5ld2Nhc3RsZQ== 22410
IEhvZw== 22411
IEVxdWlwbWVudA== 22412
MTc3 22413
IHRlcnJpdG9yaWFs 22414
IGFyY2hpdmVz 22415
MjAz 22416
RmlsdGVy 22417
IE11bmljaA== 22418
IGNvbW1hbmRlZA== 22419
IFdhbmQ= 22420
IHBpdGNoZXM= 22421
IENyb2F0 22422
IHJhdGlvcw== 22423
IE1pdHM= 22424
IGFjY3VtdWxhdGVk 22425
IFNwZWNpZmljYWxseQ== 22426
IGdlbnRsZW1hbg== 22427
YWNlcmI= 22428
IHBlbm4= 22429
IGFrYQ== 22430
IEZ1aw== 22431
IGludGVydmVuZQ== 22432
IFJlZnVnZQ== 22433
IEFsemhlaW1lcg== 22434
IHN1Y2Nlc3Npb24= 22435
b2hhbg== 22436
ZG9lcw== 22437
TG9yZA== 22438
IHNlcGFyYXQ= 22439
IGNvcnJlc3BvbmRlbmNl 22440
IHNoaW55 22441
UHJpb3I= 22442
IHN1bGY= 22443
IG1pc2VyYWJsZQ== 22444
IGRlZGljYXRpb24= 22445
KCku 22446
IHNwZWNpYWxpc3Rz 22447
IGRlZmVjdHM= 22448
IEN1bHQ= 22449
IFhpYQ== 22450
IGplb3BhcmQ= 22451
IE9yZQ== 22452
QWJpbGl0eQ== 22453
IGxlYXI= 22454
IGFtYml0aW9ucw== 22455
IEJNSQ== 22456
IEFyYWJz 22457
IDE5NDI= 22458
IHByZXNlcnZhdGlvbg== 22459
aWZpY2F0ZQ== 22460
IGFzaGFtZWQ= 22461
bG9zcw== 22462
IFJlc3RhdXI= 22463
IHJlc2VtYmxl 22464
IGVucmljaA== 22465
IEtO 22466
IENsYW4= 22467
ZmxvYXQ= 22468
IHBsYXlhYmxl 22469
SVRU 22470
IGhhcm1vbnk= 22471
YXJyaXNvbg== 22472
IFdlaW5zdGVpbg== 22473
d2VyZQ== 22474
IHBvaXNvbmluZw== 22475
IENvbXB1dA== 22476
IFdvcmRQcmVzcw== 22477
bWFqb3I= 22478
IFZhbHZl 22479
RmFu 22480
IFRocm93 22481
IFJvbWFucw== 22482
IERlcHJlc3Npb24= 22483
YWRvcw== 22484
IHRvcnR1cmVk 22485
IGJhbGFuY2luZw== 22486
Ym90dG9t 22487
IGFjcXVpcmluZw== 22488
IE1vbnRl 22489
YXJkaQ== 22490
IGF1cmE= 22491
ICMj 22492
IFN0YW5kaW5n 22493
IEF0bGFz 22494
Q0Y= 22495
IGludHJpbnM= 22496
IEJlbmdoYXpp 22497
IGNhbXBpbmc= 22498
IHRhcHBlZA== 22499
YmxhZGU= 22500
c3Ryb3Vz 22501
IFJhYmI= 22502
IFdyaXR0ZW4= 22503
dGlw 22504
IE5laWdo 22505
c3RlcmRhbQ== 22506
IEFsbG93 22507
IEhlYWxpbmc= 22508
IFJob2Q= 22509
bnVt 22510
IGNhZmZlaW5l 22511
IFBlcmNlbnQ= 22512
IGJvbw== 22513
IGFwcGxlcw== 22514
MzA1 22515
IHdlbGNvbWluZw== 22516
IGFwcGxhdWQ= 22517
IGF1c3Rlcml0eQ== 22518
wrE= 22519
IFJlYWxpdHk= 22520
ZWZl 22521
5a4= 22522
IHN1Y2tz 22523
IHRhYnM= 22524
IFBheVBhbA== 22525
IGJhY2twYWNr 22526
IGdpZnRlZA== 22527
YWJ1bGFyeQ== 22528
IFNjb3V0 22529
aXJ0ZWVu 22530
IGNoaW4= 22531
IG9taXR0ZWQ= 22532
IG5lZ2F0aXZlbHk= 22533
IGFjY2Vzc2luZw== 22534
IEVhcm4= 22535
IGFtYnVsYW5jZQ== 22536
IGhlYWRwaG9uZXM= 22537
IDIwNQ== 22538
IFJlZnJlc2g= 22539
cHJlc2lkZW50 22540
IEtpdGNoZW4= 22541
IEVudGVyZWQ= 22542
IFNueWRlcg== 22543
MDA1 22544
b21pY2Fs 22545
IGJvcnJvd2Vk 22546
IE5lbQ== 22547
IGF2aWF0aW9u 22548
IHN0YWxs 22549
cmltaW5hdGlvbg== 22550
IHVuaWZvcm1z 22551
aXRpbWU= 22552
IFNpbW1vbnM= 22553
ZW5lcmd5 22554
YWJsaXNoZWQ= 22555
eXk= 22556
cXVhbGlmaWVk 22557
IHJhbGxpZXM= 22558
IFN0dWFydA== 22559
ZmxpZ2h0 22560
IGdhbmdz 22561
cmFn 22562
IHZhdWx0 22563
bHV4 22564
IENvbXBhcg== 22565
IGRlc2lnbmF0aW9u 22566
MjA5 22567
IEpvcw== 22568
ZG9sbGFy 22569
emVybw== 22570
IHdlbGxz 22571
MzAz 22572
IGNvbnN0aXR1ZW50cw== 22573
IGhlY2s= 22574
IGNvd3M= 22575
IGNvbW1hbmRlcnM= 22576
IGRpZmZlcmVudGlhbA== 22577
IENhdGhlcmluZQ== 22578
Mjk5 22579
IHZhbHZl 22580
IGJyYWNl 22581
IHBlcnNwZWN0aXZlcw== 22582
Y2VydA== 22583
ZmFjdA== 22584
aWN1bGFybHk= 22585
IE1jTg== 22586
cGxhbmVz 22587
IGludHJpYw== 22588
IHBlYXM= 22589
b3Zhbg== 22590
IHRvc3NlZA== 22591
cmV0Y2g= 22592
IExvcGV6 22593
IHVuZmFtaWxpYXI= 22594
ZGVhdGg= 22595
IEFwYXJ0 22596
IENoYW5n 22597
IHJlbGlldmVk 22598
cm9waGU= 22599
IGFpcnBvcnRz 22600
IGZyZWFr 22601
dXRpbA== 22602
TWlsbA== 22603
IENoaW4= 22604
IE93ZW4= 22605
bWFsZQ== 22606
IEJyb2tlbg== 22607
IFdpbmRz 22608
cm9i 22609
cmlzaW5n 22610
IGZpcmVmaWdodGVycw== 22611
IGF1dGhvcml0YXJpYW4= 22612
IDE0OA== 22613
Qml0Y29pbg== 22614
ZXh0ZXJuYWw= 22615
IGJyb3dzZXJz 22616
aWNoZXZlcg== 22617
b3JpYW4= 22618
IHVuYg== 22619
IHBva2U= 22620
IFpvdA== 22621
TWlk 22622
IFBvcHVsYXI= 22623
IGNvdmVydA== 22624
IGNvbnRyaWJ1dGVz 22625
IDY1MA== 22626
IGNvbnRlbnRpb24= 22627
R2F0ZQ== 22628
IGNvbnNvbGVz 22629
IGNocm9tb3M= 22630
IElY 22631
IHZpc3VhbGx5 22632
IEVpc2Vu 22633
IGpld2Vscnk= 22634
IGRlbGVnYXRpb24= 22635
IGFjY2VsZXJhdGU= 22636
IFJpbGV5 22637
IHNsb3Bl 22638
IGluZG9vcg== 22639
aXRpYWxseQ== 22640
IGh1Z2VseQ== 22641
IHR1bm5lbHM= 22642
IGZpbmVk 22643
IGRpcmVjdGl2ZQ== 22644
IGZvcmVoZWFk 22645
dXN0b21lZA== 22646
IHNrYXRl 22647
TXVzaWM= 22648
Z2Fz 22649
IHJlY29nbml6aW5n 22650
YW1ibw== 22651
IG92ZXJ3ZWlnaHQ= 22652
IEdyYWRl 22653
2Yo= 22654
IHNvdW5kaW5n 22655
IGxvY2tpbmc= 22656
IFJFTQ== 22657
U3RvcmU= 22658
IGV4Y2F2 22659
IExpa2V3aXNl 22660
IExpZ2h0cw== 22661
IGVsYm93 22662
IFN1cHBseQ== 22663
d2lj 22664
IGhhbmRzb21l 22665
MTk5NA== 22666
Q29sbA== 22667
IGFkZXF1YXRlbHk= 22668
IEFzc29jaWF0ZQ== 22669
IHN0cmlwcw== 22670
IGNyYWNrZG93bg== 22671
IG1hcnZlbA== 22672
IEt1bg== 22673
IHBhc3NhZ2Vz 22674
QEBAQA== 22675
IFRhbGw= 22676
IHRob3VnaHRmdWw= 22677
bmFtZXNl 22678
IHByb3N0aXR1dGlvbg== 22679
YnVzaW5lc3M= 22680
IGJhbGxpc3RpYw== 22681
cGVyc29uYWw= 22682
Y2ln 22683
aXphdGlvbmFs 22684
Um91bmQ= 22685
IMKgIMKgIMKgIMKg 22686
IENvbGVtYW4= 22687
IGFkbWl0dGluZw== 22688
IFBsdWc= 22689
IGJpdGNvaW5z 22690
IFN1eg== 22691
IGZhaXJuZXNz 22692
IHN1cHBsaWVy 22693
IGNhdGFzdHJvcGhpYw== 22694
IEhlbGVu 22695
b3F1 22696
TWFyYw== 22697
IEFydGljbGVz 22698
Z2ll 22699
IGVuZGFuZ2VyZWQ= 22700
IGRlc3Rpbnk= 22701
IFZvbHQ= 22702
b2xpYQ== 22703
YXhpcw== 22704
IGNoZWF0 22705
IHVuaWZpZWQ= 22706
SUNP 22707
cXVvdGU= 22708
MzAy 22709
IFNlZA== 22710
IHN1cHByZXNzaW9u 22711
IGFuYWx5emluZw== 22712
IHNxdWF0 22713
IGZpZ3VyaW5n 22714
IGNvb3JkaW5hdGVz 22715
IGNodW5rcw== 22716
IDE5NDY= 22717
IHN1YnA= 22718
IHdpa2k= 22719
IEZvcmJlcw== 22720
IEp1cGl0ZXI= 22721
IEVyaWs= 22722
aW1lcg== 22723
IENvbW1lcmNpYWw= 22724
XCk= 22725
IGxlZ2l0aW1hY3k= 22726
IGRlbnRhbA== 22727
IE1lYW4= 22728
IGRlZmljaXRz 22729
NTUw 22730
T3JpZ2luYWxseQ== 22731
IEhvcnJvcg== 22732
IGNvbnRhbWluYXRpb24= 22733
bGxhaA== 22734
IGNvbmZpc2M= 22735
IENsYXJl 22736
VEI= 22737
IEZhaWxlZA== 22738
YW5lZA== 22739
IHJ1bGVy 22740
IENvbnRyb2xsZXI= 22741
IGZlbWluaXN0cw== 22742
Rml4 22743
Z2F5 22744
MjA3 22745
IHJhYmJpdA== 22746
VGhpcmQ= 22747
b3dudG93bg== 22748
IGdsdWU= 22749
IHZvbGF0aWxl 22750
IHNoaW5pbmc= 22751
IGZvbGw= 22752
IGltcGFpcmVk 22753
IHN1cGVycw== 22754
5og= 22755
IGNsdXRjaA== 22756
mumGkg== 22757
IHByb2xldA== 22758
ICgh 22759
IHllbGxlZA== 22760
IEtpZXY= 22761
IEVybg== 22762
IFNob2Nr 22763
S0I= 22764
IHNpdHVhdGVk 22765
cXVlcnk= 22766
IE5hcw== 22767
IGFubmV4 22768
Y2hhcmFjdGVy 22769
IEhvbGlkYXk= 22770
IGF1dG9tYXRpb24= 22771
IEppbGw= 22772
IFJlbWFzdGVyZWQ= 22773
IGxpbmVt 22774
IHdpbGRlcm5lc3M= 22775
IEhvcml6b24= 22776
IEd1aW5lYQ== 22777
QVo= 22778
IG1haW5sYW5k 22779
IHNlY3JlY3k= 22780
TEVBU0U= 22781
IHB1bms= 22782
IFByb3ZpbmNl 22783
KCks 22784
U3BlZWQ= 22785
IGhhbmRpbmc= 22786
IFNlYmFzdA== 22787
U2ly 22788
cmFzZQ== 22789
IGpvdXJuYWxz 22790
IGNvbmdlc3Q= 22791
IFR1dA== 22792
aXJyZWw= 22793
IHNjaGl6b3BocmVuaWE= 22794
IG1pc29neW4= 22795
aGVhbHRoeQ== 22796
SXJvbg== 22797
IHJlYWN0ZWQ= 22798
LSQ= 22799
MjUy 22800
IHBsdXJhbA== 22801
IHBsdW0= 22802
IGJhcmdhaW4= 22803
IGdyb3VuZGVk 22804
ZmluZGVy 22805
IGRpc3Nl 22806
IExheg== 22807
T09E 22808
IGF0cm9j 22809
RmFjdG9yeQ== 22810
IG1pbmlvbnM= 22811
IG9yaQ== 22812
IEJyYXZl 22813
IFBSRQ== 22814
IE15YW5tYXI= 22815
IEhvZA== 22816
IGV4cGVkaXRpb24= 22817
IGV4cGxvZGU= 22818
IENvb3Jk 22819
IGV4dHI= 22820
IEJyaWVm 22821
IEFESEQ= 22822
IGhhcmRjb3Jl 22823
ZmVlZGluZw== 22824
IGRpbGU= 22825
IEZydWl0 22826
IHZhY2NpbmF0aW9u 22827
IE1hbw== 22828
b3NwaGVyZQ== 22829
IGNvbnRlc3Rz 22830
LXw= 22831
IGZyZW4= 22832
aXNwaGVyZQ== 22833
Um9t 22834
IFNoYXJw 22835
IFRyZW5k 22836
IGRpc2Nvbm5lY3Q= 22837
4oCi4oCi 22838
IHBlcnNlY3V0aW9u 22839
RWFydGg= 22840
IGhlYWx0aGllcg== 22841
Mzg0 22842
IGNvYg== 22843
IFRyaW5pdHk= 22844
T1dT 22845
QU5O 22846
IHNwZWNpYWx0eQ== 22847
IGdydQ== 22848
IGNvb3BlcmF0aXZl 22849
d2h5 22850
U3RhcnRpbmc= 22851
IElzc3Vlcw== 22852
c3RyZQ== 22853
ZW5zb3I= 22854
IDE4NQ== 22855
QWR2 22856
IT8= 22857
IFJldmVs 22858
ZW1pYQ== 22859
IEh1bGs= 22860
IGNlbGVicmF0aW9ucw== 22861
IFNvdQ== 22862
cmF1ZA== 22863
IEtsZWlu 22864
IHVucmVhbA== 22865
Y29udGV4dA== 22866
IHBhcnRuZXJzaGlwcw== 22867
IGFkb3B0aW5n 22868
dGljYWw= 22869
IHNwbGFzaA== 22870
IEhlemJvbGxhaA== 22871
Y2F0ZWdvcnk= 22872
Y3ljbG9w 22873
eHRvbg== 22874
IERvdA== 22875
dXJkeQ== 22876
dHo= 22877
IGVudmVsb3Bl 22878
IE5M 22879
4pU= 22880
IHdoZXJlaW4= 22881
U3BlYw== 22882
MTg0 22883
IHRlbGV2 22884
YWxpYXRpb24= 22885
IG15dGhz 22886
5bA= 22887
IHJpZ29yb3Vz 22888
IGNvbW11bmljYXRpbmc= 22889
IG9ic2VydmVy 22890
IHJlaGU= 22891
IFdhc2g= 22892
IGFwb2xvZ2l6ZWQ= 22893
IFRpbg== 22894
IGV4cGVuZGl0dXJlcw== 22895
d29ya2Vycw== 22896
ZG9jdW1lbnQ= 22897
IGhlc2l0YXRl 22898
IExlbmlu 22899
IHVucHJlZGljdGFibGU= 22900
IHJlbmV3YWw= 22901
Y2xlcg== 22902
b2tpYQ== 22903
IENPTlQ= 22904
IHBvc3RzZWFzb24= 22905
VG9rZW5z 22906
IGV4YWNlcmI= 22907
IGJldHRpbmc= 22908
IDE0Nw== 22909
IGVsZXZhdGlvbg== 22910
V29vZA== 22911
IFNvbG9tb24= 22912
MTk0 22913
MDA0 22914
b3V0cHV0 22915
IHJlZHVuZA== 22916
IE11bWJhaQ== 22917
IHBI 22918
IHJlcHJvZHVjZQ== 22919
IER1cmF0aW9u 22920
TUFY 22921
IGJvZw== 22922
Q0JT 22923
IEJhbGFuY2U= 22924
IFNndA== 22925
IFJlY2VudA== 22926
IGNk 22927
IHBvcHBlZA== 22928
IGluY29tcGV0 22929
cHJvcA== 22930
YXlhbg== 22931
Z3V5 22932
UGFjaWZpYw== 22933
IHR5cg== 22934
IHt7 22935
IE15c3RpYw== 22936
IERhbmE= 22937
IG1hc3R1cmI= 22938
IGdlb21ldHJ5 22939
w6I= 22940
IENvcnJlY3Q= 22941
IHRyYWplY3Rvcnk= 22942
IGRpc3RyYWN0ZWQ= 22943
IGZvbw== 22944
IFdlbHNo 22945
THVj 22946
bWl0aA== 22947
IHJ1Z2J5 22948
IHJlc3BpcmF0b3J5 22949
IHRyaWFuZ2xl 22950
IDIxNQ== 22951
IHVuZGVyZ3JhZHVhdGU= 22952
IFN1cGVyaW9y 22953
Y2hhbmdpbmc= 22954
Xy0= 22955
IHJpZ2h0bHk= 22956
IHJlZmVyZWU= 22957
IGx1Y3JhdGl2ZQ== 22958
IHVuYXV0aG9yaXplZA== 22959
IHJlc2VtYmxlcw== 22960
IEdOVQ== 22961
IERlcmJ5 22962
IHBhdGh3YXlz 22963
IExlZA== 22964
IGVuZHVyYW5jZQ== 22965
IHN0aW50 22966
IGNvbGxlY3Rvcg== 22967
RmFzdA== 22968
IGRvdHM= 22969
IG5hdGlvbmFscw== 22970
IFNlY3VyaXRpZXM= 22971
IHdoaXA= 22972
UGFyYW0= 22973
IGxlYXJucw== 22974
TWFnaWM= 22975
IGRldGFpbGluZw== 22976
bW9vbg== 22977
IGJyb2FkY2FzdGluZw== 22978
IGJha2Vk 22979
MjY1 22980
aG9sbQ== 22981
IFNhaA== 22982
IEh1c3NlaW4= 22983
IENvdXJ0ZXN5 22984
MTc0 22985
IDE0Ng== 22986
IGdlb2dyYXBoaWM= 22987
cGVhY2U= 22988
IGp1ZGdpbmc= 22989
IFN0ZXJu 22990
QnVy 22991
IHN0b3J5bGluZQ== 22992
R3Vu 22993
IFN0aWNr 22994
MjQ1 22995
MzA3 22996
44K044Oz 22997
IEFkbWluaXN0cmF0b3I= 22998
IGJ1cm50 22999
IHBhdmU= 23000
Y2hvZXM= 23001
RXhlYw== 23002
IGNhbXB1c2Vz 23003
UmVzdWx0 23004
IG11dGF0aW9ucw== 23005
IENoYXJ0ZXI= 23006
IGNhcHR1cmVz 23007
IGNvbXBhcmVz 23008
IGJhZGdl 23009
U2NpZW50 23010
IGVyYWQ= 23011
aWVyeQ== 23012
b2k= 23013
ZXR0ZXM= 23014
IEVzdGF0ZQ== 23015
IHN0cmFw 23016
IHByb3VkbHk= 23017
IGZyaWVk 23018
IHdpdGhkcmF3bg== 23019
IFZveQ== 23020
cGhvbnk= 23021
SXRlbXM= 23022
IFBpZXJjZQ== 23023
YmFyZA== 23024
IGFubm90YXRpb24= 23025
YW50b24= 23026
aWxsb24= 23027
SW1wcm8= 23028
Li4uKQ== 23029
IGhhcHBpZXI= 23030
LS0tLS0t 23031
YWRqdXN0 23032
IHN0YWZmZXJz 23033
IGFjdGl2aXNt 23034
IHBlcmY= 23035
IGFscmlnaHQ= 23036
TmVlZA== 23037
IGNvbW1lbmNl 23038
IG9waW9pZA== 23039
IEFtYW5kYQ== 23040
RXM= 23041
IFBhcnM= 23042
IEthdw== 23043
V29ya3M= 23044
MjQ4 23045
IGluZG8= 23046
dGM= 23047
ZW5kYW50 23048
IE1vdG8= 23049
IGxlZ2FsaXphdGlvbg== 23050
T1RF 23051
IHRhc2tlZA== 23052
IHRzcA== 23053
IEFDVElPTlM= 23054
MTY2 23055
IHJlZnJlc2hpbmc= 23056
IE5S 23057
IFBlcmV6 23058
IGluZnJpbmdlbWVudA== 23059
U1k= 23060
TGlzdGVu 23061
aW5uaW5n 23062
a3U= 23063
IHJvdGF0ZQ== 23064
cHJvZ3JhbQ== 23065
YXJhaA== 23066
RGVzaWdu 23067
ICjCow== 23068
IHN0b3Jpbmc= 23069
IHdhcnJhbnRz 23070
IGp1ZGdlbWVudA== 23071
IEJyaXN0 23072
dXN1YWxseQ== 23073
cGhvdG8= 23074
IFJhbg== 23075
IFBpbmU= 23076
IG91dHJhZ2VvdXM= 23077
IFZhbGVudGluZQ== 23078
bHVlbmNl 23079
IEV2ZXJ5Ym9keQ== 23080
QWx0ZXJu 23081
IHJlbGV2YW5jZQ== 23082
IHRlcm1pbmF0ZWQ= 23083
IGRlc3NlcnQ= 23084
IGZ1bGZpbGxlZA== 23085
IHByb3NlY3V0ZWQ= 23086
IFdvcmRz 23087
IG1pZ3JhbnQ= 23088
IGN1bHRpdmF0aW9u 23089
w4PDgsODw4LDg8OCw4PDgsODw4LDg8OCw4PDgsODw4LDg8OCw4PDgsODw4LDg8OCw4PDgsODw4LDg8OCw4PDgg== 23090
aWRlbGl0eQ== 23091
IFZlcm4= 23092
IExvZ2lu 23093
IG1ldGFwaG9y 23094
IFRpcA== 23095
IHJlY3J1aXRz 23096
IFBpZw== 23097
cmliaW5n 23098
IGVudGh1c2lhc3Rz 23099
ZXhwZXI= 23100
IGZyaWdodGVuaW5n 23101
IEhhaXI= 23102
YW5zb24= 23103
c3RyYXRl 23104
IGhp 23105
SGVpZ2h0 23106
IG93bmluZw== 23107
bm9uZQ== 23108
IGRpc2xpa2U= 23109
IGtuaXZlcw== 23110
cGhlcmQ= 23111
IGxvdWRseQ== 23112
IEFQSXM= 23113
RGlzcGxheQ== 23114
IExhYw== 23115
IFVTUw== 23116
YWJs 23117
dmVyYWdlcw== 23118
SmV3 23119
IDE3Mg== 23120
IEhpc3RvcmljYWw= 23121
YXRvb24= 23122
IFBoeXNpY3M= 23123
aW50ZXJu 23124
IHdhcm10aA== 23125
IHRvcHA= 23126
RE0= 23127
IGd1bm1hbg== 23128
IGVtcGVyb3I= 23129
b2Rp 23130
44Oj 23131
aW5hdG9yeQ== 23132
IFJpYg== 23133
IDEzMQ== 23134
IFNhdHVybg== 23135
IFNoaW5pbmc= 23136
IHdha2luZw== 23137
UXVvdGVz 23138
IGNvbWVkaWFu 23139
ZW5iZXJn 23140
wr0= 23141
IGJlbGlldmVycw== 23142
IHBhcGVyd29yaw== 23143
Y3VzdG9t 23144
IGxldg== 23145
IGxhbWVudA== 23146
IHBvdXJpbmc= 23147
MjIy 23148
cG9saXRpY2Fs 23149
IFN1cHBsZW1lbnQ= 23150
bWFpZA== 23151
IGNydWVsdHk= 23152
IHRyZWFk 23153
eXNpY3M= 23154
QXc= 23155
cml0ZXM= 23156
IG1vZGlmaWVy 23157
IFBvc2l0aW9u 23158
QWRhbQ== 23159
bGI= 23160
dWJz 23161
IGltcGVyZmVjdA== 23162
IGNsdXN0ZXJz 23163
IEVuZ2luZWVy 23164
IENoZXJyeQ== 23165
IGluYXVndXJhdGlvbg== 23166
IFNhdQ== 23167
IGVtYm9kaW1lbnQ= 23168
IFVuY2xl 23169
IG92ZXJy 23170
IGV4cGxvc2lvbnM= 23171
Y3VsZQ== 23172
IFByaW5jZXRvbg== 23173
IEFuZHJlYQ== 23174
IGluY29ycmVjdGx5 23175
IGVhcm5lc3Q= 23176
IHBpbGdy 23177
IFNwcmludA== 23178
IHNsZWV2ZQ== 23179
IGhlYXJz 23180
IEFtYXppbmc= 23181
IGJyb3dzaW5n 23182
YWdpbg== 23183
IGhvbWVsYW5k 23184
IGhhdw== 23185
IGRpdmluZw== 23186
aXN0ZXJlZA== 23187
MTc4 23188
IGJhcmdhaW5pbmc= 23189
IEFyY2FkZQ== 23190
IGRlbGVnYXRl 23191
dGVyc29u 23192
Li4uLi4uLi4uLi4uLi4uLi4uLi4uLi4uLi4uLi4uLi4uLi4uLi4uLi4uLi4uLi4uLi4uLi4uLi4uLi4uLi4uLg== 23193
IEphY2tzb252aWxsZQ== 23194
Mjc1 23195
IHN0YWdu 23196
IGFkYW0= 23197
IFNoZXJtYW4= 23198
Q0I= 23199
IHN1YnVyYg== 23200
IEZvb2Rz 23201
IGNvbnZlcnRpbmc= 23202
IEFyaXN0 23203
IGNoYW1iZXJz 23204
bG92ZQ== 23205
IGFtaW5v 23206
IEdhbg== 23207
IG1hZG5lc3M= 23208
bWM= 23209
IFVTRQ== 23210
ZGVmaW5lZA== 23211
IHVsdHI= 23212
aW5kdXN0 23213
IHdvbHZlcw== 23214
bGFuY2U= 23215
QWRkaXRpb25hbGx5 23216
IGNyYWNrcw== 23217
YXNpYQ== 23218
IFJlYXNvbg== 23219
IFB1bXA= 23220
IGFjY2lkZW50YWw= 23221
IExhc2Vy 23222
IFJpZA== 23223
IGluaXRpYWxpemVk 23224
ZWxsaQ== 23225
IHVubmFtZWQ= 23226
IG5vdW4= 23227
IFBhc3NlZA== 23228
IGhvc3RhZ2U= 23229
IEV0aGlvcA== 23230
c2hpcnRz 23231
IHVucmVs 23232
IEVtYmFzc3k= 23233
IDE5NDE= 23234
IGF0b21z 23235
IHB1cnBvcnRlZA== 23236
MTY0 23237
IEZp 23238
IGdhbGxvbnM= 23239
IE1vbmljYQ== 23240
IHBn 23241
ZW5tZW50 23242
IHNvcnRlZA== 23243
IEdvc3BlbA== 23244
IGhlaWdodHM= 23245
IHRyYWNlZA== 23246
IHVuZGVyZ29pbmc= 23247
U2hlbGw= 23248
IHNhY2tz 23249
IHByb3BvcnRpb25z 23250
IGhhbGx1Yw== 23251
Rm9udA== 23252
YWNldA== 23253
IHdhcm1lcg== 23254
IElOVEVS 23255
IGdyYWJiaW5n 23256
UGx1Zw== 23257
IHJlYWxpemF0aW9u 23258
IEJ1cmtl 23259
IGVuY2hhbnQ= 23260
QVRFUg== 23261
IFNlZWQ= 23262
IGFidW5kYW50 23263
Rk0= 23264
IGNpdmlj 23265
VnM= 23266
aXNp 23267
IHZvdw== 23268
IHJlcGVy 23269
IFBhcnRuZXJzaGlw 23270
IHBlbmV0cmF0aW9u 23271
IGF4ZQ== 23272
IHNoYXR0ZXJlZA== 23273
IFpvbWJpZXM= 23274
IHZpbnls 23275
IEFsZXJ0 23276
ZW9u 23277
IG9ibGlnZWQ= 23278
IElsbHVzdA== 23279
IFBsYXph 23280
IEZyb250aWVy 23281
IGRhdmlkamw= 23282
IFNlcmlhbA== 23283
IEhhdg== 23284
IE51dHJpdGlvbg== 23285
Qmk= 23286
IOKWiA== 23287
IEpheXM= 23288
bGludXg= 23289
IGh1cnJ5 23290
IHZveQ== 23291
IGhvcGVsZXNz 23292
IFN0ZWFsdGg= 23293
IOOB 23294
ZXNzb3Jz 23295
dHRsZQ== 23296
Ym9yZw== 23297
IFNhZmFyaQ== 23298
ZmVsbA== 23299
IHdhcnk= 23300
ZHVl 23301
IEFib3Zl 23302
SGE= 23303
RUxM 23304
IG5vdG9y 23305
IFdvbg== 23306
VG9v 23307
IG9jY3VwYXRpb25z 23308
IHBvc3Nlc3Npb25z 23309
IGludml0aW5n 23310
IHByZWRhdG9ycw== 23311
IGFjY2VsZXJhdGVk 23312
IDE1Nw== 23313
dXRlcnRl 23314
IEN1YmU= 23315
ZWFzdA== 23316
YWNjb3VudA== 23317
R2l2ZQ== 23318
IHRyYW5zcGxhbnQ= 23319
cmVkaWVudHM= 23320
aWRhYmxl 23321
IHNjcmVlbnNob3Rz 23322
IEd1bmQ= 23323
IEZT 23324
IHRyYXZlbGVycw== 23325
IHNlbnNvcnk= 23326
IEZpYXQ= 23327
IFJvY2tldHM= 23328
jos= 23329
X3s= 23330
RnJpZW5k 23331
IGNoYXJtaW5n 23332
QUxT 23333
IGVuam95bWVudA== 23334
bXBo 23335
IDUwMDA= 23336
IFJFRw== 23337
2YY= 23338
Ymlh 23339
IGNvbXBpbGF0aW9u 23340
cm9zdA== 23341
IFZQ 23342
IFNjaG5l 23343
MjAxOQ== 23344
IGNvcHlpbmc= 23345
TU9SRQ== 23346
IEZsb3Jl 23347
ZmFsbHM= 23348
MjE1 23349
dG90YWw= 23350
IGRpc2NpcGxlcw== 23351
ZG91Ymxl 23352
IGV4Y2VlZGluZw== 23353
IHNtYXNoZWQ= 23354
IGNvbmNlcHR1YWw= 23355
IFJvbWFuaWE= 23356
IEJyZW50 23357
IElDRQ== 23358
IFRvdQ== 23359
IGdyYXA= 23360
IG5haWxz 23361
MTg5 23362
44OY 23363
IHByb2N1cmU= 23364
ZXVy 23365
IGNvbmZpcm1pbmc= 23366
IENlYw== 23367
YXdp 23368
IEVkZW4= 23369
IG5n 23370
IGVuZ2luZWVyZWQ= 23371
YXRpY3M= 23372
IGhvb2tlZA== 23373
IGRpc2d1c3Rpbmc= 23374
IE11cmRlcg== 23375
44K/ 23376
TGlicmFyeQ== 23377
IDE2OA== 23378
QWxtb3N0 23379
aGVtYXRpYw== 23380
TWVudQ== 23381
IE5vdHJl 23382
IEp1cg== 23383
IGtpZG5hcHBlZA== 23384
IGhhY2tlcg== 23385
IEphZGU= 23386
IGNyZWVweQ== 23387
IGRyYXdpbmdz 23388
IFNwb25zb3I= 23389
IGN5Y2xpc3Rz 23390
IEdvYmxpbg== 23391
IG9wdGltaXplZA== 23392
IHN0YWdlZA== 23393
IE1jRA== 23394
YmV0d2Vlbg== 23395
QWdl 23396
ZW5v 23397
U2V4 23398
IFdpZGU= 23399
bmluZ3M= 23400
YXZpcw== 23401
IGluY2FwYWJsZQ== 23402
IEtvYg== 23403
IHJld2FyZGluZw== 23404
IExvbmU= 23405
b2xlc2NlbnQ= 23406
IGNvbnRyYWN0ZWQ= 23407
IHN0aWNreQ== 23408
Sm9zZQ== 23409
QmFsbA== 23410
ZmVzdA== 23411
IElucHV0 23412
IFJlY2VudGx5 23413
IHRvbWF0 23414
c3F1YXJl 23415
QXBwbGljYXRpb24= 23416
IG5pdHJvZ2Vu 23417
IGR1cGxpY2F0ZQ== 23418
IFJlY29u 23419
IERlYXI= 23420
TG9uZG9u 23421
IGludHJh 23422
IGRvY2s= 23423
IG91dHJlYWNo 23424
IE1pbGxpb24= 23425
IG1hbW1hbHM= 23426
YW1wdG9u 23427
VkFM 23428
IHNuYXBz 23429
IGRvcw== 23430
IFdob2xl 23431
IFJlYWR5 23432
VHJ5 23433
IFdpbm5pcGVn 23434
ZWFyYW5jZQ== 23435
IGluY3VycmVk 23436
cmVuY2hlZA== 23437
IE5TVw== 23438
aWxvdA== 23439
cmFpbmU= 23440
IGN1YmU= 23441
Z290 23442
IHJ1bndheQ== 23443
ZXRlcm1pbmVk 23444
IEhhd2tz 23445
IHN1cnZpdm9y 23446
IFdpc2g= 23447
IERpbg== 23448
IERFRg== 23449
IFZhdWx0 23450
MTg3 23451
IG11c2hyb29tcw== 23452
IGNyaXNw 23453
YmV5 23454
IERpc2NvdmVyeQ== 23455
IGRldmVsb3BtZW50YWw= 23456
IHBhcmFkaWdt 23457
IGNoYW90aWM= 23458
IFRzdQ== 23459
IDMzMw== 23460
Ym9ucw== 23461
IGJhY3RlcmlhbA== 23462
IGNvbW1pdHM= 23463
IGNvc21pYw== 23464
IG1lZ2E= 23465
b2NhdGl2ZQ== 23466
IFBhaW50 23467
b3Bob2JpYw== 23468
IHZhaW4= 23469
IGNhcnZlZA== 23470
IFRoaWVm 23471
IEd1bA== 23472
b3dzaGlw 23473
IGNpdGVz 23474
IEVkaW5idXJnaA== 23475
IGRpbWluaXNoZWQ= 23476
IGFja25vd2xlZGdlcw== 23477
IEtpbGxz 23478
IG1pY3Jvdw== 23479
IEhlcmE= 23480
IHNlbmlvcnM= 23481
IHdoZXJlYnk= 23482
SG9w 23483
YXRyb24= 23484
IHVuYXZhaWxhYmxl 23485
IE5hdGU= 23486
IDQ4MA== 23487
IHNsYXRlZA== 23488
IFJlYmVjY2E= 23489
IEJhdHRlcnk= 23490
IGdyYW1tYXI= 23491
IGhlYWRzZXQ= 23492
IGN1cnNvcg== 23493
IGV4Y2x1ZGluZw== 23494
YW55ZQ== 23495
YXVuZGVyaW5n 23496
ZWJpbg== 23497
IGZlYXNpYmxl 23498
IFB1Ymxpc2hpbmc= 23499
IExhYnM= 23500
IENsaWZm 23501
IEZlcnJhcmk= 23502
IHBhYw== 23503
dmlzaWJsZQ== 23504
bWFya2Vk 23505
cGVsbA== 23506
IHBvbGl0ZQ== 23507
IHN0YWdnZXJpbmc= 23508
IEdhbGFjdGlj 23509
IHN1cGVyc3Q= 23510
IHBhcmFu 23511
IE9mZmljZXJz 23512
44CB 23513
IHNwZWNpZmljcw== 23514
dWx1cw== 23515
MjM5 23516
IFBhc3Rl 23517
QU1Q 23518
IFBhbmFtYQ== 23519
IERlbGV0ZQ== 23520
YW5ndWFyZA== 23521
cmVzdHJpYWw= 23522
IGhlcm9pYw== 23523
IER5 23524
2KfZhA== 23525
IGluY3VtYmVudA== 23526
IGNydW5jaA== 23527
dHJv 23528
IHNjb29w 23529
IGJsb2dnZXI= 23530
IHNlbGxlcnM= 23531
dXJlbg== 23532
IG1lZGljaW5lcw== 23533
IENhcHM= 23534
IEFuaW1hdGlvbg== 23535
b3h5 23536
IG91dHdhcmQ= 23537
IGlucXVpcmllcw== 23538
MjI5 23539
IHBzeWNob2xvZ2lzdA== 23540
IFNhc2s= 23541
ZXZpbA== 23542
IGNvbnRhbWluYXRlZA== 23543
44Ko 23544
aGVyZW5jZQ== 23545
IGJyYW5kZWQ= 23546
IEFiZHVs 23547
emg= 23548
IHBhcmFncmFwaHM= 23549
IG1pbnM= 23550
IGNvcnJlbGF0ZWQ= 23551
ZXJi 23552
IGltcGFydA== 23553
IG1pbGVzdG9uZQ== 23554
IFNvbHV0aW9ucw== 23555
b3RsZQ== 23556
IHVuZGVyY292ZXI= 23557
IG1hcmNoZWQ= 23558
IENoYXJnZXJz 23559
ZmF4 23560
IFNlY3JldHM= 23561
IHJ1dGg= 23562
d2VhdGhlcg== 23563
IGZlbWluaW5l 23564
IHNoYW0= 23565
IHByZXN0aWdpb3Vz 23566
aWdnaW5z 23567
IHN1bmc= 23568
aGlzdG9yeQ== 23569
ZXR0bGU= 23570
Z2dpZQ== 23571
IG91dGRhdGVk 23572
b2xhbmQ= 23573
IHBlcmNlcHRpb25z 23574
IFNlc3Npb24= 23575
IERvZGdlcnM= 23576
dWo= 23577
IEVORA== 23578
RG9j 23579
IGRlZmljaWVuY3k= 23580
R3JhbmQ= 23581
IEpva2Vy 23582
IHJldHJvc3BlY3Q= 23583
IGRpYWdub3N0aWM= 23584
IGhhcm1sZXNz 23585
IHJvZ3Vl 23586
IEF2YWw= 23587
RXF1 23588
IHRyYW5zYw== 23589
IFJvYmVydHNvbg== 23590
IERlcGVuZGluZw== 23591
IEJ1cm5z 23592
aXZv 23593
IGhvc3RpbGl0eQ== 23594
RmVhdHVyZXM= 23595
k5g= 23596
IGRpc2NvbWZvcnQ= 23597
IExDRA== 23598
c3BlY2lmaWVk 23599
IEV4cGVjdA== 23600
MzQw 23601
IGltcGVyYXRpdmU= 23602
IFJlZ3VsYXI= 23603
Q2hpbmVzZQ== 23604
IHN0YXRld2lkZQ== 23605
IHN5bW0= 23606
IGxvb3Bz 23607
IGF1dHVtbg== 23608
Tmljaw== 23609
IHNoYXBpbmc= 23610
IHF1b3Q= 23611
IGNoZXJyeQ== 23612
IENyb3NzcmVm 23613
6Kaa6YaS 23614
U3RhbmRhcmQ= 23615
aGVlZA== 23616
IERlbGw= 23617
IFZpZXRuYW1lc2U= 23618
IG9zdA== 23619
IFZhbGt5cmll 23620
T0E= 23621
QXNzYWQ= 23622
IHJlYm91bmQ= 23623
IFRyYWZmaWM= 23624
cGxhY2Vz 23625
5pg= 23626
IEJ1Yw== 23627
MTcy 23628
IHNoZWx0ZXJz 23629
IGluc2lzdGluZw== 23630
IENlcnRhaW5seQ== 23631
IEtlbm5ldGg= 23632
IFRDUA== 23633
IHBlbmFs 23634
IFJlcGxheQ== 23635
aGVhcmQ= 23636
IGRpYWxlY3Q= 23637
aXph 23638
IEZZ 23639
aXRjaGVy 23640
IERM 23641
IHNwaXJhbA== 23642
IHF1YXJ0ZXJiYWNrcw== 23643
IGh1bGw= 23644
IGdvb2dsZQ== 23645
IHRvZGQ= 23646
IFN0ZXJsaW5n 23647
IFBsYXRl 23648
IHNweWluZw== 23649
bWJvbA== 23650
IFJlYWxt 23651
IFByb2NlZA== 23652
IENyYXNo 23653
IHRlcm1pbmF0ZQ== 23654
IHByb3Rlc3Rpbmc= 23655
Q2VudGVy 23656
Z3VpZGVk 23657
IHVuY292ZXI= 23658
IGJveWNvdHQ= 23659
IHJlYWxpemVz 23660
c291bmQ= 23661
IHByZXRlbmRpbmc= 23662
IFZhcw== 23663
MTk4MA== 23664
IGZyYW1lZA== 23665
IDEzOQ== 23666
IGRlc2NlbmRlZA== 23667
IHJlaGFiaWxpdGF0aW9u 23668
IGJvcnJvd2luZw== 23669
IEJ1Y2g= 23670
IGJsdXI= 23671
Um9u 23672
IEZyb3plbg== 23673
ZW56YQ== 23674
Q2hpZWY= 23675
IFBvb3I= 23676
IHRyYW5zbGF0ZXM= 23677
TUlO 23678
IDIxMg== 23679
SkVDVA== 23680
IGVydXB0ZWQ= 23681
IHN1Y2Nlc3Nlcw== 23682
U0VD 23683
IHBsYWd1ZQ== 23684
IGdlbXM= 23685
ZG9tcw== 23686
IHN0cmV0Y2hlcw== 23687
IFNweQ== 23688
IHN0b3J5dGVsbGluZw== 23689
Q3JlZGl0 23690
IFB1c2g= 23691
IHRyYWN0aW9u 23692
IGluZWZmZWN0aXZl 23693
IEx1bmE= 23694
IHRhcGVz 23695
IGFuYWx5dGljcw== 23696
ZXJjaXNl 23697
IHByb2dyYW1tZXM= 23698
IENhcmJvbg== 23699
IGJlaG9sZA== 23700
aGVhdnk= 23701
IENvbnNlcnZhdGlvbg== 23702
IEZJUg== 23703
IHNhY2s= 23704
dGVybWlu 23705
cmlja3M= 23706
IGhvdXNlZA== 23707
IHVudXN1YWxseQ== 23708
SWNl 23709
IGV4ZWN1dGluZw== 23710
IE1vcm9j 23711
ZWRheQ== 23712
IGVkaXRpb25z 23713
IHNtYXJ0ZXI= 23714
IEJB 23715
IG91dGxhdw== 23716
IHZhbmlzaGVk 23717
aWJh 23718
QUxTRQ== 23719
IFNpbHZh 23720
MjM4 23721
Q291bGQ= 23722
IHBoaWxvc29waGVy 23723
IGV2YWN1YXRlZA== 23724
U2VjcmV0 23725
MTQy 23726
IHZpc2Fz 23727
44Ks 23728
IE1hbHQ= 23729
IENsZWFybHk= 23730
IE5pZ2Vy 23731
IENhaXJv 23732
IEZpc3Q= 23733
Mzgw 23734
IFhNTA== 23735
YXV0bw== 23736
aXRhbnQ= 23737
IHJlaW5mb3JjZWQ= 23738
UmVjb3Jk 23739
IFN1cnZpdm9y 23740
R0h6 23741
IHNjcmV3cw== 23742
cGFyZW50cw== 23743
IG9jZWFucw== 23744
bWFyZXM= 23745
IGJyYWtlcw== 23746
dmFzaXZl 23747
IGhlbGxv 23748
IFNJTQ== 23749
cmltcA== 23750
IG9yZQ== 23751
IEFybW91cg== 23752
MjQ3 23753
IHRlcnJpZmlj 23754
IHRvbmVz 23755
MTQx 23756
IE1pbnV0ZXM= 23757
RXBpc29kZQ== 23758
IGN1cnZlcw== 23759
IGluZmxhbW1hdG9yeQ== 23760
IGJhdHRpbmc= 23761
IEJlYXV0aWZ1bA== 23762
TGF5 23763
IHVucG9w 23764
dmFibGU= 23765
IHJpb3Rz 23766
IFRhY3RpY3M= 23767
YmF1Z2g= 23768
IENvY2s= 23769
IG9yZ2FzbQ== 23770
IFNhcw== 23771
IGNvbnN0cnVjdG9y 23772
ZXR6 23773
R292 23774
IGFudGFnb24= 23775
IHRoZWF0 23776
IGRlZWRz 23777
aGFv 23778
Y3V0cw== 23779
IE1jQ2w= 23780
IHVt 23781
IFNjaWVudGlzdHM= 23782
IGdyYXNzcm9vdHM= 23783
eXNzZXk= 23784
Il09Pg== 23785
IHN1cmZhY2Vk 23786
IHNoYWRlcw== 23787
IG5laWdoYm91cnM= 23788
IGFkdmVydGlz 23789
b3lh 23790
IG1lcmdlZA== 23791
VXBvbg== 23792
IGdhZA== 23793
IGFudGljaXBhdGU= 23794
QW55d2F5 23795
IHNsb2dhbg== 23796
IGRpc3Jlc3BlY3Q= 23797
SXJhbg== 23798
IFRC 23799
YWN0ZWQ= 23800
IHN1YnBvZW4= 23801
bWVkaWF0ZWx5 23802
T09PTw== 23803
IHdhaXZlcg== 23804
IHZ1bG5lcmFiaWxpdGllcw== 23805
b3R0ZXN2aWxsZQ== 23806
IEh1ZmZpbmd0b24= 23807
Sm9zaA== 23808
IERI 23809
TW9uZGF5 23810
IEVsbGVu 23811
S25vdw== 23812
eG9u 23813
aXRlbXM= 23814
MjI4 23815
IGZpbGxz 23816
IE5pa2U= 23817
IGN1bXVsYXRpdmU= 23818
YW5kYWxz 23819
SXI= 23820
IOw= 23821
IGZyaWN0aW9u 23822
aWdhdG9y 23823
IHNjYW5z 23824
IFZpZW5uYQ== 23825
bGRvbQ== 23826
IHBlcmZvcm1lcnM= 23827
UHJpbQ== 23828
IGJpZGRpbmc= 23829
TXVy 23830
IGxlYW5lZA== 23831
IFByaXg= 23832
YWxrcw== 23833
IFvigKZd 23834
IFR3aXRjaA== 23835
IERldmVsb3Blcg== 23836
IEdpcg== 23837
IGNhbGxiYWNr 23838
QWJzdHJhY3Q= 23839
IGFjY3VzdG9tZWQ= 23840
IGZyZWVkb21z 23841
IFBH 23842
dXJhY3k= 23843
IGx1bXA= 23844
aXNtYW4= 23845
LCwsLA== 23846
MTk5Mg== 23847
IFJFRA== 23848
IHdvcm0= 23849
TWF0Y2g= 23850
IFBsYXRpbnVt 23851
SUo= 23852
IE93bmVy 23853
VHJpdmlh 23854
Y29tcGw= 23855
IG5ld2Jvcm4= 23856
IGZhbnRhcw== 23857
T3du 23858
IDE5NTk= 23859
IHN5bXBhdGg= 23860
IHViaXF1 23861
IG91dHB1dHM= 23862
IGFsbGV2 23863
IHByYWc= 23864
S2V2aW4= 23865
IGZhdm9ycw== 23866
IGJ1cmlhbA== 23867
IG51cnQ= 23868
c29sZXRl 23869
Y2FjaGU= 23870
IDE1Ng== 23871
IHVubG9ja3M= 23872
dGVjaG4= 23873
TWFraW5n 23874
IGNvbnF1ZXI= 23875
YWRpYw== 23876
5pY= 23877
IGVsZg== 23878
IGVsZWN0b3JhdGU= 23879
IEt1cmRz 23880
IFN0YWNr 23881
IFNhbXVyYWk= 23882
IOKYhQ== 23883
IHt9 23884
IFNhaWQ= 23885
IEZhbGxvdXQ= 23886
IGtpbmRuZXNz 23887
IEN1c3RvbXM= 23888
IEJvdWxldmFyZA== 23889
IGhlbGljb3B0ZXJz 23890
b3RpY3M= 23891
IFZlZ2V0 23892
Y29tbWVudA== 23893
IGNyaXRpY2lzZWQ= 23894
IHBvbGlzaGVk 23895
IFJlbWl4 23896
IEN1bHR1cmFs 23897
IHJlY29ucw== 23898
IGRvaQ== 23899
YXRlbQ== 23900
U2NyZWVu 23901
IGJhcnJlZA== 23902
Q29tbWVudHM= 23903
IEdlbmVyYWxseQ== 23904
IHNsYXA= 23905
NzIw 23906
VmFyaQ== 23907
cGluZQ== 23908
IGVtcHQ= 23909
IGhhdHM= 23910
IFBsYXlpbmc= 23911
bGFi 23912
YXZlcmFnZQ== 23913
Zm9ybXM= 23914
IENvdHRvbg== 23915
IGNhbnM= 23916
IERPTg== 23917
IFNvbWFsaWE= 23918
Q3J5cHQ= 23919
IEluY3JlYXNlcw== 23920
RXZlcg== 23921
bW9kZXJu 23922
IHN1cmdlb24= 23923
MzAwMA== 23924
IHJhbmRvbWl6ZWQ= 23925
PT09PT09PT09PT09PT09PT09PT09PT09PT09PT09PT09PT09PT09PT09PT09PT09PT09PT09PT09PT09PT09PQ== 23926
QmVybg== 23927
aW1wbA== 23928
IENPUg== 23929
IHByb2NsYWlt 23930
dGhvdXNl 23931
IHRvZXM= 23932
IGFtcGxl 23933
IHByZXNlcnZpbmc= 23934
IGRpc2JlbA== 23935
Z3JhbmQ= 23936
QmVzaWRlcw== 23937
IHNpbGs= 23938
IFBhdHRlcm4= 23939
aG0= 23940
IGVudGVycHJpc2Vz 23941
IGFmZmlkYXZpdA== 23942
IEFkdmlzb3J5 23943
IGFkdmVydGlzZWQ= 23944
IFJlbGlnaW91cw== 23945
c2VjdGlvbnM= 23946
cHN5Y2g= 23947
IEZpZWxkcw== 23948
YXdheXM= 23949
IGhhc2h0YWc= 23950
IE5pZ2h0bWFyZQ== 23951
IHZhbXBpcmU= 23952
IGZvcmVuc2lj 23953
cm9zc292ZXI= 23954
bmFy 23955
IG5hdnk= 23956
IHZhY2FudA== 23957
IER1ZWw= 23958
IGhhbGx3YXk= 23959
IGZhY2Vib29r 23960
aWRlbnRhbGx5 23961
IE5SQQ== 23962
IG1hdHQ= 23963
IGh1cnJpY2FuZQ== 23964
IEtpcmJ5 23965
IFB1enpsZQ== 23966
IHNraXJ0 23967
b3VzdA== 23968
ZHVsbGFo 23969
IGFuYWxvZ3k= 23970
aW5pb24= 23971
IHRvbWF0b2Vz 23972
IE5W 23973
IFBlYWs= 23974
IE1leWVy 23975
IGFwcG9pbnRtZW50cw== 23976
IG1hc2M= 23977
IGFsbGV5 23978
cmVoZW5k 23979
IGNoYXJpdGllcw== 23980
IHVuZG8= 23981
IGRlc3RpbmF0aW9ucw== 23982
IFRlc3Rpbmc= 23983
Ij48Lw== 23984
IGRlc3RpbmVk 23985
IGltcGxlbWVudHM= 23986
IEhhcm9sZA== 23987
UkVDVA== 23988
IG9wdGltaXphdGlvbg== 23989
IGtpbG9tZXRyZXM= 23990
IGNtZA== 23991
IGltcGFpcm1lbnQ= 23992
IHVuc3VjY2Vzc2Z1bA== 23993
IHN3aWZ0bHk= 23994
IEdsYXNnb3c= 23995
YXJ0ZW4= 23996
IFNoYXJlcw== 23997
IEFuc3dlcg== 23998
IEFsYnVt 23999
IG51dHJpdGlvbmFs 24000
44OW 24001
IEZ1dA== 24002
IGJsb2M= 24003
IE5GQw== 24004
IHdob2xlc2FsZQ== 24005
IENX 24006
IG5lZ2xlY3RlZA== 24007
IGxhdW5jaGVy 24008
IGFubm91bmNlbWVudHM= 24009
T1VMRA== 24010
Y29tYg== 24011
IHJvdGF0aW5n 24012
IHJlc3Rz 24013
IFRpY2tldA== 24014
Y2hlZGVs 24015
TG91 24016
IFZpYw== 24017
ICIn 24018
IHRlbXBsYXRlcw== 24019
IHJlcGxhY2Vz 24020
QXJj 24021
Ojo6Og== 24022
IEdpbGJlcnQ= 24023
IGlsbG5lc3Nlcw== 24024
IHNjaGVkdWxlcw== 24025
IGhldGVyb3NleHVhbA== 24026
TElORQ== 24027
IGhlcmVpbg== 24028
IGNvZXJj 24029
IGRlY3JlYXNpbmc= 24030
IGRlcG9ydGF0aW9u 24031
c3Vkbw== 24032
IEluZGlnZW5vdXM= 24033
IHdlaWdocw== 24034
QWxvbmc= 24035
Jyk7 24036
IEJlbmdhbHM= 24037
NzA3 24038
IGpvaW50cw== 24039
dmVydHM= 24040
IDE0OQ== 24041
bmFpcmU= 24042
IHNpbXBsZXN0 24043
IGxvcmU= 24044
MTA4MA== 24045
ZmljdGlvbg== 24046
IERhdGFiYXNl 24047
IHJlc2VydmF0aW9u 24048
IHNvdQ== 24049
IHNhbmN0dWFyeQ== 24050
YXVkaW8= 24051
YXBsZQ== 24052
IHZlZ2V0YXJpYW4= 24053
IGFudGljaXBhdGlvbg== 24054
bWljcm8= 24055
IGVuZHVyaW5n 24056
IGRlcGFydGVk 24057
IHNpZGV3YWxr 24058
IHByb2hpYml0cw== 24059
IEZvbnQ= 24060
IGNvbXB1dGU= 24061
IFNlY3Q= 24062
IDE1OA== 24063
QmF0dGxl 24064
IGJvbWJlcg== 24065
IGRpc3RyYWN0aW9u 24066
IGVuZHVyZWQ= 24067
IHByYWN0aXRpb25lcnM= 24068
IGRpc3R1cmJlZA== 24069
IGRyYW5r 24070
b3JkZXJlZA== 24071
IHN1cnByaXNlcw== 24072
c2VhdA== 24073
U2VjdXJpdHk= 24074
IFdpc2RvbQ== 24075
b2dv 24076
IHN1YnBhcmFncmFwaA== 24077
IFBlbmluc3VsYQ== 24078
IE9yaWdpbnM= 24079
aXJlbg== 24080
IFBhdg== 24081
aWdnbGU= 24082
IGdyYXRpdHVkZQ== 24083
IEdyYXZpdHk= 24084
b3ZlcnR5 24085
aW1hbg== 24086
Y3Ry 24087
IENhZXNhcg== 24088
Y291bGQ= 24089
Z2Vt 24090
IHNraWVz 24091
IGNoYW1w 24092
IGFncmVlaW5n 24093
RmFtaWx5 24094
RGl2 24095
MTc2 24096
IG1lc3N5 24097
dW1wdGlvbg== 24098
RmVkZXJhbA== 24099
ZXJubw== 24100
IENoYXQ= 24101
QmV5b25k 24102
IGRldm90ZQ== 24103
IFdhbHNo 24104
IGR1bXBlZA== 24105
IGFjY3VtdWxhdGlvbg== 24106
c3RhZA== 24107
aGliaXRpb24= 24108
IHNtb2tlcnM= 24109
IGluc3BlY3Rvcg== 24110
RnJlbmNo 24111
aXNzYW4= 24112
IFZpdGE= 24113
IHJlc2VhcmNoaW5n 24114
UkFN 24115
IENlbHRpY3M= 24116
IGNsb2Fr 24117
IFRlcnJh 24118
TWFyeQ== 24119
c29sZA== 24120
IERPTQ== 24121
bW9kcw== 24122
SW50ZWw= 24123
IG11bHRpdHVkZQ== 24124
IEltcHJvdmVk 24125
IHJlbGlhbmNl 24126
IGFydGlmYWN0 24127
IGFsYXJtaW5n 24128
UHJvbQ== 24129
aG9u 24130
VElPTg== 24131
bWVkaXVt 24132
IHJlZmxleA== 24133
IEV4Y2Vs 24134
IHdlYWtlbmVk 24135
MTYz 24136
MjI0 24137
IGNvc3R1bWVz 24138
IHVuaXF1ZWx5 24139
IHNvcnJvdw== 24140
IG1hbnNpb24= 24141
d3A= 24142
IHNhbHY= 24143
IEdyb3Zl 24144
YnNw 24145
IFNuaXBlcg== 24146
IFNoaXBwaW5n 24147
IFBPVw== 24148
IHVuZGlz 24149
IGJyYW5kaW5n 24150
R2lybA== 24151
IEFobWFk 24152
IExha2Vz 24153
IENvcmV5 24154
IGluaGVyaXRhbmNl 24155
ZW5lcnk= 24156
IHBhY2tpbmc= 24157
IFByZXN0 24158
RGVzdA== 24159
Rlc= 24160
IHJlZ3VsYXRvcg== 24161
bG9ja2Vk 24162
IGNvbnRlc3RlZA== 24163
IE1lbGlzc2E= 24164
IER1Yw== 24165
IHVucG9wdWxhcg== 24166
IHN0YWNrZWQ= 24167
IDE5MTc= 24168
IHllYXJseQ== 24169
IHN0YXJl 24170
IGFzc2Vzc2luZw== 24171
w7g= 24172
IGJldmVyYWdlcw== 24173
IGNvbXBldGl0aW9ucw== 24174
IHN0cmVuZ3RoZW5pbmc= 24175
YWxvbmc= 24176
IEx1ZA== 24177
IG1lbHRlZA== 24178
c3RhbmJ1bA== 24179
IGJvdW50eQ== 24180
RU5D 24181
IExhbmRz 24182
IGRlY2xhcmVz 24183
IGN1c3RvbWl6ZQ== 24184
IGNvbXBvc2l0ZQ== 24185
44Os 24186
Q00= 24187
b2dyYXBoaWNz 24188
IFRlbXA= 24189
IGNvbnRlbmRlcg== 24190
IGluc2lnbg== 24191
IExBTg== 24192
IGRpc2FzdGVycw== 24193
aW5zcGlyZWQ= 24194
IGp1ZGdtZW50cw== 24195
dXN0YWluYWJsZQ== 24196
dXJzaW9u 24197
IHZhcmlhbmNl 24198
IFVsdGltYXRlbHk= 24199
IC0tLS0tLS0t 24200
dWFkb3I= 24201
IFJY 24202
IG1lbHRpbmc= 24203
IEV4dGVuZGVk 24204
IFR3ZQ== 24205
TWFqb3I= 24206
IEJpbA== 24207
IHN5cnVw 24208
cXVpY2s= 24209
IEhvbGRlcg== 24210
IGlubm9jZW5jZQ== 24211
VUxF 24212
IE1pZ2h0 24213
OTk5OQ== 24214
IGZhbA== 24215
IGNvbnRpbnVpdHk= 24216
IDE5NTM= 24217
IEJT 24218
c3RpbGw= 24219
TGF0 24220
IEFidXNl 24221
IHVuc3VwcG9ydGVk 24222
eHh4eHh4eHg= 24223
IGluc3RpdHV0ZQ== 24224
IGZyYWdtZW50 24225
IFBlcA== 24226
V2VzdGVybg== 24227
IENhdXNl 24228
IEZyYWc= 24229
IEFycw== 24230
4KU= 24231
YXN0aWNz 24232
IGJpc2hvcA== 24233
IGNyb3NzZXM= 24234
IDE1NA== 24235
IFVwZ3JhZGU= 24236
IG1pdGlnYXRl 24237
IFJheW1vbmQ= 24238
TW9kcw== 24239
IHRvbWF0bw== 24240
IHN0dW1ibGVk 24241
IGRpZmZlcnM= 24242
SW5pdGlhbA== 24243
IFJhc3BiZXJyeQ== 24244
IGlnbm9yZXM= 24245
IHRhbnQ= 24246
w6A= 24247
IHJlbGF5 24248
IGJpc2V4dWFs 24249
IGNvbmZlc3Npb24= 24250
IGRlbWVudA== 24251
aW5hcw== 24252
IEhlYXRoZXI= 24253
cGxhdGZvcm0= 24254
ZHJpdmluZw== 24255
Ym91cmc= 24256
IE11c2g= 24257
IGh5c3Rlcg== 24258
RGV0YWlscw== 24259
IGRyaWZ0 24260
IFdhbGQ= 24261
IEx1Y2tpbHk= 24262
b3Jm 24263
IGV4cGlyZQ== 24264
IFB1bmNo 24265
enltZQ== 24266
Z29sZA== 24267
IHVucGFpZA== 24268
IFRyZW50 24269
IHVuYXJtZWQ= 24270
IGlsbGljaXQ= 24271
IFRvdHRlbmhhbQ== 24272
IHNtYXNo 24273
SW50ZXJuYXRpb25hbA== 24274
aW5rZXI= 24275
IHN0aW5n 24276
IFNhZGRhbQ== 24277
IEFSVA== 24278
IHRydXRocw== 24279
YmlydGg= 24280
IHNvYmVy 24281
IE5pdA== 24282
IGli 24283
IHVzYWJsZQ== 24284
IHN0YWNrcw== 24285
IFN5bHY= 24286
IG5vcnRoZWFzdA== 24287
IGRvbWluYXRpb24= 24288
IE1vdXI= 24289
RU5TRQ== 24290
IE1lYXN1cmU= 24291
IHByb2dyYW1tZXI= 24292
IDwt 24293
MTgy 24294
IENvbmRpdGlvbg== 24295
IGJhY2t5YXJk 24296
aXJsaW5n 24297
IEplYg== 24298
IENyZWVk 24299
IEhhbmc= 24300
IENPTVA= 24301
RkVS 24302
IElzaA== 24303
IGRldGVjdGl2ZXM= 24304
LS0tLS0tLS0tLS0tLS0t 24305
IE1lc3Nlbmdlcg== 24306
IGxvb3Bo 24307
IGdhdGV3YXk= 24308
MTUx 24309
IE1hdGVyaWFscw== 24310
IERU 24311
IGRvb21lZA== 24312
b2Rv 24313
IHNsaWNlcw== 24314
IGVtYWlsZWQ= 24315
IFBlcmw= 24316
IHJlbm92 24317
VVRI 24318
b2R5bmFt 24319
IFNvdXRod2VzdA== 24320
Z2V0aWM= 24321
IFRQUA== 24322
IG9wdGltaXNt 24323
IFRvdw== 24324
dWxhdG9ycw== 24325
cHJvdGVjdGVk 24326
eWxlcw== 24327
wqs= 24328
IGV4aWxl 24329
ZW52 24330
UHJvcA== 24331
IFppbW1lcm1hbg== 24332
2Y4= 24333
Q2E= 24334
b21hbHk= 24335
44OG 24336
IHJhaWxyb2Fk 24337
TGVl 24338
MjMy 24339
IHJlcGxpY2F0ZQ== 24340
IGNvbWZvcnRhYmx5 24341
YWN0bHk= 24342
IHJhdg== 24343
IHRlbGVzY29wZQ== 24344
IGhvbmVzdHk= 24345
IFBlcHBlcg== 24346
IEJyaW5n 24347
IHJpY2hlc3Q= 24348
IG91dGRvb3Jz 24349
IGhhbGxz 24350
IGNvbnRlbmQ= 24351
SVNF 24352
IHN1Ym1pdHRpbmc= 24353
IG5haXZl 24354
YXJhdGlvbnM= 24355
IDE0Mw== 24356
IHBvaXNlZA== 24357
cmVzcG9uc2libGU= 24358
IHNvY2tz 24359
IFNrdWxs 24360
UXVlc3Rpb24= 24361
IGRpc2NvdmVyaWVz 24362
Sm9pbmVk 24363
IEVuZW1pZXM= 24364
IFdpcmVsZXNz 24365
IFJldmVuZ2U= 24366
IHB1enpsZXM= 24367
IGNlYXNlZA== 24368
Mjkw 24369
Y3JpcHRpb25z 24370
IENvbnNvbGU= 24371
IGJvaWxpbmc= 24372
IGRpc2NyZXA= 24373
IGRlZHVjdGlvbg== 24374
IGFyc2VuYWw= 24375
WFhYWA== 24376
IEFtc3RlcmRhbQ== 24377
cm94aW1hdGVseQ== 24378
IFNoYW5l 24379
IHBvc2luZw== 24380
IEFDTFU= 24381
IENvbXBhbmllcw== 24382
IHRoZW9sb2d5 24383
IFVn 24384
cXVhcnRlcg== 24385
IEhhbms= 24386
Q29pbg== 24387
IEx2 24388
IGFsbGVnYXRpb24= 24389
IEF2b2lk 24390
IGluZGVmaW5pdGVseQ== 24391
IGNvbW1vZGl0aWVz 24392
IGJyaWc= 24393
IE1hbml0 24394
IHRlbnRo 24395
bWV0aG9k 24396
IEtuaWNrcw== 24397
IOKAjg== 24398
IGludm9rZWQ= 24399
RGlhbA== 24400
QVJB 24401
IGNhdWN1cw== 24402
MjI3 24403
IEphYg== 24404
IG91bmNlcw== 24405
YmF5 24406
IGJ1ZGR5 24407
ZmFu 24408
MjM0 24409
IEhpbA== 24410
YWRo 24411
IFRZ 24412
IElORA== 24413
IDE5Mzk= 24414
IGl0ZXJhdGlvbg== 24415
IEdvbnphbGV6 24416
IFZlcnQ= 24417
IElP 24418
ZW1i 24419
cmVyYQ== 24420
ZW5jaA== 24421
IFJlcXVpcmVtZW50cw== 24422
IFdpbnM= 24423
IGxpdmVzdG9jaw== 24424
aG91cnM= 24425
IuKApg== 24426
YnJhbA== 24427
TWFyZw== 24428
IERvbmU= 24429
IHdhc3Rpbmc= 24430
aW5nZWQ= 24431
Z3JvdXBz 24432
IHdpc2hpbmc= 24433
IFR1bWJscg== 24434
IHRhcHBpbmc= 24435
IG5hdGlvbmFsaXNt 24436
IEJ5cg== 24437
IHNxdWFyZXM= 24438
IEFjdGlvbnM= 24439
44Ol 24440
SW5zaWRl 24441
ZGVidWc= 24442
IGFwcGVuZA== 24443
IHN0dWJib3Ju 24444
IENpbmQ= 24445
VGVsbA== 24446
IHRlYXJpbmc= 24447
IFJleQ== 24448
b3Jj 24449
IERheXRvbg== 24450
IE5I 24451
IE1hZG5lc3M= 24452
Q2hhcmw= 24453
IE1vcnJpc29u 24454
ZmlsdGVy 24455
IGFjY3VzZQ== 24456
IC4v 24457
IHRvcnJlbnQ= 24458
IGRlY2xpbmVz 24459
Z2FsbGVyeQ== 24460
TWluZQ== 24461
IG5lZ290aWF0aW9u 24462
IEJhc2hhcg== 24463
b3BpYQ== 24464
MTk5Mw== 24465
ZW1vcnQ= 24466
IE5vdmVs 24467
IEZhbmc= 24468
ZXJzaXZl 24469
IEluc3RhbnQ= 24470
IHJvbGxlcg== 24471
QXJvdW5k 24472
IEVsZWN0aW9ucw== 24473
R2FtZXM= 24474
IGluZXhwZW5zaXZl 24475
IHdvcnM= 24476
IHZ1bA== 24477
IEhvbGU= 24478
IHVuYmVsaWV2YWJsZQ== 24479
IG5hdXNl 24480
IGVudHI= 24481
Ym9hdA== 24482
IFNURQ== 24483
IGJ1c2g= 24484
IEhhc3Nhbg== 24485
IHdv 24486
IHBhdXNlZA== 24487
IE1pZw== 24488
bGl2ZWQ= 24489
IHNjb3V0 24490
IGxpdGg= 24491
UHVibGlzaGVk 24492
ZHVpbm8= 24493
Y29vbA== 24494
IGNpcmN1bGF0aW5n 24495
aWRhcw== 24496
IFBhbQ== 24497
dmlvbGVudA== 24498
IENyYXdmb3Jk 24499
dWRkbGU= 24500
IExldHRlcnM= 24501
R3VhcmQ= 24502
bW9ycGg= 24503
IHdhbmRlcmluZw== 24504
IHNvcGhvbW9yZQ== 24505
IHF1ZWVy 24506
IEJsaW5k 24507
cnVl 24508
IE1hcnJpYWdl 24509
RG9t 24510
IHBhZGRpbmc= 24511
IGZvbGRlcnM= 24512
IG1lYW5pbmdsZXNz 24513
IGNhbmRpZGFjeQ== 24514
YWZvcnQ= 24515
IHdoaXN0bGVibA== 24516
IElkZW50aWZpZWQ= 24517
IGNpZ2Fy 24518
IGhpZA== 24519
IER1YmFp 24520
IHBvc3R1cmU= 24521
IGhpa2luZw== 24522
IFRlcm1pbmFs 24523
TGVnZW5kYXJ5 24524
IFRQ 24525
IEFUSw== 24526
IFN0YXJidWNrcw== 24527
IFJpb3Q= 24528
MTk5MQ== 24529
IEJvdHRvbQ== 24530
ZWZmaWM= 24531
IEV1Z2VuZQ== 24532
IFd5b21pbmc= 24533
IFJvY2t5 24534
IHNhbG1vbg== 24535
IG1ldHJv 24536
IGJpbGF0ZXJhbA== 24537
IGNlbGVicmF0ZXM= 24538
TGVuZ3Ro 24539
YmlsbGlvbg== 24540
QmF0 24541
IHJlbGVn 24542
IHBzZXVkbw== 24543
RFQ= 24544
IFJob2Rl 24545
UGFyZW50 24546
cGxldGlvbg== 24547
IGF0dHJpYnV0 24548
IHR1bmluZw== 24549
IE5PVEU= 24550
IFJlYmVs 24551
aWN1cw== 24552
RnVuZA== 24553
IGNvY2t0YWls 24554
IDUwMQ== 24555
IHNwb29u 24556
IGJydXRhbGl0eQ== 24557
IHVuaXRl 24558
IG1pY3JvYmk= 24559
IFJlaWNo 24560
cG9zaXRpdmU= 24561
IGFtYXplZA== 24562
IE5U 24563
RGVzYw== 24564
RUNUSU9O 24565
IGZhbHNlbHk= 24566
IEhpZ2hsYW5kZXI= 24567
IENyaXN0 24568
IFZpY3Rvcmlhbg== 24569
IGRpc3RyaWJ1dGlvbnM= 24570
dGhlaXI= 24571
IEVpbnN0ZWlu 24572
IHBvZA== 24573
IGVwaWRlbQ== 24574
IGhlYXA= 24575
IFJhbmNo 24576
IGFudGhlbQ== 24577
IHJlYXBw 24578
IEF1YnVybg== 24579
IGNvbmN1cnJlbnQ= 24580
IFRocm91Z2hvdXQ= 24581
IFBPU1Q= 24582
4pg= 24583
IGhvbWVtYWRl 24584
a2ljaw== 24585
QmVn 24586
IGNoYXNzaXM= 24587
Y291bnRlcg== 24588
IG1lcmdlcg== 24589
IGxhcHM= 24590
MjE3 24591
dW5pb24= 24592
IFRyaWdnZXI= 24593
IGRlYmF0ZWQ= 24594
IHNpbGVudGx5 24595
IHJlc3RyYWludA== 24596
QmFs 24597
MDAwMDAwMA== 24598
IGZvcm1pZGFibGU= 24599
IEZpbGlw 24600
IHNhY3JpZmljZXM= 24601
Rm9vZA== 24602
IGR3YXJm 24603
IFNlcXU= 24604
aW5pYW4= 24605
TW9yZW92ZXI= 24606
IHRhbmdpYmxl 24607
b3BzaXM= 24608
IE1pbmVjcmFmdA== 24609
IFJlZ2lzdHJhdGlvbg== 24610
b2Fu 24611
IHJlcHJlc2VudGF0aW9ucw== 24612
IHRoaXJzdA== 24613
IGNvcnA= 24614
aXJlbWVudA== 24615
TWFkZQ== 24616
bG9l 24617
PiI= 24618
Y2F0cw== 24619
Ki4= 24620
IGdlc3R1cmVz 24621
Z2VuZXJhbA== 24622
TGVhZ3Vl 24623
IHBhY2tldHM= 24624
IEluc3BlY3Rvcg== 24625
IEJlcmc= 24626
IGZyYXVkdWxlbnQ= 24627
IGNyaXRpY2l6ZQ== 24628
RnVu 24629
IGJsYW1pbmc= 24630
bmRyYQ== 24631
IHNsYXNo 24632
IEVzdG9u 24633
IHByb3Bvc2luZw== 24634
IHdoYWxlcw== 24635
IHRoZXJhcGlzdA== 24636
IHN1YnNldA== 24637
IGxlaXN1cmU= 24638
RUxE 24639
IENWRQ== 24640
IEFjdGl2aXR5 24641
IGN1bG1pbg== 24642
c2hvcA== 24643
IERBWQ== 24644
aXNjaGVy 24645
IEFkbWlyYWw= 24646
IEF0dGFja3M= 24647
IDE5NTg= 24648
IG1lbW9pcg== 24649
IGZvbGRlZA== 24650
IHNleGlzdA== 24651
IDE1Mw== 24652
IExJ 24653
IHJlYWRpbmdz 24654
IGVtYmFycmFzc21lbnQ= 24655
IEVtcGxveW1lbnQ= 24656
d2FydA== 24657
Y2hpbg== 24658
IGNvbnRpbnVhdGlvbg== 24659
bGlh 24660
UmVjZW50bHk= 24661
IGR1ZWw= 24662
IGV2YWN1YXRpb24= 24663
IEthc2htaXI= 24664
IGRpc3Bvc2l0aW9u 24665
IFJpZw== 24666
IGJvbHRz 24667
IGluc3VyZXJz 24668
NDY3 24669
TWV4 24670
IHJldGFsaWF0aW9u 24671
IG1pc2VyeQ== 24672
IHVucmVhc29uYWJsZQ== 24673
cmFpbmluZw== 24674
SW1t 24675
IFBV 24676
ZW1lcg== 24677
IGdlbml0YWw= 24678
44Kz 24679
IENhbmR5 24680
IG9uaW9ucw== 24681
IFBhdHQ= 24682
bGluZXI= 24683
IGNvbmNlZGVk 24684
IGZh 24685
IGZvcmM= 24686
IEhlcm5hbmRleg== 24687
IEdlb2Zm 24688
ZGViaWFu 24689
IFRlYW1z 24690
IGNyaWVz 24691
IGhvbWVvd25lcnM= 24692
MjM3 24693
QUJD 24694
IHN0aXRjaA== 24695
IHN0YXRpc3RpYw== 24696
IGhlYWRlcnM= 24697
IEJpb2xvZ3k= 24698
IG1vdG9ycw== 24699
IEdFTg== 24700
IExpcA== 24701
IGhhdGVz 24702
IGhlZWw= 24703
U2VsZg== 24704
aXBs 24705
RURJVA== 24706
b3J0aW5n 24707
IGFubm90 24708
IFNwZWVjaA== 24709
b2xkZW1vcnQ= 24710
IEphdmFzY3JpcHQ= 24711
IExlQnJvbg== 24712
IGZvb3RwcmludA== 24713
IGZu 24714
IHNlaXp1cmVz 24715
bmFz 24716
aGlkZQ== 24717
IDE5NTQ= 24718
IEJlZQ== 24719
IERlY2xhcmF0aW9u 24720
IEthdGll 24721
IHJlc2VydmF0aW9ucw== 24722
TlI= 24723
ZmVtYWxl 24724
IHNhdHVyYXRlZA== 24725
IGJpYmxpY2Fs 24726
IHRyb2xscw== 24727
RGV2aWNl 24728
cGhvdG9z 24729
IGRydW1z 24730
44OJ44Op44K044Oz 24731
TmlnaHQ= 24732
ZmlnaHRlcg== 24733
IEhhaw== 24734
cmliZXI= 24735
IGN1c2g= 24736
IGRpc2NpcGxpbmFyeQ== 24737
YmF1bQ== 24738
IEdI 24739
IFNjaG1pZHQ= 24740
aWxpYnJpdW0= 24741
IHNpeHR5 24742
IEt1c2huZXI= 24743
cm90cw== 24744
IHB1bmQ= 24745
IFJhYw== 24746
IHNwcmluZ3M= 24747
IGNvbnZl 24748
QnVzaW5lc3M= 24749
RmFsbA== 24750
IHF1YWxpZmljYXRpb25z 24751
IHZlcnNlcw== 24752
IG5hcmNpc3M= 24753
IEtvaA== 24754
IFdvdw== 24755
IENoYXJsb3R0ZXN2aWxsZQ== 24756
ZWRv 24757
IGludGVycm9nYXRpb24= 24758
IFdvb2w= 24759
MzY1 24760
QnJpYW4= 24761
IOKckw== 24762
IGFsbGVnZXM= 24763
b25kcw== 24764
aWRhdGlvbg== 24765
IEphY2tpZQ== 24766
eXU= 24767
IGxha2Vz 24768
IHdvcnRod2hpbGU= 24769
IGNyeXN0YWxz 24770
IEp1ZGE= 24771
IGNvbXByZWhlbmQ= 24772
IGZsdXNo 24773
IGFic29ycHRpb24= 24774
IE9D 24775
IGZyaWdodGVuZWQ= 24776
IENob2NvbGF0ZQ== 24777
TWFydGlu 24778
IGJ1eXM= 24779
IGJ1Y2tz 24780
IGFwcGVsbA== 24781
IENoYW1waW9uc2hpcHM= 24782
IGxpc3RlbmVy 24783
IERlZmVuc2l2ZQ== 24784
IGN6 24785
dWRz 24786
IE1hdGU= 24787
IHJlcGxheQ== 24788
IGRlY29yYXRlZA== 24789
IHN1bms= 24790
IFZJUA== 24791
IEFuaw== 24792
IDE5NQ== 24793
YWFhYQ== 24794
Tm9ib2R5 24795
IE1pbGs= 24796
IEd1cg== 24797
IE1r 24798
IFNhcmE= 24799
IHNlYXRpbmc= 24800
IFdpZA== 24801
VHJhY2s= 24802
IGVtcGxveXM= 24803
IGdpZ2FudGlj 24804
QVBQ 24805
44Kn 24806
aW52ZW50b3J5 24807
IHRvd2Vs 24808
YXRjaGU= 24809
bGFzdGluZw== 24810
IFRM 24811
IGxhdGVuY3k= 24812
IGtuZQ== 24813
QmVy 24814
bWVhbmluZw== 24815
IHVwaGVsZA== 24816
IHBsYXlncm91bmQ= 24817
IG1hbnQ= 24818
U2lkZQ== 24819
IHN0ZXJlbw== 24820
IG5vcnRod2VzdA== 24821
IGV4Y2VwdGlvbmFsbHk= 24822
IHJheXM= 24823
IHJlY3VycmluZw== 24824
RHJpdmU= 24825
IHVwcmlnaHQ= 24826
IGFiZHVjdA== 24827
IE1hcmF0aG9u 24828
IGdvb2RieWU= 24829
IGFscGhhYmV0 24830
aHA= 24831
IGNvdXJ0cm9vbQ== 24832
cmluZ3Rvbg== 24833
b3RoaW5n 24834
VGFn 24835
IGRpcGxvbWF0cw== 24836
IGJhcmJhcg== 24837
IEFxdWE= 24838
MTgz 24839
MzMzMw== 24840
IG1hdHVyaXR5 24841
IGluc3RhYmlsaXR5 24842
IEFwYWNoZQ== 24843
ID09PQ== 24844
IGZhc3Rpbmc= 24845
IEdyaWQ= 24846
TW9kTG9hZGVy 24847
IDE1Mg== 24848
QWJz 24849
IE9wZXJhdGluZw== 24850
ZXR0aQ== 24851
IGFjcXVhaW50 24852
RG9ubmVsbA== 24853
IEtlbQ== 24854
IEZvcmdl 24855
IGFybW9yZWQ= 24856
TWls 24857
IHBoaWxvc29waGVycw== 24858
aW52ZXN0 24859
UGxheWVycw== 24860
4og= 24861
IG15cmlhZA== 24862
IGNvbXJhZGVz 24863
Um90 24864
IHJlbWVtYmVyaW5n 24865
IGNvcnJlc3BvbmRz 24866
IHByb2dyYW1tZXJz 24867
IEx5bm4= 24868
IG9saWc= 24869
IGNvaGVyZW50 24870
eW5jaHJvbg== 24871
IENoZW1pY2Fs 24872
IGp1Z2c= 24873
cGFpcg== 24874
cG9zdHM= 24875
RXll 24876
IElubmVy 24877
IHNlbWVzdGVy 24878
b3R0ZXN0 24879
IEVtaXJhdGVz 24880
cmljYW5lcw== 24881
b3JvdXNseQ== 24882
bWl0cw== 24883
IFdpcw== 24884
IGRvZGdl 24885
bG9jYXRpb24= 24886
IGZhZGVk 24887
QW1hem9u 24888
IFByb2NlZWQ= 24889
IElORk8= 24890
am91cm5hbA== 24891
IFRydWNr 24892
VGVu 24893
IDIxNw== 24894
IHN0YXR1dGVz 24895
bW9iaWxl 24896
IFR5cGVz 24897
UmVjb21t 24898
YnVzdGVy 24899
cGV4 24900
IGxlZ2VuZHM= 24901
IGhlYWRhY2hl 24902
ZmFjZWQ= 24903
IFdpRmk= 24904
aWZ0eQ== 24905
IEhFUg== 24906
IGNpcmN1aXRz 24907
RVJST1I= 24908
MjI2 24909
b2xpbg== 24910
IGN5bGluZGVy 24911
b3NwYWNl 24912
aWtlcnM= 24913
UHJlbQ== 24914
UXVhbnQ= 24915
IGNvbmZsaWN0aW5n 24916
IHNsaWdodGVzdA== 24917
IGZvcmdlZA== 24918
aW9uYWdl 24919
U3RlcGhlbg== 24920
IEt1Yg== 24921
IE9wcG9ydHVu 24922
IEhlYWw= 24923
IGJsbw== 24924
IHJ1bGVycw== 24925
IGh1aA== 24926
IHN1Ym1hcmluZQ== 24927
Znk= 24928
YXNzZXI= 24929
IGFsbG93YW5jZQ== 24930
IEthc2ljaA== 24931
IFRhcw== 24932
IEF1c3RyYWxpYW5z 24933
Rm9yZ2VNb2RMb2FkZXI= 24934
IOKGkQ== 24935
IE1hdHJpeA== 24936
YW1pbnM= 24937
IDEyMDA= 24938
IEFjcXU= 24939
MjM2 24940
RG9jdW1lbnQ= 24941
IEJyZWFraW5n 24942
MTkz 24943
IFN1YnN0 24944
IFJvbGxlcg== 24945
IFByb3BlcnRpZXM= 24946
IE5J 24947
dGllcg== 24948
IGNydXNoaW5n 24949
IGFkdm9jYXRpbmc= 24950
RnVydGhlcm1vcmU= 24951
a2VlcGVycw== 24952
IHNleGlzbQ== 24953
eGQ= 24954
IGNhbGxlcg== 24955
IFNlbnNl 24956
Y2hpZXZl 24957
IFRG 24958
IGZ1ZWxlZA== 24959
IHJlbWluaXNjZW50 24960
IG9ic2Vzcw== 24961
dXJzdA== 24962
IHVwaG9sZA== 24963
IEZhbnM= 24964
aGV0aWNz 24965
IOKX 24966
IEJhdGg= 24967
IGJldmVyYWdl 24968
IG9zY2lsbA== 24969
MjU0 24970
IHBvbGVz 24971
IGdyYWR1YWw= 24972
IGV4dGluZw== 24973
IFN1ZmY= 24974
IFN1ZGRlbmx5 24975
IGxpa2luZw== 24976
IDE5NDk= 24977
dW5jaWF0aW9u 24978
YW1pbmF0aW9u 24979
IE9tYXI= 24980
IExW 24981
IENvbnNlcXVlbnRseQ== 24982
IHN5bnRoZXM= 24983
IEdJRg== 24984
IHBhaW5z 24985
IGludGVyYWN0aW5n 24986
dW91c2x5 24987
aW5jcmU= 24988
IHJ1bW9y 24989
IFNjaWVudG9sb2d5 24990
MTk3 24991
IFppZw== 24992
IHNwZWxsaW5n 24993
IEFTUw== 24994
IGV4dGluZ3U= 24995
bXNvbg== 24996
IGdo 24997
IHJlbWFya2Vk 24998
IFN0cmF0ZWdpYw== 24999
IE1PTg== 25000
5aU= 25001
Z2Fl 25002
IFdIQVQ= 25003
RXJpYw== 25004
IENhbXB1cw== 25005
IG1ldGhhbmU= 25006
IGltYWdpbg== 25007
SlVTVA== 25008
IEFsbQ== 25009
WFQ= 25010
aXE= 25011
IFJTUw== 25012
IHdyb25nZG9pbmc= 25013
YXR0YQ== 25014
IGJpZ290 25015
IGRlbW9uc3RyYXRvcnM= 25016
IENhbHZpbg== 25017
IFZpbGxh 25018
IG1lbWJyYW5l 25019
IEF3ZXNvbWU= 25020
IGJlbmVmaWM= 25021
MjY4 25022
IG1hZ25pZmljZW50 25023
IExvdHM= 25024
R3JlZw== 25025
IEJvcmlz 25026
IGRldGFpbmVlcw== 25027
IEhlcm1hbg== 25028
IHdoaXNwZXJlZA== 25029
IGF3ZQ== 25030
UHJvZmVzc29y 25031
ZnVuZGluZw== 25032
IHBoeXNpb2xvZ2ljYWw= 25033
IERlc3RydWN0aW9u 25034
IGxpbWI= 25035
IG1hbmlwdWxhdGVk 25036
IGJ1YmJsZXM= 25037
IHBzZXVk 25038
IGh5ZHJh 25039
IEJyaXN0b2w= 25040
IHN0ZWxsYXI= 25041
IEV4cGFuc2lvbg== 25042
IEtlbGw= 25043
IEludGVyZXN0aW5nbHk= 25044
IG1hbnM= 25045
IGRyYWdnaW5n 25046
IGVjb2xvZ2ljYWw= 25047
IEZpdA== 25048
IGdlbnQ= 25049
IGJlbmVmaXRlZA== 25050
IEhhaXRp 25051
IHBvbHln 25052
44OO 25053
IDIwMzA= 25054
IHByb3c= 25055
IHJlY29uc3RydWN0aW9u 25056
IHdhc3Q= 25057
IHBzeWNoaWM= 25058
IEdyZWVrcw== 25059
SGFuZGxlcg== 25060
MTYy 25061
IFB1bHNl 25062
IHNvbGljaXQ= 25063
IHN5cw== 25064
IGluZmx1eA== 25065
IEdlbnRsZQ== 25066
cGVyY2VudA== 25067
IHByb2xpZmVyYXRpb24= 25068
IHRheGFibGU= 25069
IGRpc3JlZ2FyZA== 25070
IGVzY2FwaW5n 25071
IGdpbmdlcg== 25072
IHdpdGhzdGFuZA== 25073
IGRldmFzdGF0ZWQ= 25074
IERldw== 25075
c2VyaWVz 25076
IGluamVjdGVk 25077
ZWxhaWRl 25078
IHR1cm5vdmVy 25079
aGVhdA== 25080
mYI= 25081
SGFwcHk= 25082
IFNpbGVudA== 25083
44Kt 25084
aXZpc20= 25085
IGlycmF0aW9uYWw= 25086
QU1B 25087
IHJlZWY= 25088
cnVi 25089
IDE2Mg== 25090
IGJhbmtlcnM= 25091
IEV0aGljcw== 25092
dnY= 25093
IGNyaXRpY2lzbXM= 25094
S24= 25095
MTg2 25096
TW92aWU= 25097
IFRvcmllcw== 25098
IG5vb2Q= 25099
IGRpc3RvcnRpb24= 25100
RmFsc2U= 25101
b2RvcmU= 25102
IHRhc3R5 25103
UmVzZWFyY2g= 25104
IFVJRA== 25105
LSk= 25106
IGRpdm9yY2Vk 25107
IE1V 25108
IEhheWVz 25109
IElzbg== 25110
aWFuaQ== 25111
IEhR 25112
ICIj 25113
aWduYW50 25114
IHRyYXVtYXRpYw== 25115
IExpbmc= 25116
SHVu 25117
IHNhYm90 25118
b25saW5l 25119
cmFuZG9t 25120
IHJlbmFtZWQ= 25121
cmFyZWQ= 25122
S0E= 25123
ZGVhZA== 25124
w6l0 25125
IEFzc2lzdGFuY2U= 25126
IHNlYWY= 25127
KysrKysrKys= 25128
IHNlbGRvbQ== 25129
IFdlYmI= 25130
IGJvb2xlYW4= 25131
dWxldA== 25132
IHJlZnJhaW4= 25133
IERJWQ== 25134
cnVsZQ== 25135
IHNodXR0aW5n 25136
IHV0aWxpemluZw== 25137
bG9hZGluZw== 25138
IFBhcmFt 25139
Y29hbA== 25140
b290ZXI= 25141
IGF0dHJhY3Rpbmc= 25142
IERvbA== 25143
IGhlcnM= 25144
YWduZXRpYw== 25145
IFJlYWNo 25146
aW1v 25147
IGRpc2NhcmRlZA== 25148
IFBpcA== 25149
MDE1 25150
w7xy 25151
IG11Zw== 25152
SW1hZ2luZQ== 25153
Q09M 25154
IGN1cnNlZA== 25155
IFNob3dz 25156
IEN1cnRpcw== 25157
IFNhY2hz 25158
c3BlYWtpbmc= 25159
IFZpc3Rh 25160
IEZyYW1ld29yaw== 25161
b25nbw== 25162
IHN1YnJlZGRpdA== 25163
IGNydXM= 25164
IE92YWw= 25165
Um93 25166
Z3Jvd2luZw== 25167
IGluc3RhbGxtZW50 25168
IGdsYWM= 25169
IEFkdmFuY2U= 25170
RUNL 25171
IExHQlRR 25172
TEVZ 25173
IGFjZXQ= 25174
IHN1Y2Nlc3NpdmU= 25175
IE5pY29sZQ== 25176
IDE5NTc= 25177
UXVvdGU= 25178
IGNpcmN1bXN0YW5jZQ== 25179
YWNrZXRz 25180
IDE0Mg== 25181
b3J0aXVt 25182
IGd1ZXNzZWQ= 25183
IEZyYW1l 25184
IHBlcnBldHJhdG9ycw== 25185
IEF2aWF0aW9u 25186
IEJlbmNo 25187
IGhhbmRj 25188
QXA= 25189
IDE5NTY= 25190
MjU5 25191
cmFuZA== 25192
TmV0TWVzc2FnZQ== 25193
ZGlu 25194
dXJ0bGVz 25195
aGln 25196
IFZJSUk= 25197
ZmZpdGk= 25198
IFN3b3Jkcw== 25199
YmlhbA== 25200
IGtpZG5hcHBpbmc= 25201
ZGV2aWNl 25202
IGJhcm4= 25203
IEVsaQ== 25204
YXVjYXM= 25205
U2VuZA== 25206
Q29uc3RydWN0ZWQ= 25207
IMK9 25208
IG5lZWRsZXM= 25209
IGFkdmVydGlzZW1lbnRz 25210
IHZvdQ== 25211
IGV4aGliaXRlZA== 25212
IEZvcnRyZXNz 25213
QXNr 25214
QmVycnk= 25215
VFlQRQ== 25216
IGNhbmNlcnM= 25217
dW1waW5n 25218
IFRlcnJpdG9yeQ== 25219
IHBydWQ= 25220
IG5hcw== 25221
IGF0aGVpc3Q= 25222
IGJhbGFuY2Vz 25223
44Gf 25224
IFNoYXdu 25225
JiY= 25226
IGxhbmRzYw== 25227
IFJHQg== 25228
IHBldHR5 25229
IGV4Y2VsbGVuY2U= 25230
IHRyYW5zbGF0aW9ucw== 25231
IHBhcmNlbA== 25232
IENoZXY= 25233
RWFzdA== 25234
IE91dHB1dA== 25235
aW1p 25236
IGFtYmllbnQ= 25237
IFRocmVhdA== 25238
IHZpbGxhaW5z 25239
IDU1MA== 25240
SUNB 25241
IHRhbGxlcg== 25242
IGxlYWtpbmc= 25243
Y3Vw 25244
IHBvbGlzaA== 25245
IGluZmVjdGlvdXM= 25246
IEtD 25247
IEBA 25248
YmFja2dyb3VuZA== 25249
IGJ1cmVhdWNyYWN5 25250
IFNhaQ== 25251
dW5sZXNz 25252
aXRpb3Vz 25253
IFNreXBl 25254
QXRs 25255
SURFTlQ= 25256
MDA4 25257
IGh5cG9jcg== 25258
IHBpdGNoZXJz 25259
IGd1ZXNzaW5n 25260
IEZJTkFM 25261
QmV0d2Vlbg== 25262
IHZpbGxhZ2Vycw== 25263
IDI1Mg== 25264
ZmFzaGlvbg== 25265
IFR1bmlz 25266
QmVo 25267
IEV4Yw== 25268
IE1JRA== 25269
Mjg4 25270
IEhhc2tlbGw= 25271
MTk2 25272
IE5PUg== 25273
IHNwZWNz 25274
IGludmFyaQ== 25275
IGdsdXQ= 25276
IENhcnM= 25277
IGltcHVsc2U= 25278
IGhvbm9ycw== 25279
Z2Vs 25280
IGp1cmlzZGljdGlvbnM= 25281
IEJ1bmRsZQ== 25282
dWxhcw== 25283
Q2FsaWZvcm5pYQ== 25284
IEluY3JlYXNl 25285
IHBlYXI= 25286
IHNpbmdsZXM= 25287
IGN1ZXM= 25288
IHVuZGVyd2VudA== 25289
IFdT 25290
IGV4YWdnZXJhdGVk 25291
IGR1YmlvdXM= 25292
IGZsYXNoaW5n 25293
TE9H 25294
KV0u 25295
Sm91cm5hbA== 25296
dGc= 25297
VmFu 25298
IElzdGFuYnVs 25299
IEluc3A= 25300
IEZyYW5rZW4= 25301
RHJhdw== 25302
IHNhZG5lc3M= 25303
IGlyb25pYw== 25304
IEZyeQ== 25305
eGM= 25306
IDE2NA== 25307
aXNjaA== 25308
V2F5 25309
IFByb3Rlc3RhbnQ= 25310
aG9ybg== 25311
IHVuYWZm 25312
IFZpdg== 25313
aWxsYXM= 25314
IFByb2R1Y3Rpb25z 25315
IEhvZ2Fu 25316
IHBlcmltZXRlcg== 25317
IFNpc3RlcnM= 25318
IHNwb250YW5lb3Vz 25319
IGRvd25zaWRl 25320
IGRlc2NlbmRhbnRz 25321
IG9ybg== 25322
d29ybQ== 25323
SmFwYW5lc2U= 25324
IDE5NTU= 25325
IDE1MQ== 25326
IERvaW5n 25327
ZWxzZW4= 25328
dW1ibGVz 25329
IHJhZGljYWxseQ== 25330
IERydW0= 25331
IEJhY2g= 25332
IGxpYWJpbGl0aWVz 25333
IE9C 25334
IEVsZW1lbnRhcnk= 25335
IG1lbWU= 25336
eW5lcw== 25337
IGZpbmdlcnByaW50 25338
IEdyYWI= 25339
IHVuZGVydGFrZQ== 25340
TWVtYmVycw== 25341
IFJlYWRlcg== 25342
IFNpbXM= 25343
Z29k 25344
IGh5cG90aGV0aWNhbA== 25345
c2NpZW50 25346
IEFK 25347
IGNoYXJpc20= 25348
IGFkbWlzc2lvbnM= 25349
IE1pc3NpbGU= 25350
dHJhZGU= 25351
IGV4ZXJjaXNpbmc= 25352
IEJhY2tncm91bmQ= 25353
V3JpdHRlbg== 25354
IHZvY2Fscw== 25355
d2hldGhlcg== 25356
IHZp 25357
IFdpbm5lcg== 25358
IGxpdHRlcg== 25359
IFNob290aW5n 25360
U1RFTQ== 25361
44Kh 25362
IEFGTA== 25363
IHZhcmlhYmlsaXR5 25364
IGVhdHM= 25365
IERQUw== 25366
YnJvdw== 25367
IGVsZXBoYW50cw== 25368
IHN0cmF0 25369
IMU= 25370
IHNldHRsZXJz 25371
TWF0dGhldw== 25372
IGluYWR2ZXJ0 25373
SEk= 25374
IElNRg== 25375
IEdvYWw= 25376
IG5lcnZlcw== 25377
Sm9obnNvbg== 25378
ZXll 25379
YWJsaXNobWVudA== 25380
VGh1cnNkYXk= 25381
QklMSVRZ 25382
SGFk 25383
YW1vdG8= 25384
aGV0YW1pbmU= 25385
ZXBz 25386
IG1pdG9jaG9uZA== 25387
IGNvbXByZXNzZWQ= 25388
IFRyZXZvcg== 25389
IEFuaW1hbHM= 25390
VG9vbA== 25391
TG9jaw== 25392
IHR3ZWFr 25393
IHBpbmNo 25394
IGNhbmNlbGxhdGlvbg== 25395
UG90 25396
IGZvY2Fs 25397
IEFzdHJvbg== 25398
MTcz 25399
IEFTQw== 25400
IE9USEVS 25401
dW1uaQ== 25402
IGRlbWlzZQ== 25403
ZGw= 25404
2YU= 25405
U2VtaXRpc20= 25406
IGNyYWNraW5n 25407
IGNvbGxhYm9yYXRpdmU= 25408
IGV4cGxvcmVz 25409
c3Fs 25410
IGhlcmJz 25411
IGNvbmZpZ3VyYXRpb25z 25412
bWlz 25413
IFJlc3VsdA== 25414
YWNleQ== 25415
IFNtb2tl 25416
IHNhbmN0 25417
ZWxpYQ== 25418
IGRlZ2VuZXI= 25419
IGRlZXBlc3Q= 25420
IHNjcmVhbWVk 25421
IG5hcA== 25422
U29mdHdhcmU= 25423
IFNUQVI= 25424
RUY= 25425
IFhpbg== 25426
c3BvbnNvcmVk 25427
bWFuc2hpcA== 25428
MjMz 25429
IHByaW1hcmllcw== 25430
IGZpbHRlcmluZw== 25431
IGFzc2VtYmxl 25432
bWls 25433
IE15ZXJz 25434
Ym93cw== 25435
IHB1bmNoZWQ= 25436
TWlj 25437
IGlubm92YXRpb25z 25438
IGZ1bmM= 25439
YW5kbw== 25440
IGZyYWNraW5n 25441
IFZ1bA== 25442
0L7Q 25443
b3Nob3A= 25444
IEltbXVu 25445
IHNldHRsaW5n 25446
IGFkb2xlc2NlbnRz 25447
IHJlYnVpbGRpbmc= 25448
IHRyYW5zZm9ybWluZw== 25449
IHBhcm9sZQ== 25450
IGhhcmJvcg== 25451
IGJvb2tpbmc= 25452
b3Rpb25hbA== 25453
b25nZXZpdHk= 25454
IFlv 25455
YnVn 25456
IGVtZXJnZXM= 25457
IE1ldGhvZHM= 25458
IENodQ== 25459
UHJlcw== 25460
IER1bmdlb25z 25461
IHRyYWlsaW5n 25462
IFJ1bQ== 25463
IEh1Z2g= 25464
5aSp 25465
IEVyYQ== 25466
IEJhdHRsZXM= 25467
UmVzdWx0cw== 25468
IFRyYWRpbmc= 25469
IHZlcnNh 25470
Y3Nz 25471
YXhpZXM= 25472
aGVldA== 25473
IGdyZWVk 25474
MTk4OQ== 25475
IGdhcmRlbnM= 25476
IGNvbnRpbmdlbnQ= 25477
UGFyaw== 25478
IExlYWZz 25479
aG9vaw== 25480
cm9iZQ== 25481
IGRpcGxvbWFjeQ== 25482
IEZ1ZWw= 25483
IEludmFzaW9u 25484
IHVwZ3JhZGluZw== 25485
TWFsZQ== 25486
IGVsaWM= 25487
IHJlbGVudGxlc3M= 25488
IENvdmVuYW50 25489
YXBlc2g= 25490
IFRyb3A= 25491
VHk= 25492
cHJvZHVjdGlvbg== 25493
YXJ0eQ== 25494
IHB1bmNoZXM= 25495
YWtv 25496
Y3ljbG9wZWRpYQ== 25497
IFJhYmJpdA== 25498
IEhETUk= 25499
IDE0MQ== 25500
IGZvaWw= 25501
SXRlbUltYWdl 25502
IEZH 25503
IGltcGxlbWVudGF0aW9ucw== 25504
IFBvbQ== 25505
aXh0dXJlcw== 25506
IGF3YWl0 25507
IDMzMA== 25508
YW11cw== 25509
IHVtYnJlbGxh 25510
IGZvcmVzZWU= 25511
c2VwYXI= 25512
IGNpcmN1bWNpc2lvbg== 25513
IHBlcmlwaGVyYWw= 25514
U2F5 25515
IEV4cGVydA== 25516
SW5j 25517
IHdpdGhkcmV3 25518
IEFuZGVycw== 25519
ZnJpZWQ= 25520
IHJhZGlvYWN0aXZl 25521
IE9wZW5pbmc= 25522
IGJvYXJkaW5n 25523
IE5E 25524
IG92ZXJ0aHJvdw== 25525
QWN0aXY= 25526
V1A= 25527
IEFjdHM= 25528
15k= 25529
IG1vdGlvbnM= 25530
dmlj 25531
IE1pZ2h0eQ== 25532
IERlZmVuZGVy 25533
YWVy 25534
IHRoYW5rZnVs 25535
IEtpbGxpbmc= 25536
IEJyaXM= 25537
bW9pbA== 25538
IHByZWRpY3Rpbmc= 25539
MjY2 25540
Y2hvaWNl 25541
IGtpbGxlcnM= 25542
IGluY3Vi 25543
IENoZXN0 25544
YXRoZXJpbmc= 25545
IHByb2NsYWltZWQ= 25546
Zmxvd2Vy 25547
b3Nzb20= 25548
dW1ibGVkb3Jl 25549
IEN5Y2xpbmc= 25550
IE9jY3VweQ== 25551
QUdFUw== 25552
UGVu 25553
IFl1Zw== 25554
IHBhY2thZ2Vk 25555
IGhlaWdodGVuZWQ= 25556
Y290 25557
c3RhY2s= 25558
Q29uZA== 25559
IHN0YW1wcw== 25560
bWFnZQ== 25561
IHBlcnN1YWRlZA== 25562
IGVuc2w= 25563
IENhcmRpbmFs 25564
IHNvbGl0YXJ5 25565
IHBvc3Nlc3Npbmc= 25566
IENvcms= 25567
IGV2aWQ= 25568
IFRheQ== 25569
IGJsdWVz 25570
IGV4dHJlbWlzbQ== 25571
IGx1bmFy 25572
IGNsb3du 25573
VGVjaG4= 25574
IGZlc3RpdmFscw== 25575
IFB2UA== 25576
IExhcg== 25577
IGNvbnNlcXVlbnRseQ== 25578
cHJlc2VudA== 25579
IHNvbWVkYXk= 25580
546L 25581
IE1ldGVvcg== 25582
IHRvdXJpbmc= 25583
Y3VsdHVyZQ== 25584
IGJlYWNoZXM= 25585
U2hpcA== 25586
Y2F1c2U= 25587
IEZsb29k 25588
44Ov 25589
IHB1cml0eQ== 25590
dGhvc2U= 25591
IGVtaXNzaW9u 25592
Ym9sdA== 25593
IGNob3Jk 25594
IFNjcmlwdHVyZQ== 25595
THU= 25596
ICR7 25597
Y3JlYXRlZA== 25598
T3RoZXJz 25599
MjU4 25600
IGVsZW1lbnRhbA== 25601
IGFubm95ZWQ= 25602
IEFF 25603
ZGFu 25604
IFNhZw== 25605
UmVzZWFyY2hlcnM= 25606
IGZhaXJ5 25607
4oCT4oCT 25608
PT09PT09PT09PT09 25609
U21hcnQ= 25610
R0dHRw== 25611
IHNrZWxldG9ucw== 25612
IHB1cGlscw== 25613
bGlua2Vk 25614
IHVyZ2VuY3k= 25615
ZW5hYmxlZA== 25616
IEZ1Y2s= 25617
IGNvdW5jaWxs 25618
cmFi 25619
VUFM 25620
VEk= 25621
IGxpZmVz 25622
IGNvbmZlc3NlZA== 25623
QnVn 25624
IGhhcm1vbg== 25625
IENPTkZJRw== 25626
IE5ldXRyYWw= 25627
RG91Ymxl 25628
IHN0YXBsZQ== 25629
IFNIQQ== 25630
QnJpdGlzaA== 25631
IFNOUA== 25632
QVRPUg== 25633
b2Nv 25634
IHN3aW5naW5n 25635
Z2V4 25636
b2xlb24= 25637
cGxhaW4= 25638
IE1pc3Npbmc= 25639
IFRyb3BoeQ== 25640
dmFyaQ== 25641
cmFuY2g= 25642
IDMwMQ== 25643
NDQw 25644
MDAwMDAwMDAwMDAwMDAwMA== 25645
IHJlc3RvcmluZw== 25646
IGhhdWw= 25647
dWNpbmc= 25648
bmVyZw== 25649
IGZ1dHVyZXM= 25650
IHN0cmF0ZWdpc3Q= 25651
cXVlc3Rpb24= 25652
IGxhdGVyYWw= 25653
IEJhcmQ= 25654
IHNvcg== 25655
IFJob2Rlcw== 25656
IERvd250b3du 25657
Pz8/Pz8t 25658
IExpdA== 25659
IEJlbmVk 25660
IGNvaWw= 25661
c3RyZWV0 25662
IFBvcnRhbA== 25663
RklMRQ== 25664
IEdydQ== 25665
Kiw= 25666
MjMx 25667
bmV1bQ== 25668
IHN1Y2tlZA== 25669
IHJhcHBlcg== 25670
IHRlbmRlbmNpZXM= 25671
IExhdXJlbg== 25672
Y2VsbGFuZW91cw== 25673
MjY3 25674
IGJyb3dzZQ== 25675
IG92ZXJj 25676
aGVhZGVy 25677
b2lzZQ== 25678
IGJlZXQ= 25679
IEdsZQ== 25680
U3RheQ== 25681
IG11bQ== 25682
IHR5cGVk 25683
IGRpc2NvdW50cw== 25684
VGFsaw== 25685
IE9n 25686
ZXhpc3Rpbmc= 25687
IFNlbGw= 25688
dXBo 25689
Q0k= 25690
IEF1c3RyaWFu 25691
IFdhcm0= 25692
IGRpc21pc3NhbA== 25693
IGF2ZXJhZ2Vz 25694
Y2FtZXJh 25695
IGFsbGVnaWFuY2U= 25696
TEFO 25697
PSIj 25698
IGNvbW1lbnRhdG9ycw== 25699
IFNldHRpbmc= 25700
IE1pZHdlc3Q= 25701
IHBoYXJtYWM= 25702
IEVYUA== 25703
IHN0YWlubGVzcw== 25704
Q2hpY2Fnbw== 25705
IHRhbg== 25706
MjQ0 25707
IGNvdW50cnlzaWRl 25708
IFZhYw== 25709
Mjk1 25710
IHBpbm5lZA== 25711
IGNyaXNlcw== 25712
IHN0YW5kYXJkaXplZA== 25713
VGFzaw== 25714
IEphaWw= 25715
IERvY2tlcg== 25716
Y29sb3JlZA== 25717
Zm9ydGg= 25718
In0s 25719
IHBhdHJvbnM= 25720
IHNwaWNl 25721
IG1vdXJu 25722
IE1vb2Q= 25723
IGxhdW5kcnk= 25724
IGVxdWlw 25725
IE1vbGU= 25726
eWxs 25727
IFRIQw== 25728
bmF0aW9u 25729
IFNoZXJsb2Nr 25730
IGlzc3U= 25731
IEtyZQ== 25732
IEFtZXJpY2Fz 25733
IEFBQQ== 25734
IHN5c3RlbWF0aWNhbGx5 25735
IGNvbnRyYQ== 25736
IFNhbGx5 25737
IHJhdGlvbmFsZQ== 25738
IGNhcnJpYWdl 25739
IHBlYWtz 25740
IGNvbnRyYWRpY3Rpb24= 25741
ZW5zYXRpb24= 25742
IEZhaWx1cmU= 25743
IHByb3Bz 25744
IG5hbWVzcGFjZQ== 25745
IGNvdmU= 25746
ZmllbGRz 25747
44KL 25748
IHdvb2w= 25749
IENhdGNo 25750
IHByZXN1bWVk 25751
IERpYW5h 25752
cmFnb24= 25753
aWdp 25754
IGhhbW0= 25755
IHN0dW50 25756
IEdVSQ== 25757
IE9ic2VydmF0b3J5 25758
IFNob3Jl 25759
IHNtZWxscw== 25760
YW5uYWg= 25761
IGNvY2twaXQ= 25762
IER1dGVydGU= 25763
ODUw 25764
IG9wcHJlc3NlZA== 25765
YnJlYWtlcg== 25766
IENvbnRyaWJ1dA== 25767
IFBlcnU= 25768
IE1vbnNhbnRv 25769
IEF0dGVtcHQ= 25770
IGNvbW1hbmRpbmc= 25771
IGZyaWRnZQ== 25772
IFJpbg== 25773
IENoZXNz 25774
dWFsaXR5 25775
IG9s 25776
UmVwdWJsaWNhbg== 25777
IEdsb3J5 25778
IFdJTg== 25779
Li4uLi4uLg== 25780
YWdlbnQ= 25781
cmVhZGluZw== 25782
IGluaA== 25783
Sm9uZXM= 25784
IGNsaWNrcw== 25785
YWxhbg== 25786
IFtdOw== 25787
IE1hamVzdHk= 25788
IENlZA== 25789
b3B1cw== 25790
YXRlbA== 25791
w6o= 25792
QVJD 25793
IEVjdWFkb3I= 25794
44Og 25795
IEt1cm8= 25796
IHJpdHVhbHM= 25797
IGNhcHRpdmU= 25798
IG91bmNl 25799
IGRpc2FncmVlbWVudA== 25800
IHNsb2c= 25801
ZnVlbA== 25802
UGV0 25803
TWFpbA== 25804
IGV4ZXJjaXNlZA== 25805
IHNvbGlj 25806
IHJhaW5mYWxs 25807
IGRldm90aW9u 25808
IEFzc2Vzc21lbnQ= 25809
IHJvYm90aWM= 25810
b3B0aW9ucw== 25811
IFJQ 25812
IEZhbWlsaWVz 25813
IEZsYW1lcw== 25814
IGFzc2lnbm1lbnRz 25815
MDA3 25816
YWtlZG93bg== 25817
IHZvY2FidWxhcnk= 25818
UmVpbGx5 25819
IGNhdmFs 25820
Z2Fycw== 25821
IHN1cHByZXNzZWQ= 25822
IFNFVA== 25823
IEpvaG5z 25824
IHdhcnA= 25825
YnJva2Vu 25826
IHN0YXR1ZXM= 25827
IGFkdm9jYXRlZA== 25828
IDI3NQ== 25829
IHBlcmls 25830
b21vcnBo 25831
IEZlbWlu 25832
cGVyZmVjdA== 25833
IGhhdGNo 25834
TGli 25835
NTEy 25836
IGxpZmVsb25n 25837
MzEz 25838
IGNoZWVrcw== 25839
IG51bWJlcmVk 25840
IE11Zw== 25841
Qm9keQ== 25842
cmF2ZWw= 25843
V2VpZ2h0 25844
IEphaw== 25845
IEhlYXRo 25846
IGtpc3Npbmc= 25847
IEpVU1Q= 25848
IHdhdmluZw== 25849
dXBsb2Fk 25850
IGluc2lkZXI= 25851
IFByb2dyZXNzaXZl 25852
IEZpbHRlcg== 25853
dHRh 25854
IEJlYW0= 25855
IHZpb2xlbnRseQ== 25856
aXBhdGlvbg== 25857
IHNrZXB0aWNpc20= 25858
IDE5MTg= 25859
IEFubmll 25860
IFNJ 25861
IGdlbmV0aWNz 25862
IG9uYm9hcmQ= 25863
YXRs 25864
IEZyaWVkbWFu 25865
IEJyaQ== 25866
Y2VwdGl2ZQ== 25867
IHBpcmF0ZQ== 25868
IFJlcG9ydGVy 25869
Mjc4 25870
IG15dGhvbG9neQ== 25871
IGVjbGlwc2U= 25872
IHNraW5z 25873
IGdseXBo 25874
aW5naGFt 25875
RmlsZXM= 25876
Q291cg== 25877
d29tZW4= 25878
IHJlZ2ltZXM= 25879
IHBob3RvZ3JhcGhlZA== 25880
S2F0 25881
IE1BWA== 25882
T2ZmaWNpYWxz 25883
IHVuZXhwZWN0ZWRseQ== 25884
IGltcHJlc3Npb25z 25885
RnJvbnQ= 25886
Ozs7Ozs7Ozs= 25887
IHN1cHJlbWFjeQ== 25888
IHNhbmc= 25889
IGFnZ3JhdmF0ZWQ= 25890
IGFicnVwdGx5 25891
IFNlY3Rvcg== 25892
IGV4Y3VzZXM= 25893
IGNvc3Rpbmc= 25894
aWRlcHJlc3M= 25895
U3RhY2s= 25896
IFJOQQ== 25897
b2JpbA== 25898
IGdob3N0cw== 25899
bGRvbg== 25900
YXRpYmlsaXR5 25901
VG9waWNz 25902
IHJlaW1idXJzZQ== 25903
IEhN 25904
IERlZw== 25905
IHRoaWVm 25906
eWV0 25907
b2dlbmVzaXM= 25908
bGVhbmluZw== 25909
IEtvbA== 25910
IEJhc2tldGJhbGw= 25911
IGZp 25912
IFNlZWluZw== 25913
IHJlY3ljbGluZw== 25914
IFst 25915
Q29uZ3Jlc3M= 25916
IGxlY3R1cmVz 25917
UHN5 25918
IG5lcA== 25919
IG1haWQ= 25920
IG9yaWVudGVk 25921
QVg= 25922
IHJlc3BlY3RmdWw= 25923
cmVuZQ== 25924
Zmx1c2g= 25925
IFVubG9hZGVk 25926
cmVxdWVzdA== 25927
Z3JpZA== 25928
IEFsdGVybmF0aXZlbHk= 25929
IEh1Z28= 25930
IGRlY3JlZQ== 25931
IEJ1ZGRoaXNt 25932
YW5kdW0= 25933
QW5kcm9pZA== 25934
IENvbmdv 25935
IEpveWNl 25936
IGFja25vd2xlZGdpbmc= 25937
aGVzaXZl 25938
IFRvbW9ycm93 25939
IEhpcm8= 25940
dGhyZW4= 25941
IE1hY2Vk 25942
IGhvYXg= 25943
IEluY3JlYXNlZA== 25944
IFByYWRlc2g= 25945
V2lsZA== 25946
X19fX19f 25947
MTYx 25948
IGF1bnQ= 25949
IGRpc3RyaWJ1dGluZw== 25950
IFR1Y2tlcg== 25951
IFNTTA== 25952
IFdvbHZlcw== 25953
QnVpbGRpbmc= 25954
b3VsdA== 25955
IEx1bw== 25956
IFlhcw== 25957
IFNwaXI= 25958
IFNoYXBl 25959
IENhbWJvZA== 25960
IElQdg== 25961
IG1s 25962
IGV4dHJhZA== 25963
Mzkw 25964
IFBlbm55 25965
ZHJlYW0= 25966
IHN0YXRpb25lZA== 25967
b3B0aW9uYWw= 25968
ZXdvcnRoeQ== 25969
Ljwv 25970
IHVuZGVydGFraW5n 25971
IGNoaWNrZW5z 25972
IHN0aW11bGk= 25973
IEVsc2U= 25974
aWdhdG9ycw== 25975
IEJlZ2lubmluZw== 25976
Y3Rvcnk= 25977
IHByZXBhcmVz 25978
IGRlbHRh 25979
IHZpY2luaXR5 25980
dG9vbA== 25981
IHdvcmtzaG9wcw== 25982
TUh6 25983
IGFjY3VzYXRpb24= 25984
IGhpc3Rvcmllcw== 25985
cm9wb2xpcw== 25986
IENodXJjaGlsbA== 25987
IG5lb24= 25988
IGJhZmY= 25989
ZGllcw== 25990
bWF5YmU= 25991
IOijj+immumGkg== 25992
IHN5bXB0b20= 25993
RUNI 25994
IE1hbnVlbA== 25995
IGJhbmFuYQ== 25996
IEhC 25997
ICoqKio= 25998
IEtvcmVhbnM= 25999
Y29sbA== 26000
RkI= 26001
IHByYXlpbmc= 26002
IENhbm5vdA== 26003
IE1pbGU= 26004
IGVtYnJhY2luZw== 26005
IFNpbGs= 26006
Mzkz 26007
b3RlcnM= 26008
RkQ= 26009
IGRheWxpZ2h0 26010
YWxpYXM= 26011
IEJyaWdhZGU= 26012
IEhhbm5haA== 26013
IGNsZXJneQ== 26014
IHNvdXRoZWFzdA== 26015
IGFsY29ob2xpYw== 26016
IHByb3Bvc2Vz 26017
bGl2aW9u 26018
IGNhbGN1bGF0aW5n 26019
IHN0aW11bGF0ZQ== 26020
IHNwbGl0dGluZw== 26021
ZWlnaHQ= 26022
IEluZHk= 26023
cGxheXM= 26024
IFBpaw== 26025
IGRvbWVzdA== 26026
IGZvcmdpdmVuZXNz 26027
IFJpbmdz 26028
cGF0aWVudA== 26029
a2luc29u 26030
TW9udA== 26031
aWdpYmxl 26032
OyI= 26033
IHBlcmlvZGljYWxseQ== 26034
YW1tYWQ= 26035
IEJyaXR0 26036
cGFyZA== 26037
IGFyYml0cmF0aW9u 26038
IFNjaG5laWRlcg== 26039
IENvcnBvcmF0ZQ== 26040
IE1heWE= 26041
IHNuYWtlcw== 26042
YXVt 26043
IGJsYXN0ZWQ= 26044
IG15c3Rlcmllcw== 26045
IHJldml2ZQ== 26046
b2NhbXA= 26047
IERvZGdl 26048
IE9wZXJh 26049
Mjc5 26050
IG9ycGhhbg== 26051
IHNwZWNpZmllcw== 26052
IE1ldHM= 26053
RHVyYXRpb24= 26054
SGVu 26055
IGZpcmV3b3Jrcw== 26056
IHByb3NlY3V0ZQ== 26057
IFRpbGxlcnNvbg== 26058
ZHA= 26059
dXNhZ2U= 26060
bGluZXNz 26061
IERlYmlhbg== 26062
IDIyNA== 26063
cmlzZXM= 26064
IEluZmVjdA== 26065
YXRyYQ== 26066
IFJS 26067
IExvcg== 26068
ZGlmZg== 26069
IENoYXJsZXN0b24= 26070
IGFjb3VzdGlj 26071
IGFtdXNl 26072
MzMw 26073
IGNlcg== 26074
IFRhYw== 26075
IFsr 26076
IGNhcmRpYWM= 26077
IFJlc3RhdXJhbnQ= 26078
ZXJneQ== 26079
IGZ1eno= 26080
IGJpdGVz 26081
IGhhemFyZG91cw== 26082
IGJyaWdodGVy 26083
cmFucw== 26084
IFN0ZXBoYW5pZQ== 26085
ZXh0cmE= 26086
UkVU 26087
IENocmlzdGluZQ== 26088
IFN1ZQ== 26089
c3RhdGVtZW50 26090
IGJvbHN0ZXI= 26091
IGFudGl0 26092
UmFkaW8= 26093
QklU 26094
44Kw 26095
IHZpc2lvbnM= 26096
IENvbmNlcHQ= 26097
IGlubGluZQ== 26098
IFBoaWxvc29waHk= 26099
aXNhbnM= 26100
IElydmluZw== 26101
w6M= 26102
dGFraW5n 26103
IGluY29uc2lzdA== 26104
IEt1bWFy 26105
IGxpZw== 26106
IFNjaHVtZXI= 26107
IFJlZ3VsYXRpb25z 26108
IEh6 26109
dGhybw== 26110
IFZvbGRlbW9ydA== 26111
IE1FRA== 26112
IEZyZWRlcmljaw== 26113
UGFk 26114
MjIx 26115
IGFsbGVnaW5n 26116
IENvbW11bmljYXRpb24= 26117
IDE2Nw== 26118
IGZvcmVjYXN0cw== 26119
IHNwaWRlcnM= 26120
T3JnYW4= 26121
IFBhcnRpY2lwYW50cw== 26122
IE9wcw== 26123
ZGVzaWdu 26124
Q2xvc2U= 26125
IGZhY3Rv 26126
IGJvbWJlcnM= 26127
cmVzaXN0YW50 26128
YXRlZ29yaWVz 26129
U2Nob29s 26130
IGhvbWV3b3Jr 26131
IGNvcnJv 26132
VHVlc2RheQ== 26133
IEJyZW5kYW4= 26134
IE1Y 26135
IFRT 26136
IFN0cmk= 26137
IHN0YWtlaG9sZGVycw== 26138
IE1pbGxlbm5pdW0= 26139
IHRyYW5zZmVycmluZw== 26140
SnVk 26141
IHRhYw== 26142
IDE2MDA= 26143
IFNESw== 26144
cmI= 26145
IGludGVycHJldGF0aW9ucw== 26146
IFNH 26147
IHVwc3RhaXJz 26148
IEhhcnZlc3Q= 26149
IHZhZ2luYQ== 26150
IGluZ2VzdA== 26151
eGY= 26152
IE9yaW9u 26153
IEpvZXk= 26154
IHNhbmR3aWM= 26155
IGltbW9ydGFs 26156
IGZsaXBwZWQ= 26157
b3J0ZXg= 26158
dGhyZWF0ZW5pbmc= 26159
IHNuaXBlcg== 26160
IGNvbnZlcnRz 26161
IGluc3RhbGxhdGlvbnM= 26162
IEJ1bGdhcg== 26163
b3JzY2hl 26164
bWFpbHM= 26165
IGx1cmU= 26166
IG5hcnJvd2x5 26167
IGdyZW5hZGU= 26168
IEdpbmc= 26169
IHVuZGVyd2Vhcg== 26170
LS0tLS0tLS0tLS0tLS0= 26171
IGNoYXNlZA== 26172
IFZBTA== 26173
IHBhcmVudGluZw== 26174
IEhhbWI= 26175
IEJsYXo= 26176
IGFuYXJjaGlzdA== 26177
IE1lZGlhbg== 26178
IFByb2dyYW1z 26179
zr0= 26180
IG9iag== 26181
IE5va2lh 26182
b3JtYW4= 26183
YW5xdQ== 26184
YXRpc20= 26185
b3Bh 26186
IGZ1bGZpbGxpbmc= 26187
IHB1cHB5 26188
IGVudGl0 26189
IFNlYmFzdGlhbg== 26190
IHNob290ZXJz 26191
IHJpY2hlcg== 26192
6KE= 26193
IHRlbXB0ZWQ= 26194
IEFUVA== 26195
IENW 26196
IHRvcmU= 26197
UmVzb3VyY2U= 26198
IERldmlscw== 26199
NDA4 26200
aW5hdGlvbmFs 26201
IGFzc3VyYW5jZQ== 26202
IERhcnJlbg== 26203
IHdoaWNoZXZlcg== 26204
cG9zdXJl 26205
IGZ1cnk= 26206
U3RvY2s= 26207
IHVuaXZlcnNhbGx5 26208
cmVzcG9uc2U= 26209
IG9haw== 26210
IHdvcmtsb2Fk 26211
IENvcm5lcg== 26212
ZWVsZQ== 26213
Ii4uLg== 26214
IGRlcHJpdmVk 26215
a293c2tp 26216
IGNhc3Rz 26217
IGFmZmlsaWF0aW9u 26218
IEFjaA== 26219
IEFza2Vk 26220
YXRoZQ== 26221
IGxhY3Q= 26222
IFRodQ== 26223
cm0= 26224
IGFpcmxpbmVz 26225
IG5vdGlvbnM= 26226
Rm9ybWF0 26227
IEZBQQ== 26228
44OK 26229
ZHJpdmVy 26230
IHRyYW5zY2VuZA== 26231
U2V0dGluZ3M= 26232
IFByb3NlY3V0 26233
IHNwaW5hbA== 26234
IGRlZmF1bHRz 26235
Rks= 26236
IHByZWZlcnM= 26237
cmVuZGVyZWQ= 26238
dGh1cw== 26239
ZmlsbQ== 26240
IHRpZ2Vy 26241
IFNwaWNlcg== 26242
cmVjb2du 26243
IFJ1Z2J5 26244
TmV0d29yaw== 26245
IHBpdHk= 26246
IGNvbXBhcnRtZW50 26247
Y2FzdGVycw== 26248
IE1vbnJvZQ== 26249
IDcyMA== 26250
IGNvcnJlY3Rpb25z 26251
IGRvcGFtaW5l 26252
IEFa 26253
Q3V0 26254
IHJvb21t 26255
IHNwZWN1bGF0ZQ== 26256
SGFzaA== 26257
IHJlc3RyaWN0aXZl 26258
MTExMQ== 26259
cmVkaWJsZQ== 26260
b25lbA== 26261
IHJhbXBhbnQ= 26262
cmVwb3J0ZWQ= 26263
IFN1aXRl 26264
IE1pbmltdW0= 26265
YWx5cw== 26266
YXphcmQ= 26267
bG9vcA== 26268
IGxlbnQ= 26269
c2hh 26270
IHZhbmRhbA== 26271
bWVudQ== 26272
IEJvZWhuZXI= 26273
IG5hcnJhdGl2ZXM= 26274
IGF1dGhlbnRpY2l0eQ== 26275
MjY5 26276
YW5pYw== 26277
ZHV0eQ== 26278
Mjg1 26279
IHRoYW5rZWQ= 26280
IGJldHJheWVk 26281
bGlmdA== 26282
IHNvdXRod2VzdA== 26283
IERleHRlcg== 26284
IEJvZA== 26285
IGtleXdvcmRz 26286
QXZlcmFnZQ== 26287
RElT 26288
IGV0aG5pY2l0eQ== 26289
ISks 26290
IE5hdGlvbmFscw== 26291
4bk= 26292
IFRhaA== 26293
aW94aWQ= 26294
IHdpZGdldA== 26295
IHBhc3Rh 26296
IGJpbGxpbmc= 26297
IHRyaWxvZ3k= 26298
IExpbmVz 26299
IHNuaWZm 26300
IG5lcGhldw== 26301
TGF0ZQ== 26302
IHByaW5jaXA= 26303
IExvb3A= 26304
IE1hcnhpc3Q= 26305
IGRpc3NvbHZlZA== 26306
IGNvbnRleHRz 26307
IEFtb3VudA== 26308
IFNwaWtl 26309
IHRvdGFscw== 26310
IG9yZ2FuaXplcg== 26311
IHVwcmlzaW5n 26312
c2hpcHM= 26313
WVk= 26314
IE5vcnRoZWFzdA== 26315
bW9uZXk= 26316
Z3JhZGF0aW9u 26317
IGdvYWxrZWVwZXI= 26318
IEhlYXI= 26319
IHN0ZWFr 26320
IEJ1enpGZWVk 26321
IHNvbGVtbg== 26322
IFNjYW5k 26323
IHBvcHBpbmc= 26324
IGFkaGVyZQ== 26325
IEFsbGVn 26326
Ynl0ZQ== 26327
IFdvbHZlcg== 26328
IHVuaW4= 26329
IHJlY29s 26330
aXR1ZA== 26331
IG1pbWlj 26332
aWJ1cw== 26333
IHByZWRpY3Rz 26334
IEtlZXBlcg== 26335
aWF0aW5n 26336
IGRlY2VwdGlvbg== 26337
IGxlYXJudA== 26338
IGRpYXJ5 26339
IGNvbmRpdGlvbmFs 26340
IHJlbGlj 26341
IGludm9rZQ== 26342
aWVuY2Vk 26343
5Yg= 26344
IFBvbnQ= 26345
IGNlbGxwaG9uZQ== 26346
IHNwZWVkaW5n 26347
IHRhY2tsaW5n 26348
IG51ZGU= 26349
b3BlbmVk 26350
IE1hbmFmb3J0 26351
IDE5NTI= 26352
IG1ham9ycw== 26353
IFNpbGVuY2U= 26354
IGxvZ2lzdGljcw== 26355
IHdlaWdodGVk 26356
IFBzeWNoaWF0 26357
IjpbIg== 26358
IHNpY2tuZXNz 26359
IGRpdmlkZW5kcw== 26360
em9u 26361
UmVsZWFzZQ== 26362
IEtleXM= 26363
IEljaA== 26364
IGVueg== 26365
IEZlcm5hbmQ= 26366
IM6x 26367
IG1lYW5pbmdz 26368
IHBlbm55 26369
IHN0ZXJu 26370
IGxhcg== 26371
IFB1Ymxpc2hlZA== 26372
IGJhY2tkcm9w 26373
S2lt 26374
IFN5bnQ= 26375
IGRlYnV0ZWQ= 26376
d20= 26377
IElzbGU= 26378
IHJlZ3VsYXRpbmc= 26379
b3R0aQ== 26380
IFNjaG9sYXJz 26381
aWNlc3Rlcg== 26382
IENoZWY= 26383
IHBvcHM= 26384
IExhdW5jaGVy 26385
IFZhcmlvdXM= 26386
IGNvbW1lbnRpbmc= 26387
b3NsYXY= 26388
ZW56aWU= 26389
IHJpdmFscnk= 26390
4oKs 26391
UmVhbGx5 26392
IG9yYw== 26393
IGJlYW4= 26394
IEp1ZHk= 26395
Tm90aWNl 26396
IEJpa2U= 26397
P10= 26398
IHJlbnRlZA== 26399
c3Rlbg== 26400
IGZvcmVmcm9udA== 26401
IEJhbGR3aW4= 26402
IHlpZWxkZWQ= 26403
dGFpbHM= 26404
UHJpbWU= 26405
IFNvdXJjZXM= 26406
aWNhdG9y 26407
U2Vhbg== 26408
IG1hcmNoaW5n 26409
T3V0cHV0 26410
IEp1bmdsZQ== 26411
IHJlc2lkZQ== 26412
enpsZQ== 26413
IEFuZHJld3M= 26414
IHRvcnF1ZQ== 26415
QmFzaWM= 26416
QWN0dWFsbHk= 26417
c3RyYXA= 26418
cGVudGVy 26419
IGV4YW1z 26420
IFlh 26421
IDE1OQ== 26422
IERlY2lzaW9u 26423
IHJhbnNvbQ== 26424
ZXRlZW50aA== 26425
ZW5zaW5n 26426
MjEz 26427
IHN1bnNldA== 26428
NDA0 26429
IFJhcGlk 26430
IEhlaW4= 26431
IEFib3JpZ2luYWw= 26432
IG9yZ2FuaXNt 26433
IFNldmVy 26434
IGNsYQ== 26435
YWpp 26436
U2ltcGxl 26437
IEZsYXZvcg== 26438
IEV2YWw= 26439
cHJ1cw== 26440
IGNob3J1cw== 26441
REFZ 26442
IGRlbm91bmNlZA== 26443
IGJpb2dyYXBoeQ== 26444
IFR1cm5idWxs 26445
UmVjZW50 26446
Tm9ybWFs 26447
bGVjdGlvbnM= 26448
V29yZA== 26449
IGZlcnJ5 26450
IFdhZ25lcg== 26451
aG9t 26452
VW5pdA== 26453
IHN1cGVybWFya2V0 26454
IFNpdGg= 26455
IG5vbWluZWVz 26456
IGRpY3RhdG9yc2hpcA== 26457
aWRkbGVy 26458
IGFubm91bmNlcw== 26459
IFRoZW0= 26460
IE5lcHR1bmU= 26461
IGRlaXR5 26462
IFlp 26463
IG1vbmFyY2g= 26464
QVJS 26465
IGludmFkZWQ= 26466
IEhvaw== 26467
dW50YXJ5 26468
Q2VydGFpbg== 26469
ZWdh 26470
IGtpZGRpbmc= 26471
IFJlZ3VsYXRpb24= 26472
IHRyYXk= 26473
IHBob3RvZ3JhcGhlcnM= 26474
IEFyY2FuZQ== 26475
IGRpc2NoYXJnZWQ= 26476
IGV2YW5nZWxpY2Fs 26477
IGludGVyY2hhbmdl 26478
IGZpbG1tYWtlcg== 26479
IEVuZGxlc3M= 26480
IDI5MA== 26481
IFNhbHZhZG9y 26482
QVNZ 26483
IFNpZ25hbA== 26484
IHdyYXRo 26485
4pw= 26486
bG90 26487
Jy8= 26488
IHByb2plY3RpbGU= 26489
IGVtcGxveWluZw== 26490
IEludGVyZmFjZQ== 26491
MTkx 26492
YXRlbGxpdGU= 26493
IFJhdGg= 26494
cGFja2FnZQ== 26495
IGluZGljYXRpb25z 26496
SmFzb24= 26497
IGFyZ3M= 26498
IEdIeg== 26499
IHRpbHQ= 26500
bmFudHM= 26501
d29u 26502
44K1 26503
cmVkZA== 26504
cmVzY2VudA== 26505
IENhbGVuZGFy 26506
IG1vZHVsYXI= 26507
IGFzc2lzdGluZw== 26508
IHJlZGVlbQ== 26509
IEJlYW4= 26510
IHdvcnNo 26511
IGRlY2VudHJhbGl6ZWQ= 26512
KS4uLg== 26513
Mzc3 26514
IGFycmF5cw== 26515
IGFjY29tcGxpc2htZW50cw== 26516
zr8= 26517
ZG90 26518
IG11dHVhbGx5 26519
IG9ic3RydWN0 26520
IG1pc3JlcHJlc2VudA== 26521
b3Jlc3Q= 26522
aW9uaWM= 26523
cnVjZQ== 26524
JTs= 26525
IGtub3dpbmdseQ== 26526
cG9ydGluZw== 26527
aW5lbnRseQ== 26528
QXJp 26529
IFNjaHVsdHo= 26530
RGE= 26531
IENlcmU= 26532
IG9ic29sZXRl 26533
hYs= 26534
Z2l2ZQ== 26535
IGJhaXQ= 26536
IGVubGFyZw== 26537
TmVpbGw= 26538
IDE5MzM= 26539
IHJlY29uc2lkZXI= 26540
IFNlcmdlYW50 26541
IERpYW5l 26542
IENvZ24= 26543
IEljb24= 26544
UG9zaXRpb24= 26545
IGZvc3Q= 26546
IHN0aXJyaW5n 26547
c2V2ZW4= 26548
IFNwYWNlWA== 26549
dWdnZXRz 26550
IG1lZGQ= 26551
R2Fs 26552
IFNpc3Rlcg== 26553
Qm95 26554
IHRyaWdnZXJpbmc= 26555
VGFraW5n 26556
IHNjcmVhbXM= 26557
IGNhdXNhbA== 26558
IGF3YWtlbg== 26559
QXJt 26560
Mjk3 26561
IGRpc3BhdGNoZWQ= 26562
IEZBTFNF 26563
IG9yZ2FuaXphdGlvbmFs 26564
IFRvbmc= 26565
IGRpbGVtbWE= 26566
ZGVtb24= 26567
U3Bs 26568
IGhvb2tz 26569
dWRpbmc= 26570
IHZhbGlkYXRl 26571
IHBvdGlvbg== 26572
IGNsYXc= 26573
IGJ1cmds 26574
IHF1aXI= 26575
QUNB 26576
IEJyZW5uYW4= 26577
IGR1cmFiaWxpdHk= 26578
IGJvbWJpbmdz 26579
IFdpbmRvdw== 26580
IGN1bHByaXQ= 26581
MzI1 26582
VGhlcmVmb3Jl 26583
dW1iZXJlZA== 26584
cGVyZm9ybWFuY2U= 26585
d2FydHM= 26586
IGVuZm9yY2luZw== 26587
IEJsb3c= 26588
IHJlcHJpbnQ= 26589
aWZheA== 26590
YWxwaGE= 26591
IHNpbmlzdGVy 26592
IGJ1cmdlcg== 26593
ZmlnaHRpbmc= 26594
U2NvcmU= 26595
IFN0b25lcw== 26596
aWVt 26597
NDA1 26598
Y2hlbXk= 26599
IHZpbmVnYXI= 26600
bm9t 26601
IHByZXZhaWxpbmc= 26602
IExhdGVzdA== 26603
wrY= 26604
IGJh 26605
IFdyaXRlcg== 26606
IDE3Nw== 26607
IENvbndheQ== 26608
IGNvbGxlY3Rz 26609
IHF1YW50aXRhdGl2ZQ== 26610
IGhvcnJvcnM= 26611
b2dlbnM= 26612
IFNsb3Y= 26613
IGxheXM= 26614
aGF3 26615
IFNsYXNo 26616
IG5pZ2h0Y2x1Yg== 26617
IERhdmllcw== 26618
IGJyaWRl 26619
IFNjYXJsZXQ= 26620
eW1t 26621
IEFwcGxpY2F0aW9ucw== 26622
dmVsZW5ndGg= 26623
IHJldml2YWw= 26624
IHNvZnRseQ== 26625
IHpvbw== 26626
aXRhaXJl 26627
Q3Vy 26628
IGVsZWN0cm9t 26629
IHBsYW50aW5n 26630
T1RP 26631
IEVsZW1lbnRz 26632
IHN3YWxsb3c= 26633
cG9ydGVy 26634
IGxhcHRvcHM= 26635
IHBlYW51dA== 26636
IGxvYmJ5aXN0cw== 26637
zrI= 26638
UGFuZWw= 26639
IEpvYW4= 26640
aW1pbA== 26641
dG5j 26642
IHJlc2lzdGVk 26643
IG91dHdl 26644
IHJldGFpbmluZw== 26645
YXRyaQ== 26646
IHBvb3Jlcg== 26647
IFN5cmlhbnM= 26648
IEhhbW1vbmQ= 26649
IHdlbGQ= 26650
dWRlcg== 26651
dG9waWM= 26652
IFRU 26653
cmljaWE= 26654
IHRoaWV2ZXM= 26655
TGlj 26656
IEd1c3Q= 26657
IFdheXM= 26658
YXJldGg= 26659
MjQz 26660
IGJyb2FkY2FzdGVy 26661
c2hpZWxk 26662
YXNzaXVt 26663
dWJsZQ== 26664
IGFpcnN0cmlrZXM= 26665
b25zbw== 26666
IHBlZGFs 26667
IGNvbGxlY3RvcnM= 26668
IFZhbmRlcg== 26669
IE1lc2E= 26670
IGRpY3RhdG9y 26671
IGRpcg== 26672
ZW50b24= 26673
Y2FydA== 26674
c2NvcmU= 26675
YWRkZXI= 26676
Q3J5 26677
IHNzaA== 26678
Z2dlcg== 26679
IGRydW5rZW4= 26680
IEdT 26681
IFNlYXQ= 26682
IGNvcm5lcmJhY2s= 26683
IHNraXBwZWQ= 26684
IFJlc2VhcmNoZXJz 26685
IEF1ZGk= 26686
UmVmZXJlbmNl 26687
IGhhdW50ZWQ= 26688
w6s= 26689
IENsaW5pYw== 26690
Y3o= 26691
IHBz 26692
IFBhbGFkaW4= 26693
IFJlY2lwZQ== 26694
IHN0aWdtYQ== 26695
b3BweQ== 26696
IG1vbmtleXM= 26697
IEhhd2s= 26698
U2Fk 26699
Ii8+ 26700
IFdvcmtzaG9w 26701
IFJldGFpbA== 26702
IEF2YXRhcg== 26703
NjI1 26704
TmE= 26705
IFZD 26706
IFNlY3VyZQ== 26707
TVk= 26708
MTk4OA== 26709
b3NzaXA= 26710
IHByb3N0YXRl 26711
IHVuZGVu 26712
IGdhbWVy 26713
IENvbnRlbnRz 26714
IFdhcmhhbW1lcg== 26715
IFNlbnRpbmVs 26716
MzEw 26717
IHNlZ3JlZ2F0aW9u 26718
IEZsZXg= 26719
IE1BWQ== 26720
IGRyaWxscw== 26721
IERydWdz 26722
SXNsYW1pYw== 26723
IHNwdXI= 26724
IGNhZmU= 26725
IGltYWdpbmFyeQ== 26726
IGd1aWRpbmc= 26727
IHN3aW5ncw== 26728
IFRoZW1l 26729
b2J5 26730
IG51ZA== 26731
IGJlZ2dpbmc= 26732
IHN0cm9uZ2g= 26733
IHJlamVjdGluZw== 26734
IHBlZGVzdHJpYW5z 26735
IFByb3NwZWN0 26736
UmFyZQ== 26737
c2xl 26738
IGNvbmNlc3Npb25z 26739
IENvbnN0aXR1dGlvbmFs 26740
IGJlYW1z 26741
IGZpYmVycw== 26742
cG9vbg== 26743
IGluc3RpbmN0cw== 26744
cHJvcGVydHk= 26745
IEJJRw== 26746
U2FuZGVycw== 26747
aW1hdGVz 26748
IGNvYXRpbmc= 26749
IGNvcnBzZXM= 26750
IFRSVUU= 26751
Y2hlY2tlZA== 26752
IDE2Ng== 26753
QXNo 26754
IEpT 26755
IEZpY3Rpb24= 26756
IGNvbW11bmFs 26757
IGVuZXJnZXRpYw== 26758
b29vb29vb28= 26759
IG5vd2FkYXlz 26760
SUxE 26761
aWJv 26762
IFNVVg== 26763
UmVu 26764
IGR3ZWxsaW5n 26765
U2lsdmVy 26766
IHRhbGx5 26767
IE1vdmluZw== 26768
IGNvd2FyZA== 26769
IGdlbmVyYWxz 26770
IGhvcm5z 26771
IGNpcmN1bGF0ZWQ= 26772
IHJvYmJlZA== 26773
IFVubGltaXRlZA== 26774
IGhhcmFzc2Vk 26775
IGluaGliaXQ= 26776
IGNvbXBvc2Vy 26777
IFNwb3RpZnk= 26778
IHNwcmVhZHM= 26779
MzY0 26780
IHN1aWNpZGFs 26781
IG5vaXNlcw== 26782
IFN0dXI= 26783
IHNhZ2E= 26784
IEthZw== 26785
aXNv 26786
IHRoZW9yZXRpY2FsbHk= 26787
TW9uZXk= 26788
IHNpbWlsYXJpdHk= 26789
IHNsaWNlZA== 26790
dXRpbHM= 26791
aW5nZXM= 26792
Ii0= 26793
IGFudGg= 26794
IGltcGVk 26795
TW9kdWxl 26796
VGhyb3VnaG91dA== 26797
IG1lbnVz 26798
Y29tbWl0dGVl 26799
YW5kaQ== 26800
b2Jq 26801
aW5hdg== 26802
ZmlyZWQ= 26803
IEFiZHVsbGFo 26804
IHVuZGVhZA== 26805
IGZvbnRz 26806
SG9sZA== 26807
RU5H 26808
IHN1c3RhaW5hYmlsaXR5 26809
IGZsaWNr 26810
IHJhem9y 26811
IEZlc3Q= 26812
IENoYXJhY3RlcnM= 26813
IHdvcmRpbmc= 26814
IHBvcHVsaXN0 26815
IGNyaXRpY2l6aW5n 26816
IG11c2U= 26817
dmluZQ== 26818
IGNhcmRib2FyZA== 26819
IGtpbmRseQ== 26820
IGZyaW5nZQ== 26821
IFRoZWZ0 26822
aWN1bHR1cmFs 26823
IGdvdmVybm9ycw== 26824
IO+/ve+/ve+/ve+/vQ== 26825
IDE2Mw== 26826
IHRpbWVvdXQ= 26827
IEF1dGg= 26828
Q2hpbGRyZW4= 26829
QVU= 26830
IHJlZGVtcHRpb24= 26831
IEFsZ2Vy 26832
IDE5MTQ= 26833
IHdhdmVk 26834
IGFzdHJvbmF1dHM= 26835
b2dyYW1z 26836
IHN3YW1w 26837
IEZpbm5pc2g= 26838
IGNhbmRsZQ== 26839
IHRvbm5lcw== 26840
dXRt 26841
IHJheQ== 26842
IHNwdW4= 26843
IGZlYXJmdWw= 26844
YXJ0aWNsZXM= 26845
IGNhdXM= 26846
b3JpY2FsbHk= 26847
IFJlcXVpcmVz 26848
IEdvbA== 26849
IHBvcGU= 26850
IGluYXVndXJhbA== 26851
IGdsZQ== 26852
QURB 26853
IElTSUw= 26854
IE9mZmVuc2l2ZQ== 26855
IHdhdGNoZG9n 26856
IGJhbGNvbg== 26857
ZW50aXR5 26858
IEhvbw== 26859
IGdhbGxvbg== 26860
QUND 26861
IGRvdWJsaW5n 26862
IGltcGxpY2F0aW9u 26863
IFNpZ2h0 26864
IGRvY3Ry 26865
LS0tLS0tLQ== 26866
IFxc 26867
IG1hbHQ= 26868
Um9sbA== 26869
IOKJpQ== 26870
IHJlY2Fw 26871
YWRkaW5n 26872
dWNlcw== 26873
IEJlbmQ= 26874
ZmlndXJl 26875
IHR1cmtleQ== 26876
IHNvY2lldGFs 26877
IFRpY2tldHM= 26878
IGNvbW1lcmNpYWxseQ== 26879
IHNwaWN5 26880
IDIxNg== 26881
IFJhbXA= 26882
IHN1cGVyaW9yaXR5 26883
w68= 26884
IFRyYWNrZXI= 26885
Q2FybA== 26886
IENveQ== 26887
IFBhdHJpb3Q= 26888
IGNvbnN1bHRlZA== 26889
IGxpc3Rpbmdz 26890
IHNsZXc= 26891
cmVlbnNob3Q= 26892
IEdvbmU= 26893
IFsuLi5d 26894
MzA5 26895
IGhvdHRlc3Q= 26896
2LE= 26897
IHJvY2t5 26898
IERpYXo= 26899
IG1hc3NhZ2U= 26900
IHBhcmFseQ== 26901
IHBvbnk= 26902
QXo= 26903
IGNhcnRyaWRnZQ== 26904
IE5a 26905
IHNuYWNr 26906
IExhbWFy 26907
cGxlbWVudA== 26908
IExlc2xpZQ== 26909
IG1hdGVy 26910
IHNuaXBw 26911
MjQ2 26912
IGpvaW50bHk= 26913
IEJyaXNiYW5l 26914
IGlQb2Q= 26915
IHB1bXBpbmc= 26916
IGdvYXQ= 26917
IFNoYXJvbg== 26918
ZWFsaW5n 26919
IGNvcm9u 26920
IGFub21hbA== 26921
cmFoaW0= 26922
IENvbm5lY3Rpb24= 26923
IHNjdWxwdHVyZQ== 26924
IHNjaGVkdWxpbmc= 26925
IERhZGR5 26926
YXRoaW5n 26927
IGV5ZWJyb3dz 26928
IGN1cnZlZA== 26929
IHNlbnRpbWVudHM= 26930
IGRyYWZ0aW5n 26931
RHJvcA== 26932
KFs= 26933
IG5vbWluYWw= 26934
IExlYWRlcnNoaXA= 26935
IEdyb3c= 26936
IDE3Ng== 26937
IGNvbnN0cnVjdGl2ZQ== 26938
aXZhdGlvbg== 26939
IGNvcnJ1cHRlZA== 26940
Z2VyYWxk 26941
IENyb3M= 26942
IENoZXN0ZXI= 26943
IExhcA== 26944
44Gq 26945
T1RI 26946
REFUQQ== 26947
IGFsbW9uZA== 26948
cHJvYmFibHk= 26949
SW1w 26950
IGZlYXN0 26951
IFdhcmNyYWZ0 26952
Rmxvcg== 26953
IGNoZWNrcG9pbnQ= 26954
IHRyYW5zY3JpcHRpb24= 26955
IDIwNA== 26956
IHR3ZWFrcw== 26957
IHJlbGlldmU= 26958
U2NpZW5jZQ== 26959
IHBlcmZvcm1lcg== 26960
Wm9uZQ== 26961
IHR1cm1vaWw= 26962
aWdhdGVk 26963
aGliaXQ= 26964
IENhZmU= 26965
dGhlbWVk 26966
IGZsdW9y 26967
YmVuY2g= 26968
IGRlY29t 26969
IFVudA== 26970
IEJhcnJldHQ= 26971
IEZhY3Rz 26972
IHRhc3Rpbmc= 26973
IFBUU0Q= 26974
IFNlYWw= 26975
IEp1ZGFpc20= 26976
IER5bmFtaWM= 26977
IENvcnM= 26978
VmU= 26979
IE1pbmc= 26980
IFRyYW5zZm9ybQ== 26981
dm9u 26982
IERlZmVuZGVycw== 26983
IFRhY3RpY2Fs 26984
IFZvbg== 26985
IFVuaXZlcnM= 26986
IGRpc3RvcnRlZA== 26987
IEJyZWF0aA== 26988
Pyci 26989
IGFnb24= 26990
IERlYWRseQ== 26991
IGxhbg== 26992
IEN5Y2xl 26993
b3JuZWQ= 26994
IHJlbGlhYmx5 26995
IGdsb3I= 26996
IE1vbmtleQ== 26997
44Oh 26998
IGFkcmVu 26999
IG1pY3Jvd2F2ZQ== 27000
IEFsYmFu 27001
aXJjcmFmdA== 27002
ZGlnaXQ= 27003
c21hcnQ= 27004
IERyZWFk 27005
wq/Cr8Kvwq/Cr8Kvwq/Cr8Kvwq/Cr8Kvwq/Cr8Kvwq8= 27006
e3s= 27007
IFJvY2hlc3Rlcg== 27008
IHNpbXBsaWZpZWQ= 27009
IGluZmxpY3RlZA== 27010
IHRha2VvdmVy 27011
IHlvdXJzZWx2ZXM= 27012
YWRpdGlvbmFs 27013
IG11c2N1bGFy 27014
S1M= 27015
IGluZ2Vu 27016
VGF4 27017
IEZlYXR1cmU= 27018
Mjc3 27019
IGNydWM= 27020
IGNyYXRl 27021
IHVuaWRlbnRpZmllZA== 27022
IGFjY2xhaW1lZA== 27023
IE1hbmdh 27024
IEZyYW5jZXM= 27025
IE5lcGFs 27026
IEdlcmFsZA== 27027
IEt1d2FpdA== 27028
IHNsYWlu 27029
IEhlYg== 27030
IEdva3U= 27031
44Gu5g== 27032
Mjg2 27033
TXJz 27034
IENvZHk= 27035
IFNhbmN0dWFyeQ== 27036
MDE2 27037
IGRpc21hbnQ= 27038
IGRhdGFzZXQ= 27039
IEhvbmQ= 27040
YnVjaw== 27041
IFBhdHRlcnNvbg== 27042
IHBhbGV0dGU= 27043
IEdE 27044
aWNvbA== 27045
IExvZGdl 27046
IHBsYW5ldGFyeQ== 27047
YWtpbg== 27048
IFJlZ2lzdGVyZWQ= 27049
YWJ3ZQ== 27050
IFBldGVyc2J1cmc= 27051
IGhhaWxlZA== 27052
IFBpZWNl 27053
U2NoZQ== 27054
IERPSg== 27055
IGVudW1lcg== 27056
MTgx 27057
IE9ic2VydmVy 27058
IEJvbGQ= 27059
Zm91bmRlZA== 27060
Y29tbWVyY2U= 27061
IGV4cGxvaXRz 27062
IEZpbmRpbmc= 27063
VVJO 27064
IFNuZQ== 27065
IEFjaWQ= 27066
YXlldHRl 27067
IFZhbHVlcw== 27068
IGRyYXN0aWM= 27069
IGFyY2hpdGVjdHVyYWw= 27070
ICIu 27071
15U= 27072
dW1wZWQ= 27073
IHdyYXBwaW5n 27074
IHdpZG93 27075
IFNsYXllcg== 27076
bGFjZQ== 27077
b25jZQ== 27078
R2VybWFueQ== 27079
YXZvaWQ= 27080
IHRlbXBsZXM= 27081
UEFS 27082
w7Q= 27083
IEx1Y2lmZXI= 27084
IEZsaWNrcg== 27085
bG92 27086
Zm9yY2Vz 27087
IHNjb3V0aW5n 27088
IGxvdWRlcg== 27089
dGVzeQ== 27090
IGJlZm9yZWhhbmQ= 27091
xJM= 27092
IE5lb24= 27093
IFdvbA== 27094
IFR5cGljYWxseQ== 27095
IFBvbGl0aWNv 27096
LSstKw== 27097
IGJ1aWxkZXI= 27098
IGRlcml2ZQ== 27099
S2lsbA== 27100
IHBva2Vy 27101
IGFtYmlndW91cw== 27102
IGxpZnRz 27103
IGN5dA== 27104
IHJpYnM= 27105
b29kbGU= 27106
IFNvdW5kcw== 27107
aGFpcg== 27108
IFN5bmRyb21l 27109
dGY= 27110
IHByb3BvcnRpb25hbA== 27111
dWlk 27112
IHBlcnRhaW5pbmc= 27113
IEtpbmRsZQ== 27114
IE5lZ3Jv 27115
IHJlaXRlcmF0ZWQ= 27116
IFRvbmlnaHQ= 27117
b3Rocw== 27118
IENvcm5lbGw= 27119
IG93aW5n 27120
IDIwOA== 27121
ZWxmYXJl 27122
b2NhdGluZw== 27123
IEJpcmRz 27124
U3Vic2NyaWJl 27125
IGVzc2F5cw== 27126
IGJ1cmRlbnM= 27127
IGlsbHVzdHJhdGlvbnM= 27128
YXJpb3Vz 27129
RVJBTA== 27130
IENhbGN1bA== 27131
IHhlbg== 27132
IExpbmtlZElu 27133
IEp1bmc= 27134
IHJlZGVzaWdu 27135
Q29ubm9y 27136
Mjk2 27137
IHJldmVyc2Fs 27138
IEFkZWxhaWRl 27139
IExM 27140
IHNpbmtpbmc= 27141
IGd1bQ== 27142
VVNI 27143
Y2FwdA== 27144
IEdyaW1t 27145
IGZvb3RzdGVwcw== 27146
IENCRA== 27147
aXNwZXJz 27148
IHByb3Nl 27149
V2VkbmVzZGF5 27150
IE1vdmllcw== 27151
ZWRpbg== 27152
IG92ZXJ0dXJuZWQ= 27153
IGNvbnRlbnRpb3Vz 27154
VVNC 27155
fn5+fn5+fn5+fn5+fn5+fg== 27156
IENvcHBlcg== 27157
IHBvaW50bGVzcw== 27158
TlY= 27159
dmFsdWVz 27160
b2xwaGlu 27161
ZGFpbg== 27162
IGRlcG9zaXRlZA== 27163
IEdX 27164
IHByZWNlZGVk 27165
IENsYQ== 27166
IEdvbGVt 27167
IE5pbQ== 27168
IM6y 27169
IEVuZ2luZWVycw== 27170
bWlkZGxl 27171
IGZsYXR0 27172
b3BlcmF0aXZl 27173
IGNvdW5jaWxz 27174
aW1iYWJ3ZQ== 27175
ZWxpbg== 27176
IHN0cmVzc2Z1bA== 27177
IExE 27178
IHJlc2g= 27179
bGFrZQ== 27180
IHdoZWVsY2hhaXI= 27181
IEFsdGVybmF0aXZl 27182
IG9wdGltaXpl 27183
b3BlcmF0aW9u 27184
IHBlZWs= 27185
IG9uZXNlbGY= 27186
aWdpbA== 27187
IHRyYW5zaXRpb25z 27188
b3BhdGh5 27189
Ymxhbms= 27190
IDE2OQ== 27191
MTcx 27192
X19fX19fX19fX19fX19fX19fX19fX19fX19fX19fX19fX19fX19fX19fX19fX19fX19fX19fX19fX19fX19fXw== 27193
IGxhdW5kZXJpbmc= 27194
RW5j 27195
IERFQw== 27196
IHdvcmtvdXRz 27197
IHNwaWtlcw== 27198
IGRpbm9zYXVycw== 27199
IGRpc2NyaW1pbmF0b3J5 27200
UG9vbA== 27201
UmF0aGVy 27202
Mzg1 27203
Uk5B 27204
dGVzdGVycw== 27205
ZXRv 27206
IElkZW50aXR5 27207
IHZlaW4= 27208
IEJ1cnRvbg== 27209
IGFyY2FkZQ== 27210
NDIw 27211
VWx0aW1hdGVseQ== 27212
IFNhZGx5 27213
w7A= 27214
cGlsbA== 27215
IGN1Ymlj 27216
IFNwZWN0cnVt 27217
dGhlc2U= 27218
c3RhdGVz 27219
IHVub2ZmaWNpYWw= 27220
aGF3a3M= 27221
IEVWRVJZ 27222
IHJhaW5ib3c= 27223
IGluY2FyY2VyYXRpb24= 27224
YW5kaW5n 27225
IHN5bGw= 27226
IEV2ZXJ0b24= 27227
IDE3OQ== 27228
IFNlcmJpYQ== 27229
IDE4OQ== 27230
bWV0ZXI= 27231
IE1pY2tleQ== 27232
IGFudGlxdQ== 27233
IGZhY3R1YWw= 27234
bmVjaw== 27235
IE5hcmU= 27236
bm9ybQ== 27237
bXVzdA== 27238
IGhpZ2h3YXlz 27239
IGdsYW0= 27240
IGRpdmlkaW5n 27241
IFNxdWFkcm9u 27242
IE1hcnRoYQ== 27243
IGJpcnRocw== 27244
Q292ZXI= 27245
Ly8vLy8vLy8vLy8vLy8vLw== 27246
IFdvbmc= 27247
UGhvdA== 27248
IEFMUw== 27249
cmlv 27250
IE5vbmV0aGVsZXNz 27251
IExlbW9u 27252
IDIwNg== 27253
IEVF 27254
IGRlcml2YXRpdmU= 27255
IFdXSUk= 27256
dm90ZQ== 27257
IHRoZXJlaW4= 27258
IHNlcGFyYXRpbmc= 27259
NDQ2 27260
c3luYw== 27261
IFN0cmVldHM= 27262
IHJhdHQ= 27263
IG11bmljaXBhbGl0eQ== 27264
IFNob3J0bHk= 27265
IG1vbms= 27266
KSwi 27267
IHNjcnVi 27268
IG9wZXJhdGl2ZXM= 27269
TmVpdGhlcg== 27270
UGxhY2U= 27271
IExpbWl0 27272
RmVtYWxl 27273
IEFjdG9y 27274
Q2hhcmFjdGVy 27275
IGNvbnN0aXR1dGVk 27276
MzU3 27277
IHByb3Rlc3RlZA== 27278
IFN0cmF3 27279
IEhlaWdodA== 27280
aWxkYQ== 27281
IFR5cGg= 27282
IGZsb29kcw== 27283
IGNvc21ldGlj 27284
V0FZ 27285
cGVydHVyZQ== 27286
dXBvbg== 27287
dG9ucw== 27288
ZXNzaW5n 27289
IFBvY2tldA== 27290
IHJvb2Z0 27291
IENhdWNhcw== 27292
IGFudGlkZXByZXNz 27293
IGluY29tcGF0aWJsZQ== 27294
RUNE 27295
IG9wZXJh 27296
IENvbnRlc3Q= 27297
IGdlbmVyYXRvcnM= 27298
bGltZQ== 27299
RGVmZW5zZQ== 27300
MTk4Nw== 27301
Zm9ydW0= 27302
IHNhdmFnZQ== 27303
IEh1bmdhcmlhbg== 27304
bno= 27305
IG1ldGFsbGlj 27306
IGV4cGVsbGVk 27307
IHJlc2lkZW5jeQ== 27308
IGRyZXNzZXM= 27309
NjY2 27310
IENsZW1lbnQ= 27311
ZmlyZXM= 27312
Q2F0ZWdvcnk= 27313
IGdlZWs= 27314
YWxpcw== 27315
IGNlbWV0ZXJ5 27316
ZWR1Y2F0ZWQ= 27317
IGNyYXds 27318
IFVuYWJsZQ== 27319
IFR5c29u 27320
YWtpcw== 27321
IHBhcmRvbg== 27322
IFdyYQ== 27323
IHN0cmVuZ3RoZW5lZA== 27324
IEZvcnM= 27325
MzM1 27326
IEhD 27327
IE1vbmQ= 27328
IHZpc3VhbHM= 27329
IEJlYXRsZXM= 27330
ZXR0bGVtZW50 27331
IO8= 27332
Z3Jv 27333
IGJhc2g= 27334
IHBvb3Jlc3Q= 27335
IGV4Y2Vs 27336
IGFzcGlyYXRpb25z 27337
IE11bmljaXA= 27338
ZW5zaWJsZQ== 27339
IGNlcmVtb25pZXM= 27340
IGludGltaWRhdGlvbg== 27341
IENPTlRS 27342
YmVjaw== 27343
IEthcA== 27344
YXN1 27345
IHRyYWRlbWFya3M= 27346
IFNldw== 27347
IENvbXBldGl0aW9u 27348
bmV0d29yaw== 27349
IEFycmk= 27350
IFRldA== 27351
Um9hbWluZw== 27352
V0M= 27353
RGF0 27354
IHNvYg== 27355
IHBhaXJpbmc= 27356
IG92ZXJkb3Nl 27357
U0FZ 27358
YWJlcg== 27359
IHJldm9sdA== 27360
IEZhaA== 27361
YWN0aW5n 27362
ZXE= 27363
ZXN0YXRpb24= 27364
RmlnaHQ= 27365
IE1hcmtz 27366
Mjcz 27367
IDE3OA== 27368
UmF3 27369
44GL 27370
MzQ5 27371
YmxvY2tz 27372
IHZlcmdl 27373
ZXN0aW5l 27374
IFBvZGVzdGE= 27375
IGludmFzaXZl 27376
IHByb2ZvdW5kbHk= 27377
IEFv 27378
ZWFjaA== 27379
IGxlc3Q= 27380
aW50ZXJwcmV0 27381
IHNocmlua2luZw== 27382
IGVycm9uZQ== 27383
IGNoZWVz 27384
bHlz 27385
IEl2eQ== 27386
IERpcmVjdG9yeQ== 27387
IGhpbnRlZA== 27388
VklDRQ== 27389
IGNvbnRhY3Rpbmc= 27390
IEdlbnQ= 27391
aGVp 27392
IGxhYmVsaW5n 27393
IG1lcmN1cnk= 27394
IExpdGU= 27395
IGV4cGlyZXM= 27396
IGRlc3RhYmls 27397
cml0aXM= 27398
Y3U= 27399
IGZlYXRoZXJz 27400
IHN0ZWVy 27401
IHByb2dyYW1tZWQ= 27402
IFZhZGVy 27403
R29pbmc= 27404
IEVsaW0= 27405
IHlv 27406
IE1pY2hl 27407
IDIwMw== 27408
IHNsZWV2ZXM= 27409
IGJ1bGx5 27410
IEh1bWFucw== 27411
MzY4 27412
IGNvbXByZXNz 27413
IEJhbm5lcg== 27414
QVJT 27415
IGF3aGlsZQ== 27416
IGNhbGli 27417
IHNwb25zb3JzaGlw 27418
IERpZmZpY3VsdHk= 27419
IFBhcGVycw== 27420
IGlkZW50aWZpZXI= 27421
fS4= 27422
IHlvZw== 27423
IFNoaWE= 27424
IGNsZWFudXA= 27425
IHZpYmU= 27426
aW50cm9kdQ== 27427
aW1taW5n 27428
QXVzdHJhbGlh 27429
IG91dGxpbmVz 27430
IFlvdXR1YmU= 27431
dHJhaW4= 27432
IE1ha2Vz 27433
IGRlcG9ydGVk 27434
IGNlbnRy 27435
IER1Zw== 27436
IEJvdWxkZXI= 27437
IEJ1ZmZ5 27438
IGluanVuY3Rpb24= 27439
IEhhcmxleQ== 27440
IEdyb3Vwcw== 27441
IER1bWJsZWRvcmU= 27442
IENsYXJh 27443
ICIt 27444
IHNhY3JpZmljZWQ= 27445
ZXBo 27446
U2hhZG93 27447
aWJsaW5n 27448
IGZyZWVsYW5jZQ== 27449
IGV2aWRlbnRseQ== 27450
cGhhbA== 27451
IHJldGFpbnM= 27452
TWly 27453
IGZpbml0ZQ== 27454
ZGFy 27455
IENvdXM= 27456
IHJlcGFpcmVk 27457
IHBlcmlvZGlj 27458
IGNoYW1waW9uc2hpcHM= 27459
IGFzdGVyb2lk 27460
YmxpbmQ= 27461
IGV4cHJlc3NseQ== 27462
IEFzdHJvcw== 27463
IHNjYWxlZA== 27464
IGdlb2dyYXBoaWNhbA== 27465
IFJhcGlkcw== 27466
RW5qb3k= 27467
IGVsYXN0aWM= 27468
IE1vaGFtZWQ= 27469
TWFya2V0 27470
YmVnaW4= 27471
IGRpc2NvdmVycw== 27472
IHRlbGVjb21tdW5pY2F0aW9ucw== 27473
IHNjYW5uZXI= 27474
IGVubGFyZ2U= 27475
IHNoYXJrcw== 27476
IHBzeWNoZWRlbA== 27477
IFJvdWdl 27478
IHNuYXBzaG90 27479
aXNpbmU= 27480
WFA= 27481
IHBlc3RpY2lkZXM= 27482
IExTRA== 27483
IERpc3RyaWJ1dGlvbg== 27484
cmVhbGx5 27485
IGRlZ3JhZGF0aW9u 27486
IGRpc2d1aXNl 27487
IGJpb20= 27488
IEVYVA== 27489
IGVxdWF0aW9ucw== 27490
IGhhemFyZHM= 27491
IENvbXBhcmVk 27492
KSo= 27493
IHZpcnR1ZXM= 27494
IGVsZGVycw== 27495
IGVuaGFuY2luZw== 27496
IEFjcm9zcw== 27497
ZXJvcw== 27498
YW5nbGluZw== 27499
IGNvbWJ1c3Q= 27500
dWNjaQ== 27501
IGNvbmN1c3Npb24= 27502
IGNvbnRyYWNlcHRpb24= 27503
IEthbmc= 27504
IGV4cHJlc3Nlcw== 27505
IGF1eA== 27506
IFBpb25l 27507
IGV4aGliaXRz 27508
RGVidWc= 27509
T1RBTA== 27510
IEFscmVhZHk= 27511
IFdoZWVsZXI= 27512
IGV4cGFuZHM= 27513
Pzo= 27514
IHJlY29uY2lsaWF0aW9u 27515
IHBpcmF0ZXM= 27516
IHB1cnNl 27517
IGRpc2NvdXJhZ2U= 27518
IHNwZWN0YWNsZQ== 27519
UmFuaw== 27520
IHdyYXBz 27521
IFRob3VnaHQ= 27522
IGltcGVuZGluZw== 27523
T3Bw 27524
IEFuZ2xv 27525
IEVVUg== 27526
IHNjcmV3ZWQ= 27527
cmV0Y2hlZA== 27528
IGVuY291cmFnZW1lbnQ= 27529
bW9kZWxz 27530
IGNvbmZ1c2U= 27531
bW1t 27532
IFZpdGFtaW4= 27533
4paR4paR 27534
Q3J1 27535
IGtuaWdodHM= 27536
IGRpc2NhcmQ= 27537
IGJpc2hvcHM= 27538
IFdlYXI= 27539
IEdhcnJldHQ= 27540
a2Fu 27541
44Of 27542
IG1hc2N1bGluZQ== 27543
Y2FwaXRhbA== 27544
IEF1cw== 27545
IGZhdGFsbHk= 27546
dGhhbmtz 27547
IEFV 27548
IEd1dA== 27549
MTIwMA== 27550
IDAwMDAwMDAw 27551
IHN1cnJvZw== 27552
IEJJT1M= 27553
cmFpdHM= 27554
IFdhdHRz 27555
IHJlc3VycmVjdGlvbg== 27556
IEVsZWN0b3JhbA== 27557
IFRpcHM= 27558
NDAwMA== 27559
IG51dHJpZW50 27560
IGRlcGljdGluZw== 27561
IHNwcmluaw== 27562
IG11ZmY= 27563
IExJTQ== 27564
IFNhbXBsZQ== 27565
cHNj 27566
aWJp 27567
Z2VuZXJhdGVk 27568
IHNwZWNpbWVucw== 27569
IGRpc3NhdGlzZg== 27570
IHRhaWxvcmVk 27571
IGhvbGRpbmdz 27572
IE1vbnRobHk= 27573
IEVhdA== 27574
cG9vbnM= 27575
IG5lYw== 27576
IENhZ2U= 27577
IExvdHVz 27578
IExhbnRlcm4= 27579
IGZyb250aWVy 27580
IHBlbnNpb25z 27581
IGpva2Vk 27582
IEhhcmR5 27583
PS09LT0tPS0= 27584
cmFkZQ== 27585
VUlE 27586
IHJhaWxz 27587
IGVtaXQ= 27588
IHNsYXRl 27589
IHNtdWc= 27590
IHNwaXQ= 27591
IENhbGxz 27592
IEphY29icw== 27593
ZmVhdA== 27594
IFVF 27595
IHJlc3RydWN0 27596
IHJlZ2VuZXJhdGlvbg== 27597
IGVuZXJnaWVz 27598
IENvbm5vcg== 27599
T0hO 27600
IENoZWVzZQ== 27601
IGdlcg== 27602
IHJlc3VycmVjdA== 27603
bWFuYWdlbWVudA== 27604
Tlc= 27605
IHByZXNlbnRseQ== 27606
IEJydWlucw== 27607
TWVtYmVy 27608
IE1hbmc= 27609
aWRhbg== 27610
IGJvb3N0aW5n 27611
d3lu 27612
Ky4= 27613
cmVxdWlzaXRl 27614
IE5ZUEQ= 27615
IE1lZ2Fu 27616
IENvbmRpdGlvbnM= 27617
IHBpY3M= 27618
bmVzaXVt 27619
IFJhc2g= 27620
IDE3NA== 27621
IER1Y2tz 27622
IGVtYnJv 27623
enU= 27624
b25pYW4= 27625
cmVsaWdpb3Vz 27626
IGNyYXo= 27627
IEFDQQ== 27628
IFp1Y2tlcg== 27629
RU1B 27630
IFByb3M= 27631
V2VhcG9u 27632
IEtub3g= 27633
IEFyZHVpbm8= 27634
IHN0b3Zl 27635
IGhlYXZlbnM= 27636
IFB1cmNoYXNl 27637
IGhlcmQ= 27638
IGZ1bmRyYWlzZXI= 27639
RGlnaXRhbA== 27640
NTAwMA== 27641
IHByb3BvbmVudHM= 27642
L+KAiw== 27643
IGplbGx5 27644
IFZpc2E= 27645
IG1vbmtz 27646
IGFkdmFuY2VtZW50 27647
IFdlcg== 27648
IDE4Nw== 27649
ZXVz 27650
ZXJ0aWxpdHk= 27651
IGZldGFs 27652
IDE5MzY= 27653
TG8= 27654
IG91dGZpdHM= 27655
IHN0YWlyY2FzZQ== 27656
Ym9tYg== 27657
IGN1c3RvbWl6ZWQ= 27658
Y2xhaXI= 27659
VHJlZQ== 27660
IG1hcHBlZA== 27661
IENvbnNpZGVyaW5n 27662
IFRvcnJlcw== 27663
IG1ldGh5bA== 27664
IGFwcHJveGltYXRl 27665
IGRvb20= 27666
IEhhbnNlbg== 27667
IGNyb3Nzb3Zlcg== 27668
IHN0YW5kYWxvbmU= 27669
5Lw= 27670
IGludml0ZXM= 27671
IGdyYXZleWFyZA== 27672
IGhw 27673
RG9uYWxkVHJ1bXA= 27674
IGVzY29ydA== 27675
R2Fy 27676
IHByZWRlY2Vzc29ycw== 27677
IGhheQ== 27678
IGVuenltZQ== 27679
IFN0cmFpZ2h0 27680
dmlzb3Jz 27681
SW5n 27682
YW5lb3VzbHk= 27683
IEFwcGxpZWQ= 27684
IGZlYw== 27685
IER1cmFudA== 27686
IG91dHNwb2tlbg== 27687
b3Ji 27688
IHplYWw= 27689
IGRpc2dyYWNl 27690
Jyku 27691
IENoZW5n 27692
Mjg5 27693
IFJlbmE= 27694
IFN1aWNpZGU= 27695
Mjk0 27696
IG91dHJhZ2Vk 27697
IE5ld21hbg== 27698
IE52aWRpYQ== 27699
IEFiZXI= 27700
IEJlcnM= 27701
IHJlY3JlYXRpb24= 27702
V2luZG93 27703
IERQ 27704
eGU= 27705
IHBlZG9waA== 27706
IGZhbGxvdXQ= 27707
YW1ib28= 27708
IHByZXNlbnRhdGlvbnM= 27709
IEFwcHM= 27710
IGh0bWw= 27711
MzQ1 27712
IFhYWA== 27713
IHJ1YmJpbmc= 27714
IExlYXRoZXI= 27715
IGh1bWlkaXR5 27716
c2V5cw== 27717
ZXN0YWJsaXNoZWQ= 27718
IFVuaXRz 27719
NjQ2 27720
IHJlc3BlY3RhYmxl 27721
QXV0bw== 27722
IHRocml2aW5n 27723
IElubm92YXRpb24= 27724
YW5ncw== 27725
RXh0cmE= 27726
cmVndWxhdGlvbg== 27727
Mjk4 27728
cGljaw== 27729
RXhhbXBsZXM= 27730
IENK 27731
QXR0YWNr 27732
IGRyYWNvbg== 27733
TFQ= 27734
IHN0aWNrZXI= 27735
cmVycw== 27736
IHN1bm55 27737
SXNz 27738
cmVndWxhdGVk 27739
ZGlt 27740
IEFic3RyYWN0 27741
IGh1c2JhbmRz 27742
T2ZmaWNl 27743
b21pbmF0aW9u 27744
aXRhcnM= 27745
QU5HRQ== 27746
YXNjYWw= 27747
IEtyaXM= 27748
IEluZmFudHJ5 27749
IG1hbGY= 27750
IEF0aGU= 27751
IFJhbGx5 27752
YmFsYW5jZWQ= 27753
Li4uLi4uLi4uLi4uLi4uLi4uLi4uLi4u 27754
T1VQ 27755
IG1vbGVjdWxl 27756
bWV0aWNz 27757
IFNwbGl0 27758
IEluc3RydWN0aW9ucw== 27759
IE5pZ2h0cw== 27760
Y2FyZHM= 27761
IHR1Zw== 27762
IGNvbmU= 27763
5a0= 27764
IHR4 27765
IERpc2N1c3Npb24= 27766
IGNhdGFzdHJvcGhl 27767
cHBl 27768
Z2lv 27769
IGNvbW11bmlzbQ== 27770
IGhhbHRlZA== 27771
IEd1YW50 27772
Y2xlYW4= 27773
IFNjaGVk 27774
IEthbnll 27775
IHdhbmRlcg== 27776
IFNlcmlvdXNseQ== 27777
IDE4OA== 27778
ZW5uaWFs 27779
Zm9sbG93 27780
cHJvZHVjdGl2ZQ== 27781
IEZsb3c= 27782
IFNhaWw= 27783
IGNyYXc= 27784
IHNpbXVsYXRpb25z 27785
b3J1 27786
YW5nbGVz 27787
IE5vbGFu 27788
IG1lbnN0cnU= 27789
NDcw 27790
IDIwNw== 27791
YWph 27792
IGNhc3VhbGx5 27793
Ym9hcmRpbmc= 27794
IDIyMg== 27795
b3Z5 27796
IE51bWJlcnM= 27797
dW1hdA== 27798
T0U= 27799
Mjg3 27800
IENsZW1zb24= 27801
IGNlcnRz 27802
IHNsaWQ= 27803
IFRyaWJl 27804
IHRvYXN0 27805
IGZvcnR1bmVz 27806
IGZhbHM= 27807
IENvbW1pdHRlZXM= 27808
IGdw 27809
IGZpZXJ5 27810
IE5ldHM= 27811
IEFuaW1l 27812
UGFja2FnZQ== 27813
IENvbXBhcmU= 27814
bGF1Z2h0ZXI= 27815
aW5mZWN0 27816
IGF0cm9jaXRpZXM= 27817
IGp1c3RpY2Vz 27818
IGluc3VsdHM= 27819
IFZlcm5vbg== 27820
IHNoYWtlbg== 27821
IHBlcnNvbmE= 27822
ZXN0YW1w 27823
MzY3 27824
YnJhaW4= 27825
IGV4cGVyaW1lbnRpbmc= 27826
S2Vu 27827
IEVsZWN0cm9uaWNz 27828
IDE2MQ== 27829
ZG9tYWlu 27830
IGdyYXBoaWNhbA== 27831
YmlzaG9w 27832
IHdob3BwaW5n 27833
IEV2YW5nZWw= 27834
IGFkdmVydGlzZXJz 27835
IFNwZWFy 27836
IGJpZHM= 27837
IGRlc3Ryb3lz 27838
dXR6 27839
IHVuZGVyc2M= 27840
IEFERA== 27841
IGFudHM= 27842
IEN1bQ== 27843
aXBwbGVz 27844
IEZpbGw= 27845
IGdsYW5jZWQ= 27846
IGluZGljdGVk 27847
IEVmZg== 27848
IG1pc2Nvbg== 27849
IERlc2t0b3A= 27850
IGFiaWRl 27851
44OA 27852
IElv 27853
IENvdWw= 27854
IGNhcHN1bGU= 27855
IENocnlz 27856
TU9O 27857
IHVuZGVz 27858
IElSQQ== 27859
IGNpdGF0aW9u 27860
IGRpY3RhdGU= 27861
IE5ldHdvcmtz 27862
IENvbmZsaWN0 27863
IFN0dWZm 27864
eGE= 27865
aXNlYw== 27866
IENoZW1pc3RyeQ== 27867
IHF1YXJ0ZXJseQ== 27868
V2lsbGlhbXM= 27869
YW5hbg== 27870
T3B0 27871
IEFsZXhhbmRyaWE= 27872
b3V0aGVhc3Rlcm4= 27873
IFNwcmluZ2ZpZWxk 27874
IEJsYWNrcw== 27875
IGdlb2dyYXBoeQ== 27876
MjQy 27877
IHV0bW9zdA== 27878
IEV4eG9u 27879
YWJvdXRz 27880
RVZB 27881
IEVuYWJsZQ== 27882
IEJhcnI= 27883
IGRpc2FncmVlZA== 27884
IEN5cHJ1cw== 27885
IGRlbWVudGlh 27886
IGxhYnM= 27887
IHViaXF1aXRvdXM= 27888
IExPVkU= 27889
IGNvbnNvbGlkYXRlZA== 27890
c3I= 27891
IGNyZWFteQ== 27892
IFRpbWJlcg== 27893
UmVnYXJkbGVzcw== 27894
IENlcnRpZmljYXRl 27895
ICIuLi4= 27896
b2dlbm91cw== 27897
Q2FwdGFpbg== 27898
IGluc3VsdGluZw== 27899
IFNvcm9z 27900
IEluc3Ry 27901
IEJ1bGdhcmlh 27902
YmV0dGVy 27903
IHN1Y2tpbmc= 27904
IERhdmlkc29u 27905
YXR6 27906
IGNvbGxhdGVyYWw= 27907
Z2lm 27908
IHBsYWd1ZWQ= 27909
IENhbmNlbA== 27910
IEdhcmRuZXI= 27911
UkI= 27912
IHNpeHRlZW4= 27913
UmVtb3Zl 27914
dXJpc3RpYw== 27915
Y29vaw== 27916
Um9k 27917
IGNvbXByaXNpbmc= 27918
Zmxl 27919
KeKAlA== 27920
IFZpa2luZw== 27921
Z3Jvd3Ro 27922
YWdvbmFs 27923
IHNyZg== 27924
YWZldHk= 27925
bW90 27926
TmVhcmx5 27927
c3Rvd24= 27928
IEZhY3Rvcg== 27929
IGF1dG9tb2JpbGU= 27930
IHByb2NlZHVyYWw= 27931
bWFzaw== 27932
YW1waXJlcw== 27933
IGRpc2FwcGVhcnM= 27934
amFi 27935
MzE1 27936
IDE5NTE= 27937
bmVlZGVk 27938
IGRhcmluZw== 27939
bGVhZGVy 27940
IHBvZGl1bQ== 27941
IHVuaGVhbHRoeQ== 27942
IG11bmQ= 27943
IHB5cmFtaWQ= 27944
b2NyZQ== 27945
IGtpc3NlZA== 27946
IGRyZWFtZWQ= 27947
IEZhbnRhc3RpYw== 27948
IEdseQ== 27949
5Yo= 27950
IGdyZWF0bmVzcw== 27951
IHNwaWNlcw== 27952
IG1ldHJvcG9saXRhbg== 27953
IGNvbXB1bHM= 27954
aWV0cw== 27955
MTAxNg== 27956
IFNoYW0= 27957
IFB5cg== 27958
ZmxpZXM= 27959
IE1pZG5pZ2h0 27960
IHN3YWxsb3dlZA== 27961
IGdlbnJlcw== 27962
IEx1Y2t5 27963
IFJld2FyZHM= 27964
IGRpc3BhdGNo 27965
IElQQQ== 27966
IEFwcGx5 27967
IGF2ZW4= 27968
YWxpdGllcw== 27969
MzEy 27970
dGhpbmdz 27971
ICgpLg== 27972
IG1hdGVz 27973
IFN6 27974
IENPUA== 27975
b2xhdGU= 27976
T0ZG 27977
IHJlY2hhcmdl 27978
Y2Fwcw== 27979
IFlvcmtlcg== 27980
aWNvbmU= 27981
IGdhbGF4aWVz 27982
aWxlYWtz 27983
RGF2ZQ== 27984
IFB1eno= 27985
IENlbHRpYw== 27986
IEFGQw== 27987
Mjc2 27988
IFNvbnM= 27989
IGFmZmlybWF0aXZl 27990
SG9y 27991
IHR1dG9yaWFscw== 27992
IENJVFk= 27993
IFJvc2E= 27994
IEV4dGVuc2lvbg== 27995
U2VyaWVz 27996
IGZhdHM= 27997
IHJhYg== 27998
bGlz 27999
IHVuaWM= 28000
IGV2ZQ== 28001
IFNwaW4= 28002
IGFkdWx0aG9vZA== 28003
dHlw 28004
IHNlY3Rhcmlhbg== 28005
IGNoZWNrb3V0 28006
IEN5Y2w= 28007
U2luZ2xl 28008
IG1hcnR5cg== 28009
IGNoaWxsaW5n 28010
ODg4 28011
b3VmbA== 28012
IF07 28013
IGNvbmdlc3Rpb24= 28014
bWs= 28015
IFdoZXJlYXM= 28016
IDE5Mzg= 28017
dXJyZW5jaWVz 28018
ZXJpb24= 28019
IGJvYXN0 28020
IFBhdGllbnRz 28021
IGNoYXA= 28022
IEJE 28023
cmVhbERvbmFsZFRydW1w 28024
IGV4YW1pbmVz 28025
aG92 28026
IHN0YXJ0bGluZw== 28027
IEJhYnlsb24= 28028
d2lk 28029
b21ldw== 28030
YnJhbmNl 28031
IE9keXNzZXk= 28032
d2ln 28033
IHRvcmNo 28034
IFZveA== 28035
IE1veg== 28036
IFRyb2xs 28037
IEFucw== 28038
U2ltaWxhcmx5 28039
IEZ1bA== 28040
MDA2 28041
VW5sZXNz 28042
IEFsb25l 28043
c3RlYWQ= 28044
IFB1Ymxpc2hlcg== 28045
cmlnaHRz 28046
dHU= 28047
IERvZXNu 28048
IHByb2Zlc3Npb25hbGx5 28049
IGNsbw== 28050
aWN6 28051
IHN0ZWFscw== 28052
IOE= 28053
MTk4Ng== 28054
IHN0dXJkeQ== 28055
IEpvaGFubg== 28056
IG1lZGFscw== 28057
IGZpbGluZ3M= 28058
IEZyYXNlcg== 28059
ZG9uZQ== 28060
IG11bHRpbmF0aW9uYWw= 28061
IGZlZGVy 28062
IHdvcnRobGVzcw== 28063
IHBlc3Q= 28064
WWVzdGVyZGF5 28065
YW5raW5k 28066
IGdheXM= 28067
IGJvcm5l 28068
IFBPUw== 28069
UGljdHVyZQ== 28070
IHBlcmNlbnRhZ2Vz 28071
MjUx 28072
cmFtZQ== 28073
IHBvdGlvbnM= 28074
QU1E 28075
IExlYmFuZXNl 28076
IHJhbmc= 28077
IExTVQ== 28078
b25ncw== 28079
IHBlbmluc3VsYQ== 28080
IENsYXVzZQ== 28081
QUxL 28082
b2hh 28083
IE1hY0Jvb2s= 28084
IHVuYW5pbW91cw== 28085
IGxlbmRlcnM= 28086
IGhhbmdz 28087
IGZyYW5jaGlzZXM= 28088
b3JlcnM= 28089
IFVwZGF0ZXM= 28090
IGlzb2xhdGU= 28091
YW5kcm8= 28092
U29vbg== 28093
IGRpc3J1cHRpdmU= 28094
IFN1cnZl 28095
IHN0aXRjaGVz 28096
IFNjb3Jw 28097
IERvbWluaW9u 28098
IHN1cHBseWluZw== 28099
QXJn 28100
IHR1cnJldA== 28101
IEx1aw== 28102
IGJyYWNrZXRz 28103
Kik= 28104
IFJldm9sdXRpb25hcnk= 28105
IEhvbmVzdA== 28106
IG5vdGljaW5n 28107
IFNoYW5ub24= 28108
IGFmZm9yZGVk 28109
IHRoYQ== 28110
IEphbmV0 28111
IS0t 28112
IE5hcmVuZHJh 28113
IFBsb3Q= 28114
SG9s 28115
c2V2ZXI= 28116
ZWVudGg= 28117
IG9ic3RydWN0aW9u 28118
IDEwMjQ= 28119
c3RhZmY= 28120
amFz 28121
b3JnZXQ= 28122
c2NlbmVz 28123
bGF1Z2hz 28124
IEZhcmdv 28125
Y3JpbWU= 28126
IG9yY2hlc3Ry 28127
IGRlbGV0 28128
aWxpYXJ5 28129
cmlldmVk 28130
IG1pbGl0YXI= 28131
IEdyZWVuZQ== 28132
4peP 28133
44Gm 28134
IEd1YXJkcw== 28135
IHVubGVhc2hlZA== 28136
IFdlYmVy 28137
IGFkanVzdGFibGU= 28138
IGNhbGliZXI= 28139
IG1vdGl2YXRpb25z 28140
IMOg 28141
bUFo 28142
IExhbmth 28143
aGFuZGxl 28144
IHBlbnQ= 28145
IFJhdg== 28146
IEFuZ3VsYXI= 28147
IEthdQ== 28148
dW1iaW5n 28149
IHBoaWxhbnRocm9w 28150
IGRlaHlk 28151
IHRveGljaXR5 28152
ZWVy 28153
IFlPUks= 28154
d2l0eg== 28155
5bw= 28156
IElF 28157
Y29tbXVuaXR5 28158
IEFI 28159
IHJldGFsaQ== 28160
IG1hc3NpdmVseQ== 28161
IERhbmllbHM= 28162
IERFTA== 28163
IGNhcmNpbg== 28164
VXJs 28165
IHJvdXRpbmc= 28166
IE5QQ3M= 28167
IFJBRg== 28168
cnljZQ== 28169
IHdhaXZlZA== 28170
IEd1YXRlbQ== 28171
RXZlcnlib2R5 28172
IGNvdmVuYW50 28173
IDE3Mw== 28174
IHJlbGF4aW5n 28175
IHF1YXJ0 28176
YWxtb3N0 28177
IGd1YXJkZWQ= 28178
IFNvbGRpZXJz 28179
IFBMQVk= 28180
IG91dGdvaW5n 28181
TEFORA== 28182
IHJld3JpdGU= 28183
IE1PVg== 28184
IEltcGVy 28185
IFNvbHV0aW9u 28186
IHBoZW5vbWVuYWw= 28187
IGxvbmdldml0eQ== 28188
IGltcGF0 28189
IE5pc3Nhbg== 28190
aXJpZQ== 28191
IG9kb3I= 28192
IFphcg== 28193
b2tz 28194
IG1pbGl0aWFz 28195
IFNQRUM= 28196
IHRvbGVyYXRlZA== 28197
YXJzZXI= 28198
IEJyYWRmb3Jk 28199
Kyw= 28200
IHN1cnJlYWw= 28201
c2Y= 28202
Q2FuYWRpYW4= 28203
IHJlc2VtYmxhbmNl 28204
IGNhcmJvaHlkcmF0ZQ== 28205
VklFVw== 28206
IGFjY2Vzc29yeQ== 28207
bWVhbA== 28208
bGFyZ2VzdA== 28209
aWVnZWw= 28210
U29tZW9uZQ== 28211
IHRvdWdoZXN0 28212
b3Nv 28213
IGZ1bm5lbA== 28214
IGNvbmRlbW5hdGlvbg== 28215
bHVlbnQ= 28216
IHdpcmVk 28217
IFN1bnNldA== 28218
SmVzdXM= 28219
IFBTVA== 28220
IFBhZ2Vz 28221
IFR5Y29vbg== 28222
IFBG 28223
IHNlbGVjdGlvbnM= 28224
IOCk 28225
cGFydGlzYW4= 28226
IGhpZ2hz 28227
IFJ1bmU= 28228
IGNyYWZ0cw== 28229
bGVhZA== 28230
IFBhcmVudHM= 28231
IHJlY2xhaW0= 28232
ZWtlcg== 28233
IEFsbGllZA== 28234
YWVwZXI= 28235
IGxvb21pbmc= 28236
IGJlbmVmaWNpYXJpZXM= 28237
IEh1bGw= 28238
U3R1ZGVudHM= 28239
SmV3aXNo 28240
ZGo= 28241
IHBhY3Q= 28242
dGVtcGxhdGU= 28243
IE9mZmljaWFscw== 28244
IEJheWxvcg== 28245
IGhlbXA= 28246
IHlvdXRocw== 28247
IExldmVscw== 28248
IFhpYW8= 28249
IENoZXM= 28250
IGVuZGVhdm9y 28251
IFJlbW92ZWQ= 28252
IGhpcHBvY2FtcA== 28253
SGVsbA== 28254
44KK 28255
ODA1 28256
IGRpbm9zYXVy 28257
IFdyYXRo 28258
IEluZG9uZXNpYW4= 28259
IGNhbGN1bGF0b3I= 28260
IERpY3Rpb25hcnk= 28261
IDQyMA== 28262
IE1BRw== 28263
KF8= 28264
ISw= 28265
dGFyaWFucw== 28266
IHJlc3RyaWN0aW5n 28267
cmFjdXNl 28268
IHdlZWtkYXk= 28269
T1VOVA== 28270
IHNocnVnZ2Vk 28271
bGVncm91bmQ= 28272
IGJhbGQ= 28273
IERvY3RvcnM= 28274
IHRvdXRlZA== 28275
IE1heHdlbGw= 28276
IDIxNA== 28277
IGRpcGxvbWF0 28278
IHJlcHJlc3Npb24= 28279
IGNvbnN0aXR1ZW5jeQ== 28280
dmljZQ== 28281
cmFua2Vk 28282
IE5hcG9sZW9u 28283
Z2FuZw== 28284
IEZvcmV2ZXI= 28285
dHVu 28286
IGJ1bGI= 28287
IFBEVA== 28288
IENpc2Nv 28289
VkVO 28290
IHJlc3VtZWQ= 28291
U3RldmVu 28292
IE1hbml0b2Jh 28293
IGZhYnVsb3Vz 28294
IEFnZW50cw== 28295
MTk4NA== 28296
IGFtdXNpbmc= 28297
IE15c3Rlcmllcw== 28298
IG9ydGhvZG94 28299
Zmxvb3I= 28300
IHF1ZXN0aW9ubmFpcmU= 28301
IHBlbmV0cmF0ZQ== 28302
IGZpbG1tYWtlcnM= 28303
IFVuYw== 28304
IHN0YW1wZWQ= 28305
IHRoaXJ0ZWVu 28306
IG91dGZpZWxk 28307
IGZvcndhcmRlZA== 28308
IGFwcHJh 28309
IGFpZGVk 28310
dHJ5 28311
IHVuZm9jdXNlZA== 28312
IExpeg== 28313
IFdlbmR5 28314
IFNjZW5l 28315
Q2hhcmc= 28316
IHJlamVjdHM= 28317
IGxlZnRpc3Q= 28318
IFByb3ZpZGVuY2U= 28319
IEJyaWQ= 28320
cmVnbg== 28321
IHByb3BoZWN5 28322
IExJVkU= 28323
NDk5 28324
IGZvcmdl 28325
IEZNTA== 28326
IGludHJpbnNpYw== 28327
IEZyb2c= 28328
IHdvbnQ= 28329
IEhvbHQ= 28330
IGZhbWVk 28331
Q0xVUw== 28332
YWVwZXJuaWNr 28333
IEhhdGU= 28334
IENheQ== 28335
IHJlZ2lzdGVyaW5n 28336
b3J0YWxpdHk= 28337
cm9weQ== 28338
b2NhbHlwdGlj 28339
YWFu 28340
bmF2 28341
IGZhc2Npc3Q= 28342
SUZJRUQ= 28343
IGltcGxpY2F0ZWQ= 28344
IFJlc29ydA== 28345
IENoYW5kbGVy 28346
IEJyaWNr 28347
UGlu 28348
eXNj 28349
VXNhZ2U= 28350
IEhlbG0= 28351
dXNyYQ== 28352
4piF4piF 28353
IEFiYmFz 28354
IHVuYW5pbW91c2x5 28355
IGtlZXBlcg== 28356
IGFkZGljdGVk 28357
Pz8/ 28358
IGhlbG1ldHM= 28359
IGFudGlveGlk 28360
YXBzZWQ= 28361
ODA4 28362
Z2llbmU= 28363
IHdhaXRz 28364
IG1pbmlvbg== 28365
cmF2ZWQ= 28366
IFBvcnNjaGU= 28367
IGRyZWFtaW5n 28368
IDE3MQ== 28369
IENhaW4= 28370
IHVuZm9y 28371
YXNzbw== 28372
IENvbmZpZ3VyYXRpb24= 28373
a3Vu 28374
aGFyZHQ= 28375
IG5lc3RlZA== 28376
IExEUw== 28377
TEVT 28378
IHR5aW5n 28379
ZW5vcw== 28380
IGN1ZQ== 28381
IE1hcnF1 28382
c2tpcnRz 28383
IGNsaWNrZWQ= 28384
IGV4cGlyYXRpb24= 28385
IEFjY29yZGluZ2x5 28386
IFdD 28387
IGJsZXNzaW5ncw== 28388
IGFkZGljdGl2ZQ== 28389
IE5hcnI= 28390
eXg= 28391
IEphZ3VhcnM= 28392
IHJlbnRz 28393
IFNpYmVy 28394
IHRpcHBlZA== 28395
b3Vzc2U= 28396
IEZpdHpnZXJhbGQ= 28397
IGhpZXJhcmNo 28398
b3V0aW5l 28399
IHdhdmVsZW5ndGg= 28400
Pi4= 28401
Y2hpZA== 28402
IFByb2Nlc3Npbmc= 28403
Lys= 28404
cmFua2luZw== 28405
RWFzeQ== 28406
IENvbnN0cnVjdA== 28407
IHRldA== 28408
aW5zdXJlZA== 28409
SFVE 28410
IHF1b3Rpbmc= 28411
IGNvbW11bmljYXRlZA== 28412
aW54 28413
IGlubWF0ZQ== 28414
IGVyZWN0ZWQ= 28415
IEFic29sdXRlbHk= 28416
IFN1cmVseQ== 28417
IHVuaW0= 28418
IFRocm9uZQ== 28419
aGVpZA== 28420
IGNsYXdz 28421
IHN1cGVyc3Rhcg== 28422
IExlbm4= 28423
IFdoaXM= 28424
VWs= 28425
YWJvbA== 28426
IHNrZXQ= 28427
IE5pZXQ= 28428
IHBlcmtz 28429
IGFmZmluaXR5 28430
IG9wZW5pbmdz 28431
cGhhc2lz 28432
IGRpc2NyaW1pbmF0ZQ== 28433
VGlw 28434
dmM= 28435
IGdyaW5kaW5n 28436
IEplbm55 28437
IGFzdGhtYQ== 28438
aG9sZXM= 28439
IEhvbWVy 28440
IHJlZ2lzdGVycw== 28441
IEdsYWQ= 28442
IGNyZWF0aW9ucw== 28443
IGxpdGhpdW0= 28444
IGFwcGxhdXNl 28445
dW50aWw= 28446
SnVzdGljZQ== 28447
IFR1cmtz 28448
IHNjYW5kYWxz 28449
IGJha2U= 28450
dGFuaw== 28451
TWVjaA== 28452
IE1lYW5z 28453
IE1haWQ= 28454
UmVwdWJsaWNhbnM= 28455
aXNhbA== 28456
d2luZG93cw== 28457
IFNhbnRvcw== 28458
IHZlZ2V0YXRpb24= 28459
MzM4 28460
dHJp 28461
IGZsdXg= 28462
aW5zZXJ0 28463
IGNsYXJpZmllZA== 28464
IG1vcnRn 28465
IENoaW0= 28466
IFRvcnQ= 28467
IGRpc2NsYWlt 28468
bWV0YWw= 28469
IEFzaWRl 28470
IGluZHVjdGlvbg== 28471
IGluZmw= 28472
IGF0aGVpc3Rz 28473
YW1waA== 28474
IGV0aGVy 28475
IFZpdGFs 28476
IEJ1aWx0 28477
TWluZA== 28478
IHdlYXBvbnJ5 28479
U0VU 28480
IDE4Ng== 28481
YWRtaW4= 28482
Z2Ft 28483
Y29udHJhY3Q= 28484
YWZh 28485
IGRlcml2YXRpdmVz 28486
IHNuYWNrcw== 28487
IGNodXJu 28488
RWNvbm9t 28489
IGNhcHBlZA== 28490
IFVuZGVyc3RhbmRpbmc= 28491
IEhlcnM= 28492
IEl6 28493
IGR1Y3Q= 28494
SUVOVA== 28495
YXVnaHR5 28496
IOKclA== 28497
IE5Q 28498
IHNhaWxpbmc= 28499
SW5pdGlhbGl6ZWQ= 28500
IHRlZA== 28501
IHJlYWN0b3Jz 28502
IExvbWI= 28503
IGNob2tl 28504
IFdvcm0= 28505
IGFkbWlyYXRpb24= 28506
IHN3dW5n 28507
ZW5zaWJseQ== 28508
IHJhc2g= 28509
IEdvYWxz 28510
IEltcG9ydGFudA== 28511
U2hvdA== 28512
IFJhcw== 28513
IHRyYWluZXJz 28514
IEJ1bg== 28515
V29ya2luZw== 28516
IGhhcm1lZA== 28517
IFBhbmRvcmE= 28518
IExURQ== 28519
IG11c2hyb29t 28520
IENIQVI= 28521
IEZlZQ== 28522
IE1veQ== 28523
Qm9ybg== 28524
b2xpYmVyYWw= 28525
IE1hcnRpYWw= 28526
IGdlbnRsZW1lbg== 28527
IGxpbmdlcmluZw== 28528
T2ZmaWNpYWw= 28529
IGdyYWZmaXRp 28530
IE5hbWVz 28531
RGVy 28532
IHF1aW50 28533
aXN0cmF0ZQ== 28534
YXplZXJh 28535
IE5PVElDRQ== 28536
IEZsb3JlbmNl 28537
IHBheWFibGU= 28538
IGRlcGljdHM= 28539
IFNwZWNpZXM= 28540
SGVhcnQ= 28541
4pSA4pSA4pSA4pSA4pSA4pSA4pSA4pSA 28542
IGVuY2xvc2Vk 28543
SW5jcmVhc2Vz 28544
RGFpbHk= 28545
IExpcw== 28546
IGVuYWN0bWVudA== 28547
IEJhY29u 28548
IFN0ZWVsZQ== 28549
ZGVtYW5k 28550
IDE4Mw== 28551
IG1vdXRocw== 28552
IHN0cmFuZGVk 28553
IGVuaGFuY2VtZW50 28554
MDEx 28555
IFdoYXRz 28556
IGhlYWxlZA== 28557
ZW55 28558
IFJhYg== 28559
IDM0MA== 28560
IExhYnlyaW50aA== 28561
cm9hY2g= 28562
IFlvc2g= 28563
IENsaXBwZXJz 28564
IGNvbmNlcnRz 28565
SW50ZXJuZXQ= 28566
MzU1 28567
IHN0aWNrZXJz 28568
IHRlcm1lZA== 28569
IEF4ZQ== 28570
IGdyYW5kcGFyZW50cw== 28571
RnJhbmNl 28572
IENsaW0= 28573
IFVo 28574
dWxpYw== 28575
IHRocmlsbA== 28576
Y2VudHJpYw== 28577
IE92ZXJ2aWV3 28578
IENvbmR1Y3Q= 28579
IHN1YnN0YW50aXZl 28580
IDE4Mg== 28581
bXVy 28582
IHN0cmF5 28583
IENvZmY= 28584
IHJlcGV0aXRpdmU= 28585
IEZvcmdvdHRlbg== 28586
IHF1YWxpZmljYXRpb24= 28587
ZXdpdG5lc3M= 28588
IFppbWJhYndl 28589
IHNpbXVsYXRlZA== 28590
IEpE 28591
MjUz 28592
IFdhcmU= 28593
IHVuc2M= 28594
VGltZXM= 28595
IHN1bW1vbnM= 28596
IGRpc2Nvbm5lY3RlZA== 28597
IDE4NA== 28598
Y2l1cw== 28599
IEd1amFy 28600
b2RrYQ== 28601
IGVyYXNl 28602
IFRvYmFjY28= 28603
ZWxlY3RlZA== 28604
IHVuY29udA== 28605
IFNoZXBhcmQ= 28606
IExhbXA= 28607
IGFsZXJ0ZWQ= 28608
IG9wZXJhdGl2ZQ== 28609
YXJuYQ== 28610
dWludA== 28611
IG5lZ2xpZ2VuY2U= 28612
YWNlbWVudHM= 28613
IHN1cHJh 28614
IHByZXZhaWw= 28615
IFNoYXJr 28616
IGJlbHRz 28617
44Gr 28618
IHRpZ2h0ZXI= 28619
RW5naW5lZXJz 28620
IGluYWN0aXZl 28621
IGV4cG9uZW50 28622
IFdpbGxpZQ== 28623
YXBsZXM= 28624
IGhlaXI= 28625
IEhpdHM= 28626
aWFubg== 28627
IFNheXM= 28628
IGN1cnJlbnRz 28629
IEJlbmdhbA== 28630
IGFyaXN0 28631
QnVmZmVy 28632
IGJyZWV6ZQ== 28633
IFdlc2xleQ== 28634
Q29sYQ== 28635
IHByb25vdW4= 28636
IGRlZWQ= 28637
IEtsaW5n 28638
IG9mdA== 28639
IGluZmxpY3Q= 28640
IHB1bmlzaGluZw== 28641
IG5t 28642
aWt1 28643
T0RVQ1Q= 28644
MDE0 28645
IHN1YnNpZHk= 28646
IERFQQ== 28647
IEhlcmJlcnQ= 28648
IEphbA== 28649
QmFuaw== 28650
IGRlZmVycmVk 28651
IHNoaXBtZW50 28652
Qm90dA== 28653
IGFsbGU= 28654
YmVhcmluZw== 28655
SFRNTA== 28656
T2ZmbGluZQ== 28657
IDIxMw== 28658
IHNjcm9sbGluZw== 28659
IHNjYW5uZWQ= 28660
IExpYnlhbg== 28661
IFRPUA== 28662
Y2hyb20= 28663
ZHQ= 28664
Y29sdW1u 28665
UHN5TmV0TWVzc2FnZQ== 28666
WmVybw== 28667
IHRvcnNv 28668
MDUw 28669
4pWQ 28670
IGltcGVyc29u 28671
IFNjaHdhcnR6 28672
dWRpYw== 28673
IHBpc3NlZA== 28674
IFNhcHA= 28675
MjU3 28676
IElTUHM= 28677
b2ds 28678
IHN1cGVydmlzZWQ= 28679
IGFkb2xlc2NlbnQ= 28680
IGF0dGFpbmVk 28681
IERlbGl2ZXJ5 28682
IEJ1bm55 28683
IDE5Mzc= 28684
IG1pbmlhdHVyZQ== 28685
IG9z 28686
IDM3MA== 28687
NjA4 28688
IE1vdXJpbmhv 28689
IGlubmF0ZQ== 28690
IHRlbXBv 28691
IE5N 28692
IEZhbGxlbg== 28693
MDA5 28694
IHByb3ZvY2F0aXZl 28695
U3RyZWFtZXI= 28696
IEJlbmVkaWN0 28697
IEJvbHNoZQ== 28698
IHR1cnRsZQ== 28699
IFBDQg== 28700
IEVxdWFs 28701
RGlyZWN0b3I= 28702
IFJlbmQ= 28703
IGZsdWlkcw== 28704
QXV0aG9yaXRpZXM= 28705
IGNvdXNpbnM= 28706
cmVxdWVuY3k= 28707
IE5laWdoYm9y 28708
c2V0cw== 28709
c2hhcmVk 28710
Q2hhcmxlcw== 28711
cGFzc3dvcmQ= 28712
IGdlYXJz 28713
IDIxMQ== 28714
IEhhcmR3YXJl 28715
cmlrYQ== 28716
IHVwc3RyZWFt 28717
SG9t 28718
IGRpc3Byb3BvcnRpb25hdGVseQ== 28719
aXZpdGllcw== 28720
IHVuZGVmaW5lZA== 28721
IGVsZWN0cm9ucw== 28722
IGNvbW1lbW9y 28723
RXZlbnR1YWxseQ== 28724
ID48 28725
IGlycmVzcG9uc2libGU= 28726
MjE4 28727
IFJlbGVhc2Vk 28728
IE9WRVI= 28729
IElHTg== 28730
IEJyZWFk 28731
c3RlbGxhcg== 28732
IFNhZ2U= 28733
dHRlZA== 28734
ZGFtYWdl 28735
ZWRpdGlvbg== 28736
IFByZWM= 28737
IGxpbWU= 28738
IGNvbmZpbmVtZW50 28739
IGNhbG9yaWU= 28740
d2VhcG9u 28741
IGRpZmZlcmluZw== 28742
IFNpbmE= 28743
bXlz 28744
YW1k 28745
IGludHJpY2F0ZQ== 28746
a2s= 28747
IFBBVA== 28748
w6Nv 28749
c3RvbmVz 28750
bGlua3M= 28751
IHJhbmNo 28752
U2VtaXRpYw== 28753
IGRpZmZlcmVudGlhdGU= 28754
IFNpbmdlcg== 28755
b2NjdXBpZWQ= 28756
IGZvcnRyZXNz 28757
Y21k 28758
IGludGVyY2VwdGlvbg== 28759
IEFua2FyYQ== 28760
IHJlcHQ= 28761
IFNvbGl0YWlyZQ== 28762
IHJlbWFrZQ== 28763
cHJlZA== 28764
IGRhcmVk 28765
YXV0aW9ucw== 28766
IEJBQ0s= 28767
UnVubmluZw== 28768
IGRlYnVnZ2luZw== 28769
IGdyYXBocw== 28770
Mzk5 28771
IE5pZ2Vs 28772
IGJ1bg== 28773
IHBpbGxvdw== 28774
IHByb2dyZXNzZWQ= 28775
ZmFzaGlvbmVk 28776
IG9iZWRpZW5jZQ== 28777
RVJO 28778
IHJlaGVhcnM= 28779
Q2VsbA== 28780
dGw= 28781
U2hlcg== 28782
IGhlcmFsZA== 28783
IFBheW1lbnQ= 28784
IENvcnk= 28785
IERlcHQ= 28786
IHJlcGVudA== 28787
IFdlYWs= 28788
dWNrbGFuZA== 28789
IHBsZWFzaW5n 28790
IHNob3J0YWdlcw== 28791
IGp1cm9ycw== 28792
IEthYg== 28793
cXFh 28794
QW50aQ== 28795
IHdvdw== 28796
IFJDTVA= 28797
IHRzdW4= 28798
IFNpYw== 28799
IGNvbXByaXNlcw== 28800
IHNwaWVz 28801
IHByZWNpbmN0 28802
bnU= 28803
IHVyZ2Vz 28804
IHRpbWVk 28805
IHN0cmlwZXM= 28806
IEJvb3Rz 28807
IHllbg== 28808
QWR2YW5jZWQ= 28809
IGRpc2NyZXRl 28810
IEFyY2hhbmdlbA== 28811
ZW1wbG95bWVudA== 28812
RGlmZg== 28813
IG1vbnVtZW50cw== 28814
IDIwOQ== 28815
d29ya2Vy 28816
IDE5Ng== 28817
IEln 28818
dXR0ZXJzdG9jaw== 28819
VFBT 28820
SmFj 28821
IGhvbWVsZXNzbmVzcw== 28822
IGNvbW1lbnRhdG9y 28823
IHJhY2lhbGx5 28824
ZmluZw== 28825
c2VlZA== 28826
RWxl 28827
ZWxsYXRpb24= 28828
IGV0aGFub2w= 28829
IHBhcmlzaA== 28830
IERvbmc= 28831
IEF3YWtlbmluZw== 28832
IGRldmlhdGlvbg== 28833
IEJlYXJpbmc= 28834
IFRzdWs= 28835
IHJlY2Vzcw== 28836
IGx5bXBo 28837
IENhbm5hYmlz 28838
5Zw= 28839
IE5FV1M= 28840
IGRyYQ== 28841
IFN0ZWZhbg== 28842
IFdyb25n 28843
IFNBTQ== 28844
IGxvb3NlbHk= 28845
IGludGVycHJldGVy 28846
IFBsYWlu 28847
R292ZXJubWVudA== 28848
IGJpZ290cnk= 28849
IGdyZW5hZGVz 28850
YXZleg== 28851
cGljdHVyZWQ= 28852
IG1hbmRhdGVk 28853
IE1vbms= 28854
IFBlZHJv 28855
IGxhdmE= 28856
Mjc0 28857
IGN5bmljYWw= 28858
IFNjcm9sbHM= 28859
bG9ja3M= 28860
TXA= 28861
IGNvbmdyZWdhdGlvbg== 28862
b3JuaW5ncw== 28863
cGhpbA== 28864
IEliaWQ= 28865
IGZlcnY= 28866
IGRpc2FwcGVhcmluZw== 28867
IGFycm9nYW50 28868
c3lu 28869
IE1hdmVy 28870
IFN1aXQ= 28871
MjQx 28872
IGFiYnJl 28873
YWNrZXJz 28874
UGE= 28875
IFllbA== 28876
V2hlbmV2ZXI= 28877
IDIzNQ== 28878
IFZpbmU= 28879
IEFuYXQ= 28880
IGV4dGluY3Q= 28881
TEVU 28882
IGV4ZWN1dGFibGU= 28883
VkVSUw== 28884
b3hpZGU= 28885
RE5B 28886
IFByZWw= 28887
IHJlc2VudG1lbnQ= 28888
IGNvbXByaXNl 28889
IEF2aXY= 28890
IGludGVyY2VwdGlvbnM= 28891
IHByb2xpZmlj 28892
SU5B 28893
IEVyaW4= 28894
dGhvdWdodA== 28895
MjE5 28896
IFBzeWNoaWF0cnk= 28897
dW5reQ== 28898
Y2hlbWlzdA== 28899
SG8= 28900
IE1jQ295 28901
IGJyaWNrcw== 28902
TG9z 28903
cmlseQ== 28904
IFVTU1I= 28905
IHJ1ZA== 28906
IGxhdWQ= 28907
IFdpc2U= 28908
IEVtZXJhbGQ= 28909
IHJldml2ZWQ= 28910
IGRhbW5lZA== 28911
IFJlcGFpcg== 28912
aWRlbQ== 28913
Y3RpY2E= 28914
IHBhdHJpYXJjaA== 28915
IE51cnM= 28916
bWVn 28917
IGNoZWFwZXN0 28918
cmVlbWVudHM= 28919
ZW1wdHk= 28920
IENlbGVicg== 28921
IGRlcHJpdmF0aW9u 28922
Y2hhbnRlZA== 28923
IFRodW1ibmFpbHM= 28924
RW5lcmd5 28925
IEV0aGFu 28926
IFFpbmc= 28927
IG9wcG9zZXM= 28928
V0lORA== 28929
dmlr 28930
IE1hdQ== 28931
IFNVQg== 28932
NjY3 28933
R1JF 28934
IFZvbHVudGU= 28935
bnRvbg== 28936
Q29vaw== 28937
5ZA= 28938
ZXNxdWU= 28939
IHBsdW1tZXQ= 28940
IHN1aW5n 28941
IHByb25vdW5jZQ== 28942
IHJlc2lzdGluZw== 28943
IEZpc2hpbmc= 28944
IFRyaWFscw== 28945
IHllbGw= 28946
IDMxMA== 28947
IGluZHVjdA== 28948
IHBlcnNvbmFsaXplZA== 28949
b2Z0ZW4= 28950
UmVi 28951
RU1CRVI= 28952
IHZpZXdwb2ludA== 28953
IGV4aXN0ZW50aWFs 28954
KCkp 28955
cmVtb3Zl 28956
TUVOVFM= 28957
bGFzc2Vz 28958
IGV2YXBvcg== 28959
IGFpc2xl 28960
bWV0YQ== 28961
IHJlZmxlY3RpdmU= 28962
IGVudGl0bGVtZW50 28963
IGRldmlzZWQ= 28964
bXVzaWM= 28965
YXNjYWRl 28966
IHdpbmRpbmc= 28967
b2Zmc2V0 28968
IGFjY2Vzc2liaWxpdHk= 28969
a2VyZWQ= 28970
QmV0dGVy 28971
IEpvaG5zdG9u 28972
dGhpbmtpbmc= 28973
U25vdw== 28974
IENyb2F0aWE= 28975
IEF0b21pYw== 28976
Mjcx 28977
MzQ4 28978
IHRleHRib29r 28979
IFNpeHRo 28980
INin2YQ= 28981
IHNsaWRlcg== 28982
IEJ1cmdlcg== 28983
Ym9s 28984
U3luYw== 28985
IGdyYW5kY2hpbGRyZW4= 28986
IGNlcnY= 28987
Kyk= 28988
IGV0ZXJuaXR5 28989
IHR3ZWV0aW5n 28990
IHNwZWN1bGF0aXZl 28991
IHBpdm90YWw= 28992
IFdQ 28993
IFRFUg== 28994
eW5hbWlj 28995
IHVwbA== 28996
IENhdHM= 28997
cGVyaGFwcw== 28998
IGNsYXNzbWF0ZXM= 28999
IGJsYXRhbnQ= 29000
Jy0= 29001
IGxha2g= 29002
YW50aW5l 29003
IEJvcmc= 29004
aW9t 29005
Lyg= 29006
IEF0aGxldGlj 29007
IHNhcg== 29008
T1RB 29009
IEhvZmZtYW4= 29010
TmV2ZXJ0aGVsZXNz 29011
IGFkb3JhYmxl 29012
IHNwYXduZWQ= 29013
QXNzb2NpYXRlZA== 29014
IERvbWVzdGlj 29015
IGltcGxhbnQ= 29016
IEx1eGVt 29017
IEtlbnM= 29018
IHB1bXBz 29019
IFNBVA== 29020
QXR0cmlidXRlcw== 29021
NTA5 29022
YXZvdXI= 29023
IGNlbnRyYWxpemVk 29024
IFRO 29025
IGZyZXNobHk= 29026
IEFjaGlldmU= 29027
IG91dHNpZGVycw== 29028
aGVydHk= 29029
IFJlZQ== 29030
IFRvd2Vycw== 29031
IERhcnQ= 29032
YWthYmxl 29033
IG1w 29034
IEhlYXZlbmx5 29035
IHJpcGU= 29036
IENhcm9saW5l 29037
cnlhbg== 29038
IGNsYXNzaWNz 29039
IHJldGlyaW5n 29040
IDIyOA== 29041
IGFo 29042
IGRlYWxpbmdz 29043
IHB1bmNoaW5n 29044
IENoYXBtYW4= 29045
T3B0aW9ucw== 29046
bWF4d2VsbA== 29047
dm9sdW1l 29048
IHN0YWw= 29049
IGV4cG9ydGVk 29050
IFF1aXRl 29051
IG51bWVyaWNhbA== 29052
QnVybg== 29053
RmFjdA== 29054
IEtleXN0b25l 29055
IHRyZW5kaW5n 29056
IGFsdGVyaW5n 29057
IEFmcmljYW5z 29058
NDc4 29059
IE1O 29060
IEtub2Nr 29061
IHRlbXB0YXRpb24= 29062
IHByZXN0aWdl 29063
T3ZlcnZpZXc= 29064
IFRyYWRpdGlvbmFs 29065
IEJhaHJhaW4= 29066
UHJpdmF0ZQ== 29067
IEhPVQ== 29068
IGJhcnI= 29069
IFRhdA== 29070
Q3ViZQ== 29071
VVNE 29072
IEdyYW5kZQ== 29073
IEdhdA== 29074
IEZsbw== 29075
IHJlc2lkZXM= 29076
IGluZGVj 29077
dm9sZW50 29078
IHBlcnBldHVhbA== 29079
dWJlcw== 29080
IHdvcmxkdmlldw== 29081
IFF1YW50dW0= 29082
IGZpbHRlcmVk 29083
IGVuc3U= 29084
b3JnZXRvd24= 29085
RVJTT04= 29086
IE1pbGQ= 29087
Mzc5 29088
T1RU 29089
w6U= 29090
IHZpdGFtaW5z 29091
IHJpYmJvbg== 29092
IHNpbmNlcmVseQ== 29093
IEhpbg== 29094
IGVpZ2h0ZWVu 29095
IGNvbnRyYWRpY3Rvcnk= 29096
IGdsYXJpbmc= 29097
IGV4cGVjdGFuY3k= 29098
IGNvbnNwaXI= 29099
IG1vbnN0cm91cw== 29100
IDM4MA== 29101
cmVjaQ== 29102
IGhhbmRpYw== 29103
IHB1bXBlZA== 29104
IGluZGljYXRpdmU= 29105
IHJhcHA= 29106
IGF2YWls 29107
IExFR08= 29108
IE1hcmlqdWFuYQ== 29109
MTk4NQ== 29110
ZXJ0b24= 29111
IHR3ZW50aWV0aA== 29112
IyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyM= 29113
IFN3YW1w 29114
IHZhbHVhdGlvbg== 29115
IGFmZmlsaWF0ZXM= 29116
YWRqdXN0ZWQ= 29117
IEZhY2lsaXR5 29118
MjYy 29119
IGVuenltZXM= 29120
aXR1ZGluYWw= 29121
IGltcHJpbnQ= 29122
U2l0ZQ== 29123
IGluc3RhbGxlcg== 29124
IFRSQQ== 29125
bW9sb2d5 29126
bGluZWFy 29127
IENvbGxlY3RpdmU= 29128
aWdhdGluZw== 29129
IFRva2Vu 29130
IHNwZWN1bGF0ZWQ= 29131
S04= 29132
IENseQ== 29133
b3JpdHk= 29134
IGRlZmVy 29135
IGluc3BlY3RvcnM= 29136
YXBwcm92ZWQ= 29137
Uk0= 29138
IFN1bnM= 29139
IGluZm9ybWluZw== 29140
IFN5cmFjdXNl 29141
aWJsaQ== 29142
NzY1 29143
IGdsb3Zl 29144
IGF1dGhvcml6ZQ== 29145
4oCm4oCm4oCm4oCm4oCm4oCm4oCm4oCm 29146
IENydWlzZQ== 29147
IGNvbnRyYWN0aW5n 29148
c2hlbGw= 29149
SUZF 29150
IEpld2Vs 29151
cHJhY3Q= 29152
IFBob3Rvc2hvcA== 29153
IEtub3dpbmc= 29154
aGFybQ== 29155
IGF0dHJhY3Rpb25z 29156
YWRhbg== 29157
ZXR1cw== 29158
MDE4 29159
d2FnZW4= 29160
QWx0 29161
IG11bHRpcGx5 29162
IGVxdWlsaWJyaXVt 29163
Ons= 29164
IEZpZ2h0ZXJz 29165
IEVkZ2Fy 29166
IGZvdXJ0ZWVu 29167
R292ZXJu 29168
IG1pc3VzZQ== 29169
IGFidXNpbmc= 29170
IGFuY2VzdHJ5 29171
cmFtZXI= 29172
NjQ0 29173
IHdvcm1z 29174
IHRoaWNrZXI= 29175
IENvbWJpbmU= 29176
IHBlYXNhbnRz 29177
IHZpbmQ= 29178
IGNvbnF1ZXN0 29179
IG1vY2tlZA== 29180
IGNpbm5hbW9u 29181
IENhbGQ= 29182
IEdhbGx1cA== 29183
IGF2b2lkYW5jZQ== 29184
IGluY2FybmF0aW9u 29185
IFN0cmF0 29186
IHRhc3RlZA== 29187
ZW50YQ== 29188
IE5lYWw= 29189
cGFyZWQ= 29190
IHRlcm1pbm9sb2d5 29191
amVjdGlvbg== 29192
U2NpZW50aXN0cw== 29193
IElOUw== 29194
IERlZQ== 29195
IGRpcmVjdG9yaWVz 29196
Um9hZA== 29197
IFNoYXA= 29198
YnJpZ2h0 29199
IERpcmVjdG9ycw== 29200
IENvbHVtbg== 29201
IGJvYg== 29202
IHByZWZlcmFibHk= 29203
IGdsaXRjaA== 29204
ZnVydA== 29205
IGVn 29206
aWRpcw== 29207
Q0JD 29208
IHN1cnJlbmRlcmVk 29209
IHRlc3RhbWVudA== 29210
MzM2 29211
dWdnZXN0 29212
IE5pbA== 29213
YW5vdGhlcg== 29214
IHBhdGhldGlj 29215
IERvbm5h 29216
IDIxOA== 29217
IEF2ZXJ5 29218
IHdoaXNrZXk= 29219
IGZpeHR1cmU= 29220
IENvbnF1ZXN0 29221
IGJldHM= 29222
T2Nj 29223
IExlaWNlc3Rlcg== 29224
XS4i 29225
ICkpOw== 29226
IGZsYXNoZXM= 29227
NDU2 29228
IG1hc2tlZA== 29229
Z2VicmE= 29230
IGNvbXB1dGVk 29231
Y2hlbA== 29232
YXVkZXI= 29233
IGRlZmVhdHM= 29234
IExpYmVyYXRpb24= 29235
IE9zYW1h 29236
IFZpdmU= 29237
Q2hhbmdlcw== 29238
Q2hhbm5lbA== 29239
IHRhcmlmZnM= 29240
IG1hZ2U= 29241
IFNheA== 29242
IGluYWR2ZXJ0ZW50bHk= 29243
IENSRQ== 29244
IFJlYXBlcg== 29245
aW5reQ== 29246
Z3JhZGluZw== 29247
IHN0ZXJlb3R5cA== 29248
IGN1cmw= 29249
IEZBTlQ= 29250
IGZyYW1ld29ya3M= 29251
TW9t 29252
IEFuY2g= 29253
IGZsYXZvdXI= 29254
Y2FyYm9u 29255
IHBlcm1pdHRpbmc= 29256
bGV0Y2hlcg== 29257
IE1vemlsbGE= 29258
IFBhcmtpbmc= 29259
IENoYW1w 29260
U2Nyb2xs 29261
IG11cmRlcmVy 29262
IHJlc3RlZA== 29263
IG93ZXM= 29264
IFBvc3M= 29265
QURE 29266
SUZG 29267
cmVzb2x1dGlvbg== 29268
IE1pbmluZw== 29269
IGNvbXBhcmF0aXZl 29270
RGlt 29271
IG5laWdoYm91cmluZw== 29272
IEFTVA== 29273
IFRveGlj 29274
IGJpYXNlcw== 29275
IGd1bmZpcmU= 29276
dXJvdXM= 29277
IE1vbWVudA== 29278
MTk4Mw== 29279
IHBlcnZhc2l2ZQ== 29280
dHRw 29281
IE5vcm1hbGx5 29282
cmly 29283
U2FyYWg= 29284
IEFsYmFueQ== 29285
IHVuc2V0dA== 29286
IFNNUw== 29287
aXBlcnM= 29288
bGF5ZXI= 29289
IFdoaXRlcw== 29290
dXBsZQ== 29291
IHR1cmJv 29292
IExlZWRz 29293
IHRoYXRz 29294
IE1pbmVy 29295
TUVS 29296
IFJlaWdu 29297
IHBlcm1l 29298
IEJsaXR6 29299
IDE5MzQ= 29300
IGludGltaWRhdGluZw== 29301
dHViZQ== 29302
IGVjY2VudHJpYw== 29303
YWJvbGlj 29304
Ym94ZXM= 29305
IEFzc29jaWF0ZXM= 29306
dm90ZXM= 29307
IHNpbXVsYXRl 29308
dW1ibw== 29309
YXN0ZXJ5 29310
IHNoaXBtZW50cw== 29311
RkZGRg== 29312
YW50aA== 29313
IHNlYXNvbmVk 29314
IGV4cGVyaW1lbnRhdGlvbg== 29315
4pag 29316
bGF3cw== 29317
TWVldA== 29318
aWRkbGVz 29319
YW50aWNz 29320
UmF0aW5n 29321
SVNJUw== 29322
aGlmdA== 29323
IGZyb250cw== 29324
YnVm 29325
MDE3 29326
IHVuYXR0 29327
IERpbA== 29328
bGVhc2Vz 29329
IEdhcmRlbnM= 29330
Nzc3 29331
dG91Y2g= 29332
dmVsbA== 29333
NDU4 29334
ID09PT09 29335
c2F2aW5n 29336
IGVyb3Npb24= 29337
IFF1aW4= 29338
IGVhcm5z 29339
IGFjY29tcGxpc2htZW50 29340
IFdlaQ== 29341
IDxb 29342
X19fX18= 29343
IGlycmln 29344
IFRlZGR5 29345
IGNvbnF1ZXJlZA== 29346
IEFybW9yZWQ= 29347
IGFzc2VydHM= 29348
IG1hbmlwdWxhdGluZw== 29349
csOp 29350
IHRyYW5zY3JpcHRz 29351
R2FsbGVyeQ== 29352
IHBsb3R0aW5n 29353
TmVpbA== 29354
IGJldHJheWFs 29355
bG9hZGVy 29356
IFN1bA== 29357
IGRpc3BsYWNlbWVudA== 29358
IHJveWFsdHk= 29359
IFdJ 29360
aGVpdA== 29361
IERldmljZXM= 29362
YWxsZWw= 29363
IG11bmljaXBhbGl0aWVz 29364
IGNhbmFs 29365
U3RhcnM= 29366
IFVBRQ== 29367
ICLigKY= 29368
IENV 29369
YWJvdmU= 29370
IHJlc29uYW5jZQ== 29371
IGd1aUFjdGl2ZVVu 29372
YWRkZWQ= 29373
IEJyYXZlcw== 29374
IElibg== 29375
IGhlcmVieQ== 29376
IEJSRQ== 29377
IHNoYXJlaG9sZGVy 29378
IEhpcg== 29379
IEpp 29380
IHN0cmFuZ2VseQ== 29381
IGFkbWlyZWQ= 29382
IHBsaWdodA== 29383
IGJhY2hlbG9y 29384
IFBvbGU= 29385
Y2lwbGluYXJ5 29386
VG9ueQ== 29387
IEFybWVuaWFu 29388
IHVubWFu 29389
IFppb25pc3Q= 29390
U3RhZ2U= 29391
aXNjb3Zlcg== 29392
IGF1dG9tb3RpdmU= 29393
IHNpZGVsaW5lcw== 29394
IHNsaWNr 29395
IFJlbmFpc3NhbmNl 29396
IEZVTg== 29397
SW1hZ2Vz 29398
IEhhag== 29399
IHBpbmc= 29400
IHNob3J0Y3V0 29401
IEJsdmQ= 29402
IExvb2tz 29403
IGJ1cnN0cw== 29404
IGNsYW1w 29405
IG1pc2g= 29406
IHNvcnRpbmc= 29407
IHBhdHJpb3Q= 29408
IGNvcnJlY3RuZXNz 29409
IFNjYW5kaW5hdg== 29410
IENhdmFsaWVycw== 29411
cHl0aG9u 29412
YXphcg== 29413
IDM3NQ== 29414
IEphdW5l 29415
NDA5 29416
IGRldHJpbWVudGFs 29417
IHN0YWJiaW5n 29418
IHBvaXNvbmVk 29419
IGZvdW50YWlu 29420
b2NlbnQ= 29421
b3JzdA== 29422
IE1hcmk= 29423
IHJhaW5z 29424
IE92ZXJz 29425
IEluc3RpdHV0aW9u 29426
dWRnZXQ= 29427
QU1Z 29428
dGFsZQ== 29429
IEtS 29430
IFByaWNlcw== 29431
IGhlYWRhY2hlcw== 29432
IGxhbmRzbA== 29433
IEF1cmE= 29434
Qm9udXM= 29435
IFpoYW8= 29436
IEhpcA== 29437
IGhvcHM= 29438
IEt1cmRpc3Rhbg== 29439
IGV4cGxvaXRpbmc= 29440
cnlu 29441
IGh5cG9jcmlzeQ== 29442
b3BlbmluZw== 29443
IGd1bnNob3Q= 29444
IHdlZA== 29445
aW50ZXJzdGl0aWFs 29446
SW50ZXJzdGl0aWFs 29447
IGFtZW4= 29448
QnJlYWtpbmc= 29449
IG1hcmtldGVk 29450
V2lyZQ== 29451
IENyb3dk 29452
Q29udGludWU= 29453
IEtub3du 29454
IEVmZmVjdGl2ZQ== 29455
b3JlYW4= 29456
aXpvbnM= 29457
Sm9zZXBo 29458
IGVzY2FsYXRpb24= 29459
dXNlcm5hbWU= 29460
IGN1cnRhaW4= 29461
QVRFUw== 29462
IFBBUg== 29463
IE1peQ== 29464
IGNvdW50ZXJmZQ== 29465
bGVuZQ== 29466
IGNvbnRlbmRlcnM= 29467
ZGFpbHk= 29468
IEFzYw== 29469
IFBoaWxsaXA= 29470
bW9zdGx5 29471
IGZpbGVuYW1l 29472
aGVuZQ== 29473
IHJlc2VtYmxpbmc= 29474
IHN0YWdpbmc= 29475
IENobG9l 29476
IHdpcmluZw== 29477
SG9u 29478
IFJlbmV3 29479
b3R0YWdl 29480
IEh5YnJpZA== 29481
bXVjaA== 29482
IHN0cm9rZXM= 29483
IHBvbGljeW1ha2Vycw== 29484
QVBURVI= 29485
IEFya2hhbQ== 29486
cGxvdA== 29487
IGFzc2lzdGFudHM= 29488
IGRlcG9ydA== 29489
IFNlZ2E= 29490
IGluZmx1ZW56YQ== 29491
IEN1cnNlZA== 29492
IEtvYmU= 29493
IHNraW5ueQ== 29494
UHJvdmlkZXI= 29495
IFJpcA== 29496
IGluY3JlbWVudGFs 29497
cHJvZHVjdHM= 29498
QkY= 29499
IGRvbWU= 29500
IENyZWRpdHM= 29501
IGxvc2Vycw== 29502
aW50cw== 29503
IEJldHR5 29504
IFRhbGVudA== 29505
IERBTQ== 29506
THY= 29507
RXNz 29508
IGRlbnM= 29509
dGVtcA== 29510
SnVkZ2U= 29511
b2RpYw== 29512
ICco 29513
VVJFUw== 29514
ZXRzaw== 29515
Vk8= 29516
IHJldHJpZXZlZA== 29517
IGFyY2hpdGVjdHM= 29518
2Yc= 29519
IGV0aGlj 29520
IFNlY29uZGFyeQ== 29521
c3RvY2tz 29522
YWRpYQ== 29523
IDMyNQ== 29524
IE9waW5pb24= 29525
IHNpbXVsdGFuZW91cw== 29526
IGRpeno= 29527
dWxw 29528
IHNtdWdnbGluZw== 29529
aXBwZXJ5 29530
UmFuZG9t 29531
ZmFjaW5n 29532
IERhcw== 29533
IHN0b2NrcA== 29534
IGRpc2Nsb3N1cmVz 29535
cG9pbnRlcg== 29536
IGNvcmFs 29537
IFNlbGVjdGlvbg== 29538
IFBpa2U= 29539
aXZhbGVudA== 29540
IHJ1dGhsZXNz 29541
IFJpbQ== 29542
IGVuc3Vpbmc= 29543
IEV4cGVyaW1lbnQ= 29544
IGNvbmdyZXNzbWFu 29545
IGJlbGlldmVy 29546
IHVuc3BlY2lmaWVk 29547
IE1vcmQ= 29548
IGtub3dsZWRnZWFibGU= 29549
IFZFUlk= 29550
VFg= 29551
IHN0cmFwcw== 29552
IHR1cmY= 29553
YXBlc2hpZnRlcg== 29554
IG1hcml0YWw= 29555
IGZsb2Nr 29556
44GG 29557
MjYz 29558
QU1FUw== 29559
IE9wcG9zaXRpb24= 29560
IHRyZWFzdXJlcw== 29561
IEdPRA== 29562
IG1vZGVsZWQ= 29563
IFdPUkxE 29564
IChb 29565
IFVzYWdl 29566
SEY= 29567
ICQo 29568
dXNzZWQ= 29569
IHBpb25lZXI= 29570
RWlnaHQ= 29571
cGFyc2U= 29572
YnJlYWQ= 29573
cml0eg== 29574
IE1pcmFuZGE= 29575
IEthbnQ= 29576
Kysp 29577
b3Jlbg== 29578
IHByb3Zva2Vk 29579
IGJyZWVkcw== 29580
IEluY2x1ZGVz 29581
IFBhc3RlYmlu 29582
IEZsaXA= 29583
SmF2YQ== 29584
IGJyaW5r 29585
IHJ1bW9yZWQ= 29586
IHVuc2Vlbg== 29587
IGdhcm5lcmVk 29588
IERlZmlu 29589
YWx0ZWQ= 29590
IHRhdHRvb3M= 29591
IGhlc2l0YXRpb24= 29592
aXNpdGlvbnM= 29593
IFdlYXZlcg== 29594
IFJlcG9ydGluZw== 29595
IHRoZXJhcGllcw== 29596
IGNvbnN1bHRhbnRz 29597
IHJlc2lkdWFs 29598
IE1hbGk= 29599
IFJvbWE= 29600
aWFnbw== 29601
IFJlc2lkZW50cw== 29602
dWJp 29603
IHJlbWVkaWVz 29604
IGFkYXB0aXZl 29605
IEFsaXZl 29606
IEJhcmNs 29607
IHdhbGxldHM= 29608
Y3J5cHQ= 29609
ZXRlcm1pbmF0aW9u 29610
IFBlbG9zaQ== 29611
IHNsaXBwaW5n 29612
b3Rvbmlu 29613
IGFsbGlhbmNlcw== 29614
cGF0cmljaw== 29615
aXJpcw== 29616
IG9ydGg= 29617
IFBlcmtpbnM= 29618
IERlVg== 29619
IEdldHM= 29620
IGRyeWluZw== 29621
Z2Vl 29622
Zm9yZXN0 29623
IEZvcmdldA== 29624
b3JlbQ== 29625
MzM5 29626
IHZhZ3VlbHk= 29627
IERpb24= 29628
IFBvcm4= 29629
IEhPVw== 29630
IHBuZXVt 29631
IHJ1YmJsZQ== 29632
IFRhc3Rl 29633
ZW5jaWE= 29634
IEdlbA== 29635
IGRzdA== 29636
IDI0NQ== 29637
IE1vcm9jY28= 29638
aW5mbGFtbQ== 29639
IFR3aW5z 29640
IGJvdHM= 29641
ZGF1Z2h0ZXI= 29642
IEJhbGs= 29643
IGJyZXRocmVu 29644
IGxvZ29z 29645
IGdvYmw= 29646
ZnBz 29647
IHN1YmRpdmlzaW9u 29648
IHBhd24= 29649
IHNxdWVlemVk 29650
IG1vcmFsZQ== 29651
IERX 29652
JyI= 29653
IGtub3Q= 29654
b29reQ== 29655
IGRpdmlzaXZl 29656
IGJvb3N0ZWQ= 29657
Y2h5 29658
44OQ 29659
aWZhY3Q= 29660
IG5ld2NvbWVycw== 29661
IFdyZXN0bGluZw== 29662
IHNjb3V0cw== 29663
d29sdmVz 29664
UmF0 29665
IG5pbmV0ZWVudGg= 29666
IE9zYm9ybmU= 29667
U3RhdHM= 29668
IGVtcG93ZXJlZA== 29669
IHBzeWNob3BhdGg= 29670
IE9FTQ== 29671
dWdnYWdl 29672
IFBL 29673
IE1vaGFtbWFk 29674
UGFr 29675
IGFuYXJjaGlzdHM= 29676
IEV4dHJhY3Q= 29677
ZXN0aGVz 29678
IFN0b2NraG9sbQ== 29679
bG9v 29680
IEdyYXBo 29681
IGRlcGxveWluZw== 29682
IFN0cmFuZ2Vy 29683
IE1vbGQ= 29684
IHN0YWZmZXI= 29685
IGRpc2NvdW50ZWQ= 29686
dWNrbGU= 29687
cGxlYXNl 29688
IExhbmRpbmc= 29689
w61h 29690
IDE5Mw== 29691
IGFudGU= 29692
IHJlcGV0aXRpb24= 29693
ICsvLQ== 29694
IHBhcm9keQ== 29695
IGxpdmVseQ== 29696
QUFB 29697
IEhvcnVz 29698
IHBpdHM= 29699
aW5kZXJz 29700
TE9D 29701
IFZlbmljZQ== 29702
NDA2 29703
IERpc2NvdmVy 29704
4oY= 29705
ZWxsZWN0dWFs 29706
IHBlbnM= 29707
IGV5ZWw= 29708
aWd1b3Vz 29709
SW1wbA== 29710
IGpva2luZw== 29711
IGludmFs 29712
IEJlbGZhc3Q= 29713
IGNyZWRpdG9ycw== 29714
IFNreXdhbGtlcg== 29715
b3Zza3k= 29716
IGNlYXNlZmlyZQ== 29717
IHNlYWxz 29718
aXNvZnQ= 29719
KSku 29720
IEZlbGl4 29721
SVRT 29722
IHRyZXNw 29723
IEJsb2NrY2hhaW4= 29724
ZXdhcmU= 29725
IFNjaHdhcg== 29726
ZW5uZQ== 29727
bW91bnRlZA== 29728
IEJlYWNvbg== 29729
bGVzaA== 29730
IGltbWVuc2VseQ== 29731
IGNoZWVyaW5n 29732
RW1wbG95 29733
c2NlbmU= 29734
aXNobHk= 29735
YXRjaGV3YW4= 29736
IE5pY29sYXM= 29737
IGRyYWluZWQ= 29738
IEV4aXQ= 29739
IEF6ZXJi 29740
anVu 29741
IGZsb2F0ZWQ= 29742
dWFuaWE= 29743
RGVlcA== 29744
IHN1cGVydg== 29745
IG15c3RpY2Fs 29746
IERvbGxhcg== 29747
IEFwb3N0bGU= 29748
IFJFTA== 29749
IFByb3ZpZGVk 29750
IEJ1Y2tz 29751
44O0 29752
Y3V0dGluZw== 29753
IGVuaGFuY2VtZW50cw== 29754
IFBlbmd1aW5z 29755
IElzYWlhaA== 29756
IGplcms= 29757
IFd5bg== 29758
IHN0YWxsZWQ= 29759
IGNyeXB0b2N1cnJlbmNpZXM= 29760
IFJvbGFuZA== 29761
c2luZ2xl 29762
IGx1bWlu 29763
IEZlbGxvdw== 29764
IENhcGFjaXR5 29765
IEthemFraA== 29766
V04= 29767
IGZpbmFuY2Vk 29768
Mzg5 29769
IHRpZA== 29770
IGNvbGx1c2lvbg== 29771
IE15cg== 29772
7oA= 29773
U2VuYXRvcg== 29774
IHBlZGlhdHJpYw== 29775
IG5lYXRseQ== 29776
IHNhbmR3aWNoZXM= 29777
IEFyY2hpdGVjdHVyZQ== 29778
IHR1Y2tlZA== 29779
IGJhbGNvbnk= 29780
IGVhcnRocXVha2Vz 29781
cXVpcmU= 29782
RnV0dXJl 29783
IGhlZnR5 29784
6Zc= 29785
IHNwZWNpYWxpemVz 29786
IHN0cmVzc2Vz 29787
IHNlbmRlcg== 29788
IG1pc3VuZGVyc3RhbmRpbmc= 29789
IGVwaWxl 29790
IHByb3Zva2U= 29791
IENvbG9ycw== 29792
IGRpc21heQ== 29793
dWtv 29794
W18= 29795
NTg2 29796
bmV1dHJhbA== 29797
IGRvbmF0aW5n 29798
IFJhbmRhbGw= 29799
TXVsdGk= 29800
IGNvbnZlbmllbnRseQ== 29801
IFN1bmc= 29802
IENvY2E= 29803
IHRlbnRz 29804
IEFjY2VsZXI= 29805
IHBhcnRuZXJlZA== 29806
Mjcy 29807
aXJtaW5n 29808
IEJBUw== 29809
c29tZXRpbWVz 29810
IG9iamVjdGVk 29811
dWJyaWM= 29812
cG9zZWQ= 29813
TENT 29814
Z3Jhc3M= 29815
IGF0dHJpYnV0YWJsZQ== 29816
VklT 29817
SXNyYWVsaQ== 29818
IHJlcGVhdHM= 29819
IFJN 29820
dmFn 29821
dXRh 29822
aW5vdXM= 29823
IGluZXJ0 29824
IE1pZ3VlbA== 29825
5q0= 29826
IEhhd2FpaWFu 29827
Qm9hcmQ= 29828
IGFydGlmaWM= 29829
IEF6ZXJiYWk= 29830
YXNpbw== 29831
IFJlbnQ= 29832
QUlO 29833
IGFwcGxpYW5jZXM= 29834
IG5hdGlvbmFsaXR5 29835
IGFzc2hvbGU= 29836
IE5lYg== 29837
IG5vdGNo 29838
aGFuaQ== 29839
IEJyaWRl 29840
QXZhaWxhYmlsaXR5 29841
IGludGVyY2VwdGVk 29842
IGNvbnRpbmVudGFs 29843
IHN3ZWxsaW5n 29844
IFBlcnNwZWN0 29845
Ymllcw== 29846
Ljw= 29847
aXRobWV0aWM= 29848
IExhcmE= 29849
IHRlbXB0aW5n 29850
YWRkcg== 29851
IG92ZXJzZWVpbmc= 29852
Y2xhZA== 29853
IERW 29854
IEdpbmdyaWNo 29855
IG11bg== 29856
IEFwcHJvcHJp 29857
IGFsdGVyYXRpb25z 29858
IFBhdHJlb24= 29859
IGhhdm9j 29860
IGRpc2NpcGxpbmVz 29861
IG5vdG9yaW91c2x5 29862
YWt1eWE= 29863
aWVyaQ== 29864
Pyku 29865
IFdlbnQ= 29866
IHNpbGljb24= 29867
IHRyZW1i 29868
Q29udGFpbmVy 29869
S25vd24= 29870
IG1vcnRhcg== 29871
ZXN0ZQ== 29872
aWNrYQ== 29873
QXJ0aHVy 29874
IFByZXZpb3VzbHk= 29875
IE1hcnR5 29876
IHNwYXJzZQ== 29877
Z2lucw== 29878
IGlud2FyZA== 29879
IFBhcnRpY2lwYW50 29880
Q29weQ== 29881
IE1pc2M= 29882
IGFudGliaW90aWM= 29883
IFJldHJv 29884
IGVsdXNpdmU= 29885
IGFzc2FpbA== 29886
IEJhdHRhbGlvbg== 29887
IEJvdWdodA== 29888
IGRpbWluaXNo 29889
IEV1cm9wYQ== 29890
c2Vzc2lvbg== 29891
IERhbmdlcm91cw== 29892
aWVzZWw= 29893
IGRpc2JlbGllZg== 29894
IGJsYXN0cw== 29895
ZXh0cmVtZQ== 29896
IEJveWQ= 29897
IFByb2plY3Rz 29898
IEd1eXM= 29899
IHVuZGVyZ29uZQ== 29900
IGdyaWxs 29901
IER3aWdodA== 29902
IDE5Nw== 29903
VVNFUg== 29904
IGZpbGVzeXN0ZW0= 29905
IGNsb2Nrcw== 29906
VGF5bG9y 29907
IHdyYXBwZXI= 29908
IGZvbGRpbmc= 29909
b3VzYW5k 29910
IFBoaWxpcHBpbmU= 29911
QVRJT05BTA== 29912
IFBlcnRo 29913
IGFzaGVz 29914
IGFjY3VtdWxhdGU= 29915
IEdhdGV3YXk= 29916
U2hvcA== 29917
b3Jrc2hpcmU= 29918
SGFu 29919
IEJhcnJlbA== 29920
IExlaA== 29921
IFhW 29922
IHdoaW0= 29923
IHJlcG8= 29924
IENH 29925
IE1hbQ== 29926
IGluY29ycG9yYXRpbmc= 29927
IGJhaWxvdXQ= 29928
IGxpbmd1aXN0aWM= 29929
IGRpc2ludGVn 29930
Q0xF 29931
IGNpbmVtYXRpYw== 29932
IEZpYmVy 29933
U3lu 29934
aWxpb24= 29935
IENvbXBvcw== 29936
Y2hlbnM= 29937
IG5lb2M= 29938
IGJvaWxlZA== 29939
RklORQ== 29940
b25v 29941
dW5jbGU= 29942
aWtlbg== 29943
IEJN 29944
zrk= 29945
IHJlY2VpcHRz 29946
IGRpc3Bvc2Vk 29947
IFRoaXJ0eQ== 29948
IFJvdWdo 29949
IEFCUw== 29950
IG5vdHdpdGhzdGFuZGluZw== 29951
b2xsZW4= 29952
IyQ= 29953
IHVucmVsaWFibGU= 29954
IGJsb29t 29955
IG1lZGlvY3Jl 29956
IHRyYW0= 29957
IFRhc21hbg== 29958
IHNoYWtlcw== 29959
IG1hbmlmZXN0bw== 29960
IE1X 29961
IHNhdGlzZmFjdG9yeQ== 29962
IHNob3Jlcw== 29963
IGNvbXB1dGF0aW9u 29964
IGFzc2VydGlvbnM= 29965
b3Jtb25z 29966
YXJhZw== 29967
YWJpdA== 29968
RGVtb2NyYXRz 29969
IExvb3Q= 29970
IFZvbGtz 29971
aGFpcmVk 29972
IGdyYXZpdGF0aW9uYWw= 29973
U2luZw== 29974
IE1peg== 29975
IHRocm90dGxl 29976
IHR5cmFubnk= 29977
IFZpZXdz 29978
IHJvYmJlcg== 29979
IE1pbm9yaXR5 29980
IHNocmluZQ== 29981
c2NvcGU= 29982
cHVycG9zZQ== 29983
IG51Y2xldXM= 29984
b3VyY2luZw== 29985
IFVTREE= 29986
IERIUw== 29987
d3Jh 29988
IEJvd2ll 29989
U2NhbGU= 29990
IEJFTA== 29991
eGk= 29992
SXRlcg== 29993
ICgpLA== 29994
d3JpZ2h0 29995
IHNhaWxvcnM= 29996
b3VzZWQ= 29997
TkFTQQ== 29998
IFByb29m 29999
IE1pbmVyYWw= 30000
dG9rZW4= 30001
IEZE 30002
UmV3 30003
IGVsbA== 30004
NjMw 30005
IGNoYW5jZWxsb3I= 30006
IEdvcw== 30007
IGFtb3VudGVk 30008
IFJlY3Jl 30009
b21leg== 30010
IE9wdGlt 30011
IE9saXZl 30012
IHRyYWNrZXI= 30013
b3dsZXI= 30014
IFVuaXF1ZQ== 30015
Um9vdA== 30016
IG1hcml0aW1l 30017
IFF1cmFu 30018
IEFkYXB0 30019
IGVjb3N5c3RlbXM= 30020
IFJlcGVhdA== 30021
IFNveQ== 30022
IElNUA== 30023
IGdyYWR1YXRpbmc= 30024
YW5kZW0= 30025
UHVy 30026
IFJlc2V0 30027
IFRyaWNr 30028
IFBoaWxseQ== 30029
IFR1ZQ== 30030
IE1hbGF5c2lhbg== 30031
IGNsaW1heA== 30032
IGJ1cnk= 30033
IGNvbnNwaWM= 30034
IFNvdXRoYW1wdG9u 30035
IEZsb3dlcnM= 30036
IGVzY29ydGVk 30037
IEVkdWNhdGlvbmFs 30038
IElSQw== 30039
IGJydXRhbGx5 30040
ZWF0aW5n 30041
IHBpbGxhcg== 30042
IFNhbmc= 30043
IEp1ZGU= 30044
YXJsaW5n 30045
IEFtbmVzdHk= 30046
IHJlbWluZGluZw== 30047
IEFkbWluaXN0cmF0aXZl 30048
aGVzZGE= 30049
IGZsYXNoZWQ= 30050
IFBCUw== 30051
cGVyYXRl 30052
ZmVhdHVyZQ== 30053
IHN3aXBl 30054
IGdyYXZlcw== 30055
b3VsdHJ5 30056
MjYx 30057
YnJlYWtz 30058
IEd1ZXI= 30059
IHNocmltcA== 30060
IFZvdGluZw== 30061
cXVpc3Q= 30062
IGFuYWx5dGljYWw= 30063
IHRhYmxlc3Bvb25z 30064
IFNPVQ== 30065
IHJlc2VhcmNoZWQ= 30066
IGRpc3J1cHRlZA== 30067
IGpvdXI= 30068
IHJlcGxpY2E= 30069
IGNhcnRvb25z 30070
YmlhbnM= 30071
fSk= 30072
Y29weQ== 30073
R290 30074
b3VjaGVk 30075
UFVU 30076
IHN3YXJt 30077
bm90YXRpb25z 30078
c2FpZA== 30079
IHJlYnVpbHQ= 30080
IGNvbGxhYm9yYXRl 30081
IHJhZ2luZw== 30082
IG5hcg== 30083
IGRlbW9ncmFwaGljcw== 30084
IEREUg== 30085
IGRpc3RydXN0 30086
b3NzaWVy 30087
IEtybw== 30088
IHB1bXBraW4= 30089
IHJlZ3JldHM= 30090
IGZhdGFsaXRpZXM= 30091
IExlbnM= 30092
IE9sZQ== 30093
cGQ= 30094
IHB1cHBldA== 30095
IE91dGxvb2s= 30096
IFN0YW0= 30097
T2w= 30098
RmFpcg== 30099
VVU= 30100
IHJld3JpdHRlbg== 30101
xLE= 30102
IGZhc2NpbmF0ZWQ= 30103
IHZlY3RvcnM= 30104
IHRyaWJ1bmFs 30105
dWF5 30106
IE1hdHM= 30107
IENvaW5z 30108
W1s= 30109
IDE4MQ== 30110
IHJlbmRlcnM= 30111
IEthZXBlcm5pY2s= 30112
IGVzcGlvbmFnZQ== 30113
IHN1bW0= 30114
IGRpdGNo 30115
QWNjb3VudA== 30116
IHNwcmVhZHNoZWV0 30117
IG11dGFudA== 30118
cGFzdA== 30119
NDA3 30120
IGR5ZQ== 30121
IGluaXRpYXRpb24= 30122
IDQwMDA= 30123
IHB1bmlzaGFibGU= 30124
IHRoaW5uZXI= 30125
IEtoYWw= 30126
IGludGVybWVkaQ== 30127
RHVu 30128
IEdvdGhhbQ== 30129
IGVhZ2VybHk= 30130
IHZhZ2luYWw= 30131
cG93ZXJz 30132
Vlc= 30133
IFdBVENIRUQ= 30134
IHByZWRhdG9y 30135
YW1zdW5n 30136
IGRpc3Bhcml0eQ== 30137
IFsq 30138
IGFtcGg= 30139
IG91dHNraXJ0cw== 30140
IFNwaXJpdHM= 30141
IHNrZWxldGFs 30142
0Ls= 30143
IFJlYXI= 30144
IGlzc3VhbmNl 30145
IExvZ2lj 30146
cmVsZWFzZWQ= 30147
Wlo= 30148
IEJvdW5k 30149
RW50cnk= 30150
IGV4aXRz 30151
aXNvbA== 30152
IEZvdW5kZXI= 30153
IHdyZQ== 30154
IEdyZWVubGFuZA== 30155
IE1NTw== 30156
dGFrZXI= 30157
SU5D 30158
44G+ 30159
IGhvdXJseQ== 30160
aGVua28= 30161
IGZhbnRhc2llcw== 30162
IGRpc29i 30163
IGRlbW9saXRpb24= 30164
44OL 30165
IGVubGlzdGVk 30166
cmF0dWxhdGlvbnM= 30167
IG1pc2d1aWRlZA== 30168
IGVuc3VyZWQ= 30169
IGRpc2NvdXJhZ2Vk 30170
bW9ydA== 30171
IGZsYW5r 30172
IGNlc3M= 30173
IHJlYWN0cw== 30174
IFNlcmU= 30175
c2Vuc2l0aXZl 30176
IFNlcnBlbnQ= 30177
YXNzYWQ= 30178
IDI0Nw== 30179
IGNhbG1seQ== 30180
YnVzdGVycw== 30181
IGJsZWVk 30182
IFN0cm8= 30183
IGFtdXNlbWVudA== 30184
IEFudGFyY3RpY2E= 30185
IHNjZXB0 30186
IEdhdw== 30187
YXE= 30188
YXNvbmlj 30189
IHNwcmF3bGluZw== 30190
bmF0aXZl 30191
YXR1cmF0ZWQ= 30192
IEJhdHRsZWZpZWxk 30193
SVZFUlM= 30194
RUI= 30195
IEdlbXM= 30196
IE5vcnRod2VzdGVybg== 30197
IEZpbG1z 30198
IEF1dG9tYXRpYw== 30199
IGFwcHJlaGVuZA== 30200
44Go 30201
IGd1aU5hbWU= 30202
IGJhY2tlbmQ= 30203
IGV2aWRlbmNlZA== 30204
Z2VhbnQ= 30205
MDEy 30206
IFNpZWdl 30207
IGV4dGVybmFsVG8= 30208
IHVuZm9jdXNlZFJhbmdl 30209
IGd1aUFjdGl2ZVVuZm9jdXNlZA== 30210
IGd1aUljb24= 30211
IGV4dGVybmFsVG9FVkE= 30212
IGV4dGVybmFsVG9FVkFPbmx5 30213
RnJp 30214
Y2hhcmQ= 30215
ZW5hcmllcw== 30216
IGNoaWVmcw== 30217
IGNm 30218
IEhVRA== 30219
IGNvcnJvYm9y 30220
IGRC 30221
IFRha2Vu 30222
IFBhdHJpY2lh 30223
cmFpbA== 30224
IENoYXJt 30225
IExpYmVydGFyaWFu 30226
cmlldmU= 30227
UGVyc29uYWw= 30228
IE9VUg== 30229
Z2VyaWVz 30230
IGR1bXBpbmc= 30231
IG5ldXJvbG9naWNhbA== 30232
aXRpbWF0ZQ== 30233
IENsaW50b25z 30234
cmFmdGVk 30235
IE1vbGx5 30236
IHRlcm1pbmFscw== 30237
cmVnaXN0ZXI= 30238
IGZsYXJl 30239
IGVuY29kZWQ= 30240
IGF1dG9wc3k= 30241
cGVs 30242
bWFjaGluZQ== 30243
IGV4ZW1wdGlvbnM= 30244
IFJveWFscw== 30245
ZGlzdGFuY2U= 30246
IGRyYWZ0cw== 30247
IGxhbWU= 30248
IEN1bm5pbmc= 30249
IHNwb3VzZXM= 30250
IE1hcmtldHM= 30251
IENhcnJpZXI= 30252
IGltcGx5aW5n 30253
IFlhaw== 30254
c2lk 30255
IGxvc2Vy 30256
IHZpZ2lsYW50 30257
IGltcGVhY2htZW50 30258
IGF1Z21lbnRlZA== 30259
IEVtcGxveWVlcw== 30260
IHVuaW50ZW5kZWQ= 30261
dGVybmFsbHk= 30262
IFdhdHQ= 30263
IHJlY29nbml6YWJsZQ== 30264
ZXNzaW0= 30265
5p0= 30266
IGNvYXRlZA== 30267
cmhh 30268
IGxpZXV0ZW5hbnQ= 30269
IExlZ2lzbGF0aW9u 30270
cHVibGlzaGVk 30271
NDQ0 30272
MDEz 30273
IGlkZWFsbHk= 30274
IFBhc3N3b3Jk 30275
IHNpbXBsaWZ5 30276
IE1ldGE= 30277
IE1SSQ== 30278
IHBsZWFkaW5n 30279
b3JnYW5pemVk 30280
aGFuZGxlcg== 30281
IHVucmF2ZWw= 30282
Y29ycmVjdA== 30283
IGljeQ== 30284
IHBhcmFub2lk 30285
IHBhc3Nlcg== 30286
IGluc3BlY3Rpb25z 30287
b2Zlcg== 30288
IEhlYWx0aGNhcmU= 30289
Mjgz 30290
IEJydXQ= 30291
aW9sYQ== 30292
Zm9yZ2U= 30293
IE1lZGlldmFs 30294
TVNO 30295
aWV2ZXJz 30296
IFByb2dyYW1taW5n 30297
5Yk= 30298
IDIyMw== 30299
bXU= 30300
IENMRQ== 30301
dWdh 30302
IHNob3BwZXJz 30303
IGluZm9ybWF0aXZl 30304
IFBsYW5z 30305
IHN1cHBsZW1lbnRhdGlvbg== 30306
IFRlc3Rz 30307
dHlhcmQ= 30308
b2N5dGVz 30309
IFZlZ2E= 30310
IEd1amFyYXQ= 30311
ZXJtYW5lbnQ= 30312
RXhjZXB0 30313
IExPVA== 30314
YWxsYQ== 30315
IEN1bW0= 30316
IE9zdw== 30317
IHZlbm9t 30318
IERlYnQ= 30319
IERPV04= 30320
IHJldW5pb24= 30321
IG11Yw== 30322
IFJlbGllZg== 30323
IGdlb3A= 30324
IPCfmA== 30325
YWxvZ3Vl 30326
QW50aA== 30327
ZWNobw== 30328
IGNvcnJvcw== 30329
IHJlcGxpY2F0aW9u 30330
IEJsYXppbmc= 30331
IERhdWdodGVy 30332
IGluZmxpYw== 30333
IExpbmRzZXk= 30334
2Yg= 30335
Mjg0 30336
RXhpdA== 30337
IGdsb29t 30338
VEFJTg== 30339
IHVuZGVybWluaW5n 30340
IGFkdmlzaW5n 30341
aGlkZGVu 30342
IG92ZXJmbG93 30343
IGdvcg== 30344
dXJkdWU= 30345
IGVjaG9lcw== 30346
ZW5oYWdlbg== 30347
IGltcHVscw== 30348
ZHJ1Zw== 30349
Y2FzaA== 30350
IGFzeW5j 30351
IG1pcmFj 30352
YXR0cw== 30353
cHVuaw== 30354
IHBpdm90 30355
IExlZ2lzbGF0aXZl 30356
IGJsb2dnZXJz 30357
IENsYXc= 30358
c2J1cmc= 30359
ZHls 30360
IFJlY29tbWVuZA== 30361
IHZlcnRl 30362
IHByb2hpYml0aW5n 30363
IFBhbnRoZXI= 30364
Sm9uYXRoYW4= 30365
IG9taW4= 30366
IGhhdGVmdWw= 30367
Mjgx 30368
IE9yY2hl 30369
IE11cmRvY2g= 30370
ZG93bnM= 30371
IGFzeW1t 30372
R0VS 30373
QWx3YXlz 30374
IGluZm9ybXM= 30375
IFdN 30376
IFBvbnk= 30377
IEFwcGVuZGl4 30378
IEFybGluZ3Rvbg== 30379
SmFt 30380
IG1lZGljaW5hbA== 30381
IFNsYW0= 30382
SVRJRVM= 30383
IHJlYWZm 30384
IFJp 30385
Rkc= 30386
U3ByaW5n 30387
Ym9vbA== 30388
IHRoaWdocw== 30389
IG1hcmtpbmdz 30390
IFJhcXFh 30391
IExhaw== 30392
cG9sbA== 30393
dHNreQ== 30394
IE1vcnR5 30395
IERlZmluaXRpb24= 30396
IGRlYnVuaw== 30397
ZW5kZXJlZA== 30398
IExlb25l 30399
YXZlcnM= 30400
IG1vcnRnYWdlcw== 30401
QXBwYXJlbnRseQ== 30402
Tmlj 30403
aGF1cw== 30404
IFRob3VzYW5kcw== 30405
YXVsZA== 30406
IG1hc2g= 30407
c2hvb3Q= 30408
IGRpYXJy 30409
IGNvbnNjaW91c2x5 30410
SGVybw== 30411
ZWFz 30412
IE5hdHVyYWxseQ== 30413
IERlc3Ryb3llcg== 30414
IGRhc2hib2FyZA== 30415
c2VydmljZXM= 30416
Um9n 30417
IG1pbGxlbm5pYWxz 30418
IGludmFkZQ== 30419
LSg= 30420
IGNvbW1pc3Npb25z 30421
IEF1Y2tsYW5k 30422
IGJyb2FkY2FzdHM= 30423
IGZyb250YWw= 30424
IGNyYW5r 30425
IEhpc3Rvcmlj 30426
IHJ1bW91cnM= 30427
Q1RW 30428
IHN0ZXJpbA== 30429
IGJvb3N0ZXI= 30430
cm9ja2V0 30431
44K8 30432
dXRzY2hl 30433
IFBJ 30434
IDIzMw== 30435
IFByb2R1Y2Vy 30436
IEFuYWx5dGljcw== 30437
IGludmFsdWFibGU= 30438
IHVuaW50ZW50aW9u 30439
IENZ 30440
IHNjcnV0aW4= 30441
IGdpZ2c= 30442
IGVuZ3VsZg== 30443
IHByb2xldGFyaWF0 30444
IGhhY2tz 30445
IEhldw== 30446
YXJhaw== 30447
IFNsaW1l 30448
aWVsZGluZw== 30449
YWdoZXI= 30450
IEVsbGlvdA== 30451
IHRlbGVjb20= 30452
IDIxOQ== 30453
dWx0YW4= 30454
IEFyYm9y 30455
IFNjb3V0cw== 30456
QmFu 30457
IGxpZmVzcGFu 30458
IGJsYXNw 30459
Mzg4 30460
IGp1ZGljaWFyeQ== 30461
IENvbnRpbmVudGFs 30462
YXNraW5n 30463
TWND 30464
TEVE 30465
IGJhZ2dhZ2U= 30466
IFNvcmNlcmVy 30467
IHJlbW5hbnRz 30468
IEdyaWZmaXRo 30469
ZXRzdQ== 30470
IFN1YmFydQ== 30471
IFBlcnNvbmFsaXR5 30472
ZGVzaWduZWQ= 30473
dXNoaW1h 30474
YWduYXI= 30475
IHJlY29pbA== 30476
IHBhc3Npb25z 30477
XCI6 30478
IHRlZQ== 30479
IGFib2xpdGlvbg== 30480
IENyZWF0aW5n 30481
amFj 30482
IDE5NA== 30483
MDE5 30484
IHBpbGxhcnM= 30485
cmljaGVk 30486
LyI= 30487
dGs= 30488
IGxpdmVsaWhvb2Q= 30489
IHJvYXN0ZWQ= 30490
YWhvbg== 30491
IEh1dGNo 30492
YXNzZXJ0 30493
IGRpdmlkZW5k 30494
IGtuaXQ= 30495
IGRhdW50aW5n 30496
IGRpc3R1cmJhbmNl 30497
IHNoYWxl 30498
IGN1bHRpdmF0ZWQ= 30499
IHJlZnJpZ2VyYXRvcg== 30500
TEI= 30501
IE5FVA== 30502
IGNvbW1lcmNpYWxz 30503
IHRoaW5rZXJz 30504
NDU1 30505
IGNob3A= 30506
QnJvYWQ= 30507
IHN1c3BpY2lvbnM= 30508
IHRhZ2dlZA== 30509
bGlmdGluZw== 30510
IHN0eWxpc2g= 30511
IFNoaWVsZHM= 30512
U2hvcnRseQ== 30513
IHRhaWxz 30514
QXV0aA== 30515
U1RF 30516
IEdBTUU= 30517
IHNlaXNt 30518
IEtpcw== 30519
b2xvZ25l 30520
IGNvd29yaw== 30521
IGZvcmNpYmx5 30522
IHRoeXJvaWQ= 30523
IFBC 30524
QU5F 30525
bWFycmllZA== 30526
aG9yc2U= 30527
IHBvbHltZXI= 30528
IENoYWw= 30529
b2Rvcg== 30530
REVCVUc= 30531
IENvbnRleHQ= 30532
IGJsaXNz 30533
IHBpbnBvaW50 30534
IE1hdGhlbWF0 30535
bGVncmFt 30536
IFdlZWtlbmQ= 30537
IGxhYmVsbGVk 30538
IGJhcnQ= 30539
aXRsZXM= 30540
IGVzdHJvZ2Vu 30541
4oCU4oCU4oCU4oCU4oCU4oCU4oCU4oCU4oCU4oCU4oCU4oCU4oCU4oCU4oCU4oCU 30542
Iic= 30543
IHZpc2libHk= 30544
IG91dHNpZGVy 30545
YWlkYQ== 30546
QXJlYQ== 30547
IGRpc3NlbWlu 30548
IGRpc2hvbmVzdA== 30549
IENsb3NlZA== 30550
IEJ1bGxldGlu 30551
IFJhbXNleQ== 30552
c3dvcmQ= 30553
IFhJ 30554
b3VyY2Vk 30555
U2FtZQ== 30556
MzQ2 30557
IFJlcGU= 30558
IEtvdQ== 30559
Y2FrZQ== 30560
ZW1pcw== 30561
Q2FjaGU= 30562
IE1lYW5pbmc= 30563
IEVubGlnaHQ= 30564
b25vbXk= 30565
IG1hbmlmZXN0YXRpb24= 30566
c3dvcnRo 30567
SmF5 30568
IGNob3Jl 30569
w7Zy 30570
RHJlYW0= 30571
IHNhbmN0aW9uZWQ= 30572
IGN1bHR1cmFsbHk= 30573
IEFyYQ== 30574
TmF2 30575
IHRoZW9sb2dpY2Fs 30576
IHN0cnV0 30577
IFZP 30578
IEhhbmRib29r 30579
IGNvbnN0cnVjdGluZw== 30580
IMK2 30581
IEJlbmVmaXRz 30582
IFBzeWNob2xvZ2ljYWw= 30583
c2Fj 30584
5bg= 30585
cG9saWN5 30586
IE1hdHRlcnM= 30587
IFJlcG9ydGVk 30588
IEJ5dGU= 30589
IHZpdHJv 30590
IE1haWRlbg== 30591
IGxhbQ== 30592
IEplbm5pbmdz 30593
IGdhcm1lbnQ= 30594
IFJ1dGdlcnM= 30595
IFN0YWZmb3Jk 30596
IFdlbGxpbmd0b24= 30597
IGludGVybWl0dA== 30598
IG5wbQ== 30599
IG9yZGVhbA== 30600
IHBsdWdnZWQ= 30601
b29taW5n 30602
aW5pc2hlZA== 30603
ZnJhbWV3b3Jr 30604
IHRpbWJlcg== 30605
IGNhc3M= 30606
IDg1MA== 30607
aWxlc3M= 30608
IFJlZHV4 30609
NzY4 30610
U3RyZQ== 30611
IHN1cnBhc3NlZA== 30612
d2hlbA== 30613
IHBhcmFsbGVscw== 30614
IHZlaWw= 30615
IEdJ 30616
IFJFU1Q= 30617
IHJlYWRpbmVzcw== 30618
c29ydA== 30619
IG1vZGlmeWluZw== 30620
IFNsYXRl 30621
cnVmZg== 30622
IG1hcmJsZQ== 30623
IGluZnJhcmVk 30624
IGF1ZGl0b3I= 30625
IEZBTlRBU1k= 30626
IFBvdmVydHk= 30627
IFNQRA== 30628
ICIo 30629
S3k= 30630
UkFZ 30631
IGV4ZWN1dGlvbnM= 30632
IEJldmVybHk= 30633
IE1hcnhpc20= 30634
IEJ1cnN0 30635
IEthbGk= 30636
ZXN0b25lcw== 30637
Q2xlYXJseQ== 30638
RWxs 30639
44Gn 30640
IFByb2NlZWRpbmdz 30641
VG9rZW4= 30642
SUZJQw== 30643
w7Fh 30644
Q2VudHJhbA== 30645
IEhhbGV5 30646
IERyYW1h 30647
IGZvcm1hdGlvbnM= 30648
T1JO 30649
Qm9va3M= 30650
IGRvbWluYXRpbmc= 30651
IEZseWVycw== 30652
IENvbXBhbmlvbg== 30653
IGRpc2NpcGxpbmVk 30654
IFl1Z29zbGF2 30655
IFNwZWxscw== 30656
IHZlbmdlYW5jZQ== 30657
IGxhbmRsb3Jkcw== 30658
TGVu 30659
IE9ncmU= 30660
YW5vaWE= 30661
IHBpZXJjaW5n 30662
IGNvbmdyZWc= 30663
IHNjb3Jlcg== 30664
b2JpYQ== 30665
IG5pY2tlbA== 30666
IExlYXJucw== 30667
IHJlam8= 30668
IG1hc3RlcnBpZWNl 30669
Rmxhc2g= 30670
IGluaGFiaXRlZA== 30671
IE9wZW5HTA== 30672
IER1ZA== 30673
IElDTw== 30674
IGFydGVy 30675
IHBsdXI= 30676
IG1hc3Rlcnk= 30677
IGxvbmdzdGFuZGluZw== 30678
c3RlZA== 30679
IHdpbmVz 30680
IHRlbGV2aXNlZA== 30681
IFNocmluZQ== 30682
IEJheWVybg== 30683
IOKTmA== 30684
IGVuY2xvc3VyZQ== 30685
am9obg== 30686
IHByb3BoZXRz 30687
IFJlc3VycmVjdGlvbg== 30688
IE9yZGVycw== 30689
IHVuZXZlbg== 30690
cmFscw== 30691
IGR3aW5k 30692
IExhaA== 30693
IFNsb3Zlbg== 30694
Mzc4 30695
IGluc2lzdGVuY2U= 30696
YWZmbGU= 30697
IENsb25l 30698
IGhhcmRzaGlw 30699
IENvbmdyZXNzbWFu 30700
IHBsZWFk 30701
IHJldmlld2Vycw== 30702
IGN1cmVk 30703
IDE5MzU= 30704
YXNsZXk= 30705
ZmFrZQ== 30706
IFRoaW5raW5n 30707
eWRpYQ== 30708
UEFSVA== 30709
IERvdGE= 30710
b2l0 30711
IHdoaXBwZWQ= 30712
IGJvdW5jaW5n 30713
IEhpc3Bhbmljcw== 30714
Y29taW5ncw== 30715
IGNhbm5hYmlu 30716
IENoYW1iZXJz 30717
IFphY2s= 30718
T3B0aW9uYWw= 30719
IGNvYXRz 30720
IHByb3dlc3M= 30721
IE5vcnRvbg== 30722
IHBsYWlubHk= 30723
IGZyZWlnaHQ= 30724
IGluaGliaXRpb24= 30725
IGNsYW0= 30726
IDMwMw== 30727
a2Vm 30728
YWxlaWdo 30729
THVrZQ== 30730
IHBzeWNobw== 30731
YXRvcml1bQ== 30732
TUVE 30733
IHRyZWF0aWVz 30734
IGluZGlzYw== 30735
IGRj 30736
T1BT 30737
IHJlc2lsaWVudA== 30738
IEludGVyc3RhdGU= 30739
IHNsYWNr 30740
IG11bmRhbmU= 30741
IGVzdGFibGlzaGVz 30742
MzU5 30743
IHN0cmFpbmVk 30744
IG5vbmQ= 30745
U3Vz 30746
IGNhc3Rl 30747
YXJhdGU= 30748
aWV2aW5n 30749
IHVuZmFpcmx5 30750
IHBhcnNlcg== 30751
b25pYWw= 30752
dXJzaXZl 30753
Vmlh 30754
IE90dG8= 30755
IEF1dGhvcml0aWVz 30756
c3Ryb2tl 30757
S1I= 30758
IE1lcmN5 30759
IGZ1cm5pc2hlZA== 30760
IG91dHNldA== 30761
IG1ldGlj 30762
MTk4Mg== 30763
b2xpdGhpYw== 30764
IFRlbnQ= 30765
b2dpY2Fs 30766
IEFpcmNyYWZ0 30767
IGhpZGVz 30768
IEJlY2FtZQ== 30769
IGVkdWNhdG9ycw== 30770
cmVhY2hpbmc= 30771
IHZvbGF0aWxpdHk= 30772
IHRvZGRsZXI= 30773
IE5BU0NBUg== 30774
IFR3ZWx2ZQ== 30775
IEhpZ2hsaWdodHM= 30776
IGdyYXBl 30777
IHNwbGl0cw== 30778
IHBlYXNhbnQ= 30779
IHJlbmVn 30780
IE1TSQ== 30781
VGVtcA== 30782
c3RhcnM= 30783
IHRyZWs= 30784
IEh5ZGU= 30785
YmluZGluZw== 30786
IHJlYWxpc20= 30787
IG94aWRl 30788
IEhvcw== 30789
IG1vdW50cw== 30790
IGJpdGluZw== 30791
IGNvbGxhcHNpbmc= 30792
IHBvc3RhbA== 30793
IG11c2V1bXM= 30794
IGRldGFjaGVk 30795
IHJlc3BlY3Rpbmc= 30796
IG1vbm9wb2w= 30797
IHdvcmtmbG93 30798
IENha2U= 30799
VGVtcGxhdGU= 30800
IE9yZ2FuaXNhdGlvbg== 30801
IHBlcnNpc3RlbmNl 30802
MzY5 30803
Q29taW5n 30804
QnJhZA== 30805
IHJlZHVuZGFudA== 30806
IEdUQQ== 30807
IGJlbmRpbmc= 30808
IHJldm9rZWQ= 30809
IG9mZmVuZGluZw== 30810
IGZyYW1pbmc= 30811
IHByaW50Zg== 30812
Q29tbXVu 30813
bWVtYmVycw== 30814
T3V0c2lkZQ== 30815
IGNvbnN0cnVlZA== 30816
IGNvZGVk 30817
Rk9SRQ== 30818
IGNoYXN0 30819
Q2hhdA== 30820
SW5kaWFu 30821
IFlhcmQ= 30822
PyEi 30823
IFBvcnRz 30824
IFhhdmllcg== 30825
IFJFVA== 30826
Jy4i 30827
IEJvYXQ= 30828
aXZhdGVk 30829
aWNodA== 30830
dW1lcmFibGU= 30831
RHM= 30832
IER1bm4= 30833
IGNvZmZpbg== 30834
IHNlY3VyZWx5 30835
IFJhcHRvcnM= 30836
IEJlcw== 30837
SW5zdGFsbGF0aW9u 30838
IGluY2VwdGlvbg== 30839
IEhlYWx0aHk= 30840
ZW5kYW50cw== 30841
IHBzeWNob2xvZ2lzdHM= 30842
IFNoZWlraA== 30843
Y3VsdHVyYWw= 30844
IEJsYWNrQmVycnk= 30845
c2hpZnQ= 30846
RnJlZA== 30847
b2NoZQ== 30848
IGNha2Vz 30849
IFNFTw== 30850
IEdpYW4= 30851
IEFzaWFucw== 30852
b2dnaW5n 30853
ZWxlbWVudA== 30854
IHB1bmRpdHM= 30855
IFZhdWdo 30856
IEdhdmlu 30857
IGhpdHRlcg== 30858
IGRyb3duZWQ= 30859
IGNoYWxr 30860
IFppa2E= 30861
IG1lYXNsZXM= 30862
ODAy 30863
4oCmLi4= 30864
IEFXUw== 30865
XSI= 30866
IGRpc3RvcnQ= 30867
IE1hc3Q= 30868
IGFudGlib2RpZXM= 30869
IE1hc2g= 30870
TWVtb3J5 30871
IFVnYW5kYQ== 30872
IFByb2I= 30873
IHZvbWl0aW5n 30874
IFR1cm5z 30875
IG9jY3VweWluZw== 30876
IGV2YXNpb24= 30877
IFRoZXJhcHk= 30878
IHByb21v 30879
IGVsZWN0cg== 30880
IGJsdWVwcmludA== 30881
IERyZQ== 30882
cHJpY2Vk 30883
IERlcG90 30884
IGFsbGV2aWF0ZQ== 30885
IFNvbWFsaQ== 30886
bWFyZw== 30887
bmluZQ== 30888
IG5vc3RhbGdpYQ== 30889
IFNoZXBoZXJk 30890
IGNhdmFscnk= 30891
IHRvcnBlZA== 30892
IEJsb29keQ== 30893
eGI= 30894
IHNhbms= 30895
IGdvYWx0 30896
cmVwb3J0cHJpbnQ= 30897
ZW1iZWRyZXBvcnRwcmludA== 30898
Y2xvbmVlbWJlZHJlcG9ydHByaW50 30899
IEluaXRpYWxseQ== 30900
IEZpc2NoZXI= 30901
IG5vdGV3b3J0aHk= 30902
Y2Vybg== 30903
IGluZWZmaWNpZW50 30904
cmF3ZG93bmxvYWQ= 30905
cmF3ZG93bmxvYWRjbG9uZWVtYmVkcmVwb3J0cHJpbnQ= 30906
Y2F0aW9u 30907
IER5bmFzdHk= 30908
bGFn 30909
REVT 30910
IGRpc3RpbmN0bHk= 30911
IEVzdG9uaWE= 30912
IG9wZW5uZXNz 30913
IGdvc3NpcA== 30914
cnVjaw== 30915
V2lkdGg= 30916
IElicmFoaW0= 30917
IHBldHJvbGV1bQ== 30918
IGF2YXRhcg== 30919
IEhlZA== 30920
YXRoYQ== 30921
IEhvZ3dhcnRz 30922
IGNhdmVz 30923
Njc4 30924
IHNhZmVndWFyZA== 30925
IE1vZw== 30926
aXNzb24= 30927
IER1cmhhbQ== 30928
c2xhdWdodA== 30929
IEdyYWR1YXRl 30930
IHN1YmNvbnNjaW91cw== 30931
IEV4Y2VsbGVudA== 30932
IER1bQ== 30933
LS0tLS0= 30934
IHBpbGVz 30935
IFdPUks= 30936
IEdhcm4= 30937
IEZvbA== 30938
IEFUTQ== 30939
IGF2b2lkcw== 30940
IFR1bA== 30941
IGJsZWFr 30942
RUxZ 30943
aXZpc3Q= 30944
bGlnaHRseQ== 30945
UGVycw== 30946
IERvYg== 30947
IExT 30948
IGluc2FuaXR5 30949
zrU= 30950
YXRhbGll 30951
RW5sYXJnZQ== 30952
IHR3aXN0cw== 30953
IGZhdWx0eQ== 30954
IHBpcmFjeQ== 30955
IGltcG92ZXI= 30956
IHJ1Z2dlZA== 30957
IEZhc2hpb24= 30958
IHNhbmRz 30959
Jz8= 30960
c3dpY2s= 30961
IG5hdGl2ZXM= 30962
IGhlbg== 30963
IE5vaXNl 30964
44OX 30965
IGdyZWVucw== 30966
IGZyZWV6ZXI= 30967
IGR5bmFzdHk= 30968
IEZhdGhlcnM= 30969
IE5ld2Fyaw== 30970
IGFyY2hhZW9sb2dpY2Fs 30971
IG90 30972
b2Jhcg== 30973
IGJsb2NrYWRl 30974
IGFsbGVyZw== 30975
TFY= 30976
IGRlYml0 30977
IFJGQw== 30978
IE1pbHRvbg== 30979
IFByZXNzdXJl 30980
IHdpbGxpbmdseQ== 30981
IGRpc3Byb3BvcnRpb25hdGU= 30982
IG9wcHJlc3NpdmU= 30983
IGRpYW1vbmRz 30984
IGJlbG9uZ2luZ3M= 30985
MTk3MA== 30986
IGJlbGxz 30987
IGltcGVyaWFsaXNt 30988
IDIyNw== 30989
IGV4cGxvZGluZw== 30990
IEVjbGlwc2U= 30991
IDE5MTk= 30992
IHJhbnQ= 30993
IG5vbWluYXRpb25z 30994
MzQ3 30995
IHBlYWNlZnVsbHk= 30996
cmljYQ== 30997
IEZVQ0s= 30998
IHZpYnJhdGlvbg== 30999
bWFsaW5r 31000
IHJvcGVz 31001
IEl2YW5rYQ== 31002
IEJyZXdlcnk= 31003
IEJvb2tlcg== 31004
IE93ZW5z 31005
Z29lcnM= 31006
U2VydmljZXM= 31007
IFNuYXBl 31008
IDE5MQ== 31009
Mzk1 31010
IDI5OQ== 31011
anVzdGljZQ== 31012
IGJyaQ== 31013
IGRpc2Nz 31014
IHByb21pbmVudGx5 31015
IHZ1bGdhcg== 31016
IHNraXBwaW5n 31017
bHZlcw== 31018
IHRzdW5hbWk= 31019
Mzc0 31020
IFVydWc= 31021
IEVpZA== 31022
cmVjYXRlZA== 31023
cGhlbg== 31024
IGZhdWx0cw== 31025
IFN0YXJ0ZWQ= 31026
OTUw 31027
IHBp 31028
IGRldGVjdG9y 31029
IGJhc3RhcmQ= 31030
IHZhbGlkYXRlZA== 31031
U3BhY2VFbmdpbmVlcnM= 31032
T1VSQ0U= 31033
ICh+ 31034
IHVuc3Vy 31035
IGFmZmlybWVk 31036
IGZhc2Npc20= 31037
IHJlc29sdmluZw== 31038
IENoYXZleg== 31039
IEN5bg== 31040
IGRldHJhY3Q= 31041
TG9zdA== 31042
IHJpZ2dlZA== 31043
IGhvbWFnZQ== 31044
IEJydW5v 31045
NTU1 31046
ZWNh 31047
IHByZXNzZXM= 31048
IGh1bW91cg== 31049
IHNwYWNpbmc= 31050
ICcv 31051
b2xraWVu 31052
Q291bg== 31053
T1BFUg== 31054
VHJl 31055
U29u 31056
IENhbWJvZGlh 31057
aWVycmU= 31058
bW9uZw== 31059
b3p5 31060
IGxpcXVpZGl0eQ== 31061
IFNvdmlldHM= 31062
IEZlcm5hbmRv 31063
IDIyOQ== 31064
IHNsdWc= 31065
IENhdGFsYW4= 31066
ZWxlY3RyaWM= 31067
IHNjZW5lcnk= 31068
IEhlYXJ0aA== 31069
IGNvbnN0cmFpbmVk 31070
IGdvYWxpZQ== 31071
IEd1aWRlbGluZXM= 31072
IEFtbW8= 31073
IFBlYXJzb24= 31074
IHRheGVk 31075
IGZldHVz 31076
UmVzcG9uc2U= 31077
IEFsZXhpcw== 31078
dGhpYQ== 31079
R3V5 31080
IHJlY29uc3RydWN0 31081
IGV4dHJlbWVz 31082
IGNvbmNsdWRpbmc= 31083
IFBlZw== 31084
b29rcw== 31085
IGRlZHVjdGlvbnM= 31086
Um9zZQ== 31087
IGdyb3VuZGJyZWFraW5n 31088
IFRhcmc= 31089
44OB 31090
IFJldmU= 31091
cmVzb3VyY2U= 31092
IG1vb25z 31093
IGVsZWN0cm9tYWduZXRpYw== 31094
IGFtaWRzdA== 31095
IFZpa3Rvcg== 31096
TkVTUw== 31097
QkFDSw== 31098
IGNvbW11dGU= 31099
IEFuYWhlaW0= 31100
IGZsdWN0dWF0aW9ucw== 31101
NjQw 31102
IG5vb2RsZXM= 31103
IENvcGVuaGFnZW4= 31104
IFRpZGU= 31105
IEdyaXp6 31106
IFNFRQ== 31107
IHBpcGVsaW5lcw== 31108
IHNjYXJz 31109
ZW5kbw== 31110
YWd1cw== 31111
IEVURg== 31112
LyM= 31113
IEJlY29tZQ== 31114
NDQ4 31115
IHZpc2M= 31116
IFJlY29tbWVuZGVk 31117
IGp1bXBlcg== 31118
IGNvZ25pdGlvbg== 31119
IGFzc2Fzc2lu 31120
IHdpdG5lc3Npbmc= 31121
IFNldHVw 31122
IGxhYw== 31123
dmlt 31124
SVNN 31125
cGFnZXM= 31126
U1NM 31127
MzU4 31128
IGFkamVjdA== 31129
aW5kdXN0cmlhbA== 31130
bG9yZQ== 31131
Y2hlcnk= 31132
IGdsaXR0ZXI= 31133
IGNhbGY= 31134
RmxvcmlkYQ== 31135
IHNwb2lsZXJz 31136
IHN1Y2NlZWRz 31137
IGNoYW50aW5n 31138
IHNsb2dhbnM= 31139
IFRyYWN5 31140
VmlzaXQ= 31141
cm9sb2d5 31142
IG1vcm5pbmdz 31143
IGxpbmVhZ2U= 31144
IHNpcA== 31145
IGludGVuc2VseQ== 31146
IGZsb3VyaXNo 31147
IFNsZWVwaW5n 31148
IEZlbQ== 31149
b3Jwb3I= 31150
IEtsYW4= 31151
IERhcnRo 31152
aGFjaw== 31153
IE5pZWxzZW4= 31154
IHR1bW9ycw== 31155
IHByb2N1cmVtZW50 31156
IFlvcmtzaGlyZQ== 31157
IHJhaWRlZA== 31158
S1k= 31159
QW5uYQ== 31160
IC8vWw== 31161
IERpc29yZGVy 31162
IE11c3Rhbmc= 31163
IFdlbg== 31164
IFRyeWluZw== 31165
c3E= 31166
IGRlbGl2ZXJpZXM= 31167
IHNodXR0ZXI= 31168
IGNlcmVicmFs 31169
IGJpcG9sYXI= 31170
IENO 31171
bGFzcw== 31172
amV0 31173
IGRlYmF0aW5n 31174
Pjo= 31175
IGVhZ2xl 31176
Z3JhZGVz 31177
IERpeG9u 31178
VUdD 31179
TUFT 31180
IERyYWNv 31181
IE1hY2hpbmVz 31182
YWZmZXI= 31183
IGVtYW4= 31184
wrI= 31185
cHJvbg== 31186
IEd5bQ== 31187
IGNvbXBhcmF0aXZlbHk= 31188
IFRyaWJ1bmFs 31189
UFJP 31190
IGxleA== 31191
IGZlcnRpbGU= 31192
IGRlcHJlc3Npbmc= 31193
IHN1cGVyZmljaWFs 31194
ZXNzZW50aWFs 31195
IEh1bnRlcnM= 31196
Z3A= 31197
IHByb21pbmVuY2U= 31198
TGliZXI= 31199
IEFuY2VzdA== 31200
b3RlY2hub2xvZ3k= 31201
IG1vY2tpbmc= 31202
IFRyYWZm 31203
lpo= 31204
TWVkaXVt 31205
SXJhcQ== 31206
IHBzeWNoaWF0cmlzdA== 31207
UXVhbnRpdHk= 31208
IExlY3Q= 31209
IG5vaXN5 31210
NTIw 31211
R1k= 31212
IHNsYXBwZWQ= 31213
IE1UVg== 31214
IHBhcmE= 31215
cHVsbA== 31216
TXVsdGlwbGU= 31217
YXNoZXI= 31218
IG5vdXI= 31219
IFNlZw== 31220
U3BlbGw= 31221
dm91cw== 31222
b3JkaWFs 31223
U2VuaW9y 31224
IEdvbGRiZXJn 31225
IFBsYXNtYQ== 31226
bmVlZA== 31227
IG1lc3Nlbmdlcg== 31228
ZXJldA== 31229
IHRlYW1lZA== 31230
IGxpdGVyYWN5 31231
IExlYWg= 31232
IERveWxl 31233
IGVtaXR0ZWQ= 31234
VVg= 31235
IGV2YWRl 31236
IG1hemU= 31237
IHdyb25nbHk= 31238
IExhcnM= 31239
IHN0ZXJlb3R5cGU= 31240
IHBsZWRnZXM= 31241
IGFyb21h 31242
IE1FVA== 31243
IGFjcmU= 31244
IE9E 31245
IGZm 31246
IGJyZXdlcmllcw== 31247
IEhpbHRvbg== 31248
dW5kbGU= 31249
IEthaw== 31250
IFRoYW5rZnVsbHk= 31251
IENhbnVja3M= 31252
aW5jdGlvbnM= 31253
IEFwcGVhcnM= 31254
IGNvZXI= 31255
IHVuZGVybWluZWQ= 31256
cm92ZXJz 31257
QW5kcmU= 31258
IGJsYXpl 31259
dW1lcnM= 31260
IGZhbWluZQ== 31261
YW1waGV0YW1pbmU= 31262
dWxrYW4= 31263
QW1vdW50 31264
IGRlc3BlcmF0aW9u 31265
d2lraXBlZGlh 31266
ZGV2ZWxvcG1lbnQ= 31267
IENvcmludGg= 31268
dXNzaWE= 31269
SmFja3Nvbg== 31270
TEk= 31271
TmF0aXZl 31272
UnM= 31273
T2hpbw== 31274
IEthdGhsZWVu 31275
Rm9ydHVuYXRlbHk= 31276
IGF0dGVuZGFudA== 31277
IFByZWZlcnJlZA== 31278
IERpZG4= 31279
IFZz 31280
TWlz 31281
IHJlc3BvbmRlbnQ= 31282
IGJvdW4= 31283
c3RhYmxl 31284
IHBhdmVk 31285
IHVuZXhwbA== 31286
IENoZW5leQ== 31287
TE0= 31288
IEN1bGw= 31289
Ymxvd24= 31290
IGNvbmZyb250aW5n 31291
b2Nlc2U= 31292
c2VydmluZw== 31293
V2k= 31294
IExpdGh1YW5pYQ== 31295
YW5uaQ== 31296
IHN0YWxr 31297
aGQ= 31298
IHZlbmVy 31299
QVBI 31300
eW5jaHJvbm91cw== 31301
VVJS 31302
dW1hYmx5 31303
aGlzdG9yaWM= 31304
SGFsZg== 31305
SGF5 31306
IHJlc2lsaWVuY2U= 31307
c3BlY3Rpb24= 31308
IGFiYW5kb25pbmc= 31309
T2Jz 31310
IERlYmJpZQ== 31311
IGdyYWRpZW50 31312
IFBsYWludA== 31313
IENhbmFs 31314
QVJDSA== 31315
IGV4cGFuc2l2ZQ== 31316
IGZ1bmc= 31317
IGJvdW5jZWQ= 31318
VW5k 31319
IHByZWNhdXRpb25z 31320
IGNsYXJpZmljYXRpb24= 31321
IGRhZ2dlcg== 31322
IGdyaXBz 31323
IMK1 31324
IFJpdmVyYQ== 31325
IFVuZGVhZA== 31326
aXNpdGVz 31327
IEZJUlNU 31328
w7Fv 31329
YXVkaQ== 31330
IGhvc3RhZ2Vz 31331
IGNvbXBsaWFudA== 31332
IGFsdW1uaQ== 31333
U2V2ZW4= 31334
IGN5YmVyc2VjdXJpdHk= 31335
ZWl0aGVy 31336
Q29sbGVjdA== 31337
IGludmFyaWFibHk= 31338
IFNvY2k= 31339
IGxhd21ha2Vy 31340
IGFsZQ== 31341
IFBlcnNvbmFsbHk= 31342
TmF6aQ== 31343
IGN1c3RvbWl6YXRpb24= 31344
IFByb2M= 31345
IFNhc2thdGNoZXdhbg== 31346
ZWF0dXJpbmc= 31347
IHNwYXJlZA== 31348
IGRpc2NvbnRpbnVlZA== 31349
IGNvbXB1dGF0aW9uYWw= 31350
IE1vdG9yb2xh 31351
IHN1cHJlbWFjaXN0 31352
Z292ZXJubWVudGFs 31353
IHBhcmFkaXNl 31354
IERvd25pbmc= 31355
IE5pa29u 31356
IGNhdGFseXN0 31357
YmVycmE= 31358
VG9yb250bw== 31359
ODc1 31360
YmV0YQ== 31361
IE1hY3Jvbg== 31362
IHVucmVhbGlzdGlj 31363
dmVjdG9y 31364
IFZlaGljbGVz 31365
aXRpdmVuZXNz 31366
IFJW 31367
IENvbGJlcnQ= 31368
c2lu 31369
b2pp 31370
ZW50aW4= 31371
IEtyaXNo 31372
aGVsbG8= 31373
ZmZpZWxk 31374
b2t5 31375
IFRhdGU= 31376
IG1hcGxl 31377
IGFpZHM= 31378
Y2hlbWljYWw= 31379
MzM0 31380
bnV0cw== 31381
IFdhcnA= 31382
IHh4 31383
IFJvYmI= 31384
dW1lcm91cw== 31385
Xy1f 31386
ZnRpbWU= 31387
IFZX 31388
IHdpbmdlcg== 31389
IERvbWU= 31390
dG9vbHM= 31391
IFBW 31392
IEdlb3JnZXRvd24= 31393
IGdlYXJlZA== 31394
IGppaGFkaXN0cw== 31395
IGNw 31396
IHN0ZXJvaWRz 31397
TW90aGVy 31398
Y2xlcm9zaXM= 31399
IERSTQ== 31400
bmVzaWE= 31401
IGxpbmdlcg== 31402
IGltbWVyc2l2ZQ== 31403
IENPVU4= 31404
IG91dHdlaWdo 31405
ZW5zdWFs 31406
QmFuZA== 31407
IHRyYW5zZm9ybXM= 31408
bWF0Y2hlZA== 31409
cHNvbnM= 31410
IEp1ZGljaWFs 31411
ZmFjdG9y 31412
IHJlZmVycmFs 31413
IG9kZGx5 31414
IFdlbmdlcg== 31415
QnJpbmc= 31416
IEJvd3M= 31417
NjAy 31418
SUNMRQ== 31419
IGxpb25z 31420
IEFjYWRlbWlj 31421
IFRob3Ju 31422
IFJhaWRlcg== 31423
a2VmZWxsZXI= 31424
U3RvcmFnZQ== 31425
TG93ZXI= 31426
IE9ydA== 31427
IEVxdWFsaXR5 31428
QUxU 31429
IFNPQw== 31430
VHlwZXM= 31431
IGx5bg== 31432
IEFzc2V0 31433
Y29hdA== 31434
VFBQ 31435
Q1ZF 31436
IFBpb25lZXI= 31437
YXBwbGljYXRpb24= 31438
TW9kZXJu 31439
IEhL 31440
RW52aXJvbm1lbnQ= 31441
QWxyaWdodA== 31442
UmFpbg== 31443
SVBQ 31444
IFNoaWl0ZQ== 31445
IG1vdW5k 31446
IEFiaWxpdGllcw== 31447
Y29uZGl0aW9u 31448
U3RhZmY= 31449
IGNvbXBldGVuY2U= 31450
IE1vb3I= 31451
IERpYWJsbw== 31452
IHdpdGhoZWxk 31453
IG9zdGVuc2libHk= 31454
IEJyb20= 31455
IG1zZw== 31456
IGRlbm9taW4= 31457
IFJlZmVyZW5jZXM= 31458
IEZQ 31459
IHBsdW5nZWQ= 31460
IHBhbXBo 31461
bW92aW5n 31462
Y2VudHJhbA== 31463
IGRvd25yaWdodA== 31464
IGZhZGluZw== 31465
VGFs 31466
VHlw 31467
IFRoeQ== 31468
dWtlcw== 31469
aXRoZQ== 31470
IG92ZQ== 31471
IGJhdHRsZWQ= 31472
IHNlYWZvb2Q= 31473
IGZpZ3Vy 31474
IFJE 31475
Y3JvcA== 31476
IHNxdWFkcw== 31477
e1w= 31478
4Lk= 31479
IEVo 31480
IGludGVydmlld2luZw== 31481
IFFpbg== 31482
IGFzcGlyaW5n 31483
UExJQw== 31484
IGNsYXVzZXM= 31485
IEdhc3Q= 31486
IE5pcg== 31487
IGx1Z2dhZ2U= 31488
IGhvc2U= 31489
IHN5c3RlbWQ= 31490
IGRlc2NlbmRpbmc= 31491
IFJldmlzZWQ= 31492
IFJhaWxz 31493
YWxpZ24= 31494
NzA5 31495
MzM3 31496
IGZ1Zw== 31497
Y2hhcmdpbmc= 31498
dGFncw== 31499
IHV0ZXI= 31500
a2lzaA== 31501
V0FSTklORw== 31502
NDkw 31503
cHJvZml0cw== 31504
IHZveWFnZQ== 31505
IGFjZQ== 31506
IFZhbmd1YXJk 31507
IFRhbmtz 31508
IE11aw== 31509
IDIyNg== 31510
U2FmZQ== 31511
QXJtb3I= 31512
IHZvbGNhbmlj 31513
IHdvbWI= 31514
IE1JTA== 31515
IGJlZ2lubmVy 31516
IFJlY29nbg== 31517
IEFBUA== 31518
UExBWQ== 31519
KSE= 31520
IGRldGVjdGluZw== 31521
Y24= 31522
IGJyZWFjaGVz 31523
QmFzaWNhbGx5 31524
IFBhZw== 31525
IE11bmljaXBhbA== 31526
IEluZGll 31527
IExhZg== 31528
IERpc2FibGU= 31529
IE9sc29u 31530
IHJlc3RyYWluZWQ= 31531
IHJ1bGluZ3M= 31532
IGh1bWFuZQ== 31533
ZXZlbnRz 31534
IENpbmVtYQ== 31535
ZGlzcGxheVRleHQ= 31536
IEhhdGNo 31537
YWN0aW9uRGF0ZQ== 31538
b25uYWlzc2FuY2U= 31539
IGFzc2F1bHRpbmc= 31540
IEx1Zw== 31541
Q0hBVA== 31542
IHZpZ29yb3Vz 31543
IFBlcnNl 31544
IGludG9sZXJhbmNl 31545
IFNuYXBjaGF0 31546
IFNoYXJrcw== 31547
IGR1bW15 31548
IERpYWdu 31549
IEd1aXRhcg== 31550
aW1ldGVycw== 31551
NDAz 31552
UkVH 31553
QXg= 31554
IHNlcGFyYXRlcw== 31555
IE1haG0= 31556
IHR2 31557
amFo 31558
T09M 31559
Q2lyYw== 31560
IFdpbmRzb3I= 31561
dXNzaWFu 31562
IGludHVpdGlvbg== 31563
IGRpc2RhaW4= 31564
IERvbm92YW4= 31565
IDIyMQ== 31566
RW1i 31567
IGNvbmRlbW5pbmc= 31568
IGdlbmVyb3NpdHk= 31569
enp5 31570
IHBhbnRpZXM= 31571
IFByZXZlbnQ= 31572
QWN0aW9uQ29kZQ== 31573
QU5B 31574
MzQy 31575
ZXh0ZXJuYWxBY3Rpb25Db2Rl 31576
IHNwZWNpZnlpbmc= 31577
IGNyeXN0YWxs 31578
SmVyZQ== 31579
IHJ1cHQ= 31580
IEFwcHJlbnRpY2U= 31581
IHByb2ZpbGluZw== 31582
0Lo= 31583
U3RyaWtl 31584
IHNpZGVsaW5l 31585
IG9ibGlnYXRlZA== 31586
IG9jY3VsdA== 31587
IGJ1cmVhdWNyYXRpYw== 31588
YW50aWNhbGx5 31589
cnVwdGVk 31590
bmVnYXRpdmU= 31591
IEV0aGlvcGlh 31592
IENpdmlj 31593
IGluc2lkZXJz 31594
ZWxpZ2libGU= 31595
IFRWcw== 31596
IEJBUg== 31597
IFRJ 31598
aW9sb2dpc3Q= 31599
IEFJUg== 31600
IHN1YnN0aXR1dGVk 31601
QXJhYg== 31602
IFNhdWw= 31603
IFlvZw== 31604
cHJlbQ== 31605
IGJ1aWxkZXJz 31606
IHN0YXRpb25hcnk= 31607
IGRvdWJ0ZnVs 31608
IHZpZ29yb3VzbHk= 31609
IHRocmlsbGluZw== 31610
UGh5c2ljYWw= 31611
IENhcmV5 31612
IEh5ZHJh 31613
Z2VvbmluZw== 31614
IFNseQ== 31615
eXRvbg== 31616
IGJvcnJvd2Vycw== 31617
IFBhcmtpbnNvbg== 31618
IOs= 31619
IEphbWFpY2E= 31620
IHNhdGly 31621
IGluc3VyZ2VudHM= 31622
IEZpcm0= 31623
IGlzb3Q= 31624
IEthcm4= 31625
b3VybmluZw== 31626
YWtlbnM= 31627
ZG9jcw== 31628
bGl0dGxl 31629
IE1vbmFjbw== 31630
Q0xBU1M= 31631
VHVya2V5 31632
THk= 31633
IENvbmFu 31634
YXNzaWM= 31635
IHN0YXJyZWQ= 31636
IFBhY2Vycw== 31637
ZXRpZXM= 31638
IHRpcHBpbmc= 31639
TW9vbg== 31640
IFJ3 31641
c2FtZQ== 31642
IGNhdml0eQ== 31643
IGdvb2Y= 31644
IFpv 31645
U2hvY2s= 31646
dW1tZXI= 31647
IGVtcGhhc2l6ZXM= 31648
IHJlZ3JldHQ= 31649
IG5vdmVsdHk= 31650
IGVudnk= 31651
IFBhc3NpdmU= 31652
cnc= 31653
NTA1 31654
IGluZGlmZmVyZW50 31655
IFJpY2E= 31656
IEhpbXNlbGY= 31657
IEZyZWRkaWU= 31658
IGFkaXA= 31659
5LiA 31660
IGJyZWFrb3V0 31661
IGh1cnJpZWQ= 31662
IEh1YW5n 31663
IERpc2s= 31664
IHJvYW1pbmc= 31665
Pz8/Pz8tPz8/Pz8t 31666
VVY= 31667
IFJpY2t5 31668
IFNpZ21h 31669
IG1hcmdpbmFsaXplZA== 31670
IGVkaXRz 31671
IDMwNA== 31672
bWVtb3J5 31673
IHNwZWNpbWVu 31674
Mjkz 31675
44Gv 31676
IHZlcnRpY2FsbHk= 31677
IGF1ZGl0aW9u 31678
IEhlY2s= 31679
IGNhc3Rlcg== 31680
IEhvbGRpbmdz 31681
YWRhbA== 31682
IENyb24= 31683
IExpYW0= 31684
IGRlZmxlY3Q= 31685
UGljaw== 31686
IERlYnVn 31687
UkVG 31688
IHZlcnNhdGlsaXR5 31689
b3RoZXM= 31690
Y2xhc3NpZmllZA== 31691
IE1haGFy 31692
IEhvcnQ= 31693
Q291bnRlcg== 31694
c3Rhc3k= 31695
bm90aWNlZA== 31696
MzMx 31697
IFNoaW0= 31698
ZnVjaw== 31699
IEJpZQ== 31700
IGFpcmluZw== 31701
IFByb3RlaW4= 31702
IEhvbGRpbmc= 31703
IHNwZWN0YXRvcnM= 31704
aWxpYXRlZA== 31705
IFRoYXRjaGVy 31706
bm9zaXM= 31707
44O844Oz 31708
VGVsZQ== 31709
Qm9zdG9u 31710
IFRlbXBs 31711
c3RheQ== 31712
IGRlY2xhcmF0aW9ucw== 31713
NDc5 31714
Vm9sdW1l 31715
IERlc2lnbmVy 31716
IE92ZXJ3YXRjaA== 31717
aWRhZQ== 31718
IG9ud2FyZHM= 31719
IG5ldHM= 31720
IE1hbmlsYQ== 31721
cGFydGljdWxhcmx5 31722
IHBvbGl0aWM= 31723
b290aGVy 31724
IHBvcnRyYWl0cw== 31725
IHBhdmVtZW50 31726
Y2ZmZmY= 31727
IHNhaW50cw== 31728
IGJlZ2lubmVycw== 31729
RVNQTg== 31730
IHNob3J0Y29taW5ncw== 31731
4pWQ4pWQ 31732
IGNvbWV0 31733
IE9yZ2FuaWM= 31734
cXVlbA== 31735
IGhvc3BpdGFsaXplZA== 31736
QnJlYWs= 31737
IHBlZWw= 31738
ZHlsaWI= 31739
YXNweA== 31740
dXJhbmNlcw== 31741
IFRJTQ== 31742
UGc= 31743
IHJlYWRhYmxl 31744
IE1hbGlr 31745
IG11enpsZQ== 31746
IGJlbmNobWFya3M= 31747
ZGFs 31748
IFZhY2M= 31749
IEhpY2tz 31750
NjA5 31751
IEJpYmxpY2Fs 31752
aGVuZw== 31753
IG92ZXJsb2Fk 31754
IENpdmlsaXphdGlvbg== 31755
IGltbW9yYWw= 31756
IGZyaWVz 31757
44KS 31758
IHJlcHJvZHVjZWQ= 31759
IGZvcm11bGF0aW9u 31760
anVn 31761
aXJleg== 31762
Z2Vhcg== 31763
IGNvYWNoZWQ= 31764
TXBTZXJ2ZXI= 31765
IFNK 31766
IEt3 31767
SW5pdA== 31768
ZGVhbA== 31769
IE9ybw== 31770
IExva2k= 31771
IFNvbmdz 31772
IDIzMg== 31773
IExvdWlzZQ== 31774
YXNpb25hbGx5 31775
IHVuY29uZA== 31776
b2xseXdvb2Q= 31777
IHByb2dyZXNzaXZlcw== 31778
IEVub3VnaA== 31779
IERvZQ== 31780
IHdyZWNrYWdl 31781
IGJydXNoZWQ= 31782
IEJhc2VUeXBl 31783
IHpvbmluZw== 31784
aXNoYWJsZQ== 31785
aGV0aWNhbGx5 31786
IENhdWN1cw== 31787
IEh1ZQ== 31788
IGthcm1h 31789
IFNwb3J0aW5n 31790
IHRyYWRlcg== 31791
IHNlZW1pbmc= 31792
IENhcHR1cmU= 31793
NDMw 31794
YmlzaA== 31795
IHR1bmVz 31796
IGluZG9vcnM= 31797
IFNwaGVyZQ== 31798
IERhbmNpbmc= 31799
VEVSTg== 31800
IG5vYg== 31801
IEdTVA== 31802
bWFwcw== 31803
IHBlcHBlcnM= 31804
Rml0 31805
IG92ZXJzZWVz 31806
IFJhYmJp 31807
IFJ1bGVy 31808
dmVydGlzaW5n 31809
b2ZmaWNl 31810
eHh4 31811
IHJhZnQ= 31812
Q2hhbmdlZA== 31813
IHRleHRib29rcw== 31814
TGlua3M= 31815
IE9tbg== 31816
44CR 31817
IGluY29udmVuaWVuY2U= 31818
IERvbmV0c2s= 31819
PX4= 31820
IGltcGxpY2l0bHk= 31821
IGJvb3N0cw== 31822
IEJvbmVz 31823
IEJvb20= 31824
Q291cnRlc3k= 31825
IHNlbnNhdGlvbmFs 31826
QU5Z 31827
IGdyZWVkeQ== 31828
ZWRlbg== 31829
IGluZXhwZXI= 31830
IExlcg== 31831
IFZhbGU= 31832
IHRpZ2h0ZW4= 31833
IEVBUg== 31834
IE51bQ== 31835
IGFuY2VzdG9y 31836
U2VudA== 31837
IEhvcmRl 31838
dXJnaWNhbA== 31839
YWxsYWg= 31840
IHNhcA== 31841
YW1iYQ== 31842
IFNwcmVhZA== 31843
dHdpdGNo 31844
IGdyYW5kc29u 31845
IGZyYWN0dXJl 31846
IG1vZGVyYXRvcg== 31847
IFNldmVudGg= 31848
IFJldmVyc2U= 31849
IGVzdGltYXRpb24= 31850
Q2hvb3Nl 31851
IHBhcmFjaA== 31852
IGJhcnJpYw== 31853
44CQ 31854
IGNvbXBhc3M= 31855
IGFsbGVyZ2lj 31856
4oCV 31857
T1RIRVI= 31858
ZXJyaWxsYQ== 31859
IHdhZ29u 31860
IHppbmM= 31861
IHJ1YmJlZA== 31862
IEZ1bGxlcg== 31863
IEx1eGVtYm91cmc= 31864
IEhvb3Zlcg== 31865
IGxpYXI= 31866
IEV2ZW5pbmc= 31867
IENvYmI= 31868
ZXN0ZWVt 31869
IHNlbGVjdG9y 31870
IEJyYXds 31871
aXNhbmNl 31872
IEVr 31873
IHRyb29w 31874
IGd1dHM= 31875
IEFwcGVhbA== 31876
IFRpYmV0YW4= 31877
IHJvdXRpbmVz 31878
IE1lbnQ= 31879
IHN1bW1hcml6ZWQ= 31880
c3RlYW1hcHBz 31881
IHRyYW5xdQ== 31882
IDE5Mjk= 31883
b3Jhbg== 31884
IEF1dGhlbnQ= 31885
IGdtYXh3ZWxs 31886
IGFwcHJlaGVucw== 31887
IHBvZW1z 31888
IHNhdXNhZ2U= 31889
IFdlYnN0ZXI= 31890
dXJ1cw== 31891
IHRoZW1lZA== 31892
IGxvdW5nZQ== 31893
IGNoYXJnZXI= 31894
U3BvaWxlcg== 31895
IHNwaWxsZWQ= 31896
aG9n 31897
IFN1bmRlcg== 31898
IEFpbg== 31899
IEFuZ3J5 31900
IGRpc3F1YWw= 31901
IEZyZXF1ZW5jeQ== 31902
IEV0aGVybmV0 31903
IGhlbHBlcg== 31904
UGVyY2VudA== 31905
IGhvcnJpZnlpbmc= 31906
IGFpbA== 31907
IEFsbGFu 31908
RUVF 31909
IENyb3NzaW5n 31910
NDQ5 31911
IGhvbG9n 31912
IFB1enpsZXM= 31913
IEdvZXM= 31914
ZXJlbm4= 31915
NjA0 31916
44GP 31917
IFJhZmFlbA== 31918
IGF0dGVu 31919
IEVtYW51ZWw= 31920
IHVwcm8= 31921
IFN1c3A= 31922
UHN5Y2g= 31923
IFRyYWluZXI= 31924
IE5FUw== 31925
IEh1bnRz 31926
YmVjdWU= 31927
IGNvdW5zZWxvcg== 31928
UnVsZQ== 31929
IHRveGlucw== 31930
IGJhbm5lcnM= 31931
cmlmaWNl 31932
IGdyZWV0aW5n 31933
IGZyZW56eQ== 31934
IGFsbG9jYXRl 31935
ICop 31936
ZXhwcg== 31937
NTAz 31938
IENoaWNr 31939
IFRvcm4= 31940
IGNvbnNvbGlkYXRpb24= 31941
IEZsZXRjaGVy 31942
c3dpdGNo 31943
ZnJhYw== 31944
Y2xpcHM= 31945
IE1jS2lu 31946
IEx1bmFy 31947
TW9udGg= 31948
SVRDSA== 31949
IHNjaG9sYXJseQ== 31950
cmFwZWQ= 31951
Mzk4 31952
IDE5MTA= 31953
IGVncmVn 31954
IGluc2VjdXJl 31955
IHZpY3RvcmlvdXM= 31956
Y2ZmZmZjYw== 31957
IHNpbmdsZWQ= 31958
IGVsdmVz 31959
IFdvbmQ= 31960
YnVyc3Q= 31961
IGNhbW91Zmw= 31962
IEJMQUNL 31963
IGNvbmRpdGlvbmVk 31964
54k= 31965
YW5zd2VyZWQ= 31966
IGNvbXB1bHNvcnk= 31967
YXNjaXN0 31968
IHBvZGNhc3Rz 31969
IEZyYW5rZnVydA== 31970
Ym5i 31971
IG5lb2xpYmVyYWw= 31972
IEtleWJvYXJk 31973
IEJlbGxl 31974
d2FybQ== 31975
IHRydXN0cw== 31976
IGluc3VyZWQ= 31977
IEJ1Y2M= 31978
dXNhYmxl 31979
NjA3 31980
IFBsYWlucw== 31981
IDE4OTA= 31982
IHNhYm90YWdl 31983
IGxvZGdlZA== 31984
ZmVsdA== 31985
IGdh 31986
IE5hcmM= 31987
IFNhbGVt 31988
IHNldmVudHk= 31989
IEJsYW5r 31990
cG9ja2V0 31991
IHdoaXNwZXI= 31992
IG1hdGluZw== 31993
b21pY3M= 31994
IFNhbG1hbg== 31995
IEthZA== 31996
IGFuZ2VyZWQ= 31997
IGNvbGxpc2lvbnM= 31998
IGV4dHJhb3JkaW5hcmlseQ== 31999
IGNvZXJjaW9u 32000
R2hvc3Q= 32001
YmlyZHM= 32002
6IA= 32003
a29r 32004
IHBlcm1pc3NpYmxl 32005
YXZvcmFibGU= 32006
IHBvaW50ZXJz 32007
IGRpc3NpcA== 32008
YWNp 32009
IHRoZWF0cmljYWw= 32010
IENvc21pYw== 32011
IGZvcmdldHRpbmc= 32012
IGZpbmFsaXplZA== 32013
5aSn 32014
eW91dA== 32015
bGlicmFyeQ== 32016
IGJvb21pbmc= 32017
IEJlbGlldmU= 32018
IFRlYWNoZXI= 32019
IExpdg== 32020
IEdPT0RNQU4= 32021
IERvbWluaWNhbg== 32022
T1JFRA== 32023
IFBhcnRpZXM= 32024
IHByZWNpcGl0YXRpb24= 32025
IFNsb3Q= 32026
Um95 32027
IENvbWJpbmVk 32028
IGludGVncmF0aW5n 32029
IGNocm9tZQ== 32030
IGludGVzdGluYWw= 32031
IFJlYmVsbA== 32032
IG1hdGNodXBz 32033
IGJsb2NrYnVzdGVy 32034
IExvcmVu 32035
IExldnk= 32036
IHByZWFjaGluZw== 32037
IFNlbmRpbmc= 32038
IFB1cnBvc2U= 32039
cmF4 32040
Zmlm 32041
IGF1dGhvcml0YXRpdmU= 32042
IFBFVA== 32043
YXN0aWNhbA== 32044
IGRpc2hvbg== 32045
IGNoYXR0aW5n 32046
ICIkOi8= 32047
Q29ubmVjdGlvbg== 32048
IHJlY3JlYXRl 32049
IGRlbGlucXU= 32050
IGJyb3Ro 32051
IERpcnR5 32052
IEFkbWlu 32053
em1hbg== 32054
IHNjaG9sYXJzaGlwcw== 32055
IDI1Mw== 32056
Y29udGFjdA== 32057
YWxzYQ== 32058
NzY3 32059
Y3JlZW4= 32060
YWJiYWdl 32061
IDE5MTU= 32062
IGJsZW5kZWQ= 32063
IGFsYXJtZWQ= 32064
TGFuZ3VhZ2U= 32065
MzU2 32066
IGJsZW5kcw== 32067
IENoYW5nZWQ= 32068
V29sZg== 32069
IGhlcGF0 32070
Q3JlYXRpbmc= 32071
IHBlcnNlY3V0 32072
IHN3ZWV0bmVzcw== 32073
YXJ0ZQ== 32074
IGZvcmZlaXR1cmU= 32075
IFJvYmVydG8= 32076
aW1wcm8= 32077
TkZM 32078
IE1hZ25ldA== 32079
RGV0YWlsZWQ= 32080
IGluc2lnbmlmaWNhbnQ= 32081
IFBPTElU 32082
IEJCUQ== 32083
IENQUw== 32084
IHNlYXc= 32085
YW1pbmVy 32086
bUw= 32087
ZW5kaWY= 32088
ZmluYWxz 32089
IDI2NQ== 32090
dWlzaA== 32091
IH0p 32092
IFByb2JsZW1z 32093
IGVtYmxlbQ== 32094
IHNlcmlvdXNuZXNz 32095
IHBhcnNpbmc= 32096
IHN1YnN0aXR1dGlvbg== 32097
IHByZXNzdXJlZA== 32098
IHJlY3ljbGVk 32099
YWxlYg== 32100
UnVieQ== 32101
IHByb2ZpY2llbmN5 32102
RHJpdmVy 32103
IFdlc3Rlcg== 32104
Oic= 32105
QUZUQQ== 32106
IG1hbnRsZQ== 32107
IENsYXl0b24= 32108
ZmxhZw== 32109
IHByYWN0aXRpb25lcg== 32110
Y292ZXJlZA== 32111
IFN0cnVjdA== 32112
YWRkYWZp 32113
NDI1 32114
IFRvd25zaGlw 32115
IEh5ZHJv 32116
TG91aXM= 32117
MzQz 32118
IGNvbmRv 32119
IFRhbw== 32120
IHV0aWxpemF0aW9u 32121
IG5hdXNlYQ== 32122
IERlbXM= 32123
cmlkZ2Vz 32124
cGF1c2U= 32125
IGZvcm11bGFz 32126
IGNoYWxsZW5nZXI= 32127
Mzc2 32128
IGRlZmVjdGl2ZQ== 32129
IFJhaWx3YXk= 32130
IFB1Yk1lZA== 32131
IHlvZ3VydA== 32132
bGJz 32133
IE5vcmZvbGs= 32134
T1BF 32135
IE1vb2R5 32136
IGRpc3RyaWJ1dG9y 32137
IHNjcm9sbHM= 32138
IGV4dHJhY3Rz 32139
U3Rhbg== 32140
IHZpYWJpbGl0eQ== 32141
IGV4cG9zZXM= 32142
IHN0YXJ2YXRpb24= 32143
IFN0ZXBz 32144
IERvZGQ= 32145
ZmV3 32146
U1RE 32147
MzMy 32148
IGNsb3N1cmVz 32149
IGNvbXBsZW1lbnRhcnk= 32150
IFNhc2hh 32151
dW1weQ== 32152
IG1vbmV0 32153
IGFydGljdWxhdGU= 32154
IERvY3Q= 32155
a2lsbGVy 32156
IHNjcmlt 32157
IDI2NA== 32158
IHByb3N0aXR1dGVz 32159
IHNldmVyZWQ= 32160
IGF0dGFjaG1lbnRz 32161
IGNvb2xlZA== 32162
TGV2 32163
IEZhbGs= 32164
ZmFpbA== 32165
IHBvbGljZW1hbg== 32166
IERhZw== 32167
IHByYXllZA== 32168
IEtlcm5lbA== 32169
IGNsdXQ= 32170
IGNhdGg= 32171
IGFub21hbHk= 32172
U3Rvcm0= 32173
ZW1ha2Vy 32174
IEJyZWFrZmFzdA== 32175
dWxp 32176
b2lyZQ== 32177
Sko= 32178
aHo= 32179
T3BlcmF0aW9u 32180
IFNpY2s= 32181
MzU0 32182
IEd1YXRlbWFsYQ== 32183
UmF0ZQ== 32184
IGV4cG9zdXJlcw== 32185
ZmFjZXM= 32186
IEFyY2hhZQ== 32187
cmFm 32188
IE1pYQ== 32189
IDIwMjU= 32190
IG9wYXF1ZQ== 32191
IGRpc2d1aXNlZA== 32192
IEhlYWRxdWFydGVycw== 32193
U2Fo 32194
IHBvdHM= 32195
OTc4 32196
IE1hbGY= 32197
IGZyb3duZWQ= 32198
IHBvaXNvbm91cw== 32199
IENvbnZlcnM= 32200
ZWVrcw== 32201
IGNyYWI= 32202
LiIi 32203
IHRyZWFzb24= 32204
IHJhbmM= 32205
IGVzY2FsYXRpbmc= 32206
IHdhcnI= 32207
IG1vYnM= 32208
IGxhbXBz 32209
IFN1bnNoaW5l 32210
IEJydW5zd2ljaw== 32211
UGhvbmVz 32212
IHNwZWxsZWQ= 32213
IFNraXA= 32214
IDIwNTA= 32215
IDE5MTE= 32216
IFBsdXRv 32217
IEFtZW5k 32218
IG1lYXRz 32219
Mzg3 32220
IHN0b21w 32221
IFpob3U= 32222
IExldmlhdGhhbg== 32223
IEhhemFyZA== 32224
YWR2 32225
IE9yd2VsbA== 32226
IGFsb3Vk 32227
IGJ1bXBlcg== 32228
IEFuYXJjaA== 32229
dWJ1bnR1 32230
IFNlcmlvdXM= 32231
Zml0dGluZw== 32232
IE9wdGlvbmFs 32233
IENlY2ls 32234
UkVBTQ== 32235
IHNlcm90b25pbg== 32236
IGN1bHRpdmF0ZQ== 32237
YWdvZ3Vl 32238
fVw= 32239
IG1vc3F1ZXM= 32240
IFN1bm55 32241
IHJlYWN0aXZl 32242
cmV2b2x1dGlvbg== 32243
IEx1cA== 32244
IEZlZG9yYQ== 32245
IGRlZmVuc2VtYW4= 32246
IFZJRA== 32247
aXN0aW5l 32248
IGRyb3duaW5n 32249
IEJyb2FkY2FzdGluZw== 32250
IHRocmlsbGVy 32251
IFNjeQ== 32252
IGFjY2VsZXJhdGluZw== 32253
IGRpcmVjdHM= 32254
b2RpZWQ= 32255
YmlrZQ== 32256
ZHVyYXRpb24= 32257
IHBhaW5mdWxseQ== 32258
UmVkZA== 32259
IHByb2R1Y3Rpb25z 32260
IGdhZw== 32261
IHdoaXN0 32262
IHNvY2s= 32263
IGluZmluaXRlbHk= 32264
IENvbmNlcm4= 32265
IENpdGFkZWw= 32266
IGxpZXU= 32267
IGNhbmRsZXM= 32268
b2dlbmVvdXM= 32269
YXJnZXI= 32270
IGhlYXZlbmx5 32271
aW5mbGFtbWF0b3J5 32272
UGVyZm9ybWFuY2U= 32273
Q3M= 32274
cnVjdG9zZQ== 32275
YXpha2k= 32276
IHBlc3NpbQ== 32277
IGluZmVyZW5jZQ== 32278
IHBvd2Q= 32279
IFpvZQ== 32280
IHBhaW50cw== 32281
IGRheno= 32282
cHRh 32283
LS0tLS0tLS0tLS0= 32284
IGluc3Bpcg== 32285
IEV4cGVyaW1lbnRhbA== 32286
IEtuaWZl 32287
cmVnb3I= 32288
Ym9ycw== 32289
IHNob3dlcnM= 32290
cm9tZWRh 32291
IHNhaW50 32292
IGJlbmlnbg== 32293
IEppYW5n 32294
IGVudmlzaW9uZWQ= 32295
IHNocm91ZA== 32296
SUZU 32297
SE8= 32298
IHNodWZm 32299
IElDQw== 32300
IHNlZ3JlZw== 32301
IHJldmlzaXQ= 32302
aWdodGhvdXNl 32303
TGk= 32304
IHN1YnN0cmF0ZQ== 32305
IFNlYXM= 32306
IFJld2FyZA== 32307
IEhlcA== 32308
IEJyYXNz 32309
c2Jt 32310
IGVsaW1pbmF0ZXM= 32311
IHN0YW1pbmE= 32312
IFZBVA== 32313
IExvYW4= 32314
IGNvbnN0cmFpbnQ= 32315
IGFwcHJvcHJpYXRlZA== 32316
IHBlcw== 32317
IEFMRQ== 32318
cmFuZ2luZw== 32319
IDQwNA== 32320
Mzky 32321
IGludGVsbGVjdHVhbHM= 32322
YWNodQ== 32323
IHJlc3RydWN0dXJpbmc= 32324
IExldmlu 32325
IHJ1bmVz 32326
IGRlbGlnaHRmdWw= 32327
IGNhcmJvaHlkcmF0ZXM= 32328
IE1vZGVscw== 32329
IEV4cG8= 32330
IHRyYW5zcG9ydGluZw== 32331
YWxsb2M= 32332
IHJpbmdpbmc= 32333
U2Ftc3VuZw== 32334
IHNjYXJjZWx5 32335
IFVSTHM= 32336
IE1BUw== 32337
IHByb3RvdHlwZXM= 32338
IG5hcnJhdG9y 32339
IENQVXM= 32340
Y2Ru 32341
IEJhcnRvbg== 32342
IGRlY2lkZWRseQ== 32343
IFNodQ== 32344
aXhpcg== 32345
b2Npb3Vz 32346
IE15c3Q= 32347
TmludGVuZG8= 32348
IHJldXNl 32349
IGZvcmdpdmVu 32350
RmV3 32351
aW5pY2Fs 32352
bmF0 32353
IHNlYW1sZXNz 32354
IEV2YQ== 32355
IEVWRQ== 32356
IEpP 32357
bGFuZGVycw== 32358
IHNvZnRlcg== 32359
bmVnaWU= 32360
IHRyYW5zaWVudA== 32361
IG9yYml0YWw= 32362
IGZ1bGZpbA== 32363
IEtvbQ== 32364
SG9wZWZ1bGx5 32365
IGR5bmFtaWNhbGx5 32366
IEh1bmdlcg== 32367
5Zs= 32368
IEFybWVuaWE= 32369
ZWxtYW4= 32370
YmVydG8= 32371
IHBpZ2U= 32372
IElEcw== 32373
bGltaXQ= 32374
IHZlaW5z 32375
IHNvYXJpbmc= 32376
cGFja3M= 32377
R29sZGVu 32378
IENyYWI= 32379
aXN0b3I= 32380
IFJQTQ== 32381
ICQk 32382
Z3Jlc3Npb24= 32383
IGppaGFkaXN0 32384
IGdhbWJsZQ== 32385
IGNhcmVn 32386
IGluZmxhdGVk 32387
RmFjZQ== 32388
IEZpcmVhcm1z 32389
IEVtbWFudWVs 32390
4p0= 32391
IHNob2Nrcw== 32392
Z3JhYg== 32393
IHNwbGVuZA== 32394
IEhQVg== 32395
YWJvcnRpb24= 32396
QWJvdmU= 32397
RW50aXR5 32398
cGxheWVycw== 32399
IGNvbW1lbmNlZA== 32400
dWxlbmNl 32401
IGZ1bGZpbGxtZW50 32402
IGVtYm9kaW1lbnRz 32403
IFdlbGZhcmU= 32404
IGhhaWw= 32405
IDxA 32406
dHRlbg== 32407
IGNhdGNoZXI= 32408
IEphemVlcmE= 32409
IHZvbGNhbm8= 32410
IHN0YWJpbGl6ZQ== 32411
IEhhbmRsZXI= 32412
IGludGVuc2lmaWVk 32413
IEFicmFtcw== 32414
IGh1bWlsaWF0aW9u 32415
cGFjZWQ= 32416
NjA1 32417
IENlbnRPUw== 32418
U3BlY2lmaWM= 32419
IGhlZWQ= 32420
IENBTQ== 32421
IEdhbGlsZQ== 32422
RGll 32423
IGFib2xpc2hlZA== 32424
IFRob21zb24= 32425
IFRlYWNoZXJz 32426
IFdhc3M= 32427
am9uZw== 32428
IElTQk4= 32429
IEFsbGllcw== 32430
c2hha2U= 32431
5bc= 32432
dmljdA== 32433
SG93YXJk 32434
IGRlZW0= 32435
IGV4Y2VlZGluZ2x5 32436
IFNtYXJ0c3RvY2tz 32437
aWJl 32438
IGRvb3J3YXk= 32439
IGNvbXBldGVk 32440
aWdtYXQ= 32441
IG5hdGlvbmFsaXN0cw== 32442
IGdyb29t 32443
IEtlZW4= 32444
IGRpc3Bvc2FibGU= 32445
ZGVjbA== 32446
IFRvbGtpZW4= 32447
IFNjaGVtZQ== 32448
IGJpb2Q= 32449
IGF2aWQ= 32450
IEVsb24= 32451
YWdhcg== 32452
IFRTQQ== 32453
Um9tYW4= 32454
IGFydGlmaWNpYWxseQ== 32455
IGFkdmlzb3Jz 32456
WEw= 32457
IEluZmVybm8= 32458
MzY2 32459
IHRlZGlvdXM= 32460
IFBob3RvZ3JhcGh5 32461
IENhcnJpZQ== 32462
IHRyb3Bl 32463
IFNhbmRyYQ== 32464
IGRlY2ltYWw= 32465
UXVlZW4= 32466
IEd1bmRhbQ== 32467
IE9N 32468
b3RlY2g= 32469
TkJB 32470
IDE5MzI= 32471
IGVudHJlbmNoZWQ= 32472
IE1hcmlvbg== 32473
IGZyYXRlcm5pdHk= 32474
TGFib3Vy 32475
SGVucnk= 32476
IGxhdGl0dWRl 32477
RWl0aGVy 32478
IGVuaGFuY2Vz 32479
IFBvdGVudGlhbA== 32480
IHNoaW5lcw== 32481
aWRhZA== 32482
IGJyZWFkdGg= 32483
IGNhcGFjaXRpZXM= 32484
IPCfmYI= 32485
IEJyb254 32486
IHNleGVz 32487
IGRpZmZlcmVudGlhdGlvbg== 32488
IGhlYXZ5d2VpZ2h0 32489
IFRhag== 32490
ZHJh 32491
IG1pZ3JhdGU= 32492
IGV4aGF1c3Rpb24= 32493
IFJVTg== 32494
ZWxzaXVz 32495
IEN1b21v 32496
IGd1aXRhcnM= 32497
IGNsb25lcw== 32498
IFNvbWV3 32499
IFByeQ== 32500
LS0tLS0tLS0tLS0tLQ== 32501
IHdhcnJhbnRlZA== 32502
Y3ljbGVz 32503
IHNhbHZhZ2U= 32504
IGRpc2tz 32505
UkFOVA== 32506
IE5HT3M= 32507
IE1hcnRpYW4= 32508
IjpbeyI= 32509
IGFkZGljdHM= 32510
b2p1cmU= 32511
aWxsZXQ= 32512
IGFtYXppbmdseQ== 32513
YXJ0bWVudHM= 32514
cGl4ZWw= 32515
IEdQVXM= 32516
TGF5b3V0 32517
6KM= 32518
IFRhbWls 32519
IEJhc2ls 32520
IGltcGFydGlhbA== 32521
IFN0cnVjdHVyZQ== 32522
Zm9yaw== 32523
YnJ5Y2U= 32524
IHJpZGdl 32525
IEhhbWJ1cmc= 32526
cmlvdXM= 32527
IGJsaXR6 32528
Y2lnYXJldHRlcw== 32529
IGNhbm5lZA== 32530
NDAy 32531
IGlyb25pY2FsbHk= 32532
IGNvbXBhc3Npb25hdGU= 32533
IEhhd2tpbnM= 32534
LiM= 32535
IENhdGhlZHJhbA== 32536
IHJhbGxpZWQ= 32537
aW50ZXJuYWw= 32538
IHF1b3Rh 32539
c3Rha2Vz 32540
VEVYVA== 32541
bW9t 32542
IGNvbXBsZXRlcw== 32543
IDIzOA== 32544
IHNocnVn 32545
44OR 32546
IE5pbnRo 32547
IHJldmlzZQ== 32548
IFByb3ZpZGVy 32549
IHRyZWFjaGVy 32550
IHF1YXNp 32551
IFBSRVM= 32552
IGRlcG9zaXRpb24= 32553
IGNvbmZpZGVudGlhbGl0eQ== 32554
aXNzb3Jz 32555
IGltYmFsYW5jZQ== 32556
IHNwYW5uaW5n 32557
IGFuZ3VsYXI= 32558
IEN1bA== 32559
Y29tbXVuaWNhdGlvbg== 32560
IE5vcmE= 32561
IEdlbml1cw== 32562
b3B0ZXI= 32563
IHNhY2tlZA== 32564
U3BvdA== 32565
IGZpbmVseQ== 32566
IENIUg== 32567
Mjgy 32568
d2F2ZXM= 32569
UGFsZXN0 32570
IFJvaGluZw== 32571
Tkw= 32572
6L8= 32573
IHNoaXR0eQ== 32574
IFNjYWxpYQ== 32575
NDc1 32576
UHJvZ3Jlc3M= 32577
IHJlZmVyZW5jaW5n 32578
IGNsYXNzcm9vbXM= 32579
YWJlZQ== 32580
IHNvZA== 32581
aGVzaW9u 32582
NzA4 32583
IFp1Y2tlcmJlcmc= 32584
IEZpbmlzaA== 32585
IFNjb3RpYQ== 32586
IFNhdmlvcg== 32587
IEluc3RhbGxhdGlvbg== 32588
YW50aGE= 32589
KC0= 32590
IDMwMg== 32591
IFB1bms= 32592
IGNyYXRlcg== 32593
eW91dHU= 32594
IHJvYXN0 32595
IGluZmx1ZW5jaW5n 32596
IGR1cA== 32597
IEpS 32598
IEdyYXY= 32599
IHN0YXR1cmU= 32600
IGJhdGhyb29tcw== 32601
QXNpZGU= 32602
V2lraQ== 32603
bWVhbg== 32604
IFphaw== 32605
IE9uZXM= 32606
IE5hdGg= 32607
IGh5cGVydA== 32608
IGNvbW1lbmNlbWVudA== 32609
Q2l2aWw= 32610
IG1vZGVyYXRlbHk= 32611
IGRpc3RyaWJ1dG9ycw== 32612
IGJyZWFzdGZlZWRpbmc= 32613
IDk4MA== 32614
IFNpaw== 32615
IENpZw== 32616
IEFNRVI= 32617
UklQ 32618
IENhcmVlcg== 32619
dXN0aW5n 32620
IG1lc3NlZA== 32621
IGVo 32622
IEplbnNlbg== 32623
LyQ= 32624
IGJsYWNrbWFpbA== 32625
IGNvbnZlcnNpb25z 32626
IHNjaWVudGlmaWNhbGx5 32627
IG1hbnRyYQ== 32628
cGF5aW5n 32629
IGl2b3J5 32630
IENvdXJ0cw== 32631
T1VHSA== 32632
YXVudGxldA== 32633
U2VyaWFs 32634
QnJvdw== 32635
IEh1bmRyZWRz 32636
MzIz 32637
IHBlZQ== 32638
IGxpbnV4 32639
IHN1Ym1lcg== 32640
IFByaW5jaXBhbA== 32641
NDg1 32642
IERTTA== 32643
IENvdXNpbnM= 32644
IGRvY3RyaW5lcw== 32645
IEF0aGxldGljcw== 32646
IDMxNQ== 32647
IEthcm1h 32648
IGF0dGVudA== 32649
dXJnZXI= 32650
IHByZXNjcmliZQ== 32651
IGVuY2Fwcw== 32652
IENhbWU= 32653
IHNlY3JldGl2ZQ== 32654
IENyaW1lcw== 32655
ZG4= 32656
Q2xlYW4= 32657
IEVneXB0aWFucw== 32658
IENhcnBlbnRlcg== 32659
IGxs 32660
SHVt 32661
IE1pbG8= 32662
IGNhcGl0YWxpc3Rz 32663
IGJyaWVmZWQ= 32664
VHdl 32665
IEJhc2lu 32666
ZWx2ZXQ= 32667
TW9z 32668
IHBsdW5nZQ== 32669
IEthaXNlcg== 32670
IEZ1ag== 32671
aWxsaW4= 32672
IHNhZmVndWFyZHM= 32673
IG9zdGU= 32674
IE9wcG9ydHVuaXR5 32675
IE1hZmlh 32676
IENhbGxpbmc= 32677
YXBh 32678
dXJiYW4= 32679
YnJ1c2g= 32680
aWxsYXJk 32681
Y8Op 32682
aW50ZWxsaWdlbmNl 32683
IExvYg== 32684
IERydWlk 32685
IHNtb290aGVy 32686
IGZvb3Rpbmc= 32687
IG1vdG9yaXN0cw== 32688
YXJjaXR5 32689
IG1hc2N1bGluaXR5 32690
IG1pc20= 32691
IGFiZG9taW5hbA== 32692
IFRhdmVybg== 32693
IFJvaA== 32694
IGVzY2FwZXM= 32695
c2lnbmVk 32696
QW50aG9ueQ== 32697
IHNhY3JpZmljaW5n 32698
IGludGltYWN5 32699
IGFudGVyaW9y 32700
IEtvZA== 32701
IG1vdGlm 32702
IGdyYXo= 32703
IHZpc3VhbGl6YXRpb24= 32704
IGd1aXRhcmlzdA== 32705
IFRyb3Rza3k= 32706
bWFnaWM= 32707
RGFy 32708
IE1vcmk= 32709
IHdhcmRz 32710
IHRvaWxldHM= 32711
bGVzdA== 32712
IHRlbGVwb3J0 32713
IFN1bmRheXM= 32714
IFBsYXQ= 32715
RVRT 32716
IGVTcG9ydHM= 32717
UGF0cmljaw== 32718
IEthdGhlcmluZQ== 32719
ZW5rbw== 32720
IGhhc3NsZQ== 32721
IE1pY2s= 32722
Z2dsZXM= 32723
IGhvYg== 32724
YWludGFpbg== 32725
IGFpcmJvcm5l 32726
IHNwYW5z 32727
IGNoaWxp 32728
IGFwZXJ0dXJl 32729
IHZvbHVudGVlcmVk 32730
IEluY2lkZW50 32731
IEZyZXM= 32732
IFZldGVyYW4= 32733
YXVnaHRlcmVk 32734
aW5nbw== 32735
IHVuaW5zdXJlZA== 32736
Q0xPU0U= 32737
IGZ1c2U= 32738
IGVyb3RpYw== 32739
IGFkdmVydGlzZQ== 32740
cmFpc2luZw== 32741
VGV4dHVyZQ== 32742
IGF0dGVuZHM= 32743
IFJFQUw= 32744
dWRkbGVk 32745
IHNtb290 32746
IDMwNQ== 32747
IFdpbGxpcw== 32748
IGJsb25k 32749
QW5hbHlzaXM= 32750
IFZU 32751
b25pY2E= 32752
IHN0cm9uZ2hvbGQ= 32753
UkY= 32754
Tk0= 32755
Lj4+ 32756
IHByb3NwZXJvdXM= 32757
IGJvYXN0ZWQ= 32758
Mjky 32759
IE1hbnVmYWN0dXJpbmc= 32760
UFJFU1M= 32761
Z3Jlbg== 32762
IHBoYXJtYWN5 32763
IFJvY2tlZmVsbGVy 32764
a2Fp 32765
IHRodW1icw== 32766
IEh1dA== 32767
IG1vdGhlcmJvYXJk 32768
IGd1YXJkaWFucw== 32769
IEFsdGVy 32770
bGx1bGFy 32771
IHNoYWNr 32772
IHdpc2VseQ== 32773
IGJhY2tib25l 32774
ZXJ2YQ== 32775
IHN1aWNpZGVz 32776
IE1jR3JlZ29y 32777
aWphaA== 32778
RW1lcg== 32779
IEJyYXY= 32780
IGRlc2lnbmF0ZQ== 32781
UE9TVA== 32782
cHJvZHVjZWQ= 32783
IGNsZWFuc2luZw== 32784
aXJsd2luZA== 32785
ZXhpc3RlbnQ= 32786
IEh1bXBo 32787
IFBheW5l 32788
IHZlc3RlZA== 32789
xaE= 32790
IHN0cmluZ2VudA== 32791
aW9uYQ== 32792
IHVuc3Vi 32793
IHN1bW1lZA== 32794
IEhlcmN1bGVz 32795
c3ViamVjdA== 32796
IFJhZ25hcg== 32797
IE5vcw== 32798
IGNoYXJhY3Rlcml6YXRpb24= 32799
IHNhdnZ5 32800
IERhd3Nvbg== 32801
IENhc2lubw== 32802
IGZyaQ== 32803
IEJhcnJpZXI= 32804
IG1pc2luZm9ybWF0aW9u 32805
IGluc3VsYXRpb24= 32806
IGNvcnJpZG9ycw== 32807
IGFpcnBsYW5lcw== 32808
IE5vY3Q= 32809
YWhp 32810
IDE5MTY= 32811
a2I= 32812
YXJtYWM= 32813
IHNodW4= 32814
IHNjaGVtYQ== 32815
IGhvcnJpZmllZA== 32816
IDIzOQ== 32817
YXVuZGVycw== 32818
TkI= 32819
aWF0ZXM= 32820
ZXJpdHk= 32821
IFNoYXJk 32822
IHJhcml0eQ== 32823
IGdyb3VwZWQ= 32824
IEdoYW5h 32825
YWdhaW5zdA== 32826
IEJpb2xvZ2ljYWw= 32827
IEF3YXJl 32828
b3dlbGw= 32829
z4Q= 32830
IEJlYXU= 32831
c2hhdw== 32832
SGFjaw== 32833
IEp1bGl1cw== 32834
VVNT 32835
b2xzb24= 32836
YXVuYQ== 32837
Y3J1 32838
IE1hdXJpY2U= 32839
IElr 32840
IHNlcXVlbmNpbmc= 32841
IHJhZGljYWxz 32842
ICg/LA== 32843
dmlydHVhbA== 32844
IGFueXdheXM= 32845
IHJlcGVyYw== 32846
IGhhbmRsZXJz 32847
IGhlc2l0YW50 32848
6YM= 32849
IE1G 32850
cGxlbWVudGF0aW9u 32851
YXNzb2NpYXRlZA== 32852
IGNhbXBhaWduZWQ= 32853
IFl1ZQ== 32854
dXRhdGlvbnM= 32855
IFlvZ2E= 32856
IHNpbW1lcg== 32857
IHJvZHM= 32858
IG1lbG9keQ== 32859
IGNvbnZveQ== 32860
dmlkZW9z 32861
IHNjcmVlbmVk 32862
TmVn 32863
b2NoZW1pY2Fs 32864
ICgpKQ== 32865
IHVsdHJhcw== 32866
IGFudGlw 32867
IElzbGFuZGVycw== 32868
NzA0 32869
IGZldGlzaA== 32870
IHJpZGljdWxvdXNseQ== 32871
IEthcnQ= 32872
IG1pdG9jaG9uZHJpYWw= 32873
IGludGVyZmVyaW5n 32874
QnVpbGRlcg== 32875
IG92ZXJmbA== 32876
IGFjbmU= 32877
IE11ZA== 32878
IEtlcnI= 32879
ZmxleA== 32880
IFBvc3RhbA== 32881
IEJhbHRpYw== 32882
NDc3 32883
IFBlcnNvbnM= 32884
b3VyYWdl 32885
SEI= 32886
IE11c2U= 32887
IEltbW9ydGFs 32888
IERyaXZpbmc= 32889
IHBldGl0aW9ucw== 32890
IHN1YnNjcmlwdA== 32891
IHNvcmNl 32892
IFByb2Nlc3Nvcg== 32893
dXRvbg== 32894
U29ueQ== 32895
IHBob24= 32896
IHJhY2Vk 32897
IEFudGhyb3A= 32898
IGRheXRpbWU= 32899
IEV4ZXJjaXNl 32900
QWRkaW5n 32901
IGVuZ2FnZXM= 32902
IFF1YWxjb21t 32903
IG1pcmFjbGVz 32904
IG1lbWVz 32905
IERyaW5r 32906
IE9yaW9sZXM= 32907
IGhhaXJz 32908
IFBvbGFy 32909
YXRob20= 32910
IHNsaXBwZXJ5 32911
IFJlbXk= 32912
IGNhcmFtZWw= 32913
IFlFQVI= 32914
IGFsaw== 32915
SWdu 32916
YXV0aW9u 32917
IE1lcmxpbg== 32918
IENyYW4= 32919
IGFwb2xvZ2llcw== 32920
IDQxMA== 32921
IG91dGluZw== 32922
IE1lbW9yaWVz 32923
YXBwb2ludGVk 32924
IGNvdW50ZXJlZA== 32925
dWxk 32926
cG9zaW5n 32927
IGZpcmV3YWxs 32928
IFdhc3Q= 32929
IFdldA== 32930
d29ya2Vk 32931
c2VsbGVy 32932
IHJlcGVhbGVk 32933
ZXJlbw== 32934
YXNzdW1pbmc= 32935
QkxJQw== 32936
bWl0ZQ== 32937
IENFT3M= 32938
IENoYXBlbA== 32939
ZWxsaWdlbnQ= 32940
X19fX19fX19fX19fX19fX19fX19fX19f 32941
RG9n 32942
IHdhcnQ= 32943
IHN1YnNjcmliZXI= 32944
c3BvcnRz 32945
IGJlZ2dlZA== 32946
IE1W 32947
IHNlbWlm 32948
ZXRoaWNhbA== 32949
IHByZWFjaA== 32950
IHJldml0YWw= 32951
IHB1bml0aXZl 32952
IHNob3J0Y3V0cw== 32953
IGluc3RpdHV0ZWQ= 32954
IFdhcnNhdw== 32955
IGFiZG9tZW4= 32956
IEtJTkc= 32957
IHN1cGVyaW50ZW5kZW50 32958
IGZyeQ== 32959
IEdlbw== 32960
VE9S 32961
IGNvbnRyYWRpY3Rpb25z 32962
YXB0aWM= 32963
IGxhbmRzY2FwZXM= 32964
YnVncw== 32965
IGNsdXN0 32966
IHZvbGxleQ== 32967
Y3JpYmVk 32968
IHRhbmRlbQ== 32969
IHJvYmVz 32970
V0hBVA== 32971
IHByb21vdGVy 32972
IGVsb3F1 32973
cmV2aWV3ZWQ= 32974
IERL 32975
IFBsYXRv 32976
IGZwcw== 32977
VGFuaw== 32978
IERlcnJpY2s= 32979
IHByaW9yaXRpemU= 32980
YXNwZXI= 32981
IEhvbmR1cmFz 32982
IENvbXBsZXRlZA== 32983
bmVj 32984
IG1vZw== 32985
bmly 32986
IE1heW8= 32987
REVG 32988
c3RhbGw= 32989
aW5uZXNz 32990
IFZvbGtzd2FnZW4= 32991
IHByZWNhdXRpb24= 32992
IE1lbGw= 32993
aWFr 32994
aXN0cmllcw== 32995
IDI0OA== 32996
IG92ZXJsYXBwaW5n 32997
U2VuYXRl 32998
IEVuaGFuY2U= 32999
cmVzeQ== 33000
cmFjaWFs 33001
T1JUUw== 33002
IE1vcm1vbnM= 33003
U3Ryb25n 33004
IENvY2g= 33005
TWV4aWNv 33006
IE1hZHVybw== 33007
IGphcnM= 33008
IGNhbmU= 33009
V2lr 33010
b2xsYQ== 33011
aWZmZXJlbmNl 33012
IHBoeXNpY2lzdA== 33013
IE1hZ2dpZQ== 33014
IDI4NQ== 33015
IGRlcGljdGlvbg== 33016
IE1jTGFyZW4= 33017
SnU= 33018
IHNsb3dz 33019
IGNvbW1pc3Npb25lcnM= 33020
IFdpbGxvdw== 33021
IEV4cGxvcw== 33022
aG92YWg= 33023
IHRlY2huaWNpYW4= 33024
IGhvbWljaWRlcw== 33025
IEZsYXY= 33026
IFRydW1hbg== 33027
IDEwMDAw 33028
dWN0b3I= 33029
IHNoYWRlcg== 33030
TmV3c2xldHRlcg== 33031
NDU3 33032
IHJldmVy 33033
IGhhcmRlbmVk 33034
IHdoZXJlYWJvdXRz 33035
IHJlZGV2ZWxvcA== 33036
IGNhcmJz 33037
IHRyYXZlcnM= 33038
IHNxdWlycmVs 33039
IGZvbGxvd2Vy 33040
IHNpbmdz 33041
NTA4 33042
IHJhYmJpdHM= 33043
ZW1vbml1bQ== 33044
IGRvY3VtZW50aW5n 33045
IG1pc3VuZGVyc3Rvb2Q= 33046
KSc= 33047
Umljaw== 33048
Z2dpZXM= 33049
IHByZW1pZQ== 33050
IHNrYXRpbmc= 33051
IHBhc3Nwb3J0cw== 33052
IGZpc3Rz 33053
YWdlZGRvbg== 33054
SGF3 33055
QUNQ 33056
MDgw 33057
IFRob3VnaHRz 33058
IENhcmxzb24= 33059
IHByaWVzdGhvb2Q= 33060
aHVh 33061
IGR1bmdlb25z 33062
IExvYW5z 33063
IGFudGlz 33064
IGZhbWlsaWFyaXR5 33065
IFNhYmI= 33066
b3BhbA== 33067
IEluaw== 33068
c3RyaWtl 33069
IGNyYW0= 33070
IGxlZ2FsaXplZA== 33071
IGN1aXNpbmU= 33072
IGZpYnJl 33073
VHJhdmVs 33074
IE1vbnVtZW50 33075
T0RZ 33076
ZXRoeQ== 33077
IGludGVyc3RhdGU= 33078
IFBVUg== 33079
ZW1wb3Jhcnk= 33080
IEFyYWJpYW4= 33081
ZGV2ZWxvcGVk 33082
IHNhZGRsZQ== 33083
IGdpdGh1Yg== 33084
IE9mZmVy 33085
IElTUA== 33086
cm9sZXQ= 33087
IFNVUEVS 33088
IERlbmlz 33089
IG11bHRpcGxpZXI= 33090
IHN0aXJyZWQ= 33091
SW50ZXJlc3RpbmdseQ== 33092
IGN1c3RvbWFyeQ== 33093
IGJpbGxlZA== 33094
aGV4 33095
IG11bHRpcGxpZWQ= 33096
IGZsaXBwaW5n 33097
IENyb3NieQ== 33098
IGZ1bmRhbWVudGFscw== 33099
aWFl 33100
IFBsYXllZA== 33101
IEF0b20= 33102
YW1hem9u 33103
IEZsYW0= 33104
ZWV6 33105
YWN0aXZhdGVk 33106
IHRhYmxlc3Bvb24= 33107
IGxpYmVyYWxpc20= 33108
IFBhbGlu 33109
IFBhdGVs 33110
TnVt 33111
IFRBTQ== 33112
IHN1cm4= 33113
IFJlbG9hZGVk 33114
IGNvaW5lZA== 33115
Il0s 33116
IENsYXNo 33117
IEFndQ== 33118
IHByYWdtYXRpYw== 33119
IEFjdGl2YXRl 33120
IDgwMg== 33121
IHRyYWlsZXJz 33122
IHNpbGhvdQ== 33123
IHByb2Jlcw== 33124
IGNpcmN1cw== 33125
IEJhaW4= 33126
IExpbmRzYXk= 33127
IEFiYmV5 33128
RGVsaXZlcnk= 33129
IGNvbmNlc3Npb24= 33130
IGdhc3Rybw== 33131
IFNwcml0ZQ== 33132
xJ8= 33133
YW5kZWw= 33134
IGdpbW0= 33135
IGF1dG9iaQ== 33136
IFR1cnRsZQ== 33137
IHdvbmRlcmZ1bGx5 33138
IEhhcmFt 33139
IFdvcmxkd2lkZQ== 33140
IEhhbmRsZQ== 33141
IHRoZW9yaXN0cw== 33142
IHNsZWVr 33143
IFpodQ== 33144
b2dyYXBoaWNhbGx5 33145
RUdB 33146
IE93bmVycw== 33147
YXRocw== 33148
IEFudGFyY3RpYw== 33149
bmF0YWw= 33150
PSIi 33151
ZmxhZ3M= 33152
YGBgYA== 33153
IHN1bA== 33154
S2g= 33155
IHBvdGFzc2l1bQ== 33156
IGxpbmVtYW4= 33157
IGNlcmVhbA== 33158
IFNlYXNvbnM= 33159
IDIwMjI= 33160
IG1hdGhlbWF0aWM= 33161
IGFzdHJvbm9tZXJz 33162
cHJvZmVzc2lvbmFs 33163
IGZhcmVz 33164
Y2tub3dsZWQ= 33165
IGNoaQ== 33166
IHlvdW5nc3RlcnM= 33167
IG1pc3Rha2VubHk= 33168
IGhlbWlzcGhlcmU= 33169
IERpdmluaXR5 33170
cm9uZQ== 33171
ICIs 33172
cmluZ3M= 33173
IGF0dHJhY3Rz 33174
dmFuYQ== 33175
5bk= 33176
Q0FQ 33177
IHBsYXlsaXN0 33178
IHBvcmNo 33179
44Gj 33180
IGluY29ycG9yYXRlcw== 33181
IHNvYWs= 33182
IGFzc2VydGluZw== 33183
IFRlcnJvcmlzbQ== 33184
IFBhYmxv 33185
SmE= 33186
Y2VzdGVy 33187
IGZlYXJpbmc= 33188
IFByYXllcg== 33189
IGVzY2FsYXRlZA== 33190
R1c= 33191
IHJvYmU= 33192
IEJyaWdodG9u 33193
YWNpc3Rz 33194
IFN5bXBob255 33195
IER3YXJm 33196
IFBhcmFkZQ== 33197
IExlZ28= 33198
IGluZXhwbA== 33199
IGxvcmRz 33200
bGVhZg== 33201
UkFH 33202
bGliZXI= 33203
IGNpZ2Fycw== 33204
IEplaG92YWg= 33205
NjA2 33206
V0lORE9XUw== 33207
IExpYmVyaWE= 33208
ZWJ1cw== 33209
SGVhdnk= 33210
IGx1YnJpYw== 33211
IFJX 33212
YW5ndWFnZXM= 33213
IG5hcnJvd2Vk 33214
Y29tcHV0ZXI= 33215
IEVtYmVy 33216
IG11cmRlcmluZw== 33217
IGRvd25zdHJlYW0= 33218
IFR1bHM= 33219
IFRhYmxlcw== 33220
VG9waWM= 33221
IEFjY3VyYWN5 33222
PS8= 33223
bG9zdA== 33224
IFJlaQ== 33225
IHByb2dyZXNzZXM= 33226
YmVhcg== 33227
IGVzdGFibGlzaG1lbnRz 33228
SnVzdGlu 33229
IFBlYWNo 33230
IEdvbWV6 33231
5b8= 33232
IFRyaWFuZ2xl 33233
SWRlbnQ= 33234
IEhpdmU= 33235
UmVzb3VyY2Vz 33236
IG1peGVz 33237
IEFzc3VtaW5n 33238
TXU= 33239
IGh5cG9j 33240
IHNhbmU= 33241
IFdhbg== 33242
aWRpb3Vz 33243
U3VjY2Vzcw== 33244
IGlv 33245
QW5nZWw= 33246
IGRhbmdlcm91c2x5 33247
IENyZWF0dXJl 33248
V09SSw== 33249
Ols= 33250
IEthdHJpbmE= 33251
TGlzdGVuZXI= 33252
TWlsbGVy 33253
IElkbGli 33254
aGFuZw== 33255
IGNpcmN1bXZlbnQ= 33256
aHJlZg== 33257
IGNlbGVzdGlhbA== 33258
IFdlZWtz 33259
IFB1Zw== 33260
IERhbHRvbg== 33261
IHN1YnBvZW5h 33262
dWt1 33263
IHBlcnNpc3RlZA== 33264
cGVp 33265
b2xkaW5n 33266
IERvY3VtZW50cw== 33267
IEhhc3Q= 33268
IENFTlQ= 33269
IHByaW1lcg== 33270
IHN5bm9ueW1vdXM= 33271
IG5pYg== 33272
b21icw== 33273
IG5vdGF0aW9u 33274
IERpc2g= 33275
IEF0bW9zcA== 33276
IGZvcmJpZA== 33277
IEFORw== 33278
cGF0dGVybg== 33279
bG9z 33280
IHByb2plY3RpbGVz 33281
YnJvd24= 33282
LiIs 33283
IFZlbm9t 33284
IGZpZXJjZWx5 33285
dWJsaXNoZWQ= 33286
IFVyYW4= 33287
IE5pY2FyYWc= 33288
NDEw 33289
IENBTA== 33290
T1RPUw== 33291
IE1pcmFjbGU= 33292
IEVuY2hhbnQ= 33293
IGd1YXJkaW5n 33294
YXBwZW5k 33295
QXR0YWNo 33296
IGxldmVsZWQ= 33297
IGNvbmRvbXM= 33298
aWhpbGF0aW9u 33299
NjQ5 33300
IG5pZ2h0bWFyZXM= 33301
IFRIRVk= 33302
IFNUQVJU 33303
IEtpbm4= 33304
IHJvb21tYXRl 33305
IGh5Z2llbmU= 33306
b3BwaW5n 33307
Sm9i 33308
IGx2bA== 33309
IFZFUg== 33310
IEtlZXBpbmc= 33311
YWJldGlj 33312
IGZvcm1hdHRpbmc= 33313
ZXJhbGE= 33314
IHJldmlzaW9ucw== 33315
IHJlc3VyZw== 33316
VGVs 33317
IEdvb2RtYW4= 33318
MzUz 33319
cG9k 33320
IGluZGlzcA== 33321
IFRyYW5zbGF0aW9u 33322
IGdvd24= 33323
IE11bmQ= 33324
IGNpcw== 33325
IGJ5c3RhbmQ= 33326
Y29sbGVjdA== 33327
IFB1bmphYg== 33328
YWN0aXZlbHk= 33329
IEdhbWI= 33330
dGVsbA== 33331
IGltcG9ydGluZw== 33332
Z2VuY2llcw== 33333
IGxvY29t 33334
IEJyaWxs 33335
SG9seQ== 33336
IEJlcmdlcg== 33337
IHNob3dkb3du 33338
IHJlc3BvbmRlcnM= 33339
SUxZ 33340
IHRha2Vkb3du 33341
bGV0ZWQ= 33342
IG1hdHRlcmVk 33343
IHByZWRpY3RpdmU= 33344
IG92ZXJsYXk= 33345
R1BV 33346
IFZpY2s= 33347
IGNvbnZleWVk 33348
VGFi 33349
cGVlcg== 33350
U2Nhbg== 33351
IGRlZmVuc2l2ZWx5 33352
dmFl 33353
IGFwcHJvdmluZw== 33354
IHRpZXJz 33355
IFZpYQ== 33356
cXVlcmFkZQ== 33357
IFNhdWRpcw== 33358
IGRlbW9saXNoZWQ= 33359
IFByb3BoZQ== 33360
IG1vbm8= 33361
IGhvc3BpdGFsaXR5 33362
SEFN 33363
IEFyaWVs 33364
TU9E 33365
IFRvcmFo 33366
IGJsYWg= 33367
IEJlbGFydXM= 33368
ZXJlbnRpYWw= 33369
IFR1Yw== 33370
IGJhbmtlcg== 33371
Mzk3 33372
IG1vc3F1aXQ= 33373
IFNjaWVudGlzdA== 33374
IE11c2ljYWw= 33375
IGh1c3Q= 33376
U2hpZnQ= 33377
IHRvcm1lbnQ= 33378
IHN0YW5kb2Zm 33379
RWR1Yw== 33380
IEZvZw== 33381
IGFtcGxpZmllcg== 33382
U2hhcGU= 33383
SW5zdGFuY2U= 33384
IENyaXRpY3M= 33385
IGRhZW1vbg== 33386
SG91c3Rvbg== 33387
IG1hdHRyZXNz 33388
IElERg== 33389
IG9ic2NlbmU= 33390
IEFtZXI= 33391
aGV0dGk= 33392
IGNvbXBpbGluZw== 33393
MzUy 33394
dmVyZXR0 33395
IFJlZHVjdGlvbg== 33396
aXN0cmF0aW9u 33397
IEJsZXNzZWQ= 33398
IEJhY2hlbG9y 33399
MzE2 33400
IHByYW5r 33401
IFZ1bGNhbg== 33402
ZGRpbmc= 33403
IG1vdXJuaW5n 33404
IFF1aW50 33405
IEJsYXN0ZXI= 33406
dGVzdGluZw== 33407
IHNlZGltZW50 33408
Pj4+ 33409
IEV0ZXJuaXR5 33410
IFdIRVJF 33411
IE1hemU= 33412
IHJlYWN0aW5n 33413
IEFsdg== 33414
b21zZGF5 33415
IENSQQ== 33416
IHRyYW5zbGF0b3I= 33417
IGJvZ3Vz 33418
YXR1 33419
V2Vic2l0ZQ== 33420
b2xscw== 33421
IGJhcHRpc20= 33422
IHNpYmxpbmc= 33423
IEF1dHVtbg== 33424
dmV6 33425
44Gu6Q== 33426
Z3VhcmRz 33427
R2Vvcmc= 33428
YXNzYWRvcnM= 33429
IEZyZXVk 33430
IGNvbnRpbmVudHM= 33431
IFJlZ2lzdHJ5 33432
QmVybmll 33433
lprlo6s= 33434
IHRvbGVyYW50 33435
IFVX 33436
IGhvcnJpYmx5 33437
OTk1 33438
IE1JREk= 33439
IGltcGF0aWVudA== 33440
b2NhZG8= 33441
ZXJp 33442
IFdvcnN0 33443
IE5vcnJpcw== 33444
IFRhbGtpbmc= 33445
IGRlZmVuZHM= 33446
ZW5zYWJsZQ== 33447
IDIwMjE= 33448
IGFuYXRvbXk= 33449
TGV3 33450
IGRyYXdlcg== 33451
IENhbmJlcnJh 33452
IHBhdHJpb3RpYw== 33453
6b6N5Zaa5aOr 33454
IEF2Zw== 33455
QVJN 33456
IHVuZGlzY2xvc2Vk 33457
IGZhcmV3ZWxs 33458
NDU5 33459
YmFibGU= 33460
IEFsbGlzb24= 33461
T0xPRw== 33462
IGNvbmNv 33463
dGlnaHQ= 33464
IEFDUEk= 33465
IE1pbmVz 33466
bGljaA== 33467
IOKUnA== 33468
cmVwcmVzZW50ZWQ= 33469
MjAwMDAw 33470
IGVudGh1c2lhc3Q= 33471
T1RT 33472
Ymls 33473
IEluZ3JlZGllbnRz 33474
IGludmVudG9y 33475
IE15U1FM 33476
wqDCoMKg 33477
IEFCT1VU 33478
d2l0aGlu 33479
IG1r 33480
QnVs 33481
IEZha2U= 33482
IGRyYWNvbmlhbg== 33483
V2E= 33484
aGVsbQ== 33485
IFRlcnJhbg== 33486
ZXJ2aWxsZQ== 33487
IGNvbW1vbnBsYWNl 33488
U0laRQ== 33489
ICI8 33490
cmVwbGFjZQ== 33491
b2dyYXBocw== 33492
IFNFTEVDVA== 33493
aW5jaWJsZQ== 33494
IE1vc3RseQ== 33495
IFNoZWZmaWVsZA== 33496
IElERQ== 33497
dWdnbGU= 33498
IGNpdGF0aW9ucw== 33499
aHVyc3Q= 33500
IFVuaXg= 33501
IHVubGVhc2g= 33502
IFBpcGVy 33503
IE5hbm8= 33504
IHN1Y2N1bWI= 33505
IHJlbHVjdGFuY2U= 33506
IDI1MDA= 33507
IE1lcmNoYW50 33508
IHdpcmV0 33509
IGNvbWJvcw== 33510
IEJpcnRoZGF5 33511
IGNoYXJjb2Fs 33512
IFVQUw== 33513
IEZhaXJmYXg= 33514
IGRyaXZld2F5 33515
IFRlaw== 33516
IFBpdGNo 33517
b3ZlcmU= 33518
IHRlY2huaWNpYW5z 33519
IEFjdHVhbA== 33520
ZmxhdGlvbg== 33521
IEZpc2NhbA== 33522
IEVtcHR5 33523
YW5hbW8= 33524
IG1hZ25lc2l1bQ== 33525
IHNsdXQ= 33526
IGdyb3dlcnM= 33527
SW52ZXN0aWdhdG9ycw== 33528
KCk6 33529
IFNhdGVsbGl0ZQ== 33530
IEtleW5lcw== 33531
bWlzc2l2ZQ== 33532
bGFuZQ== 33533
IGJvcm91Z2g= 33534
MzQ0 33535
IFRFQU0= 33536
IEJldGhlc2Rh 33537
Q1Y= 33538
aG93ZXI= 33539
IFJBRA== 33540
IGNoYW50 33541
IFJpeQ== 33542
IGNvbXBvc2l0aW9ucw== 33543
IG1pbGRseQ== 33544
IG1lZGRsaW5n 33545
IGFnaWxpdHk= 33546
YW5lZXJz 33547
NTAx 33548
IHN5bnRo 33549
bGluZ2Vy 33550
Mjkx 33551
IGV4Y2xhaW1lZA== 33552
UGFydHk= 33553
IGNvbnRhbWlu 33554
IE1hbm9y 33555
IFJlc3BvbmQ= 33556
IHByYWlzaW5n 33557
IG1hbm5lcnM= 33558
ZmxlZXQ= 33559
U3VtbWVy 33560
IEx5bmQ= 33561
IERlZmluaXRlbHk= 33562
Z3JpbQ== 33563
IGJvd2xpbmc= 33564
c3RyaQ== 33565
55s= 33566
eW50 33567
IG1hbmRhdGVz 33568
RElW 33569
IHJlY29uY2lsZQ== 33570
dmlld3M= 33571
IERhbW9u 33572
dmV0dGU= 33573
Rmxv 33574
IEdyZWF0ZXN0 33575
aWxvbg== 33576
aWNpYQ== 33577
IHBvcnRyYXlhbA== 33578
IGN1c2hpb24= 33579
NTA0 33580
MTk3OQ== 33581
b3NzYWw= 33582
QXBwbGlj 33583
c2NyaXB0aW9u 33584
IG1pdGlnYXRpb24= 33585
QVRT 33586
cGFj 33587
IGVyYXNlZA== 33588
IGRlZmljaWVuY2llcw== 33589
IEhvbGxhbmRl 33590
IFh1 33591
IGJyZWQ= 33592
IHByZWduYW5jaWVz 33593
ZmVtaW4= 33594
IGVtcGg= 33595
IHBsYW5uZXJz 33596
IG91dHBlcg== 33597
dXR0ZXJpbmc= 33598
IHBlcnBldHJhdG9y 33599
IG1vdHRv 33600
IEVsbGlzb24= 33601
IE5FVkVS 33602
IGFkbWl0dGVkbHk= 33603
QVJJ 33604
IEF6ZXJiYWlqYW4= 33605
IG1pbGxpc2Vj 33606
IGNvbWJ1c3Rpb24= 33607
IEJvdHRsZQ== 33608
IEx1bmQ= 33609
IFBz 33610
IERyZXNz 33611
IGZhYnJpY2F0ZWQ= 33612
IGJhdHRlcmVk 33613
IHNpZGVs 33614
IE5vdHRpbmc= 33615
Rm9yZWlnbg== 33616
IEplcm9tZQ== 33617
MDIw 33618
IEFyYml0 33619
IGtub3Rz 33620
IFJJR0hU 33621
TW92aW5n 33622
44GZ 33623
IHN1cmdlcmllcw== 33624
IGNvdXJ0aG91c2U= 33625
IG1hc3RlcmVk 33626
IGhvdmVyaW5n 33627
IEJyYW4= 33628
IEFsaXNvbg== 33629
IHNhZmVzdA== 33630
bWlsaXRhcnk= 33631
IGJ1bGxpZWQ= 33632
IGJhcnJhZ2U= 33633
UmVhZGVy 33634
RVNF 33635
IEdlb2dyYXBoaWM= 33636
VG9vbHM= 33637
MzE0 33638
IEdlZWs= 33639
cm90aA== 33640
Z2xlcnM= 33641
IEZJTg== 33642
z4E= 33643
IEFzdG9u 33644
YWx0ZXJu 33645
NDg4 33646
IHZldGVyaW4= 33647
R2FtZXI= 33648
IGludGVs 33649
cmVuY2hlcw== 33650
U2hpZWxk 33651
IGFtbmVzdHk= 33652
IEJoYXI= 33653
IHBpbGVk 33654
IGhvbm9yYWJsZQ== 33655
IEluc3RpdHV0ZXM= 33656
IHNvYWtlZA== 33657
IGNvbWE= 33658
IEVGRg== 33659
MzQx 33660
Ynl0ZXM= 33661
IEdtYWls 33662
bGVpbg== 33663
IENhbmFkaWVucw== 33664
bWF0ZXJpYWw= 33665
SWw= 33666
IGluc3RydWN0b3Jz 33667
IEtZ 33668
IGNvbmNlaXZl 33669
dWJi 33670
IFBvc3NpYmxl 33671
IGVhc2luZw== 33672
IENocmlzdGluYQ== 33673
IGNhcmlj 33674
IEhEUg== 33675
Uk9N 33676
IHNob3ZlbA== 33677
ZGVsZXRl 33678
IHB1ZmY= 33679
IENoYW5naW5n 33680
IHNlYW1sZXNzbHk= 33681
QXR0cmlidXRl 33682
IGFjcXVpc2l0aW9ucw== 33683
YWtlcnk= 33684
IEVG 33685
IGF1dGlzdGlj 33686
IFRha2Vz 33687
IFBvd2Rlcg== 33688
IFN0aXI= 33689
NTEw 33690
IEJ1YmJsZQ== 33691
c2V0dGluZ3M= 33692
IEZvd2xlcg== 33693
IG11c3RhcmQ= 33694
IG1vcmVvdmVy 33695
IGNvcHlyaWdodGVk 33696
IExFRHM= 33697
MTUwMA== 33698
5ok= 33699
IEhJUw== 33700
ZW5m 33701
IGN1c3RvZA== 33702
IEh1Y2s= 33703
R2k= 33704
IGltZw== 33705
QW5zd2Vy 33706
Q3Q= 33707
amF5 33708
IEluZnJhc3RydWN0dXJl 33709
IGZlZGVyYWxseQ== 33710
TG9j 33711
IG1pY3JvYmVz 33712
IG92ZXJydW4= 33713
ZGRz 33714
b3RlbnQ= 33715
YWRpYXRvcg== 33716
Pj4+Pj4+Pj4= 33717
IHRvcm5hZG8= 33718
IGFkanVk 33719
IGludHJpZ3VlZA== 33720
IHNp 33721
IFJldmVsYXRpb24= 33722
cHJvZ3Jlc3M= 33723
IGJ1cmdsYXJ5 33724
IFNhaXlhbg== 33725
IEthdGh5 33726
IHNlcnBlbnQ= 33727
IEFuZHJlYXM= 33728
IGNvbXBlbA== 33729
ZXNzbGVy 33730
IFBsYXN0aWM= 33731
IEFkdmVudA== 33732
IFBvc2l0aXZl 33733
IFF0 33734
IEhpbmR1cw== 33735
cmVnaXN0ZXJlZA== 33736
dWxhcml0eQ== 33737
IHJpZ2h0ZW91c25lc3M= 33738
IGRlbW9uaWM= 33739
dWl0aXZl 33740
IEJEUw== 33741
IEdyZWdn 33742
Y2lh 33743
IENydXNhZGU= 33744
IFNpbmFp 33745
V0FSRQ== 33746
Kyg= 33747
IG1lbGw= 33748
IGRlcmFpbA== 33749
eWFyZHM= 33750
QXN0 33751
IG5vdGljZWFibHk= 33752
IE9iZXI= 33753
UmFt 33754
IHVubm90aWNlZA== 33755
IHNlcQ== 33756
YXZhZ2U= 33757
VHM= 33758
IDY0MA== 33759
IGNvbmNlZGU= 33760
IF0p 33761
RmlsbA== 33762
IGNhcHRpdml0eQ== 33763
IEltcHJvdmVtZW50 33764
IENydXNhZGVy 33765
YXJhb2g= 33766
TUFQ 33767
5pc= 33768
IHN0cmlkZQ== 33769
YWx3YXlz 33770
Rmx5 33771
Tml0 33772
IGFsZ2Fl 33773
IENvb2tpbmc= 33774
IERvb3Jz 33775
TWFsbGV5 33776
IHBvbGljZW1lbg== 33777
44GN 33778
IGFzdHJvbmF1dA== 33779
YWNjZXNzaWJsZQ== 33780
NDk1 33781
IFJBVw== 33782
Y2xpZmZl 33783
dWRpY3JvdXM= 33784
IGRlcGVuZGVk 33785
YWxhY2g= 33786
IHZlbnR1cmVz 33787
cmFrZQ== 33788
IHRpdHM= 33789
IEhvdQ== 33790
IGNvbmRvbQ== 33791
b3Jtb25hbA== 33792
IGluZGVudA== 33793
IHVwbG9hZGluZw== 33794
Rm9vdG5vdGU= 33795
SW1wb3J0YW50 33796
IDI3MQ== 33797
IG1pbmRmdWw= 33798
IGNvbnRlbmRz 33799
Q3Jh 33800
IGNhbGlicg== 33801
IE9FQ0Q= 33802
cGx1Z2lu 33803
RmF0 33804
IElTUw== 33805
IER5bmFtaWNz 33806
YW5zZW4= 33807
Njg2 33808
Jyks 33809
IHNwcml0ZQ== 33810
IGhhbmRoZWxk 33811
IEhpcHA= 33812
PX49fg== 33813
VHJ1c3Q= 33814
IHNlbWFudGljcw== 33815
IEJ1bmRlcw== 33816
IFJlbm8= 33817
IExpdGVyYXR1cmU= 33818
c2Vuc2U= 33819
R2FyeQ== 33820
IEFlZw== 33821
IFRyaW4= 33822
RUVL 33823
IGNsZXJpYw== 33824
IFNTSA== 33825
IGNocmlzdA== 33826
IGludmFkaW5n 33827
aWJ1 33828
IGVudW0= 33829
YXVyYQ== 33830
IGFsbGVnZQ== 33831
IEluY3JlZGlibGU= 33832
QkJD 33833
IHRocnU= 33834
IHNhaWxlZA== 33835
IGVtdWxhdGU= 33836
IGluc2VjdXJpdHk= 33837
IGNyb3U= 33838
IGFjY29tbW9kYXRpb25z 33839
IGluY29tcGV0ZW50 33840
IHNsaXBz 33841
IEVhcnRocXU= 33842
c2FtYQ== 33843
SUxMRQ== 33844
IGlQaG9uZXM= 33845
YXNha2k= 33846
IGJ5ZQ== 33847
IGFyZA== 33848
IGV4dHJhcw== 33849
IHNsYXVnaHRlcmVk 33850
IGNyb3dkZnVuZGluZw== 33851
cmVzc28= 33852
IGZpbGli 33853
IEVSUk9S 33854
IFRMUw== 33855
ZWdn 33856
IEl0YWw= 33857
IGVubGlzdA== 33858
IENhdGFsb25pYQ== 33859
IFNjb3Rz 33860
IHNlcmdlYW50 33861
IGRpc3NvbHZl 33862
Tkg= 33863
IHN0YW5kaW5ncw== 33864
cmlxdWU= 33865
SVE= 33866
IGJlbmVmaWNpYXJ5 33867
IGFxdWFyaXVt 33868
WW91VHViZQ== 33869
IFBvd2VyU2hlbGw= 33870
IGJyaWdodGVzdA== 33871
IFdhcnJhbnQ= 33872
U29sZA== 33873
V3JpdGluZw== 33874
IGJlZ2lubmluZ3M= 33875
IFJlc2VydmVk 33876
IExhdGlub3M= 33877
aGVhZGluZw== 33878
IDQ0MA== 33879
IHJvb2Z0b3A= 33880
QVRJTkc= 33881
IDM5MA== 33882
VlBO 33883
R3M= 33884
a2VybmVs 33885
dHVybmVk 33886
IHByZWZlcmFibGU= 33887
IHR1cm5vdmVycw== 33888
IEhlbHM= 33889
U2E= 33890
IFNoaW5qaQ== 33891
dmVo 33892
IE1PRFVMRQ== 33893
VmlvbA== 33894
IGV4aXRpbmc= 33895
IGphYg== 33896
IFZhbmlsbGE= 33897
IGFjcm9u 33898
IEdhcA== 33899
YmVybg== 33900
QWs= 33901
IE1jR3U= 33902
IGVuZGxlc3NseQ== 33903
IEZhcmFnZQ== 33904
IE5vZWw= 33905
VmE= 33906
TUs= 33907
IGJydXRl 33908
IEtydQ== 33909
IEVTVg== 33910
IE9saXZpYQ== 33911
4oCg 33912
IEthZg== 33913
IHRydXN0aW5n 33914
IGhvdHM= 33915
MzI0 33916
IG1hbGFyaWE= 33917
IGpzb24= 33918
IHBvdW5kaW5n 33919
b3J0bWVudA== 33920
Q291bnRyeQ== 33921
IHBvc3Rwb25lZA== 33922
IHVuZXF1aXY= 33923
Pyks 33924
IFJvb25leQ== 33925
dWRkaW5n 33926
IExlYXA= 33927
dXJyZW5jZQ== 33928
c2hhcGVzaGlmdGVy 33929
IEhBUw== 33930
b3NhdGU= 33931
IGNhdmVybg== 33932
IGNvbnNlcnZhdGlzbQ== 33933
IEJBRA== 33934
IG1pbGVhZ2U= 33935
IGFycmVzdGluZw== 33936
VmF1bHRz 33937
IG1peGVy 33938
RGVtb2NyYXRpYw== 33939
IEJlbnNvbg== 33940
IGF1dGhvcmVk 33941
ODAwMA== 33942
IHByb2FjdGl2ZQ== 33943
IFNwaXJpdHVhbA== 33944
dHJl 33945
IGluY2FyY2VyYXRlZA== 33946
IFNvcnQ= 33947
IHBlYWtlZA== 33948
IHdpZWxkaW5n 33949
cmVjaWF0aW9u 33950
15nX 33951
UGF0Y2g= 33952
IEVtbXk= 33953
IGV4cXU= 33954
dHRv 33955
IFJhdGlv 33956
IFBpY2tz 33957
IEdyeQ== 33958
cGhhbnQ= 33959
IGZyZXQ= 33960
IGV0aG4= 33961
IGFyY2hpdmVk 33962
JS0= 33963
Y2FzZXM= 33964
IEJsYXpl 33965
IGltYg== 33966
Y3Y= 33967
eXNz 33968
aW1vbnk= 33969
IGNvdW50ZG93bg== 33970
IGF3YWtlbmluZw== 33971
IFR1bmlzaWE= 33972
IFJlZmVy 33973
IE1K 33974
IHVubmF0dXJhbA== 33975
IENhcm5lZ2ll 33976
aXplbg== 33977
IE51Z2dldHM= 33978
aGVzcw== 33979
IGV2aWxz 33980
NjQ3 33981
IGludHJvZHVjdG9yeQ== 33982
bG92aW5n 33983
IE1jTWFob24= 33984
IGFtYmlndWl0eQ== 33985
TGFiZWw= 33986
IEFsbWlnaHR5 33987
IGNvbG9yaW5n 33988
IENsYXVz 33989
c2V0dGluZw== 33990
TlVMTA== 33991
IEZhdm9yaXRl 33992
IFNJRw== 33993
Pig= 33994
IFNoaXZh 33995
IE1heWVy 33996
IHN0b3JtZWQ= 33997
IENvdmVyYWdl 33998
d2VhcG9ucw== 33999
aWdoYW0= 34000
IHVuYW5zd2VyZWQ= 34001
IGxldmU= 34002
IGNveQ== 34003
Y2Fz 34004
YmFncw== 34005
YXN1cmVk 34006
U2VhdHRsZQ== 34007
IFNhbnRvcnVt 34008
c2VyaW91cw== 34009
IGNvdXJhZ2VvdXM= 34010
IFNvdXA= 34011
IGNvbmZpc2NhdGVk 34012
IC8vLw== 34013
IHVuY29udmVudGlvbmFs 34014
IG1vbXM= 34015
IFJvaGluZ3lh 34016
IE9yY2hlc3RyYQ== 34017
IFBvdGlvbg== 34018
IGRpc2NyZWRpdA== 34019
IEZJTA== 34020
Zml4ZWQ= 34021
IERlZXI= 34022
ZG9p 34023
IERpbWVuc2lvbg== 34024
IGJ1cmVhdWNyYXRz 34025
ZXRlZW4= 34026
IGFjdGlvbkdyb3Vw 34027
b2ht 34028
IGJ1bXBz 34029
IFV0aWxpdHk= 34030
IHN1Ym1hcmluZXM= 34031
cmVuaGVpdA== 34032
cmVzZWFyY2g= 34033
IFNoYXBpcm8= 34034
IHNrZXRjaGVz 34035
IGRlY2VwdGl2ZQ== 34036
IFZpbA== 34037
ZXNhbWU= 34038
IEVzc2VudGlhbGx5 34039
IHJhbXBhZ2U= 34040
aXNreQ== 34041
IG11dHRlcmVk 34042
dGhyaXRpcw== 34043
IDIzNg== 34044
ZmV0 34045
YmFycw== 34046
IHB1cGls 34047
IFRob3U= 34048
b1M= 34049
c29uZw== 34050
IGZyYWN0dXJlZA== 34051
IHJldmVydA== 34052
cGljdHVyZQ== 34053
IGNyaXRlcmlvbg== 34054
dXNoZXI= 34055
IHJlcGVyY3Vzc2lvbnM= 34056
IFZpbnRhZ2U= 34057
IFN1cGVyaW50ZW5kZW50 34058
T2ZmaWNlcnM= 34059
IGZsYWdnZWQ= 34060
IGJsYW1lcw== 34061
IGludmVyc2U= 34062
b2dyYXBoZXJz 34063
IG1ha2VzaGlmdA== 34064
IGRldm9pZA== 34065
IGZvc3NpbHM= 34066
IEFyaXN0b3RsZQ== 34067
IEZ1bmRz 34068
IGRlcGxldGVk 34069
IEZsdQ== 34070
IFl1YW4= 34071
IHdvZXM= 34072
IGxpcGlk 34073
IHNpdHU= 34074
cmVxdWlzaXRlcw== 34075
IGZ1cm5pc2g= 34076
IFNhbWFy 34077
IHNoYW1lZnVs 34078
IGFkdmVyc2VseQ== 34079
IGFkZXB0 34080
IHJlbW9yc2U= 34081
IG11cmRlcm91cw== 34082
dWNrbGVz 34083
IEVTTA== 34084
IDMxNA== 34085
c2VudA== 34086
IHJlZGVm 34087
IENhY2hl 34088
IFB1cnM= 34089
aWdhbnM= 34090
IDQ2MA== 34091
IHByZXNjcmlwdGlvbnM= 34092
IGZyZXM= 34093
RnVjaw== 34094
b2NyYXRlcw== 34095
VHdlbnR5 34096
IFdlaXJk 34097
IFRvZ2dsZQ== 34098
IENhbGxlZA== 34099
aXRpemVucw== 34100
IHBvdWx0cnk= 34101
IGhhcnZlc3Rpbmc= 34102
44Km44K5 34103
Qm90dG9t 34104
IGNhdXRpb25lZA== 34105
dG4= 34106
Mzk2 34107
IE5pa2tp 34108
IGV2YWx1YXRpb25z 34109
IGhhcmFzc2luZw== 34110
IGJpbmRpbmdz 34111
IE1vbmV0YXJ5 34112
IGhpdHRlcnM= 34113
IGFkdmVyc2FyeQ== 34114
dW50cw== 34115
IHNldGJhY2s= 34116
IGVuY3J5cHQ= 34117
IENhaXQ= 34118
IGxvd3M= 34119
ZW5nZXM= 34120
IE5vcm4= 34121
IGJ1bGJz 34122
IGJvdHRsZWQ= 34123
IFZveWFnZXI= 34124
MzE3 34125
IHNwaGVyZXM= 34126
cG9saXRpY3M= 34127
IHN1YnRyYWN0 34128
IHNlbnNhdGlvbnM= 34129
IGFwcGFsbGluZw== 34130
IDMxNg== 34131
IGVudmlyb25tZW50YWxseQ== 34132
IFNURU0= 34133
IHB1Ymxpc2hlcw== 34134
NTYw 34135
IGRpbGlnZW5jZQ== 34136
NDg0 34137
IGFkdmlzZXM= 34138
IHBldHJvbA== 34139
IGltYWdpbmluZw== 34140
IHBhdHJvbHM= 34141
IEludGVnZXI= 34142
IEFzaGVz 34143
YWN0dXM= 34144
IFJhZGlhbnQ= 34145
IExU 34146
aXRhYmlsaXR5 34147
aHRha2luZw== 34148
U2V0dGluZw== 34149
IG51YW5jZWQ= 34150
IFJlZWY= 34151
IERldmVsb3BlcnM= 34152
Tmk= 34153
cGllY2Vz 34154
OTkw 34155
TGljZW5zZQ== 34156
IGxvd2Vycw== 34157
IE90dG9tYW4= 34158
MzI3 34159
b29v 34160
IHF1aXR0aW5n 34161
bWFya2V0cw== 34162
QmVoaW5k 34163
IGJhc2lu 34164
IGRvY3M= 34165
YW5pZQ== 34166
Zmxhc2g= 34167
Y3Rs 34168
IGNpdmlsaXplZA== 34169
IEZ1a3VzaGltYQ== 34170
Il0sIg== 34171
IEtT 34172
IEhvbmVzdGx5 34173
YXJhdA== 34174
IGNvbnN0cnVjdHM= 34175
IExhbnM= 34176
IERpcmU= 34177
IExJS0U= 34178
IFRyb3VibGU= 34179
IHdpdGhob2xkaW5n 34180
IE9ibGl2aW9u 34181
IHNhbml0eQ== 34182
YW55YQ== 34183
Q29uc3Q= 34184
IGdyb2Nlcg== 34185
IENlbHNpdXM= 34186
IHJlY291bnRlZA== 34187
IFdpZmU= 34188
Qm9yZGVy 34189
YXRlcmVk 34190
aGFwcHk= 34191
IHNwb2lsZXI= 34192
IGxvZ2ljYWxseQ== 34193
SGFsbA== 34194
IHN1Y2NlZWRpbmc= 34195
IHBvbHltb3JwaA== 34196
IGF4ZXM= 34197
IFNob3RndW4= 34198
IFNsaW0= 34199
IFByaW5jaXBsZXM= 34200
IExldGg= 34201
YXJ0YQ== 34202
IHNjb3I= 34203
U2NyZWVuc2hvdA== 34204
IHJlbGF4YXRpb24= 34205
IyQjJA== 34206
IGRldGVycmVudA== 34207
aWRkeQ== 34208
IHBvd2VybGVzcw== 34209
IGxlc2JpYW5z 34210
IGNob3Jkcw== 34211
IEVkaXRlZA== 34212
c2VsZWN0ZWQ= 34213
IHNlcGFyYXRpc3Rz 34214
MDAwMg== 34215
IGFpcnNwYWNl 34216
IHR1cm5hcm91bmQ= 34217
IGN1bm5pbmc= 34218
UEFUSA== 34219
UG9seQ== 34220
IGJvbWJlZA== 34221
IHRpb24= 34222
eHM= 34223
IHdpdGhob2xk 34224
IHdhZ2Vk 34225
IExpYmVydGllcw== 34226
RmxhZw== 34227
IGNvbWZvcnRpbmc= 34228
NDU0 34229
IElyaXM= 34230
YXJlcnM= 34231
IHJhZw== 34232
IHJlbG9jYXRlZA== 34233
IEd1YXJhbnQ= 34234
IHN0cmF0ZWdpY2FsbHk= 34235
IGdhbW1h 34236
dWJlcnR5 34237
IExvY2toZWVk 34238
Z3Jlcw== 34239
IGdyaWxsZWQ= 34240
IExvd2U= 34241
c3RhdHM= 34242
IFJvY2tz 34243
IHNlbnNpbmc= 34244
IHJlbnRpbmc= 34245
IEdlb2xvZ2ljYWw= 34246
2KfY 34247
b3Ryb3A= 34248
IHNldw== 34249
IGltcHJvcGVybHk= 34250
NDg2 34251
IOKWoA== 34252
IHN0YXJ2aW5n 34253
IEJq 34254
RGlzY3Vzc2lvbg== 34255
MzI4 34256
IENvbWJv 34257
IEZpeGVz 34258
TkFU 34259
IHN0cml2aW5n 34260
dGhvcmE= 34261
IGhhcnZlc3RlZA== 34262
IFBpbmc= 34263
IHBsYXlmdWw= 34264
IGF2ZW51ZXM= 34265
IG9jY3VwYXRpb25hbA== 34266
IHdha2Vz 34267
IENvdXJpZXI= 34268
IGRydW1tZXI= 34269
IEJyb3dzZXI= 34270
IEhvdXRo 34271
aXR1 34272
IGFwcGFyZWw= 34273
cGFzdGU= 34274
IGh1bnRlZA== 34275
IFNlY29uZGx5 34276
bGFpbg== 34277
WFk= 34278
IFBJTg== 34279
aWNvbnM= 34280
IGNvY2t0YWlscw== 34281
IHNpemFibGU= 34282
IGh1cmRsZXM= 34283
ZXN0aW5hbA== 34284
IFJlY3JlYXRpb24= 34285
IGVjbw== 34286
NjQ4 34287
IERpZWQ= 34288
bWludA== 34289
IGZpbmdlcnByaW50cw== 34290
IGRpc3Bvc2U= 34291
IEJvc25pYQ== 34292
dHN5 34293
MjIwMA== 34294
IGluc3BlY3RlZA== 34295
IEZvdQ== 34296
IGZ1c3M= 34297
IGFtYnVzaA== 34298
IFJhaw== 34299
IG1hbmlmZXN0ZWQ= 34300
UHJvc2VjdXQ= 34301
IHN1ZmZpY2U= 34302
cmVuY2Vz 34303
IGNvbXBlbnNhdGVk 34304
IEN5cnVz 34305
IGdlbnVz 34306
IFdvbHZlcmluZQ== 34307
IFRyZW5kcw== 34308
IGhpa2Vz 34309
IFNlZW4= 34310
IGVucm9s 34311
Q29sZA== 34312
IHBvbGl0ZWx5 34313
IFNsYXY= 34314
IFJ1cGVydA== 34315
IGV5ZXdpdG5lc3M= 34316
IEFsdG8= 34317
IHVuY29tcA== 34318
IHBvc3Rlcmlvcg== 34319
TXVzdA== 34320
IEhlcno= 34321
IHByb2dyZXNzaXZlbHk= 34322
IDIzNA== 34323
IGluZGlmZmVyZW5jZQ== 34324
IEN1bm5pbmdoYW0= 34325
IGFjYWRlbWlh 34326
IHNld2Vy 34327
IGFzdG91bmRpbmc= 34328
IEFFUw== 34329
cmF0aGVy 34330
IGVsZGVzdA== 34331
IGNsaW1icw== 34332
IEFkZHM= 34333
IG91dGNyeQ== 34334
IGNvbnRhZw== 34335
IEhvdXNlcw== 34336
IHBlcHQ= 34337
IE1lbGFuaWE= 34338
aW50ZXJlc3RlZA== 34339
IFVDSA== 34340
IFJvb3Rz 34341
IEh1YmJhcmQ= 34342
IFRCRA== 34343
IFJvbWFuaWFu 34344
ZmlsZW5hbWU= 34345
U3RvbmU= 34346
IEltcGw= 34347
IGNocm9tb3NvbWU= 34348
Q2xl 34349
ZHg= 34350
IHNjcmFtYmxlZA== 34351
IFB0 34352
IDI0Mg== 34353
T1BMRQ== 34354
IHRyZW1lbmRvdXNseQ== 34355
U3RyZWV0 34356
IGNyYXZpbmc= 34357
IGJ1bmRsZWQ= 34358
IFJH 34359
cGlwZQ== 34360
IGluanVyaW5n 34361
IGFyY2FuZQ== 34362
UGFydGljaXA= 34363
IEhlcm9pYw== 34364
c3R5 34365
IHRvcHBpbmc= 34366
IFRlbXBlc3Q= 34367
cmVudGljZXM= 34368
Ymg= 34369
IHBhcmFub2lh 34370
IFVuaWNvZGU= 34371
IGVncmVnaW91cw== 34372
IFwn 34373
IE9zd2FsZA== 34374
IGdyYXZlbA== 34375
IFNpbXBzb25z 34376
IGJsYW5k 34377
IEd1YW50YW5hbW8= 34378
V3JpdGVy 34379
bGluZXJz 34380
IERpY2U= 34381
SkM= 34382
IHBhcml0eQ== 34383
IHNpZGVk 34384
IDIzNw== 34385
IFB5cnJoYQ== 34386
YXR0ZXJz 34387
ZGs= 34388
RmluZQ== 34389
Y29tcGFu 34390
IGZvcm11bGF0ZWQ= 34391
IElkb2w= 34392
aWxlcnM= 34393
aGVtb3Ro 34394
IEZhdg== 34395
IGludHJ1c2lvbg== 34396
IGNhcnJvdHM= 34397
IExheWVy 34398
IEhhY2tlcg== 34399
IC0tLS0tLS0tLS0tLS0tLS0= 34400
IG1vZGVyYXRpb24= 34401
6YE= 34402
b2NvYw== 34403
IGNoYXJhY3Rlcml6ZQ== 34404
IFRlcmVzYQ== 34405
IHNvY2lvZWNvbm9taWM= 34406
IHBlcms= 34407
IFBhcnRpY2lwYXRpb24= 34408
dHJhaW5pbmc= 34409
IFBhdWxv 34410
cGh5cw== 34411
IHRydXN0d29ydGh5 34412
IGVtYm9kaWVk 34413
IE1lcmNo 34414
Y3VycmVuY3k= 34415
IFByaW9yaXR5 34416
IHRlYXNpbmc= 34417
IGFic29yYmluZw== 34418
IHVuZmluaXNoZWQ= 34419
IENvbXBhcmlzb24= 34420
IGRpc3BsZQ== 34421
d3JpdGVycw== 34422
IHByb2Zlc3Npb25z 34423
IFBlbmd1aW4= 34424
IGFuZ3JpbHk= 34425
IExJTks= 34426
Njg4 34427
IENvcnJlc3BvbmQ= 34428
IHByZXZhaWxlZA== 34429
IGNhcnRlbA== 34430
bHA= 34431
YXNtcw== 34432
IFJlZGVtcHRpb24= 34433
IElzbGFtaXN0cw== 34434
ZWZmZWN0cw== 34435
ZG9zZQ== 34436
IExhdHRlcg== 34437
IEhhbGlmYXg= 34438
IHZhcw== 34439
IFRvcGljcw== 34440
IE5hbWVk 34441
YWR2ZXJ0aXNpbmc= 34442
enph 34443
SUNFUw== 34444
IHJldGFyZGVk 34445
YWNoYWJsZQ== 34446
IFB1cHBldA== 34447
IEl0ZW1MZXZlbA== 34448
IHJldHJhY3Q= 34449
IGlkZW50aWZpYWJsZQ== 34450
QWFyb24= 34451
IEJ1c3Rlcg== 34452
c29s 34453
aGVsbGU= 34454
YXNzZW1i 34455
SG9wZQ== 34456
cmFuZ2Vk 34457
QmE= 34458
IFB1cmNo 34459
6YA= 34460
IFNpcmk= 34461
IGFycml2YWxz 34462
IDE5MTI= 34463
IHNob3J0ZW5lZA== 34464
IDMxMg== 34465
IGRpc2NyZXBhbmN5 34466
IFRlbXBlcmF0dXJl 34467
IFdhbHRvbg== 34468
IGtpbmRlcmc= 34469
cG9saXQ= 34470
IHJlbWl4 34471
IGNvbm5lY3RvcnM= 34472
44OY44Op 34473
IEthemFraHN0YW4= 34474
ZG9taW5hdGVk 34475
IHN1Z2Fycw== 34476
aW1ibGU= 34477
IFBhbmlj 34478
IERlbWFuZA== 34479
IENvbG9ueQ== 34480
b25lbg== 34481
IE1FUg== 34482
Nzc1 34483
dXJpYQ== 34484
YXphYXI= 34485
IERlZ3JlZQ== 34486
UHJp 34487
IHN1bnNoaW5l 34488
IDI1MQ== 34489
IHBzeWNoZWRlbGlj 34490
IGRpZ2l0YWxseQ== 34491
IEJyYXVu 34492
IHNoaW1tZXI= 34493
IHNoYXZl 34494
IFRlbGVzYw== 34495
IEFzdHJhbA== 34496
IFZlbmV6dWVsYW4= 34497
IE9H 34498
IGNyYXdsaW5n 34499
SW50ZWc= 34500
IEZlYXRoZXI= 34501
IHVuZm9sZGluZw== 34502
IGFwcHJvcHJpYXRpb24= 34503
IOijj+g= 34504
IE1vYmlsaXR5 34505
IE5leQ== 34506
LS4= 34507
YmlsdA== 34508
TElO 34509
IFR1YmU= 34510
IENvbnZlcnNlbHk= 34511
IGtleWJvYXJkcw== 34512
IENhbw== 34513
IG92ZXJ0aA== 34514
IGxhdXJl 34515
Pj5c 34516
IFZpcGVy 34517
YWNoYQ== 34518
T2Zmc2V0 34519
IFJhbGVpZ2g= 34520
IEphZQ== 34521
Sm9yZGFu 34522
anA= 34523
IHRvdGFsaXRhcmlhbg== 34524
Q29ubmVjdG9y 34525
IG9ic2VydmVz 34526
IFNwYXJ0YW4= 34527
IEltbWVkaWF0ZWx5 34528
IFNjYWw= 34529
Q29vbA== 34530
IHRhcHM= 34531
IHJvYXI= 34532
UGFzdA== 34533
IGNoYXJz 34534
IEJlbmRlcg== 34535
IFNoZWxkb24= 34536
IHBhaW50ZXI= 34537
IGJlYWNvbg== 34538
IENyZWF0dXJlcw== 34539
IGRvd250dXJu 34540
IGhpbmRlcg== 34541
IEFuZHJvbWVkYQ== 34542
w5s= 34543
Y2NvbGk= 34544
IEZpdG5lc3M= 34545
ZXRyaWNhbA== 34546
IHV0aWxpemVz 34547
IHNlbmF0ZQ== 34548
IGVuc2VtYmxl 34549
IGNoZWVycw== 34550
VFc= 34551
IGFmZmx1ZW50 34552
a2ls 34553
cnlsaWM= 34554
b3JkZXJpbmc= 34555
Q29tcHV0ZXI= 34556
IGdydWVzb21l 34557
b3N0aWNz 34558
IFViaXNvZnQ= 34559
IEtlbGxleQ== 34560
IHdyZW5jaA== 34561
IGJvdXJnZW9pc2ll 34562
SUJMRQ== 34563
IFByZXN0b24= 34564
d29ybg== 34565
YXJpc3Q= 34566
cmVhdGluZw== 34567
IHN0YWluZWQ= 34568
YXJpbmU= 34569
IHNsaW1l 34570
RU5O 34571
IGNoZXN0cw== 34572
IGdyb3VuZHdhdGVy 34573
YW5ub3Q= 34574
IFRyYXk= 34575
IExvY2tl 34576
IENUUg== 34577
IGR1ZGVz 34578
IEV4dGVybmFs 34579
IERlY29kZXI= 34580
IHBhcmFtZWQ= 34581
IE1lZGxpbmU= 34582
ODA5 34583
IERpbm5lcg== 34584
cnVwYWw= 34585
Z3o= 34586
IEd1bQ== 34587
IERlbW8= 34588
amVl 34589
IGRo 34590
YmVybWFu 34591
YXJjaHM= 34592
IGVucXU= 34593
IEVwc3RlaW4= 34594
IGRldmFzdGF0aW9u 34595
IGZyaWVuZHNoaXBz 34596
IEFyZA== 34597
IDIzMQ== 34598
IFJ1Ymlu 34599
IERpc3RhbmNl 34600
IHNwdXJyZWQ= 34601
IGRvc3NpZXI= 34602
IG92ZXJsb29raW5n 34603
XFxcXFxcXFxcXFxcXFxcXA== 34604
Rm9yZXN0 34605
IENvbWVz 34606
XCIs 34607
IElyYW5pYW5z 34608
IGZpeHR1cmVz 34609
TGF1Z2hz 34610
IGN1cnJ5 34611
IEtpbmdzdG9u 34612
IHNxdWFzaA== 34613
IGNhdGFsb2d1ZQ== 34614
IGFibm9ybWFsaXRpZXM= 34615
IGRpZ2VzdGl2ZQ== 34616
Li4uLi4uLi4u 34617
IHN1Ym9yZGluYXRl 34618
b2dseQ== 34619
IDI0OQ== 34620
TWlkZGxl 34621
IG1hc3NhYw== 34622
IGJ1cmdlcnM= 34623
IGRvd25zdGFpcnM= 34624
IDE5MzE= 34625
Mzk0 34626
IFZH 34627
IGxhc2Vycw== 34628
IFNpa2g= 34629
IEFsZXhh 34630
ZGVyaXZlZA== 34631
IGN5Y2xpc3Q= 34632
44Gu6a2U 34633
b25lbGluZXNz 34634
ISEhISEhISE= 34635
IGJ1ZmZz 34636
bGVnYXRl 34637
IHJhcGluZw== 34638
IHJlY29tbWVuZGluZw== 34639
cm9yZWQ= 34640
IG11bHRpY3VsdHVyYWw= 34641
dW5pcXVl 34642
IGJ1c2luZXNzbWVu 34643
IHVuZWFzeQ== 34644
IE1BUA== 34645
IGRpc3BlcnNlZA== 34646
Y2lwbGluZQ== 34647
SmVzcw== 34648
IEtlcmFsYQ== 34649
5ac= 34650
IGFic3RyYWN0aW9u 34651
U3Vydg== 34652
VWg= 34653
IHByaW50ZXJz 34654
aWph 34655
b3dkZXI= 34656
IGFuYWxvZ291cw== 34657
IEFTUA== 34658
YWZlcg== 34659
IHVuZm9sZGVk 34660
IGxldmVsaW5n 34661
IGJyZWFjaGVk 34662
IEhlYXJpbmc= 34663
IG5hdA== 34664
IHRyYW5zbGF0aW5n 34665
Y3JpdGljYWw= 34666
IGFudGFnb25pc3Q= 34667
IFllc3RlcmRheQ== 34668
IGZ1enp5 34669
d2FzaA== 34670
bWVyZQ== 34671
IGJld2lsZA== 34672
IE1hZQ== 34673
VmlyZ2lu 34674
cGhyYXNl 34675
IHNpZ25hbGVk 34676
IEhJR0g= 34677
IHByb3Rlc3Rlcg== 34678
IGdhcm5lcg== 34679
dW5rbm93bg== 34680
IGtheQ== 34681
IGFiZHVjdGVk 34682
IHN0YWxraW5n 34683
YW1u 34684
IGRlc2VydmluZw== 34685
IFJpdg== 34686
IEpvcmdl 34687
IHNjcmF0Y2hpbmc= 34688
IFNhdmluZw== 34689
aXBpbmc= 34690
IHRlYXNl 34691
IG1pc3Npb25hcnk= 34692
IE1vcnJvdw== 34693
VElNRQ== 34694
UHJlc2VudA== 34695
IGNoZW1vdGhlcmFweQ== 34696
dGVybmVzcw== 34697
IEhvbWVz 34698
IFB1cmR1ZQ== 34699
IHN0YXVuY2g= 34700
IFdoaXRuZXk= 34701
IFRIRVJF 34702
zrw= 34703
aWF0dXM= 34704
IEVybmVzdA== 34705
IERlcGxveQ== 34706
IGNvdmV0ZWQ= 34707
Rk1M 34708
IERpYWxvZ3Vl 34709
IGV4aXRlZA== 34710
ZnJ1aXQ= 34711
IG5lcmQ= 34712
IjoiIiwi 34713
IHZpdm8= 34714
cnVseQ== 34715
NDYw 34716
IEFtZW4= 34717
cmVoZW5zaWJsZQ== 34718
IOKY 34719
RElS 34720
IGFkaGVyZW5jZQ== 34721
IGNoZXc= 34722
IENva2U= 34723
IFNlcmdlaQ== 34724
ZGlnaXRhbA== 34725
IE5lY2s= 34726
Z2VudGx5 34727
ZW50aGFs 34728
Lyk= 34729
IHdlYXJ5 34730
IGd1aXNl 34731
IENvbmNvcmQ= 34732
IE9uaW9u 34733
YXRjaGVy 34734
IGJpbmdl 34735
IERpcmVjdGl2ZQ== 34736
IG1hbm5lZA== 34737
YW5zaw== 34738
IGlsbHVzaW9ucw== 34739
IGJpbGxpb25haXJlcw== 34740
Mzgz 34741
b2x5bg== 34742
b2R5bmFtaWM= 34743
IFdoZWF0 34744
IEFsaWM= 34745
IGNvbG91cmVk 34746
IE5BRlRB 34747
YWJv 34748
IG1hY3Jvcw== 34749
aW5kZXBlbmRlbnQ= 34750
c3dlZXQ= 34751
IHNwYWM= 34752
IEthYnVs 34753
IMQ= 34754
ZW1l 34755
IGRpY3RhdGVk 34756
IHNob3V0cw== 34757
PXs= 34758
IHJpcHBpbmc= 34759
IFNoYXk= 34760
IENyaWNrZXQ= 34761
ZGlyZWN0ZWQ= 34762
IGFuYWx5c2Vk 34763
IFdBUlJBTlQ= 34764
YWdvbnM= 34765
IEJsYXplcnM= 34766
IGNoZWVyZWQ= 34767
IGFyaXRobWV0aWM= 34768
IFRhbno= 34769
Mzcz 34770
IEZsYWdz 34771
IDI5NQ== 34772
IHdpdGNoZXM= 34773
IEluY2x1ZGVk 34774
IEdhaW5lZA== 34775
IEJsYWRlcw== 34776
R2Ft 34777
IFNhbWFudGhh 34778
IEF0bGFudGlz 34779
IFByYXR0 34780
IHNwb2lsZWQ= 34781
IElC 34782
IFJhbWlyZXo= 34783
UHJvYmFibHk= 34784
cmVybw== 34785
IE5n 34786
IFdhcmxvY2s= 34787
dHA= 34788
IG92ZXJoZQ== 34789
IGFkbWluaXN0cmF0aW9ucw== 34790
IHRpbnQ= 34791
IHJlZ2ltZW50 34792
IHBpc3RvbHM= 34793
IGJsYW5rZXRz 34794
IGVwaXN0 34795
IGJvd2xz 34796
IGh5ZHJhdWxpYw== 34797
IGRlYW4= 34798
IGp1bmc= 34799
IGFzY2VuZA== 34800
NzA1 34801
IFNhbnRpYWdv 34802
w64= 34803
IHVuYXZvaWQ= 34804
IFNoYW1hbg== 34805
cmVi 34806
IHN0ZW1taW5n 34807
OTk4 34808
IE1H 34809
c3RpY2tz 34810
ZXN0aGVzaWE= 34811
RVJP 34812
IG1vcmJpZA== 34813
IEdyaWxs 34814
IFBvZQ== 34815
YW55bA== 34816
IGRlbGV0aW5n 34817
IFN1cnZlaWxsYW5jZQ== 34818
IGRpcmVjdGl2ZXM= 34819
IGl0ZXJhdGlvbnM= 34820
IFJveA== 34821
IE1pbGt5 34822
RmF0aGVy 34823
IHBhdGVudGVk 34824
NDQ3 34825
IHByZWN1cnNvcg== 34826
IG1haWRlbg== 34827
IFBoZW4= 34828
IFZlZ2Fu 34829
IFBhdGVudA== 34830
S2VsbHk= 34831
UmVkZGl0b3I= 34832
IG5vZHM= 34833
IHZlbnRpbGF0aW9u 34834
IFNjaHdhcno= 34835
IHdpemFyZHM= 34836
IG9taW5vdXM= 34837
IEhlYWRz 34838
IEJH 34839
IGx1bWJlcg== 34840
IFNwaWVs 34841
IGlzRW5hYmxlZA== 34842
IGFuY2VzdHJhbA== 34843
IFNoaXBz 34844
IHdyZXN0bGVy 34845
cGhp 34846
IHl1YW4= 34847
IFJlYmVsbGlvbg== 34848
IGljZWJlcmc= 34849
IG1hZ2ljYWxseQ== 34850
IGRpdmVyc2lvbg== 34851
YXJybw== 34852
eXRobQ== 34853
IFJpZGVycw== 34854
IFJvYmJpZQ== 34855
IEthcmE= 34856
IE1haW50ZW5hbmNl 34857
IEhlcmI= 34858
IGhhcm1z 34859
cGFja2Vk 34860
IEZlaW5zdGVpbg== 34861
IG1hcnJ5aW5n 34862
IGJsZW5kaW5n 34863
IFJhdGVz 34864
IDE4ODA= 34865
IHdyaW5r 34866
IFVuY2g= 34867
IFRvcmNo 34868
ZGVzY3JpYmVk 34869
IGh1bWFub2lk 34870
aWxpdGF0aW5n 34871
IENvbnY= 34872
IEZlbGQ= 34873
SUdIVFM= 34874
IHdoaXN0bGVibG93ZXI= 34875
b3J0bXVuZA== 34876
ZXRzeQ== 34877
YXJyZXR0 34878
IE1vbm8= 34879
IElrZQ== 34880
IENOQkM= 34881
IFdBWQ== 34882
IE1ETUE= 34883
IEluZGl2aWR1YWxz 34884
IHN1cHBsZW1lbnRhbA== 34885
IHBvd2VyaG91c2U= 34886
IFN0cnU= 34887
Rm9jdXM= 34888
YXBoYWVs 34889
IENvbGxlZw== 34890
YXR0aQ== 34891
WkE= 34892
IHBlcmVubg== 34893
IFNpZ25hdHVyZQ== 34894
IFJvZG5leQ== 34895
IGN1YmVz 34896
aWRkbGVk 34897
IERhbnRl 34898
IElOVg== 34899
aWxpbmd1YWw= 34900
IEN0aA== 34901
IHNvZmE= 34902
IGludGltaWRhdGU= 34903
IFJvZQ== 34904
IERpcGxvbQ== 34905
IENvdW50cmllcw== 34906
YXlzb24= 34907
IGV4dHJhZGl0aW9u 34908
IGRpc2FibGluZw== 34909
IENhcmRpZmY= 34910
IG1lbW9yYW5kdW0= 34911
IFRyYWNl 34912
ID8/Pw== 34913
c2VjdG9y 34914
IFJvdWhhbmk= 34915
IFlhdGVz 34916
IEZyZWV6ZQ== 34917
IGJsYWRkZXI= 34918
TW90b3I= 34919
IFByb21pc2U= 34920
YW50YXN5 34921
IGZvcmVzZWVhYmxl 34922
IENvbG9nbmU= 34923
Y29udGFpbmVy 34924
IFRyZWVz 34925
IEdvcnM= 34926
IFNpbmNsYWly 34927
IGJhcnJpbmc= 34928
a2V5ZQ== 34929
IHNsYXNoZWQ= 34930
IFN0YXRpc3RpY2Fs 34931
6Yc= 34932
IOKWug== 34933
QWxsb3dz 34934
IGh1bWlsaXR5 34935
IGRyaWxsZWQ= 34936
IEZ1cm4= 34937
NDQz 34938
IHNld2FnZQ== 34939
IGhvbWVwYWdl 34940
IGNvdXJ0eWFyZA== 34941
IHZpbGU= 34942
IHN1YnNpZGlhcmllcw== 34943
YWpv 34944
ZGlyZWN0b3J5 34945
IGFtbW9u 34946
VmVycw== 34947
Y2hhcmdlcw== 34948
IH19 34949
IENoYWlucw== 34950
IDI0Ng== 34951
bm9i 34952
IHBlcmNlcHQ= 34953
IGdyaXQ= 34954
IGZpc2hlcm1lbg== 34955
IElyYXFpcw== 34956
IERJU1RS 34957
IEZVTEw= 34958
IEV2YWx1YXRpb24= 34959
Z3JhcGg= 34960
YXRpYWw= 34961
IGNvb3BlcmF0aW5n 34962
IG1lbGFu 34963
IGVubGlnaHRlbmVk 34964
IGFsaQ== 34965
dGFpbGVk 34966
IHNhbHV0ZQ== 34967
IHdlYWtlc3Q= 34968
IEJ1bGxkb2dz 34969
VUE= 34970
IEFsbG95 34971
IHNlbWVu 34972
b2NlbmU= 34973
IFdpbGxpYW1zb24= 34974
c3By 34975
LOKAlA== 34976
IEdG 34977
aXR0ZW5z 34978
QmVhdA== 34979
IEp1bms= 34980
aXBoYXRl 34981
IEZhcm1lcnM= 34982
IEJpdGNvaW5z 34983
aWdlcnM= 34984
ZGg= 34985
IExveWFs 34986
cGF5ZXI= 34987
IGVudGVydGFpbmVk 34988
IHBlbm5lZA== 34989
IGNvdXBvbg== 34990
UXVldWU= 34991
IHdlYWtlbmluZw== 34992
Y2Fycnk= 34993
IHVuZGVyZXN0aW1hdGU= 34994
IHNob290b3V0 34995
IGNoYXJpc21hdGlj 34996
IFByb2NlZHVyZQ== 34997
IHBydWRlbnQ= 34998
aW5hbmNlcw== 34999
IHJpY2hlcw== 35000
IGNvcnRpY2Fs 35001
IHN0cmlkZXM= 35002
IGRyaWI= 35003
IE9pbGVycw== 35004
NTQw 35005
IFBlcmZvcm0= 35006
IEJhbmdrb2s= 35007
IGV1dGg= 35008
U0VS 35009
IHNpbXBsaXN0aWM= 35010
dG9wcw== 35011
Y2FtcGFpZ24= 35012
UXVhbGl0eQ== 35013
IGltcG92ZXJpc2hlZA== 35014
IEVpc2VuaG93ZXI= 35015
IGF1Z21lbnQ= 35016
IEhhcmRlbg== 35017
IGludGVydmVuZWQ= 35018
IGxpc3RlbnM= 35019
IEtvaw== 35020
IHNhZ2U= 35021
IHJ1YmJpc2g= 35022
IERlZA== 35023
IG11bGw= 35024
cGVsbGluZw== 35025
IHZpZGVvdA== 35026
UHJvZHVjdGlvbg== 35027
REo= 35028
bWlhaA== 35029
IGFkYXB0YXRpb25z 35030
IG1lZGljYWxseQ== 35031
IGJvYXJkZWQ= 35032
IGFycm9nYW5jZQ== 35033
IHNjcmFwcGVk 35034
IG9wcHJlc3M= 35035
Rk9STUFUSU9O 35036
IGp1bmN0aW9u 35037
NDE1 35038
RUVFRQ== 35039
U2tpbGw= 35040
IHN1YmR1 35041
IFN1Z2dlc3Q= 35042
IFBldHQ= 35043
IGxldHQ= 35044
IE1hbmlw 35045
IENhZg== 35046
IENvb3BlcmF0aW9u 35047
VGhlcg== 35048
IHJlZ2FpbmVk 35049
tuY= 35050
cmVmbGVjdA== 35051
IHRodWdz 35052
IFNoZWxieQ== 35053
IGRpY3RhdGVz 35054
IFdlaW5lcg== 35055
IEhhbGU= 35056
IGJhdHRsZWdyb3VuZA== 35057
c2NoaWxk 35058
IGNvbmRvbA== 35059
aHVudA== 35060
b3NpdG9yaWVz 35061
IGFjY3VzZXM= 35062
RmlsZW5hbWU= 35063
IHNocmk= 35064
IG1vdGl2YXRl 35065
IHJlZmxlY3Rpb25z 35066
TnVsbA== 35067
IExvYmJ5 35068
pbU= 35069
IFNBVEE= 35070
IEJhY2t1cA== 35071
0YM= 35072
bmlu 35073
IENvcnJlY3Rpb24= 35074
IGp1aWN5 35075
dXRyYQ== 35076
IFByaWM= 35077
IHJlc3RyYWluaW5n 35078
IEFpcmJuYg== 35079
IEFycmVzdA== 35080
IGFwcHJvcHJpYXRpb25z 35081
IHNsb3Blcw== 35082
IG1hbnNsYXVnaHRlcg== 35083
IHdvcmtpbmdz 35084
IEh1c3M= 35085
IEZyZXk= 35086
TGVhdmU= 35087
IEhhcm1vbnk= 35088
IEZlZGVy 35089
IDQzMA== 35090
IHRyZW5jaA== 35091
IGdsYWRseQ== 35092
IGJ1bGxwZW4= 35093
IEdhdQ== 35094
Ym9uZXM= 35095
IGdyb292ZQ== 35096
IHByZXRleHQ= 35097
44WL 35098
IHRyYW5zbWl0dGVy 35099
IENvbXBvbmVudA== 35100
IHVuZGVyYWdl 35101
IEVtcGlyZXM= 35102
VGlsZQ== 35103
IG95 35104
IE1hcnZpbg== 35105
IENBUw== 35106
IGJsb3Nz 35107
IHJlcGxpY2F0ZWQ= 35108
IE1hcmluZXJz 35109
TWFyY3Vz 35110
IEJsb2Nrcw== 35111
IGxpYmVyYXRlZA== 35112
IGJ1dHRlcmZseQ== 35113
RmVlbA== 35114
IGZlcm1lbnRhdGlvbg== 35115
IHlvdXR1YmU= 35116
IG9mZmVuZA== 35117
IFRlcm0= 35118
cmVzaXN0 35119
IGNlc3NhdGlvbg== 35120
IGluc3VyZ2VuY3k= 35121
IGJpcg== 35122
IFJhaXNl 35123
NTk1 35124
IGh5cG90aGVzZXM= 35125
NTAy 35126
IHBsYXF1ZQ== 35127
b2NyYXQ= 35128
IGphY2tldHM= 35129
IEh1ZmZQb3N0 35130
YW1vbmc= 35131
IGNvbmZlcg== 35132
NDg3 35133
IExpbGx5 35134
IGFkYXB0aW5n 35135
IEZheQ== 35136
IHNob3ZlZA== 35137
dmVj 35138
IHJlZmluZQ== 35139
IGdvbg== 35140
IGd1bm1lbg== 35141
emFp 35142
IFNodXR0bGU= 35143
IEl6YW4= 35144
IDE5MTM= 35145
IHBsZXRob3Jh 35146
wrfCtw== 35147
IDUxMA== 35148
IHB1YmVydHk= 35149
IDI0MQ== 35150
IFdlYWx0aA== 35151
IEFsbWE= 35152
IE1FTQ== 35153
IEFkdWx0cw== 35154
Q2Fz 35155
cHJpc29u 35156
UmFjZQ== 35157
IHdhdGVycHJvb2Y= 35158
IGF0aGxldGljaXNt 35159
IGNhcGl0YWxpemU= 35160
IEp1aWNl 35161
IGlsbHVtaW5hdGVk 35162
IFBhc2NhbA== 35163
IGlycml0YXRpb24= 35164
IFdpdG5lc3Nlcw== 35165
YWRsZQ== 35166
IEFzdHJv 35167
IGZheA== 35168
IEVsdmlz 35169
UHJpbWFyeQ== 35170
IExpY2g= 35171
IEVsdmVz 35172
IHJlc2lkaW5n 35173
IHN0dW1ibGU= 35174
MzE5 35175
IFBLSw== 35176
IGFkdmVyc2FyaWVz 35177
RE9T 35178
IFJpdHVhbA== 35179
IHNtZWFy 35180
IGFyc29u 35181
aWRlbnRhbA== 35182
IHNjYW50 35183
IG1vbmFyY2h5 35184
IGhhbGZ0aW1l 35185
IHJlc2lkdWU= 35186
IGluZGlnbg== 35187
IFNoYXVu 35188
IEVsbQ== 35189
YXVyaQ== 35190
QWZm 35191
V0FUQ0g= 35192
IEx5b24= 35193
aGVscHM= 35194
MzYx 35195
IGxvYmJ5aXN0 35196
IGRpbWluaXNoaW5n 35197
IG91dGJyZWFrcw== 35198
IGdvYXRz 35199
ZmF2b3JpdGU= 35200
IE5haA== 35201
c29uaWFu 35202
IEJvb3N0ZXI= 35203
IHNhbmRib3g= 35204
IEZhcmU= 35205
IE1hbHRh 35206
IGF0dFJvdA== 35207
IE1PUg== 35208
bGRl 35209
IG5hdmlnYXRpbmc= 35210
VG91Y2g= 35211
IHVudHJ1ZQ== 35212
IERpc2FzdGVy 35213
IGx1ZGljcm91cw== 35214
UGFzc3dvcmQ= 35215
IEpGSw== 35216
YmxvZ3Nwb3Q= 35217
NDE2 35218
IFVOREVS 35219
ZXJuYWw= 35220
IGRlbGF5aW5n 35221
VE9Q 35222
IGltcGxhbnRz 35223
IEFWRw== 35224
IEh1Z2U= 35225
YXR0cg== 35226
IGpvdXJuYWxpc3RpYw== 35227
IFBleXRvbg== 35228
IElB 35229
UmFw 35230
Z29hbA== 35231
IFByb2dyYW1tZQ== 35232
IHNtYXNoaW5n 35233
d2l2ZXM= 35234
cHJpbnRsbg== 35235
IFBsYWd1ZQ== 35236
aW51cw== 35237
RUVQ 35238
IGNydWlzZXI= 35239
IFBhcmlzaA== 35240
dW1pbml1bQ== 35241
IG9jY3VwYW50cw== 35242
IEppaGFk 35243
bW9w 35244
IHBpbnQ= 35245
IGhlY3Q= 35246
IE1lY2Nh 35247
ZGlyZWN0b3I= 35248
IEZ1bmRpbmc= 35249
IE1peGVk 35250
IHN0YWc= 35251
VGllcg== 35252
IGd1c3Q= 35253
IGJyaWdodGx5 35254
b3JzaQ== 35255
IHVwaGlsbA== 35256
UkQ= 35257
IGxlc2lvbnM= 35258
IEJ1bmR5 35259
bGl2aW91cw== 35260
IGJpb2xvZ2lzdA== 35261
IEZhY3VsdHk= 35262
IEF1dGhvcml6YXRpb24= 35263
IDI0NA== 35264
QWxsb3c= 35265
77g= 35266
IEdpdWw= 35267
IHBlcnRpbmVudA== 35268
b3RhdXI= 35269
ZXNzZQ== 35270
IFJvb2Y= 35271
IHVubWFubmVk 35272
MzUx 35273
IFNoYWs= 35274
IE9yaWVudA== 35275
IGVuZGFuZ2Vy 35276
RGly 35277
IHJlcGxlbg== 35278
ZWRpZW50 35279
IHRhaWxvcg== 35280
IGdhZGdldHM= 35281
IGF1ZGlibGU= 35282
4piG 35283
TmljZQ== 35284
IGJvbWJhcmQ= 35285
IFJhcGU= 35286
IGRlZmlhbmNl 35287
IFRXTw== 35288
IEZpbGlwaW5v 35289
IHVuYWZmZWN0ZWQ= 35290
ZXJ2YXRpdmVz 35291
IHNvYXJlZA== 35292
IEJvbHRvbg== 35293
IGNvbXByb21pc2luZw== 35294
IEJyZXdlcnM= 35295
UkFM 35296
IEFITA== 35297
aWN5Y2xl 35298
IHZhbXBpcmVz 35299
IGRpcHBlZA== 35300
b3llcg== 35301
IFhJSUk= 35302
IHNpZGV3YXlz 35303
IFdhc3Rl 35304
IERpc3M= 35305
IOKUnOKUgOKUgA== 35306
JC4= 35307
IGhhYml0YXRz 35308
IEJlZWY= 35309
dHJ1dGg= 35310
dHJhaW5lZA== 35311
c3BsaXQ= 35312
UnVz 35313
QW5keQ== 35314
IEJyYW0= 35315
UkVQ 35316
cGlk 35317
6KOF 35318
IE11dGFudA== 35319
QW5pbQ== 35320
IE1hcmluYQ== 35321
IGZ1dGlsZQ== 35322
aGlnaGVzdA== 35323
ZnJlcXVlbmN5 35324
IGVwaWxlcHN5 35325
IGNvcGluZw== 35326
IGNvbmNpc2U= 35327
IHRyYWNpbmc= 35328
IFNVTg== 35329
cGFuZWw= 35330
IFNvcGhpZQ== 35331
IENyb3dsZXk= 35332
IEFkb2xm 35333
IFNob290ZXI= 35334
IHNoYWt5 35335
IElH 35336
IExpZXM= 35337
IEJhcmJlcg== 35338
cGtn 35339
IHVwdGFrZQ== 35340
IHByZWRhdG9yeQ== 35341
VUxUUw== 35342
Lyoq 35343
IGludG94aWNhdGVk 35344
IFdlc3Ricm9vaw== 35345
b2RkZXI= 35346
aGVtZW50 35347
IGJhc2VtYW4= 35348
QVBE 35349
c3RvcmFnZQ== 35350
IEZpZnR5 35351
ZWRpdG9y 35352
R0VO 35353
VVRJT04= 35354
aXJ0aW5n 35355
IHNld2luZw== 35356
cmlmdA== 35357
IGFnb255 35358
IFNhbmRz 35359
IDI1NA== 35360
Q2FzaA== 35361
IGxvZGdl 35362
IHB1bnQ= 35363
TmF0dXJhbA== 35364
IElkZWFz 35365
IGVycm9uZW91cw== 35366
IFNlbnNvcg== 35367
IEhhbm5pdHk= 35368
IDE5MjE= 35369
IG1vdWxk 35370
IEdvbg== 35371
a2F5YQ== 35372
IGFub255bW91c2x5 35373
IEtFWQ== 35374
IHNpbXVsYXRvcg== 35375
V2ludGVy 35376
IHN0cmVhbWVk 35377
NTA3 35378
PyIs 35379
IHRlYXNlZA== 35380
IGNvZWZmaWNpZW50 35381
IHdhcnRpbWU= 35382
IFRIUg== 35383
Jycu 35384
IEJhbmtpbmc= 35385
bXBpcmU= 35386
IGZhbmRvbQ== 35387
IGxpYQ== 35388
R2E= 35389
IGRvd25oaWxs 35390
IGludGVycHJldGluZw== 35391
SW5kaXZpZHVhbA== 35392
Tm9ybQ== 35393
IGplYWxvdXN5 35394
Yml0Y29pbg== 35395
IHBsZWFzdXJlcw== 35396
IFRveXM= 35397
IENoZXZyb2xldA== 35398
IEFkdmlzb3I= 35399
SVpF 35400
IHJlY2VwdGlvbnM= 35401
NzA2 35402
Q3Jv 35403
IDI2Mg== 35404
IGNpdHJ1cw== 35405
aXJ1 35406
UmV2aWV3ZXI= 35407
amVjdGVk 35408
VUVT 35409
YW56 35410
MTk4MQ== 35411
IFdvcmtlcg== 35412
IGNvbXBsaWVk 35413
b3Jlc2NlbnQ= 35414
Y29udGluZW50YWw= 35415
VG9u 35416
IFByaXNt 35417
IFNoZWVw 35418
IDI4OA== 35419
bm94 35420
IFZvZw== 35421
T3Jk 35422
IHJlYWxtcw== 35423
dGVr 35424
IGlycmlnYXRpb24= 35425
IGJpY3ljbGVz 35426
IGVsZWN0cm9uaWNhbGx5 35427
cG9seQ== 35428
dGFsbA== 35429
KCkpOw== 35430
IGFlc3RoZXRpY3M= 35431
IEludGVncmF0ZWQ= 35432
RXhwbG9yZQ== 35433
IGR1bms= 35434
NDc2 35435
cGFpbg== 35436
IEphY3F1ZXM= 35437
IERtaXQ= 35438
RnJhbWVz 35439
IHJldW5pdGVk 35440
IGh1bWlk 35441
RHJv 35442
UG9saXRpY2Fs 35443
IHlvdXRoZnVs 35444
IGVudGFpbHM= 35445
IG1vc3F1aXRv 35446
MzYz 35447
c3BlY2llcw== 35448
IGNvb3JkaW5hdGluZw== 35449
IE1heWhlbQ== 35450
IE1hZ251cw== 35451
TW91bnQ= 35452
SW1wcm92ZWQ= 35453
IFNUQVRF 35454
QVRUTEU= 35455
IGZsb3dlZA== 35456
IHRhY2tsZWQ= 35457
IGZhc2hpb25lZA== 35458
IHJlb3JnYW4= 35459
aXZhcmk= 35460
ZmluZ2Vy 35461
IHJlbHVjdGFudGx5 35462
ZXR0aW5n 35463
IFZhbmQ= 35464
eW91bmc= 35465
IEdhcmxhbmQ= 35466
IHByZXN1bXB0aW9u 35467
IGFtZW5pdGllcw== 35468
IFBsZWFzYW50 35469
b25lbnRpYWw= 35470
IE94eQ== 35471
IG1vcmFscw== 35472
IFlhaA== 35473
UmVhZHk= 35474
U2ltb24= 35475
RW5o 35476
RGVtb24= 35477
IGNsaWNo 35478
TW9uaXRvcg== 35479
IERV 35480
IHdlbGNvbWVz 35481
IHN0YW5kb3V0 35482
IGRyZWFkZnVs 35483
IGJhbmFuYXM= 35484
IGJhbGxvb25z 35485
aG9vdGluZw== 35486
YmFzaWM= 35487
IHN1ZmZpeA== 35488
IGR1bHk= 35489
Y2Fubw== 35490
Q2hhaW4= 35491
YXRvcw== 35492
IGdlb3BvbGl0aWNhbA== 35493
ICgm 35494
IEdlbWluaQ== 35495
w4PDgsODw4LDg8OCw4PDgsODw4LDg8OCw4PDgsODw4LDg8OCw4PDgsODw4LDg8OCw4PDgsODw4LDg8OCw4PDgsODw4LDg8OCw4PDgsODw4LDg8OCw4PDgsODw4LDg8OCw4PDgsODw4LDg8OCw4PDgsODw4LDg8OCw4PDgsODw4I= 35496
IGFjcXVpdHRlZA== 35497
THVjaw== 35498
cHJvdGVjdA== 35499
MTAyNA== 35500
IHNjYXJjaXR5 35501
IG1pbmRmdWxuZXNz 35502
ZWNpZGVk 35503
RE4= 35504
cHJpbWU= 35505
IFByZXNpZGVudHM= 35506
IFZJREVP 35507
ICjiiJI= 35508
YWRkb2Nr 35509
Tk9S 35510
IFBydQ== 35511
cHVu 35512
IExPTA== 35513
KSkpKQ== 35514
IExpcXU= 35515
IFNBUw== 35516
IHN0eWxpbmc= 35517
IHB1bmlzaG1lbnRz 35518
IG51bWI= 35519
IGFzY2VydGFpbg== 35520
IFJvY2tpZXM= 35521
Zmx1 35522
VGh1bWJuYWls 35523
IHBlcnBldHJhdGVk 35524
IFNlbWk= 35525
IGRpc2FybQ== 35526
IE9sZGVy 35527
IEV4Y2VwdGlvbg== 35528
IGV4cG9uZW50aWFsbHk= 35529
IENvbW11bml0aWVz 35530
IGFib2xpc2g= 35531
IFBhcnRuZXI= 35532
cHRvbXM= 35533
IDc3Nw== 35534
IEZvbGV5 35535
IENhc2Vz 35536
IGdyZWFzZQ== 35537
IFJlYmlydGg= 35538
R3JvdW5k 35539
IDsp 35540
IERvY3RyaW5l 35541
aWtpbmk= 35542
WWU= 35543
IEJsb3Nzb20= 35544
IHBlcnNpc3Rz 35545
YmlsbA== 35546
IGluZnVzaW9u 35547
IGJ1ZGRpZXM= 35548
OTEx 35549
IFBhdGllbnQ= 35550
IGRlbW9z 35551
IGFjcXVhaW50YW5jZQ== 35552
IFBhdw== 35553
YXRhcmk= 35554
IHhtbA== 35555
IGZhc2NpbmF0aW9u 35556
IFNlcnZl 35557
z4I= 35558
YnJhbmRlZA== 35559
IGF6 35560
UmV0dXJucw== 35561
IG92ZXJzaGFkb3c= 35562
IHJvYW0= 35563
IHNwZWVkeQ== 35564
bnVtYmVyZWQ= 35565
aGVsaWFs 35566
IGRpc2NpcGxl 35567
IGFzc3VyYW5jZXM= 35568
Z2l2ZW4= 35569
cGVjdGluZw== 35570
IE5hdGFsaWU= 35571
55Sw 35572
IG1vc3F1aXRvZXM= 35573
cm90ZWlu 35574
IG51bWVyaWM= 35575
IGluZGVwZW5kZW50cw== 35576
IHRyYW5zaXRpb25hbA== 35577
IHJlYWN0aW9uYXJ5 35578
IE1lY2hkcmFnb24= 35579
ZG9jdG9y 35580
IHNob3J0ZXN0 35581
IHNlcXVlbnRpYWw= 35582
IEJhYw== 35583
IEFjY291bnRz 35584
44GM 35585
YWNoeQ== 35586
cmFjdGl2ZQ== 35587
IFJlZ2ltZW50 35588
IGJyZWF0aHRha2luZw== 35589
ZmZpY2llbmN5 35590
IEJhdGVz 35591
IDMxMQ== 35592
IHdhcmRyb2Jl 35593
ZnRz 35594
IEJlcms= 35595
U2ltcGx5 35596
IFJpdmVyc2lkZQ== 35597
aXZlcmluZw== 35598
aWRlbnRpYWw= 35599
bHVjZW50 35600
IGVucmljaGVk 35601
IENvbnZlcg== 35602
IEdpdmluZw== 35603
44OZ 35604
IGxlZ2FsaXpl 35605
IEZUQw== 35606
IGZyZWFraW5n 35607
TWl4 35608
IHRlcnJlc3RyaWFs 35609
ZXNpYW4= 35610
Y2llbnRz 35611
V2luZw== 35612
TE9BRA== 35613
IGxlZGdl 35614
IFZpb2xlbnQ= 35615
IE1ldGFsbA== 35616
IDMwOA== 35617
IHNvdXRoZWFzdGVybg== 35618
aGV0dG8= 35619
TWVhdA== 35620
IHNsb3dkb3du 35621
IHJldHJlYXRlZA== 35622
SmVyZW15 35623
ZW5kYXM= 35624
KioqKio= 35625
ZXJpYw== 35626
IHJlaW5z 35627
b3BwYWJsZQ== 35628
IEh1bWFuaXR5 35629
ZWFyYW5jZXM= 35630
cmlnYW4= 35631
Q2FtZXJh 35632
IHdhaXZlcnM= 35633
c29j 35634
IGFsdGVyYXRpb24= 35635
dHJhbnNmb3Jt 35636
IENlbWV0ZXJ5 35637
NTA2 35638
IGluZGVmaW5pdGU= 35639
IHN0aW11bGF0aW5n 35640
eWc= 35641
NjAz 35642
IFNvcA== 35643
IGRlc2NyaXB0aXZl 35644
UGhhc2U= 35645
IEVkbXVuZA== 35646
IHBuZXVtb25pYQ== 35647
dmVudHVz 35648
QW1i 35649
IGxhYm9yYXRvcmllcw== 35650
IEV4Y2x1c2l2ZQ== 35651
dWdhcg== 35652
V2VyZQ== 35653
IG1hbGZ1bmN0aW9u 35654
IGhvbW9zZXh1YWxz 35655
IC0tLS0tLS0= 35656
dW5p 35657
IHR1cmJpbmVz 35658
IEVxdWl0eQ== 35659
RHU= 35660
IG1pbmRlZA== 35661
IFJI 35662
IEJsYWNraGF3a3M= 35663
IGZlYXRz 35664
IDE3MDA= 35665
cmVwbA== 35666
MzYy 35667
bGFkZW4= 35668
IGluZGlzcGVuc2FibGU= 35669
bHlzcw== 35670
dHRp 35671
IHJlZWw= 35672
IGRpdmVydGVk 35673
IGxpa2VuZXNz 35674
IHN1YnNjcmlwdGlvbnM= 35675
IGZpbmdlcnQ= 35676
IGZpbHRoeQ== 35677
ZGVzdHJ1Y3Q= 35678
ZHJhZnQ= 35679
IEJlcm5hcmRpbm8= 35680
bGF1bmNo 35681
IHBlcnBsZXg= 35682
IFNVTQ== 35683
Y2FyYg== 35684
IHN3ZWF0ZXI= 35685
IFZlbnR1cmU= 35686
IEphZw== 35687
IENlbGVi 35688
IFZvdGVycw== 35689
IHN0ZWFkZmFzdA== 35690
IGF0aGxldGljcw== 35691
IEhhbnNvbg== 35692
IERyYWM= 35693
VHJhY2tlcg== 35694
IGNvbW1lbmQ= 35695
IFByZXNpZGVuY3k= 35696
IERJRA== 35697
aW5mb3JtZWQ= 35698
IHdlYnBhZ2U= 35699
UHJldHR5 35700
IGZvcmNlZnVsbHk= 35701
44OD44Kv 35702
IHJlbG9jYXRpb24= 35703
IHNhdGlyZQ== 35704
4ok= 35705
IFN1bmRlcmxhbmQ= 35706
5oQ= 35707
Vm9pY2U= 35708
Pz8/Pz8/Pz8= 35709
IGluZm9ybWFudA== 35710
IGJvd2Vs 35711
IFVuaWZvcm0= 35712
IC4uLiI= 35713
IHB1cmdl 35714
IHBpY25pYw== 35715
IFVtYg== 35716
IFVQREFURQ== 35717
IFNhcHBoaXJl 35718
IFN0YWxs 35719
bGVhcm4= 35720
IG9iamVjdGl2ZWx5 35721
IG9ibGl0ZXI= 35722
IGxvb3Bob2xl 35723
IGpvdXJuZXlz 35724
IG9taXNzaW9u 35725
UHJvcw== 35726
IFNpZG5leQ== 35727
cGxvbWE= 35728
IHNwcmF5ZWQ= 35729
IGd1cnU= 35730
IHRyYWl0b3I= 35731
IHRpbWV0 35732
IHNuYXBwaW5n 35733
IFNldmVudA== 35734
dXJuYWw= 35735
IFVraXA= 35736
IGJvd2Vk 35737
cG9yYWw= 35738
bGliZXJhbA== 35739
Um9z 35740
UXVlc3Rpb25z 35741
aU9T 35742
IHN1bW1hcml6ZQ== 35743
U1RBVA== 35744
IDE4NTA= 35745
YXBlc3Q= 35746
IGxlbmRlcg== 35747
IFZhcmlhYmxl 35748
YnJpbmdpbmc= 35749
IExPUkQ= 35750
LCk= 35751
IGNvbGxhcHNlcw== 35752
eGlldHk= 35753
IE5lZA== 35754
WUQ= 35755
IFNjaGE= 35756
IGFudGlib2R5 35757
IGRpc2JhbmQ= 35758
eXJl 35759
aWxsdXNpb24= 35760
IHJvdmVy 35761
c2hlZA== 35762
IEhpcm9zaA== 35763
Y2Np 35764
IGNhbGFt 35765
IE1vcnRvbg== 35766
UGludGVyZXN0 35767
IDE5Mjg= 35768
IEV1cmFz 35769
b3JkZXM= 35770
IGZlbmNlcw== 35771
IEludmVudG9yeQ== 35772
IFZhbGVuY2lh 35773
IFVk 35774
IFRpZmY= 35775
IHNxdWU= 35776
IHF1b3RhdGlvbg== 35777
IHRyb3VibGVzb21l 35778
ZXJrZXI= 35779
UVVFU1Q= 35780
IEtpbmdkb21z 35781
c291dGg= 35782
IGxldnk= 35783
UHJpbmNl 35784
IFN0aW5n 35785
IG5pY2tuYW1lZA== 35786
IGFwcGU= 35787
IHBob3RvZ3JhcGhpYw== 35788
IGNvcnB1cw== 35789
cmVmZXJlbmNl 35790
IFRyb2c= 35791
VW50 35792
KT0o 35793
IExhdHZpYQ== 35794
IGFjdGl2YXRpbmc= 35795
IGxpY2Vuc2Vl 35796
IGRpc3Bhcml0aWVz 35797
IE5ld3NsZXR0ZXI= 35798
44OD44OI 35799
IGZyZWVpbmc= 35800
IEplZXA= 35801
IFBlcmNlcHRpb24= 35802
aW5zaw== 35803
IHNpbGljb25l 35804
IEhheWRlbg== 35805
TGVhbg== 35806
IFN1enVraQ== 35807
aWJyYXJpYW4= 35808
NjY4 35809
IHNwb3I= 35810
IGNvcnJlbGF0aW9ucw== 35811
YWdoZXR0aQ== 35812
IHR1YmVy 35813
IElQQ0M= 35814
aWx1cw== 35815
IFZ1 35816
IHdlYWx0aGllc3Q= 35817
IENhcmJ1bmNsZQ== 35818
YW56YQ== 35819
IGZvb2xlZA== 35820
IFp1cg== 35821
IGRhZGR5 35822
cmFubw== 35823
aWxpYW4= 35824
IGtub2Nrb3V0 35825
Zm1hbg== 35826
cmVxdWlyZWQ= 35827
IFdpa2lsZWFrcw== 35828
IER1ZmZ5 35829
T05U 35830
IGluc29s 35831
IE9iamVjdHM= 35832
IGJvdQ== 35833
IE5vcmRpYw== 35834
IEluc2VydA== 35835
c2Nhbg== 35836
IGRhbmNlcnM= 35837
IGlkaW90cw== 35838
bWFqb3JpdHk= 35839
IE5ldmlsbGU= 35840
IEZyZWVCU0Q= 35841
IHRhcnQ= 35842
cGFuaWM= 35843
Njkw 35844
IGNvY29h 35845
IHNhbXBsZWQ= 35846
IGxvb2t1cA== 35847
SW5kdXN0 35848
IGluamVjdGlvbnM= 35849
Z2VucmU= 35850
IGF1 35851
IHJvYWR3YXk= 35852
IGdlbml0YWxz 35853
S2luZA== 35854
IEV4YW1pbmVy 35855
IFlheg== 35856
RnJlc2g= 35857
IHBhcmFseXNpcw== 35858
IEFsdW1pbnVt 35859
IHJlYXA= 35860
b2vDqQ== 35861
IHNsb3BweQ== 35862
IFR1bm5lbA== 35863
cG9zaXVt 35864
bmVyeQ== 35865
ZW5pYw== 35866
IGhlcmJhbA== 35867
IE91dGVy 35868
IEJ1aWxkZXI= 35869
IGluY3Vy 35870
IGlkZW9sb2dpZXM= 35871
IGJhY2t1cHM= 35872
Y29uc3VtaW5n 35873
IERldGVjdA== 35874
ZGVjaw== 35875
IEtOT1c= 35876
IEdyZXQ= 35877
IE1JQw== 35878
IHRvdWdobmVzcw== 35879
IEV4aGliaXQ= 35880
IGhpdmU= 35881
TGVz 35882
IFNDSE9PTA== 35883
IEF0YXJp 35884
YWxkZQ== 35885
IE51bGw= 35886
YW5kZXN0aW5l 35887
bW91c2U= 35888
IGJyaWdhZGU= 35889
NDg5 35890
IHJldm9s 35891
IExhd3Nvbg== 35892
IFdhaA== 35893
b3BvbHk= 35894
ZWJ0ZWQ= 35895
IFNhdW5kZXJz 35896
IDMxMw== 35897
IFdpbmM= 35898
IHRhYm9v 35899
IEhlbG1ldA== 35900
IHdlZGdl 35901
Y2hpcA== 35902
IFRpbmE= 35903
Ymc= 35904
IGluZnVyaQ== 35905
cm4= 35906
IGFub21hbGllcw== 35907
IFN5bmM= 35908
IEV4YW0= 35909
IENvbW1pdA== 35910
IERpYXJ5 35911
IEFMU08= 35912
IERlYm9y 35913
b21lZGljYWw= 35914
IGNvbXByZWhlbnNpb24= 35915
NjU1 35916
IGVtcG93ZXJpbmc= 35917
IGlyZQ== 35918
IGp1aWNlcw== 35919
IEVUSA== 35920
IEJveGluZw== 35921
PSIv 35922
IGZhY2lsaXRhdGVk 35923
cG9rZQ== 35924
IFBhcnNvbnM= 35925
IE1vZGVy 35926
dHJhdmVs 35927
IGNpdmlsaXphdGlvbnM= 35928
IGxpYmVydGFyaWFucw== 35929
IHJ1bmU= 35930
IENsYXJrcw== 35931
YXRoZWQ= 35932
IGNhbXBhaWduZXJz 35933
IERpc3BhdGNo 35934
IEZhaHJlbmhlaXQ= 35935
IENhcGNvbQ== 35936
LS0tLS0tLS0tLQ== 35937
IGxhY2U= 35938
IGRyYWluaW5n 35939
IGxpbmVy 35940
IEFydGlmaWNpYWw= 35941
w6lu 35942
dGFzaw== 35943
XSku 35944
IEdNTw== 35945
IE9wZXJhdG9y 35946
b3JkaW5hcnk= 35947
IEluZmx1ZW5jZQ== 35948
IFVwcw== 35949
IHBvdGVuY3k= 35950
dXNzZW4= 35951
b3Nwb25z 35952
IFN3aW0= 35953
IERlYWRsaW5l 35954
VW5pdHk= 35955
IGN1bGluYXJ5 35956
IGVubGlnaHRlbm1lbnQ= 35957
IHdlYXJlcg== 35958
IG1pbmVk 35959
IHBseQ== 35960
IGluY2VzdA== 35961
IERWRHM= 35962
V2Fsaw== 35963
QlRD 35964
VHJhZGU= 35965
IGRldmFs 35966
aWJhbmQ= 35967
IE92ZXJzaWdodA== 35968
UGFsZXN0aW5pYW4= 35969
IGRhcnQ= 35970
IG11bA== 35971
TFI= 35972
IHJlbW92YWJsZQ== 35973
IFJlYWxtcw== 35974
7J0= 35975
IG1pc2Nhcg== 35976
IFZ1bGthbg== 35977
Njg1 35978
w6hyZQ== 35979
IFNhcA== 35980
IG1lcmdpbmc= 35981
IENhcmx5 35982
Y2hlc3Rlcg== 35983
IGJyaXNr 35984
IGx1eHVyaW91cw== 35985
IEdlbmVyYXRvcg== 35986
IGJpdHRlcm5lc3M= 35987
IGVkaWJsZQ== 35988
IDI0Mw== 35989
VEc= 35990
IHJlY3RhbmdsZQ== 35991
V2l0aE5v 35992
YmVsb3c= 35993
SmVubg== 35994
IGRhcmtlc3Q= 35995
IGhpdGNo 35996
IGRvc2FnZQ== 35997
IHNjYXZlbg== 35998
IEtlbGxlcg== 35999
IElsbHVzdHJhdGVk 36000
Q2VydGFpbmx5 36001
IE1hdmVyaWNrcw== 36002
TWFyZ2luYWw= 36003
IGRpYXJyaGVh 36004
IGVub3Jtb3VzbHk= 36005
IDk5OQ== 36006
c2hy 36007
cXVhcnQ= 36008
IGFkYW1hbnQ= 36009
IE1ldw== 36010
IHJlbm92YXRpb24= 36011
IGNlcnZpY2Fs 36012
IFBlcmNlbnRhZ2U= 36013
ZW5lcnM= 36014
IEtpbWJlcg== 36015
IGZsb2F0cw== 36016
IGRleA== 36017
IFdpdGNoZXI= 36018
IFN3YW5zZWE= 36019
ZG0= 36020
IHNhbHR5 36021
eWVsbG93 36022
IGNhcGU= 36023
IERyYWlu 36024
IFBhdWxh 36025
IFRvbGVkbw== 36026
bGVzaQ== 36027
TWFnYXppbmU= 36028
IFdpY2s= 36029
IE1u 36030
IEFjaw== 36031
IFJpZGluZw== 36032
QVNPTg== 36033
IGhvbW9waG9iaWM= 36034
QVJQ 36035
IHdhbmRlcmVk 36036
Q1BV 36037
b29kb28= 36038
IFBpcGU= 36039
IHRpZ2h0ZW5pbmc= 36040
IEJ1dHQ= 36041
MzE4 36042
IGRlc2VydGVk 36043
U2Vzc2lvbg== 36044
IGZhY2lsaXRhdGluZw== 36045
SnVtcA== 36046
IGVtZXJnZW5jaWVz 36047
T1dFUg== 36048
IGV4aGF1c3RpdmU= 36049
IEFGVEVS 36050
IGhlYXJ0YmVhdA== 36051
IExhYmVs 36052
YWNreQ== 36053
IENlcnRpZmllZA== 36054
aWx0cmF0aW9u 36055
WmU= 36056
IFV0dA== 36057
IDEzMDA= 36058
IHByZXN1bWU= 36059
IERpc3A= 36060
IHN1cmdlZA== 36061
IGRvbGxz 36062
Q29sdW1i 36063
IGNoaW1wYW4= 36064
IFJhem9y 36065
IHRpY2tz 36066
IGNvdW5jaWxsb3I= 36067
IHBpbGdyaW1hZ2U= 36068
IFJlYmVscw== 36069
IFFD 36070
IEF1Y3Rpb24= 36071
eGlh 36072
aWtr 36073
YnJlZA== 36074
IGluc2VydGlvbg== 36075
IGNvYXJzZQ== 36076
ZEI= 36077
U0VF 36078
IFphcA== 36079
IEZvbw== 36080
IGNvbnRlbXBvcg== 36081
IFF1YXJ0ZXJseQ== 36082
b3Rpb25z 36083
IEFsY2hlbWlzdA== 36084
IFRyZXk= 36085
IER1bw== 36086
U3dlZXQ= 36087
ODA0 36088
IEdpb3Y= 36089
IGZ1bm4= 36090
Tmlu 36091
aG9mZg== 36092
IHJhbWlmaWNhdGlvbnM= 36093
IDE5MjI= 36094
IEV4cGVydHM= 36095
YXplcw== 36096
IGdhcm1lbnRz 36097
YXJpYWw= 36098
IE5hYg== 36099
IDI1Nw== 36100
IFZlZA== 36101
IGh1bW9yb3Vz 36102
IFBvbXBl 36103
IG55bG9u 36104
IGx1cmtpbmc= 36105
IFNlcmdleQ== 36106
IE1hdHRpcw== 36107
IG1pc29neW55 36108
IENvbXBvbmVudHM= 36109
IFdhdGNoaW5n 36110
IEZvbGs= 36111
cmFjdGljYWw= 36112
QnVzaA== 36113
IHRhcGVk 36114
IGdyb3VwaW5n 36115
IGJlYWRz 36116
IDIwNDg= 36117
IGNvbmR1 36118
cXVlcnF1ZQ== 36119
UmVhZGluZw== 36120
IGdyaWV2YW5jZXM= 36121
VWx0cmE= 36122
IGVuZHBvaW50 36123
SGln 36124
IFN0YXRpYw== 36125
IFNjYXJib3JvdWdo 36126
THVh 36127
IE1lc3Np 36128
YXF1 36129
IFBzeU5ldA== 36130
IFJ1ZGQ= 36131
IGF2ZW51ZQ== 36132
dnA= 36133
SmVy 36134
IHNoYWR5 36135
IFJlc2lzdA== 36136
IEFydGVtaXM= 36137
IGNhcmVsZXNz 36138
IGJyb2tlcnM= 36139
IHRlbXBlcmFtZW50 36140
IDUyMA== 36141
VGFncw== 36142
IFR1cm5pbmc= 36143
IHV0dGVyZWQ= 36144
IHBlZGQ= 36145
IGltcHJvdmlzZWQ= 36146
IDoo 36147
IHRhYmw= 36148
IHBsYWlucw== 36149
MTYwMA== 36150
cHJlc3N1cmU= 36151
IEVzc2VuY2U= 36152
bWFyZ2lu 36153
ZnJpZW5kcw== 36154
IFJlc3RvcmF0aW9u 36155
IHBvbGx1dA== 36156
IFBva2Vy 36157
IEF1Z3VzdGluZQ== 36158
IENJUw== 36159
IFNFQUw= 36160
b3JhbWE= 36161
IHRod2FydA== 36162
c2Vlaw== 36163
IHBhZ2Fu 36164
wro= 36165
Y3B1 36166
IGdhcm4= 36167
IGFzc29ydG1lbnQ= 36168
IElMQ1M= 36169
dG93ZXI= 36170
UmVjb21tZW5kZWQ= 36171
IHVuYm9ybg== 36172
IFJhbmRvbVJlZGRpdG9y 36173
IFJhbmRvbVJlZGRpdG9yV2l0aE5v 36174
IHBhcmFseXplZA== 36175
IGVydXB0aW9u 36176
IGludGVyc2VjdA== 36177
IFN0b2tl 36178
IFNjbw== 36179
QmluZA== 36180
5b4= 36181
IFBORw== 36182
IE5lZ2F0aXZl 36183
IE5PQUE= 36184
TGVvbg== 36185
IGFsbG95 36186
IExhbWE= 36187
IERpdmVyc2l0eQ== 36188
NTc1 36189
IHVuZGVyZXN0aW1hdGVk 36190
IFNjb3I= 36191
IG11cmFs 36192
IGJ1c3RlZA== 36193
c29vbg== 36194
bGlm 36195
IG5vbmV4 36196
IGFsbGVyZ3k= 36197
IFVuZGVyd29ybGQ= 36198
IFJheXM= 36199
IEJsYXNpbw== 36200
IGhycw== 36201
IERpcg== 36202
IDMyNw== 36203
Ynl0ZXI= 36204
IHJlcGxhY2VtZW50cw== 36205
IGFjdGl2YXRlcw== 36206
cml2ZWQ= 36207
TUg= 36208
IHBhbnM= 36209
IEhJ 36210
IGxvbmdpdHVkaW5hbA== 36211
IG51aXNhbmNl 36212
YWxlcg== 36213
IHN3ZWxs 36214
IFNpZ25lZA== 36215
c2Np 36216
IElzbGVz 36217
IEFHQQ== 36218
IGRlZmlhbnQ= 36219
IHNvbmlj 36220
b2Nvbg== 36221
S0M= 36222
IEFpbQ== 36223
dGll 36224
YWhhaA== 36225
IG1M 36226
RFg= 36227
IGJpc2M= 36228
IEJpbGxib2FyZA== 36229
IFNZU1RFTQ== 36230
TkVZ 36231
Z2FhcmQ= 36232
IGRpc3RyZXNzZWQ= 36233
Zm9ybWVybHk= 36234
QWxhbg== 36235
IGNoZWZz 36236
IG9wdGljcw== 36237
IENvbWV0 36238
IEFNQw== 36239
IHJlZGVzaWduZWQ= 36240
aXJtYXRpb24= 36241
IHNpZ2h0aW5ncw== 36242
Mzgy 36243
MzEx 36244
IFdC 36245
IGNvbnRyYWN0aW9u 36246
IFRPVEFM 36247
RHVhbA== 36248
IHN0YXJ0bGVk 36249
IHVuZGVyc3RhbmRhYmx5 36250
IHN1bmdsYXNzZXM= 36251
RVRIT0Q= 36252
IGRvY2tlcg== 36253
IHN1cmZpbmc= 36254
IEhFTA== 36255
IFNsYWNr 36256
dG9uZXM= 36257
IHNoYWx0 36258
VmlzdWFs 36259
NDk4 36260
RGVwYXJ0bWVudA== 36261
Y3Vzc2lvbg== 36262
IHVucmVzdHJpY3RlZA== 36263
IHRhZA== 36264
IHJlbmFtZQ== 36265
ZW1wbG95ZWQ= 36266
IGVkdWNhdGluZw== 36267
IGdyaW5uZWQ= 36268
YmVkcm9vbQ== 36269
IEFjdGl2aXRpZXM= 36270
IFZlbHZldA== 36271
IFNXQVQ= 36272
IHNodWZmbGU= 36273
aWdvcg== 36274
IHNhdHVyYXRpb24= 36275
RmluZGluZw== 36276
Y3JlYW0= 36277
aWN0ZXI= 36278
IHZvZGth 36279
dHJhY2tpbmc= 36280
dGVj 36281
IGZvcmVncm91bmQ= 36282
aWVzdGE= 36283
IHZlaGVtZW50 36284
IEVDQg== 36285
IFRpZQ== 36286
RXk= 36287
IHR1cnRsZXM= 36288
IFJhaWxyb2Fk 36289
IEthdHo= 36290
IEZyYW1lcw== 36291
IG1lbmFjZQ== 36292
IEZlbGxvd3NoaXA= 36293
IEVzc2VudGlhbA== 36294
dWdnaXNo 36295
IGRyaXA= 36296
Y2h3aXR6 36297
IEt5b3Rv 36298
c2I= 36299
IE5pbmE= 36300
UGFyYW1ldGVy 36301
IGFsYXJtcw== 36302
IENsYXVk 36303
IHBpb25lZXJpbmc= 36304
IGNoaWVmbHk= 36305
IFNjcmVhbQ== 36306
Q29sbGVjdGlvbg== 36307
IHRoYW5rZnVsbHk= 36308
IFJvbmFsZG8= 36309
5a2Q 36310
c3RyaXA= 36311
IERpc25leWxhbmQ= 36312
Y29tbWVyY2lhbA== 36313
U2VlaW5n 36314
U291bA== 36315
IGV2YWN1YXRl 36316
IGNpdg== 36317
IEFzaGU= 36318
IGRpdmlkZXM= 36319
IERhZ2dlcg== 36320
cmVoZW5zaXZl 36321
IGJlcnJpZXM= 36322
IERG 36323
IHN1c2hp 36324
IHBsdXJhbGl0eQ== 36325
V0k= 36326
IGRpc2FkdmFudGFnZWQ= 36327
IGJhdHRhbGlvbg== 36328
b2JpbGVz 36329
NDUx 36330
IGNsaW5n 36331
IHVuZGVuaWFibGU= 36332
IExvdW5nZQ== 36333
IGhhdW50 36334
cGhl 36335
IHF1YW50aWZ5 36336
IGRpZmZlcmVk 36337
IFsqXQ== 36338
IFZpeg== 36339
Y3Vt 36340
c2xhdmU= 36341
IHZpZGVvZw== 36342
IHF1YXI= 36343
IGJ1bmRsZXM= 36344
IEFsb25zbw== 36345
dGFja2xl 36346
IG5ldXJvbmFs 36347
IGxhbmRzbGlkZQ== 36348
Y29uZmlybWVk 36349
IERlcHRo 36350
IHJlbmV3YWJsZXM= 36351
QmVhcg== 36352
IE1hY2Vkb25pYQ== 36353
IGplcnNleXM= 36354
IGJ1bms= 36355
IFNwYXdu 36356
IENvbnRyb2xz 36357
IEJ1Y2hhbmFu 36358
IHJvYm90aWNz 36359
IGVtcGhhc2l6aW5n 36360
IFR1dG9yaWFs 36361
aHlw 36362
aXN0b24= 36363
IG1vbnVtZW50YWw= 36364
5rA= 36365
IENhcnJ5 36366
IHRic3A= 36367
ZW5hbmNl 36368
SGlsbA== 36369
YXJ0aGVk 36370
IHJvdHRlbg== 36371
RGVhbg== 36372
IHR3aXN0aW5n 36373
IGdvb2R3aWxs 36374
IGltbWVyc2lvbg== 36375
TGl2aW5n 36376
IGJydXNoZXM= 36377
IENHSQ== 36378
IEF0aw== 36379
dHJhZGl0aW9uYWw= 36380
IHBoYW50b20= 36381
IFN0YW1pbmE= 36382
IGV4cGFuc2lvbnM= 36383
IE1hcmlu 36384
IGVtYmFya2Vk 36385
IEVn 36386
aW50ZXN0aW5hbA== 36387
IFBFT1BMRQ== 36388
IEJvb3Ro 36389
IEFwcGFsYWNo 36390
IHJlbGVnYXRlZA== 36391
VlQ= 36392
TUlU 36393
IG11c3Rlcg== 36394
IHdpdGhkcmF3aW5n 36395
IG1pY3Jvc2NvcGU= 36396
IEdhdGhlcmluZw== 36397
IENyZXNjZW50 36398
IEFyZ2VudGluZQ== 36399
IERlY3Jl 36400
IERvbWluaWM= 36401
IGJ1ZHM= 36402
YW50YWdl 36403
IElvbg== 36404
IHdpZGVuZWQ= 36405
T05TT1JFRA== 36406
IEdsb3Zlcw== 36407
aWFubm9wb3Vsb3M= 36408
cmF6ZW4= 36409
ZmVlbA== 36410
IHJlcGF5bWVudA== 36411
IGhpbmRzaWdodA== 36412
IFJFQUxMWQ== 36413
IFBpc3RvbA== 36414
IEJyYWg= 36415
IHdhdHRz 36416
IHN1cnZpdmVz 36417
IGZsdXJyeQ== 36418
aXNzeQ== 36419
QWxlcnQ= 36420
IFVydWd1YXk= 36421
UGhvZW5peA== 36422
U2xvdw== 36423
IEdyYXZl 36424
IEZpcg== 36425
IG1hbmFnZWFibGU= 36426
IHRhcmlmZg== 36427
IFVEUA== 36428
IFBpc3RvbnM= 36429
IE5pZ2VyaWFu 36430
IHN0cmlrZW91dHM= 36431
IGNvc21ldGljcw== 36432
d2hlbG1pbmc= 36433
ZmFi 36434
Y2FwZQ== 36435
cHJveHk= 36436
IHJldGhpbms= 36437
IG92ZXJjb21pbmc= 36438
c2ltcGxl 36439
IHdvbw== 36440
IGRpc3RyYWN0aW5n 36441
IFN0YW50b24= 36442
IFR1bHNh 36443
IERvY2s= 36444
NjU5 36445
IGRpc2NvcmQ= 36446
IEVtYWNz 36447
IFZlcw== 36448
IFJPQg== 36449
IHJlYXNzdXJpbmc= 36450
IGNvbnNvcnRpdW0= 36451
TXVzbGltcw== 36452
MzIx 36453
IHByb21wdHM= 36454
c2Vp 36455
IEhpdGNo 36456
aW1wb3NlZA== 36457
IEZvb2w= 36458
IGluZGlzY3JpbQ== 36459
d3Jvbmc= 36460
YnVxdWVycXVl 36461
RGF2aXM= 36462
IV0= 36463
IHRpbWVsZXNz 36464
IE5FRUQ= 36465
IHBlc3RpY2lkZQ== 36466
IHJhbGx5aW5n 36467
IENhbGRlcg== 36468
IOWk 36469
IHhw 36470
IFVubGU= 36471
IEV4cG9ydA== 36472
bHVhag== 36473
QnVmZg== 36474
KTwv 36475
Qm9vdA== 36476
IENocnlzbGVy 36477
b3JhdGl2ZQ== 36478
TWVzcw== 36479
IG5lZ2xpZ2libGU= 36480
ZXJ0b2Rk 36481
IE11c2hyb29t 36482
IEdhbGU= 36483
Z2M= 36484
IENvc2J5 36485
IFJ1cmFs 36486
cml0aWNhbA== 36487
QmVsbA== 36488
IHR1cmJpbmU= 36489
MDAyMDAwMDA= 36490
IGxlZ2l0aW1hdGVseQ== 36491
IEFuaW1hdGVk 36492
VEVE 36493
IFRoZW9kb3Jl 36494
Y29uZHVjdA== 36495
IEhpZXI= 36496
IGNvdW50ZXJmZWl0 36497
IEFsZ2VyaWE= 36498
IHVuYmVhdA== 36499
Y29udHJvbGxlcg== 36500
IHVucmVz 36501
IHNjcmFtYmxpbmc= 36502
IEZhbGxvbg== 36503
VGVz 36504
IGFtYmVy 36505
IHJveWFsdGllcw== 36506
IFNoZWx0ZXI= 36507
IExlc3Rlcg== 36508
IGNsYXNzaWZ5 36509
UmVtb3Rl 36510
IHVuaGVhcmQ= 36511
IGNvbnRyb3ZlcnNpZXM= 36512
IGVucmljaG1lbnQ= 36513
IFlhbmtlZQ== 36514
Z2FtZXI= 36515
IHBsYXRpbnVt 36516
IGVjb2xvZ3k= 36517
IFNhcms= 36518
IHVudG91Y2hlZA== 36519
IHN1cGVydmlzb3Jz 36520
ICIl 36521
IGZvb3Ro 36522
IGNvbW1vbnM= 36523
IG5hcmNvdGljcw== 36524
IGluZGljZXM= 36525
IFBseQ== 36526
IGFkZGl0aW9uYWxseQ== 36527
IEdhd2tlcg== 36528
IEVR 36529
UGxheWluZw== 36530
IGNhdmVhdA== 36531
IEFic29sdXRl 36532
b3NzdXM= 36533
QmFieQ== 36534
IHJhdGlvbg== 36535
IHJlc2lu 36536
IGNhbGlicmF0aW9u 36537
IE5ld3BvcnQ= 36538
IGtub2Nrcw== 36539
dnQ= 36540
IGNvbXBvc3Q= 36541
U2NlbmU= 36542
IHNhcmNhc3Q= 36543
IGtpc3Nlcw== 36544
IG5z 36545
YWxsaQ== 36546
IE1hcmNlbA== 36547
IFBpZXQ= 36548
aWF0cmljcw== 36549
IHN1cnJvdW5kcw== 36550
IFJlcHJvZHU= 36551
IFBoaWxsaWVz 36552
IHVuY2VydGFpbnRpZXM= 36553
IEV1cg== 36554
IFJvbWFuY2U= 36555
IEhhdGg= 36556
IE5lZWRz 36557
IENsb2Fr 36558
IGNyZW0= 36559
cXVldWU= 36560
IDM1NQ== 36561
IHVwZnJvbnQ= 36562
XSk7 36563
IHJlY2lwcm9j 36564
IDE5Mjc= 36565
IDExMDA= 36566
dXRzdQ== 36567
IGRlcHJlc3NpdmU= 36568
b3dtZW50 36569
RmFucw== 36570
IG1lY2g= 36571
IGFubmloaWw= 36572
IGNvdW50ZXJ0ZXJyb3Jpc20= 36573
IEZpZ3VyZXM= 36574
Ym9sZA== 36575
IE1vaW5lcw== 36576
IERyaXZlcnM= 36577
IG1hbnVzY3JpcHRz 36578
IENyeXB0bw== 36579
IGh5cG5vdA== 36580
cmVkZGl0cw== 36581
IHByb3NlY3V0aW9ucw== 36582
IGRpdmVydA== 36583
Q1JJUA== 36584
IEJlbmU= 36585
IFJlZ2dpZQ== 36586
IHRheGluZw== 36587
IE1vcmFsZXM= 36588
ZW50aW5n 36589
dHVy 36590
c2lnbmlmaWNhbnQ= 36591
IFBST1Y= 36592
IHN0cmFuZHM= 36593
IHBvdWNo 36594
IFJvb2tpZQ== 36595
u5I= 36596
IG5pY2Vy 36597
aGVteQ== 36598
aHc= 36599
RUNB 36600
IGludGltaWRhdGVk 36601
IHN0cmljdGVy 36602
IG1pY3JvYmlhbA== 36603
ZGV0YWlscw== 36604
IHZvd3M= 36605
IHF1YWtl 36606
aGhoaA== 36607
IHJlaW52ZW50 36608
VWI= 36609
IHJlbGlucXU= 36610
IEJ1ZmZldHQ= 36611
bGljZW5zZWQ= 36612
aXR0ZXJlZA== 36613
IFBpY2FyZA== 36614
IGNoZXdpbmc= 36615
dWNs 36616
b3JnYW5pYw== 36617
IGxvY2FsaXplZA== 36618
IEVjb25vbWlzdA== 36619
IGFjcXVhaW50ZWQ= 36620
RGVmaW5pdGlvbg== 36621
c2Vk 36622
Q3JpdGljcw== 36623
IGNj 36624
NDUz 36625
Mzgx 36626
IGZlbGxvd3M= 36627
IGNoZWNrcG9pbnRz 36628
MDI1 36629
IHJlZWxlY3Rpb24= 36630
IG1lZGlhdGVk 36631
IEtERQ== 36632
IGh1cmRsZQ== 36633
IHRleHRpbmc= 36634
UGVyZmVjdA== 36635
IHRydXN0ZWVz 36636
ZmVjdHVyZQ== 36637
IGRpY2g= 36638
bW9uYXJ5 36639
IGRpc3RpbmN0aW9ucw== 36640
IDE0MDA= 36641
IHVzaGVy 36642
IHBhcmFzaXRlcw== 36643
IFNoYXJpbmc= 36644
IFZpbQ== 36645
IGJhcmJlY3Vl 36646
IE1pbmlzdGVycw== 36647
ZXJlbGxh 36648
IGVi 36649
IG1j 36650
IFNvbWVob3c= 36651
IEluc2VjdA== 36652
Y2hhbmdlcw== 36653
YnJvYWQ= 36654
IEJ5eg== 36655
IGdyYXBlcw== 36656
NjY5 36657
ID09PT09PT09PT09PT09PT09 36658
IGFzc2ltaWw= 36659
IGhhdW50aW5n 36660
IGZpcmVwb3dlcg== 36661
IGRlZmFtYXRpb24= 36662
ZW1waGFzaXM= 36663
IGNvbXBvc2U= 36664
IGFsbGVyZ2llcw== 36665
IHN0cmFuZw== 36666
cm9sbGVycw== 36667
YmFuZw== 36668
IGJyZXdlcnM= 36669
cm9uZ2g= 36670
cmlvdA== 36671
cG9vcg== 36672
Y29sZA== 36673
U2FtcGxl 36674
IGJ1b3k= 36675
MDQw 36676
IENvdXJ0bmV5 36677
IDI2OA== 36678
IFdlZGRpbmc= 36679
NzAy 36680
IG9ic2Vzc2l2ZQ== 36681
IGJyYWtpbmc= 36682
IExhbA== 36683
YW5pY2Fs 36684
5aY= 36685
YXRlbg== 36686
Q29uc3RydWN0aW9u 36687
IGNsaW5pY2FsbHk= 36688
aWVyc2hpcA== 36689
TmFtZXM= 36690
IERpc2N1c3M= 36691
IFJhbW9z 36692
IGxvY2FsZQ== 36693
IEFncmljdWx0dXJhbA== 36694
RW5hYmxl 36695
IGhvcnNlcG93ZXI= 36696
ZW50dXJl 36697
UHJlZg== 36698
Q291cnQ= 36699
IHN0YWZmaW5n 36700
IGZ1dHVyaXN0aWM= 36701
ZHJpdmVycw== 36702
IE1hcmtldHBsYWNl 36703
5oim 36704
RnJpZW5kcw== 36705
IGRhbW5pbmc= 36706
IEN1c3RvbWVycw== 36707
IHdlZWRz 36708
IE1haQ== 36709
IGFnaWxl 36710
IFRhdHQ= 36711
aWNlbnQ= 36712
UmFua2Vk 36713
Y3JvZnQ= 36714
IEthdHk= 36715
RXh0cmVtZQ== 36716
IGNhcnZl 36717
IFJvdmVy 36718
IEJ5cm9u 36719
Mzcy 36720
IGNvbmR1Y3Rz 36721
cmF0Y2g= 36722
aXRpYQ== 36723
IFB1bXBraW4= 36724
U2FkbHk= 36725
UmVsb2FkZWQ= 36726
UG9saWN5 36727
IGxpY2s= 36728
cGVhaw== 36729
aXNrcw== 36730
IENEcw== 36731
IEVuY3ljbG9wZWRpYQ== 36732
aW5pdGlhbA== 36733
Q29z 36734
IEF3YXJlbmVzcw== 36735
IERyYW0= 36736
JCQkJA== 36737
IHJpZmY= 36738
IHNjcmlwdHVyZQ== 36739
cnVubmVycw== 36740
IGJvaWxlcg== 36741
b25zb24= 36742
b2lu 36743
IGhhbXN0cmluZw== 36744
IGNhdGFseQ== 36745
IEFyY2hiaXNob3A= 36746
Y2hhbGw= 36747
IGZhdXg= 36748
b2tpbg== 36749
bG9jYWxob3N0 36750
IE5BTUU= 36751
YWRvYmU= 36752
U0FO 36753
YW1hdGU= 36754
IHNjcmFtYmxl 36755
IGNhcmM= 36756
IE1hbmlmZXN0 36757
IENlZGFy 36758
IFNlcmdpbw== 36759
bGF0ZXI= 36760
ZmZlcg== 36761
IGdyYXBwbGluZw== 36762
IERldXRzY2hl 36763
YWdvbmlzdHM= 36764
IE5ld3Nw 36765
IHByZXRlbmRlZA== 36766
YXJjaG1lbnQ= 36767
IGN1cmF0ZWQ= 36768
IGhlYWRwaG9uZQ== 36769
IFVuY29tbW9u 36770
IFNJR04= 36771
QWdlbnQ= 36772
IGRlYWRsaW5lcw== 36773
IGhvcml6b250YWxseQ== 36774
IE1BVA== 36775
IFN1bW1lcnM= 36776
IG9yZGFpbmVk 36777
IExhc3RseQ== 36778
IEtlbmRhbGw= 36779
IGZyaWc= 36780
IE1hY2hpbmE= 36781
IFdhdGVybG9v 36782
IE1leGljYW5z 36783
IHByb3RlY3Rvcg== 36784
IGdsYXJl 36785
fSI= 36786
UHJlbWl1bQ== 36787
IHJpZnQ= 36788
IFRlbGVzY29wZQ== 36789
TWV0YWw= 36790
IHJlY2FwdA== 36791
IDs7 36792
IGluY2xpbmF0aW9u 36793
IGltcG9zZXM= 36794
aW5nZW4= 36795
Xns= 36796
IGhhc3Rl 36797
IGRvbHBoaW5z 36798
IGNvbW11dGVycw== 36799
cGxhbm5lZA== 36800
Y29uZw== 36801
bXg= 36802
IFVwbG9hZA== 36803
IGV4dHJhcA== 36804
IFR1Y3Nvbg== 36805
IEV4cGxvcmF0aW9u 36806
ZWZlYXRlZA== 36807
IHNsZW5kZXI= 36808
NzAz 36809
IEJ1aw== 36810
aXNlbA== 36811
IGNvbXBldGl0aXZlbmVzcw== 36812
Y2hsb3I= 36813
IFBlcm1hbmVudA== 36814
IEV2ZXJldHQ= 36815
IFNwZWNpYWxpc3Q= 36816
IFNPTA== 36817
IGN5YW4= 36818
IEV4YWN0bHk= 36819
VUY= 36820
IExJRkU= 36821
YXJ5bA== 36822
b25ldA== 36823
IEVtcGxveWVl 36824
YXdlZA== 36825
IFJhdGluZ3M= 36826
IGV4dHJhdmFn 36827
dWxodQ== 36828
IFBsYW5l 36829
IGVsZXZhdGU= 36830
IENvb3JkaW5hdG9y 36831
IFdhdGtpbnM= 36832
IGV4Y2x1ZGVz 36833
IHNlbnRpZW50 36834
IGVwb2No 36835
IGFsbG9j 36836
UHJldmlvdXNseQ== 36837
IFNoeQ== 36838
IFNsb3Zha2lh 36839
TE9DSw== 36840
IG1hcmtlZGx5 36841
IGtub2I= 36842
IGFkdmVudHVyZXJz 36843
IEJlZW4= 36844
IENvc3Rz 36845
YW1tZXJz 36846
IG9uc2xhdWdodA== 36847
IFN1cHBvcnRlZA== 36848
IFRhdQ== 36849
aWthcnA= 36850
IFNvdmVyZQ== 36851
IEhhbXB0b24= 36852
44KJ 36853
UHJldg== 36854
IFdvcnNl 36855
IGNvdHRhZ2U= 36856
IEhhZGVz 36857
bGV6 36858
Ym93bA== 36859
IGZyYWdyYW5jZQ== 36860
IExvaw== 36861
RU1PVEU= 36862
IFBldHJv 36863
IDE5MjU= 36864
IFBlbmQ= 36865
cHJvZHVjaW5n 36866
IHJlbG9jYXRl 36867
dmF0aQ== 36868
cG9sZQ== 36869
IHNlbWlu 36870
IE5VTQ== 36871
IHJvY2tlZA== 36872
YnVmZg== 36873
Ymx5 36874
UmVwbHk= 36875
IEhhaQ== 36876
IGFydGljdWxhdGVk 36877
IElzbGFtYWJhZA== 36878
NjY1 36879
IENsYWltcw== 36880
RGVza3RvcA== 36881
IHRydXN0ZWU= 36882
IHNjcmlwdGluZw== 36883
IFNvYg== 36884
IEFzeWx1bQ== 36885
U1RET1VU 36886
IENsb3du 36887
IERvcnRtdW5k 36888
IERldm9u 36889
bGl0ZQ== 36890
IE1hcmJsZQ== 36891
IGJ1bmtlcg== 36892
IGNyZXN0 36893
IGFyb3VzYWw= 36894
IFNlYXJz 36895
IEJ1ZGR5 36896
ZXJlZGl0aA== 36897
IFBvbGx5 36898
IGRlY29kZQ== 36899
IFZpc2g= 36900
IFJlZmxlY3Q= 36901
YW5vbg== 36902
IHJlZnVuZHM= 36903
aW1tZXJz 36904
SE0= 36905
IHdpcGluZw== 36906
IHB1enpsZWQ= 36907
IG1hdHRl 36908
dW5v 36909
UGllcnJl 36910
KSks 36911
IHRhaW50ZWQ= 36912
IHN5bWJvbGlzbQ== 36913
IEZyYXo= 36914
IHByb3Rlc3RvcnM= 36915
ZXRoZXVz 36916
JSUlJQ== 36917
V3Jh 36918
IGxheA== 36919
YWRlbQ== 36920
YXR1cmF0aW9u 36921
44OT 36922
IFRyYWlsZXI= 36923
IEVORw== 36924
IEJvd3Nlcg== 36925
IGF0dG0= 36926
RHVy 36927
ODA3 36928
IHNpZHg= 36929
IGNpZGVy 36930
IEFmZmVjdA== 36931
IHdvdmVu 36932
IEJhcmtlcg== 36933
YmVuZWY= 36934
IGRzdGc= 36935
IFJ5dQ== 36936
Pls= 36937
IHNxb3I= 36938
U2F1ZGk= 36939
IGlzdGc= 36940
IGluZHVsZ2U= 36941
cHJvYw== 36942
IGRpc2d1c3RlZA== 36943
IGNvbXBvdW5kZWQ= 36944
IG5lbQ== 36945
IHNjaG9vbGluZw== 36946
IEN1cmU= 36947
cHJvY2Vzc2luZw== 36948
U29s 36949
IHByb3ZlcmI= 36950
aXRpemVk 36951
IEFsdmFyZXo= 36952
IHNjYXJm 36953
IHJlY3Rhbmd1bGFy 36954
cmV2ZQ== 36955
IGhvcm1vbmFs 36956
IFN0cmVzcw== 36957
aXRpemVu 36958
IDQyNQ== 36959
Z2lybHM= 36960
IE5vaXI= 36961
IFJhcHA= 36962
IG1hcmNoZXM= 36963
Y2h1cmNo 36964
IFVzZXM= 36965
IDQwNQ== 36966
IEJlcm0= 36967
IG9yZGluYW5jZXM= 36968
IEp1ZGdtZW50 36969
Q2hhcmdlcw== 36970
IFppbg== 36971
IGR1c3R5 36972
IHN0cmF3YmVycmllcw== 36973
IHBlcmNl 36974
IFRodXI= 36975
IERlYm9yYWg= 36976
bmV0ZmxpeA== 36977
IExhbWJlcnQ= 36978
IGFtdXNlZA== 36979
IEd1YW5n 36980
WU9V 36981
UkdC 36982
IENDVFY= 36983
IGZpYXQ= 36984
cmFuZw== 36985
IGZlZGVyYXRpb24= 36986
IE1hbnQ= 36987
IEJ1c3Q= 36988
IE1hcmU= 36989
cmVzcGVjdGl2ZQ== 36990
IE1pZ3JhdGlvbg== 36991
IEJJVA== 36992
NTkw 36993
IHBhdHJpb3Rpc20= 36994
IG91dGxpbmluZw== 36995
cmVnaW9u 36996
IEpvc8Op 36997
IGJsYXN0aW5n 36998
IEV6cmE= 36999
QnM= 37000
IHVuZGVybWluZXM= 37001
IFNtb290aA== 37002
IGNsYXNoZWQ= 37003
cmFkaW8= 37004
IHRyYW5zaXRpb25pbmc= 37005
IEJ1Y2NhbmVlcnM= 37006
IE93bA== 37007
IHBsdWdz 37008
IGhpYXR1cw== 37009
IFBpbmJhbGw= 37010
IG1pZw== 37011
IE51dHI= 37012
IFdvbGZl 37013
IGludGVnZXJz 37014
IG9yYml0cw== 37015
IEVkd2lu 37016
IERpcmVjdFg= 37017
Yml0ZQ== 37018
IGJsYXppbmc= 37019
dnI= 37020
RWRnZQ== 37021
IFBJRA== 37022
ZXhpdA== 37023
IENvbWVk 37024
IFBhdGhmaW5kZXI= 37025
IEd1aWQ= 37026
IFNpZ25z 37027
IFplcg== 37028
IEFnZW5kYQ== 37029
IHJlaW1idXJzZW1lbnQ= 37030
TWVzaA== 37031
aVBob25l 37032
IE1hcmNvcw== 37033
IFNpdGVz 37034
aGF0ZQ== 37035
ZW5idXJn 37036
IHNvY2tldHM= 37037
cGVuZA== 37038
QmF0bWFu 37039
dmly 37040
IFNIT1c= 37041
IHByb3Zpc2lvbmFs 37042
Y29ubg== 37043
IERlYXRocw== 37044
QVRJVkU= 37045
UHJvZmlsZQ== 37046
c3lt 37047
SkE= 37048
IG5pbmph 37049
aW5zdGFsbGVk 37050
aWRhdGVz 37051
ZWJyYQ== 37052
IE9tYWhh 37053
IHNlaXppbmc= 37054
IEJlYXN0cw== 37055
IHNhbHRz 37056
TWlzc2lvbg== 37057
R2VuZXJhbGx5 37058
IFRyaWxvZ3k= 37059
aGVvbg== 37060
bGVnYXRlcw== 37061
IGRpbWU= 37062
IGZhaXJl 37063
cGFyYWJsZQ== 37064
R3JhcGg= 37065
IHRvdGFsaW5n 37066
IGRpYWdyYW1z 37067
IFlhbnVr 37068
cGxldA== 37069
IE1laA== 37070
IG15dGhpY2Fs 37071
IFN0ZXBoZW5z 37072
YXV0aWNhbA== 37073
b2NoZW1pc3RyeQ== 37074
IGtpbG9ncmFtcw== 37075
IGVsYm93cw== 37076
YW5jb2Nr 37077
IEJDRQ== 37078
IFByYWd1ZQ== 37079
IGltcHJvdg== 37080
IERldmlu 37081
ICJc 37082
cGFyYWxsZQ== 37083
IHN1cHJlbWFjaXN0cw== 37084
IEJpbGxpb24= 37085
IHJlZ2ltZW4= 37086
aW5uYWNsZQ== 37087
IHJlcXVpc2l0ZQ== 37088
YW5nYW4= 37089
IEJ1cmxpbmd0b24= 37090
YWlubWVudA== 37091
IE9iamVjdGl2ZQ== 37092
b21za3k= 37093
R1Y= 37094
IHVuaWxhdGVyYWw= 37095
IHRj 37096
IGhpcmVz 37097
bWVudGFs 37098
IGludm9sdW50YXJ5 37099
IHRyYW5zcGw= 37100
IEFTQ0lJ 37101
wqg= 37102
RXZlbnRz 37103
IGRvdWJ0ZWQ= 37104
IEthcGxhbg== 37105
IENvdXJhZ2U= 37106
aWdvbg== 37107
IE1hbmFnaW5n 37108
IFRhcnQ= 37109
IGZhbHNlaG9vZA== 37110
IFZpb2xldA== 37111
IGFpcnM= 37112
IGZlcnRpbGl6ZXI= 37113
QnJpdGFpbg== 37114
IGFxdWF0aWM= 37115
b3Vm 37116
V29yZHM= 37117
IEhhcnRmb3Jk 37118
IGV2ZW5pbmdz 37119
IFZlbmdlYW5jZQ== 37120
cXVpdGU= 37121
R2FsbA== 37122
IFByZXQ= 37123
IHBkZg== 37124
IExN 37125
IFNvY2hp 37126
IEludGVyY2VwdA== 37127
OTIw 37128
IHByb2ZpdGFiaWxpdHk= 37129
IElkbGU= 37130
IE1hY0RvbmFsZA== 37131
IEVzdGFibGlzaG1lbnQ= 37132
dW1zeQ== 37133
IGdhdGhlcmluZ3M= 37134
IE5hag== 37135
Q2hhcmxpZQ== 37136
IGFzY2VudA== 37137
IFByb3RlY3Rvcg== 37138
IGFsZ2VicmE= 37139
IGJpb3M= 37140
Zm9ydW1z 37141
RUxT 37142
SW50cm9kdWNlZA== 37143
IDMzNQ== 37144
IGFzdHJvbm9teQ== 37145
Q29udHJpYnV0 37146
IFBvbGlj 37147
UGxhdGZvcm0= 37148
IGNvbnRhaW5tZW50 37149
d3JhcA== 37150
IGNvcm9uYXJ5 37151
IEplbGx5 37152
bWFuYWdlcg== 37153
IGhlYXJ0YnJlYWtpbmc= 37154
Y2Fpcg== 37155
IENoZXJv 37156
Y2dp 37157
TWVkaWNhbA== 37158
IEFjY291bnRhYmlsaXR5 37159
ISEi 37160
b3BoaWxl 37161
IHBzeWNob3RpYw== 37162
IFJlc3RyaWN0 37163
IGVxdWl0YWJsZQ== 37164
aXNzdWVz 37165
IDE5MDU= 37166
IE5law== 37167
Y2lzZWQ= 37168
IFRyYWNraW5n 37169
IG96b25l 37170
IGNvb2tlcg== 37171
cm9zaXM= 37172
IHJlb3Blbg== 37173
IGluZmluaXR5 37174
IFBoYXJtYWNldXRpY2Fs 37175
ZW5zaW9uYWw= 37176
QXR0ZW1wdA== 37177
IFJvcnk= 37178
TWFyY28= 37179
IGF3YWl0cw== 37180
SE9X 37181
dHJlYXRlZA== 37182
IGJvbHN0 37183
IHJldmVyZWQ= 37184
IHBvZHM= 37185
b3BwZXJz 37186
MDAxMA== 37187
IGFtcGxpdHVkZQ== 37188
cmljYW4= 37189
U1BPTlNPUkVE 37190
IHRyb3VzZXJz 37191
IGhhbHZlcw== 37192
IEthaW5l 37193
IEN1dGxlcg== 37194
IEFVVEg= 37195
IHNwbGVuZGlk 37196
IHByZXZlbnRpdmU= 37197
IER1ZGxleQ== 37198
aWZhY3Rz 37199
dW1pbmF0aQ== 37200
IFlpbg== 37201
IGFkbW9u 37202
IFZhZw== 37203
IGludmVydGVk 37204
IGhhc3RpbHk= 37205
IEhhZ3Vl 37206
THlu 37207
IGxlZGdlcg== 37208
IGFzdHJvbm9taWNhbA== 37209
Z2V0dGluZw== 37210
IGNpcmNh 37211
IENpYw== 37212
IFRlbm5pcw== 37213
TGltaXRlZA== 37214
IGRydQ== 37215
IEJZVQ== 37216
IHRyYXZlbGxlcnM= 37217
IHBhbmU= 37218
IEludHJv 37219
IHBhdGllbnRseQ== 37220
IGFpZGluZw== 37221
IGxvb3M= 37222
IFRvdWdo 37223
IDI5Mw== 37224
IGNvbnN1bWVz 37225
U291cmNlRmlsZQ== 37226
ICIiIg== 37227
IGJvbmRpbmc= 37228
IHRpbHRlZA== 37229
IG1lbnN0cnVhbA== 37230
IENlbGVzdGlhbA== 37231
VUxBUg== 37232
UGx1Z2lu 37233
IHJpc2tpbmc= 37234
TmF6 37235
IFJpeWFkaA== 37236
IGFjY3JlZGl0ZWQ= 37237
IHNraXJt 37238
6Zs= 37239
IGV4YW1pbmVy 37240
IG1lc3Npbmc= 37241
IG5lYXJpbmc= 37242
IENoZXJu 37243
IEJlY2toYW0= 37244
IHN3YXBwZWQ= 37245
IGdvb3Nl 37246
S2F5 37247
IGxvZnR5 37248
IFdhbGxldA== 37249
IFsn 37250
IGFwb2NhbHlwc2U= 37251
IGJhbWJvbw== 37252
IFNQQUNF 37253
IEVsZW5h 37254
IDMwNg== 37255
YWNvbnM= 37256
IHRpZ2h0ZW5lZA== 37257
IGFkb2xlc2NlbmNl 37258
IHJhaW55 37259
IHZhbmRhbGlzbQ== 37260
IE5ld3Rvd24= 37261
IGNvbmplY3Q= 37262
Y2FrZXM= 37263
IGNoZWF0ZWQ= 37264
IG1vZGVyYXRvcnM= 37265
cGFyYW1z 37266
RUZG 37267
IGRlY2VpdA== 37268
IFNUTA== 37269
IFRhbnphbmlh 37270
IFJJ 37271
IDE5MjM= 37272
IEV4aWxl 37273
dGhlbA== 37274
IHRoZW9sb2c= 37275
IHF1aXJreQ== 37276
IElydmluZQ== 37277
IG5lZWR5 37278
b3Jpcw== 37279
VW0= 37280
S2E= 37281
IG1haWxib3g= 37282
MzIy 37283
IGJvcw== 37284
IFBldHJh 37285
S0lORw== 37286
IGVubGFyZ2Vk 37287
T2Z0ZW4= 37288
IGJhZGFzcw== 37289
IDM0Mw== 37290
IFBsYWNlcw== 37291
IENBRA== 37292
IHByaXN0aW5l 37293
IGludGVydmVuaW5n 37294
ZGlyZWN0aW9u 37295
IGxheg== 37296
IERTTQ== 37297
IHByb2plY3Rpbmc= 37298
IEZ1bms= 37299
YWdvZw== 37300
cGF5bWVudA== 37301
bm92 37302
IGNoYXR0ZXI= 37303
QVJC 37304
IGV4YW1pbmF0aW9ucw== 37305
IEhvdXNlaG9sZA== 37306
IEd1cw== 37307
Rm9yZA== 37308
NDE0 37309
Qm9zcw== 37310
IG15c3RpYw== 37311
IGxlYXBz 37312
IEJhdg== 37313
dWx6 37314
YnVkZ2V0 37315
Rm9vdGJhbGw= 37316
IHN1YnNpZGl6ZWQ= 37317
IGZpcnN0aGFuZA== 37318
IGNvaW5jaWRl 37319
b2N1bGFy 37320
Q29ubg== 37321
IENvbGxhYm9y 37322
IGZvb2xz 37323
YW11cmE= 37324
YWhhcg== 37325
cmlzdHM= 37326
IHN3b2xsZW4= 37327
IGV4cGVuZGVk 37328
IFBhdQ== 37329
c3Vw 37330
IHNwYXI= 37331
IGtleW5vdGU= 37332
c3VmZg== 37333
IHVuZXF1YWw= 37334
IHByb2dyZXNzaW5n 37335
c3RyaW5ncw== 37336
IEdhbWVyZ2F0ZQ== 37337
RGlzbmV5 37338
IEVsZXZlbg== 37339
b21uaWE= 37340
IHNjcmlwdGVk 37341
IGVhcm5lcnM= 37342
YnJvdGhlcg== 37343
IEVuYWJsZWQ= 37344
5rM= 37345
IGxhcnZhZQ== 37346
IExPQw== 37347
bWVzcw== 37348
V2lsc29u 37349
IFRlbXBsYXRl 37350
c3VjY2Vzc2Z1bGx5 37351
IHBhcmFtb3VudA== 37352
IGNhbW91ZmxhZ2U= 37353
IGJpbmRz 37354
IFF1aWV0 37355
IFNodXR0ZXJzdG9jaw== 37356
cnVzaA== 37357
IG1hc2NvdA== 37358
Zm9ydHVuZQ== 37359
IENvbHQ= 37360
IEJleW9u 37361
aGFiaQ== 37362
IGhhaXJj 37363
IDI2Nw== 37364
IERldXM= 37365
IHR3aXRjaA== 37366
IGNvbmNlbnRyYXRpbmc= 37367
IG5pcHBsZXM= 37368
Y2libGU= 37369
IGdpcg== 37370
Tlo= 37371
TWF0aA== 37372
bmlo 37373
UmVxdWlyZWQ= 37374
IHBvbmRlcg== 37375
IFNBTg== 37376
IHdlZGRpbmdz 37377
IGxvbmVsaW5lc3M= 37378
TkVT 37379
IE1haGpvbmc= 37380
Njk1 37381
YWRkbGU= 37382
IEdhcm5lcg== 37383
IENPVVI= 37384
QnJpZGdl 37385
IHNwcmVl 37386
IENhbGR3ZWxs 37387
IGJyaWJlcnk= 37388
IO+/ve+/ve+/ve+/ve+/ve+/ve+/ve+/vQ== 37389
cGx1Z2lucw== 37390
IHJhY2tldA== 37391
IGNoYW1wYWduZQ== 37392
dmVyc2libGU= 37393
Vm90ZQ== 37394
IG1vZGlmaWVycw== 37395
TWF5b3I= 37396
Njgw 37397
IGFzc2VtYmxpZXM= 37398
IFN1bHRhbg== 37399
IE5pbmc= 37400
IExhZGllcw== 37401
IHN1bGZ1cg== 37402
IG9yYnM= 37403
IC0tLS0t 37404
X19fX19fXw== 37405
IEpvdXJuYWxpc20= 37406
IGVzcG9ydHM= 37407
IGx1c2g= 37408
IGh1ZQ== 37409
IHNwZWN0cmFs 37410
SG9uZXN0 37411
44OP 37412
IGJ1c2hlcw== 37413
IHJlaW5mb3JjZW1lbnQ= 37414
IHJlb3BlbmVk 37415
IFdoZWVscw== 37416
IE1vcmc= 37417
cmlldmluZw== 37418
IGF1eGlsaWFyeQ== 37419
IGpRdWVyeQ== 37420
IEJBVA== 37421
dGVzcXVl 37422
IHZlcnRleA== 37423
cHVyZQ== 37424
ZnJleQ== 37425
44K6 37426
ZG9z 37427
IHR5cGg= 37428
IGN1bGw= 37429
IGVx 37430
IGRlY29u 37431
IHRvc3Npbmc= 37432
IGRpc3BhcmF0ZQ== 37433
IEJyaWdoYW0= 37434
cHJpbnRm 37435
bGVkZ2Vk 37436
IHN1bmQ= 37437
IGNvenk= 37438
IGhlcGF0aXRpcw== 37439
cGVyZm9ybWluZw== 37440
IGF2YWw= 37441
IEdH 37442
ZnV0dXJl 37443
IHBldGVydG9kZA== 37444
IEtvc292bw== 37445
IG1hZ25ldHM= 37446
QWxyZWFkeQ== 37447
IEVkaXNvbg== 37448
IENlcmVz 37449
IFJBSUQ= 37450
IGJyaWxsaWFuY2U= 37451
NTc2 37452
IGRlcml2ZXM= 37453
IGh5cGVydGVuc2lvbg== 37454
IM6U 37455
IGxhbWJkYQ== 37456
IGZsYWly 37457
IG1pc3Npb25hcmllcw== 37458
IHJhcGVz 37459
IFN0YXJ0ZXI= 37460
IE1vbnRocw== 37461
IGRlZnk= 37462
IHNlaXNtaWM= 37463
IFJhcGhhZWw= 37464
IGV1cm96b25l 37465
NjU2 37466
enNjaGU= 37467
IHNjcmF0Y2hlZA== 37468
IGJvd3M= 37469
IExlbm5vbg== 37470
IEdhaWE= 37471
IGRyaXBwaW5n 37472
ZmFjdHM= 37473
QWxl 37474
IGZyb2dz 37475
IEJyZWFzdA== 37476
b2dlbmVpdHk= 37477
IFByb3NlY3V0b3I= 37478
IGFtcGxpZmllZA== 37479
IEhvZGc= 37480
IEZu 37481
VGhvdXNhbmRz 37482
IE5JSA== 37483
IE1vbml0b3Jpbmc= 37484
RlRXQVJF 37485
IFByaWVidXM= 37486
IEdyb3dpbmc= 37487
aHVudGVy 37488
IGRpYWdub3Nl 37489
IE1hbGQ= 37490
IExS 37491
IGNyb3duZWQ= 37492
IGJ1cnN0aW5n 37493
IGRpc3NvbHV0aW9u 37494
amF2YXNjcmlwdA== 37495
IHVzZWZ1bG5lc3M= 37496
IEV4ZWN1dGlvbg== 37497
Oig= 37498
IEl2b3J5 37499
YWFo 37500
IHBlcnNlY3V0ZWQ= 37501
dmlvbGVuY2U= 37502
aXN0YXM= 37503
IENyYXRl 37504
IGltcHVsc2Vz 37505
IFNwYW5p 37506
ZWRlcw== 37507
SGFuZGxl 37508
IFplcmc= 37509
dGhpbmthYmxl 37510
TGFzdGx5 37511
IHNwb250YW5lb3VzbHk= 37512
IGluY29udmVuaWVudA== 37513
IGRpc21pc3Npbmc= 37514
IHBsb3R0ZWQ= 37515
IGVpZ2h0eQ== 37516
IDczNw== 37517
cmlzaA== 37518
IFRob3JudG9u 37519
YXRoYW0= 37520
IHNpdGNvbQ== 37521
VmVu 37522
UmVjaXBl 37523
dGVs 37524
bHVuZA== 37525
IGNsZWFycw== 37526
IFNhc3VrZQ== 37527
IDI1OA== 37528
IG9wdGluZw== 37529
IGVucmFnZWQ= 37530
ZXN0aGV0aWM= 37531
IEFl 37532
dWNocw== 37533
UHJlcA== 37534
Rmxvdw== 37535
IHJ1bm9mZg== 37536
IEVhdGluZw== 37537
IEdpbGVz 37538
IEFjdGluZw== 37539
cmVzb3VyY2Vz 37540
aWJhYmE= 37541
IHJwbQ== 37542
IHNrZXdlZA== 37543
IEJsYW5j 37544
IFNha3V5YQ== 37545
IGhvdHRlcg== 37546
IDE5MjQ= 37547
b3BpYW4= 37548
Y2tv 37549
IGNydW1ibGluZw== 37550
IGNhcHRhaW5z 37551
IEFwcHJvcHJpYXRpb25z 37552
bGVhZGVycw== 37553
ZHJvcHBpbmc= 37554
YW51dHM= 37555
IHJldmVyc2luZw== 37556
IFBvc2U= 37557
IFNlaw== 37558
U2NvdA== 37559
IElkZWE= 37560
Y2lzZQ== 37561
IFNsb3Zlbmlh 37562
IDMxNw== 37563
RG9jdG9y 37564
IGNyb2NvZA== 37565
YWxkaQ== 37566
U2Vh 37567
IEZhcnJlbGw= 37568
IG1lcmNlbmFyaWVz 37569
IFJOQw== 37570
IEd1ZXNz 37571
IHBhY2luZw== 37572
TWFjaGluZQ== 37573
U3RyZWFtZXJCb3Q= 37574
IENoYXJpdHk= 37575
IDI5OA== 37576
IGNhbm5vbnM= 37577
IFRvYnk= 37578
VFBQU3RyZWFtZXJCb3Q= 37579
IFBhc3Npb24= 37580
Y2Zn 37581
VGhvbQ== 37582
IGJhZGdlcw== 37583
IEJlcm5zdGVpbg== 37584
LuKAkw== 37585
IFBPUA== 37586
IENvbmo= 37587
IGluaXRpYWxpemF0aW9u 37588
IGJpb2RpdmVyc2l0eQ== 37589
RHVi 37590
IGZldWRhbA== 37591
IGRpc2NsYWltZXI= 37592
IGNyb3c= 37593
IGlnbml0aW9u 37594
YXJm 37595
U0hB 37596
IGtIeg== 37597
aGF6YXJk 37598
IEFydGlzdHM= 37599
b2V1dg== 37600
Njc5 37601
IFJ1ZHk= 37602
TmluZQ== 37603
IFJhbWFkYW4= 37604
5b0= 37605
aXR0bw== 37606
IGFkcmVuYWxpbmU= 37607
Q2VydA== 37608
IHNtZWxsZWQ= 37609
IGltcHVuaXR5 37610
IGFnZW5kYXM= 37611
IFJlYm9ybg== 37612
IENvbmNlbnQ= 37613
IFNlZW1z 37614
IG9tZWdh 37615
IER1c3Rpbg== 37616
IGJhY2tlcg== 37617
IFNhdWNl 37618
IEJveWxl 37619
V0lO 37620
IHNwaW5z 37621
IHBhdXNlcw== 37622
dXB0 37623
IHNocmVkZGVk 37624
IHN0cmFwcGVk 37625
IENvcnJ1cHRpb24= 37626
IHNjcmF0Y2hlcw== 37627
IG5p 37628
IGF0dGlyZQ== 37629
IFNBRg== 37630
RmFjdG9yeVJlbG9hZGVk 37631
IElQUw== 37632
ICgl 37633
IHNlbWluYXI= 37634
Zm9jdXM= 37635
Y2l2aWw= 37636
IDE4NjA= 37637
aW50b3No 37638
IGNvbnRpbnVhbA== 37639
IGFiYnJldmk= 37640
IFNvaw== 37641
b2NvYm8= 37642
WE0= 37643
IGZyYW50aWM= 37644
IHVuYXZvaWRhYmxl 37645
IGFydGVyeQ== 37646
IGFubm90YXRpb25z 37647
YmF0aA== 37648
Q2xpbWF0ZQ== 37649
IGRvcnM= 37650
IFNsaWRl 37651
Y29vcmQ= 37652
IFJlbG9hZA== 37653
IExETA== 37654
IExvdmVjcmFmdA== 37655
IHVuaW1hZ2lu 37656
IHJlc2VtYmxlZA== 37657
IGJhcnJhY2tz 37658
bnA= 37659
IHN1cnJvZ2F0ZQ== 37660
IGNhdGVnb3JpemVk 37661
44Kp 37662
IHZhY2NpbmF0ZWQ= 37663
IGRyYWluYWdl 37664
IGluZGlzdA== 37665
IFdoYXRzQXBw 37666
IDE4NzA= 37667
b2xlcmFuY2U= 37668
aW52b2tl 37669
YW1vcnBo 37670
IHJlY29ubmVjdA== 37671
IGVtYW5j 37672
IGJsaW5kbmVzcw== 37673
IDEyODA= 37674
aW50ZXJuZXQ= 37675
Y29sbGFy 37676
IGFsdHJ1 37677
IGFieXNz 37678
IFRSSQ== 37679
NjU3 37680
IGluZnVzZWQ= 37681
SEVBRA== 37682
IGZvcmVzdHJ5 37683
IFdvb2R5 37684
IENp 37685
d2k= 37686
c2Ft 37687
Nzg0 37688
aG9saWRheQ== 37689
IG1vZ3Vs 37690
IEZlZXM= 37691
IERFTg== 37692
SW50ZXJuYWw= 37693
dXJiZWQ= 37694
ZnVzYw== 37695
YXRvbQ== 37696
IElsbHVzaW9u 37697
IHBvbGxlZA== 37698
IGZsYXA= 37699
IGNvYXg= 37700
TEdCVA== 37701
QW5hbHk= 37702
IFNlY3Rpb25z 37703
IENhbGlmb3Ju 37704
ZW1u 37705
IGhpdGhlcg== 37706
IE5JR0hU 37707
IG5haWxlZA== 37708
IFBpcGVsaW5l 37709
Mzkx 37710
b29m 37711
IFByaW1hbA== 37712
dmVyZW5k 37713
IHNsYXNoaW5n 37714
IHJldHJp 37715
YXZpb3Vy 37716
IGRlcGFydGluZw== 37717
Z2ls 37718
SVND 37719
IG1pZHdheQ== 37720
IHVsdHJhc291bmQ= 37721
IGJlaGF2aW5n 37722
IFRhcmE= 37723
Y2xhc3Nlcw== 37724
VmlydHVhbA== 37725
IENvbG9uaWFs 37726
IHN0cmlwcGluZw== 37727
IG9yY2hlc3RyYXRlZA== 37728
IEdyYXZlcw== 37729
NDUy 37730
IElyb25pY2FsbHk= 37731
IFdyaXRlcnM= 37732
IGxlbmRz 37733
IE1hbno= 37734
IHJhdmVu 37735
IG94aWRhdGl2ZQ== 37736
IDI2Ng== 37737
RUxG 37738
YWN0dWFsbHk= 37739
YXNjYXI= 37740
RHJhZnQ= 37741
IGZhdm91cmFibGU= 37742
IGh1bWlsaWF0aW5n 37743
IGZpZGVsaXR5 37744
IEhvZg== 37745
IFh1YW4= 37746
NDk2 37747
IGxheWVyZWQ= 37748
YXRpcw== 37749
Nzkw 37750
IHBheWNoZWNr 37751
aXRvbg== 37752
S2Fy 37753
IFZNd2FyZQ== 37754
IEZhcm1lcg== 37755
IHNlcnZpYw== 37756
Z2xvbWVy 37757
IHNsdW1w 37758
IEZhYnJpYw== 37759
IERPQw== 37760
ZXN0aW5n 37761
IHJlYXNzdXJl 37762
IHBoeWw= 37763
dm9sdA== 37764
aXRvcnk= 37765
UnVsZXM= 37766
IG94aWRhdGlvbg== 37767
IHByaXplZA== 37768
IG1pc3RyZXNz 37769
IERqYW5nbw== 37770
V0FSTg== 37771
5ZE= 37772
IGVuY29kZQ== 37773
IEZlZWRiYWNr 37774
IHN0dXBpZGl0eQ== 37775
SWFu 37776
IFl1Z29zbGF2aWE= 37777
16g= 37778
YWNs 37779
VVRF 37780
MTk3Nw== 37781
IHF1YWxpZmllcw== 37782
IHB1bHNlcw== 37783
cHJldHR5 37784
IGZyb3pl 37785
IHNz 37786
SXRlcmF0b3I= 37787
IHVyZ2VudGx5 37788
IG1haWxlZA== 37789
IENoYW0= 37790
IHN1c3RhaW5pbmc= 37791
IGJhc2ls 37792
IHB1cHBpZXM= 37793
aWxhbnQ= 37794
IFBMRUFTRQ== 37795
bGFw 37796
YWNlb3Vz 37797
RmVhcg== 37798
IE1hc3Rlcnk= 37799
YXV0b21hdGlj 37800
IFRBRw== 37801
IGFudGlt 37802
YWdsZXM= 37803
NDcz 37804
ZnJhbWVz 37805
IHdoaXNwZXJz 37806
IFdob2V2ZXI= 37807
IGJyYXZlcnk= 37808
IFVLSVA= 37809
cmFjdGlvbnM= 37810
IiIi 37811
IHRhbWU= 37812
IHBhcnRlZA== 37813
ZXZlcnl0aGluZw== 37814
Q09OVA== 37815
IGluZGVidGVk 37816
IGFkZHI= 37817
cmVr 37818
SVJFRA== 37819
IGVtaW5lbnQ= 37820
Y2xpbnRvbg== 37821
IG91c3RlZA== 37822
IHJldmlld2Vy 37823
IG1lbHRkb3du 37824
IHJlYXJy 37825
IFlhbw== 37826
dGhlcmVhbA== 37827
YWJ5dGU= 37828
IHN0dW1ibGluZw== 37829
IGJhdGNoZXM= 37830
IDI1OQ== 37831
IGNvbnRyYWNlcHRpdmU= 37832
IHByb3N0aXR1dGU= 37833
ZW5zaXM= 37834
RGVjbA== 37835
IFN0cmlrZXM= 37836
TWlsaXRhcnk= 37837
IE9hdGg= 37838
dmFjYw== 37839
cHBpbmdz 37840
MDUy 37841
IHBhcnROYW1l 37842
YW1waW5n 37843
UmVwb3J0cw== 37844
S0k= 37845
Q0hS 37846
IHN1YnRseQ== 37847
c3dlcnM= 37848
Qmxha2U= 37849
dXN1YWw= 37850
IGNvbnRlc3RhbnRz 37851
IGNhcnRyaWRnZXM= 37852
IEdSRUFU 37853
IGJsdXNo 37854
IOKAug== 37855
NDcy 37856
IHJlYXNvbmVk 37857
44Ok 37858
cGFyYWxsZWxlZA== 37859
IGR5bg== 37860
YWdhdGU= 37861
IG5pZ2h0bHk= 37862
5YY= 37863
NTU2 37864
IHNlbWFudGlj 37865
IEFkdm9j 37866
ICEh 37867
IGRpc2FncmVlcw== 37868
IEJX 37869
VmVo 37870
IGhhcm1pbmc= 37871
IGVtYnJhY2Vz 37872
IHN0cml2ZXM= 37873
IGlubGFuZA== 37874
IEthcmQ= 37875
IGhlYXRz 37876
IEdpbm55 37877
dXRhbg== 37878
ZXJuYXV0 37879
eWxlbmU= 37880
IEVsZXY= 37881
SkQ= 37882
IGhhcnM= 37883
IFN0YXJy 37884
IHNreXNj 37885
IGNvbGxhYm9yYXRvcnM= 37886
VXN1YWxseQ== 37887
IHJldm9sdXRpb25z 37888
IFNUQVRT 37889
IGRpc21hbnRsZQ== 37890
IGNvbmZpZGVudGx5 37891
IGtpbmV0aWM= 37892
QWxp 37893
IHBlcmNlbnRpbGU= 37894
IGV4dHJhY3Rpbmc= 37895
aWxsaWFu 37896
ZXN0ZWFk 37897
IHBoeXNpY2lzdHM= 37898
IE1hcnNoYWw= 37899
IGZlbGxvd3NoaXA= 37900
IGRhc2hlZA== 37901
IFVS 37902
IFNpb3V4 37903
IENvbXBhY3Q= 37904
YW1pZGU= 37905
UHl0aG9u 37906
IExlaWdo 37907
IFBoYXJtYWM= 37908
aXN0cmF0ZXM= 37909
aGVyaWNhbA== 37910
IGZ1ZQ== 37911
IEVtaW4= 37912
ICh7 37913
IE5laWdoYm9yaG9vZA== 37914
IGRpc3J1cHRpbmc= 37915
IER1cA== 37916
IGdsYW5k 37917
IFNldg== 37918
IE1hcmlhbg== 37919
YXJnb24= 37920
IER1bmQ= 37921
IDwhLS0= 37922
IHN0cmFuZA== 37923
IHN0YWRpdW1z 37924
em9z 37925
IHBzeWNob3Npcw== 37926
IFJhY2s= 37927
IGJyaWxsaWFudGx5 37928
77iP 37929
IHN1Ym1lcmdlZA== 37930
IEluc3RpdA== 37931
IENob3c= 37932
IGNhZ2Vz 37933
IEhhdHM= 37934
IFVycw== 37935
IGRpbHV0ZWQ= 37936
dXNhdA== 37937
aWVubmU= 37938
IE1lbWJlcnNoaXA= 37939
IEJ1cms= 37940
IGll 37941
IGFyY2hldHlwZQ== 37942
RHJ1Zw== 37943
dWx0b24= 37944
IFNwb2Nr 37945
IE1jS2F5 37946
IERlcGVuZA== 37947
RmVhdHVyZWQ= 37948
U29j 37949
MTk3OA== 37950
IEJlcmU= 37951
IHJlbGVudGxlc3NseQ== 37952
IGNyaXBwbGluZw== 37953
IGFydGhyaXRpcw== 37954
55Sf 37955
IFRyb3BpY2Fs 37956
IEJ1bGc= 37957
IENoZXJ5bA== 37958
IGFkbWlyYWJsZQ== 37959
IHN1YnRpdGxl 37960
T3ZlcnJpZGU= 37961
IG9yaWdpbmF0aW5n 37962
IENDUA== 37963
IHN3b3Jl 37964
IFNvbGU= 37965
IERpc29yZGVycw== 37966
MzI5 37967
IHByb2Nlc3Npb24= 37968
IHJlZnVyYg== 37969
IGltbWVyc2Vk 37970
cmVxdWVudGx5 37971
IHNrZXB0aWNz 37972
IGNlcmFtaWM= 37973
bWl0dGVy 37974
ZW5zdGVpbg== 37975
YmVsdA== 37976
IFRJVA== 37977
YmlkZGVu 37978
IGZpcg== 37979
bWlzdA== 37980
Pl0= 37981
IHdlYXZl 37982
IFBhcmFkb3g= 37983
IGVudHJ1c3RlZA== 37984
IEJhcmNsYXlz 37985
IG5vdmVsaXN0 37986
b2dpZQ== 37987
ODA2 37988
IG5pbmV0eQ== 37989
IGRpc2FncmVlbWVudHM= 37990
QEBAQEBAQEA= 37991
IEF1c2Nod2l0eg== 37992
Y2Fycw== 37993
IExFVA== 37994
dHVi 37995
YXJhbnRpbmU= 37996
UE9T 37997
IGJhY2tzdG9yeQ== 37998
IGNoZWVyZnVs 37999
IFJhZw== 38000
ZWth 38001
Ymlhc2Vk 38002
IGluZXhwZXJpZW5jZWQ= 38003
YWtyYQ== 38004
IFdpdHQ= 38005
dGFu 38006
IHJhcGlzdA== 38007
IHBsYXRlYXU= 38008
Y2hhbA== 38009
IElucXVpcw== 38010
ZXhwcmVzc2lvbg== 38011
IGNpcGhlcg== 38012
IHNoYXZpbmc= 38013
YWRkZW4= 38014
cmVseQ== 38015
KFw= 38016
aXNtYQ== 38017
IFJlZ3VsYXRvcnk= 38018
Q0hBUg== 38019
aWx5bg== 38020
TlZJRElB 38021
R1U= 38022
IG11cm0= 38023
bGF1cw== 38024
Q2hyaXN0b3BoZXI= 38025
IGNvbnRyYWN0dWFs 38026
IFByb3h5 38027
IEphaW1l 38028
IE1ldGhvZGlzdA== 38029
IHN0ZXdhcmRz 38030
c3Rh 38031
cGVyaWE= 38032
IHBoeXNpb2xvZ3k= 38033
IGJ1bXBlZA== 38034
IGZydWN0b3Nl 38035
QXVzdHJhbGlhbg== 38036
IE1ldGFsbGlj 38037
IE1hc3F1ZXJhZGU= 38038
YXJi 38039
IHByb211bA== 38040
IGRvd25mYWxs 38041
IGJ1dGNoZXI= 38042
IGJvdXI= 38043
IElORk9STUFUSU9O 38044
IEJpcw== 38045
cGVjdHM= 38046
YWRlbmE= 38047
IGNvbnRlbXBsYXRpbmc= 38048
YXJvbw== 38049
Y2VudGVyZWQ= 38050
IFBlYWtz 38051
VXNlZA== 38052
IG1vZGVt 38053
IGdlbmRlcnM= 38054
IDgwMDA= 38055
Mzcx 38056
IG1hdGVybml0eQ== 38057
IFJheg== 38058
IHJvY2tpbmc= 38059
IGhhbmRndW5z 38060
IERBQ0E= 38061
QXV0b20= 38062
IE5pbGU= 38063
IHR1bXVsdA== 38064
IEJlbmVmaXQ= 38065
IEFwcHJvYWNo 38066
d29ya3Nob3A= 38067
IExlYXZpbmc= 38068
R2Vy 38069
aW5zdGVhZA== 38070
IHZpYnJhdGlvbnM= 38071
IHJlcG9zaXRvcmllcw== 38072
NDk3 38073
IEF1bnQ= 38074
IEp1Yg== 38075
IEV4cGVkaXRpb24= 38076
QWxwaGE= 38077
IHNhbnM= 38078
IG92ZXJkdWU= 38079
IG92ZXJjcm93ZA== 38080
IGxlZ2lzbGF0dXJlcw== 38081
IHBhdGVybmFs 38082
IExlb25hcmRv 38083
IGV4cHJlc3NpdmU= 38084
IGRpc3RyYWN0aW9ucw== 38085
IHNpbGVuY2Vk 38086
dHJ1c3Q= 38087
IGJpa2luZw== 38088
IDU2MA== 38089
IHByb3ByaWV0 38090
IGltcG9zaXRpb24= 38091
IGNvbmdsb21lcg== 38092
ID09PT09PT09PT09PT09PT09PT09PT09PT09PT09PT09PT09PT09PT09PT09PT09PT09PT09PT09PT09PT09PT09 38093
IFRlYWNoaW5n 38094
IFlvc2U= 38095
aW50ZW5zaXZl 38096
VG93bg== 38097
IHRyb2xsaW5n 38098
IEdyYWM= 38099
IEFTVVM= 38100
WW8= 38101
IHNwZWNpYWxz 38102
IE5lcGg= 38103
IEdvZHppbGxh 38104
RGF0YWJhc2U= 38105
IEhlZ2Vs 38106
IDI3Mg== 38107
MTk3Ng== 38108
IEdsb3JpYQ== 38109
IGRpc2VtYg== 38110
IEludmVzdGlnYXRpb25z 38111
IEJhbmU= 38112
YWdlbWVudHM= 38113
U3RyYW5nZQ== 38114
IHRyZWFzdXJ5 38115
IFBsYXlz 38116
IHVuZGVzaXJhYmxl 38117
IHdpZGVuaW5n 38118
IHZlcmJhbGx5 38119
IGluZmFuY3k= 38120
IGN1dHRlcg== 38121
Zm1s 38122
IDIxMDA= 38123
cHJvdG90eXBl 38124
ZmluZQ== 38125
IGRlY3JpbWluYWw= 38126
IGR5c2Z1bmN0aW9uYWw= 38127
IGJlc2ll 38128
IEVybnN0 38129
emVi 38130
IG5vcnRoZWFzdGVybg== 38131
IGF1c3Q= 38132
cG9yYXRl 38133
IE1hcmxpbnM= 38134
IHNlZ3JlZ2F0ZWQ= 38135
ZXdvcmxk 38136
IE1haGVy 38137
IHRyYXZlcnNl 38138
IG1vbmFzdGVyeQ== 38139
dXJneQ== 38140
R2Vhcg== 38141
c2FuZA== 38142
Q29tcGw= 38143
IEVNUA== 38144
IHBsZW50 38145
IE1lcmNlcg== 38146
IDI3Ng== 38147
VEFCTEU= 38148
Q29uZmlndXJhdGlvbg== 38149
SHVuZHJlZHM= 38150
IHByaWM= 38151
IGNvbGxhYm9yYXRpbmc= 38152
IFBhcmFtb3VudA== 38153
IEN1bW1pbmdz 38154
ICg8 38155
IHJlY29yZGVy 38156
IGZsYXRz 38157
IDQxNg== 38158
d2hvc2U= 38159
Rm9udFNpemU= 38160
IE9yYml0 38161
WVI= 38162
IHdyaXN0cw== 38163
IGJha2VyeQ== 38164
KX0= 38165
IEJvdW50eQ== 38166
IExhbmNhc3Rlcg== 38167
IGVuZGluZ3M= 38168
YWNjb3JkaW5n 38169
IFNhbGFt 38170
ZWFzeQ== 38171
NzU1 38172
IEJ1cnI= 38173
IEJhcm5ldHQ= 38174
b25vbW91cw== 38175
VW5pb24= 38176
IHByZWNlZGVuY2U= 38177
IFNjaG9sYXJzaGlw 38178
IFVY 38179
IHJvbGxvdXQ= 38180
IGJvb24= 38181
YWxt 38182
IENhbnRlcg== 38183
5rU= 38184
IHJvdW5kaW5n 38185
IGNsYWQ= 38186
IHZhcA== 38187
IEZlYXR1cmVk 38188
aXNhdGlvbnM= 38189
IDU0MA== 38190
cG9saWNl 38191
IHVuc2V0dGxpbmc= 38192
IGRyaWZ0aW5n 38193
IEx1bWlh 38194
IE9iYW1hQ2FyZQ== 38195
IEZhdm9y 38196
SHlwZXI= 38197
IFJvdGhzY2hpbGQ= 38198
IE1pbGliYW5k 38199
YW5hbHk= 38200
IEp1bGlldA== 38201
SHU= 38202
IHJlY2FsbGluZw== 38203
YWhlYWQ= 38204
Njk2 38205
IHVuZmF2b3JhYmxl 38206
IGRhbmNlcw== 38207
T3g= 38208
IGxlZ2FsaXR5 38209
IDQwMw== 38210
cm9tYW5jZXI= 38211
IGlucXVpcmU= 38212
IE1vdmVz 38213
XCI+ 38214
IFZhcmlhbnQ= 38215
IE1lc3NpYWg= 38216
IExDUw== 38217
IEJhaMOh 38218
NzU2 38219
IGV5ZWJyb3c= 38220
IMKl 38221
IE1jRg== 38222
IEZvcnR5 38223
TWFz 38224
IHBhbmlja2Vk 38225
IHRyYW5zZm9ybWF0aW9ucw== 38226
cXE= 38227
IHJldm9sdmVz 38228
cmluZ2U= 38229
IEFp 38230
YXhl 38231
IG9ud2FyZA== 38232
IENGUg== 38233
IEJhcmU= 38234
bG9naW4= 38235
IGxpcXVpZHM= 38236
IGRlY29tcA== 38237
c2Vjb25kYXJ5 38238
aWxhbg== 38239
IENvbnZlcnQ= 38240
YW1peWE= 38241
IHByb3NlY3V0aW5n 38242
IOKJoQ== 38243
IFlvcmtlcnM= 38244
IEJ5cm5l 38245
c2xvdw== 38246
YXdlaQ== 38247
SmVhbg== 38248
IDI2OQ== 38249
IFNreWRyYWdvbg== 38250
IMOp 38251
IE5pY2FyYWd1YQ== 38252
IEh1Y2thYmVl 38253
IEhpZ2hseQ== 38254
IGFtcGhpYg== 38255
IFBhc3Rvcg== 38256
IExldHM= 38257
IGJsdXJyZWQ= 38258
IHZpc2NlcmFs 38259
IENCTw== 38260
IGNvbGxhYm9yYXRlZA== 38261
emln 38262
TGVnYWw= 38263
IGFwYXJ0aGVpZA== 38264
IGJyaWQ= 38265
IHByZXNldA== 38266
IERFVA== 38267
IEFNQQ== 38268
15Q= 38269
YXJjaGluZw== 38270
YXVjdXNlcw== 38271
YnVpbGRlcg== 38272
IHBvZXRpYw== 38273
IGVtdWxhdG9y 38274
IE1vbGVjdWxhcg== 38275
IGhvbm9yaW5n 38276
aXNldW0= 38277
IHRyYWN0b3I= 38278
IENsdXN0ZXI= 38279
IENhbG0= 38280
YXJlZGV2aWw= 38281
IHNpZGV3YWxrcw== 38282
IHZpb2xpbg== 38283
IGdlbmVyYWxpemVk 38284
IEFsZWM= 38285
IGVtYmFyZ28= 38286
IGZhc3RiYWxs 38287
IEhUVFBT 38288
IExhY2s= 38289
IENoaWxs 38290
cml2ZXI= 38291
Q2hlbA== 38292
IFN3YXJt 38293
IExldmluZQ== 38294
cm95aW5n 38295
TGF1bmNo 38296
IGtpY2tlcg== 38297
IGFkZGl0aXZl 38298
IERlYWxz 38299
V2lkZ2V0 38300
Y29udGFpbmluZw== 38301
IGVzY2FsYXRl 38302
IE9QRU4= 38303
IHR3ZWFrZWQ= 38304
IHN0YXNo 38305
IHNwYXJrcw== 38306
IEVzc2V4 38307
IEVjYw== 38308
IGNvbnZpY3Q= 38309
IGJsb2dnaW5n 38310
SUVS 38311
IEhM 38312
IG11cmRlcmVycw== 38313
NzU5 38314
IEhpYg== 38315
IGRlcGw= 38316
IEpvcmQ= 38317
U2Fj 38318
IGRpc3NlY3Q= 38319
IEhvd2U= 38320
b3NoZXI= 38321
IGN1c3RvbWl6YWJsZQ== 38322
IEZyYW56 38323
IGF0cm8= 38324
xIc= 38325
IDAwMDQ= 38326
IG91dHBvc3Q= 38327
Um9zcw== 38328
IGdseXBob3NhdGU= 38329
IEhhc3Rpbmdz 38330
IEJFRk9SRQ== 38331
IHNob3Zl 38332
b3BwZWQ= 38333
IFNjYWxh 38334
IGFtdWxldA== 38335
YW5pYW4= 38336
IGV4YWNlcmJhdGVk 38337
IGVhdGVy 38338
NDcx 38339
VU1F 38340
IHB1bHA= 38341
aXpvbnRhbA== 38342
IFphbQ== 38343
IEFUSQ== 38344
aW1tdW5l 38345
YWJ5dGVz 38346
IHVubmVjZXNzYXJpbHk= 38347
IENBVA== 38348
IEF4aXM= 38349
IHZpc3VhbGl6ZQ== 38350
w4k= 38351
IFJhZGljYWw= 38352
Zm0= 38353
RG9jdW1lbnRz 38354
IEZvcnJlc3Q= 38355
IGNvbnRleHR1YWw= 38356
IFN5bWJvbA== 38357
IHRlbnRhdGl2ZQ== 38358
IERPRVM= 38359
IEdvb2Rz 38360
IGludGVybWl0dGVudA== 38361
fTo= 38362
bWVkaWF0ZWQ= 38363
IHJpZGljdWxl 38364
IGF0aGVpc20= 38365
IHBhdGhvZ2Vucw== 38366
IE11bQ== 38367
IHJlaW50cm9kdQ== 38368
IDMwNw== 38369
aUhVRA== 38370
IGZsYXNobGlnaHQ= 38371
IHN3ZWFyaW5n 38372
IHBlbmd1 38373
QnU= 38374
IHJvdGF0ZWQ= 38375
IENyYW5l 38376
ICgpKTs= 38377
IGZhc2hpb25hYmxl 38378
IGVuZG9yc2luZw== 38379
NDYz 38380
KVs= 38381
IGluZ2VzdGlvbg== 38382
IGNvb2tz 38383
IDk1MA== 38384
b3RvbXk= 38385
IEltYW0= 38386
IGth 38387
IHRlYXNlcg== 38388
IEdob3N0cw== 38389
IOOCtQ== 38390
MTk2OQ== 38391
z4M= 38392
dWJieQ== 38393
IGNvbnZlcnRlcg== 38394
emFubmU= 38395
ZW5kZQ== 38396
IFByZXBhcg== 38397
IE5pY2tlbA== 38398
IENoaW1lcmE= 38399
aGlt 38400
IFR5cmFubg== 38401
IFNhYmJhdGg= 38402
IE5pY2hvbHM= 38403
IHJhcHQ= 38404
aWhhcg== 38405
IHNoZWxsaW5n 38406
IGlsbHVtaW5hdGU= 38407
IGRlbnRpc3Q= 38408
dXRvcg== 38409
IEludGVncmF0aW9u 38410
IHdoaW1z 38411
IExpdGVyYXJ5 38412
QmVhdXQ= 38413
IHBhcmNobWVudA== 38414
YWdhcmE= 38415
QnJhbmQ= 38416
IGRlcm9n 38417
4oCmKQ== 38418
IE5vcnNl 38419
IHVud2l0dGluZw== 38420
IGN1Yw== 38421
IGJvcmRlcmxpbmU= 38422
IHVwc2V0dGluZw== 38423
IHJlY291cnNl 38424
IGRyYXBlZA== 38425
IFJhZGFy 38426
IGNvbGRlcg== 38427
IFBlcHNp 38428
aW1pbmFyeQ== 38429
XSxb 38430
NjU4 38431
Vmk= 38432
IEZyZW0= 38433
IFBlcw== 38434
IHZldGVyaW5hcnk= 38435
IFRFRA== 38436
IEVwaWRlbQ== 38437
bm92YQ== 38438
a2lk 38439
IGRldm91dA== 38440
b2N0 38441
amFk 38442
TW9o 38443
IFBBWQ== 38444
IGdlb21ldHJpYw== 38445
IDMyMw== 38446
IGNpcmN1bWZlcmVuY2U= 38447
aWNoaWNr 38448
MTk3NQ== 38449
IFl1cmk= 38450
IFNoYWxs 38451
IEhvdmVy 38452
dW5pbg== 38453
U3By 38454
IGdyYWZ0 38455
IEhhcHBpbmVzcw== 38456
IGRpc2FkdmFudGFnZXM= 38457
YXR0YWNrcw== 38458
IGh1YnM= 38459
IFN0YXJDcmFmdA== 38460
6ZY= 38461
IGdhbGxlcmllcw== 38462
IEtvcnJh 38463
IGdyb2Nlcmllcw== 38464
IEdvcnN1Y2g= 38465
IHJhcGlzdHM= 38466
IGZ1bmdp 38467
IFR5cGhvb24= 38468
VmVjdG9y 38469
IEVtcHJlc3M= 38470
YmF0dGxl 38471
NDY4 38472
IHBhcmFzaXRl 38473
IEJvbWJlcg== 38474
U0c= 38475
ZXhpc3Q= 38476
IFBm 38477
IHVuc2U= 38478
IHN1cmdlb25z 38479
QmlydGg= 38480
IFVuc3VyZQ== 38481
IFByaW50ZWQ= 38482
IEJlaGF2aW9yYWw= 38483
IEFzdGVy 38484
UGFraXN0YW4= 38485
IHVuZXRoaWNhbA== 38486
IHN2 38487
IElvVA== 38488
IGxheW91dHM= 38489
UGFpbg== 38490
IGNvbnN0YW50cw== 38491
IExX 38492
IEJha2U= 38493
IHRvd2Vscw== 38494
IGRldGVyaW9yYXRpb24= 38495
IEJvbGl2aWE= 38496
IGJsaW5kZWQ= 38497
IFdhcmRlbg== 38498
IE1pc3RyZXNz 38499
IG9uc3RhZ2U= 38500
IGNsYW5z 38501
IEJFU1Q= 38502
MTk2MA== 38503
IGFudGlxdWU= 38504
IHJoZXRvcmljYWw= 38505
IFBlcmN5 38506
IFJ3YW5kYQ== 38507
LC4= 38508
QnJ1Y2U= 38509
IHRyYXVtYXQ= 38510
IFBhcmxpYW1lbnRhcnk= 38511
IGZvb3Rub3Rl 38512
aWRpYQ== 38513
IExlYXJuZWQ= 38514
c2Vla2luZw== 38515
Z2VuaWM= 38516
IGRpbWVuc2lvbmFs 38517
SGlkZQ== 38518
6ICF 38519
IGludHJpZ3Vl 38520
aW5zZQ== 38521
IGxlYXNlcw== 38522
IGFwcHJlbnRpY2Vz 38523
d2FzaGluZw== 38524
IDE5MjY= 38525
VklMTEU= 38526
IHN3b29w 38527
c2Ns 38528
IGJlZHJvb21z 38529
b25pY3M= 38530
IENydW5jaA== 38531
Y29tcGF0aWJsZQ== 38532
IGluY2FwYWM= 38533
IFllbWVuaQ== 38534
YXNodHJh 38535
emhvdQ== 38536
ZGFuZ2Vy 38537
IG1hbmlmZXN0YXRpb25z 38538
IERlbW9ucw== 38539
QUFG 38540
U2VjcmV0YXJ5 38541
QUNURUQ= 38542
TE9E 38543
IGFteQ== 38544
cmFwZXI= 38545
ZXRobmlj 38546
NDE3 38547
IHBvc2l0aXZlcw== 38548
IDI3Mw== 38549
IFJlZnVnZWVz 38550
IHVzYg== 38551
IFZhbGQ= 38552
b2RkeQ== 38553
IE1haG1vdWQ= 38554
QXNpYQ== 38555
IHNrdWxscw== 38556
IEV4b2R1cw== 38557
IENvbXBldA== 38558
IExJQw== 38559
IE1hbnNpb24= 38560
IEFtZQ== 38561
IGNvbnNvbGlkYXRl 38562
c3Rvcm1z 38563
b250ZW50 38564
OTk2 38565
IGNsZW4= 38566
IG11bW15 38567
ZmxhdA== 38568
NzU4 38569
IFZPTA== 38570
b3Rlcmlj 38571
bmVu 38572
IE1pbnV0ZQ== 38573
U292 38574
IGZpbmVy 38575
Umg= 38576
bHljZXI= 38577
IHJlaW5mb3JjZW1lbnRz 38578
IEpvaGFubmVz 38579
IEdhbGxhZ2hlcg== 38580
IGd5bW4= 38581
U3VkZGVubHk= 38582
IGV4dG9ydGlvbg== 38583
a3I= 38584
aWF0b3I= 38585
VGE= 38586
IGhpcHBvY2FtcHVz 38587
TlBS 38588
IENvbXB1dGluZw== 38589
IHNxdWFyZWx5 38590
IG1vZGVsbGluZw== 38591
IEZvcnVtcw== 38592
IExpc3A= 38593
IEtyaXNobmE= 38594
IDMyNA== 38595
IHJ1c2hlcw== 38596
IGVuc3VlZA== 38597
IGNyZWVwaW5n 38598
b250ZQ== 38599
bmFp 38600
aWxhdGVy 38601
IEhvcm5ldHM= 38602
IG9ibGl2aW91cw== 38603
SU5TVA== 38604
NTU5 38605
IGplb3BhcmR5 38606
IGRpc3Rpbmd1aXNoaW5n 38607
anVyZWQ= 38608
IGJlZ3M= 38609
c2ltaWxhcg== 38610
cGhvdA== 38611
NTMw 38612
IFBhcmt3YXk= 38613
IHNpbmtz 38614
IEhlYXJ0aHN0b25l 38615
aWJ1cg== 38616
IEJhdG9u 38617
QXZvaWQ= 38618
IGRhbmNlcg== 38619
IG1hZ2lzdHJhdGU= 38620
YXJ5bg== 38621
IGRpc3R1cmJhbmNlcw== 38622
IFJvbWVybw== 38623
IHBhcmFwaA== 38624
IG1pc2NoaWVm 38625
4paT 38626
IFNoYXJpYQ== 38627
IHVyaW5hcnk= 38628
cm91dGU= 38629
aXZhcw== 38630
Zml0dGVk 38631
IGVqZWN0ZWQ= 38632
IEFsYnVxdWVycXVl 38633
IDQ3MA== 38634
IGlycml0YXRlZA== 38635
IFppcA== 38636
IEJpb2w= 38637
w40= 38638
IGRlbm91bmNl 38639
IGJpbmFyaWVz 38640
IFZlcnNl 38641
IG9wcG9z 38642
IEtlbmRyaWNr 38643
IEdQTA== 38644
IHNwZXc= 38645
IEVsaWphaA== 38646
IEVhcw== 38647
IGRyaWZ0ZWQ= 38648
c29mYXI= 38649
IGFubm95YW5jZQ== 38650
IEJFVA== 38651
NDc0 38652
IFN0cm9uZ2g= 38653
aXRhdGVz 38654
IENvZ25pdGl2ZQ== 38655
b3Bob25l 38656
IElkZW50aWZpY2F0aW9u 38657
b2NyaW5l 38658
Y29ubmVjdGlvbg== 38659
IGJveGVy 38660
IEFTRA== 38661
IEFyZWFz 38662
WWFuZw== 38663
dGNo 38664
dWxsYWg= 38665
IGRlY2VpdmU= 38666
Q29tYmF0 38667
ZXBpc29kZQ== 38668
Y3JldGU= 38669
V2l0bmVzcw== 38670
IGNvbmRvbGVuY2Vz 38671
aHRhcg== 38672
IGhlYWxz 38673
IGJ1Y2tldHM= 38674
IExBVw== 38675
Qmx1 38676
IHNsYWI= 38677
IE9SREVS 38678
b2Ns 38679
YXR0b24= 38680
IFN0ZXZlbnNvbg== 38681
IEdpbmdlcg== 38682
IEZyaWVuZGx5 38683
IFZhbmRlcmJpbHQ= 38684
c3Bpcml0 38685
aWds 38686
IFJlZ2FyZGluZw== 38687
IFBST0c= 38688
IHNlYWxpbmc= 38689
c3RhcnRpbmc= 38690
IGNhcmRpbmFs 38691
IFZlYw== 38692
IEJlaXI= 38693
IG1pbGxpc2Vjb25kcw== 38694
d2Vhaw== 38695
cGVyc2U= 38696
IHN0ZXJpbGU= 38697
IENvbnRlbXBvcmFyeQ== 38698
IFBoYW50 38699
IENsbw== 38700
IG91dHA= 38701
IGV4aWxlZA== 38702
IDI3Nw== 38703
IHNlbGZpZQ== 38704
IG1hbmlj 38705
IG5hbm8= 38706
dGVybXM= 38707
QWxleGFuZGVy 38708
IHJlc29sdmVz 38709
IG1pbGxlbm5pYQ== 38710
IGV4cGxvZGVz 38711
IGNvbnN0ZWxsYXRpb24= 38712
IGFkdWx0ZXJ5 38713
bW90aW9u 38714
RE9D 38715
IGJyb2FkY2FzdGVycw== 38716
IGtpbmRlcmdhcnRlbg== 38717
IE1heXdlYXRoZXI= 38718
IEVjbw== 38719
aWNobw== 38720
IDI4Nw== 38721
bGF1bg== 38722
IG11dGU= 38723
IGRpc2NyZWV0 38724
IHByZXNjaG9vbA== 38725
IHByZWVtcHQ= 38726
RGVsZXRl 38727
IEZyZWVk 38728
UGk= 38729
SEs= 38730
IGJsb2NrZXI= 38731
IEN1bWJlcg== 38732
IHdyb3VnaHQ= 38733
ZGF0aW5n 38734
IGluc3VyZXI= 38735
IHF1b3Rhcw== 38736
IHByZWFjaGVk 38737
IGV2aWN0aW9u 38738
IFJlZ2luYQ== 38739
IFBlbnM= 38740
IHNldmVudGVlbg== 38741
IE5hc3M= 38742
RGljaw== 38743
IGZvbGRz 38744
IGRvdHRlZA== 38745
IEFhZA== 38746
VW5pdmVyc2Fs 38747
IHBpeno= 38748
IEd1cnU= 38749
IHNvaWxz 38750
IG5vdmljZQ== 38751
IE5lYW5kZXI= 38752
IHN0b29s 38753
IGRldG9uYXRlZA== 38754
IFBpa2FjaHU= 38755
IE1hc3NpdmU= 38756
SVZFUg== 38757
IEFiZGVs 38758
IHN1YmR1ZWQ= 38759
IHRhbGxlc3Q= 38760
IHByZWNhcmlvdXM= 38761
IGF5 38762
cmlmaWNhdGlvbg== 38763
IE9iag== 38764
Y2FsZQ== 38765
IHVucXVlc3Rpb24= 38766
Y3Vsb3Npcw== 38767
YWRhcw== 38768
aWdyYXRlZA== 38769
RGF5cw== 38770
IHF1ZWVucw== 38771
IEdhemV0dGU= 38772
IENvbG91cg== 38773
IEJvd21hbg== 38774
IEpK 38775
w692ZQ== 38776
IGRvbWluYXRlcw== 38777
U3R1ZGVudA== 38778
IG11 38779
IGJhY2tsb2c= 38780
IEVsZWN0cm8= 38781
VHJ1dGg= 38782
NDgz 38783
IGNvbmRlbnNlZA== 38784
cnVsZXM= 38785
IENvbnNwaXJhY3k= 38786
IGFjcm9ueW0= 38787
aGFuZGxlZA== 38788
IE1hdHRl 38789
anJp 38790
IEltcG9zc2libGU= 38791
bHVkZQ== 38792
Y3JlYXRpb24= 38793
IHdhcm1lZA== 38794
IFNsYXZl 38795
IG1pc2xlZA== 38796
IGZlcm1lbnQ= 38797
IEthaA== 38798
aW5raQ== 38799
a2VsZXRvbg== 38800
Y3ls 38801
IEthcmlu 38802
SHVudGVy 38803
UmVnaXN0ZXI= 38804
IFN1cnJleQ== 38805
IHN0YXJlcw== 38806
IFdpZHRo 38807
IE5heQ== 38808
IFNraQ== 38809
IGJsYWNrbGlzdA== 38810
dWNrZXQ= 38811
IGV4cHVsc2lvbg== 38812
aW1ldA== 38813
IHJldHdlZXQ= 38814
dmFudGFnZQ== 38815
RmVhdHVyZQ== 38816
IHRyb29wZXJz 38817
IGhvbWVycw== 38818
OTY5 38819
IGNvbnRpbmdlbmN5 38820
IFdUQw== 38821
IEJyZXdlcg== 38822
Zm9yZWlnbg== 38823
V2FyZQ== 38824
U29sYXI= 38825
IHVuZHVl 38826
UkVD 38827
dWxuZXJhYmxl 38828
cGF0aGlj 38829
IEJvaXNl 38830
IDMyMg== 38831
IGFyb3VzZWQ= 38832
IFlpbmc= 38833
5LiN 38834
dWVsZXNz 38835
IHBhcw== 38836
IG1vcnA= 38837
IGZsb3JhbA== 38838
RXhwcmVzcw== 38839
dWRnaW5n 38840
a0I= 38841
IEdyYW50ZWQ= 38842
2K8= 38843
IE1pY2hh 38844
IEdvdGhpYw== 38845
IFNQRUNJQUw= 38846
IFJpY2FyZG8= 38847
RnJhbg== 38848
IGFkbWluaXN0ZXJpbmc= 38849
NjIw 38850
cG9yYQ== 38851
IMKu 38852
IGNvbXByb21pc2Vz 38853
IGJpdHRlbg== 38854
QWNjZXB0 38855
VGhpcnR5 38856
0LI= 38857
IG1hdGVyaWFsbHk= 38858
IFRlcnI= 38859
aWdtYXRpYw== 38860
Y2hhaW5z 38861
IGRvdmU= 38862
c3RhZHQ= 38863
TWFydmVs 38864
RkFVTFQ= 38865
IHdpbmRzaGllbGQ= 38866
IDMzNg== 38867
YWRpZXI= 38868
IHN3YXBwaW5n 38869
IGZsYXdsZXNz 38870
IFByZWRhdG9y 38871
IE1pY2hlbGU= 38872
IHByb3B1bHNpb24= 38873
IFBzeWNoaWM= 38874
IGFzc2lnbmluZw== 38875
IGZhYnJpY2F0aW9u 38876
IGJhcmxleQ== 38877
bHVzdA== 38878
IHRvd2VyaW5n 38879
IGFsdGVyY2F0aW9u 38880
IEJlbnRsZXk= 38881
U3BoZXJl 38882
IHR1bmE= 38883
IENsYXNzZXM= 38884
RnJlZWRvbQ== 38885
dW5lcg== 38886
TGFkeQ== 38887
dm9pY2U= 38888
IGNvb2xlc3Q= 38889
b3Jy 38890
IHBhbHA= 38891
JHs= 38892
IGh5c3Rlcmlh 38893
IE1ldGF0cm9u 38894
cGFudHM= 38895
IHNwYXduaW5n 38896
RXhwZXJ0cw== 38897
IEludmVzdG9ycw== 38898
IEFuYXJjaHk= 38899
IHNocnVuaw== 38900
IFZpY3RpbQ== 38901
IDI4OQ== 38902
IGVjc3Rhc3k= 38903
IEJpbmRpbmc= 38904
NTg1 38905
IE1lbG9keQ== 38906
NTc4 38907
b3RhbGx5 38908
IEV0c3k= 38909
bGlnYQ== 38910
IGFwcGxhdWRlZA== 38911
IHN3ZWF0aW5n 38912
IHJlZGlzdHJpYnV0ZWQ= 38913
IHBvcGNvcm4= 38914
IHNlbWluYWw= 38915
ZnVy 38916
IE5ldXJvc2NpZW5jZQ== 38917
UmFuZA== 38918
IE9zdA== 38919
IE1hZGRlbg== 38920
IEluY3JlYXNpbmc= 38921
IERhd2tpbnM= 38922
IFN1YndheQ== 38923
IGFyc2Vu 38924
Y29uc2Vydg== 38925
QlVS 38926
IHNwaWtlZA== 38927
IEx5ZnQ= 38928
IEltcGVyaXVt 38929
IERyb3Bib3g= 38930
IGZhdm91cmVk 38931
IGVuY29tcGFzc2Vz 38932
Z2hvc3Q= 38933
IGluc3BpcmVz 38934
IGJ1cmdlb25pbmc= 38935
IFlvc2hp 38936
IFZlcnRpY2Fs 38937
IEF1ZGl0b3I= 38938
IGludGVuZGluZw== 38939
IGZpbGlidXN0ZXI= 38940
Qmxvb20= 38941
ZmFj 38942
IENhdnM= 38943
aWduaW5n 38944
IGNvd29ya2Vycw== 38945
IEJhcmJhcmlhbg== 38946
cmVtZW1iZXI= 38947
RkxBRw== 38948
IGF1ZGl0b3J5 38949
YXNvbnJ5 38950
Q29sbGVnZQ== 38951
IG11dGVk 38952
Z2Vtb255 38953
b2Jpbg== 38954
IFBzeWNobw== 38955
OTY4 38956
IGxhdmlzaA== 38957
IGhpZXJhcmNoaWNhbA== 38958
IERyb25l 38959
b3Vr 38960
IGNyaXBwbGVk 38961
IE1heGlt 38962
U2xvdA== 38963
IHF1aXo= 38964
IFZpZA== 38965
aWZsaW5n 38966
IGFyY2hhZW9sb2dpc3Rz 38967
IGFiYW5kb25tZW50 38968
ZGlhbA== 38969
bGVvbg== 38970
IEZhcw== 38971
VGVk 38972
IHJhc3BiZXJyeQ== 38973
IG1hbmV1dmVycw== 38974
IGJlaGF2aW91cnM= 38975
IGluc3VyZQ== 38976
IHJlbW9k 38977
U3dpdGNo 38978
aG9l 38979
IHNwYWNlZA== 38980
IGFmZm9yZGFiaWxpdHk= 38981
IEZlcm4= 38982
bm90YXRpb24= 38983
IEJhbGFuY2Vk 38984
IG9jY3VwaWVz 38985
ZW52aXJvbm1lbnQ= 38986
IG5lY2tsYWNl 38987
IHNlZGFu 38988
RlU= 38989
IEJyYXZv 38990
IGFidXNlcnM= 38991
IEFuaXRh 38992
bWV0YWRhdGE= 38993
IEdpdGh1Yg== 38994
YWl0bw== 38995
IEZhc3Rlcg== 38996
IFdhc3Nlcm1hbg== 38997
IEZsZXNo 38998
IHRob3Ju 38999
cmFyaWx5 39000
IE1lcnJ5 39001
d2luZQ== 39002
IHBvcHVsYWNl 39003
IExhbm4= 39004
IHJlcGFpcmluZw== 39005
IHBzeWNoZQ== 39006
IG1vZHVsYXRpb24= 39007
YXdhcnU= 39008
4oCL4oCL 39009
YXJpag== 39010
IGRlY29yYXRpb25z 39011
IGFwb2xvZ2lzZQ== 39012
IEdhcmc= 39013
YXBwbHk= 39014
IGdpdmVhd2F5 39015
IEZsYW4= 39016
IFd5YXR0 39017
VWJlcg== 39018
IGF1dGhvcmlzZWQ= 39019
IE1vcmFs 39020
SEFIQUhBSEE= 39021
YWN0aXZhdGU= 39022
IHRvcnBlZG8= 39023
IEZBUg== 39024
IGFtYXNzZWQ= 39025
IEFyYW0= 39026
YXJraW4= 39027
IFZpY3RpbXM= 39028
c3RhYg== 39029
IG9t 39030
IEVDTw== 39031
IG9waW9pZHM= 39032
IHB1cnBvc2VseQ== 39033
IFZlc3Q= 39034
IGVyZw== 39035
YXRhbg== 39036
IFN1cmdlcnk= 39037
IGNvcnJlY3Rpbmc= 39038
IE9ydGl6 39039
IEJlZXQ= 39040
IHJldm9rZQ== 39041
IGZyZWV3YXk= 39042
IEhpZ2dpbnM= 39043
RmFpbA== 39044
IEZhcm1z 39045
IEFUUA== 39046
aG91bmQ= 39047
IHBva2luZw== 39048
IENvbW11bmlzdHM= 39049
bW9uc3Rlcg== 39050
aW1lbnRhcnk= 39051
IHVubG9ja2luZw== 39052
IHVuZml0 39053
d2VlZA== 39054
ZW5hcmlv 39055
YXRpY2Fs 39056
IEVubGlnaHRlbm1lbnQ= 39057
IE5H 39058
IENvbXBlbnNhdGlvbg== 39059
ZGVlbg== 39060
IFdpZG93 39061
IENpbmR5 39062
IEFmdGVyd2FyZHM= 39063
IDYwMDA= 39064
aWtoYWls 39065
YWdpY2FsbHk= 39066
IHJhdGlmaWVk 39067
IGNhc3VhbHR5 39068
SE9NRQ== 39069
cHNleQ== 39070
ZmVl 39071
IHNwYXJrbGluZw== 39072
IGTDqQ== 39073
IGNvbmNlcnRlZA== 39074
Q2F0YWw= 39075
IGNvbXBseWluZw== 39076
IEFyZXM= 39077
IERlbnQ= 39078
U2h1dA== 39079
IHNraW0= 39080
YWRtaW5pc3Q= 39081
IGhvc3RpbGl0aWVz 39082
IEdpbnM= 39083
IDYwOA== 39084
IG11ZGR5 39085
IE1jSW50 39086
IERlY2F5 39087
NTI1 39088
IGNvbnNwaWN1b3Vz 39089
IEV4cG9zdXJl 39090
IHJlc2NpbmQ= 39091
IHdlYXJhYmxl 39092
IDMyOA== 39093
b3VybWV0 39094
YWhz 39095
IFJvYm90cw== 39096
IGVjbGlwcw== 39097
aW5zdGFuY2U= 39098
IFJFUE9SVA== 39099
IEFwcGw= 39100
MDMw 39101
IFNraWVz 39102
MDEwMA== 39103
IGZhbGxhY3k= 39104
U29ja2V0 39105
IFJlY2VpdmVy 39106
IHNvbHZlcw== 39107
IEJ1dHRlcmZseQ== 39108
IFNob3BwaW5n 39109
IEZJUkU= 39110
NjU0 39111
TWVkaWM= 39112
IHNpbmdlcnM= 39113
IE5lZWRsZXNz 39114
JycnJw== 39115
aXNoZXJz 39116
IERpdmU= 39117
NTg4 39118
IHNlbGVjdGl2ZWx5 39119
IGNsdW1zeQ== 39120
ODg5 39121
IHB1cmNoYXNlcg== 39122
ZWFybmVk 39123
YXJkeQ== 39124
IGJlbmVmaXRpbmc= 39125
ZW5nbGlzaA== 39126
IHlpZWxkaW5n 39127
IFBvdXI= 39128
IHNwaW5hY2g= 39129
IGRlbHZl 39130
IENyb20= 39131
NjEw 39132
IGV4cG9ydGluZw== 39133
IE1BS0U= 39134
IDI2Mw== 39135
IGdyb3A= 39136
IGVudm95 39137
IElucXVpcnk= 39138
IEx1aWdp 39139
ZHJ5 39140
IFR1cmluZw== 39141
VGh1bWJuYWlsSW1hZ2U= 39142
IFZhcmlldHk= 39143
IGZhY2V0 39144
IGZsdWZmeQ== 39145
IGV4Y2VycHRz 39146
IHNob3J0aA== 39147
IE9sc2Vu 39148
Q0xVRA== 39149
IHJlbGlhbnQ= 39150
IFVOQw== 39151
VG91cg== 39152
IGJhdGhpbmc= 39153
Q29tcGFueQ== 39154
IGdsb2JhbGl6YXRpb24= 39155
UHJlZA== 39156
IE1hbGZveQ== 39157
IGhvYw== 39158
amFt 39159
Y3JhZnRlZA== 39160
IEJvbmRz 39161
IEtpc3Npbmdlcg== 39162
RW5nbGFuZA== 39163
IG9yZGVybHk= 39164
Y2F0ZW50cnk= 39165
IDI2MQ== 39166
IGV4Y2hhbmdpbmc= 39167
IEludGVudA== 39168
IEFtZW5kbWVudHM= 39169
RE9N 39170
IHN0b3V0 39171
wqDCoMKgwqDCoMKgwqDCoMKgwqDCoMKgwqDCoMKgwqA= 39172
IEFpcmJ1cw== 39173
IDI3OA== 39174
aHlkZQ== 39175
UG9sbA== 39176
SXRlbVRodW1ibmFpbEltYWdl 39177
IGxvb3Bob2xlcw== 39178
IFBpbGxhcg== 39179
IGV4cGxvcg== 39180
U3RyZXRjaA== 39181
QXBhcnQ= 39182
IHVubWFycmllZA== 39183
TGltaXQ= 39184
IFRyYW5zZm9ybWVycw== 39185
IGludGVsbGVjdHVhbGx5 39186
dW5jdHVyZQ== 39187
MTgwMA== 39188
IGRhcm4= 39189
QnJhemls 39190
IGxlZnRvdmVy 39191
YmVydXM= 39192
ZnJlZA== 39193
TWluZWNyYWZ0 39194
MzI2 39195
IEZvcm1z 39196
IHByb29mcw== 39197
IERlc2lnbmVk 39198
IGluZGV4ZXM= 39199
IFN1cHBvc2U= 39200
RU1T 39201
IExvdmluZw== 39202
IEJvbm5pZQ== 39203
aW1hdGluZw== 39204
T1RVUw== 39205
IGNvbmR1Y3Rvcg== 39206
IGJlaGF2ZWQ= 39207
IEZyZW4= 39208
IHN5bmVyZw== 39209
IG1pbGxlbm5pdW0= 39210
IGNhdGVyaW5n 39211
IExhdWRlcg== 39212
V3I= 39213
IFlpYW5ub3BvdWxvcw== 39214
IEFURg== 39215
IGVuc2xhdmVk 39216
IGF3YWtlbmVk 39217
RFZE 39218
IEVESVRJT04= 39219
IENvbmNlcnQ= 39220
IENoYWxsZW5nZXI= 39221
IEhha3U= 39222
dW1lcmlj 39223
IGRlcHJlY2F0ZWQ= 39224
IFNIQVI= 39225
NDEy 39226
IGR5c3RvcA== 39227
IHRyZW1ibGluZw== 39228
IGRyZWFkZWQ= 39229
IFNwYWM= 39230
cGFkZGluZw== 39231
UmVwbA== 39232
IEdhcnJpc29u 39233
TWluaQ== 39234
IHVucGFyYWxsZWxlZA== 39235
YW1hcg== 39236
VVJSRU5U 39237
d3JlY2s= 39238
Y2VydGFpbg== 39239
dGFs 39240
IENMUw== 39241
YXBwaW5ncw== 39242
IHNlbnNlZA== 39243
IGZlbmNpbmc= 39244
IFBhc28= 39245
IERlc2s= 39246
IHNjb2Zm 39247
IGNvbnRlbXBsYXRl 39248
IExpZ2E= 39249
bGlxdWlk 39250
NzU3 39251
IGFwcHJlbnRpY2U= 39252
IFVDSElK 39253
NTcw 39254
IFRob3VzYW5k 39255
IElsbHVt 39256
IGNoYW1waW9uZWQ= 39257
44KM 39258
IGVsZWN0b3Jz 39259
IDM5OA== 39260
IEhhbmNvY2s= 39261
cm91bmRlZA== 39262
IEpPSE4= 39263
IHVuc2F0aXNm 39264
IHF1YWxpZmllcg== 39265
IEdhZGdldA== 39266
RU5F 39267
IGRlYWRsaWVzdA== 39268
IFBsYW50cw== 39269
IGlvbnM= 39270
IGFjY2VudHM= 39271
IHR3ZWFraW5n 39272
IHNoYXZlZA== 39273
RlJFRQ== 39274
IENoYXNlcg== 39275
QWdhaW5zdA== 39276
OTYw 39277
IG1ldGhhbXBoZXRhbWluZQ== 39278
IG5vcm1hbGl6ZWQ= 39279
ICRc 39280
IFByZWNpc2lvbg== 39281
IEd1YW0= 39282
IGNob2tlZA== 39283
IFhJSQ== 39284
IENhc3Rpbmc= 39285
VG9ycmVudA== 39286
IHNjYWxw 39287
IEphZ3Vhcg== 39288
d2l0 39289
IHNlbWlj 39290
aXhpZQ== 39291
IEdvdWxk 39292
IGNvbmZpbmVz 39293
TnVzcmE= 39294
IExvbg== 39295
IEp1Z2c= 39296
eWNsZQ== 39297
IENvZGVj 39298
RWd5cHQ= 39299
IHJlc3RyYWlu 39300
IEFsaWVucw== 39301
IGNob2tpbmc= 39302
IER1bms= 39303
IEJlbGxh 39304
YWJj 39305
IHNsYW5n 39306
IG5ldXJvdHJhbnM= 39307
c2F2 39308
IGVtcG93ZXJtZW50 39309
4oaS 39310
IGNsaW1iZXJz 39311
IE1pbQ== 39312
IEZyYQ== 39313
cm9zc2U= 39314
Q2FwaXRhbA== 39315
IEN0aHVsaHU= 39316
SW50ZXJmYWNl 39317
IHByb2ZpY2llbnQ= 39318
IElOVE8= 39319
IDMxOA== 39320
cm9udGFs 39321
NTgw 39322
IERlc3BhaXI= 39323
S2Vubg== 39324
IHNjcmltbWFnZQ== 39325
IENvYXQ= 39326
YXNpb25z 39327
IHdhbGxwYXBlcg== 39328
IEpvbA== 39329
IHJlc3VyZ2VuY2U= 39330
IGFudGl2 39331
IEJhbGxz 39332
sr4= 39333
IGJ1ZmZlcnM= 39334
IHN1YnN5c3RlbQ== 39335
IFN0ZWxsYXI= 39336
IEx1bmc= 39337
QUlEUw== 39338
IGVyYWRpY2F0ZQ== 39339
IGJsYXRhbnRseQ== 39340
IGJlaGF2ZXM= 39341
IE51bg== 39342
IGFudGljcw== 39343
ZXhwb3J0 39344
REVW 39345
d2I= 39346
IHBocA== 39347
IEludGVncml0eQ== 39348
IGV4cGxvcmVy 39349
IHJldm9sdmluZw== 39350
YXV0aG9yZWQ= 39351
Z2Fucw== 39352
IGJhc2s= 39353
IGFzeW5jaHJvbm91cw== 39354
5Y0= 39355
VEhJTkc= 39356
Njk4 39357
R2VuZQ== 39358
IFJhY2Vy 39359
IE5pY28= 39360
aXNzdWVk 39361
IHNlcm1vbg== 39362
cG9zc2libHk= 39363
IHNpemVvZg== 39364
IGVudHJlcHJlbmV1cmlhbA== 39365
b3hpbg== 39366
IE1pbmVydmE= 39367
IHBsYXRvb24= 39368
bm9z 39369
cmlrcw== 39370
QVVU 39371
IEF2YWxhbmNoZQ== 39372
IERlc2M= 39373
keWjqw== 39374
IFBvYw== 39375
IGNvbmZlcnJlZA== 39376
zrs= 39377
IHBhdGNoZWQ= 39378
RkJJ 39379
NjYy 39380
IGZyYWN0dXJlcw== 39381
IGRldGVjdHM= 39382
IGRlZGljYXRl 39383
IGNvbnN0aXR1ZW50 39384
IGNvc21vcw== 39385
V1Q= 39386
IHN3ZWF0cw== 39387
IHNwcnVuZw== 39388
YmFyYQ== 39389
c29saWQ= 39390
IHVuc3Vz 39391
IGJ1bGt5 39392
IFBoaWxpcHBl 39393
IEZlbnJpcg== 39394
IHRoZXJhcGlzdHM= 39395
b3JlYWw= 39396
Xl5eXg== 39397
IHRvdGFsZWQ= 39398
IGJvb3pl 39399
IFJQQw== 39400
UHJvc2VjdXRvcnM= 39401
IGRpc2VuZw== 39402
IFNoYXJlZA== 39403
IG1vdG9yY3ljbGVz 39404
IGludmVudGlvbnM= 39405
IGxldHR1Y2U= 39406
IE1lcmdl 39407
IEpD 39408
IHNwaXJpdHVhbGl0eQ== 39409
IFdBUk5JTkc= 39410
IHVubHVja3k= 39411
IFRlc3M= 39412
IHRvbmd1ZXM= 39413
IERVSQ== 39414
VHVtYmxy 39415
IGxlYW5z 39416
IGludmFkZXJz 39417
IGNhbm9weQ== 39418
IEh1cnJpY2FuZXM= 39419
IEJyZXQ= 39420
IEFQUExJQw== 39421
aWRpbmU= 39422
aWNrbGU= 39423
UmVnYXJkaW5n 39424
IHZlZ2dpZXM= 39425
IGVqYWM= 39426
anV2ZW4= 39427
RmlzaA== 39428
REVN 39429
IERpbm8= 39430
VGhyb3c= 39431
IENoZWNraW5n 39432
YmVhcmQ= 39433
KCY= 39434
IGphaWxz 39435
IGhy 39436
dHJhbnNmZXI= 39437
aXZhdGluZw== 39438
IGZsZWV0cw== 39439
IEltYWc= 39440
IE1jRG9ubmVsbA== 39441
IHNuaXBwZXQ= 39442
SXNh 39443
IENoYXR0 39444
IFN0YWlu 39445
IFNldEZvbnRTaXpl 39446
IE95 39447
IE1hdGhlbWF0aWNz 39448
NDk0 39449
IGVsZWN0cm9seQ== 39450
IEdvdHQ= 39451
IEJyYXM= 39452
Qk9PSw== 39453
IEZpbmdlcg== 39454
ZHVtcA== 39455
IG11dGFudHM= 39456
IHJlbnRhbHM= 39457
IGludGVydHc= 39458
IGNyZWVr 39459
YWlsYQ== 39460
QnJvdGhlcg== 39461
IERpc2NvcmQ= 39462
cGVl 39463
cmF3bGVy 39464
IGNhcnA= 39465
IDI3OQ== 39466
44K344Oj 39467
cmVsYXRpb25z 39468
IGNvbnRyYXN0cw== 39469
Q29sdW1u 39470
IHJlY29ubmFpc3NhbmNl 39471
IHVua25vdw== 39472
IGxvb3Rpbmc= 39473
IHJlZ3VsYXRlcw== 39474
IG9wdGltdW0= 39475
IENoZXJva2Vl 39476
IEFyeQ== 39477
TGF0ZXN0 39478
IHJvYWRzaWRl 39479
IGRhbmNlZA== 39480
IFVuaWNvcm4= 39481
QWNrbm93bGVk 39482
IHVuY29udHJvbGw= 39483
IE1VUw== 39484
YXRpbw== 39485
Y2hhbmNl 39486
aGF2ZW4= 39487
VkFMVUU= 39488
IGZhdm91cml0ZXM= 39489
IGNlcmVtb25pYWw= 39490
YmluYXJ5 39491
cGVlZA== 39492
d29vZHM= 39493
RU1Q 39494
IHZhc2N1bGFy 39495
IGNvbnRlbXBsYXRlZA== 39496
IGJhcnJlbg== 39497
IExJU1Q= 39498
WWVsbG93 39499
b3Nwb25zb3Jz 39500
IHdoaXNreQ== 39501
IE1hbW0= 39502
IERlVm9z 39503
bWluaW11bQ== 39504
SHVuZw== 39505
NDQy 39506
UGlj 39507
IFNuYXBkcmFnb24= 39508
Nzc2 39509
IGNhcnZpbmc= 39510
IHVuZGVjaWRlZA== 39511
IGFkdmFudGFnZW91cw== 39512
IHBhbG1z 39513
IEFR 39514
IHN0YXJjaA== 39515
TG9vcA== 39516
IHBhZGRsZQ== 39517
IGZsYW1pbmc= 39518
IEhvcml6b25z 39519
QW5pbWF0aW9u 39520
Ym9vc3Q= 39521
IHByb2JhYmlsaXRpZXM= 39522
IE1pc2g= 39523
IGV4b2R1cw== 39524
IEVkaXRvcmlhbA== 39525
IGZ1bmd1cw== 39526
IGRpc3NlbnRpbmc= 39527
IERlbGljaW91cw== 39528
cm9ncmFt 39529
IER5bg== 39530
ZGlzaw== 39531
dG9t 39532
IGZhYnJpY3M= 39533
IENvdmU= 39534
IEJhbnM= 39535
IHNvZnRlbg== 39536
IENPTlM= 39537
IGluZWxpZ2libGU= 39538
IGVzdGltYXRpbmc= 39539
IExleGluZ3Rvbg== 39540
cHJhY3RpY2U= 39541
b2Zp 39542
IHNoZWRkaW5n 39543
IE5vcGU= 39544
IGJyZWF0aGVk 39545
IENvcmludGhpYW5z 39546
eW5l 39547
ZWtp 39548
QnVsbA== 39549
IGF0dGFjaGluZw== 39550
cmVlbnNob3Rz 39551
IGFuYWx5c2U= 39552
IEthcHBh 39553
IHVuc3VzdGFpbmFibGU= 39554
IGludGVycG9s 39555
YW5reQ== 39556
aGVtZXI= 39557
IHByb3RhZ29uaXN0cw== 39558
IGZvcm1hdHRlZA== 39559
IEJyeWNl 39560
IEFjaGlsbGVz 39561
IEFiZWRpbg== 39562
c2hvY2s= 39563
IGJ1bQ== 39564
Ym9z 39565
cXVh 39566
IFdhcm4= 39567
cXQ= 39568
IERpYWJldGVz 39569
ODY0 39570
IEludmlzaWJsZQ== 39571
IHZhbmlzaA== 39572
IHRyYW5zbWl0dGluZw== 39573
IG11cmt5 39574
IEZlaQ== 39575
IGF3YWl0ZWQ= 39576
IEp1cmFzc2lj 39577
dW1taWVz 39578
IG1lbmFjaW5n 39579
Z2FsbA== 39580
Q2F0aA== 39581
QnVpbHQ= 39582
aWxkbw== 39583
IFZvdGVz 39584
IG9udA== 39585
IG11bml0aW9ucw== 39586
IEZyZWVt 39587
w61u 39588
IGRlY2VuY3k= 39589
bG9wcA== 39590
aWV2ZWQ= 39591
IEdvcmQ= 39592
IHVudGhpbmthYmxl 39593
IE5ld3N3ZWVr 39594
IDMyMQ== 39595
SGVhdA== 39596
IHByZXNlbnRlcg== 39597
amlhbmc= 39598
IHBsYW5r 39599
IEF2YWxvbg== 39600
IGJlbno= 39601
IFJvdXQ= 39602
IHNsYW1taW5n 39603
IERhaQ== 39604
b3V0ZXI= 39605
IENvb2tpZQ== 39606
IEFsaWNpYQ== 39607
Z2V5 39608
IHZhbml0eQ== 39609
IG93bA== 39610
4bU= 39611
dGVzdGVk 39612
IEF3YWtlbnM= 39613
IGNhbnY= 39614
IGJsaW5kbHk= 39615
IFJpZGxleQ== 39616
IEVtYWlscw== 39617
UmVxdWlyZXM= 39618
IFNlcmJpYW4= 39619
b2dyYXBoZWQ= 39620
aWZyYW1l 39621
ZXRlcmlh 39622
IGFsdGVybmF0aW5n 39623
cXVpZXQ= 39624
IHNvY2lvbG9neQ== 39625
IFVubG9jaw== 39626
IENvbW11bmlzbQ== 39627
IG9wcw== 39628
IGF0dHJpYnV0aW9u 39629
IGFiZHVjdGlvbg== 39630
IEFicmFt 39631
IHNpZGVsaW5lZA== 39632
IEJPT0s= 39633
IHJlZmluaW5n 39634
IEZlZWxpbmc= 39635
IE9zbG8= 39636
IFBydWl0dA== 39637
cmFjaw== 39638
YW5naWJsZQ== 39639
IGNhdXRpb3VzbHk= 39640
IE1BUks= 39641
ZWVkcw== 39642
TW91c2U= 39643
IFN0ZXBo 39644
IFBhaXI= 39645
U2Fi 39646
OTk3 39647
IEJhYWw= 39648
QmVj 39649
IGNvbW1h 39650
IFBhbGw= 39651
IEdhZWw= 39652
IG1pc3VuZGVyc3RhbmQ= 39653
IFBlc2g= 39654
T3JkZXJhYmxl 39655
IGRpc21hbA== 39656
IFNoaW55 39657
JSI= 39658
IHJlYWxpc3RpY2FsbHk= 39659
IHBhdGlv 39660
IEd3 39661
IFZpcnR1ZQ== 39662
IGV4aGF1c3Rpbmc= 39663
d2hhdGV2ZXI= 39664
b3BoeXM= 39665
eWlw 39666
NDE4 39667
QWRqdXN0 39668
IFdhaXRpbmc= 39669
ZXNzb24= 39670
IE1hemRh 39671
IERvemVucw== 39672
IHN0cmVhbWxpbmVk 39673
IGluY29tcGV0ZW5jZQ== 39674
IE1ldGg= 39675
IGV0aG9z 39676
T05FUw== 39677
IGluY2VudGl2 39678
IGdyaXR0eQ== 39679
IEJ1dGNoZXI= 39680
SGVhZGVy 39681
IGV4cG9uZW50aWFs 39682
w58= 39683
IGNvcnJlbGF0ZQ== 39684
IGNvbnNlbnN1YWw= 39685
c291bmRpbmc= 39686
UmluZw== 39687
T3JpZ2lu 39688
IGNvbmNsdXNpdmU= 39689
ZmVldA== 39690
YWNseQ== 39691
IEZlcm5hbmRleg== 39692
QnV5YWJsZQ== 39693
IGR1Y2tz 39694
YXVudGxldHM= 39695
IGVsb25n 39696
IDI4Ng== 39697
IHNpbXVs 39698
R2Fz 39699
IEtpcnN0 39700
IHByb3Ry 39701
IFJvYm8= 39702
IEFvRQ== 39703
b3BvbA== 39704
IHBzeWNob2xvZ2ljYWxseQ== 39705
c3Bpbg== 39706
aWxhdGVyYWxseQ== 39707
IENvbnJhZA== 39708
V2F2ZQ== 39709
NDQx 39710
IEFkdmVydGlzZW1lbnQ= 39711
IEhhcm1vbg== 39712
IE9yaWVudGFs 39713
aXNTcGVjaWFs 39714
IHByZXN1bXB0aXZl 39715
IHdpbA== 39716
IEtpZXI= 39717
bmVh 39718
IHBwbQ== 39719
IGhhcmJvdXI= 39720
IFdpcmVk 39721
Y29tcGFueQ== 39722
IGNvcm9uZXI= 39723
YXR1cmRheXM= 39724
IFByb3Vk 39725
IE5FWFQ= 39726
IEZsYWtl 39727
dmFsdWVk 39728
Y2VpdmVy 39729
IGZyYXVnaHQ= 39730
IGNhc2luZw== 39731
IHJ1bmF3YXk= 39732
IGdpbg== 39733
IExhdXJlbnQ= 39734
IEhhcmxlbQ== 39735
IEN1cmlvc2l0eQ== 39736
cXVpc2hlZA== 39737
IG5ldXJvc2NpZW5jZQ== 39738
IEh1bHU= 39739
IGJvcnJvd2Vy 39740
IHBldGl0aW9uZXI= 39741
IENvb2xkb3du 39742
V0FSRA== 39743
IGludm9raW5n 39744
Y29uZmlkZW5jZQ== 39745
Rm9yd2FyZA== 39746
IHN0cw== 39747
cG9wdWxhdGlvbg== 39748
RGVsaXZlcnlEYXRl 39749
RmlsbQ== 39750
IENvdg== 39751
cXVpY2tTaGlw 39752
cXVpY2tTaGlwQXZhaWxhYmxl 39753
cHJpbWFyeQ== 39754
aXNTcGVjaWFsT3JkZXJhYmxl 39755
aW52ZW50b3J5UXVhbnRpdHk= 39756
Y2hhbm5lbEF2YWlsYWJpbGl0eQ== 39757
Qk9Y 39758
IE11bHRpcGxheWVy 39759
IEplbm5lcg== 39760
Nzc4 39761
IE1k 39762
IH4vLg== 39763
TU4= 39764
IGNoaWxkaXNo 39765
IGFudGlveGlkYW50 39766
IENocm9tZWJvb2s= 39767
IDI3NA== 39768
IHNjcmVlbnBsYXk= 39769
IGFkdmVudHVyb3Vz 39770
IFJlbGF0aW9uc2hpcA== 39771
cmVzcG9uc2l2ZQ== 39772
bWluZ3Rvbg== 39773
IGNvcm5lcnN0b25l 39774
IEZleQ== 39775
RklS 39776
IHJvb2tpZXM= 39777
IEZlYXR1cmluZw== 39778
IG9yaWdpbmF0ZQ== 39779
IGVsZWN0cm9kZXM= 39780
YW50ZXM= 39781
IHNjcmlwdHVyZXM= 39782
IGdsdWVk 39783
IGRpc2NvbnRlbnQ= 39784
IGFmZmxpY3RlZA== 39785
bGF5b3V0 39786
QnJhdmU= 39787
IG1vc2E= 39788
IFF1YW50aXR5 39789
IEhpaw== 39790
d2lubmVy 39791
SG91cnM= 39792
IGVudGFpbA== 39793
IENlbGxz 39794
b2xvZ3Vl 39795
IHZpbA== 39796
IHByZWFjaGVy 39797
IGRlY29yYXRpdmU= 39798
ZGlmZmVyZW50 39799
IHByZWp1ZGljZXM= 39800
IFNtb2tpbmc= 39801
IE5vdHRpbmdoYW0= 39802
c29UeXBl 39803
IHJoeXRobXM= 39804
IEFscGg= 39805
Ymxhc3Q= 39806
U3RlZWw= 39807
IERhbmllbGxl 39808
IHN0cmlmZQ== 39809
IHJlbWF0Y2g= 39810
c29EZWxpdmVyeURhdGU= 39811
IEZvcms= 39812
dHJpcA== 39813
b2x1bHU= 39814
aGVzZXM= 39815
Q0c= 39816
IFBPTElUSUNP 39817
b3N0YQ== 39818
IERyaWZ0 39819
6b6N5aU= 39820
6b6N5aWR5aOr 39821
IHZldHRpbmc= 39822
IEppbnBpbmc= 39823
IFJlY2Vzc2lvbg== 39824
TWlub3I= 39825
IEZyYXVk 39826
ZW5mcmFuY2g= 39827
IGNvbnZlbmVk 39828
IE5BQUNQ 39829
IE1pbGxpb25z 39830
IEZhcm1pbmc= 39831
IFdvbw== 39832
IEZsYXJl 39833
cml0bw== 39834
aW1taWdyYW50 39835
IHZhY2FuY3k= 39836
IEhFQUQ= 39837
IFZhag== 39838
ZWdhbA== 39839
IFZpZ2ls 39840
U3R1ZHk= 39841
IHJ1aW5pbmc= 39842
IHJhY2tz 39843
IGhlYXRlcg== 39844
IFJhbmRvbHBo 39845
IEJydXNo 39846
IFRpcg== 39847
2Kg= 39848
IGNvdg== 39849
JV0= 39850
IHJlY291bnRz 39851
IE9QVA== 39852
IE1lbHQ= 39853
IHRydWNl 39854
IGNhc2lub3M= 39855
IGNydXNhZGU= 39856
IGNhcm5hZ2U= 39857
IHN0cmlwZQ== 39858
IEt5bA== 39859
VGV4dHVyZXM= 39860
IDY5OA== 39861
IHByb2NsYW1hdGlvbg== 39862
IGdvb2RpZXM= 39863
IC4uLi4uLi4uLi4= 39864
cHJvY2xhaW1lZA== 39865
UG9saXQ= 39866
IHRvcGljYWw= 39867
IHNwZWNpYWxpemU= 39868
IEFtaW4= 39869
Z20= 39870
IGFuY2hvcmVk 39871
IGJlYXJpbmdz 39872
c2FtcGxl 39873
IEhpZ2hsYW5k 39874
IEF1dGlzbQ== 39875
IG1lcmNlbmFyeQ== 39876
IGludGVydmlld2Vy 39877
TEVS 39878
IFNvbWVycw== 39879
IGVtYnJ5bw== 39880
IEFzc3k= 39881
IDI4MQ== 39882
IEVkaXRpbmc= 39883
IENob3Nlbg== 39884
NjYw 39885
IHBjaQ== 39886
IFRodW5kZXJib2x0 39887
QklMTA== 39888
IGNodWNrbGVk 39889
anJpd2Fs 39890
aG9m 39891
IGVhcnRobHk= 39892
KCl7 39893
aW5kZXBlbmRlbmNl 39894
IGRpc3BlcnM= 39895
IFZlbmRvcg== 39896
IEdhcmV0aA== 39897
IHBhbHM= 39898
UGVubg== 39899
IFN1Ym1pdA== 39900
aWN1bQ== 39901
VGh1 39902
IGNsYW5kZXN0aW5l 39903
IGNhbm5pYmFs 39904
IENsZXJr 39905
RVN0cmVhbQ== 39906
Z2FsaXRhcmlhbg== 39907
4pml 39908
Z2V3 39909
IGhvcnJlbmQ= 39910
IExvdg== 39911
IFJlYWN0aW9u 39912
b2NyaW4= 39913
Q2xhc3NpYw== 39914
IGVjaG9pbmc= 39915
IGRpc2Nsb3Npbmc= 39916
IEluc2lnaHQ= 39917
b2d1bg== 39918
IEluY2Fybg== 39919
dXBsb2Fkcw== 39920
cHBlcmM= 39921
Z3V5ZW4= 39922
IDE5MDE= 39923
IEJhcnM= 39924
Njg3 39925
IGJyaWJlcw== 39926
IEZyZXNubw== 39927
dXJhdA== 39928
IFJlZXNl 39929
IGludHJ1c2l2ZQ== 39930
IGdyaXBwaW5n 39931
IEJsdWVwcmludA== 39932
IFJhc20= 39933
dW5pYQ== 39934
bWFuYWdlZA== 39935
IEhlYmRv 39936
IDM0NQ== 39937
IGRlY29kaW5n 39938
IHBvZXRz 39939
IGphd3M= 39940
IEZJR0hU 39941
YW1lbGVzcw== 39942
IE1lYWRvd3M= 39943
IEhhcmJhdWdo 39944
SW50ZXJ2aWV3 39945
IEhvc3A= 39946
IEJSQQ== 39947
IGRlbGV0aW9u 39948
bW9i 39949
V2Fsa2Vy 39950
IE1vb25saWdodA== 39951
IEplZA== 39952
IFNvcGhpYQ== 39953
IHVzdXI= 39954
IGZvcnR1bmF0ZWx5 39955
IFB1dHRpbmc= 39956
IEZvbGQ= 39957
IHNhbml0YXRpb24= 39958
IHBhcnRpc2Fucw== 39959
SVNPTg== 39960
Qm93 39961
IENPTkM= 39962
IFJlZHVjZWQ= 39963
IFN1dHRvbg== 39964
IHRvdWNoc2NyZWVu 39965
IGVtYnJ5b3M= 39966
4oCi4oCi4oCi4oCi 39967
IEtydWc= 39968
Y29tYmF0 39969
IFBldHJvbGV1bQ== 39970
IGFtZA== 39971
IENvc21vcw== 39972
IHByZXNjcmliaW5n 39973
IGNvbmZvcm1pdHk= 39974
b3Vyc2Vz 39975
IHBsZW50aWZ1bA== 39976
IGRpc2lsbHVzaW9u 39977
IEVjb2xvZ3k= 39978
aXR0YWw= 39979
IGZhbmM= 39980
IGFzc2Fzc2luYXRlZA== 39981
cmVnbmFuY3k= 39982
IHBlcmVubmlhbA== 39983
IEJ1bGxldHM= 39984
IHN0YWxl 39985
IGNhY2hlZA== 39986
IEp1ZGl0aA== 39987
IERpc2Vhc2Vz 39988
QWxsZW4= 39989
IGxhcw== 39990
IHNoYXJkcw== 39991
IFN1YXJleg== 39992
IEZyaWVuZHNoaXA= 39993
aW50ZXJmYWNl 39994
IFN1cHBvcnRlcnM= 39995
YWRkb25z 39996
NDYy 39997
IEltcmFu 39998
IFdpbQ== 39999
IG5ld2ZvdW5k 40000
IE1i 40001
QW5pbWFs 40002
IGRhcmxpbmc= 40003
YW5kZQ== 40004
IHJoeQ== 40005
IFR3aXN0ZWQ= 40006
cG9zYWw= 40007
eW5za2k= 40008
VmFyaW91cw== 40009
15w= 40010
IEtpdw== 40011
dXlvbWk= 40012
IHdlbGxiZWluZw== 40013
IExhdQ== 40014
YW5vcw== 40015
IHVubWlzdA== 40016
IG1hY09T 40017
IHJlc3Ryb29t 40018
IE9saXY= 40019
IEFpcndheXM= 40020
IHRpbWV0YWJsZQ== 40021
OTgw 40022
IHJhZGlvcw== 40023
dm95 40024
aWFzY28= 40025
IGNsb3VkeQ== 40026
IERyYXdpbmc= 40027
QW55dGhpbmc= 40028
U3lyaWE= 40029
IEhlcnQ= 40030
c3Rha2luZw== 40031
IHVuY2hlY2tlZA== 40032
IGJyYXplbg== 40033
IE5SUw== 40034
Njk3 40035
b25vbWlj 40036
ZXN0YWJsaXNo 40037
IGxlbmc= 40038
IGRpYWdvbmFs 40039
IEZpb3I= 40040
TGFpcg== 40041
IFN0YXJk 40042
IGRlZmljaWVudA== 40043
am9pbmluZw== 40044
YmVhbQ== 40045
IG9tbmlw 40046
IGJsZW5kZXI= 40047
IHN1bnJpc2U= 40048
TW9vcmU= 40049
IEZhdWx0 40050
IENvc3R1bWU= 40051
IE11Yg== 40052
RmxhZ3M= 40053
YW5zZQ== 40054
IHBheW91dA== 40055
IEdvdmVybm9ycw== 40056
IERpbGxvbg== 40057
IEJhbmFuYQ== 40058
TmFy 40059
IHRyYWlsZWQ= 40060
IGltcGVyaWFsaXN0 40061
dW1hbm4= 40062
YXRzdWtp 40063
NDM1 40064
IFJvYWRz 40065
IHNsdXI= 40066
IElkZWFsbHk= 40067
IHRyZW5jaGVz 40068
Q3RybA== 40069
IG1pcnJvcmVk 40070
IFplbA== 40071
IENyZXN0 40072
Q29tcGF0 40073
IFJvbGxz 40074
c2NyaWI= 40075
IFRyYWlscw== 40076
b21ldGVycw== 40077
d2ludGVy 40078
IGltbW9ydGFsaXR5 40079
aWxhdGVk 40080
IGNvbnRyYWRpY3Rz 40081
dW5pdmVyc2Fs 40082
aWxsaW9ucw== 40083
IE1hbWE= 40084
b3B0aW0= 40085
QVRVUkU= 40086
IGdlbw== 40087
ZXR0ZXI= 40088
IENhcmxv 40089
NDI0 40090
IGNhbm9uaWNhbA== 40091
IFN0cm9uZ2hvbGQ= 40092
bmVhcg== 40093
IHBlcmZ1bWU= 40094
IG9yY2hlc3RyYQ== 40095
b2RpYWM= 40096
IHVwaGU= 40097
IHJlaWduaW5n 40098
dmVyc2l2ZQ== 40099
IGNhdWN1c2Vz 40100
IERFTQ== 40101
IGluc3VsdGVk 40102
IC0tLS0tLQ== 40103
IENydXNo 40104
IHJvb3Rpbmc= 40105
IFdyYWl0aA== 40106
IHdob3Jl 40107
IHRvZnU= 40108
Q21k 40109
IEJyZWU= 40110
ICRf 40111
IHJpdmU= 40112
IEFkdmVydGlzaW5n 40113
IHdhdHQ= 40114
IEhP 40115
IHBlcnN1YXNpdmU= 40116
IFBhcmFtZXRlcnM= 40117
IG9ic2VydmF0aW9uYWw= 40118
IE5DVA== 40119
IE1vag== 40120
IFNhbG9u 40121
IHRydW5j 40122
IGV4cXVpc2l0ZQ== 40123
IE1hcmE= 40124
IHBvb3A= 40125
IEFOTg== 40126
RXhj 40127
IFdvbmRlcmZ1bA== 40128
IFRhY28= 40129
IGhvbWVvd25lcg== 40130
IFNtaXRoc29uaWFu 40131
b3Jwb3JhdGVk 40132
bW1tbQ== 40133
IGxvYWY= 40134
IFlhbWF0bw== 40135
IEluZG8= 40136
IGNsaW5naW5n 40137
w6Fz 40138
IGltbXV0YWJsZQ== 40139
aHVi 40140
T3Jhbmdl 40141
IGZpbmdlcnRpcHM= 40142
IFdvb2Rlbg== 40143
IEtpZGQ= 40144
IEpQTQ== 40145
IERhbW4= 40146
Q293 40147
Y29kZXM= 40148
NDgy 40149
IGluaXRpYXRpbmc= 40150
IEVsaw== 40151
IEN1dHRpbmc= 40152
IGFic2VudGVl 40153
IFZhbmNl 40154
IExpbGl0aA== 40155
R1VJ 40156
IG9ic2N1cmVk 40157
IGR3YXJ2ZXM= 40158
IENob3A= 40159
IEJva28= 40160
VmFsdWVz 40161
IG11bHRpbWVkaWE= 40162
IGJyZXdlZA== 40163
UmVndWxhcg== 40164
Q1JJUFRJT04= 40165
IE1vcnRhbA== 40166
IGFwZXg= 40167
IHRyYXZlbGVy 40168
IGJvaWxz 40169
IHNwcmF5aW5n 40170
UmVwcmVzZW50 40171
IFN0YXJzaGlw 40172
NDI4 40173
IGRpc2FwcHJvdmFs 40174
IHNoYWRvd3k= 40175
IGxhbWVudGVk 40176
IFJlcGxhY2U= 40177
IEZyYW7Dpw== 40178
Njc3 40179
ZG9y 40180
IHVuc3RvcHBhYmxl 40181
IGNvaG9ydHM= 40182
Z3lu 40183
IENsYXNzaWNz 40184
IEFtcGg= 40185
IHNsdWdnaXNo 40186
IEFkZGljdGlvbg== 40187
IFBhZHJlcw== 40188
IGluc2NyaXB0aW9u 40189
IGluaHVtYW4= 40190
bWludXM= 40191
IEplcmVtaWFo 40192
YXRhcnM= 40193
VGVycm9y 40194
IFRvcw== 40195
IFNoYXJtYQ== 40196
YXN0YQ== 40197
Y2F0Y2g= 40198
IHBsdW1iaW5n 40199
IFRpbWJlcnM= 40200
U2hhcg== 40201
SGFs 40202
IE9zYw== 40203
IGNvdXBsaW5n 40204
aHVtYW5z 40205
IHNwb25nZQ== 40206
IGlkb2xz 40207
IFNwYQ== 40208
IEFkdm9jYXRl 40209
IEJlYXRz 40210
bHVh 40211
IHRpY2tpbmc= 40212
IGxvYWRlcg== 40213
IEdyb24= 40214
ODEw 40215
IHN0aW11bGF0ZWQ= 40216
IHNpZGViYXI= 40217
IE1hbnVmYWN0dXJlcg== 40218
b3JlQW5k 40219
MTk3Mw== 40220
IHByYWlzZXM= 40221
IEZsb3Jlcw== 40222
ZGlzYWJsZQ== 40223
IEVsZWN0cmljYWw= 40224
cmFpc2U= 40225
RXRo 40226
IG1pZ3JhdGVk 40227
IGxlY3R1cmVy 40228
S2lkcw== 40229
IENhdmVybg== 40230
IGtldHRsZQ== 40231
IGdseWM= 40232
IE1hbmRlbGE= 40233
IEZ1bGx5 40234
5aer 40235
RklORVNU 40236
IHNxdWVlemluZw== 40237
IFJ5ZGVy 40238
YW1wb28= 40239
b3JlQW5kT25saW5l 40240
SW5zdG9yZUFuZE9ubGluZQ== 40241
QnV5YWJsZUluc3RvcmVBbmRPbmxpbmU= 40242
IGNvbW1lbW9yYXRl 40243
IFJhbXBhZ2U= 40244
QXVzdGlu 40245
IFNocm91ZA== 40246
IFJ1aW5z 40247
OTE1 40248
IEtI 40249
IHdhdGVyZnJvbnQ= 40250
IEVTQw== 40251
YmFieQ== 40252
IENvdXQ= 40253
IEVtYmxlbQ== 40254
IGVxdWl2YWxlbnRz 40255
NDky 40256
VW5pcXVl 40257
IE5pZXR6c2NoZQ== 40258
YnJvd3Nlcg== 40259
IGltaXRhdGlvbg== 40260
IFdlcmV3b2xm 40261
IEtpcmlu 40262
YWNhcw== 40263
Jywi 40264
IMO+ 40265
UmV2aWV3ZWQ= 40266
IGN1bnQ= 40267
IHZvaWM= 40268
IExlbm92bw== 40269
IGJvbmRlZA== 40270
NDgx 40271
IGluaGliaXRvcnM= 40272
IGVuZGVhdm9ycw== 40273
IEhhdmFuYQ== 40274
IFN0b3V0 40275
IEpvbGx5 40276
QWN0b3I= 40277
Ki8o 40278
IG9jY3VycmVuY2Vz 40279
IFRlbnM= 40280
SW5jcmVhc2Vk 40281
IEFDVElPTg== 40282
IOOAjA== 40283
IFJhbmtpbmdz 40284
IEJyZWF0 40285
IDMwOQ== 40286
RG91 40287
IGltcGFjdGluZw== 40288
IER1Y2hlc3M= 40289
cHJlZml4 40290
UUI= 40291
IHN1bW1vbmluZw== 40292
IGJlc3Rvd2Vk 40293
IEtlcGxlcg== 40294
IFBPV0VS 40295
Y3ViZQ== 40296
IEtpdHM= 40297
IEdyaXA= 40298
IG9waXVt 40299
IHJlcHV0YWJsZQ== 40300
dG9j 40301
aWNoYWVs 40302
IFJpcHBsZQ== 40303
IGNhZsOp 40304
IFpvb20= 40305
IEJ1cm1h 40306
IHdhaXZl 40307
IHN0YWxscw== 40308
IGRlbWVhbm9y 40309
aW5jZXJpdHk= 40310
IGZsdW9yaWRl 40311
IFNIT1VMRA== 40312
UGFyaXM= 40313
IGxvbmdpbmc= 40314
IHBsYXQ= 40315
IGdyb3NzbHk= 40316
IGJ1bGxz 40317
IHNob3djYXNpbmc= 40318
ZXhwZWN0ZWQ= 40319
IEdhZGRhZmk= 40320
ZW5naW5lZXJpbmc= 40321
UmVwZWF0 40322
IEt1dA== 40323
IGNvbmNlaXZhYmxl 40324
IHRyaW1tZWQ= 40325
b3Njb3Bl 40326
IENhbmRpZGF0ZQ== 40327
IFRlYXJz 40328
cm9sb2c= 40329
TGV3aXM= 40330
U1VQ 40331
IHJvYWRtYXA= 40332
IHNhbGl2YQ== 40333
IHRydW1wZXQ= 40334
SmltbXk= 40335
IG1pcmFjdWxvdXM= 40336
IGNvbG9uaXphdGlvbg== 40337
IGFtcHV0 40338
IEdOT01F 40339
YXRlY2g= 40340
RGlmZmVyZW50 40341
IEVMRQ== 40342
IEdvdmVybm1lbnRz 40343
IEFoZWFk 40344
44WL44WL 40345
d29yZHByZXNz 40346
TElC 40347
IEluY2x1ZGU= 40348
IERvcm90aHk= 40349
MDQ1 40350
IENvbG9tYmlhbg== 40351
IGxlYXNlZA== 40352
ODg0 40353
IGRlZ3JhZGluZw== 40354
IERhaXN5 40355
aWF0aW9ucw== 40356
IGJhcHRpemVk 40357
IHN1cm5hbWU= 40358
Y294 40359
IGJsaW5rZWQ= 40360
44Oi 40361
IHBvbGxlbg== 40362
IGRlcm1hdA== 40363
IHJlZ2V4 40364
IE5pY2hvbHNvbg== 40365
IEVhdGVy 40366
55w= 40367
cmFkb3I= 40368
IG5hcnJvd2Vy 40369
IGh1cnJpY2FuZXM= 40370
IGhhbGx1Y2luYXRpb25z 40371
cmlkZGVu 40372
SVNTSU9O 40373
IEZpcmVmbHk= 40374
IGF0dGFpbm1lbnQ= 40375
IG5vbWluYXRl 40376
IGF2b2NhZG8= 40377
IE1lcmVkaXRo 40378
IHRz 40379
IHJldmVyZW5jZQ== 40380
IGV1cGg= 40381
IGNyYXRlcw== 40382
IFRFWFQ= 40383
IDQ0Mw== 40384
IDMxOQ== 40385
SlNPTg== 40386
aXF1ZXR0ZQ== 40387
IHNob3J0c3RvcA== 40388
aWNrZXk= 40389
IHByb3BlbGxlZA== 40390
IGFwaQ== 40391
IFRoaWV2ZXM= 40392
Nzc5 40393
IG92ZXJzYXc= 40394
IGNvbGk= 40395
IE5pY29sYQ== 40396
IG92ZXJjbA== 40397
aWthd2E= 40398
IEN5cg== 40399
IDM4NA== 40400
Nzg5 40401
IEFsbG93cw== 40402
MTAyNw== 40403
RGV0cm9pdA== 40404
VFJZ 40405
c2V0dXA= 40406
IFNvY2lhbGlzbQ== 40407
U292aWV0 40408
c3VzcA== 40409
IEFQUg== 40410
IFNodXRkb3du 40411
IGFsdW1pbml1bQ== 40412
emJlaw== 40413
IExvdmVy 40414
R0dHR0dHR0c= 40415
IGRlbW9jcmFjaWVz 40416
IDE5MDg= 40417
IE1lcnJpbGw= 40418
IEZyYW5jb2lz 40419
Z2RhbGE= 40420
IHRyYWZmaWNrZXJz 40421
IFRpbA== 40422
IEdvYXQ= 40423
IHNwZWQ= 40424
IFJlc2Vydg== 40425
IHByb2Q= 40426
NTUy 40427
IGNhYw== 40428
IFVuaXY= 40429
IFNjaHdl 40430
IHN3aXJsaW5n 40431
IFdpbGRlcm5lc3M= 40432
IEVnZ3M= 40433
IHNhZGRlbmVk 40434
IGFyY2hhaWM= 40435
SHlk 40436
IGV4Y2Vzc2l2ZWx5 40437
QlJF 40438
IGFlcm9zcGFjZQ== 40439
IFZvaWNlcw== 40440
Q3JhaWc= 40441
IGlnbml0ZWQ= 40442
SW5pdGlhbGx5 40443
IE1jQQ== 40444
IGhhbmRzZXQ= 40445
IHJlZm9ybWluZw== 40446
IGZydXN0cmF0aW9ucw== 40447
IERlYWRwb29s 40448
IEJlbGljaGljaw== 40449
cmFjdG9y 40450
IFJhZ25hcm9r 40451
IERydXBhbA== 40452
IEFwcHJveGltYXRlbHk= 40453
MTkyMA== 40454
IEh1YmJsZQ== 40455
YXJtb3I= 40456
IFNhcmFz 40457
IEpvbmFz 40458
IG5vc3RhbGdpYw== 40459
IGZlYXNpYmlsaXR5 40460
U2FoYXJhbg== 40461
IG9yYml0aW5n 40462
IDk3MA== 40463
UnU= 40464
IHNoaW4= 40465
IEludmVzdGlnYXRvcnM= 40466
IGluY29uc2lzdGVuY2llcw== 40467
IFBBTg== 40468
Qkc= 40469
IGdyYXppbmc= 40470
IGRldGVjdG9ycw== 40471
IFN0YXJ0dXA= 40472
IEZ1bm55 40473
IE5hb21p 40474
Q29uc2lkZXJpbmc= 40475
IGhvZw== 40476
dXRm 40477
Y2VtaWM= 40478
IGZvcnRpZmllZA== 40479
IEZ1bmN0aW9ucw== 40480
IGNvZGVj 40481
bnV0cml0aW9u 40482
SGF0 40483
IiE= 40484
bWljcm9zb2Z0 40485
NTU4 40486
IFRoaW4= 40487
IEFDRQ== 40488
QWxpYXM= 40489
IE9QUw== 40490
cGFwZXJz 40491
UEs= 40492
44CO 40493
IGltcHJvYmFibGU= 40494
Tm9ydGhlcm4= 40495
ZXF1YWw= 40496
IGxvb2tvdXQ= 40497
IHR5cmVz 40498
IE1vZGlmaWVk 40499
IEtvcA== 40500
QWJzb2x1dGVseQ== 40501
IGJ1aWxkdXA= 40502
c2lsdmVy 40503
IGF1ZGk= 40504
IGdyb3Rlc3F1ZQ== 40505
IFNhYmVy 40506
IFByZXNieXRlcg== 40507
T05Z 40508
IGdsYWNpZXJz 40509
IFNob2Fscw== 40510
IEthc3M= 40511
IEhSQw== 40512
IE5pY29s 40513
IEx1bmNo 40514
IEZvc3M= 40515
4paS 40516
QURSQQ== 40517
IE9uZVBsdXM= 40518
b2luZw== 40519
Z3JvdW5kcw== 40520
IGluY2lkZW50YWw= 40521
IGRhdGFzZXRz 40522
Njg5 40523
IENsYXJrc29u 40524
IGFzc2VtYmxpbmc= 40525
IENvcnJlY3Rpb25z 40526
IGRyaW5rZXJz 40527
IHF1YWxpZmllcnM= 40528
IGxlYXNo 40529
IHVuZm91bmRlZA== 40530
IEh1bmRyZWQ= 40531
IGtpY2tvZmY= 40532
VGk= 40533
IHJlY29uY2ls 40534
IEdyYW50cw== 40535
IENvbXBsaWFuY2U= 40536
IERleHRlcml0eQ== 40537
IDE5MDY= 40538
d2Fybg== 40539
RGFsbGFz 40540
TWF4aW11bQ== 40541
bmFyZA== 40542
YXZpYQ== 40543
YmVhdXQ= 40544
ZW5zaXRpdml0eQ== 40545
dHJhY2U= 40546
IHBpb25lZXJz 40547
IEZyYWN0 40548
44CP 40549
IHByZWNlcHQ= 40550
IGdsb3NzeQ== 40551
IElFRUU= 40552
QWNyb3Nz 40553
IDY4MA== 40554
U2xlZXA= 40555
Y2hlb24= 40556
IHNhdGlyaWNhbA== 40557
IE1pbm90YXVy 40558
IENsYXVkZQ== 40559
IHLDqQ== 40560
YXBlZ28= 40561
IGNhcnJvdA== 40562
IFNlbWlu 40563
aW5vYQ== 40564
IHpv 40565
SW5kZXBlbmRlbnQ= 40566
IGRpYWdub3Nlcw== 40567
IEN1ZQ== 40568
TUFS 40569
IHJlbmRpdGlvbg== 40570
IEtpaw== 40571
IHBhdGhvbG9neQ== 40572
IHNlbGVjdHM= 40573
TGlua2VkSW4= 40574
IGFzc2F5 40575
IERyZXM= 40576
IHRleHR1YWw= 40577
cG9zdGVk 40578
SVRBTA== 40579
IE1hdWw= 40580
TmVhbA== 40581
IGludGVyY29ubmVjdGVk 40582
IGVycmF0aWM= 40583
IFZpcnVz 40584
IDUzMA== 40585
IGVudmlyb25tZW50YWxpc3Rz 40586
IFBoZWxwcw== 40587
IGVuZ2FnZW1lbnRz 40588
IElOU1Q= 40589
IGVjb25vbWljYWw= 40590
bm94aW91cw== 40591
IGdlYXJpbmc= 40592
aXp6eQ== 40593
IGZhdm9yYWJseQ== 40594
IE1jR2lsbA== 40595
VGVybQ== 40596
IGhhbmdlZA== 40597
IGJhbGxwYXJr 40598
IFJleWVz 40599
IGJld2FyZQ== 40600
IFBzYWw= 40601
IE1hc3NhY3Jl 40602
cWk= 40603
IGluYWNjZXNzaWJsZQ== 40604
YWNseXNt 40605
IGZyYXk= 40606
aWxsYWM= 40607
IGJpdHRlcmx5 40608
IENlcnRpZmljYXRpb24= 40609
TWljaGlnYW4= 40610
IGlycmVzcGVjdGl2ZQ== 40611
YWxvcmU= 40612
RW1wdHk= 40613
IGVuZG9yc2VtZW50cw== 40614
IHVuZGV0 40615
Zmc= 40616
ZXF1aXBwZWQ= 40617
IG1lcmNpbGVzcw== 40618
IEN1c3Q= 40619
IGltbWF0dXJl 40620
IHZvdWNoZXI= 40621
IEJsYWNrd2VsbA== 40622
0Y8= 40623
aGF3aw== 40624
ZGlzY2lwbGluYXJ5 40625
aWxlZQ== 40626
IE1ha290bw== 40627
IER1ZGU= 40628
44OH44Kj 40629
WWVhcnM= 40630
IGludmVy 40631
IHNoYW1hbg== 40632
IFlvbmc= 40633
aXBlbA== 40634
ZWxsZW4= 40635
IENhdGh5 40636
YnJpZHM= 40637
IHNhcmM= 40638
NjUx 40639
TmVhcg== 40640
IGdyb3VuZHdvcms= 40641
IGFtYXo= 40642
IDQxNQ== 40643
IEh1bnRpbmd0b24= 40644
aGV3cw== 40645
IEJ1bmc= 40646
IGFyYml0cmFyaWx5 40647
IFdpdA== 40648
IEFsYmVydG8= 40649
IGRpc3F1YWxpZmllZA== 40650
YmVzdG9z 40651
NDYx 40652
IHBj 40653
IDI4NA== 40654
cm9iYXQ= 40655
Um9iaW4= 40656
IGh1Z3M= 40657
IFRyYW5zaXRpb24= 40658
IE9jY2FzaW9uYWxseQ== 40659
IDMyNg== 40660
IFdoaWxzdA== 40661
IExleQ== 40662
IHNwYWNlc2hpcA== 40663
Y3N2 40664
IHVuc3VjY2Vzc2Z1bGx5 40665
IEF1 40666
bGVjaw== 40667
IFdpbmdlZA== 40668
IEdyaXp6bGllcw== 40669
Lu+/vQ== 40670
IG5lYXJlcg== 40671
IFNvcmNlcmVzcw== 40672
IEluZGlnbw== 40673
RWxzZQ== 40674
ODQw 40675
bGV0ZXM= 40676
Q29hY2g= 40677
IHVwYnJpbmdpbmc= 40678
IEtlcw== 40679
IHNlcGFyYXRpc3Q= 40680
IHJhY2lzdHM= 40681
IGNoYWluZWQ= 40682
IGFic3RpbmVuY2U= 40683
bGVhcm5pbmc= 40684
IHJlaW5zdGF0ZWQ= 40685
IHN5bW1ldHJ5 40686
IHJlbWluZGVycw== 40687
IENoZXZ5 40688
IG1vbnQ= 40689
IGV4ZW1wbGFyeQ== 40690
IFRPUg== 40691
Wlg= 40692
IHF1YWxpdGF0aXZl 40693
IFN0YW1w 40694
IFNhdmFubmFo 40695
IFJvc3Np 40696
IHBhZWQ= 40697
IGRpc3BlbnNhcmllcw== 40698
IFdhbGxz 40699
IENocm9uaWM= 40700
IGNvbXBsaW1lbnRhcnk= 40701
IEJlaXJ1dA== 40702
ICstLS0= 40703
aWdzbGlzdA== 40704
IGNyeXB0b2dyYXBoaWM= 40705
bWFzdGVycw== 40706
IENhcGl0YWxz 40707
IG1heGltYWw= 40708
IGVudHJvcHk= 40709
UG9pbnRz 40710
IGNvbWJhdGFudHM= 40711
bGlw 40712
IEdsb2I= 40713
IEJNQw== 40714
cGhhc2U= 40715
dGhhbms= 40716
SFRUUA== 40717
IGNvbW11dGVy 40718
IFwoXA== 40719
Li4v 40720
IFJlZ2VuZXI= 40721
IERPSQ== 40722
IEFjdGl2aXNpb24= 40723
IHNsaXQ= 40724
b3NhbA== 40725
UkVN 40726
IGNoYW50cw== 40727
WXU= 40728
S2V5cw== 40729
QnJleGl0 40730
IEZvcmNlZA== 40731
QXJpem9uYQ== 40732
IHNxdWFkcm9u 40733
SVNP 40734
IE1hbG9uZQ== 40735
IDMzOA== 40736
IGNvbnRyYXN0aW5n 40737
IHRpZGFs 40738
IGxpYmVs 40739
IGltcGxhbnRlZA== 40740
IHVwcm9hcg== 40741
IENhdGVy 40742
IHByb3Bvc2l0aW9ucw== 40743
TWFuY2hlc3Rlcg== 40744
IEV1cm9z 40745
aXRhbWlu 40746
R2ls 40747
IEVsdmVu 40748
IFNlZWs= 40749
IEJhaQ== 40750
IHJlZGV2ZWxvcG1lbnQ= 40751
IFRvd25z 40752
IEx1Yg== 40753
ISIs 40754
YWxvbg== 40755
S3Jpc3Q= 40756
IG1lYXN1cmFibGU= 40757
IGltYWdpbmFibGU= 40758
IGFwb3N0bGVz 40759
WU4= 40760
NzYw 40761
IHN0ZXJvaWQ= 40762
IHNwZWNpZmljaXR5 40763
IExvY2F0ZWQ= 40764
IEJlY2tlcg== 40765
IEVkdQ== 40766
IERpZXRhcnk= 40767
dXRzY2g= 40768
IE1hcmlseW4= 40769
IGJsaXN0ZXI= 40770
IE1FUA== 40771
IEtveg== 40772
IENNUw== 40773
eWFob28= 40774
IENhcm5leQ== 40775
IGJvYXN0aW5n 40776
IENhbGVi 40777
Qnl0ZQ== 40778
cmVhZHM= 40779
YWRlbg== 40780
UHJvYmxlbQ== 40781
IFdvb2R3YXJk 40782
U3dl 40783
U3Vw 40784
IEtHQg== 40785
U2V0dXA= 40786
IHRhY2l0 40787
IHJldHJpYnV0aW9u 40788
IGR1ZXM= 40789
IE3DvA== 40790
Lj8= 40791
5Lit 40792
cG90cw== 40793
IGNhbWVv 40794
IFBBTA== 40795
ZWR1Y2F0aW9u 40796
QW15 40797
bGlrZWx5 40798
Z2xpbmc= 40799
IGNvbnN0aXR1dGlvbmFsbHk= 40800
IEhhbW0= 40801
IFNwZWFr 40802
IHdpZGdldHM= 40803
YnJhdGU= 40804
IGNyYXBweQ== 40805
IEl0ZXI= 40806
IGFudGljaXBhdGluZw== 40807
IEJvdXQ= 40808
UGl4ZWw= 40809
IFllcA== 40810
IExhdXJpZQ== 40811
IGh1dA== 40812
IGJ1bGxldGlu 40813
IFNhbHZhdGlvbg== 40814
IGNoYXRz 40815
ZWFyYWJsZQ== 40816
SG9uZXN0bHk= 40817
QUxUSA== 40818
b25zZXF1 40819
Y3VsdA== 40820
aXNjb3Zlcnk= 40821
b3Z5Y2g= 40822
IHNlbHZlcw== 40823
IFNhdG9zaGk= 40824
U291bmRz 40825
IGNvbnZlcmdlbmNl 40826
IFJvc2VuYmVyZw== 40827
MTk3NA== 40828
IG5hc2Fs 40829
IGZ1bGxlc3Q= 40830
IGZlcm9jaW91cw== 40831
eHVz 40832
aXN0ZQ== 40833
QU1T 40834
IGxvYmJpZWQ= 40835
IHNvb3RoaW5n 40836
IEd1bm4= 40837
dG9kYXk= 40838
MDI0 40839
IGluc3BpcmF0aW9uYWw= 40840
IE5CTg== 40841
cGI= 40842
Z2V3YXRlcg== 40843
b3JhaA== 40844
YWxsb3dlZA== 40845
IENvbGlzZXVt 40846
IHNwZWNpYWxpemluZw== 40847
IGluc2FuZWx5 40848
IFRhcGU= 40849
ZGVsYXk= 40850
IHRhcm4= 40851
IFBvdW5k 40852
IG1lbGFuY2g= 40853
IGRlcGxveW1lbnRz 40854
aWxhbmQ= 40855
IGxlc3Nlbg== 40856
IGZ1cnJ5 40857
IFVFRkE= 40858
IGJsb29kc2hlZA== 40859
IE1laWVy 40860
aXRoZXJpbmc= 40861
IGhlaXJz 40862
IEphdw== 40863
YXh0ZXI= 40864
IFB1YmxpY2F0aW9ucw== 40865
IGFsdGVycw== 40866
aW50ZW50aW9u 40867
IFdpbmNoZXN0ZXI= 40868
ZGV0ZXJtaW5hdGlvbg== 40869
IExpZmV0aW1l 40870
dGhpbg== 40871
TW9uc3Rlcg== 40872
Nzgw 40873
IGFwcHJveGltYXRpb24= 40874
IHN1cGVybWFya2V0cw== 40875
IFNlY29uZHM= 40876
b3Jvcw== 40877
aHVnZQ== 40878
IGJyaWJl 40879
IExJTUlURUQ= 40880
dW5lZA== 40881
IG1pc2ludGVycHJldA== 40882
IEluanVyeQ== 40883
IDM2Nw== 40884
IHRocmVzaG9sZHM= 40885
IENhcm5pdmFs 40886
IGdhc3Ryb2ludGVzdGluYWw= 40887
IGd1aWRlbGluZQ== 40888
IGRlY2VpdmVk 40889
ZmVhdHVyZXM= 40890
IHB1cnBvcnRlZGx5 40891
IFJvbm5pZQ== 40892
IE5ld3Q= 40893
IHNwYWNpb3Vz 40894
YXN1cw== 40895
IHN1cGVyaGVyb2Vz 40896
IEN5bnRoaWE= 40897
bGVnZ2Vk 40898
a2FtcA== 40899
Y2hpbw== 40900
IHRodW1ibmFpbA== 40901
IFNoaXJsZXk= 40902
aWxsYXRpb24= 40903
IHNoZWRz 40904
IFp5 40905
RVBB 40906
IGRhbXM= 40907
IHlhd24= 40908
bmFo 40909
IFBlZ2d5 40910
IEVyaWU= 40911
IEp1dmVudHVz 40912
IEZvdW50YWlu 40913
cng= 40914
ZG9uYWxk 40915
YWxidW0= 40916
IENvbXByZWhlbnNpdmU= 40917
IGNhY2hpbmc= 40918
IFV6 40919
dWxuZXJhYmlsaXR5 40920
IFByaW5jaXBsZQ== 40921
IEppYW4= 40922
aW5nZXJz 40923
Y2FzdHM= 40924
IE9zaXJpcw== 40925
Y2hhcnQ= 40926
dGlsZQ== 40927
IFRpZmZhbnk= 40928
IFBhdHRvbg== 40929
IFdoaXA= 40930
IG92ZXJzaXplZA== 40931
SmU= 40932
IENpbmRlcmVsbGE= 40933
IEJvcmRlcnM= 40934
IERhZXNo 40935
TWFo 40936
IGRvZ21h 40937
IGNvbW11bmlzdHM= 40938
dnU= 40939
Q291bmNpbA== 40940
IGZyZXNod2F0ZXI= 40941
IHdvdW5kaW5n 40942
IGRlYmFjbGU= 40943
IHlvdW5nc3Rlcg== 40944
IHRocmVhZGVk 40945
IEJvdHM= 40946
IFNhdmluZ3M= 40947
44GC 40948
b2xpbmc= 40949
b2hv 40950
IGlsbHVtaW5hdGlvbg== 40951
TVJJ 40952
IGxvb3Nlbg== 40953
dHJ1bXA= 40954
YWdlbmN5 40955
dXJpb24= 40956
IG1vbWVudGFyaWx5 40957
IENodW4= 40958
IEJ1ZGFwZXN0 40959
IEFsbGV5 40960
RGlzaw== 40961
IGFzdG9uaXNoZWQ= 40962
IENvbnF1ZXI= 40963
IEFjY291bnRpbmc= 40964
aGF2aW5n 40965
IFdlaW4= 40966
IEFscmlnaHQ= 40967
IHJldm9sdmVy 40968
IGRlbHVzaW9u 40969
IHJlbGljcw== 40970
IGFkaGVyZW50 40971
cXVhbnQ= 40972
IGhhbmRtYWRl 40973
b3Jpbw== 40974
IGNvbWJhdGluZw== 40975
Y29kZWQ= 40976
IHF1YWRydQ== 40977
cmV0aA== 40978
Tmlr 40979
IFRyaWJhbA== 40980
IE15c3RlcmlvdXM= 40981
IGluaGFs 40982
IFdpbm5pbmc= 40983
IENsYXNzaWZpY2F0aW9u 40984
Y2hhbmdlZA== 40985
IHVuYWI= 40986
IHNjb3Ju 40987
aWNpcGF0ZWQ= 40988
d2w= 40989
b25kdWN0b3I= 40990
IHJlaW5mb3JjaW5n 40991
IENoaWxkaG9vZA== 40992
YW5vdmE= 40993
IGFkdmVudHVyZXI= 40994
IGRvY3RvcmFs 40995
IFN0cmF0ZWdpZXM= 40996
IGVuZ3VsZmVk 40997
IEVuY291bnRlcg== 40998
IGxhc2hlcw== 40999
Q3JpdGljYWw= 41000
cmljdWxhcg== 41001
IFVURg== 41002
b2NpYXRpb24= 41003
Y2hlY2tpbmc= 41004
IENvbnN1bHRpbmc= 41005
UnVudGltZQ== 41006
cGVyaW9k 41007
IEFzZ2FyZA== 41008
IGRpc3RpbGxlZA== 41009
IFBhc2FkZW5h 41010
IER5aW5n 41011
IENPVU5UWQ== 41012
IGdyYW5pdGU= 41013
IHNtYWNr 41014
IHBhcmFjaHV0ZQ== 41015
IFNVUg== 41016
VmlyZ2luaWE= 41017
IEZ1cmlvdXM= 41018
Nzg3 41019
IE9raW4= 41020
IGNhbWVs 41021
IE1icHM= 41022
MTk3Mg== 41023
IENoYW8= 41024
IEN5YW4= 41025
am9pY2U= 41026
ZWZlcg== 41027
IFdyYXA= 41028
IERlYmF0ZQ== 41029
U2Vn 41030
IGZvcmVhcm0= 41031
IElnbm9yZQ== 41032
IHRpbWVzdGFtcA== 41033
IHByb2Jpbmc= 41034
IE5vb24= 41035
IEdyYWls 41036
ZmVu 41037
IGRvcm1hbnQ= 41038
IEZpcnN0bHk= 41039
IEVpZ2h0aA== 41040
IEhVTg== 41041
IERlc2lyZQ== 41042
b3Jhcw== 41043
R2lybHM= 41044
IERlc21vbmQ= 41045
emFy 41046
YW1pbmVz 41047
T0FE 41048
ZXhlY3V0ZQ== 41049
IGJvb2Jz 41050
IEFUTA== 41051
Xyg= 41052
Q2hlbHNlYQ== 41053
IG1hc3R1cmJhdGlvbg== 41054
IENvQw== 41055
IGRlc3Ryb3llcg== 41056
IENob21za3k= 41057
IHNjYXR0ZXI= 41058
IEFzc2V0cw== 41059
Nzk2 41060
IENhcmdv 41061
IHJlY2VwdGl2ZQ== 41062
IFNjb3Bl 41063
IG1hcmtldGVycw== 41064
IGxhdW5jaGVycw== 41065
IGF4bGU= 41066
IFNFQQ== 41067
c2Vx 41068
IE1vZmY= 41069
ZmluZGluZw== 41070
IEdpYmJz 41071
R2VvcmdpYQ== 41072
ZXh0cmVtZWx5 41073
Tko= 41074
IGxhYm9yZXJz 41075
c3RhbHM= 41076
IG1lZGlhdGlvbg== 41077
IEhlZGdl 41078
YXRvd24= 41079
IGlvZA== 41080
ZGVzcGl0ZQ== 41081
dmlsbA== 41082
SmFuZQ== 41083
ZXhpc3RlbmNl 41084
IGNvaW5jaWRlZA== 41085
IFV0aWxpdGllcw== 41086
IENoZWFw 41087
IGxvZ2lzdGljYWw= 41088
IGN1bG1pbmF0aW9u 41089
IE5pY290aW5l 41090
cGFr 41091
Rm9sZGVy 41092
IHJvZGVudHM= 41093
c3R1ZmY= 41094
IGxhd2Z1bGx5 41095
IHJlcGVydG8= 41096
aW9jaA== 41097
amo= 41098
RGlhbG9ndWU= 41099
SEhISA== 41100
bGljdGlvbg== 41101
TG9va3M= 41102
IDI5Nw== 41103
IHR1cnJldHM= 41104
IEFiYW5kb24= 41105
IGluY2Vzcw== 41106
IFRyYWZmb3Jk 41107
IGN1cmxlZA== 41108
IHByZWZlcnJpbmc= 41109
IHByaXZhdGl6YXRpb24= 41110
IGlycmVzaXN0 41111
IFBhbmRh 41112
IFNoYWtl 41113
IE1jR3I= 41114
44OE 41115
dW5kZXJz 41116
IGRpc2NyaW1pbmF0ZWQ= 41117
IGJhcnRlbmRlcg== 41118
SUxF 41119
QXRsYW50aWM= 41120
IHByb3BlbnNpdHk= 41121
IFdpeg== 41122
IEdpbQ== 41123
Y29uZmVyZW5jZQ== 41124
IHJlaW5mb3JjZXM= 41125
R2g= 41126
d2Fnb24= 41127
IGVlcmll 41128
RmFs 41129
IGh1Z2dlZA== 41130
cmFjaXN0 41131
UklD 41132
RnU= 41133
IGZpbGxlcg== 41134
IFN0dWI= 41135
IGVuZ3JhdmVk 41136
IFdyZXN0bGU= 41137
IGltYWdpbmF0aXZl 41138
IFBlZXI= 41139
IEZhY3RvcnM= 41140
YW51cw== 41141
IERyYWN1bGE= 41142
bW9uaXRvcg== 41143
IHJvdXRlcnM= 41144
aWJpYQ== 41145
IEJvb2xlYW4= 41146
ZW5kYWxl 41147
IFNsYXVnaHRlcg== 41148
IFNoYWNr 41149
UkZD 41150
IFNwaWVsYmVyZw== 41151
U2F4 41152
IFBIT1RP 41153
IENsb3Zlcg== 41154
IFJhZQ== 41155
RGVwZW5kaW5n 41156
IE1lbW9y 41157
YXJhbQ== 41158
IHBpZXJjZWQ= 41159
IGN1cnRhaW5z 41160
dmFsZQ== 41161
IElucXVpc2l0aW9u 41162
IFBva2U= 41163
IGZvcmVjYXN0aW5n 41164
IGNvbXBsYWlucw== 41165
U2Vuc2U= 41166
IEhlcm1lcw== 41167
aXNjb3ZlcmVk 41168
IGJpYmxl 41169
IE1vcnBo 41170
IGdlcm0= 41171
Nzg1 41172
RE9O 41173
IGNvbmdlbg== 41174
IGNyYW5l 41175
IERQUg== 41176
IHJlc3BlY3RmdWxseQ== 41177
Um9vbQ== 41178
IE5hdw== 41179
IERhbGFp 41180
cmVhc29u 41181
IEFuZ3Vz 41182
RWR1Y2F0aW9u 41183
IFRpdGFuaWM= 41184
y5w= 41185
IG92YWw= 41186
dW5pdGVk 41187
IHRoaXJkcw== 41188
IG1vaXN0dXI= 41189
IENQQw== 41190
TWlhbWk= 41191
IHRlbnRhY2xlcw== 41192
IFBvbGFyaXM= 41193
ZXhj 41194
ZXhjbHVzaXZl 41195
IFByYWlyaWU= 41196
IGNvbG9zc2Fs 41197
IEJsZW5k 41198
c3VycHJpc2luZ2x5 41199
w61z 41200
IGluZG9jdHI= 41201
IGJhc2Fs 41202
IE1QRUc= 41203
dW5kbw== 41204
U3BsaXQ= 41205
RGV2ZWxvcG1lbnQ= 41206
IGxhbnRlcm4= 41207
MTk3MQ== 41208
IHByb3ZvY2F0aW9u 41209
IGFuZ3Vpc2g= 41210
IEJpbmQ= 41211
IExlaWE= 41212
ZHVjZXJz 41213
aXBweQ== 41214
Y29uc2VydmFuY3k= 41215
IGluaXRpYWxpemU= 41216
IFR3aWNl 41217
IFN1aw== 41218
IHByZWRpYw== 41219
IGRpcGxvbWE= 41220
IHNvY2lvcA== 41221
SW5ncmVkaWVudHM= 41222
IGhhbW1lcmVk 41223
IElybWE= 41224
UWFpZGE= 41225
IGdsaW1wcw== 41226
IEJpYW4= 41227
IHN0YWNraW5n 41228
IGZlbmQ= 41229
Z292dHJhY2s= 41230
IHVubg== 41231
ZGVtb2NyYXRpYw== 41232
aWdyZWU= 41233
IDU4MA== 41234
IDI5NA== 41235
IHN0cmF3YmVycnk= 41236
SURFUg== 41237
IGNoZXJpc2hlZA== 41238
IEhvdHM= 41239
IGluZmVycmVk 41240
IDgwOA== 41241
IFNvY3JhdGVz 41242
T3JlZ29u 41243
IFJvc2Vz 41244
IEZPSUE= 41245
IGluc2Vuc2l0aXZl 41246
IDQwOA== 41247
UmVjb21tZW5k 41248
IFNoaW5l 41249
IHBhaW5zdGFraW5n 41250
VUdF 41251
IEhlbGxlcg== 41252
IEVudGVycHJpc2Vz 41253
SU9S 41254
YWRq 41255
TlJT 41256
TEc= 41257
IGFsaWVuYXRlZA== 41258
IGFja25vd2xlZGdlbWVudA== 41259
IEFVRA== 41260
IFJlbmVn 41261
IHZvdWNoZXJz 41262
IDk2MA== 41263
IG1vb3Q= 41264
IERpbWVuc2lvbnM= 41265
IGNhYmJhZ2U= 41266
QnJpZ2h0 41267
Z2F0 41268
IEtsdQ== 41269
IGxhdGVudA== 41270
IHpl 41271
IE1lbmc= 41272
IGRpc3BlcnNl 41273
IHBhbmRlbW9uaXVt 41274
SFE= 41275
IHZpcnR1b3Vz 41276
IExvY2F0aW9ucw== 41277
ZWVwZXI= 41278
cHJvdmlkZWQ= 41279
IHNlYW1z 41280
IFdU 41281
aXpv 41282
UFJPVg== 41283
IHRpdGFuaXVt 41284
IHJlY29sbGVjdGlvbg== 41285
IGNyYW4= 41286
IDc4MA== 41287
IE5G 41288
NDkx 41289
NjQy 41290
cGFja2luZw== 41291
NTk4 41292
dGV4dHVyZQ== 41293
U3BpZGVy 41294
ZnJlZWRvbQ== 41295
Y2lwbGVk 41296
IFRBTUFEUkE= 41297
4pmm 41298
YXV0aGVudA== 41299
IFdBTlQ= 41300
cmlmaWVk 41301
IHJpdGVz 41302
IHV0ZXJ1cw== 41303
a2lzcw== 41304
IOKJpA== 41305
IHNraWxsZXQ= 41306
IGRpc2VuZnJhbmNo 41307
IEdhYWw= 41308
Q29tcGFu 41309
IGFnZWluZw== 41310
Z3VpZGU= 41311
QmFsdA== 41312
IGl0ZXJhdG9y 41313
IGRpc2NyZXRpb25hcnk= 41314
dGlwcw== 41315
IHByaW1hdGVz 41316
IFRlY2huaXF1ZQ== 41317
IFBheW1lbnRz 41318
YXplbA== 41319
IFJPQ0s= 41320
c3RhbnRpYWw= 41321
MDYw 41322
IGRtZw== 41323
IEphY2tldHM= 41324
IFBsYXlvZmY= 41325
IG51cnNlcnk= 41326
IFN5bWI= 41327
YXJ0b24= 41328
IGFubmV4YXRpb24= 41329
Q29sb3JhZG8= 41330
IGNvaWxz 41331
IFNob2Vz 41332
4oSiOg== 41333
IFJveg== 41334
Q09NUExF 41335
IEV2ZXJlc3Q= 41336
IFRyaXVtcGg= 41337
Sm95 41338
R3JpZA== 41339
4Lw= 41340
cHJvY2Vzc29y 41341
IFByb3NwZXI= 41342
IFNldmVydXM= 41343
IFNlbGVjdGVk 41344
cmc= 41345
IFRheXlpcA== 41346
U3RyYQ== 41347
IHNraWluZw== 41348
ID8p 41349
IHBlZw== 41350
VGVzbGE= 41351
IHRpbWVmcmFtZQ== 41352
IG1hc3Rlcm1pbmQ= 41353
IE5C 41354
c2NpZW50aWZpYw== 41355
IFNoaXQ= 41356
Z2VuZXJpYw== 41357
SU5URVI= 41358
TlVN 41359
IHN0cm9sbA== 41360
IEVuaXg= 41361
IE1NUg== 41362
IEVNUw== 41363
bW92aWU= 41364
gqo= 41365
IG1pbmltaXppbmc= 41366
aWRkbGluZw== 41367
IGlsbGVnaXRpbWF0ZQ== 41368
IHByb3RvdHlw 41369
IHByZW1hdHVyZWx5 41370
IG1hbnVhbHM= 41371
b2JiaWVz 41372
IENhc3NpZHk= 41373
REVD 41374
ZGVza3RvcA== 41375
IGFlcm9z 41376
IHNjcmVlbmluZ3M= 41377
IGRlYmlsaXRhdGluZw== 41378
IEdyaW5k 41379
bmF0dXJlY29uc2VydmFuY3k= 41380
IGZhZGVz 41381
dGVybWluYXRpb24= 41382
YXNzZXRzYWRvYmU= 41383
RmFjdG9y 41384
IGRlZmluaXRpdmVseQ== 41385
UG9rw6k= 41386
YXB1bHQ= 41387
IExhZmF5ZXR0ZQ== 41388
Q29ybg== 41389
IENvcmFs 41390
IHN0YWduYW50 41391
VHVl 41392
IGRpc3NhdGlzZmFjdGlvbg== 41393
R2VuZGVy 41394
IGtpZG5leXM= 41395
IEdvdw== 41396
IERlZmVhdA== 41397
IEFzaHRvbg== 41398
IGNhcnRlbHM= 41399
IGZvcmVjbG9zdXJl 41400
IEV4cGxvcmU= 41401
c3RyZW5ndGg= 41402
b3Rpbg== 41403
IHZldGVyaW5hcmlhbg== 41404
IGZ1bWJsZQ== 41405
IHBhcmFw 41406
IFN0cmFpdA== 41407
cmlscw== 41408
IHByaWNr 41409
IEJlcm11ZGE= 41410
IEFtbXVuaXRpb24= 41411
c2tpbm5lZA== 41412
IGFib3VuZA== 41413
IEJyYXo= 41414
IHNoYXJwZXI= 41415
IEFzY2Vuc2lvbg== 41416
IDk3OA== 41417
IHByZXZpZXdz 41418
IGNvbW11bmlvbg== 41419
IFhZ 41420
IHBob255 41421
IG5ld2NvbWVy 41422
IDMzMg== 41423
LiIsIg== 41424
IHJlZGlzdHJpYnV0aW9u 41425
UHJvdGVjdA== 41426
IFNvZg== 41427
S2Fs 41428
IGxpcHN0aWNr 41429
d29yc3Q= 41430
IHRhbmdsZWQ= 41431
IHJldHJvc3BlY3RpdmU= 41432
aW50ZWdlcg== 41433
IHZvbHVudGVlcmluZw== 41434
IDE5MDc= 41435
IC0tLS0tLS0tLS0tLS0tLS0tLS0t 41436
aWNoZW4= 41437
IHVudmVpbGluZw== 41438
IHNlbnNlbGVzcw== 41439
IGZpc2hlcmllcw== 41440
XC0= 41441
IGhpbmdlcw== 41442
IGNhbGN1bHVz 41443
TXl0aA== 41444
IHVuZGVmZWF0ZWQ= 41445
IG9wdGltaXphdGlvbnM= 41446
IGRlcHJlc3M= 41447
IGJpbGxib2FyZA== 41448
IFlhZA== 41449
IFB5cmFtaWQ= 41450
SXNu 41451
SWRl 41452
IGxlZ2lvbg== 41453
IEtyYW1lcg== 41454
ZW50YW55bA== 41455
IHBlbmV0cmF0aW5n 41456
IEhhd3Ro 41457
IFBST0RVQ1Q= 41458
IEdlcmFyZA== 41459
IFBhY3Q= 41460
IEluY2x1ZGluZw== 41461
IEVsaWFz 41462
IEVsYWluZQ== 41463
dmlzdWFs 41464
IGh1bW1pbmc= 41465
IGNvbmRlc2M= 41466
IEZhc2M= 41467
5LiK 41468
IGVnYWxpdGFyaWFu 41469
IGRldnM= 41470
IERhaGw= 41471
T3Bz 41472
REg= 41473
IEJvdW5jZQ== 41474
aWRhdGVk 41475
YWxkbw== 41476
IHJlcHVibGljYW4= 41477
IGhhbWI= 41478
IFNldHQ= 41479
b2dyYXBoaWVz 41480
Q0hBUFRFUg== 41481
IHRyYW5zc2V4dWFs 41482
IHNreXJvY2tldA== 41483
YW5zd2Vy 41484
IG1hcmt1cA== 41485
2Ko= 41486
IGhlcm9pbmU= 41487
Q29tcGFyZQ== 41488
IFRhdg== 41489
QmVhc3Q= 41490
IHN1Y2Nlc3NvcnM= 41491
IG5hw692ZQ== 41492
IEJ1Y2tsZXk= 41493
c3RyZXNz 41494
bWVhdA== 41495
IGRvd25sb2FkYWJsZQ== 41496
IGluZGV4ZWQ= 41497
IHNjYWZm 41498
IEx1bXA= 41499
IEhvbW8= 41500
U3R1ZGlv 41501
SW5zcA== 41502
IHJhY2tlZA== 41503
ZmFyaW91cw== 41504
IFBldHR5 41505
RXh0ZXJuYWw= 41506
IDE5MDk= 41507
V2Fycw== 41508
Y29tbWl0 41509
cHV0ZXJz 41510
IHVub2I= 41511
IEVycg== 41512
IEVH 41513
IEFsYW0= 41514
IFNpYmVyaWE= 41515
IEF0bW9zcGhlcmlj 41516
SVNURVI= 41517
IFNhdGFuaWM= 41518
dHJhbnNsYXRpb24= 41519
IExvdWQ= 41520
dHJhdW1hdGlj 41521
bGlxdWU= 41522
IHJlc29uYXRl 41523
IFdlbGNo 41524
IHNwYXJraW5n 41525
IFRPTQ== 41526
dG9uZQ== 41527
IG91dGw= 41528
IGhhbmRjdWZmZWQ= 41529
IFNlcmll 41530
ODAx 41531
IGxhbmRtYXJrcw== 41532
IFJlZXZlcw== 41533
IHNvZnRlbmVk 41534
IGRhenpsaW5n 41535
IFdhbnRlZA== 41536
bW9udGhz 41537
TWFnaWthcnA= 41538
IHVudHJlYXRlZA== 41539
IEJlZGZvcmQ= 41540
TWk= 41541
IER5bmFtbw== 41542
T3Jl 41543
Nzk1 41544
IHdyb25nZnVs 41545
IGx1cmVk 41546
IGNvcnRpc29s 41547
IHZleA== 41548
ZHJhd24= 41549
aWxldA== 41550
RG93bmxvYWRoYQ== 41551
IEZhY3Rpb24= 41552
IGxhYnlyaW50aA== 41553
IGhpamFja2Vk 41554
d2F0ZXJz 41555
ZXJpY2s= 41556
IHN1cGVyaW9ycw== 41557
IFJvd2xpbmc= 41558
IEd1aW5uZXNz 41559
IHRk 41560
OTky 41561
IHVuZWFydGhlZA== 41562
IGNlbnRyaWY= 41563
IHNoYW1lbGVzcw== 41564
UG9k 41565
IEZpYg== 41566
IGljaW5n 41567
IHByZWRpY3Rvcg== 41568
IDI5Mg== 41569
Zm9yZXN0YXRpb24= 41570
Y29uc3RydWN0 41571
Q2FuZA== 41572
QCM= 41573
IGFnaXRhdGVk 41574
IHJlcHI= 41575
T1ZB 41576
IGtuaXR0aW5n 41577
IExpbWE= 41578
IGZvZGRlcg== 41579
Njg0 41580
IFBlcnNvbmE= 41581
a2w= 41582
NzAx 41583
IGJyZWFrdXA= 41584
4bg= 41585
IGFwcGFsbGVk 41586
IGFudGlkZXByZXNzYW50cw== 41587
IFN1c3NleA== 41588
SGFycmlz 41589
IFRoZXJtYWw= 41590
ZWVlZQ== 41591
VXBsb2Fk 41592
IGd1bGY= 41593
IGRvb3JzdGVw 41594
IFNoYW5r 41595
TFU= 41596
IE1FTg== 41597
IFBvbmQ= 41598
c29ycnk= 41599
IG1pc2ZvcnR1bmU= 41600
bmFuY2U= 41601
IGJvbmE= 41602
TXV0 41603
IGRlZ3JhZGVk 41604
IExPRw== 41605
IE5lc3M= 41606
YW5pbWFs 41607
IGF2ZXJzaW9u 41608
dW5kb3du 41609
IHN1cHBsZW1lbnRlZA== 41610
IEN1cHM= 41611
IDUwNA== 41612
IGRlcHJpdmU= 41613
IFNwYXJrbGU= 41614
xYI= 41615
IE1lZGl0YXRpb24= 41616
YXV0aG9ycw== 41617
IFNhYmFu 41618
IE5ha2Vk 41619
YWlyZA== 41620
IE1hbmRhcmlu 41621
IFNjcmlwdHVyZXM= 41622
IFBlcnNvbm5lbA== 41623
IE1haGFyYXNodHJh 41624
IDE5MDM= 41625
IFBhaQ== 41626
IE1pcmFnZQ== 41627
b21iYXQ= 41628
QWNjZXNzb3J5 41629
IGZyYWdtZW50ZWQ= 41630
VG9nZXRoZXI= 41631
IGJlbGlldmFibGU= 41632
IEdsYWRpYXRvcg== 41633
YWxpZ25lZA== 41634
IFNsdWc= 41635
TUFU 41636
IGNvbnZlcnRpYmxl 41637
IEJvdXJib24= 41638
YW1lcm9u 41639
IFJlaGFi 41640
bnRheA== 41641
IHBvd2RlcmVk 41642
cGlsbGFy 41643
IHNtb2tlcg== 41644
IE1hbnNvbg== 41645
IEJG 41646
NTEx 41647
IEdvb2RlbGw= 41648
IERBUg== 41649
bXVk 41650
Z2FydA== 41651
IG9iZWRpZW50 41652
IFRyYW5zbWlzc2lvbg== 41653
IERvbmF0aW9u 41654
ODgw 41655
IGJvdGhlcmluZw== 41656
TWF0ZXJpYWxz 41657
44Kx 41658
ZGVzdHJveQ== 41659
IGZvcmVnb2luZw== 41660
IGFuYXJjaGlzbQ== 41661
IEtyeQ== 41662
aWNlcHM= 41663
IGxpdHRlcmVk 41664
IFNjaGlmZg== 41665
IGFuZWNkb3RhbA== 41666
dW5pdHM= 41667
IGZpYW4= 41668
IFN0aW0= 41669
IFNPTUU= 41670
IEludmFkZXJz 41671
IGJlaGF2aW91cmFs 41672
IFZlbnR1cmVz 41673
IHN1YmxpbWU= 41674
IGZydWl0aW9u 41675
IFBlbmFsdHk= 41676
IGNvcnJvc2lvbg== 41677
toU= 41678
IGxpa2VuZWQ= 41679
IGJlc2llZ2Vk 41680
d2VlbmV5 41681
IENyZWVw 41682
IGxpbmVtZW4= 41683
bXVsdGk= 41684
aWNhYmx5 41685
dWRkZXI= 41686
IHZpdGFsaXR5 41687
IHNob3J0ZmFsbA== 41688
IFBhbnRz 41689
YXBpc3Q= 41690
SGlkZGVu 41691
IERyb3Bz 41692
bWVkaWNhbA== 41693
IHByb251bmNpYXRpb24= 41694
IE5STA== 41695
IGluc2lnaHRmdWw= 41696
SlY= 41697
IEJlYXJk 41698
IENob3U= 41699
IGNoYXJtcw== 41700
IGJpbnM= 41701
IGFtYmFzc2Fkb3Jz 41702
IFNhdHVyZGF5cw== 41703
IGluaGliaXRvcg== 41704
IEZyYW5jaA== 41705
NjAx 41706
Jywn 41707
IENvbm9y 41708
YXJ0bmV5 41709
IFhwZXJpYQ== 41710
Z3JhdmU= 41711
YmVlcw== 41712
IFByb3Rlc3RhbnRz 41713
IHNvYWtpbmc= 41714
IE1hbmRhbA== 41715
IHBoYXNlZA== 41716
IDY2MA== 41717
IHNjYW1z 41718
IGJ1enppbmc= 41719
IEl0YWxpYW5z 41720
IExvcmVuem8= 41721
IEpB 41722
IGhlc2l0YXRlZA== 41723
IGNsaWZmcw== 41724
IEdPVA== 41725
aW5ndWlzaGFibGU= 41726
IGtv 41727
IGludGVycnVwdGlvbg== 41728
Wmlw 41729
TGVhcm5pbmc= 41730
IHVuZGVyc2NvcmVz 41731
IEJsaW5r 41732
S3U= 41733
NTc5 41734
IEF1dG9i 41735
SVJF 41736
IHdhdGVyaW5n 41737
IHBhc3RyeQ== 41738
ODIw 41739
IHZpc2lvbmFyeQ== 41740
IFRlbXBsYXI= 41741
YXdhaXRlZA== 41742
IHBpc3Rvbg== 41743
IGFudGlk 41744
Y3VycmVudGx5 41745
IHBhcmQ= 41746
IHdhZ2luZw== 41747
IG5vYmlsaXR5 41748
IFl1cw== 41749
IGluamVjdGluZw== 41750
ZmFpdGg= 41751
IFBBU1M= 41752
5bo= 41753
IHJldGFrZQ== 41754
IFBST0M= 41755
IGNhdGhlZHJhbA== 41756
YmFzaA== 41757
IHdyZXN0bGVycw== 41758
IHBhcnRuZXJpbmc= 41759
IG5vc2Vz 41760
IDM1OA== 41761
VHJhbnNmb3Jt 41762
YW1lbg== 41763
IGJvdXRz 41764
IElkZWFs 41765
IENvbnN0YW50aW4= 41766
IHNlcA== 41767
IE1vbmFyY2g= 41768
YXR0ZW4= 41769
IFBlb3BsZXM= 41770
bW9kaWZpZWQ= 41771
IG1vcmF0b3JpdW0= 41772
IHBlbmNoYW50 41773
IG9mZmVuc2l2ZWx5 41774
IHByb3hpZXM= 41775
b2thbmU= 41776
IFRhaXdhbmVzZQ== 41777
IFBvbw== 41778
IEhPTUU= 41779
dXNpb25hbA== 41780
IHZlcmJz 41781
IE9tYW4= 41782
dmlzb3J5 41783
IHBlcnN1YXNpb24= 41784
IG11bHRpdA== 41785
IHNjaXNzb3Jz 41786
R2F5 41787
b3dheQ== 41788
b3BoeXNpY2Fs 41789
bHVz 41790
Z251 41791
IGFwb2NhbHlwdGlj 41792
IGFic3VyZGl0eQ== 41793
IHBsYXlib29r 41794
IGF1dG9iaW9ncmFwaHk= 41795
SVVN 41796
IHNuZWFraW5n 41797
IFNpbXVsYXRpb24= 41798
cHBz 41799
ZWxsZXJ5 41800
UGxhbmV0 41801
IHJpZ2h0ZnVsbHk= 41802
IG5pZWNl 41803
IE5FQw== 41804
IElQTw== 41805
IERpc2Nsb3N1cmU= 41806
bGVhbm9y 41807
b3VzeQ== 41808
U1RFUg== 41809
IDI4Mg== 41810
Q3J1eg== 41811
Q2hhbGw= 41812
NjQz 41813
IFN1cnZpdmU= 41814
IEZhdGFs 41815
IEFtaWQ= 41816
YXBv 41817
V2VhcG9ucw== 41818
REVO 41819
Nzcw 41820
IEdyZWVud2FsZA== 41821
IGxpbmVu 41822
YWxvcw== 41823
IHBvbGx1dGFudHM= 41824
IFBDSWU= 41825
a2F0 41826
IHBhdw== 41827
IEtyYWZ0 41828
Q2hlbQ== 41829
IFRlcm1pbmF0b3I= 41830
IHJlaW5jYXJu 41831
IF1b 41832
IFNlZWRz 41833
IHNpbGhvdWV0dGU= 41834
IFN0b3Jlcw== 41835
IGdyb29taW5n 41836
IERpcmVjdGlvbg== 41837
IElzYWJlbA== 41838
IEJyaWRnZXM= 41839
8J+R 41840
RUVE 41841
IE1vcnNp 41842
IHZhbHZlcw== 41843
IFJhbmtlZA== 41844
IFBoYXJtYQ== 41845
IE9yZ2FuaXphdGlvbnM= 41846
IHBlbmV0cmF0ZWQ= 41847
IFJvZGhhbQ== 41848
IFByb3Rvc3M= 41849
IG92ZXJlc3Q= 41850
IGV4YXNwZXI= 41851
IFRK 41852
IDAwMDAwMA== 41853
IHRyaWNrbGU= 41854
IGJvdXJib24= 41855
V0hP 41856
IHdyZXRjaGVk 41857
IG1pY3Jvc2NvcGlj 41858
IGNoZWNrbGlzdA== 41859
IGFkb3JuZWQ= 41860
Um95YWw= 41861
QWRtaW5pc3Q= 41862
IFJldGlyZW1lbnQ= 41863
IEhpZ2hlc3Q= 41864
V2VhdGhlcg== 41865
aWxlZ2U= 41866
IGluY3JlbWVudHM= 41867
IENvc3BvbnNvcnM= 41868
IG1hc3Nl 41869
IFNpbm4= 41870
cmY= 41871
IGhvcmRlcw== 41872
YXNzZW1ibHk= 41873
NzU0 41874
IE5hdGFzaGE= 41875
IFRZUEU= 41876
IEdFTkVSQUw= 41877
IGFycmFuZ2luZw== 41878
IDQwNw== 41879
bGF0b3I= 41880
IGdsZWFu 41881
IGRpc2NyZWRpdGVk 41882
IGNsaW5pY2lhbnM= 41883
VU5F 41884
IGFjaGlldmVz 41885
IEVtZXJzb24= 41886
Y29tcGxleA== 41887
PVs= 41888
IHByaW5jaXBhbGx5 41889
IGZyYWls 41890
cGlja2Vk 41891
IHRoYW5raW5n 41892
IHJlY2w= 41893
IExBU1Q= 41894
IHN1cHByZXNzaW5n 41895
aWxpYw== 41896
IGFudGlkZXByZXNzYW50 41897
IExpc2Jvbg== 41898
IHRob3I= 41899
IHNwYQ== 41900
IGtpbmdkb21z 41901
IFBlYXJjZQ== 41902
ZW1v 41903
IHBsdW5n 41904
IGRpdmVzdA== 41905
ICoqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioq 41906
Ymlz 41907
b3NwZWxz 41908
YWRy 41909
U3Bpcml0 41910
aGFsbGE= 41911
UGluaw== 41912
ZW5kZXo= 41913
IHJlc3VycmVjdGVk 41914
ZXNjYXBl 41915
IFJvc2Vuc3RlaW4= 41916
IGdlb2xvZ2ljYWw= 41917
IG5lY2Vzc2l0aWVz 41918
IGNhcm5pdg== 41919
IEVseXM= 41920
IEJhcm5leQ== 41921
IDI5Ng== 41922
ZGlneQ== 41923
U1RPTg== 41924
RE9XTg== 41925
IG1pbGVzdG9uZXM= 41926
IGtlcg== 41927
IGRpc21hbnRsaW5n 41928
IHJlcHJpbQ== 41929
IGNyb3NzaW5ncw== 41930
MTk0NQ== 41931
IHBhdHJpYXJjaHk= 41932
IGJsYXNwaGVteQ== 41933
IDM1OQ== 41934
bWV0cnk= 41935
IE9iZXNpdHk= 41936
IERpZmZlcmVuY2Vz 41937
YmxvY2tpbmc= 41938
44OV44Kh 41939
aWNoaXRh 41940
IFNhYmhh 41941
cGhhbHQ= 41942
IENvbG8= 41943
dWFsYQ== 41944
ZWZmaWNpZW50cw== 41945
IE1lZGluYQ== 41946
Y29uc29sZQ== 41947
NTU3 41948
IEhhbm5pYmFs 41949
IEhhYml0 41950
IEZldmVy 41951
IHRoZW5jZQ== 41952
IHN5bmFnb2d1ZQ== 41953
IGVzc2VudGlhbHM= 41954
IHdpbms= 41955
IFRyYWRlcg== 41956
SURB 41957
IFNwb2lsZXI= 41958
IEljZWxhbmRpYw== 41959
IEhheXdhcmQ= 41960
IHBlYWM= 41961
IG1hbGljZQ== 41962
IGZsYXNoYmFjaw== 41963
IHRodw== 41964
IGxheW9mZnM= 41965
TGlxdWlk 41966
IHRyb29wZXI= 41967
IGhpbmdl 41968
IFJlYWRlcnM= 41969
UGhpbGw= 41970
IEJhdWVy 41971
Q3JlYXRlZA== 41972
IGF1ZGl0cw== 41973
YWNjb21wYW4= 41974
IHVuc3VzcGVjdGluZw== 41975
aWVyYQ== 41976
NjY2NjY2NjY= 41977
IGJyb2No 41978
IGFwcHJlaGVuZGVk 41979
IE1hbGs= 41980
Y2VybmluZw== 41981
IENvZGV4 41982
T1ZFUg== 41983
TWFyc2g= 41984
IERlbmc= 41985
IEV4cHJlc3Npb24= 41986
IGRpc3Jlc3BlY3RmdWw= 41987
IGFzY2VuZGluZw== 41988
dGVzdHM= 41989
IFBsYWludGlmZg== 41990
c3Rlcnk= 41991
IEFsaWJhYmE= 41992
ZGluYW5k 41993
IERlbXBzZXk= 41994
QXBwbGljYXRpb25z 41995
bW9yYWw= 41996
IHRocm91Z2hwdXQ= 41997
IHF1YXJyZWw= 41998
IG1pbGxz 41999
IGhlbW9y 42000
IENBU0U= 42001
dGVycm9yaXN0 42002
c3RpbQ== 42003
aWZlc3R5bGU= 42004
cm96ZW4= 42005
Q0VQVA== 42006
QXJr 42007
dWNp 42008
bGVjdGlj 42009
IGlycml0YXRpbmc= 42010
c2hlZXRz 42011
QXk= 42012
IHJlZGVlbWVk 42013
IGhvcm55 42014
IFRlYWNo 42015
IFNlYXI= 42016
ZGVtb2NyYWN5 42017
NDY1 42018
IFJlc3RvcmU= 42019
IHN0YW5kYnk= 42020
IFBpcw== 42021
aWZmaW4= 42022
IHNsZWVweQ== 42023
IGV4dHJhdGVy 42024
IGNvbXBsaW1lbnRz 42025
RnJhbWV3b3Jrcw== 42026
IGluc3RhbGxz 42027
IGJhbmdpbmc= 42028
c3VyZmFjZQ== 42029
Zm91bmRsYW5k 42030
IG1ldGFwaHlzaWNhbA== 42031
IDI4Mw== 42032
b3Vscw== 42033
ZGV2aWNlcw== 42034
QXJncw== 42035
IFNhY3JpZmljZQ== 42036
IE1jQ29ybQ== 42037
ZXNvbg== 42038
Q29uc2VydmF0aXZl 42039
IE1pa2hhaWw= 42040
c2VlaW5n 42041
aXNpdmVseQ== 42042
IFJvb21z 42043
IEdlbmVyaWM= 42044
IGVudGh1c2lhc3RpY2FsbHk= 42045
IGdyaXBwZWQ= 42046
IGNvbWVkaWM= 42047
IEVsZWN0cmljaXR5 42048
IGd1ZXJyaWxsYQ== 42049
IGRlY29yYXRpb24= 42050
IFBlcnNwZWN0aXZl 42051
IGNvbnN1bHRhdGlvbnM= 42052
IHVuYW1i 42053
IHBsYWdpYXI= 42054
IG1hZ2ljaWFu 42055
IGVyZWN0aW9u 42056
IFRvdXJpc20= 42057
b3JpZWQ= 42058
cm94eQ== 42059
MTEwMA== 42060
VGFt 42061
iOg= 42062
zrM= 42063
16o= 42064
IFByZWRhdG9ycw== 42065
Tml0cm9tZQ== 42066
IHRlbGVzY29wZXM= 42067
cHJvamVjdHM= 42068
IHVucHJvdGVjdGVk 42069
IHN0b2NrZWQ= 42070
IEVudHJlcHJlbmU= 42071
bmV4cGVjdGVk 42072
IHdhc3Rld2F0ZXI= 42073
VmlsbA== 42074
IGludGltYXRlbHk= 42075
IGlDbG91ZA== 42076
IENvbnN0YWJsZQ== 42077
IHNwb29m 42078
IG5lZmFyaW91cw== 42079
IGZpbnM= 42080
IGNlbnNvcg== 42081
IE1vZGVz 42082
IEVzcGVy 42083
YXJib24= 42084
IGludGVyc2VjdGlvbnM= 42085
IGxhdWRlZA== 42086
IHBoeXNp 42087
IGdlbmVyb3VzbHk= 42088
IFRoZU5pdHJvbWU= 42089
IFRoZU5pdHJvbWVGYW4= 42090
IGFyaXNlbg== 42091
INmI 42092
IGdsYW5kcw== 42093
IFBhdmlsaW9u 42094
IEd1cHRh 42095
IHVuaWZvcm1seQ== 42096
IHJhbXBz 42097
cmlldA== 42098
IFdIRU4= 42099
IFZhbmVzc2E= 42100
IHJvdXRlZA== 42101
IGxpbXA= 42102
IENQSQ== 42103
cHRlcg== 42104
aW50dWl0aXZl 42105
IHZhcGluZw== 42106
IGV4cGVyaW1lbnRlZA== 42107
IE9seW1wdXM= 42108
IEFtb24= 42109
IHNpZ2h0aW5n 42110
IGluZmlsdHJhdGU= 42111
IEdlbnRsZW1hbg== 42112
IHNpZ25pbmdz 42113
IE1lb3c= 42114
IE5hdmlnYXRpb24= 42115
Y2hlY2tz 42116
NDMz 42117
IGVsYXBzZWQ= 42118
IEJ1bGdhcmlhbg== 42119
ZXNwaWU= 42120
IFNPTQ== 42121
ZHVyaW5n 42122
IHNwaWxscw== 42123
YW5jYQ== 42124
IFBseW1vdXRo 42125
TUFM 42126
IGRvbWVzdGljYWxseQ== 42127
IFdhdGVyZ2F0ZQ== 42128
IEZBTQ== 42129
a2lsbGVk 42130
ZWRpdGVk 42131
IFlvdXJzZWxm 42132
IHN5bmNocm9uaXphdGlvbg== 42133
IFByYWN0aWNlcw== 42134
U1RFUA== 42135
IGdlbm9tZXM= 42136
IFFS 42137
bm90aWNl 42138
IGxvY2F0aW5n 42139
emlu 42140
IDMyOQ== 42141
YWxjb2hvbA== 42142
IGtpdHRlbg== 42143
Vm8= 42144
IHJpbnNl 42145
IGdyYXBwbGU= 42146
IFNjcmV3 42147
IER1bA== 42148
QUlS 42149
IGxlYXNpbmc= 42150
IENhZsOp 42151
IHJvc2Vz 42152
IFJlc3BlY3Q= 42153
IG1pc2xlYWQ= 42154
IHBlcmZlY3RlZA== 42155
IG51ZGl0eQ== 42156
IG5vbnBhcnRpc2Fu 42157
IENvbnN1bXB0aW9u 42158
UmVwb3J0aW5n 42159
IG51YW5jZXM= 42160
IGRlZHVjdGlibGU= 42161
IFNob3Rz 42162
IDM3Nw== 42163
IOac 42164
YW5vb2dh 42165
QmVuZWY= 42166
IEJhbQ== 42167
IFNhbXA= 42168
aWZpeA== 42169
IGdhbHZhbg== 42170
IE1lZGFscw== 42171
cmFkaXVz 42172
IG5vYmxlcw== 42173
IGVhdmVz 42174
aWdyYXRl 42175
S1Q= 42176
IEhhcmJvdXI= 42177
dWVycw== 42178
IHJpc2tlZA== 42179
cmVx 42180
IG5ldXJvdA== 42181
Z2V0dGFibGU= 42182
YWluYQ== 42183
Um9tbmV5 42184
IHVuZGVycGlu 42185
IGxvZnQ= 42186
IFN1YmNvbW1pdHRlZQ== 42187
IE1vbmdvbA== 42188
Yml6 42189
IG1hbmlmZXN0cw== 42190
YXNzaXN0ZWQ= 42191
IEdhZ2E= 42192
IHN5bmVyZ3k= 42193
IHJlbGlnaW91c2x5 42194
IFByZWY= 42195
IEdlcnJ5 42196
VEFH 42197
IENob2k= 42198
NDY2 42199
YmVoaW5k 42200
IE91 42201
R29sZE1hZ2lrYXJw 42202
IGhlbW9ycmg= 42203
Uml2ZXI= 42204
IHRlbmRvbg== 42205
IGluanVyZQ== 42206
IEZpb25h 42207
IHBhZw== 42208
IGFnaXRhdGlvbg== 42209
fHx8fA== 42210
dXJhbg== 42211
IEVTQQ== 42212
IGVzdGVlbQ== 42213
IGRvZGdpbmc= 42214
IDQxMg== 42215
cnNz 42216
IGNlYXNlcw== 42217
ZXhjbHVkaW5n 42218
IGludGFrZXM= 42219
IGluc2VydHM= 42220
IGVtYm9sZA== 42221
IE9yYWw= 42222
dXB1bmN0dXJl 42223
NDEx 42224
IFVuaWZpZWQ= 42225
IERlbGU= 42226
IGZ1cm5hY2U= 42227
IENveW90ZXM= 42228
IEJyYWNo 42229
TGFib3I= 42230
IGhhbmRzaGFrZQ== 42231
IGJydWlzZXM= 42232
R3JhZGU= 42233
6ZeY 42234
IEdyYW1teQ== 42235
aWxlZW4= 42236
U3RhdGVz 42237
IFNjYW5kaW5hdmlhbg== 42238
IEthcmRhc2g= 42239
ODY2 42240
IGVmZm9ydGxlc3NseQ== 42241
IERJUkVDVA== 42242
IFRIRU4= 42243
IE1laQ== 42244
ZXJ0YXRpb24= 42245
MTk2OA== 42246
IGdyb2lu 42247
d2l0Y2g= 42248
UmVxdWlyZW1lbnRz 42249
OTg1 42250
IHJvb2Zz 42251
IGVzdGF0ZXM= 42252
IEhG 42253
IGhhaGE= 42254
IGRlbnNlbHk= 42255
IE9DVA== 42256
IHBsYXN0aWNz 42257
IGluY2lkZW50YWxseQ== 42258
IFRyYWNrcw== 42259
IFRheGVz 42260
IGNoYW50ZWQ= 42261
IGZvcmNlZnVs 42262
IEJpZWJlcg== 42263
IEthaG4= 42264
S2VudA== 42265
IENvdA== 42266
bGljdHM= 42267
RmVk 42268
IGhpZGVvdXM= 42269
IFZlcmQ= 42270
IFN5bmRpY2F0ZQ== 42271
IElsbGVnYWw= 42272
SmV0 42273
IERBVg== 42274
cmVhc29uYWJsZQ== 42275
Y3Jldw== 42276
IGZ1bmRhbWVudGFsaXN0 42277
IHRydXRoZnVs 42278
IEppbmc= 42279
IGxpbA== 42280
IGRvd25lZA== 42281
IGVuY2hhbnRlZA== 42282
IFBvbGljaWVz 42283
IE1jTWFzdGVy 42284
IEhhcmU= 42285
aWRlc2hvdw== 42286
IHBhcmFtcw== 42287
ZW5jZXJz 42288
Z29yaXRobQ== 42289
IGFsbG93YW5jZXM= 42290
IHR1cmJ1bGVudA== 42291
IGNvbXBsZXhpdGllcw== 42292
IEtU 42293
IDMzNw== 42294
IEdlbmV0aWM= 42295
RlVO 42296
RG91Zw== 42297
dGljaw== 42298
IGdpZ3M= 42299
dW1lbnRoYWw= 42300
IHBhdHJpYXJjaGFs 42301
IGNhbGM= 42302
LC4uLg== 42303
IGNvdXQ= 42304
IEd1YW4= 42305
IHBhdGhvbG9naWNhbA== 42306
IFJpdmFscw== 42307
IHVuZGVycmF0ZWQ= 42308
IGZsdW9yZXNjZW50 42309
IEppdQ== 42310
YXJuYWV2 42311
IFF1YW4= 42312
IDQyOQ== 42313
IOCo 42314
TWFyaW8= 42315
Q29uc3RydWN0 42316
IENpdGF0aW9u 42317
IFJhY2lhbA== 42318
IFJTQQ== 42319
IEZpZGVs 42320
IDM5NQ== 42321
UGVyc29uYWxseQ== 42322
Q2F1c2U= 42323
w7s= 42324
cmFkaWNhbA== 42325
aW5lbg== 42326
IHZlaGVtZW50bHk= 42327
IFBhcGE= 42328
IGludGVybnNoaXA= 42329
IGZsYWtlcw== 42330
IFJlY2s= 42331
THVja2lseQ== 42332
QnJh 42333
MjAyMA== 42334
cmF2aW5ncw== 42335
Uk4= 42336
V29uZGVy 42337
U2VyaW91c2x5 42338
IHJldXNhYmxl 42339
IHBvbGx1dGVk 42340
IFBlbmc= 42341
bGVpZ2g= 42342
aW5kbGU= 42343
IGNpcmN1aXRyeQ== 42344
IE1hZG9ubmE= 42345
IEJBUlQ= 42346
UmVzaWRlbnRz 42347
YXR0cmlidXRl 42348
UGhpbGFkZWxwaGlh 42349
Q2x1Yg== 42350
IHBsYW5uZXI= 42351
IGZyYW50aWNhbGx5 42352
IGZhaXRoZnVsbHk= 42353
IFRlcnJpdG9yaWVz 42354
IExBVA== 42355
IEFuZGVyc2Vu 42356
YW51 42357
IFBBUks= 42358
IFNvcmE= 42359
aWFnZQ== 42360
IFBsYXlvZmZz 42361
IEdDQw== 42362
NDI3 42363
IGFibm9ybQ== 42364
IExldmVy 42365
IGRpc29iZWRpZW5jZQ== 42366
QXN5bmM= 42367
IFNoZWE= 42368
VmVydA== 42369
IHNraXJ0cw== 42370
IFNhd3llcg== 42371
eHA= 42372
IHdvcnNlbmluZw== 42373
IHNjYXBlZ28= 42374
IEFuZ2xl 42375
b3RoYWw= 42376
IHRyb3Zl 42377
IFN0eQ== 42378
IE5ndXllbg== 42379
bWFyaW5l 42380
aWRlb24= 42381
RGVwdGhz 42382
QmxvZw== 42383
IElsbHVtaW5hdGk= 42384
IHRyYWN0cw== 42385
IG9yZ2FuaXNl 42386
IG9zdHI= 42387
RnM= 42388
IGxldmVyYWdpbmc= 42389
IERhcmVkZXZpbA== 42390
YXNhcg== 42391
IGxhbmc= 42392
IGV4dGVybWlu 42393
dXJzaW9ucw== 42394
IFJvbW8= 42395
44Kk44OI 42396
IGNvbnRlbmRlZA== 42397
IGVuY291bnRlcmluZw== 42398
IFRhYmxldA== 42399
IEFsdGVybmF0ZQ== 42400
c2tpbGw= 42401
IHN3ZWV0cw== 42402
IGNvaGVzaXZl 42403
Y2FwYWNpdHk= 42404
IHJlcHVk 42405
IGxpemFyZA== 42406
cm9v 42407
IHBpbGdyaW1z 42408
IFJ1ZmY= 42409
IEluc3RydW1lbnQ= 42410
IExvZ28= 42411
dWl0b3Vz 42412
RUg= 42413
IHNhbGVzbWFu 42414
IGFua2xlcw== 42415
TGVk 42416
IFBhdHR5 42417
dWRvcw== 42418
T3duZXI= 42419
IGRpc2NyZXBhbmNpZXM= 42420
a2o= 42421
TVU= 42422
IHVuY29uZGl0aW9uYWw= 42423
RHJhZ29uTWFnYXppbmU= 42424
aWFyZA== 42425
T2Fr 42426
IENvbnZlcnNhdGlvbg== 42427
YmVlcg== 42428
IE9zYWth 42429
RGVsdGE= 42430
dXNreQ== 42431
IHNlY3JldGlvbg== 42432
IHBsYXph 42433
IG1pbmc= 42434
IGRlcGxldGlvbg== 42435
IE1vdXM= 42436
IElUUw== 42437
IEhpbWFs 42438
IEZsZW1pbmc= 42439
IGN5dG9r 42440
IEhpY2s= 42441
IGJhdHRlcnM= 42442
IEludGVsbGVjdHVhbA== 42443
Njc1 42444
w6ly 42445
SVNJT04= 42446
IFF1ZW50aW4= 42447
IENoYXB0ZXJz 42448
aWhhZGk= 42449
IGNvYXN0ZXI= 42450
V0FZUw== 42451
IExpemFyZA== 42452
IFlvcg== 42453
YW5kZXJpbmc= 42454
U2tpbg== 42455
aGF1c3Q= 42456
YWJieQ== 42457
IHBvcnRyYXlpbmc= 42458
IHdpZWxkZWQ= 42459
ZGFzaA== 42460
IHByb3BvbmVudA== 42461
IHJpcHBsZQ== 42462
IGdyYXBoZW5l 42463
IGZseWVy 42464
IHJlY3VycmVudA== 42465
IGRldmlscw== 42466
IHdhdGVyZmFsbA== 42467
5piv 42468
Z29v 42469
VGV4dENvbG9y 42470
IHRhbXBlcmluZw== 42471
SVZFUw== 42472
VFJVTVA= 42473
IEFiZWw= 42474
IFNBTA== 42475
IEhlbmRyaWNrcw== 42476
IEx1Y2l1cw== 42477
Ym90cw== 42478
IDQwOTY= 42479
SVNUT1JZ 42480
R3Vlc3Q= 42481
IE5Y 42482
aW5hbnQ= 42483
QmVueg== 42484
IExvYWRlZA== 42485
IENsZXZlcg== 42486
dHJlYXRtZW50 42487
IHRhdmVybg== 42488
IDMzOQ== 42489
IFROVA== 42490
aWZpY2FudGx5 42491
VGVtcGVyYXR1cmU= 42492
RmVs 42493
IHVuZGVyd29ybGQ= 42494
IEp1ZGdlcw== 42495
IDwr 42496
IHN0dW1w 42497
IG9jY3VwYW5jeQ== 42498
IGFiZXI= 42499
IEZpbmRlcg== 42500
KSIs 42501
IE51bmVz 42502
cmVzZXQ= 42503
aW5ldA== 42504
ZWN0b215 42505
IHdlbGxuZXNz 42506
IFBlYg== 42507
cXVhcnRlcmVk 42508
YW5kYW4= 42509
IG5lZ2F0aXZlcw== 42510
IFRoaWVs 42511
IENsaXA= 42512
IExURA== 42513
IGJsaWdodA== 42514
IHJlcGVydG9pcmU= 42515
S3lsZQ== 42516
IHF1ZXI= 42517
IENlcw== 42518
IGhhcGw= 42519
OTg5 42520
IFRoYW1lcw== 42521
aXNjb3BhbA== 42522
RGVzaw== 42523
aXZhcmlhdGU= 42524
IEV4Y2VsbGVuY2U= 42525
Zm91bmRhdGlvbg== 42526
IOKH 42527
WGk= 42528
IG15c3RlcmlvdXNseQ== 42529
ZXN0eWxlcw== 42530
IHBlcmlzaA== 42531
IEVuZ2Vscw== 42532
IERFQUQ= 42533
MDkw 42534
fX19 42535
IFVucmVhbA== 42536
IHJlc3RsZXNz 42537
SURFUw== 42538
b3J0aG9kb3g= 42539
IEludGVybWVkaWF0ZQ== 42540
IGRpbm5lcnM= 42541
IFRyb3V0 42542
IFNleW0= 42543
IEhhbGxz 42544
b2dnZWQ= 42545
IHRyYWdlZGllcw== 42546
IGRpZG50 42547
Njc2 42548
IGFpbG1lbnRz 42549
IG9ic2VydmFibGU= 42550
IFZpZGU= 42551
YWRhcHQ= 42552
IER1c2s= 42553
IHByb2Zlc3Npb25hbGlzbQ== 42554
IFByZXNjb3R0 42555
IEluZGllcw== 42556
cG94 42557
IE1laHJhbg== 42558
V2lkZQ== 42559
IGVuZGVtaWM= 42560
IFBhcmFu 42561
QmlyZA== 42562
IHBlZGFscw== 42563
IElV 42564
IEFkYW1hbnQ= 42565
IEh1cnQ= 42566
IGNvcnJlbGF0ZXM= 42567
dXJkZW4= 42568
IHNwb25zb3Jpbmc= 42569
Y2xpbWF0ZQ== 42570
IFVuaXZlcnNpdGllcw== 42571
IEtub3Q= 42572
ZW5uZXM= 42573
IERhbWlhbg== 42574
IEF4ZWw= 42575
U3BvcnQ= 42576
IGJhcmI= 42577
IFNubw== 42578
c2hvd24= 42579
c3RlZW4= 42580
dWRlbmNl 42581
IG5vbnZpb2xlbnQ= 42582
IGhvbW9waG9iaWE= 42583
IGJpb21hc3M= 42584
IERldGFpbA== 42585
IHNyZk4= 42586
IFR1bmU= 42587
YWNjb21wYW5pZWQ= 42588
SUVOQ0U= 42589
QWxiZXJ0 42590
IE1vbmdv 42591
eng= 42592
IENlcmJlcnVz 42593
b3JiaXQ= 42594
Y2Vucw== 42595
IHNsYXk= 42596
U0hBUkU= 42597
SFk= 42598
IGJyYXds 42599
IFByb2Jl 42600
IG5vbmV4aXN0ZW50 42601
IENsYXJlbmNl 42602
IEJsYWNrYnVybg== 42603
IHBvcnRhbHM= 42604
IFJpdGE= 42605
IFJlbWFpbg== 42606
IExldmFudA== 42607
IHRyaWNrZWQ= 42608
IEZlcnJ5 42609
YXZlcmluZw== 42610
IFN0cmF3YmVycnk= 42611
IEFuc3dlcnM= 42612
IGhvcnJlbmRvdXM= 42613
IEFtYW4= 42614
U3VwcGxlbWVudA== 42615
IFRvYWQ= 42616
IHBlZWxlZA== 42617
IG1hbm9ldXY= 42618
IFV6YmVr 42619
bW9uZHM= 42620
IEhlY3Rvcg== 42621
IDQwMg== 42622
cGVlcw== 42623
Zml4ZXM= 42624
IGRq 42625
IHJlc3VtZXM= 42626
IGFjY291bnRhbnQ= 42627
IGFkdmVyc2l0eQ== 42628
IGhhbXBlcmVk 42629
IExhcnNvbg== 42630
IGRvcGluZw== 42631
cGFydHM= 42632
SHVy 42633
IGJlYXJkZWQ= 42634
IHly 42635
IFBsdWdpbg== 42636
5aWz 42637
IC8qKg== 42638
cm9sbGV5 42639
IHdhdGVyc2hlZA== 42640
IFN1Ym1pc3Npb24= 42641
aWZsb3dlcg== 42642
QVND 42643
IGNob2ly 42644
IHNjdWxwdHVyZXM= 42645
bUE= 42646
aW5jcmVhc2luZw== 42647
YWlp 42648
IHNuZWFrZXJz 42649
IGNvbmZyb250cw== 42650
IEVsZXBoYW50 42651
IEVsaXhpcg== 42652
IHJlY2Fs 42653
IFRUTA== 42654
d2lkZ2V0 42655
IFdheA== 42656
IEdyYXlzb24= 42657
IGhhaXJzdA== 42658
IGh1bWlsaWF0ZWQ= 42659
IFdBUk4= 42660
YXBwaW5lc3M= 42661
IFRUQw== 42662
RnVlbA== 42663
IHBvbGlv 42664
IGNvbXBsZXhlcw== 42665
IGJhYmU= 42666
IFhJVg== 42667
UEY= 42668
KS5b 42669
UGFydHM= 42670
IDQzNQ== 42671
TWVn 42672
IFlhcmRz 42673
IEFMUA== 42674
IHllbGxz 42675
IHByaW5jZXM= 42676
IGJ1bGxpZXM= 42677
IENhcGl0YWxpc20= 42678
ZXhlbXB0 42679
RkFR 42680
IFNwb25nZQ== 42681
IEFsYQ== 42682
IHBsZWFzYW50bHk= 42683
IGJ1Zg== 42684
IGRlbm90ZQ== 42685
IHVucHVibGlzaGVk 42686
IGtuZWVsaW5n 42687
YXNjYQ== 42688
IGxhcHNl 42689
YWxpZW4= 42690
OTk0 42691
IHJlZmVyZWVz 42692
IExhd3llcnM= 42693
U2FudGE= 42694
IHB1enpsaW5n 42695
IFByb21ldGhldXM= 42696
IFBoYXJhb2g= 42697
IERlbGF5 42698
IGZhY2lsaXRhdGVz 42699
IENFUw== 42700
IGpld2Vscw== 42701
IGJvb2tsZXQ= 42702
b25kaW5n 42703
IHBvbGFyaXphdGlvbg== 42704
IE1vcmFu 42705
IFNhbGFk 42706
IFNPUw== 42707
IEFkdmljZQ== 42708
UEhPVE9T 42709
SUNBTg== 42710
aWF0dXJlcw== 42711
ZXhwcmVzcw== 42712
IFdvbmRlcmxhbmQ= 42713
IENPREU= 42714
IENMQVNT 42715
OTc1 42716
IGdyZXA= 42717
IERpZXNlbA== 42718
IEdsYWM= 42719
IT8i 42720
IHJt 42721
b2luZQ== 42722
ZGlzY3JpbWluYXRpb24= 42723
IE51cnNl 42724
bWFsbG93 42725
IHZvcnRleA== 42726
IENvbnNvcnRpdW0= 42727
IGxhcmdlRG93bmxvYWQ= 42728
c3RyYWlnaHQ= 42729
YXVnaGxpbg== 42730
R3JhZA== 42731
IHB1YmxpY2l6ZWQ= 42732
IFdhdmVz 42733
IFJlZGQ= 42734
IGZlc3Rpdml0aWVz 42735
IE1hbmU= 42736
YXJvdg== 42737
IGZsZWV0aW5n 42738
IERydW5r 42739
dWdlbg== 42740
Q2VsZQ== 42741
IGNocm9tb3NvbWVz 42742
IERPVA== 42743
LSstKy0rLSs= 42744
IGJ1c2llc3Q= 42745
IEJlYXZlcg== 42746
U3lyaWFu 42747
IEt5cg== 42748
a2Fz 42749
IENyb3NzUmVm 42750
MTk1MA== 42751
NzYwMQ== 42752
IHJlcGVhbGluZw== 42753
IFdpbm5lcnM= 42754
IE1hY3Jv 42755
IERPRA== 42756
YmxhbmNl 42757
U29ydA== 42758
NjQx 42759
IG1ldHJl 42760
IERpcms= 42761
IGdvZ2dsZXM= 42762
IGRyYXdiYWNrcw== 42763
IGNvbXBsYWluYW50 42764
IGF1dGhvcml6aW5n 42765
IGFudGl0cnVzdA== 42766
b3BlcmF0ZWQ= 42767
IG1haA== 42768
IGV4YWdnZXJhdGlvbg== 42769
QW1hemluZw== 42770
IFNlcmFwaA== 42771
IGhhemU= 42772
d293 42773
IGV4dGluZ3Vpc2hlZA== 42774
IGNhbnlvbg== 42775
IEJvc2g= 42776
IHZlbnRz 42777
IHNjcmFwZQ== 42778
Q29ycmVjdA== 42779
NDI2 42780
IGF2Zw== 42781
RGVtYW5k 42782
IOKIvA== 42783
IG1pY3JvYmlvdGE= 42784
In1dLCI= 42785
IFN0ZXY= 42786
Qmlv 42787
IFBsYW5lcw== 42788
IHN1Z2dlc3RpdmU= 42789
IGRlY2lwaGVy 42790
IFJlZnVnZWU= 42791
IEtlanJpd2Fs 42792
IEdyZWVucGVhY2U= 42793
IGRlY2xhc3M= 42794
IFNvdW5kZXJz 42795
IHRobw== 42796
IGRlY3J5cHQ= 42797
IGJydXNoaW5n 42798
IEphbmVpcm8= 42799
aXBvcA== 42800
U2k= 42801
ODc3 42802
IEdlb2ZmcmV5 42803
IGNwdQ== 42804
IEhhemVs 42805
IHZpZXdwb2ludHM= 42806
IGNyaXNweQ== 42807
IE5vdGlmaWNhdGlvbg== 42808
IHNvbGRlcg== 42809
IE1vZGVzdA== 42810
IEhlbWlzcGhlcmU= 42811
IGNhc3NldHRl 42812
aW5jbHVkZXM= 42813
IGlkZW50aWZpZXJz 42814
IENBTEw= 42815
aW5jZW50 42816
VG9kZA== 42817
IFN3ZWVw 42818
IDMzNA== 42819
Ym9zcw== 42820
IHNtaXI= 42821
Z2lueA== 42822
IHRvd25zaGlw 42823
IGdyaWV2aW5n 42824
IE1vc3F1ZQ== 42825
TmV0ZmxpeA== 42826
QVNFRA== 42827
IE1pbGxlbm5pYWxz 42828
b2NvbQ== 42829
MTk2Nw== 42830
IGJvbGRseQ== 42831
c2xlZXA= 42832
IGVzY2hl 42833
YXJpanVhbmE= 42834
IHN3aXJs 42835
IFBlbmFs 42836
IG5lZ2xpZ2VudA== 42837
IFN0ZXBoZW5zb24= 42838
S0VS 42839
IFpvcm8= 42840
cmlzaXM= 42841
IGxvY2FsaXphdGlvbg== 42842
IFNleW1vdXI= 42843
IEFuZ2xpYw== 42844
cmVkaXRhdGlvbg== 42845
cHJvdGVjdGlvbg== 42846
IFBhaWdl 42847
IG9taXQ= 42848
IFJvdXNzZQ== 42849
IFR1Yg== 42850
IGludml0YXRpb25z 42851
dHR5 42852
IG1vc3M= 42853
cGh5c2ljYWw= 42854
Q3JlZGl0cw== 42855
IGFuYXJjaHk= 42856
IGNoaWxkY2FyZQ== 42857
IGx1bGw= 42858
IE1law== 42859
IExhbmd1YWdlcw== 42860
bGF0ZXN0 42861
IFNhbmZvcmQ= 42862
IHVzYWJpbGl0eQ== 42863
IGRpZmZ1c2U= 42864
IERBVEE= 42865
IHNwcml0ZXM= 42866
IFZlZ2V0YQ== 42867
IFByb21vdGlvbg== 42868
44O844Kv 42869
cmljdGluZw== 42870
emVl 42871
VHVya2lzaA== 42872
IFREcw== 42873
cHJvdmVu 42874
NTcx 42875
IHNtdWdnbGVycw== 42876
NzA3MTA= 42877
IHJlZm9ybWVk 42878
IExvaXM= 42879
IHVuZmw= 42880
IFdJVEhPVVQ= 42881
IFJldHVybmluZw== 42882
YW5uaWU= 42883
IFRvbWFz 42884
RnJhbmM= 42885
IFByb2ZpdA== 42886
IFNFUlY= 42887
IFJ1bWJsZQ== 42888
aWt1bWFu 42889
ZXNhbg== 42890
IHRlc3RlcnM= 42891
IGdhZGdldA== 42892
IGJyYWNlbGV0 42893
IEZTQQ== 42894
Y29tcG9uZW50 42895
IHBhcmFtZWRpY3M= 42896
IGphbg== 42897
IFJlbWVt 42898
IFNraW5uZXI= 42899
IGxvdg== 42900
IFF1YWtl 42901
cm9tYQ== 42902
IGZsYXNr 42903
UHJpbmM= 42904
IG92ZXJwb3dlcg== 42905
IGxvZGdpbmc= 42906
IEtLSw== 42907
cmV0dGU= 42908
IGFic29yYnM= 42909
d3JvdGU= 42910
ICwi 42911
S2luZ3M= 42912
IEhhaWw= 42913
IEZhbGxpbmc= 42914
eHRhcA== 42915
IEhlbGVuYQ== 42916
aXJlbnM= 42917
TGFycnk= 42918
IHBhbXBobGV0 42919
IENQUg== 42920
R3Jv 42921
IEhpcm9zaGltYQ== 42922
IGhvbGlzdGlj 42923
Ii5b 42924
IGRldGFjaG1lbnQ= 42925
IGFzcGlyZQ== 42926
IGNvbXBsaWNpdA== 42927
IEdyZWVud29vZA== 42928
IHJlc3Bhd24= 42929
IFN0dXBpZA== 42930
IEZpbmlzaGVk 42931
ZmFs 42932
YmFzcw== 42933
IGFiaG9y 42934
IG1vY2tlcnk= 42935
IEZlYXN0 42936
VklERU8= 42937
IGNvbnNlYw== 42938
IEh1bmdyeQ== 42939
UHVsbA== 42940
IEh1c3Q= 42941
aXRhbmNl 42942
P+OAjQ== 42943
KS0t 42944
IFBhcmFsbGVs 42945
Y29udg== 42946
NDY5 42947
aGFhcg== 42948
d2FudA== 42949
UGFwZXI= 42950
bWlucw== 42951
IFRvcm8= 42952
IFRSVU1Q 42953
IFJhaQ== 42954
RFc= 42955
IFdpY2tlZA== 42956
IExlcA== 42957
IGZ1bmt5 42958
IGRldHJpbWVudA== 42959
aW9zaXM= 42960
YWNoZXY= 42961
IGRlZ3JhZGU= 42962
aW1pbGF0aW9u 42963
IHJldGFyZA== 42964
IGZyYWdtZW50YXRpb24= 42965
IGNvd2JveQ== 42966
IFlQRw== 42967
IEhBTA== 42968
UGFyZW50cw== 42969
IFNpZWc= 42970
IFN0cmF1c3M= 42971
IFJ1YmJlcg== 42972
15A= 42973
RnJhZw== 42974
IHB0 42975
IG9wdGlvbmFsbHk= 42976
IFpJUA== 42977
IFRyYW5zY3JpcHQ= 42978
IER3ZWxs 42979
ODgy 42980
TWVyYw== 42981
IE1PVA== 42982
44Ov44Oz 42983
IGh1bnRz 42984
IGV4ZWN1dGVz 42985
SW5jbHVkZXM= 42986
IGFjaWRpYw== 42987
IFJlc3BvbnNpYmlsaXR5 42988
IER1bWI= 42989
d2Vp 42990
QW5kZXJzb24= 42991
IEphc3Blcg== 42992
aWdodG9u 42993
YWJzb2x1dGVseQ== 42994
QWR1bHQ= 42995
IHBsdW5kZXI= 42996
TW9ybmluZw== 42997
IFRvdXJz 42998
IERhbmU= 42999
zro= 43000
IFRFU1Q= 43001
IEdpbmE= 43002
IGNhbmluZQ== 43003
YXdhbg== 43004
IHNvY2lhbGlzdHM= 43005
IFNvZGE= 43006
IGltcGV0dXM= 43007
IFN1cHBsZW1lbnRhcnk= 43008
b2xpYXRo 43009
IEtpbm5pa3VtYW4= 43010
bWl0dGVkbHk= 43011
c2Vjb25kcw== 43012
IG9yZ2FuaXNlcnM= 43013
IGRvY3VtZW50YXJpZXM= 43014
VmFyaWFibGU= 43015
R1JFRU4= 43016
IHJlc29ydHM= 43017
IGJyYWdnaW5n 43018
IDM2OA== 43019
QXJ0aXN0 43020
d2s= 43021
YmxlcnM= 43022
VW5jb21tb24= 43023
IFJldHJpZXZlZA== 43024
IGhlY3RhcmVz 43025
IHRveGlu 43026
cmFuaw== 43027
IGZhaXRocw== 43028
IEdyYXBoaWM= 43029
IHZlYw== 43030
IExJQQ== 43031
QWZyaWNhbg== 43032
IGFyZGVudA== 43033
ZW5kaWFyeQ== 43034
TGFrZQ== 43035
IERPUw== 43036
Y2llbnRpb3Vz 43037
IE9rYXdhcnU= 43038
IEFsbHk= 43039
IFRpbWVsaW5l 43040
RGFzaA== 43041
IElj 43042
Y29udGludWU= 43043
IHRpZHk= 43044
IGluc3RpbmN0aXZlbHk= 43045
IFBvc3NpYmx5 43046
IE91dGRvb3I= 43047
IFdvdWxkbg== 43048
IGxpY2g= 43049
IEJyYXk= 43050
IEFY 43051
IMOJ 43052
ICsj 43053
XCc= 43054
RGlyZWN0b3J5 43055
YWJpZGluZw== 43056
IGZlcmFs 43057
aWNhdGl2ZQ== 43058
YnV0dA== 43059
IHBlcnZlcnNl 43060
U2FsdA== 43061
IHdhcnBlZA== 43062
IG5pbmV0ZWVu 43063
IGNhYmluZXRz 43064
IHNyZkF0dGFjaA== 43065
IFNsb2Fu 43066
IHBvd2VyaW5n 43067
cmVnYXRpb24= 43068
RmxpZ2h0 43069
c2V2ZXJl 43070
IHN0cmVu 43071
IGNvZw== 43072
YXBhY2hl 43073
IOKd 43074
IGNhZmV0ZXJpYQ== 43075
cGFjZXM= 43076
IEdyaW1vaXJl 43077
dXRvbml1bQ== 43078
IHJhaW5pbmc= 43079
IGNpcmNsaW5n 43080
IGxpbmViYWNrZXJz 43081
Y3JlZGl0 43082
IHJlcGF0cmk= 43083
IENhbWRlbg== 43084
bGljZW5zZQ== 43085
IGx5cmlj 43086
IGRlc2NyaXB0b3I= 43087
IHZhbGxleXM= 43088
IHJlcQ== 43089
IGJhY2tzdGFnZQ== 43090
IFByb2hpYml0aW9u 43091
IEtldA== 43092
T3BlbmluZw== 43093
U3lt 43094
5pa5 43095
IHNlcnZpbmdz 43096
IG92ZXJzZWVu 43097
IGFzdGVyb2lkcw== 43098
IE1vZHM= 43099
IFNwcmluZ2Vy 43100
IENvbnRhaW5lcg== 43101
6Ls= 43102
IE1lbnM= 43103
IG11bHRpbQ== 43104
IGZpcmVmaWdodGVy 43105
cGVj 43106
IGNobG9yaW5l 43107
0Lw= 43108
ZW5kaQ== 43109
IHNwYXJpbmc= 43110
IHBvbHlnYW15 43111
IFJO 43112
IFBlbGw= 43113
IHRpZ2Vycw== 43114
IGZsYXNoeQ== 43115
IE1hZGFtZQ== 43116
U3dvcmQ= 43117
IHByZWZyb250YWw= 43118
IHByZXJlcXVpc2l0ZQ== 43119
dWNh 43120
IHdpZmk= 43121
IG1pc2NvbmNlcHRpb24= 43122
IGhhcnNobHk= 43123
IFN0cmVhbWluZw== 43124
b3RvbQ== 43125
IEdpdWxpYW5p 43126
Zm9vdGVk 43127
IHR1YmluZw== 43128
aW5kaXZpZHVhbA== 43129
emVr 43130
bnVjbGVhcg== 43131
bW9s 43132
IHJpZ2h0ZnVs 43133
NDkz 43134
IHNwZWNpYWxpemF0aW9u 43135
IHBhc3Npb25hdGVseQ== 43136
IFZlbG9jaXR5 43137
IEF2YWlsYWJpbGl0eQ== 43138
VGVubg== 43139
IGxhdGNo 43140
IFNvbWVib2R5 43141
IGhlbGl1bQ== 43142
Y2xhdw== 43143
IGRpcHBpbmc= 43144
WFhY 43145
IGludGVycGVyc29uYWw= 43146
NzEw 43147
IHN1YnRlcg== 43148
IGJpb2xvZ2lzdHM= 43149
IExpZ2h0aW5n 43150
IG9wdGlj 43151
IGRlbmlt 43152
ZW5kb24= 43153
IENvcm0= 43154
IDM0MQ== 43155
IENvdXA= 43156
IGZlYXJsZXNz 43157
IGFsb3Q= 43158
IENsaWZmb3Jk 43159
IFJ1bnRpbWU= 43160
IFByb3Zpc2lvbg== 43161
dXBkYXRlZA== 43162
bGVuZWNr 43163
IG5ldXJvbg== 43164
IGdyYWRpbmc= 43165
IEN0 43166
c2VxdWVuY2U= 43167
aW5pYQ== 43168
Y29uY2VwdA== 43169
IHJvYXJpbmc= 43170
cml2YWw= 43171
IENhdWNhc2lhbg== 43172
IG1vbm9n 43173
a2V5ZXM= 43174
IGFwcGVsbGF0ZQ== 43175
IGxpYWlzb24= 43176
RVN0cmVhbUZyYW1l 43177
IFBsdW0= 43178
IS4= 43179
IHNwaGVyaWNhbA== 43180
IHBlcmlzaGVk 43181
IGJsb3Q= 43182
IGJlbmNoZXM= 43183
IDQxMQ== 43184
IHBpb25lZXJlZA== 43185
IGh1cmxlZA== 43186
SmVubmlmZXI= 43187
IFlvc2VtaXRl 43188
Q2hhaXI= 43189
IHJlZWZz 43190
IGVsZWN0b3I= 43191
IEFudGhlbQ== 43192
NjUy 43193
IHVuaW5zdGFsbA== 43194
IGltcGVkZQ== 43195
IGJsaW5raW5n 43196
IGdvdG8= 43197
RGVjcmU= 43198
QXJlbg== 43199
IHN0YWJpbGl6YXRpb24= 43200
IERpc2FibGVk 43201
IFlhbnVrb3Z5Y2g= 43202
IG91dGxhd2Vk 43203
IFZlbnR1cmE= 43204
dGVuZXNz 43205
IHBsYW50YXRpb24= 43206
IHlhY2h0 43207
IEh1YXdlaQ== 43208
IHNvbHZlbnQ= 43209
IGdyYWNpb3Vz 43210
IGN1cmlvdXNseQ== 43211
IGNhcGFjaXRvcg== 43212
IGN4 43213
IFJlZmxleA== 43214
UGh5cw== 43215
IENm 43216
cHRpbg== 43217
Y29uc2VydmF0aXZl 43218
IGludm9jYXRpb24= 43219
Y291cg== 43220
Rk4= 43221
IE5ld2x5 43222
SG91cg== 43223
QXNpYW4= 43224
IExlYWRpbmc= 43225
IEFlcm9zcGFjZQ== 43226
QW5uZQ== 43227
IHByZW5hdGFs 43228
IGRldGVyaW9yYXRpbmc= 43229
SENS 43230
IE5vcm1hbmR5 43231
b2xpbmk= 43232
IEFtYnJv 43233
OTEw 43234
IHNldGJhY2tz 43235
IFRSRQ== 43236
IHNpZw== 43237
IFNjb3VyZ2U= 43238
NTk3 43239
Nzk4 43240
R2FtZXBsYXk= 43241
IG1zZWM= 43242
TVg= 43243
IHByaWNleQ== 43244
IExMUA== 43245
YWtlcnU= 43246
IG92ZXJhcmNoaW5n 43247
IEJhbGU= 43248
IHdvcmxkbHk= 43249
Q2xhcms= 43250
IHNjZW5pYw== 43251
IGRpc2xpa2Vk 43252
IENvbnRyb2xsZWQ= 43253
VGlja2V0cw== 43254
IEVX 43255
YWJpZXM= 43256
IFBsZW50eQ== 43257
Tm9uZXRoZWxlc3M= 43258
IGFydGlzYW4= 43259
VHJhbnNmZXI= 43260
IEZhbW91cw== 43261
IGluZmllbGQ= 43262
YmxleQ== 43263
IHVucmVzb2x2ZWQ= 43264
IE1MQQ== 43265
44KC 43266
Q29ycmVjdGlvbg== 43267
IGRlbW9jcmF0 43268
IE1vcmVubw== 43269
cm9jYWw= 43270
aWxpbmdz 43271
IHNhaWxvcg== 43272
IHJpZmU= 43273
aHVuZw== 43274
IHRyb3Blcw== 43275
IHNuYXRjaGVk 43276
IExJTg== 43277
IEJpYg== 43278
RVNB 43279
IFByZXY= 43280
IENhbWVs 43281
cnVudGltZQ== 43282
IG9ibm94aW91cw== 43283
NDM3 43284
IHN1bW1lcnM= 43285
IHVuZXhwbGFpbmVk 43286
IFdhbHRlcnM= 43287
Y2FsaWJlcg== 43288
IGd1bGw= 43289
IEVuZHVyYW5jZQ== 43290
5L2c 43291
IDM0Nw== 43292
SXJpc2g= 43293
IGFlcm9iaWM= 43294
IGNyYW1wZWQ= 43295
IEhvbm9sdWx1 43296
4Kk= 43297
dXNlcmM= 43298
ZWNhc3Q= 43299
QUNZ 43300
IFF1ZXJ5 43301
44K544OI 43302
QmV0YQ== 43303
IHN1c2NlcHRpYmlsaXR5 43304
IFNoaXY= 43305
IExpbWJhdWdo 43306
IMOW 43307
IE5YVA== 43308
IE11c3M= 43309
IEJyaXRvbnM= 43310
RVNDTw== 43311
RUdJTg== 43312
ICUl 43313
IHNlY2Vzc2lvbg== 43314
IFBhdHJvbg== 43315
IEx1YQ== 43316
bmFpcmVz 43317
IEpQTW9yZ2Fu 43318
dXNi 43319
b2N5dGU= 43320
IGNvdW5jaWxsb3Jz 43321
IExpYW5n 43322
ZmFybQ== 43323
IG5lcnZvdXNseQ== 43324
IGF0dHJhY3RpdmVuZXNz 43325
IEtvdg== 43326
anVtcA== 43327
UGxvdA== 43328
IHN0YWlucw== 43329
IFN0YXR1ZQ== 43330
IEFwb3N0bGVz 43331
aGV0ZXI= 43332
IFNVUFBPUlQ= 43333
IG92ZXJ3aGVsbQ== 43334
WUVT 43335
IDI5MQ== 43336
ZGVuc2l0eQ== 43337
IHRyYXBwaW5n 43338
TWl0 43339
IGZpZGU= 43340
IFBhbWVsYQ== 43341
YXRsYW50aWM= 43342
RGFtbg== 43343
IHB0cw== 43344
T1BB 43345
IHNlcnZpY2luZw== 43346
IG92ZXJmbG93aW5n 43347
dWxv 43348
IEVyaXQ= 43349
dGlja2V0 43350
bGlnaHRpbmc= 43351
IEhtbQ== 43352
44O844Or 43353
aW1vdG8= 43354
IGNodWNrbGU= 43355
NDIz 43356
44GV 43357
c2hhcGU= 43358
IHF1ZXVlcw== 43359
IGFuY2hvcnM= 43360
44K844Km44K5 43361
RmVy 43362
IGF3b2tl 43363
IDY2Ng== 43364
aGFuZHM= 43365
IGRpdmVyZ2VuY2U= 43366
IDUwNQ== 43367
VGlwcw== 43368
IGRlcG90 43369
IHNrZXc= 43370
IERlbGl2ZXI= 43371
b3BvdA== 43372
IGRpdnVs 43373
IEVC 43374
dW5zaWduZWQ= 43375
IFVuaQ== 43376
WGJveA== 43377
IGZvcmtz 43378
IDcwMg== 43379
5a8= 43380
IHByb21vdGVycw== 43381
IFZhcG9y 43382
IGxldmllZA== 43383
c2xvdA== 43384
IHBpZ21lbnQ= 43385
IGN5bGluZGVycw== 43386
Q1JF 43387
IHNuYXRjaA== 43388
IHBlcnBldHVhbGx5 43389
IGxpY2tpbmc= 43390
IEZlZXQ= 43391
IEtyYWtlbg== 43392
IEhvbGRlbg== 43393
IENMU0lE 43394
bXI= 43395
IHByb2plY3Rvcg== 43396
IGRlbm90ZXM= 43397
IGNoYXBlbA== 43398
IFRvcnJlbnQ= 43399
Ymxlcg== 43400
Um91dGU= 43401
IERlZmVuZGFudA== 43402
IFB1Ymxpc2hlcnM= 43403
IE1hbGVz 43404
IElubm92 43405
IEFnaWxpdHk= 43406
cml0ZXI= 43407
dHltb2xvZ3k= 43408
c3RvcmVz 43409
TGluZA== 43410
IGZvbGx5 43411
IFp1cmljaA== 43412
Qmxl 43413
IG51cnR1cmU= 43414
IGNvYXN0bGluZQ== 43415
dWNoaW4= 43416
RG9taW4= 43417
IGZyaXZvbA== 43418
IENvbnNvbGlk 43419
cmVzdWx0cw== 43420
TUo= 43421
IHBoeWxvZ2Vu 43422
IGhhdWxlZA== 43423
IFdpbGV5 43424
IEplc3NpZQ== 43425
IFByZXBhcmU= 43426
IEVwcw== 43427
IHRyZWFzdXJlcg== 43428
SUFT 43429
IGNvbG9uaXN0cw== 43430
IGludW5k 43431
IFdXRg== 43432
IENvbnZlcnRlZA== 43433
NjAwMA== 43434
b3V0c2lkZQ== 43435
IEFwcGVhcmFuY2U= 43436
IFJlbGlj 43437
IE1pc3Rlcg== 43438
c2F3 43439
IHJlc3VsdGFudA== 43440
IGFkamVjdGl2ZQ== 43441
IExhdXJlbA== 43442
IEhpbmRp 43443
YmRh 43444
UGVhY2U= 43445
IHJlYmlydGg= 43446
IG1lbWJyYW5lcw== 43447
IGZvcndhcmRpbmc= 43448
IGNvbGxpZGVk 43449
IENhcm9seW4= 43450
S2Fuc2Fz 43451
NTk5 43452
IFNvbGlkR29sZE1hZ2lrYXJw 43453
QmVjaw== 43454
IHN0cmVzc2luZw== 43455
IEdvbw== 43456
IENvb3BlcmF0aXZl 43457
IGZz 43458
IEFyY2hpZQ== 43459
TGl0ZXI= 43460
IEtsb3Bw 43461
SmVycnk= 43462
IGZvb3R3ZWFy 43463
V2FycmVu 43464
IHNjcmVl 43465
aGFyZQ== 43466
VW5kZXJzdGFuZGluZw== 43467
UGVk 43468
IGFudGhvbG9neQ== 43469
IEFubm91bmNl 43470
TWVnYQ== 43471
IGZsdWVudA== 43472
IGJvbmRhZ2U= 43473
IERpc2NvdW50 43474
aWxpYWw= 43475
Q2FydA== 43476
IE5pZ2h0bWFyZXM= 43477
U2hhbQ== 43478
IEJvbGw= 43479
dXNzaWU= 43480
SHR0cA== 43481
QXRsYW50YQ== 43482
IHVucmVjb2du 43483
IEJpZA== 43484
IHVuZGVyZ3JhZA== 43485
IGZvcmdpdmluZw== 43486
IEdsb3Zlcg== 43487
QUFBQUFBQUE= 43488
NDQ1 43489
Vkc= 43490
cGFpbw== 43491
a2lsbGVycw== 43492
IHJlc3BvbnNpYmx5 43493
IG1vYmlsaXpl 43494
IGVmZmVjdGVk 43495
IEx1bWlu 43496
IGthbGU= 43497
IGluZnJpbmdpbmc= 43498
YW5ub3VuY2Vk 43499
IGZpdHQ= 43500
YmF0Y2g= 43501
IFRhY2tsZQ== 43502
IExpbWU= 43503
IEFQUA== 43504
dWtlbWlh 43505
IHJ1Ynk= 43506
IGV4b25lcg== 43507
IENhc3VhbA== 43508
MDcw 43509
IHBlbHZpYw== 43510
IGF1dG9tYXRl 43511
IEtlYXI= 43512
IENvYXN0YWw= 43513
IGNyZWVk 43514
IGJvcmVkb20= 43515
IFN0dW4= 43516
cmlvdHQ= 43517
go4= 43518
IHJlZ2VuZXJhdGU= 43519
IGNvbWVkaWFucw== 43520
IE9QRVI= 43521
U3BvbnM= 43522
aWRpdW0= 43523
b25pcw== 43524
TG9jYXRlZA== 43525
MDU3 43526
IHN1c3BlbnNl 43527
IERhdGluZw== 43528
Q2Fzcw== 43529
IG5lb2NvbnM= 43530
IFNoaW56bw== 43531
IGF3b2tlbg== 43532
Y2hyaXN0 43533
IE1lc3NhZ2Vz 43534
YXR0bGVk 43535
IFNwcmF5 43536
IFNwaWNl 43537
Q1c= 43538
IHNoaWVsZGluZw== 43539
IEdhdWw= 43540
QW1pZA== 43541
IHBhcmFtaWxpdGFyeQ== 43542
IG11bHRpZg== 43543
IFRhbm5lcg== 43544
aWxr 43545
IGdvZGRhbW4= 43546
Z2VtZW50cw== 43547
IGJlZnJpZW5k 43548
bW9iaQ== 43549
IDM4OA== 43550
Zm9sZGVy 43551
YWNjYQ== 43552
IGluc2lu 43553
Z2Fw 43554
TmV2 43555
ZmlmdGg= 43556
IHBzeWNoaWF0cnk= 43557
YmFua3M= 43558
VEhJUw== 43559
IGhhcmI= 43560
YWNxdQ== 43561
IGZhY2FkZQ== 43562
IFBvd2VyUG9pbnQ= 43563
ODAz 43564
IGJsdWZm 43565
U2hhcmVz 43566
IGZhdm9yaW5n 43567
RWxpemFiZXRo 43568
w43DjQ== 43569
IHJhbmdlcg== 43570
Nzcy 43571
IEFyY2hl 43572
aGFr 43573
IEdlbmV0aWNz 43574
IEZFTUE= 43575
IGV2b2x2ZXM= 43576
IGVzdGU= 43577
IFBldHM= 43578
IE3DqQ== 43579
IEludGVyZXN0aW5n 43580
IENhbnRlcmJ1cnk= 43581
Y2hhcHRlcg== 43582
IFN0YXJmbGVldA== 43583
U3BhbmlzaA== 43584
IGRyYXdiYWNr 43585
IE5vcndpY2g= 43586
OTcw 43587
bm9ydGg= 43588
YWdhbmRh 43589
IHRyYW5zZm9ybWF0aXZl 43590
cmFtaWRz 43591
YmlvbG9neQ== 43592
YWRheQ== 43593
IHByb3BhZ2F0aW9u 43594
IEdhbW1h 43595
IERlbmlzZQ== 43596
IENhbGN1bGF0b3I= 43597
ZW50aW1lcw== 43598
IEJldHQ= 43599
IGFwcGVuZGl4 43600
IEhERA== 43601
QUtJTkc= 43602
IHN0aWdtYXQ= 43603
IGhvbHN0ZXI= 43604
IG9yZGluYXJpbHk= 43605
Q2hhbmNl 43606
IENvbnRyYXJ5 43607
IGFkaGVzaXZl 43608
IGdhdGhlcnM= 43609
NjEy 43610
cmVhdQ== 43611
b255bXM= 43612
ZXdheXM= 43613
IGluZHVjZXM= 43614
IGludGVyY2hhbmdlYWJsZQ== 43615
c2Vt 43616
V2hpdA== 43617
IHRyYW5jZQ== 43618
IGluY29ycG9yYXRpb24= 43619
IEV4dHJhcw== 43620
RmluYW5jaWFs 43621
IGF3a3dhcmRseQ== 43622
IFN0dXJnZW9u 43623
IEhZ 43624
Tm9ybWFsbHk= 43625
IEVuZGluZw== 43626
IEFzc2lzdA== 43627
ZW5jcnlwdGVk 43628
IHN1Ymp1Zw== 43629
IG5vcw== 43630
IGZhbmF0aWM= 43631
Q3Vi 43632
Q1U= 43633
PyIu 43634
IGlycmV2ZXJzaWJsZQ== 43635
5YI= 43636
MDMx 43637
IEhBUg== 43638
c3ByZWFk 43639
dWxpYQ== 43640
PSQ= 43641
U2NvcGU= 43642
TG90cw== 43643
IGxpZmVzdHlsZXM= 43644
b2xvbg== 43645
IGZlZHM= 43646
IGNvbmdyYXR1bGF0ZQ== 43647
d2Via2l0 43648
IGluZGlzdGluZ3Vpc2hhYmxl 43649
IFN3aW5n 43650
IGNvbW1hbmRtZW50cw== 43651
cXVpbGE= 43652
YWJlbGxh 43653
bWV0aHls 43654
YW5uYWJpbg== 43655
IG92ZXJl 43656
IGxvYnN0ZXI= 43657
IFFVRVNU 43658
IENPTlRJTg== 43659
YmVybmF0b3JpYWw= 43660
Ojo6Ojo6Ojo= 43661
IFRyYXZl 43662
IFNhbW9h 43663
QU5J 43664
NzUy 43665
0LQ= 43666
dXNlcmNvbnRlbnQ= 43667
IE1vZGVyYXRl 43668
eWVhaA== 43669
IEtpdHQ= 43670
IHdlZQ== 43671
IHN0dWZmaW5n 43672
IEludGVydmVudGlvbg== 43673
IERpZ24= 43674
IHdhcmVob3VzZXM= 43675
IEZpamk= 43676
IHBlbGxldHM= 43677
IHRha2Vhd2F5 43678
IFRBQkxF 43679
IENsYXNzaWNhbA== 43680
Y29sbGVjdGlvbg== 43681
IGxhbmRmYWxs 43682
IE11c2NsZQ== 43683
IHNldHRsZXM= 43684
IEFEVg== 43685
IDM0NA== 43686
TGF1cmE= 43687
IGZhcmVk 43688
IFBhcnRpYWw= 43689
NDM2 43690
b3NzaWJpbGl0eQ== 43691
IERhbHk= 43692
IFRhcmFudA== 43693
IEZ1amk= 43694
YW1s 43695
Y2VuY2U= 43696
NTUx 43697
IFByb2NlZHVyZXM= 43698
IE9DRA== 43699
IFVE 43700
dGlu 43701
UVVJ 43702
YWNobw== 43703
NDM4 43704
IGdsaXRjaGVz 43705
IGVuY2hhbnRtZW50 43706
IGNhbGN1bGF0ZXM= 43707
SVJP 43708
IEh1YQ== 43709
YWx5c2Vz 43710
IExpZnQ= 43711
dW1v 43712
IGxlYXB0 43713
IGh5cG90aGVzaXplZA== 43714
IEd1c3Rhdg== 43715
aXRhbnM= 43716
VkVSU0lPTg== 43717
5qA= 43718
Um9nZXI= 43719
IHJhbmQ= 43720
IEFkYXB0ZXI= 43721
IDMzMQ== 43722
IFBldGl0aW9u 43723
a2llcw== 43724
TWFycw== 43725
IHVuZGVyY3V0 43726
emVlcw== 43727
IEx5b25z 43728
IERIQ1A= 43729
TWlzc2luZw== 43730
IHJldGlyZWVz 43731
IGluc2lkaW91cw== 43732
ZWxp 43733
Pik= 43734
LuOAjQ== 43735
IGZpbmFsaXN0cw== 43736
IEF1cmU= 43737
IGFjY3VzZXI= 43738
IHdhc3Rlcw== 43739
IFlz 43740
IExvcmk= 43741
IGNvbnN0aXR1ZW5jaWVz 43742
IHN1cHBlcg== 43743
IG1heWhlbQ== 43744
b3Jhbmdl 43745
IG1pc3BsYWNlZA== 43746
IG1hbmFnZXJpYWw= 43747
IGV4Y2U= 43748
IENMSQ== 43749
IHByaW1hbA== 43750
IExlbnQ= 43751
Q3J5c3RhbA== 43752
aG92ZXI= 43753
IE5UUw== 43754
ZW5kdW0= 43755
IGR3 43756
IEFsYw== 43757
bm9zdGlj 43758
IHByZXNlcnZlcw== 43759
IFRzYXJuYWV2 43760
IHRyaXBsZWQ= 43761
cmVsYXRpdmU= 43762
QXJjYWRl 43763
a2lsbGluZw== 43764
IFdFRUs= 43765
IEhhbm5h 43766
RHVzdA== 43767
Q29tcGxldGVk 43768
gas= 43769
IGFwcHJvdmVz 43770
IFN1cmY= 43771
IEx1dGhlcmFu 43772
dmVuYW50cw== 43773
IHJvYmJlcmllcw== 43774
d2VpZ2h0cw== 43775
c29mdHdhcmU= 43776
YXRhbmE= 43777
dWdhbA== 43778
IGdyYXZ5 43779
IENhbmNl 43780
T0xPR1k= 43781
bHlhaw== 43782
VG9uaWdodA== 43783
IHVudmVpbA== 43784
IDE5MDQ= 43785
IE1pbmlvbg== 43786
ZW50aW91cw== 43787
c3RpY2U= 43788
cGFja2FnZXM= 43789
IEdFQVI= 43790
IGdvbA== 43791
IEh1dGNoaW5zb24= 43792
IFByb2Zlc3Npb24= 43793
IEdVTg== 43794
IERpZmZlcmVuY2U= 43795
IFRzdWt1eW9taQ== 43796
IExlc2JpYW4= 43797
Njcw 43798
IGZ1Z2l0aXZl 43799
IFBsYW5ldGFyeQ== 43800
LS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0= 43801
IGFjY3J1ZWQ= 43802
IGNoaWNrcw== 43803
IHN0b3Bw 43804
IGJsb2NrZXJz 43805
Q29k 43806
IGNvbW1lbnRlcnM= 43807
IFNvbWV3aGVyZQ== 43808
IFBob3RvZ3JhcGhlcg== 43809
dGhlbWU= 43810
IG1heW9yYWw= 43811
d3U= 43812
IGFudGVubmFz 43813
IHJldmFtcGVk 43814
IFN1YmplY3Rz 43815
aXTDqQ== 43816
aW11cmE= 43817
IGVudHJhbmNlcw== 43818
bGl0ZXJhbGx5 43819
IHRlbmV0cw== 43820
IE9NRw== 43821
IE1QSA== 43822
IERvbmtleQ== 43823
IE9mZmVuc2U= 43824
ICIr 43825
U25hcA== 43826
IEFGQg== 43827
IGFuaW1hdGU= 43828
IFNvZA== 43829
SGlzcGFuaWM= 43830
IGluY29uc2lzdGVuY3k= 43831
RGI= 43832
Rlk= 43833
RXhwb3J0 43834
IGFwZQ== 43835
IHBlYXJs 43836
aWJlbA== 43837
IFBBQ3M= 43838
IHtc 43839
IGFjdHU= 43840
IEhTQkM= 43841
Y2FtcHVz 43842
IHBheW9mZg== 43843
IGRlaXRpZXM= 43844
IE5hdG8= 43845
b3VwbGU= 43846
IGNlbnNvcmVk 43847
IENsb2p1cmU= 43848
IGNvbmZvdW5kaW5n 43849
ZW5p 43850
IHJlY2tvbg== 43851
b3BoZQ== 43852
IHNwb3R0aW5n 43853
IHNpZ25pZmllcw== 43854
IHByb3BlbA== 43855
IGZlc3RpdmU= 43856
U3VnZ2VzdA== 43857
IHBsZWRnaW5n 43858
IEJlcm1hbg== 43859
IHJlYmVsbGlvdXM= 43860
IG92ZXJzaGFkb3dlZA== 43861
IGluZmlsdHJhdGVk 43862
am9icw== 43863
Njcy 43864
IHNjYWxhYmxl 43865
IGRvbWluaW9u 43866
IE5ld2ZvdW5kbGFuZA== 43867
IE1lYWRvdw== 43868
IHBhcnRpdGlvbnM= 43869
QU1J 43870
IHN1cHBsZW1lbnRhcnk= 43871
c3RydW1lbnQ= 43872
IGhhaXJ5 43873
IHBlcnBldHVhdGU= 43874
IG51dHNoZWxs 43875
IFBvdGF0bw== 43876
IEhvYmJpdA== 43877
IGN1cnNlcw== 43878
RmxvYXQ= 43879
IHF1aWV0ZXI= 43880
IGZ1ZWxpbmc= 43881
IGNhcHN1bGVz 43882
IEx1c3Q= 43883
IEhhdW50ZWQ= 43884
RXhlY3V0aXZl 43885
IGNoaWxkYmlydGg= 43886
R3Jl 43887
IHJhZGlhbnQ= 43888
5Y4= 43889
IG1hbGxz 43890
IGluZXB0 43891
IFdhcnJhbnR5 43892
IHNwZWN0YXRvcg== 43893
RWg= 43894
dGhlbnM= 43895
IGN1bG1pbmF0aW5n 43896
5qk= 43897
YXJ5YQ== 43898
44Ku 43899
aWxpdGFyaWFu 43900
IE9SSUc= 43901
IFNwZW5kaW5n 43902
cHRpdmVz 43903
IFNpcmVu 43904
IFJlY29yZGluZw== 43905
YXluZQ== 43906
IHZpbQ== 43907
IHNwcmFuZw== 43908
VGFuZw== 43909
IE1GVA== 43910
bW9ybmluZw== 43911
IFdlZWQ= 43912
bXBlZw== 43913
Y2Vzc2lvbg== 43914
IENodW5n 43915
NzMw 43916
d2FybmluZw== 43917
NTYy 43918
aGFuZGVkbHk= 43919
UG9vcg== 43920
UG9saXRpY3M= 43921
OiM= 43922
IHBpYW4= 43923
IGZlY2Vz 43924
IERvY3VtZW50YXRpb24= 43925
IGJhbmlzaGVk 43926
IDM5OQ== 43927
IEFSQw== 43928
IGhlaW5vdXM= 43929
SmFrZQ== 43930
IEFtaXI= 43931
d2F5bmU= 43932
dnJl 43933
b3NoZW5rbw== 43934
IG5vdGVib29rcw== 43935
IGZvdW5kYXRpb25hbA== 43936
IG1hcnZlbG91cw== 43937
aXh0YXBl 43938
IHdpdGhkcmF3YWxz 43939
IGhvcmRl 43940
IERoYWJp 43941
aXNhYmxl 43942
IEtE 43943
IGNvbnRhZ2lvdXM= 43944
IERpcA== 43945
IEFycm93cw== 43946
IHByb25vdW5z 43947
IG1vcnBoaW5l 43948
IEJVUw== 43949
Njgy 43950
IGtvc2hlcg== 43951
ZmluaXNoZWQ= 43952
IEluc3RydW1lbnRz 43953
IGZ1c2Vk 43954
eWRlbg== 43955
IFNhbG1vbg== 43956
RmFi 43957
YWZmZWN0ZWQ= 43958
S0VO 43959
Q0VOVA== 43960
RG9tYWlu 43961
IHBva2Vtb24= 43962
IERyaW5raW5n 43963
R3Jvd2luZw== 43964
IEludmVzdGlnYXRpdmU= 43965
IEFldGhlcg== 43966
ZW1p 43967
IHRhYmxvaWQ= 43968
IHJlcHJv 43969
IE5vdHdpdGhzdGFuZGluZw== 43970
IEJlcnNlcmtlcg== 43971
IGRyYW1hcw== 43972
IGNsaWNow6k= 43973
IGJ1bmc= 43974
IFVSSQ== 43975
IERvcw== 43976
MDQ0 43977
IHBhc3RvcnM= 43978
IGxz 43979
IGFjcnlsaWM= 43980
YXVudHM= 43981
RWR3YXJk 43982
IG1ham9yaXRpZXM= 43983
QmFuZw== 43984
IGZpZWxkaW5n 43985
IFJlcGxhY2VtZW50 43986
IEFsY2hlbXk= 43987
cHBhcmQ= 43988
IFJvbWVv 43989
IFNhbmN0 43990
IExhdnJvdg== 43991
aWJibGU= 43992
SW5zdHJ1Y3Q= 43993
IGltcHJhY3RpY2Fs 43994
IFBsYXlib3k= 43995
Y2VwaGFs 43996
IHN3YXBz 43997
IGthbg== 43998
IFRoZW8= 43999
IGlsbHVzdHJhdGluZw== 44000
IGRpc21hbnRsZWQ= 44001
IFRyYW5zZ2VuZGVy 44002
IEd1dGg= 44003
VUdI 44004
IHRyaXVtcGhhbnQ= 44005
IGVuY29tcGFzcw== 44006
IGJvb2ttYXJr 44007
dWRkaW4= 44008
amVy 44009
IHByZWRpY2F0ZQ== 44010
RVNI 44011
IHdoZW5jZQ== 44012
IEFCRQ== 44013
IG5vbnByb2ZpdHM= 44014
U2VxdQ== 44015
IGRpYWJldGlj 44016
IHBlbmQ= 44017
IGhlYXJ0ZmVsdA== 44018
c2hp 44019
IGludGVyYWN0cw== 44020
IFRlbGVjb20= 44021
IGJvbWJhcmRtZW50 44022
ZGVwZW5kaW5n 44023
IExvd3J5 44024
IEFkbWlzc2lvbg== 44025
IEJsb29taW5n 44026
dXN0cmF0aW9u 44027
ZW5lZ2dlcg== 44028
QnJldw== 44029
IG1vbHRlbg== 44030
IE5lcmQ= 44031
UElO 44032
4paA 44033
YXZlbWVudA== 44034
IHRvdXJlZA== 44035
IGNvZWZmaWNpZW50cw== 44036
IFRyYXl2b24= 44037
YW5zc29u 44038
IHNhbmR5 44039
dG9sZA== 44040
Zmxvd3M= 44041
IHBvcHVsb3Vz 44042
IFRpbmRlcg== 44043
IEJsaXNz 44044
UmFjaGVs 44045
TWluaW11bQ== 44046
IGNvbnRlc3RhbnQ= 44047
IFJlZHVjZQ== 44048
IE1vcnNl 44049
IEdyYXNzbGV5 44050
IENsaWNrZXI= 44051
IGV4cHI= 44052
IHNpbmNlcml0eQ== 44053
IG1hcnF1 44054
IGVsaWNpdA== 44055
IFByb3Bvc2l0aW9u 44056
IERlbW9uaWM= 44057
IHRhY29z 44058
R3JlZWs= 44059
IHBvc3R3YXI= 44060
IGluc29mYXI= 44061
IFBvcms= 44062
IDM1Mg== 44063
ZG9jdG9yYWw= 44064
d2Fsa2luZw== 44065
IG1pZHRlcm0= 44066
IFNhbW15 44067
c2lnaHRlZA== 44068
IFRSQU5T 44069
aWNp 44070
QUxE 44071
IFVTTA== 44072
IEZJU0E= 44073
IEFtcGw= 44074
IEFsZXhhbmRyYQ== 44075
aW5lbGxp 44076
VHJhaW4= 44077
IHNpZ25pZnk= 44078
IFZlcnN1cw== 44079
IG9iZnVzYw== 44080
IGto 44081
IGFnZ3Jv 44082
IFJlbmF1bHQ= 44083
IDM0OA== 44084
NTE4 44085
b3hpY2l0eQ== 44086
MDIy 44087
IFR3aXN0 44088
IGdvb2Z5 44089
RHluYW1pYw== 44090
IGJyaWVmaW5ncw== 44091
bWlnaHQ= 44092
ODk5 44093
IGRlcm9nYXRvcnk= 44094
VHJv 44095
IGZvcmdpbmc= 44096
IEtvcmFu 44097
IE1hcnJpZWQ= 44098
IEJ1Y3M= 44099
IHBhbGF0ZQ== 44100
IENvbnZlcnNpb24= 44101
bWFibGU= 44102
NDEz 44103
IChf 44104
IHNpcGg= 44105
IE5FTw== 44106
Y29sbGVnZQ== 44107
IG1hcmdpbmFsbHk= 44108
IGZsaXJ0 44109
IFRyYXBz 44110
IFBhY2U= 44111
6buS 44112
IGdvYWx0ZW5kZXI= 44113
IGZvcmJpZHM= 44114
IGNsZXJrcw== 44115
IFRhbnQ= 44116
IFJvYmJpbnM= 44117
IFByaW50aW5n 44118
IHByZW1pZXJlZA== 44119
IG1hZ25pZmljYXRpb24= 44120
IFRH 44121
IFJvdXNl 44122
IE1vY2s= 44123
b2R5bmFtaWNz 44124
IHByZWNsdWRl 44125
aXNtbw== 44126
IFB1bGl0emVy 44127
IGF2YWxhbmNoZQ== 44128
IEtvZGk= 44129
cmlidW5l 44130
IExlbmE= 44131
RWxlY3RyaWM= 44132
IHJlZmluZXJ5 44133
IGVuZG93ZWQ= 44134
IGNvdW5zZWxvcnM= 44135
IGRvbHBoaW4= 44136
IE1pdGg= 44137
IGFybW91cmVk 44138
aGliaXRlZA== 44139
QmVnaW4= 44140
IFBX 44141
T2ls 44142
IFZvcg== 44143
IFNoYXJpZg== 44144
IEZyYXppZXI= 44145
ZXN0YXRl 44146
IGphbXM= 44147
UHJveHk= 44148
IGJhbmRpdHM= 44149
IFByZXNieXRlcmlhbg== 44150
IFByZW1pZXJl 44151
dGlueQ== 44152
IENydWVs 44153
VGVzdGluZw== 44154
IGhvbWVy 44155
IFZFUlM= 44156
IFByb2w= 44157
IERlcG9zaXQ= 44158
IENvZmZpbg== 44159
IHNlbWluYXJz 44160
IHNxbA== 44161
IERlZmVuZGFudHM= 44162
QWx0ZXJuYXRpdmVseQ== 44163
IFJhdHM= 44164
56s= 44165
ZXRoeXN0 44166
Jz4= 44167
IGlzc3Vlcg== 44168
NTg5 44169
IGNoYWlyZWQ= 44170
IEFjY2Vzc29yaWVz 44171
bWFuZW50 44172
IG1hcnJvdw== 44173
IFByaW1vcmRpYWw= 44174
Q04= 44175
IGxpbWl0bGVzcw== 44176
IENhcm5hZ2U= 44177
IHVuZHJhZnRlZA== 44178
cXY= 44179
SU5FU1M= 44180
b25ldw== 44181
IGNvaGVzaW9u 44182
OTg3 44183
IG5lY2tz 44184
IGZvb3RiYWxsZXI= 44185
IEdFUg== 44186
IGRldGVjdGFibGU= 44187
IFN1cHBvcnRpbmc= 44188
IENTVg== 44189
b2NhbGx5 44190
a0h6 44191
IHVuZGU= 44192
IHNob25l 44193
IGJ1ZGRpbmc= 44194
dHJhaw== 44195
U3RhbmRpbmc= 44196
IFN0YXJjcmFmdA== 44197
IEtlbXA= 44198
QmVuY2g= 44199
IHRod2FydGVk 44200
IEdyb3VuZHM= 44201
YXRoaQ== 44202
TGlzYQ== 44203
RGlhbG9n 44204
IFNY 44205
VmlzaW9u 44206
IGluZ2VuaW91cw== 44207
2ZA= 44208
IGZvc3RlcmluZw== 44209
IFph 44210
IEluZ3JhbQ== 44211
ICJA 44212
TmF0dXJhbGx5 44213
NjE2 44214
MDM1 44215
IEZBQw== 44216
SG1t 44217
NTU0 44218
IGFjY2VsZXJhdG9y 44219
IFZlbmQ= 44220
IHN1bnNjcmVlbg== 44221
IHR1YmVyY3Vsb3Npcw== 44222
cmF2aW9sZXQ= 44223
IEZ1bmN0aW9uYWw= 44224
IEVycm9ycw== 44225
ZWRhcg== 44226
MTk2Ng== 44227
IFNwZWN0cmU= 44228
IFJlY2lwZXM= 44229
ODg1 44230
IE1hbmtpbmQ= 44231
TGl2ZXJwb29s 44232
IHwtLQ== 44233
IHN1YnN0aXR1dGVz 44234
IFhU 44235
d2lyZWQ= 44236
IGluY28= 44237
IEFmZ2g= 44238
RXZh 44239
aWNj 44240
U29uZw== 44241
S25pZ2h0 44242
IGRpbGlnZW50bHk= 44243
IEJyb2FkY2FzdA== 44244
QWlk 44245
IGFmYXI= 44246
IEhNUw== 44247
YXRvbmlu 44248
IEdyYXRlZnVs 44249
IGZpcmVwbGFjZQ== 44250
IE9tbmk= 44251
ZXVybw== 44252
IEZSRQ== 44253
IFNoaWI= 44254
IERpZ2VzdA== 44255
dG9nZ2xl 44256
IGhlYWRzZXRz 44257
IGRpZmZ1c2lvbg== 44258
IFNxdWlycmVs 44259
IEZO 44260
IGRhcmtlbmVk 44261
b3V0aGVy 44262
IHNsZWVwcw== 44263
IFhlcg== 44264
Z3Vucw== 44265
IHNldHVwcw== 44266
IHBhcnNlZA== 44267
IG1hbW1vdGg= 44268
IEN1cmlvdXM= 44269
Z29i 44270
IEZpdHpwYXRyaWNr 44271
IEVtaWw= 44272
aW1vdg== 44273
Li4uLi4uLi4uLi4uLg== 44274
IEJlbm55 44275
U2Vjb25kbHk= 44276
IGhlYXJ0eQ== 44277
IGNvbnNvbg== 44278
c3RhaW5lZA== 44279
IGdhbGFjdGlj 44280
Y2xhdmU= 44281
IHBsdW1tZXRlZA== 44282
IHBlc3Rz 44283
IHN3YXQ= 44284
IHJlZmVycmFscw== 44285
IExpb25lbA== 44286
aG9seQ== 44287
IHVuZGVyZG9n 44288
IFNsYXRlcg== 44289
IFByb3ZpZGU= 44290
IEFtYXI= 44291
cmVzc29y 44292
5Yw= 44293
b25nYQ== 44294
IHRpbWlk 44295
IHBpZXR5 44296
IERlaw== 44297
IHN1cmdpbmc= 44298
YXpv 44299
IDYxMA== 44300
IGRlc2tz 44301
IFNwb2thbmU= 44302
IEFuZmllbGQ= 44303
IHdhcnNoaXBz 44304
IENvYnJh 44305
IGFybWluZw== 44306
Y2x1c2l2ZWx5 44307
IEJhZGdl 44308
YWdhc2Nhcg== 44309
IFBSRVNT 44310
IE1jS2Vuemll 44311
IEZlcmRpbmFuZA== 44312
YnVybmluZw== 44313
QWZlZQ== 44314
IHR5cmFubg== 44315
IEl3 44316
IEJvb25l 44317
MTAwNw== 44318
IFJlcHQ= 44319
CsKg 44320
IGNhcmF2YW4= 44321
IERpbGw= 44322
IEJ1bmRlc2xpZ2E= 44323
Q2h1Y2s= 44324
IGhlYWxlcg== 44325
44O844OG 44326
IEhvYmJ5 44327
IG5lZ2F0ZQ== 44328
IGNyaXRpcXVlcw== 44329
c2VjdGlvbmFs 44330
bW9wb2xpdGFu 44331
IGR4 44332
IG91dHNvdXJjaW5n 44333
IENpcGhlcg== 44334
dGFw 44335
U2hhcnA= 44336
IHVwYmVhdA== 44337
IGhhbmdhcg== 44338
IGNydWlzaW5n 44339
IE5pYWdhcmE= 44340
IDM0Mg== 44341
aWxsdXM= 44342
IFN2 44343
IHN1YnRpdGxlcw== 44344
IHNxdWFyZWQ= 44345
IGJvb2tzdG9yZQ== 44346
IHJldm9sdXRpb25hcmllcw== 44347
IENhcmx0b24= 44348
YWJhbA== 44349
VXRhaA== 44350
IGRlc3Bpc2U= 44351
IFVN 44352
Y29uc2lkZXI= 44353
YWlkbw== 44354
IGNhcnRz 44355
IFR1cnRsZXM= 44356
VHJhaW5pbmc= 44357
IGhvbm9yYXJ5 44358
wqI= 44359
IHRyaWFuZ2xlcw== 44360
NDIy 44361
IHJlcHJpbnRlZA== 44362
IGdyYWNlZnVs 44363
IE1vbmdvbGlh 44364
IGRpc3J1cHRpb25z 44365
IEJvaA== 44366
IDM0OQ== 44367
IGRyYWlucw== 44368
IGNvbnN1bGF0ZQ== 44369
IGJlbmRz 44370
IG1hZmlh 44371
dXJvbg== 44372
IEZ1bHRvbg== 44373
bWlzYw== 44374
IHJlbmFs 44375
IGluYWN0aW9u 44376
Y2tpbmc= 44377
IHBob3RvbnM= 44378
IGJydWlzZWQ= 44379
IENvZGVz 44380
b2dp 44381
IG5lc3Rz 44382
IExvdmVseQ== 44383
IExpYnJl 44384
IERhcnls 44385
ICMjIw== 44386
U3lz 44387
Liwi 44388
IGZyZWV6ZXM= 44389
ZXN0YWJsaXNobWVudA== 44390
YW5kb3dza2k= 44391
IGN1bWJlcnM= 44392
IFN0YXJn 44393
IEJvbWJz 44394
IGxlZ2lvbnM= 44395
IGhhbmR3cml0aW5n 44396
IGdydW4= 44397
IENhaA== 44398
c2VxdWVudA== 44399
IG1vdGg= 44400
IE1TTQ== 44401
SW5zZXJ0 44402
Rmlm 44403
IG1vdGVs 44404
IGRleHRlcg== 44405
IEJpbGQ= 44406
aGVhcnRlZGx5 44407
IHByb3Bl 44408
IFRleHR1cmU= 44409
IEp1bmN0aW9u 44410
eW50aGVzaXM= 44411
b2NhcmQ= 44412
IFZlcmE= 44413
IEJhcnRo 44414
IM68Zw== 44415
IGxhc2hlZA== 44416
IDM1MQ== 44417
IFphbWI= 44418
IFN0YXBsZXM= 44419
IENvcnRleA== 44420
IENvcmtlcg== 44421
IGNvbnRpbnV1bQ== 44422
IFdSSVRF 44423
dW50YQ== 44424
cmlkb3I= 44425
IGRlZW1z 44426
MDMz 44427
IEdPTEQ= 44428
cGFz 44429
IHJlcHJlc3NpdmU= 44430
44OG44Kj 44431
IGJhZmZsZWQ= 44432
U2Nhcg== 44433
IGNyYXZl 44434
IF9fX19fXw== 44435
IGVudHJlcHJlbmV1cnNoaXA= 44436
IERpcmVjdG9yYXRl 44437
ICdb 44438
IHZpbmVz 44439
IGFzY2VuZGVk 44440
IEdST1VQ 44441
IEdvb2RieWU= 44442
IGRvZ2dlZA== 44443
44O044Kh 44444
TWFudWZhY3Q= 44445
IHVuaW1hZ2luYWJsZQ== 44446
cmlvdHM= 44447
aWVycmV6 44448
IHJlbGF0aXZpdHk= 44449
IENyYWZ0aW5n 44450
cmF1Z2h0 44451
dWRlbg== 44452
Y29va2ll 44453
IGFzc2Fzc2lucw== 44454
IGRpc3NhdGlzZmllZA== 44455
YWNjaQ== 44456
IGNvbmR1aXQ= 44457
U3ByZWFk 44458
IFJpY2Fu 44459
bmljZQ== 44460
aXp6bGU= 44461
IHNjYXJlcw== 44462
IFdIWQ== 44463
cGhhbnM= 44464
NTM1 44465
IHByb3RyYWN0ZWQ= 44466
IEtyaXN0ZW4= 44467
NTM2 44468
IFNjcmli 44469
IE5laA== 44470
IHR3ZW50aWVz 44471
IHByZWRpY2FtZW50 44472
IGhhbmRjdWZmcw== 44473
IGZydWl0ZnVs 44474
IFVM 44475
IEx1ZHdpZw== 44476
IGF0dGVzdA== 44477
IEJyZWFrZXI= 44478
IGJpb2xvZ2ljYWxseQ== 44479
IERlYWxlcg== 44480
IHJlbm92YXRpb25z 44481
Znc= 44482
ZXNzZW4= 44483
QWxpY2U= 44484
IEhlbnJp 44485
IHVuaWxhdGVyYWxseQ== 44486
IFNpZGQ= 44487
aGFp 44488
IFN0cmV0Y2g= 44489
U2FsZXM= 44490
IGN1bWJlcnNvbWU= 44491
IEphdmllcg== 44492
IHRyZW5keQ== 44493
IHJvdHRpbmc= 44494
IENoYWxsZW5nZXM= 44495
IHNjcmFwcw== 44496
IGZhY2V0cw== 44497
IFZlcm9uaWNh 44498
IFZlcmdl 44499
IFNhbmE= 44500
QWxpZW4= 44501
IFJpaA== 44502
IHJhZGlhbA== 44503
ZWN0YXI= 44504
IDYzMA== 44505
Y2xp 44506
TWFyaWU= 44507
IHdpbGRmaXJl 44508
IENhdG8= 44509
aGFuZGVy 44510
IHdhaXRyZXNz 44511
IGNob3Bz 44512
IFNFQ1RJT04= 44513
IGJsdW50bHk= 44514
IENhdGFsb2c= 44515
bmlhbg== 44516
c3R1ZHk= 44517
IHBhdHJvbGxpbmc= 44518
IFRlbnRo 44519
bmV4dXM= 44520
IE5PTg== 44521
b3BzeQ== 44522
IHNjYXRoaW5n 44523
c2ll 44524
IGRldGVyaW9yYXRlZA== 44525
VkI= 44526
TmF6aXM= 44527
IGRlcGljdGlvbnM= 44528
IGF1dGhlbnRpY2F0ZWQ= 44529
IENvbmNl 44530
a3JpdA== 44531
IHByb211bGc= 44532
IExPTkc= 44533
VUZD 44534
IFZpc2l0b3Jz 44535
IFJlY2FsbA== 44536
IHJlaGFiaWxpdA== 44537
IFNMSQ== 44538
IGdsYWNpZXI= 44539
IEJpdGU= 44540
IDUwMw== 44541
IHZvbWl0 44542
IGZlcm1lbnRlZA== 44543
IEtoYWxpZA== 44544
IGdyYWRlZA== 44545
IE1hZ2lja2E= 44546
IEljaGlnbw== 44547
cG93ZXJmdWw= 44548
aWNhdG9ycw== 44549
NzUz 44550
IHNocmV3 44551
IDM1Ng== 44552
IGxlZ2FsaXppbmc= 44553
IGFsbG90dGVk 44554
IEFyY2hkZW1vbg== 44555
aXRoaW5n 44556
aWdndXJhdA== 44557
Vk9M 44558
TGVvZA== 44559
IG9pbHk= 44560
IGluZHVjaW5n 44561
IGFteWdkYWxh 44562
IGFkbWlucw== 44563
IEFjcXVpc2l0aW9u 44564
Q0FO 44565
IHNjaGVtYXRpYw== 44566
IG1vYW4= 44567
IENhbWVyb29u 44568
IHRpbms= 44569
IG1lcnJ5 44570
IGJ1dHRlcmZsaWVz 44571
IEdvZmY= 44572
IHdvcmtzcGFjZQ== 44573
IENvcm9uYQ== 44574
IGphdmFzY3JpcHQ= 44575
IERvbHBoaW4= 44576
IENhbnRvcg== 44577
NDY0 44578
dG9l 44579
QVBT 44580
IEFnaW5n 44581
IHBhZGRlZA== 44582
IFpoZW5n 44583
IEhlbGQ= 44584
IGVzdHJhbmdlZA== 44585
IDc3MA== 44586
Ln0= 44587
IER1bmhhbQ== 44588
IHNtb2tlcw== 44589
IGNhcGl0YWxz 44590
dW5kYWk= 44591
U2hpbg== 44592
IEZvdW5kaW5n 44593
IGVudGl0bGU= 44594
IGNlbnRlcnBpZWNl 44595
RGlzY292ZXI= 44596
IHRoZXJldG8= 44597
YWxlcnQ= 44598
IE5vdQ== 44599
IEFuYWx5c3Q= 44600
bGM= 44601
Rkg= 44602
RklFTEQ= 44603
IFBPVg== 44604
Z3JheQ== 44605
IGFyY3M= 44606
IEhPVA== 44607
IHJz 44608
IG9ibGlnYXRvcnk= 44609
IEFyY2hpdGVjdHM= 44610
IFN2ZW4= 44611
IEZFQw== 44612
MDIwMA== 44613
Q2hyaXN0bWFz 44614
IEFsYmFuaWE= 44615
cmF0b20= 44616
NTg3 44617
IGhhcmRzaGlwcw== 44618
IGF1dG9z 44619
IENoYXJnZXM= 44620
IGFwZXM= 44621
IDM3Ng== 44622
d2FsbGV0 44623
IGludG94aWNhdGlvbg== 44624
IGdvYmxpbg== 44625
IDU3MA== 44626
KysrKysrKysrKysrKysrKw== 44627
IFllbHA= 44628
IE1hZ25ldGlj 44629
IEJyaWdncw== 44630
UmFpbA== 44631
IHNwYXducw== 44632
IFdpZ2dpbnM= 44633
IHNob3djYXNlZA== 44634
IHJlc29ydGVk 44635
dWJlbg== 44636
IHdoaXBwaW5n 44637
IGltaXRhdGU= 44638
IGRpZ2VzdGlvbg== 44639
IFVTUFM= 44640
IEdlc3Q= 44641
IHllYQ== 44642
IFRpZ2h0 44643
aW5kYWw= 44644
aWNhcw== 44645
YC4= 44646
Q0FTVA== 44647
Jyc7 44648
IEZldA== 44649
b3BhdGhpYw== 44650
SW52YWxpZA== 44651
IHJlZ3JldHRlZA== 44652
IGJyb2Njb2xp 44653
IFNjb3Jlcw== 44654
ZXZl 44655
IHBvc3Rpbmdz 44656
IGFjY3VtdWxhdGluZw== 44657
IG5lZWRsZXNz 44658
ZWxmdGg= 44659
IG1heW9ycw== 44660
IHNjcmli 44661
IGFuZWNkb3Rlcw== 44662
IGJvdGNoZWQ= 44663
IFJpYmJvbg== 44664
IENvbnN0YW50aW5l 44665
aXVzZXM= 44666
ZXNzZXM= 44667
IGRldmlzZQ== 44668
Q29tcGFyZWQ= 44669
IHB1ZGRpbmc= 44670
IGdhcmc= 44671
IGV2b2tl 44672
Nzk3 44673
IGRldG94 44674
OTA5 44675
IFBpZWNlcw== 44676
IE1jQ2FydG5leQ== 44677
IG1ldGFzdA== 44678
IEtyeXB0 44679
UE9S 44680
IHRlbmRpbmc= 44681
IE1lcmNoYW50cw== 44682
UHJvb2Y= 44683
IFZhcmc= 44684
IFBvcnRhYmxl 44685
44O844OG44Kj 44686
QnJhaW4= 44687
MjUwMA== 44688
IGZvbGlhZ2U= 44689
2Lk= 44690
IG1lbnRvcnM= 44691
IEFpcmVz 44692
IG1pbmltYWxpc3Q= 44693
IGluZ2VzdGVk 44694
IFRyb2phbg== 44695
IFFpYW4= 44696
aW52b2x2ZWQ= 44697
MDI3 44698
IGVyb2RlZA== 44699
UkFGVA== 44700
IGJsdXJyeQ== 44701
TW9i 44702
IGJ1ZmZldA== 44703
IEZuYXRpYw== 44704
YWVh 44705
S05PV04= 44706
IEluaXQ= 44707
c2FmZXR5 44708
ZW51bQ== 44709
QUNUSU9O 44710
IENydXNoZXI= 44711
IERhdGVz 44712
IC4uLi4uLi4uLi4uLi4uLi4= 44713
Y2FsbGluZw== 44714
YWtvdg== 44715
IHZlbnR1cmVk 44716
IDU1NQ== 44717
YXVnYQ== 44718
SGFydA== 44719
IEFlcm8= 44720
TUFD 44721
IHRoaW5seQ== 44722
IGFycmE= 44723
U1RBVEU= 44724
aWxkZQ== 44725
IEphY3F1 44726
IEZlbWFsZXM= 44727
IHRoZW9yZW0= 44728
IDM0Ng== 44729
IHNtYXJ0ZXN0 44730
IFBVQkxJQw== 44731
IEtyb24= 44732
IEJpdHM= 44733
IFZlc3NlbA== 44734
IFRlbGVwaG9uZQ== 44735
IGRlY2Fw 44736
IGFkanVuY3Q= 44737
IFNFTg== 44738
bWVyZ2E= 44739
IHJlZGFjdGVk 44740
IHByZWhpc3Rvcmlj 44741
IGV4cGxhbmF0b3J5 44742
IFJ1bnM= 44743
IFV0dGFy 44744
IE1hbm55 44745
IEFVVEhPUg== 44746
IFVubGVhc2hlZA== 44747
IEJvd2xpbmc= 44748
YmVhbnM= 44749
Nzkz 44750
IHVuaXZlcnNlcw== 44751
IHNlbnNpdA== 44752
IEt1bmc= 44753
cmVwZWF0 44754
Y3RybA== 44755
IHBhY2Vk 44756
IGZ1bGxlcg== 44757
Q2xvY2s= 44758
IHJlY29tYg== 44759
IEZhdWw= 44760
IEJ1bmtlcg== 44761
IHBvb2xlZA== 44762
IGFuYQ== 44763
IE1vdXRo 44764
TExPVw== 44765
aHVtYW5l 44766
IGJ1bGxkbw== 44767
IE1pY2hhZWxz 44768
ZmFt 44769
IHdyZWNrZWQ= 44770
IHBvcnRyYXlz 44771
IFdoYWxl 44772
IEhlcw== 44773
IGd1ZXNzZXM= 44774
IEJyb3dzZQ== 44775
IExBUEQ= 44776
IGNvbnNlcXVlbnRpYWw= 44777
IElubm9jZW50 44778
IERSQUc= 44779
IHRyYW5zZ3Jlc3M= 44780
IE9ha3M= 44781
IHRyaXZpYQ== 44782
IFJlc29u 44783
IEFEUw== 44784
LS0r 44785
IFRvbGw= 44786
IGdyYXNwaW5n 44787
IFRIRU0= 44788
IFRhZ3M= 44789
IENvbmNsdXNpb24= 44790
IHByYWN0aWNhYmxl 44791
IGhvb3A= 44792
IHVuaW50ZW50aW9uYWxseQ== 44793
IGlnbml0ZQ== 44794
IE1vdg== 44795
dXJpemVk 44796
bGVoZW0= 44797
VGVybWlu 44798
IGNvbG91cmZ1bA== 44799
IExpbmVhcg== 44800
IEVsbGll 44801
R3k= 44802
IG1hbnBvd2Vy 44803
IGpz 44804
IGVtb2pp 44805
IFNIQVJFUw== 44806
Xy4= 44807
MDAwMDc= 44808
IHNvcGhpc3RpY2F0aW9u 44809
IHVuZGVyc2NvcmU= 44810
IHByYWN0aXNl 44811
IGJsb2I= 44812
b3BlbnM= 44813
VWtyYWluZQ== 44814
S2VlcGluZw== 44815
WUM= 44816
SlI= 44817
dWx0aW1hdGU= 44818
Q2xhaW0= 44819
IGF1dG9tb2JpbGVz 44820
OTkz 44821
c3RlZWw= 44822
IHBhcnRpbmc= 44823
IExhbms= 44824
Li4uPw== 44825
IDM4NQ== 44826
IHJlbWVtYnJhbmNl 44827
IGVhc2Vk 44828
IGNvdmFyaQ== 44829
IFNpbmQ= 44830
RWZmZWN0aXZl 44831
IGRpc3NlbWluYXRpb24= 44832
IE1vb3Nl 44833
IENsYXBwZXI= 44834
YnJhdGVz 44835
QXBwbHk= 44836
IGludmlz 44837
IHdvcnNlbmVk 44838
4oCULQ== 44839
IGxlZ2lzbGF0b3I= 44840
IExvbA== 44841
IFJvd2U= 44842
IGRlYWxlcnNoaXA= 44843
dW1hcg== 44844
aWRlbmNlcw== 44845
IGludmVzdGlnYXRlcw== 44846
IGNhc2NhZGU= 44847
IGJpZGRlcg== 44848
IEJFTg== 44849
SXJvbmljYWxseQ== 44850
IHByZXNpZGluZw== 44851
IGRpbmc= 44852
IGNvbnRyYWRpY3RlZA== 44853
IHNodXRz 44854
IEZJWA== 44855
IDM2Ng== 44856
RGlzdHJpY3Q= 44857
IHNpbmZ1bA== 44858
IENoYXJpc21h 44859
b29wcw== 44860
IHRvdGFsaXR5 44861
IHJlc3RpdHV0aW9u 44862
IE9wdGltdXM= 44863
IERhaA== 44864
IGNsdWVsZXNz 44865
dXJuZWQ= 44866
IG51dHJpdA== 44867
IGxhbmRvd25lcnM= 44868
IGZsdXNoZWQ= 44869
IGJyb2FkZW4= 44870
bWll 44871
IHByaW50bG4= 44872
IG5pZw== 44873
IENvcnB1cw== 44874
SmVu 44875
IHByb3Rv 44876
IFdpa2ltZWRpYQ== 44877
IFBhbG8= 44878
Q09S 44879
IHN0b3J5bGluZXM= 44880
IGV2YW5nZWxpY2Fscw== 44881
IERhcnJlbGw= 44882
IHJvdG9y 44883
IEhX 44884
c2tpbGxlZA== 44885
ZXJ5bA== 44886
IGJlZ2c= 44887
IEJsdW1lbnRoYWw= 44888
IHdlYXZpbmc= 44889
IGRvd253YXJkcw== 44890
IEphY2tldA== 44891
IEFOR0VM 44892
VGVjaG5vbG9neQ== 44893
IGVzb3Rlcmlj 44894
YWxkZWh5ZGU= 44895
IGZ1cmlvdXNseQ== 44896
IGZvcmVpZ25lcg== 44897
V2Vhaw== 44898
Q0hP 44899
IEhvdW5k 44900
RXhwZXJpZW5jZQ== 44901
IFBsYXlzdGF0aW9u 44902
IE1JQQ== 44903
IFVuZw== 44904
Y2xvdGg= 44905
YWdhbGw= 44906
IGNhbG1pbmc= 44907
aXplbnM= 44908
U3RydWN0 44909
IFdpdGNoZXM= 44910
IENlbGVicmF0aW9u 44911
IC4uLi4uLi4uLi4uLi4u 44912
cHRyb2xsZXI= 44913
IFRDVQ== 44914
IGJ1bm55 44915
44ON 44916
dXRvcmlhbA== 44917
IHVwc2NhbGU= 44918
IFN0YQ== 44919
IENvbG9zc3Vz 44920
IGNobG9yaWRl 44921
IFphYw== 44922
IFJlYXNvbnM= 44923
IEJyb29raW5ncw== 44924
IFdISVRF 44925
XVsv 44926
IExvc2U= 44927
OTA1 44928
IHVuZGVyc2lkZQ== 44929
ZXJuZWxz 44930
IHZhcGU= 44931
ZG96ZW4= 44932
dXBwZXQ= 44933
IFNUT1A= 44934
bWF0aWNhbA== 44935
IFN0YXRlbWVudHM= 44936
aGVkZGFy 44937
UEFD 44938
Q3VzdG9tZXI= 44939
IG1lbW9z 44940
IFBK 44941
ZW5kYXJz 44942
IExpbWl0cw== 44943
bGF1Z2g= 44944
IHN0YWJpbGl6ZWQ= 44945
IEFMRUM= 44946
WUE= 44947
VXBncmFkZQ== 44948
YWxhbQ== 44949
IHRlY2hubw== 44950
IGFuZXc= 44951
Zm9yZXNlZW4= 44952
IGNvbGxlZ2lhdGU= 44953
IFB5cm8= 44954
IERpc20= 44955
IGZyb250bGluZQ== 44956
IGFtbW9uaWE= 44957
SVU= 44958
UXVpdGU= 44959
Sm9obm55 44960
YXNzaW4= 44961
R09Q 44962
IFN0eWxlcw== 44963
IFNvdmVyZWlnbg== 44964
YWN0ZXJpYWw= 44965
NTQ5 44966
IFJJUA== 44967
IExpc3Rz 44968
IDM2NA== 44969
IFJlY2Vw 44970
c29ja2V0 44971
IEJ5cmQ= 44972
IENhbmRsZQ== 44973
QW5jaWVudA== 44974
IGFwcGVsbGFudA== 44975
ZW5mb3JjZW1lbnQ= 44976
YWNlYQ== 44977
YW5za2k= 44978
IG9sZHM= 44979
ODg2 44980
IHNsdXJz 44981
IGVtcGlyZXM= 44982
IGJ1Y2tsZQ== 44983
IGFsaWVuYXRpb24= 44984
IEFiZXJkZWVu 44985
IHVuaWNvcm4= 44986
IG92ZXJyaWRpbmc= 44987
IExY 44988
cHBh 44989
IGRlc3Bpc2Vk 44990
IEJ1Z3M= 44991
IEJTVA== 44992
U291dGhlcm4= 44993
NTMz 44994
IGhhbGxtYXJr 44995
IFBvc3Rlcg== 44996
IHN0ZW1tZWQ= 44997
IHByaW5jaXBhbHM= 44998
IFRFQ0g= 44999
IFNhbmR3aWNo 45000
SXRhbHk= 45001
IGNoZWVzeQ== 45002
IFNldFRleHRDb2xvcg== 45003
IFByb3RlY3RpdmU= 45004
IENvaG4= 45005
Sk8= 45006
YXB0b3A= 45007
UmVhc29u 45008
TGVhZGVy 45009
IFVuZGVyc3RhbmQ= 45010
IEZyaWRheXM= 45011
IENvbnRpbnVvdXM= 45012
IGNsaXBwaW5n 45013
IFJ5ZQ== 45014
IGJlcnRo 45015
dGltZXI= 45016
YW5uaXM= 45017
cmVhY3Q= 45018
IGJ1ZmZhbG8= 45019
IFBhcmFz 45020
IDY1NQ== 45021
IHByZXNpZGVk 45022
IFN1bnJpc2U= 45023
IHZldHM= 45024
IGNsb3Zlcw== 45025
IE1jQ3VsbA== 45026
U3RyZW5ndGg= 45027
R0FO 45028
IGlsbGl0ZXI= 45029
IFByaWNpbmc= 45030
bMOp 45031
IHJlc2lzdG9y 45032
IGJydW4= 45033
IFN1ZmZvbGs= 45034
0Ys= 45035
IExpdmVy 45036
UmVsZWFzZWQ= 45037
IHdoYXRz 45038
ODYw 45039
IE1lYXN1cmVz 45040
IGRlbm91bmNpbmc= 45041
IFJ5emVu 45042
IHNvdXZlbg== 45043
IGNhcmVnaXZlcnM= 45044
Y2hpbmk= 45045
IFNjYXJsZXR0 45046
IHRyb3VnaA== 45047
Q29uZ3JhdHVsYXRpb25z 45048
IHRheGlz 45049
IFRyYWRpdGlvbg== 45050
aml0 45051
IHRhYmxldG9w 45052
IGhpdGhlcnRv 45053
IGRpc2luZm9ybWF0aW9u 45054
b2ZmZW5zaXZl 45055
aHJh 45056
IERJU1RSSUNU 45057
IGNvbXBsaWNhdGU= 45058
Y2hlbmtv 45059
IFJlY29uc3RydWN0aW9u 45060
IHBhbHBhYmxl 45061
IGF1c3A= 45062
IDQyOA== 45063
IHNob3djYXNlcw== 45064
IFB1YmxpY2F0aW9u 45065
a25vd2xlZGdl 45066
aW5ub24= 45067
NDE5 45068
IHJldHJpZXZhbA== 45069
YW5kZXJz 45070
IHJlZnV0ZQ== 45071
IGlucXVpcmVk 45072
Z3Vy 45073
IG5lZ2F0aXZpdHk= 45074
IGNvbnNlcnZl 45075
IGFmdGVybGlmZQ== 45076
IHByZXN1cHA= 45077
IEdpbGxlc3BpZQ== 45078
IG10 45079
IERO 45080
VGFw 45081
IHBlcnBlbmQ= 45082
IFNteQ== 45083
ZG9lc24= 45084
IHNwaWxsaW5n 45085
IGh5cGVycw== 45086
S2F0ZQ== 45087
wq4s 45088
a2VwdA== 45089
IFBvd2VyZWQ= 45090
IGph 45091
IEtsdXg= 45092
YXJkZQ== 45093
YWJhbg== 45094
IDQ0NA== 45095
IGZsYXR0ZW5lZA== 45096
IEltcHJvdmVtZW50cw== 45097
dXJnYQ== 45098
IEt1bmQ= 45099
IGluc2NyaWJlZA== 45100
IGZhY3VsdA== 45101
IHVucHJlcGFyZWQ= 45102
IENvbnN1bWVycw== 45103
IHNhdGlzZmllcw== 45104
IHB1bG1vbmFyeQ== 45105
IGluZmlsdHJhdGlvbg== 45106
IGV4dGVybmFsbHk= 45107
IGNvbmdyYXR1bGF0aW9ucw== 45108
YWdoYW4= 45109
IGFpcmxpbmVy 45110
IGZsdW5n 45111
IGZseWVycw== 45112
R0Q= 45113
IHNuaXBwZXRz 45114
IHJlY3Vyc2l2ZQ== 45115
IG1hc3RlcmluZw== 45116
TGV4 45117
IG92ZXJ0bHk= 45118
dmc= 45119
IGx1Y2tpbHk= 45120
IGVuY3Jv 45121
IExhbmNldA== 45122
IEFieXNzYWw= 45123
ZnVuY3Rpb25hbA== 45124
IHNvdw== 45125
IHNxdWlk 45126
IG5hcnJhdGlvbg== 45127
IG5hdWdodHk= 45128
IEhvbm91cg== 45129
IFNwYXJ0YW5z 45130
IHNoYXR0ZXI= 45131
IFRhY29tYQ== 45132
IENhbG9yaWVz 45133
IFJhY2Vz 45134
U3VibWl0 45135
IHB1cnBvc2VmdWxseQ== 45136
d2F2 45137
IFlvaw== 45138
RmVzdA== 45139
IEdlcnI= 45140
TWV0cm8= 45141
IGl0aW5lcg== 45142
ZmFtb3Vz 45143
ICJ7 45144
aW5saW5l 45145
d2FzaGVy 45146
SXNzdWU= 45147
IENMSUVOVA== 45148
b3pv 45149
VmVyc2lvbnM= 45150
NzI1 45151
IEdsb2Nr 45152
IHNoaWVsZGVk 45153
IFBDUg== 45154
RU5DWQ== 45155
IFdlbGQ= 45156
IFNpbXBs 45157
IHJlZGlyZWN0ZWQ= 45158
IEtoYW0= 45159
ICg+ 45160
IGxhYm91 45161
IGRpYXBlcnM= 45162
c3Ns 45163
IGNlbGxhcg== 45164
b3JnYW5pc21z 45165
b3Jlc2M= 45166
IEJlcmtz 45167
ZGlkbg== 45168
U2hpcHBpbmc= 45169
Q2hlc3Q= 45170
IHVuZG9uZQ== 45171
IG1pbGxpb25haXJl 45172
IGNvcmRz 45173
IFlvdW5nZXI= 45174
YXBwcm9wcmlhdGVseQ== 45175
IHNlcXVlbHM= 45176
dXZl 45177
YW50aWNpcGF0ZWQ= 45178
IGxld2Q= 45179
IFNoaXJ0 45180
IERtaXRyeQ== 45181
VmV0ZXI= 45182
IHNsYXlpbmc= 45183
IFlhcg== 45184
IGNvbXBsaWNhdGlvbg== 45185
SW93YQ== 45186
IEVyaWNh 45187
IEJMTQ== 45188
Z2lybGZyaWVuZA== 45189
Ym9kaWVk 45190
NjI2 45191
MTk2Mw== 45192
IGludGVybWVkaWFyeQ== 45193
IGNvbnNvbGF0aW9u 45194
TWFzaw== 45195
IFNpZW0= 45196
b3dhbg== 45197
QmVnaW5uaW5n 45198
IGZpeG1l 45199
IGN1bG1pbmF0ZWQ= 45200
IGNvbmR1Yw== 45201
IFZvbHVudGVlcg== 45202
IHBvc2l0aW9uYWw= 45203
IGdyZWV0cw== 45204
IERlZmluaXRpb25z 45205
IHRoaW5rZXI= 45206
IGluZ2VudWl0eQ== 45207
IGZyZXNobWVu 45208
IE1vbWVudHM= 45209
IDM1Nw== 45210
YXRldXJz 45211
IEZlZEV4 45212
c2c= 45213
Njk0 45214
IGR3aW5kbGluZw== 45215
IEJPWA== 45216
c2VsYWdl 45217
IHRtcA== 45218
IHN0ZW4= 45219
IFN1dA== 45220
IG5laWdoYm91cmhvb2Rz 45221
IGNsYXNzbWF0ZQ== 45222
ZmxlZGdlZA== 45223
IGxlZnRpc3Rz 45224
IGNsaW1hdGVz 45225
QVRIRVI= 45226
IFNjeXRoZQ== 45227
dWxpZmZl 45228
IHNhZw== 45229
IGhvcHBlZA== 45230
IEZ0 45231
IEVjaw== 45232
IENL 45233
IERvb21zZGF5 45234
a2lkcw== 45235
IGdhc3BlZA== 45236
IG1vbmlrZXI= 45237
IExvZA== 45238
IENGTA== 45239
dGlvbnM= 45240
cnVtcw== 45241
Zm9saW9z 45242
IG1k 45243
IHVuY2Fubnk= 45244
IHRyYW5zcG9ydHM= 45245
IExhYnJhZG9y 45246
IHJhaWx3YXlz 45247
IGFwcGxpYW5jZQ== 45248
IENUUkw= 45249
5oA= 45250
UG9wdWxhdGlvbg== 45251
IENvbmZlZGVyYWN5 45252
IHVuYmVhcmFibGU= 45253
IGRvcnNhbA== 45254
IEluZm9ybQ== 45255
b3B0ZWQ= 45256
IEtJTEw= 45257
TWFyeA== 45258
IGh5cG9jcml0aWNhbA== 45259
cXVz 45260
IE51bWVyb3Vz 45261
IEdlb3JnaWFu 45262
IEFtYnJvc2U= 45263
IExvY2g= 45264
IGd1YmVybmF0b3JpYWw= 45265
IFhlb24= 45266
IFN1cHBvcnRz 45267
ZW5zZXI= 45268
ZWVseQ== 45269
IEF2ZW5nZXI= 45270
MTk2NQ== 45271
QXJteQ== 45272
IGp1eHRhcA== 45273
IGNob3BwaW5n 45274
IFNwbGFzaA== 45275
IFN1c3RhaW5hYmxl 45276
IEZpbmNo 45277
IDE4NjE= 45278
aWN0aXZl 45279
YXRtZWFs 45280
IEdvaGFu 45281
IGxpZ2h0c2FiZXI= 45282
IEdQQQ== 45283
dWd1 45284
IFJFUEw= 45285
dmFyaWFibGU= 45286
IGhlcnBlcw== 45287
IGRlc2VydHM= 45288
YWNpb3VzbHk= 45289
IHNpdHVhdGlvbmFs 45290
d2Vla2x5 45291
b2Js 45292
IHRleHRpbGU= 45293
IENvcm53YWxs 45294
IGNvbnRyYWNlcHRpdmVz 45295
IEFrZQ== 45296
XS0= 45297
5LmL 45298
Oiw= 45299
IFdlbQ== 45300
IEJpaGFy 45301
ICcu 45302
IGJlcmU= 45303
IGFuYWxvZ3Vl 45304
IENvb2tpZXM= 45305
IHRha2VvZmY= 45306
V2hlZWw= 45307
IG1hamVzdGlj 45308
IGNvbW11dGluZw== 45309
MDIz 45310
IENvcnBzZQ== 45311
YXNzbWVudA== 45312
bWluaQ== 45313
IGdvcmlsbGE= 45314
IEFsYXM= 45315
ZXJlZQ== 45316
IGFjcXVhaW50YW5jZXM= 45317
IEFkdmFudGFnZQ== 45318
IHNwaXJpdHVhbGx5 45319
IGV5ZWQ= 45320
cG13aWtp 45321
IEVuZGVy 45322
IHRyYW5zbHVjZW50 45323
IG5pZ2h0dGltZQ== 45324
IElNQUdFUw== 45325
NTQ1 45326
IEthbXA= 45327
IEZyZWFr 45328
IGln 45329
UG9ydGxhbmQ= 45330
NDMy 45331
IE1hdGE= 45332
IG1hcmluZXM= 45333
IGhvcnM= 45334
YXRlcmFzdQ== 45335
IEF0dHJpYnV0aW9u 45336
IC0tLS0tLS0tLQ== 45337
IGtpbnM= 45338
IEJFTE9X 45339
Kysr 45340
IHJlZWxpbmc= 45341
b2xlZA== 45342
IGNsdXR0ZXI= 45343
IFJlbGF0aXZl 45344
IDQyNw== 45345
QlVT 45346
IGF2ZXJ0 45347
IENoZW9uZw== 45348
IEFibGU= 45349
IFByeW9y 45350
RGV2ZWxvcGVy 45351
IGVuY3ljbG9wZWRpYQ== 45352
IFVTQUY= 45353
IEdhcnJ5 45354
U3BhaW4= 45355
QmxvY2tz 45356
IGV4cG9zaXRpb24= 45357
IEdhbWVyR2F0ZQ== 45358
V09S 45359
IHN0b2NrcGlsZQ== 45360
IGNsb3RoZWQ= 45361
IFRvbmU= 45362
IFJ1ZQ== 45363
dHVtYmxy 45364
IHRyZWFjaGVyb3Vz 45365
IGZyeWluZw== 45366
0Yw= 45367
IFNwaA== 45368
IHJlc3RyYWludHM= 45369
IGVtYm9kaWVz 45370
IEdlcw== 45371
U2FmZXR5 45372
IG5lZ290aWF0b3Jz 45373
bWluaW5n 45374
IEFwcGFsYWNoaWFu 45375
TE9T 45376
IEplbm5h 45377
IHBhc3NlcnM= 45378
54s= 45379
c25hcA== 45380
IHNob3J0ZW4= 45381
Y3JlYXRvcg== 45382
IGlubnVtZXJhYmxl 45383
dXRoZXJsYW5k 45384
Njc0 45385
IFdPTQ== 45386
IEFzY2VuZA== 45387
IEFybW9yeQ== 45388
IFRyYW5zYWN0aW9u 45389
S2ljaw== 45390
IHN1aXRjYXNl 45391
ZGF5TmFtZQ== 45392
IHdhc3RlZnVs 45393
bWFycmlhZ2U= 45394
IE1jQ2FiZQ== 45395
aXRlY2g= 45396
IE9zcw== 45397
Q2xvc3VyZQ== 45398
IFRyZWFzdXJlcg== 45399
IGluZGVjZW50 45400
IER1bGw= 45401
IHJlc2lkZW5jZXM= 45402
MTk1OQ== 45403
IFNldHRsZW1lbnQ= 45404
SGFtaWx0b24= 45405
IHNlbGZpZXM= 45406
IFJhbmtpbmc= 45407
IEJhcmtsZXk= 45408
IEJvcmU= 45409
IFdDUw== 45410
IE1hcml0aW1l 45411
IEh1aA== 45412
IEZvcmVzdHJ5 45413
IGN1bHRpdmF0aW5n 45414
IEJhbGxhcmQ= 45415
IGdhcnJpc29u 45416
IFNETA== 45417
OTMw 45418
IG5hc2NlbnQ= 45419
IGlycmVzaXN0aWJsZQ== 45420
IGF3ZnVsbHk= 45421
XC9cLw== 45422
IGVxdWF0ZQ== 45423
IGFudGhyb3BvbG9neQ== 45424
IFN5bHZpYQ== 45425
IGludGVzdGluZQ== 45426
IGlubm9jdW91cw== 45427
Y2Vzc2l2ZQ== 45428
YWdyYQ== 45429
IE1ldHJvaWQ= 45430
R3JhbnQ= 45431
ODU1 45432
gZY= 45433
ICJf 45434
44OD44OJ 45435
IGFwcHJhaXNhbA== 45436
IEZyZWRkeQ== 45437
MDQ2 45438
IDQwNg== 45439
IDE4MzA= 45440
IGRvY2tpbmc= 45441
U3RhdGlj 45442
IHBvbnQ= 45443
IFZvbHRhZ2U= 45444
IFN0ZWFk 45445
IE1vcnRnYWdl 45446
IEpvbmFo 45447
WUw= 45448
Q0xBU1NJRklFRA== 45449
IGFzYmVzdG9z 45450
bmlrb3Y= 45451
IGNvbGxhZ2Vu 45452
IE9yYml0YWw= 45453
UG9ja2V0 45454
Nzk5 45455
IGh5YnJpZHM= 45456
aW5jaGVz 45457
IGludm9pY2U= 45458
dW5keQ== 45459
IGluZXF1YWxpdGllcw== 45460
VHJlbmQ= 45461
d2FzaGVk 45462
QkFMTA== 45463
IGx1Y2lk 45464
IENvbW1lbnRhcnk= 45465
IHdpdHR5 45466
QnJhbmRvbg== 45467
IGJydWlzaW5n 45468
IDYyMA== 45469
ZXNjZW50 45470
Ym94aW5n 45471
UE9M 45472
IDM3OA== 45473
UmVjdA== 45474
IGxpY2VuY2Vz 45475
IE1jR2Vl 45476
cHJlc3NlZA== 45477
RGFubnk= 45478
IGphbW1lZA== 45479
b3JkaW5hdGU= 45480
IGxldGg= 45481
IGRpc3Rpbmd1aXNoZXM= 45482
IFlhbWFoYQ== 45483
SUxT 45484
IEh1bWU= 45485
IENhdGVnb3JpZXM= 45486
Um9iZXJ0cw== 45487
Q2hhcnQ= 45488
IGJlZXRsZQ== 45489
IEdyYXZleWFyZA== 45490
ICgkKQ== 45491
b8Sf 45492
IHR3aWxpZ2h0 45493
YXJlbGxh 45494
4b0= 45495
IGJvb3Rocw== 45496
IEhIUw== 45497
IEZlbGRtYW4= 45498
IGV4Y2F2YXRpb24= 45499
IHBoaWxvc29waGllcw== 45500
YXRvZ3JhcGh5 45501
IEdhcmFnZQ== 45502
dGVjaG5vbG9neQ== 45503
IHVuZm9yZ2V0dGFibGU= 45504
IHZlcmlmeWluZw== 45505
IHN1Ym9yZGluYXRlcw== 45506
RWxz 45507
IG5lYg== 45508
R2FtaW5n 45509
RU5B 45510
IEFjaGlldmVtZW50 45511
aXR0ZXJz 45512
IEdhYmU= 45513
IGR1bXBz 45514
Zm9yY2Vy 45515
IHBvaWduYW50 45516
IE1CQQ== 45517
IEhlaWRp 45518
aW1laQ== 45519
IG1hZ2Vz 45520
IGxpYmVyYXRl 45521
IGNpcmN1bWNpc2Vk 45522
IE1lcm1haWQ= 45523
IE1hdHRo 45524
dG9nZXRoZXI= 45525
IFdpY2hpdGE= 45526
IHN0b3JlZnJvbnQ= 45527
IEFkaW4= 45528
VklJ 45529
Rm91cnRo 45530
IGV4cGxvcmVycw== 45531
V0VS 45532
Tm90YWJsZQ== 45533
QnJvb2s= 45534
bWVucw== 45535
RmFpdGg= 45536
LS0tLS0tLS0t 45537
IEpvdQ== 45538
rLw= 45539
IHBpbmVhcHBsZQ== 45540
IGFtYWxn 45541
ZWxu 45542
YXJrYWJsZQ== 45543
IOOCteODvOODhuOCow== 45544
IOOCteODvOODhuOCo+ODr+ODsw== 45545
IG92YXJpYW4= 45546
IEVjaG9lcw== 45547
IGhhaXJjdXQ= 45548
IHBhdg== 45549
IGNoaWxsZWQ= 45550
YW5hc2lh 45551
IHN0eWxlZA== 45552
IGRhYg== 45553
bmlwZXI= 45554
IG1pbmlzdGVyaWFs 45555
IERVUA== 45556
VGFu 45557
IHN1bHBo 45558
IERldGVy 45559
IEJvaGVt 45560
b2Rhbg== 45561
IGVkdWNhdG9y 45562
4pOY 45563
c3Bpcg== 45564
Q2hpY2tlbg== 45565
IEVsZWFub3I= 45566
IHF1aQ== 45567
IGhlYXZpZXN0 45568
IGdyYXNwZWQ= 45569
VVJB 45570
IGNyb29rZWQ= 45571
SmVzc2ljYQ== 45572
cHJvYmxlbQ== 45573
IHByZWRldGVybWluZWQ= 45574
IG1hbmlhYw== 45575
IGJyZWF0aHM= 45576
IExhdWRlcmRhbGU= 45577
IGhvYmJpZXM= 45578
eXo= 45579
Q3JpbWU= 45580
IGNoYXJpc21h 45581
ZEw= 45582
IGxlYXBpbmc= 45583
IGtpdHRlbnM= 45584
QW5nZWxv 45585
IEpBQ0s= 45586
IFN1emFubmU= 45587
IGhhbHRpbmc= 45588
RU5USU9O 45589
IHN3YWxsb3dpbmc= 45590
IEVhcnRocXVha2U= 45591
IGVpZ2h0ZWVudGg= 45592
IE5JQw== 45593
IElORg== 45594
IENvbnNjaW91cw== 45595
IHBhcnRpY3VsYXJz 45596
Y2lyY2xl 45597
NzQw 45598
IGJlbmV2b2xlbnQ= 45599
IDc0Nw== 45600
IDQ5MA== 45601
IHJ1bmRvd24= 45602
IFZhbGVyaWU= 45603
IEJVUg== 45604
IGNpdmlsaXNhdGlvbg== 45605
IFNjaG4= 45606
V0I= 45607
b3RpZGU= 45608
aW50ZXJuYXRpb25hbA== 45609
IGpvaG4= 45610
IDE5MDI= 45611
IHBlYW51dHM= 45612
IGZsYXZvcmVk 45613
a3Vz 45614
IHJvYXJlZA== 45615
IGN1dG9mZg== 45616
6aM= 45617
IG9ybmFtZW50 45618
IGFyY2hpdGVjdHVyZXM= 45619
IDM2OQ== 45620
b2xvcg== 45621
IFdpbGRl 45622
IENSQw== 45623
IEFkanVzdGVk 45624
IHByb3Zva2luZw== 45625
bGFuZGlzaA== 45626
IHJhdGlvbmFsaXR5 45627
IGp1c3RpZmllcw== 45628
IGRpc3BlbA== 45629
IGFtZXJpYw== 45630
IFBvbGVz 45631
2Kk= 45632
IGVudmlz 45633
IERvb2RsZQ== 45634
5L2/ 45635
aWdzYXc= 45636
YXVsZHJvbg== 45637
VGVjaG5pY2Fs 45638
VGVlbg== 45639
dXBoZW0= 45640
IFhpYW5n 45641
IGRldHJhY3RvcnM= 45642
IFpp 45643
IEpvdXJuYWxpc3Rz 45644
IGNvbmR1Y2l2ZQ== 45645
IFZvbHVudGVlcnM= 45646
IHNk 45647
S25vd2luZw== 45648
IHRyYW5zbWlzc2lvbnM= 45649
IFBMQU4= 45650
IExJQg== 45651
IGFsbHVkZWQ= 45652
IG9iZQ== 45653
IGRvcGU= 45654
IEdvbGRzdGVpbg== 45655
IHdhdmVsZW5ndGhz 45656
IERlc3RpbmF0aW9u 45657
bmRh 45658
dWdp 45659
IGF0dGVudGl2ZQ== 45660
IExlYW4= 45661
cmFsdGFy 45662
IG1hbmc= 45663
bWJ1ZHM= 45664
YWtpbmdz 45665
YmVuZGVy 45666
IGFjY29s 45667
IGNyYXdsZWQ= 45668
Tk9X 45669
TWlubmVzb3Rh 45670
IGZsb3VyaXNoZWQ= 45671
IFp1cA== 45672
IFN1cGVydmlzb3I= 45673
IE9saXZpZXI= 45674
RXhjZWxsZW50 45675
IHdpZGVu 45676
RG9uZQ== 45677
IHdpZw== 45678
IG1pc2NvbmNlcHRpb25z 45679
Q29ycA== 45680
V2Fu 45681
IHZlbmVyYWJsZQ== 45682
IE5vdGFibHk= 45683
IEtsaW5nb24= 45684
YW5pbWF0ZQ== 45685
Qm9vc3Q= 45686
IFNBWQ== 45687
bWlzc2luZw== 45688
aWJsaW9ncmFwaHk= 45689
bWVsb24= 45690
IHBheWRheQ== 45691
2LM= 45692
Ym9sZQ== 45693
IHZlaWxlZA== 45694
IEFscGhhYmV0 45695
SXRhbGlhbg== 45696
IGV2ZXJsYXN0aW5n 45697
IFJJUw== 45698
IENyZWU= 45699
cm9tcHQ= 45700
IGhhdGluZw== 45701
IGdyaW5uaW5n 45702
IGdlb2dyYXBoaWNhbGx5 45703
T1NI 45704
IHdlZXBpbmc= 45705
IMKgIMKgIMKgIMKgIMKgIMKgIMKgIMKg 45706
IGltcGVjYw== 45707
TGV0dGVy 45708
IGJsb2F0ZWQ= 45709
UExB 45710
IEZlaW4= 45711
IHBlcnNldmVy 45712
VGh1bmRlcg== 45713
IGF1cg== 45714
IFJM 45715
IHBpdGZhbGxz 45716
4pa6 45717
IHByZWRvbWluYW50 45718
IDUyNQ== 45719
NzE4 45720
QVBF 45721
NzE0 45722
IGZhcm1sYW5k 45723
IFFpYW8= 45724
IHZpb2xldA== 45725
IEJhaGFtYXM= 45726
IGluZmxpY3Rpbmc= 45727
IEVmZmljaWVuY3k= 45728
IGhvbWVicmV3 45729
IHVuZGVydG9vaw== 45730
IGN1cmx5 45731
IEhhcmRpbmc= 45732
bWFuaWE= 45733
NTk2 45734
IHRlbXBlcmVk 45735
IGhhcnJvd2luZw== 45736
IFBsZWRnZQ== 45737
IEZyYW5rZW5zdGVpbg== 45738
6Ko= 45739
TW90aW9u 45740
IHByZWRpY3RhYmx5 45741
IEV4cGxvc2lvbg== 45742
b2N1c2luZw== 45743
ZXJk 45744
Y29sbw== 45745
RkZFUg== 45746
IGJhY2tmaWVsZA== 45747
IFZJREU= 45748
dWVibA== 45749
TmFycg== 45750
IEFyZ3VtZW50 45751
IGdlbm9taWM= 45752
IGJvdXRpcXVl 45753
IGJhdHRlZA== 45754
IEJpbmFyeQ== 45755
IGdhbWI= 45756
IFJoeXRobQ== 45757
Njcz 45758
IGFmbG9hdA== 45759
IE9seW1waWE= 45760
WUlORw== 45761
IGVuZGlm 45762
aXNpbg== 45763
IHdpbnRlcnM= 45764
IHNjYXR0ZXJpbmc= 45765
SXY= 45766
RGlzdGFuY2U= 45767
IHRydQ== 45768
IENvbWZvcnQ= 45769
IG5leHVz 45770
IGFpcmZsb3c= 45771
IEJ5emFudGluZQ== 45772
cGF5ZXJz 45773
Y29uaQ== 45774
IEJldHN5 45775
RGVhbA== 45776
IE51Zw== 45777
IENvbnRpbmVudA== 45778
cmVkaWJseQ== 45779
IG9wdGltaXppbmc= 45780
YWxiZWl0 45781
IGVjc3RhdGlj 45782
IFByb3Rv 45783
57c= 45784
aXZvdA== 45785
4paE 45786
ZW1w 45787
cm91bmRlcg== 45788
IGNsb3V0 45789
IElTVA== 45790
NjYz 45791
IERvbGxhcnM= 45792
IERBQw== 45793
IHN1YnNjcmliZWQ= 45794
IHJlaGVhcnNhbA== 45795
IGFtcHM= 45796
IFNoYW5n 45797
ZXNt 45798
IHNwcmlua2xl 45799
IGFzc2FpbGFudA== 45800
IE9v 45801
IENvaW5iYXNl 45802
VGFjdA== 45803
IHJldGluYQ== 45804
IG51bnM= 45805
Uk9O 45806
YXR0bw== 45807
IGp1Zw== 45808
IFNWRw== 45809
IGJpa2luaQ== 45810
IEZJTEU= 45811
IEZvdW5kZXJz 45812
ZXBvcnQ= 45813
IEtQ 45814
IHJlc3RvcmVz 45815
IFRoaWNr 45816
IGFzaG9yZQ== 45817
IGFwcHJvdmFscw== 45818
UmVuZGVy 45819
TUFH 45820
R3JhaGFt 45821
IENvcnRhbmE= 45822
44Oz44K4 45823
c3No 45824
b3JpYW5z 45825
YXJzaXR5 45826
IEluc3BpcmVk 45827
dXBwZXI= 45828
IHNpZ25hbGxpbmc= 45829
IHJlYnVrZQ== 45830
IGZsYXJlcw== 45831
IGRvd250aW1l 45832
U3R1ZGllcw== 45833
IHN0YWduYXRpb24= 45834
IFNlcXVlbmNl 45835
IGdydW50 45836
IGFzc3VyZXM= 45837
IFBMQQ== 45838
NTky 45839
IGludHJhdmVu 45840
ZGVwZW5k 45841
U3VzYW4= 45842
IE1hbnppZWw= 45843
TWFuaWE= 45844
Q29udHJhY3Q= 45845
IHNsYW1z 45846
IGN1bHR1cmVk 45847
IGNyZWRpdG9y 45848
TElTVA== 45849
IEhVTQ== 45850
IENoYXR0YW5vb2dh 45851
c2VydmVk 45852
IGNsb2FrZWQ= 45853
IEZUUA== 45854
cG93ZGVy 45855
IFN0ZWxsYQ== 45856
dWN0aXZl 45857
IGNoZWFwbHk= 45858
IE1VQ0g= 45859
IEdhbGlsZW8= 45860
IHN1aXRlcw== 45861
c3BlZWNo 45862
IGRlbGliZXJhdGlvbnM= 45863
IENoaXBz 45864
q5g= 45865
QmFsYW5jZQ== 45866
IFd5bm5l 45867
IEFrcm9u 45868
QXNzZXQ= 45869
IGhvbm91cmVk 45870
IGVkZ2Vk 45871
TGlrZXdpc2U= 45872
YW5pbW91cw== 45873
IFdhZ2U= 45874
IEV6ZWs= 45875
YWR2ZXJ0aXNlbWVudA== 45876
IFJUWA== 45877
IE1BRA== 45878
IG1pZ3JhdGluZw== 45879
IFNRVQ== 45880
IDQ3NQ== 45881
RWRpdGVk 45882
IHNob3J0aGFuZA== 45883
IEJhc2ljcw== 45884
IGNyb3RjaA== 45885
IEVWRU4= 45886
IHZt 45887
ZWZmaWNpZW5jeQ== 45888
IGNhbHZlcw== 45889
IEZyaWU= 45890
IEJyaWxsaWFudA== 45891
IHN0cmlrZXJz 45892
IHJlcGVudGFuY2U= 45893
IGFydGVyaWVz 45894
cmw= 45895
QmVk 45896
aGFw 45897
IGNyeXB0b2dyYXBoeQ== 45898
IFNhYnJlcw== 45899
IDQxNA== 45900
dmlrcw== 45901
aWhhcmE= 45902
YXBzZXM= 45903
VGFsa2luZw== 45904
IGludGVydHdpbmVk 45905
IGRvY2tz 45906
IGFsbGVsZQ== 45907
IEFydGlmYWN0 45908
IEhJTQ== 45909
dG9ybg== 45910
55U= 45911
IG9wYWNpdHk= 45912
IEVseQ== 45913
b3N1a2U= 45914
IG5pcHBsZQ== 45915
IGhhbmR3cml0dGVu 45916
IFZL 45917
IENoYW1iZXJsYWlu 45918
IExhb3M= 45919
aWdyYXBo 45920
Z3Jvdw== 45921
IHRyaWxsaW9ucw== 45922
IGRlc2NlbmRhbnQ= 45923
IFNhaWxvcg== 45924
YXN1cmluZw== 45925
IGNlaWxpbmdz 45926
IFdhcmVob3VzZQ== 45927
Zmx5aW5n 45928
IEdsb3c= 45929
IG5vbnQ= 45930
IG1pc2NhcnJpYWdl 45931
IHJpZ3M= 45932
IG1pbmlzdHJpZXM= 45933
IGVsYWJvcmF0ZWQ= 45934
IGRlbHVzaW9uYWw= 45935
IEh1bWFuZQ== 45936
IDM3OQ== 45937
bmV0cw== 45938
IGJsYWNrb3V0 45939
YWRkZXJz 45940
IG5w 45941
IFRpcmU= 45942
cm9zYw== 45943
IHN1YmRpdg== 45944
IGxpbmthZ2U= 45945
IGNocm9ub2xvZ2ljYWw= 45946
IEhFUk8= 45947
IHJlc2V0dGxlbWVudA== 45948
IFZpbnls 45949
IHBhc3RvcmFs 45950
IE1vYmls 45951
IEJhcmJhcg== 45952
Q29vbGRvd24= 45953
IEZyaXR6 45954
Y3JpbWluYWw= 45955
cmVwZQ== 45956
IGJlbGxpZw== 45957
IEJyZWVk 45958
IDQxOA== 45959
IHNlbWJsYW5jZQ== 45960
aWpr 45961
IGN1cnRhaWw= 45962
IGNsaW5jaA== 45963
Y29udGFpbmVk 45964
IFByb21wdA== 45965
YXN0b24= 45966
IHdp 45967
IHB1cnN1aXRz 45968
NTE1 45969
IEdsb3Nz 45970
IGZsaXBz 45971
IGNvdXBvbnM= 45972
IGNsb25pbmc= 45973
IExpa2VseQ== 45974
UmVtb3ZlZA== 45975
IFF1YXJ0eg== 45976
cmljZXM= 45977
IFNwZWFycw== 45978
IHBpb3Vz 45979
IGRlcHJlY2lhdGlvbg== 45980
IERhcmU= 45981
b3VuY2Vz 45982
YW1heg== 45983
T250 45984
IHBpbm5hY2xl 45985
ZG9ja2Vy 45986
MDI2 45987
IFd5cg== 45988
IFByb3Blcg== 45989
y4g= 45990
bmls 45991
Qnl0ZXM= 45992
IHNlZWtlcg== 45993
dHJpYWw= 45994
IHVuZm9sZHM= 45995
IE1hcnNl 45996
IGV4dHJhdmFnYW50 45997
IFN1cnZpdm9ycw== 45998
UkVEQUNURUQ= 45999
IFNwZWVkd2F5 46000
IENyYWlnc2xpc3Q= 46001
c3VibWl0 46002
IEdlbmVyYXRpb25z 46003
IHVwaG9sZGluZw== 46004
IGJsb29kc3RyZWFt 46005
IE1pc3Npb25z 46006
IExhd24= 46007
IGxpbWJv 46008
ZW5laQ== 46009
SHVo 46010
IFdpbGRjYXRz 46011
cHJlcA== 46012
IE1hcmt1cw== 46013
IEZvcmJpZGRlbg== 46014
cml0aWM= 46015
SU5P 46016
IGV4aGliaXRpbmc= 46017
cmVxdWVudA== 46018
Y2h1aw== 46019
IGhhYml0dWFs 46020
IENvbXBhdGliaWxpdHk= 46021
RHJhZw== 46022
UklQVA== 46023
dWphaA== 46024
R1JPVU5E 46025
IGRlbGlucXVlbnQ= 46026
IGJ1cm5lcg== 46027
IGNvbnRlbXBvcmFyaWVz 46028
IGdpbW1pY2s= 46029
bG9hZHM= 46030
IG5venpsZQ== 46031
cG9kY2FzdA== 46032
IFdhaw== 46033
IFN0YXRlbg== 46034
IEt1aA== 46035
44GT 46036
aW50ZXJydXB0ZWQ= 46037
IGludmluY2libGU= 46038
IEJ1cm5ldHQ= 46039
Y2lnYXJldHRl 46040
IFBlYmJsZQ== 46041
IFRlbXBvcmFyeQ== 46042
IE1hcmlubw== 46043
NTgy 46044
IHdhc3RlbGFuZA== 46045
aWRlbnRseQ== 46046
VHg= 46047
IHJpdGU= 46048
IFBhbmFzb25pYw== 46049
IE1pZGRsZXM= 46050
IEhvcnRvbg== 46051
YWV1cw== 46052
IGN1cmluZw== 46053
IG1hdHM= 46054
IGFkam91cm4= 46055
IGZlYXJzb21l 46056
cGV6 46057
Ym9hdHM= 46058
IHByb3BlbGw= 46059
IGNvbmZsaWN0ZWQ= 46060
IEFuZ2Vy 46061
IGluc3VyZ2VudA== 46062
S2FybA== 46063
IGNvYWxlcw== 46064
IHNvdXRod2VzdGVybg== 46065
IGRpc3N1 46066
IE92ZXJ0 46067
KioqKioqKioqKioq 46068
IGJveGVk 46069
IEJydW5l 46070
YWFh 46071
IGdhcmRlbmluZw== 46072
IEVuZ2Vs 46073
dHJhY2tz 46074
IHB1cmlmaWVk 46075
IHBsYWNlaG9sZGVy 46076
IExpa2Vz 46077
IGRhbg== 46078
R2Fi 46079
IGVjdA== 46080
IEZhdw== 46081
IEVsaW90 46082
ICcs 46083
b3Ryb3BpYw== 46084
IFJ1aW4= 46085
aGVkb24= 46086
IGNhdWw= 46087
IGFmdA== 46088
IENhZGlsbGFj 46089
Z2hh 46090
YXNzaWFu 46091
dWRlYg== 46092
IFRpY2s= 46093
IGFkanVzdHM= 46094
QVJHRVQ= 46095
NTM3 46096
aXNjaGU= 46097
YW50eQ== 46098
IEZyaWVkcmljaA== 46099
IEJsaXp6 46100
IEFPTA== 46101
Q2FtcGFpZ24= 46102
IG1hbW1hbA== 46103
IFZlaWw= 46104
IEtldg== 46105
IE1hdXJpdA== 46106
IERhbWllbg== 46107
TmF0aW9u 46108
RWFzdGVybg== 46109
IHs6 46110
ID09PT09PT09PT09PT09PT09PT09PT09PT09PT09PT09PQ== 46111
IHN0ZXJlb3R5cGljYWw= 46112
IGF0dGlj 46113
IEN5Ym9yZw== 46114
cmVxdWlyZQ== 46115
IGF3YXJkaW5n 46116
IFBhcHVh 46117
YnRu 46118
YmVudA== 46119
Qm9v 46120
ICg9 46121
IFhhbmRlcg== 46122
IFNvbWVyc2V0 46123
IGNhdGNoeQ== 46124
IGNlcnRpZnk= 46125
U1RSVUNU 46126
IGl0YWw= 46127
IHRpZGVz 46128
IEJyYW5kcw== 46129
R3JheQ== 46130
Y29tcGV0aXRpdmU= 46131
IGN1cmF0b3I= 46132
IERH 46133
b21pbml1bQ== 46134
IEdNT3M= 46135
Y2lhdGluZw== 46136
IENhcm1lbg== 46137
b3dhcmQ= 46138
QmFsdGltb3Jl 46139
IHJnYg== 46140
Q3U= 46141
IHdpcGVz 46142
c3BlbGw= 46143
SVRORVNT 46144
IHN1bW1hcml6ZXM= 46145
IFJldmlz 46146
IHdoaXN0bGVibG93ZXJz 46147
IEJyZWFjaA== 46148
IGNyb2NoZXQ= 46149
a29z 46150
ZXdza2k= 46151
IHJlcGV0 46152
IGNyaW1zb24= 46153
IEthcmFjaGk= 46154
cmVhZGFibGU= 46155
ZGltZW5zaW9u 46156
IElnb3I= 46157
aWxkZWQ= 46158
IFplZA== 46159
IEtlYW5l 46160
IENvc21ldGlj 46161
REVQ 46162
IHJldHJlYXRpbmc= 46163
IFVB 46164
ZW5zaWNhbA== 46165
IGR1c2s= 46166
IERpY2tlbnM= 46167
IGFyZW5hcw== 46168
IFBhc3NhZ2U= 46169
bGV2ZWxz 46170
IGN1cnY= 46171
UG9wZQ== 46172
IGNob3Jlcw== 46173
IEVsaXNl 46174
IENvbXBhc3M= 46175
YnVi 46176
IG1hbW1hbGlhbg== 46177
IFNhbnNrcml0 46178
IEFOQw== 46179
IENyYWNr 46180
UXVhbA== 46181
TGF1bg== 46182
YW1wdW5r 46183
IGxlYXJuZXJz 46184
IGdsYW1vcm91cw== 46185
IGZ1cnRoZQ== 46186
ZXJtb3R0 46187
Y2FuZA== 46188
R2VuZXJpYw== 46189
IG5hcnJhdGVk 46190
IGRpc29yZGVybHk= 46191
IFRyYW5zYWN0aW9ucw== 46192
IERldGVudGlvbg== 46193
IFJva3U= 46194
xI0= 46195
IHVuZGVyc3RhdGVtZW50 46196
IFNhdXI= 46197
IFJvZHJpZ28= 46198
IEFTQVA= 46199
U2lu 46200
IHJlam9pY2U= 46201
TWV0aG9kcw== 46202
IGVsZWN0cm9kZQ== 46203
IHdvcnNoaXBwZWQ= 46204
IGlkaQ== 46205
IFBoeXNpY2lhbnM= 46206
IHBvcHVw 46207
IGRlZnQ= 46208
IFJlbW92YWw= 46209
IEJ1ZW5vcw== 46210
dmVyYnM= 46211
IGZ1bms= 46212
dXNoYQ== 46213
cmljdGlvbg== 46214
b3JlYQ== 46215
IEJhbmdhbG9yZQ== 46216
IEtlbm9iaQ== 46217
enpp 46218
IG5vcm1hdGl2ZQ== 46219
IGdvYmxpbnM= 46220
IGNhZmVz 46221
IFVOQ0xBU1NJRklFRA== 46222
IEZpcmVk 46223
U0lHTg== 46224
IHNjbGVyb3Npcw== 46225
IFZvdGVy 46226
IFNvbm55 46227
IEV4dGVuZA== 46228
IEVWcw== 46229
QXJzZW5hbA== 46230
IHBzaQ== 46231
IHdpZGVzdA== 46232
IFR1cw== 46233
IGxvb21z 46234
IGp1c3RpZnlpbmc= 46235
IEdyYW5nZXI= 46236
6K8= 46237
UmVmZXI= 46238
NTgz 46239
IGZsb3VyaXNoaW5n 46240
YWJyZQ== 46241
IHJhdmU= 46242
IENvbnRyYQ== 46243
IDE4OTg= 46244
QWRkcw== 46245
IGZ1bA== 46246
IENvb2tl 46247
c29tZW9uZQ== 46248
PSM= 46249
Njcx 46250
IHlhaw== 46251
IGFydGU= 46252
IE1pc2NlbGxhbmVvdXM= 46253
IERldGVjdGlvbg== 46254
IENsYW5jeQ== 46255
4oE= 46256
YXNzaWVz 46257
IHZhbGlhbnQ= 46258
IEZlbWluaXN0 46259
Y29ycnVwdGlvbg== 46260
VmVs 46261
UGVhcg== 46262
IHN1Y2NpbmN0 46263
IHF1aWNrZXN0 46264
a3c= 46265
IHNwaXR0aW5n 46266
IExpYnJhcmllcw== 46267
5YWJ 46268
YW50eg== 46269
RGFk 46270
IFNwZWNpZmljYXRpb25z 46271
cnVwdWxvdXM= 46272
YW5kcg== 46273
UkVTVUxUUw== 46274
IHNub3diYWxs 46275
IHByZWRpcw== 46276
IEJheHRlcg== 46277
IE51cnNpbmc= 46278
IENoYWZm 46279
c3dl 46280
IG91dGFnZQ== 46281
IG5lc3Rpbmc= 46282
IG5vdG9yaWV0eQ== 46283
dHJpZ2dlcg== 46284
b25pdGU= 46285
am9u 46286
IGZvdQ== 46287
b29rZWQ= 46288
IENlbGVicml0eQ== 46289
cmVhbGl0eQ== 46290
IGZhdGln 46291
IGh1Z2dpbmc= 46292
IGJvdGhlcnM= 46293
IFBhbnplcg== 46294
IENoYW5kcmE= 46295
ZmlndXJlZA== 46296
IHZvbHRz 46297
IENsb3Vkcw== 46298
IGZlZWJsZQ== 46299
IEN1cnZl 46300
IEFzdXM= 46301
Nzg2 46302
YWJzb3I= 46303
IFZJQ0U= 46304
IEhlc3M= 46305
IG1hbnVmYWN0dXJlcw== 46306
IGdyaXp6 46307
IFBvd2VyZnVs 46308
YWNpZA== 46309
IHN1YnNlY3Rpb25z 46310
IEtydWdtYW4= 46311
IEFscHM= 46312
aXN1 46313
IHNlcXVlc3Q= 46314
IFVsdHJvbg== 46315
IFRpbmtlcg== 46316
IEdvb3Nl 46317
IG1pc21hdGNo 46318
QXR0b3JuZXk= 46319
IG1vcnBob2xvZ3k= 46320
IFNpeGVycw== 46321
dXR0ZXJlZA== 46322
IEVMRUNU 46323
Z3Jhbg== 46324
UnVzc2VsbA== 46325
IEdTTA== 46326
IGZvcnRuaWdodA== 46327
IC4p 46328
IGFwb3N0bGU= 46329
cHJvbmU= 46330
ZWxpc3Q= 46331
VW50aXRsZWQ= 46332
IEltcGxlbWVudGF0aW9u 46333
aXN0b3Jz 46334
IHRhbmtlcg== 46335
IHBsdXNo 46336
IGF0dGVuZGFudHM= 46337
IFRpaw== 46338
IEdyZWVud2ljaA== 46339
IFlvbg== 46340
IFNQTA== 46341
Y2VsbHM= 46342
dW50bGVk 46343
U29sdXRpb24= 46344
IFF1w6k= 46345
IHZhY2F0ZWQ= 46346
IHVwdGljaw== 46347
IE1lcmlkaWFu 46348
5oM= 46349
IERyaWxs 46350
OTI1 46351
NTg0 46352
IHJlbm92YXRlZA== 46353
IEt1YnJpY2s= 46354
enlr 46355
IGxvdXN5 46356
cHBlbA== 46357
b2h5ZHJhdGU= 46358
IEl6enk= 46359
bGVzaWFzdGljYWw= 46360
Q0ND 46361
IEFqYXg= 46362
IGFkYXB0ZXJz 46363
IFBldHJhZXVz 46364
IGFmZmlybWF0aW9u 46365
IFNUT1I= 46366
bGVtcw== 46367
YWRvZXM= 46368
IENvbnN0YW50aW5vcGxl 46369
IHBvbmllcw== 46370
IGxpZ2h0aG91c2U= 46371
IGFkaGVyZW50cw== 46372
IEJyZWVz 46373
b21vcnBoaWM= 46374
RmlnaHRpbmc= 46375
IHBsYXN0ZXI= 46376
IFBWQw== 46377
IE9ic3Q= 46378
IGRlYXJseQ== 46379
IFRvb3Ro 46380
aWNrc29u 46381
IHNoYW1pbmc= 46382
UGxleA== 46383
QWdn 46384
IOKApiI= 46385
IHN1YnJlZGRpdHM= 46386
IHBpZ2Vvbg== 46387
IFJlc2lkZW50aWFs 46388
IFBhc3Npbmc= 46389
IGx1bQ== 46390
IFBlbnNpb24= 46391
IHBlc3NpbWlzdGlj 46392
IDQzMg== 46393
emluc2tp 46394
Y2FkZQ== 46395
MDc1 46396
IGFwb2xvZ2lzZWQ= 46397
aXlhaA== 46398
UHV0dGluZw== 46399
IGdsb29teQ== 46400
IEx5bWU= 46401
PS09LT0tPS09LT0tPS09LQ== 46402
IFRvbWU= 46403
IFBzeWNoaWF0cmlj 46404
IEhJVA== 46405
Y21z 46406
YXBvbG9n 46407
IGJyZWFrZXI= 46408
IGRlZXBlbg== 46409
IHRoZW9yaXN0 46410
IEhpZ2hsYW5kcw== 46411
IGJha2Vy 46412
IHN0YXBsZXM= 46413
IGludGVyZmVyZWQ= 46414
IEFib3J0aW9u 46415
am9pbmVk 46416
Y2h1 46417
IGZvcm11bGF0ZQ== 46418
IHZhY2NpbmF0aW9ucw== 46419
IGJhbnRlcg== 46420
cGhldXM= 46421
IG91dGZpZWxkZXI= 46422
IE1ldGVy 46423
ICMjIyMj 46424
IDE4OTU= 46425
IG5hcnJvd2luZw== 46426
IFNUT1JZ 46427
ZnA= 46428
IENTVA== 46429
aWdub3Jl 46430
IHByb2NsYWltaW5n 46431
IFJV 46432
IEJBTEw= 46433
eW5h 46434
NjUz 46435
IHBvc2l0 46436
UFJF 46437
NTk0 46438
IFJlZ2lzdHJhcg== 46439
IFBpbGdyaW0= 46440
aWNpbw== 46441
IHByZXR0 46442
IGxpZmVsZXNz 46443
IF9fXw== 46444
TmVpZ2g= 46445
IENodXJjaGVz 46446
b3Jubw== 46447
IG9yY3M= 46448
IGtpbmRyZWQ= 46449
IEF1ZGl0 46450
IG1pbGxlbm5pYWw= 46451
IFBlcnNpYQ== 46452
Z3Jhdml0eQ== 46453
IERpc2FiaWxpdHk= 46454
IERBUks= 46455
V3M= 46456
b2Rvbg== 46457
IGdyYW5kZGF1Z2h0ZXI= 46458
IEJyb29rZQ== 46459
IEFEQQ== 46460
RVJB 46461
IHBpY2t1cHM= 46462
IFdpbGtpbnNvbg== 46463
IFNoYXJkcw== 46464
IE5L 46465
IGV4cGVs 46466
IEtpc2x5YWs= 46467
IGphcmdvbg== 46468
IHBvbGFyaXplZA== 46469
aWFuZQ== 46470
UHVibGlzaGVy 46471
IHJlYnV0dA== 46472
IGFwcHJlaGVuc2lvbg== 46473
IEtlc3NsZXI= 46474
IHByaXNt 46475
RlVM 46476
MTk2NA== 46477
IExvbGw= 46478
5L8= 46479
bGV0aGFs 46480
xZ8= 46481
IGdoZXR0bw== 46482
IGJvdWxkZXI= 46483
IFNsb3dseQ== 46484
IE9zY2Fycw== 46485
IEluc3RydWN0aW9u 46486
IFVsdHI= 46487
IE1vZQ== 46488
TmljaA== 46489
IFBBVEg= 46490
KCo= 46491
IFJFTEVBU0U= 46492
dW5pbmc= 46493
cm91c2U= 46494
ZW5lZw== 46495
IHJlaW1i 46496
IERldGVjdGVk 46497
RG9T 46498
IHN0ZXJsaW5n 46499
IGFnZ3JlZ2F0aW9u 46500
IExvbmVseQ== 46501
IEF0dGVuZA== 46502
aGlnaGVy 46503
IGFpcnN0cmlrZQ== 46504
a3Nvbg== 46505
U0VMRUNU 46506
IGRlZmxhdGlvbg== 46507
IEhlcnJlcmE= 46508
Q29sZQ== 46509
cml0Y2g= 46510
IGFkdmlzYWJsZQ== 46511
RmF4 46512
IHdvcmthcm91bmQ= 46513
IHBpZA== 46514
bW9ydGVt 46515
ZXJzZW4= 46516
IHR5cG8= 46517
IGFsdW0= 46518
Nzgy 46519
IEphbWFs 46520
c2NyaXB0cw== 46521
IGNhcHRpdmVz 46522
IFByZXNlbmNl 46523
IExpZWJlcm1hbg== 46524
YW5nZWxv 46525
IGFsY29ob2xpc20= 46526
YXNzaQ== 46527
IHJlY2l0ZQ== 46528
IGdhcGluZw== 46529
IGJhc2tldHM= 46530
IEdvdQ== 46531
QnJvd3Nlcg== 46532
bmVhdQ== 46533
IGNvcnJlY3RpdmU= 46534
dW5kYQ== 46535
c2NvcmluZw== 46536
IFhE 46537
IGZpbGFtZW50 46538
IGRlZXBlbmluZw== 46539
IFN0YWlubGVzcw== 46540
SW50ZWdlcg== 46541
IGJ1Z2d5 46542
IHRlbmFuY3k= 46543
IE11YmFyYWs= 46544
IHR1cGxl 46545
IERyb2lk 46546
IFNpdHRpbmc= 46547
IGZvcmZlaXQ= 46548
IFJhc211c3Nlbg== 46549
aXh0aWVz 46550
ZXNp 46551
IEtpbW1lbA== 46552
IG1ldGljdWxvdXNseQ== 46553
IGFwb3B0 46554
IFNlbGxlcg== 46555
MDg4 46556
ZWNha2U= 46557
aGVtYXRpY2FsbHk= 46558
VE4= 46559
IG1pbmRsZXNz 46560
IGRpZ3M= 46561
IEFjY29yZA== 46562
b25zZW5zZQ== 46563
ZW1pbmc= 46564
YnJhY2U= 46565
IGVCb29r 46566
IERpc3RyaWJ1dA== 46567
IEludmVzdG1lbnRz 46568
d3Q= 46569
XSks 46570
YmVoYXZpb3I= 46571
NTYz 46572
IGJsaW5kaW5n 46573
IFByb3Rlc3RlcnM= 46574
dG9waWE= 46575
IHJlYm9ybg== 46576
IEtlbHZpbg== 46577
IERvdmVy 46578
IERhaXJ5 46579
IE91dHM= 46580
IFsv 46581
z4A= 46582
YnA= 46583
IFZhbml0eQ== 46584
IFJlY2Fw 46585
IEhPVVNF 46586
IEZBQ0U= 46587
IDQyMg== 46588
Njky 46589
IEFudGlvY2g= 46590
Y29va2Vk 46591
IGNvbGxpZGU= 46592
IGFwcg== 46593
IHNsZWVwZXI= 46594
IEphcnZpcw== 46595
IGFsdGVybmF0aXZlbHk= 46596
IExlYXZlcw== 46597
IE1hdw== 46598
IGFudGlxdWl0eQ== 46599
IEFkaW5pZGE= 46600
IGFidXNlcg== 46601
UG9rw6ltb24= 46602
IGFzc29ydGVk 46603
IFJldmlzaW9u 46604
IFBpYW5v 46605
IEdpZGVvbg== 46606
T2NlYW4= 46607
IHNhbG9u 46608
IGJ1c3RsaW5n 46609
b2duaXRpdmU= 46610
IFJhaG1hbg== 46611
IHdhaXRlcg== 46612
IHByZXNldHM= 46613
IE9zaA== 46614
IEdIQw== 46615
b3BlcmF0b3I= 46616
IHJlcHRpbGVz 46617
IDQxMw== 46618
IEdhcnI= 46619
IENoYWs= 46620
IGhhc2hlcw== 46621
IGZhaWxpbmdz 46622
IGZvbGtsb3Jl 46623
IGFibA== 46624
IENlbmE= 46625
IE1hY0FydGh1cg== 46626
IENPVVJU 46627
IHBlcmlwaGVyeQ== 46628
YXBwZXJz 46629
IHJlY2tvbmVk 46630
IEluZmx1 46631
IENFVA== 46632
IDM3Mg== 46633
IERlZmluaXRpdmU= 46634
YXNzYXVsdA== 46635
NDIx 46636
IHJlc2Vydm9pcnM= 46637
IGRpdmVz 46638
IENvaWw= 46639
REFR 46640
IHZpdmlkbHk= 46641
IFJK 46642
IEJlbGxldg== 46643
IGVjbGVjdGlj 46644
IFNob3dkb3du 46645
IEtN 46646
aXBlZA== 46647
cmVldGluZ3M= 46648
IEFzdWth 46649
TGliZXJhbA== 46650
IM+E 46651
IGJ5c3RhbmRlcnM= 46652
IEdvb2R3aW4= 46653
dWtvbmc= 46654
U2l0 46655
IFRyZW0= 46656
IGNyaW1pbmFsbHk= 46657
IENpcmN1cw== 46658
Y2hyb21l 46659
ODg3 46660
IG5hbm9w 46661
IE9iaQ== 46662
IExPVw== 46663
b2do 46664
IEF1dGhvcnM= 46665
b2J5bA== 46666
VXJiYW4= 46667
IHRp 46668
IFdlaXI= 46669
dHJhcA== 46670
YWd5 46671
IHBhcmVudGhlc2Vz 46672
IG91dG51bWJlcmVk 46673
IGNvdW50ZXJwcm9kdWN0aXZl 46674
IFRvYmlhcw== 46675
dWJpcw== 46676
UGFyc2Vy 46677
U1RBUg== 46678
IHN5bmFwdGlj 46679
IEdlYXJz 46680
IGhpYmVy 46681
IGRlYnVua2Vk 46682
IGV4YWx0ZWQ= 46683
YXdhdHRz 46684
SE9V 46685
Q2h1cmNo 46686
IFBpeGll 46687
IFVyaQ== 46688
IEZvcm1hdGlvbg== 46689
IFByZWRpY3Rpb24= 46690
Q0VP 46691
IHRocm90dA== 46692
IEJyaXRhbm4= 46693
IE1hZGFnYXNjYXI= 46694
64s= 46695
IGJpbGxib2FyZHM= 46696
IFJQR3M= 46697
IEJlZXM= 46698
Y29tcGxldGVseQ== 46699
RklM 46700
IGRvZXNudA== 46701
IEdyZWVuYmVyZw== 46702
cmV5cw== 46703
IHNsaW5n 46704
IGVtcHRpZWQ= 46705
IFBpeGFy 46706
IERoYXJtYQ== 46707
bHVjaw== 46708
aW5ndWlzaGVk 46709
IGVuZG90 46710
IGJhYnlz 46711
MDU5 46712
Y2hlc3Q= 46713
cmF0cw== 46714
IHJpZGRlbg== 46715
IGJlZXRsZXM= 46716
IGlsbHVtaW5hdGluZw== 46717
IGZpY3RpdGlvdXM= 46718
IFByb3ZpbmNpYWw= 46719
IDc2OA== 46720
IHNoZXBoZXJk 46721
IFJlbmRlcg== 46722
IDE4OTY= 46723
Q3Jldw== 46724
IG1vbGRlZA== 46725
IFhpYW9taQ== 46726
IFNwaXJhbA== 46727
IGRlbGlt 46728
IG9yZ2FuaXNpbmc= 46729
IGhvb3Bz 46730
IEJlaQ== 46731
emhlbg== 46732
IGZ1Y2tpbg== 46733
IGRlY2Fk 46734
IHVuYmlhc2Vk 46735
YW1teQ== 46736
c3dpbmc= 46737
IHNtdWdnbGVk 46738
IGtpb3M= 46739
IFBFUlNPTg== 46740
IElucXVpc2l0b3I= 46741
IHNub3d5 46742
IHNjcmFwaW5n 46743
IEJ1cmdlc3M= 46744
UHRy 46745
YWdhbWU= 46746
Ulc= 46747
IGRyb2lk 46748
IEx5cw== 46749
IENhc3NhbmRyYQ== 46750
SmFjb2I= 46751
IDM1NA== 46752
IHBhc3R1cmU= 46753
IGZyYW5j 46754
IFNjb3RjaA== 46755
IEVuZHM= 46756
IElHRg== 46757
ZGVmaW5pdGlvbg== 46758
IGh5c3RlcmljYWw= 46759
IEJyb3duZQ== 46760
Nzcx 46761
IG1vYmlsaXphdGlvbg== 46762
5pU= 46763
aXF1ZW5lc3M= 46764
VGhvcg== 46765
IHNwZWFyaGVhZGVk 46766
IGVtYnJvaWxlZA== 46767
IGNvbmplY3R1cmU= 46768
anVkaWNpYWw= 46769
Q2hvaWNl 46770
IHBhcGVyYmFjaw== 46771
UGly 46772
IHJlY292ZXJz 46773
IFN1cmdl 46774
IFNob2d1bg== 46775
IFBlZGlhdHJpY3M= 46776
44Gg 46777
IHN3ZWVwcw== 46778
IExhYm9yYXRvcmllcw== 46779
IFBhY2tz 46780
YWx1cw== 46781
YWRkaW4= 46782
IGhlYWRsaWdodHM= 46783
Z3Jh 46784
RXZpZGVuY2U= 46785
Q09MT1I= 46786
QWRtaW4= 46787
irE= 46788
IGNvbmNvY3Q= 46789
c3VmZmljaWVudA== 46790
IHVubWFya2Vk 46791
IHJpY2huZXNz 46792
IGRpc3NlcnRhdGlvbg== 46793
IHNlYXNvbmluZw== 46794
IGdpYg== 46795
IE1hZ2Vz 46796
dW5jdGlvbnM= 46797
IE5pZA== 46798
Y2hlYXQ= 46799
IFRNWg== 46800
Y2l0aXplbnM= 46801
IENhdGhvbGljaXNt 46802
bmI= 46803
IGRpc2VtYmFyaw== 46804
IFBST0dSQU0= 46805
YXF1ZXM= 46806
VHlsZXI= 46807
T3Jn 46808
IFNsYXk= 46809
IE5lcm8= 46810
IFRvd25zZW5k 46811
SU5UT04= 46812
dGVsZQ== 46813
IG1lc21lcg== 46814
OTAx 46815
IGZpcmViYWxs 46816
ZXZpZGVuY2U= 46817
YWZmaWxpYXRlZA== 46818
IEZyZW5jaG1hbg== 46819
IEF1Z3VzdGE= 46820
MDIx 46821
IHNsZWQ= 46822
IHJldXNlZA== 46823
IEltbXVuaXR5 46824
IHdyZXN0bGU= 46825
YXNzZW1ibGVk 46826
TWFyaWE= 46827
IGd1bnNob3Rz 46828
IEJhcmJpZQ== 46829
IGNhbm5hYmlub2lkcw== 46830
IFRvYXN0 46831
IEtpbmRlcg== 46832
SVJE 46833
IHJlanV2ZW4= 46834
IGdvcmU= 46835
IHJ1cHR1cmU= 46836
IGJyZWFjaGluZw== 46837
IENhcnRvb24= 46838
IDQ1NQ== 46839
IFBhbGVv 46840
NjE0 46841
IHNwZWFycw== 46842
IEFtZXM= 46843
YWJ1cw== 46844
TWFkaXNvbg== 46845
R1JPVVA= 46846
IGFib3J0ZWQ= 46847
eWFo 46848
IGZlbG9u 46849
IGNhdXNhdGlvbg== 46850
IHByZXBhaWQ= 46851
IHBpdHRlZA== 46852
b3BsYW4= 46853
IFNoZWxsZXk= 46854
IFJ1c3Nv 46855
IFBhZ2Fu 46856
IHdpbGxmdWxseQ== 46857
IENhbmF2ZXI= 46858
dW5kcnVt 46859
IFNhbGFyeQ== 46860
IEFycGFpbw== 46861
cmVhZGVy 46862
IFJhdGlvbmFs 46863
IE92ZXJzZQ== 46864
IENhdXNlcw== 46865
ICou 46866
IHdvYg== 46867
S2VpdGg= 46868
IENvbnNlbnQ= 46869
bWFuYWM= 46870
Nzcz 46871
NjIz 46872
IGZhdGVmdWw= 46873
ZXRpbWVz 46874
IHNwaXJpdGVk 46875
IER5cw== 46876
IGhlZ2Vtb255 46877
IGJveWNvdA== 46878
IEVucmlxdWU= 46879
ZW1vdXRo 46880
IHRpbWVsaW5lcw== 46881
IFNhaGFyYQ== 46882
IFJlbGF4 46883
IFF1aW5jeQ== 46884
IExlc3NvbnM= 46885
IEVRVQ== 46886
U0VB 46887
Tks= 46888
IENvc3Rjbw== 46889
SW5jcmVhc2U= 46890
IG1vdGl2YXRpbmc= 46891
IENob25n 46892
YW1hcnU= 46893
IERpdmlkZQ== 46894
IHBlZGlncmVl 46895
IFRhc21hbmlh 46896
IFByZWx1ZGU= 46897
TGFz 46898
OTQw 46899
NTc0 46900
IGNoYXU= 46901
IFNwaWVnZWw= 46902
dW5pYw== 46903
LS0+ 46904
IFBoaWxpcHM= 46905
IEthZmth 46906
IHVwaGVhdmFs 46907
IHNlbnRpbWVudGFs 46908
IHNheA== 46909
IEFraXJh 46910
c2VyaWFs 46911
TWF0cml4 46912
IGVsZWN0aW5n 46913
IGNvbW1lbnRlcg== 46914
IE5lYnVsYQ== 46915
cGxldHM= 46916
IE5hZHU= 46917
IEFkcmVu 46918
IGVuc2hy 46919
IFJBTkQ= 46920
ZmluYW5jaWFs 46921
IENseWRl 46922
dXRoZXJmb3Jk 46923
IHNpZ25hZ2U= 46924
IGRlbGluZQ== 46925
IHBob3NwaGF0ZQ== 46926
cm92ZXJzaWFs 46927
ZmFzY2lzdA== 46928
IFZhbGw= 46929
IEJldGhsZWhlbQ== 46930
IGZvcnM= 46931
IGVuZ2xpc2g= 46932
U29saWQ= 46933
TmF0dXJl 46934
IHZh 46935
IEd1ZXN0cw== 46936
IHRhbnRhbA== 46937
IGF1dG9pbW11bmU= 46938
Ozs7Ozs7Ozs7Ozs7 46939
IFRvdGFsbHk= 46940
IE92 46941
IGRlZmVuY2Vz 46942
IENvY29udXQ= 46943
IHRyYW5xdWls 46944
IHBsb3k= 46945
IGZsYXZvdXJz 46946
IEZsYXNr 46947
44Ko44Or 46948
IFdlc3Rvbg== 46949
IFZvbHZv 46950
ODcw 46951
IG1pY3JvcGhvbmVz 46952
dmVyYmFs 46953
UlBH 46954
IGlpaQ== 46955
O30= 46956
MDI4 46957
IGhlYWRsaW5lZA== 46958
IHByaW1lZA== 46959
IGhvYXJk 46960
IFNoYWQ= 46961
IEVOVEVS 46962
IHRyaWFuZ3VsYXI= 46963
IGNhcGl0 46964
bGlr 46965
IEFuY2llbnRz 46966
IGxhc2g= 46967
IGNvbnZvbA== 46968
IGNvbG9uZWw= 46969
ZW5lbXk= 46970
R3Jh 46971
IHB1YnM= 46972
dXR0ZXJz 46973
IGFzc2lnbnM= 46974
IFBlbmV0 46975
IE1vbnN0cm91cw== 46976
IEJvd2Vu 46977
aWx2ZXI= 46978
SGF1bnRlZA== 46979
IERpbmc= 46980
c3RhcnRlZA== 46981
cGxpbg== 46982
IGNvbnRhbWluYW50cw== 46983
IERPRQ== 46984
ZmZlbg== 46985
IFRlY2huaWNpYW4= 46986
Unk= 46987
IHJvYmJlcnM= 46988
IGhvdGxpbmU= 46989
IEd1YXJkaW9sYQ== 46990
IEthdWZtYW4= 46991
cm93ZXI= 46992
IERyZXNkZW4= 46993
IEFscGluZQ== 46994
RWxm 46995
IGZtdA== 46996
IFNhcmQ= 46997
dXJzZXM= 46998
Z3B1 46999
VW5peA== 47000
IHVuZXF1aXZvY2FsbHk= 47001
IENpdGl6ZW5zaGlw 47002
cXVhZA== 47003
bWlyZQ== 47004
IFN3ZWVuZXk= 47005
QmF0dGVyeQ== 47006
NjE1 47007
IHBhbmNha2Vz 47008
IG9hdHM= 47009
TWFwcw== 47010
IENvbnRyYXN0 47011
bWJ1ZHNtYW4= 47012
IEVQUw== 47013
IHN1YmNvbW1pdHRlZQ== 47014
IHNvdXJjaW5n 47015
IHNpemluZw== 47016
IEJ1ZmZlcg== 47017
IE1hbmRhdG9yeQ== 47018
IG1vZGVyYXRlcw== 47019
IFBhdHRlcm5z 47020
IENob2NvYm8= 47021
IFphbg== 47022
IFNUQVRFUw== 47023
IEp1ZGdpbmc= 47024
IEluaGVy 47025
Kjo= 47026
IGJpbA== 47027
IFllbg== 47028
IGV4aGlsYXI= 47029
b2xsb3dlcg== 47030
emVycw== 47031
IHNudWc= 47032
bWF4aW11bQ== 47033
IGRlc3BpY2FibGU= 47034
IFBBQ0s= 47035
IEFubmV4 47036
IHNhcmNhc3RpYw== 47037
IGxhdGV4 47038
IHRhbXA= 47039
IFNhbw== 47040
YmFo 47041
IFJldmVyZW5k 47042
IENoaW5hdG93bg== 47043
IEFVVA== 47044
ZG9jdW1lbnRlZA== 47045
IEdBQkE= 47046
IENhbmFhbg== 47047
INmF 47048
IGdvdmVybnM= 47049
cHJldg== 47050
RXNj 47051
IEVzdGltYXRlcw== 47052
T1NQ 47053
IGVuZGVhdm91cg== 47054
IENsb3Npbmc= 47055
b21ldGltZQ== 47056
ZXZlcnlvbmU= 47057
IHdvcnNlbg== 47058
IHNjYW5uZXJz 47059
IGRldmlhdGlvbnM= 47060
IFJvYm90aWNz 47061
IENvbXB0b24= 47062
IHNvcmNlcmVy 47063
IGVuZG9nZW5vdXM= 47064
IGVtdWxhdGlvbg== 47065
IFBpZXJjaW5n 47066
IEFwaA== 47067
IFNvY2tldA== 47068
IGJvdWxk 47069
IE9V 47070
IEJvcmRlcmxhbmRz 47071
IDE4NjM= 47072
R29yZG9u 47073
IFdUTw== 47074
IHJlc3RyaWN0cw== 47075
IG1vc2FpYw== 47076
IG1lbG9kaWVz 47077
54Q= 47078
VGFy 47079
IGRpc3Nvbg== 47080
IFByb3ZpZGVz 47081
IC4uLi4uLg== 47082
YmVr 47083
RklY 47084
IGJyb29t 47085
YW5zaGlw 47086
RG9jdG9ycw== 47087
IG5lcmRz 47088
IFJlZ2lvbnM= 47089
bmFpc3NhbmNl 47090
IG1ldGU= 47091
IGNyZXB0 47092
cGxpbmdz 47093
IGdpcmxmcmllbmRz 47094
a25pdA== 47095
aWdlbnQ= 47096
b3dl 47097
IHVzaGVyZWQ= 47098
IEJheg== 47099
TW9iaWw= 47100
NDM0 47101
IFByZXNlbnRz 47102
b3JpZ2lu 47103
IGluc29tbmlh 47104
IEF1eA== 47105
NDM5 47106
IENoaWxp 47107
aXJzY2g= 47108
R0FNRQ== 47109
IGdlc3RhdGlvbg== 47110
YWxnaWE= 47111
cm9taXNpbmc= 47112
JCw= 47113
Y3Jvdw== 47114
IEluc3BlY3Rpb24= 47115
YXRvbWlj 47116
UmVsYXRpb25z 47117
Sk9ITg== 47118
cm9tYW4= 47119
IENsb2Nrd29yaw== 47120
IEJha3I= 47121
bW9uZQ== 47122
TUVU 47123
IHRoaXJzdHk= 47124
IGJj 47125
IGZhY3VsdGllcw== 47126
UnVt 47127
IG51YW5jZQ== 47128
IERhcml1cw== 47129
cGxldGluZw== 47130
ZnRlcnM= 47131
ZXRjaHVw 47132
UmVnaXN0cmF0aW9u 47133
IEtF 47134
UmFo 47135
IHByZWZlcmVudGlhbA== 47136
IExhc2g= 47137
IEhI 47138
VmFsaWQ= 47139
IE5BVg== 47140
IHN0YXJ2ZQ== 47141
IEdvbmc= 47142
enluc2tp 47143
IEFjdHJlc3M= 47144
IHdpaw== 47145
IHVuYWNjb21wYW5pZWQ= 47146
bHZs 47147
QnJpZGU= 47148
QURT 47149
IENvbW1hbmRv 47150
IFZhdWdobg== 47151
V2FsbGV0 47152
IGhvcHBpbmc= 47153
IFZpZQ== 47154
IGNhdmVhdHM= 47155
IGFsYXM= 47156
aWZsZWQ= 47157
YWJ1c2U= 47158
NjYx 47159
IGlibg== 47160
IGd1bA== 47161
IHJvYmJpbmc= 47162
dGls 47163
SUxB 47164
IG1pdGlnYXRpbmc= 47165
IGFwdGx5 47166
IHR5cmFudA== 47167
IG1pZGRheQ== 47168
IEdpbG1vcmU= 47169
IERlY2tlcg== 47170
IMKnwqc= 47171
cGFydGlhbA== 47172
RXhhY3RseQ== 47173
IHBoZW5vdHlwZQ== 47174
IFsrXQ== 47175
IFBsZXg= 47176
IElwcw== 47177
dmVyc2lvbnM= 47178
IGVib29r 47179
IGNoaWM= 47180
Z3Jvc3M= 47181
IjoiIn0seyI= 47182
IFN1cnByaXNpbmdseQ== 47183
TW9yZ2Fu 47184
IHJlc2lkdWVz 47185
IENvbmZlZGVyYXRpb24= 47186
aW5mZWxk 47187
IGx5cg== 47188
bW9kZXJhdGU= 47189
IHBlcnBlbmRpY3VsYXI= 47190
Vks= 47191
IHN5bmNocm9uaXplZA== 47192
IHJlZnJlc2hlZA== 47193
IGFkb3Jl 47194
IFRvcm1lbnQ= 47195
b2xpbmE= 47196
IDI2MDA= 47197
SXRlbVRyYWNrZXI= 47198
IHBpZXM= 47199
IEZBVA== 47200
IFJIUA== 47201
MDQ4 47202
IFJFU1A= 47203
IEJK 47204
YWxsb3dz 47205
UGFuZA== 47206
IHVud2VsY29tZQ== 47207
IFZvYw== 47208
IEJhc3RhcmQ= 47209
IE9X 47210
IExBUg== 47211
IEhlYWxlcg== 47212
RW52aXJvbm1lbnRhbA== 47213
IEtlbnlhbg== 47214
IFRyYW5jZQ== 47215
IFBhdHM= 47216
IGFsaWFzZXM= 47217
IEdhcmZpZWxk 47218
IGNhbXBhaWduZXI= 47219
IGFkdmFuY2VtZW50cw== 47220
IE9raW5hd2E= 47221
IENvaA== 47222
b3dza3k= 47223
IHN0YXJ2ZWQ= 47224
IHNpemVhYmxl 47225
IDotKQ== 47226
IG1STkE= 47227
IHN1c3BlbnNpb25z 47228
aXN0YXI= 47229
U2NvdGxhbmQ= 47230
UHJpbg== 47231
LS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0t 47232
IDUwMg== 47233
IHRlYXNwb29ucw== 47234
IDEwNTA= 47235
IGNvZXJjaXZl 47236
IE1hc29uaWM= 47237
ZWRkZWQ= 47238
IFBhc3Nlbmdlcg== 47239
IGxhdHQ= 47240
IGJyYWNlcw== 47241
IFN0ZWFs 47242
IE5ZVA== 47243
IEthdHM= 47244
IENlbGVzdA== 47245
YWV6 47246
VHU= 47247
IENvdWx0ZXI= 47248
8J+Y 47249
RmxpY2ty 47250
IFdpbG1pbmd0b24= 47251
aXRocw== 47252
Kys7 47253
IHZlbmRpbmc= 47254
IG5lZ3Jv 47255
IFBoaQ== 47256
IFllbGxvd3N0b25l 47257
Q2FsbGJhY2s= 47258
IHNoYW1wb28= 47259
IFNoYWRlcw== 47260
d2F0 47261
IHN1cGVyaHVtYW4= 47262
IHJpZGljdWxlZA== 47263
IGhvbGllc3Q= 47264
b21ibw== 47265
IGludGVybnM= 47266
IGhvbmU= 47267
IFBhcmFndQ== 47268
VVJJ 47269
IGRhbmdsaW5n 47270
44K7 47271
c292 47272
aWN0aW9uYWw= 47273
YXZhaWxhYmlsaXR5 47274
IHJldm9jYXRpb24= 47275
IGRvdw== 47276
aW5pYw== 47277
IFRIRUlS 47278
IGlzbw== 47279
IG91dGluZ3M= 47280
IExldGhhbA== 47281
ICkpKQ== 47282
IGluYWNjdXI= 47283
IG91dGxhbmRpc2g= 47284
IGFudXM= 47285
bGV0aWNv 47286
aWRvbg== 47287
bG9s 47288
IHVucmVndWxhdGVk 47289
IHN1Y2N1bWJlZA== 47290
IGN1ZmY= 47291
IFdhc3RlbGFuZA== 47292
bGV0YWw= 47293
IHN1YnN0cg== 47294
IGNvZmZlcnM= 47295
IGF1dG9tYWtlcnM= 47296
b3Zp 47297
IFh1ZQ== 47298
IERheXRvbmE= 47299
IGphcnJpbmc= 47300
IGZ1bWVz 47301
IGRpc2JhbmRlZA== 47302
emlr 47303
aXR0b24= 47304
IHN0cmlraW5nbHk= 47305
IHNwb3Jlcw== 47306
QWRhcHRlcg== 47307
Lik6 47308
IEx5bmRvbg== 47309
aXZhbHJ5 47310
IG9yYWxseQ== 47311
IHR1bXVsdHVvdXM= 47312
IGRpc3BsZWFzdXJl 47313
IGNvbmVz 47314
b3JyZWN0 47315
IGFwcGVhc2U= 47316
IGRlcmJ5 47317
IFRyaXBvbGk= 47318
IEFsZXNz 47319
IHBva2Vk 47320
IEd1aWx0eQ== 47321
dlA= 47322
RW5vdWdo 47323
IG9yaWdpbmFscw== 47324
Njk5 47325
IHJhYmJp 47326
IHByb3ZlcmJpYWw= 47327
IHBvc3Rwb25l 47328
ZWxvcGU= 47329
IE1pc3R5 47330
IHN0YWZmZWQ= 47331
IFVuZW1wbG95bWVudA== 47332
cmVkaXRhcnk= 47333
IGRpbGlnZW50 47334
cmVjb21t 47335
bWVhc3VyZXM= 47336
YXNpbg== 47337
ODI1 47338
IHBvbmRz 47339
IG1tb2w= 47340
IFNBUg== 47341
IENBUkU= 47342
IDM3MQ== 47343
IGNsZW5jaGVk 47344
IENvcnNhaXI= 47345
IGNhcmljYXR1cmU= 47346
em4= 47347
YXR0YWNo 47348
IFNjaHJv 47349
c3BlYWs= 47350
cGFpbnRlZA== 47351
IFN1Yw== 47352
IEVOVA== 47353
IGNlbGx1bA== 47354
IFBhaWQ= 47355
ZGlhZ24= 47356
V0hFUkU= 47357
IHRleHRlZA== 47358
QmFybg== 47359
IHJldHJhY3RlZA== 47360
IFJlZmVycmVk 47361
U2F2 47362
IHVwa2VlcA== 47363
IHdvcmtwbGFjZXM= 47364
IFRva2Vucw== 47365
IGFtcGxpZnk= 47366
Y2xpbmljYWw= 47367
IG11bHRpYw== 47368
bWJlcmc= 47369
IGNvbnZvbHV0ZWQ= 47370
UmVnaW9u 47371
NTY1 47372
IFRvcGlj 47373
IHNuYWls 47374
IHNhbGluZQ== 47375
IGluc3VycmVjdGlvbg== 47376
IFBldHI= 47377
Zm9ydHM= 47378
QkFU 47379
IE5hdmFqbw== 47380
IHJ1ZGltZW50YXJ5 47381
IExha3No 47382
T05ET04= 47383
TWVhc3VyZQ== 47384
IHRyYW5zZm9ybWVy 47385
IEdvZGRhcmQ= 47386
IGNvaW5jaWRlcw== 47387
aXJpbg== 47388
UmV4 47389
IEJvaw== 47390
cXVpdA== 47391
IHNob3RndW5z 47392
IHByb2xldGFyaWFu 47393
IHNjb3Jw 47394
IEFkYQ== 47395
NTE0 47396
IHNsYW5kZXI= 47397
cmVjb3JkZWQ= 47398
IGVtYmVsbA== 47399
cmlzb21l 47400
IGFwb2xvZ2l6aW5n 47401
IE11bGNhaXI= 47402
IEdpYnJhbHRhcg== 47403
Q2xh 47404
IGFsbG90 47405
IEF0dGVudGlvbg== 47406
IDQzMw== 47407
bGVhdmU= 47408
IHdoaW5l 47409
IElzc2E= 47410
IEZhdXN0 47411
IEJhcnJvbg== 47412
aGVueQ== 47413
IHZpY3RpbWl6ZWQ= 47414
SmV3cw== 47415
IG51cnR1cmluZw== 47416
ZXR0ZWw= 47417
V2luZ2Vk 47418
IFN1YnRsZQ== 47419
IGZsYXZvcmZ1bA== 47420
IFJlcHM= 47421
ZW5nZWQ= 47422
Y2FsbGJhY2s= 47423
IGRpcmVjdGlvbmFs 47424
IGNsYXNw 47425
IERpcmVjdGlvbnM= 47426
cGxhbmV0 47427
aWN1bHR1cmU= 47428
SGVscGVy 47429
aWNpb24= 47430
YWNpYQ== 47431
IOelng== 47432
IHN1cmdlcw== 47433
IGNhbm9l 47434
IFByZW1pZXJzaGlw 47435
YmVlbg== 47436
IGRlZmllZA== 47437
IFRyb29wZXI= 47438
IHRyaXBvZA== 47439
IGdhc3A= 47440
IEV1cGg= 47441
IEFkcw== 47442
dmVybmlnaHQ= 47443
aGlnaGx5 47444
Um9sZQ== 47445
IGVudGFuZ2xlZA== 47446
IFplaXQ= 47447
NjE4 47448
IFJ1c3R5 47449
IGhhdmVucw== 47450
IFZhdWdoYW4= 47451
SEFFTA== 47452
IFNFUlZJQ0U= 47453
Lyw= 47454
IHN0cmlja2Vu 47455
IGRlbHVzaW9ucw== 47456
IGJpcw== 47457
IEhhZg== 47458
IGdyYXRpZmljYXRpb24= 47459
IGVudGljaW5n 47460
VU5DSA== 47461
QWRhbXM= 47462
IE9MRUQ= 47463
IEJlZXRsZQ== 47464
IDE4OTk= 47465
IFNPRlRXQVJF 47466
YXRlZ29y 47467
Vkw= 47468
IFRvdGVt 47469
IEdhdG9ycw== 47470
QVRVUkVT 47471
IGltcGVkYW5jZQ== 47472
UmVnaXN0ZXJlZA== 47473
IENhcnk= 47474
IEFlcmlhbA== 47475
b25uZQ== 47476
ZW5pdW0= 47477
IGRyZWQ= 47478
IEJlZw== 47479
IGNvbmN1cnJlbnRseQ== 47480
IHN1cGVycG93ZXI= 47481
IFhhbg== 47482
amV3 47483
aW1lc3Rlcg== 47484
IERpY2tpbnNvbg== 47485
4pSB 47486
Rmxh 47487
IHByZWU= 47488
IFJvbGxpbnM= 47489
qbbm 47490
IGRlbm9taW5hdGlvbg== 47491
IExhbmE= 47492
NTE2 47493
IGluY2l0aW5n 47494
c2NyaWJlZA== 47495
anVyaWVz 47496
IFdvbmRlcnM= 47497
YXBwcm94aW1hdGVseQ== 47498
IHN1c3BlbmRpbmc= 47499
IG1vdW50YWlub3Vz 47500
IExhdWdo 47501
b2lkYWw= 47502
TnM= 47503
RGV0ZWN0 47504
KT0= 47505
IEx1dGhvcg== 47506
IFNjaHdhcnplbmVnZ2Vy 47507
IE11bGxlcg== 47508
IERldmk= 47509
ZWN5Y2xl 47510
SmFy 47511
NjEz 47512
IExvbmdo 47513
QmFo 47514
IFNQT1JUUw== 47515
bnc= 47516
IHJlZmluZW1lbnQ= 47517
IHdhdGVyd2F5cw== 47518
IGRpbmVy 47519
QmxhZGU= 47520
Njgz 47521
RmFj 47522
IGluaXRpYWxz 47523
IHJvZw== 47524
IHBhcmFub3JtYWw= 47525
QlVU 47526
IFso 47527
IFN3YW5zb24= 47528
IE1lc2g= 47529
4pas 47530
SW1wcm92ZQ== 47531
IFJhZGlhdGlvbg== 47532
IEVzdGhlcg== 47533
IEVzaw== 47534
IEFseQ== 47535
aWt5 47536
IGlycmFk 47537
IEJ1Y2tpbmdoYW0= 47538
IHJlZmlsbA== 47539
IC5f 47540
UmVwZQ== 47541
Q09OQ0xVUw== 47542
IGRpZmZlcmVudGlhdGVk 47543
IGNoaXJvcA== 47544
IEF0a2lucw== 47545
UGF0dGVybg== 47546
IGV4Y2lzZQ== 47547
IGNhYmFs 47548
TlNB 47549
IFNUQQ== 47550
IFNJTA== 47551
IFBhcmFseQ== 47552
IHJ5ZQ== 47553
IEhvd2VsbA== 47554
IENvdW50ZG93bg== 47555
bmVzc2Vz 47556
YWx5c2Vk 47557
IHJlc2l6ZQ== 47558
44K9 47559
IGJ1ZGdldGFyeQ== 47560
IFN0cmFz 47561
d2FuZw== 47562
IGFwaWVjZQ== 47563
IHByZWNpbmN0cw== 47564
IHBlYWNo 47565
IHNreWxpbmU= 47566
IDM1Mw== 47567
cG9wdWxhcg== 47568
QXBwZWFyYW5jZXM= 47569
IE1lY2hhbmljcw== 47570
IERldk9ubGluZQ== 47571
U3VsbGl2YW4= 47572
WmVu 47573
IHB1 47574
b3BvbGlz 47575
NTQ0 47576
IGRlZm9ybQ== 47577
IGNvdW50ZXJhY3Q= 47578
IExhbmdl 47579
IDQxNw== 47580
Q29uc29sZQ== 47581
Nzc0 47582
IG5vZGRpbmc= 47583
IHBvcHVsaXNt 47584
IGhlcA== 47585
IGNvdW5zZWxsaW5n 47586
Y29tcGxpYW5jZQ== 47587
VUZG 47588
IHVuZGVuaWFibHk= 47589
IHJhaWxpbmc= 47590
IEhvcm93aXR6 47591
IFNpbW9uZQ== 47592
IEJ1bmdpZQ== 47593
IGFr 47594
IFRhbGtz 47595
eGZm 47596
Zmxha2U= 47597
Q3Jhc2g= 47598
IHN3ZWF0eQ== 47599
IGJhbnF1ZXQ= 47600
IE9GRklD 47601
IGludmVudGl2ZQ== 47602
IGFzdHJvbm9tZXI= 47603
IFN0YW1mb3Jk 47604
IFNjYXJl 47605
IEdSRUVO 47606
b2xpY2l0ZWQ= 47607
IHJ1c2hlcg== 47608
IGNlbnRyaXN0 47609
aWdodGluZw== 47610
IHN1YmNsYXNz 47611
IGRpc2F2 47612
IGRlZnVuZA== 47613
IE5hbnRv 47614
b2NpYXRl 47615
bWFzdA== 47616
IHBhY2lm 47617
IG1lbmQ= 47618
ZWVycw== 47619
aW1taWdyYXRpb24= 47620
RVNTSU9O 47621
IG51bWJlcmluZw== 47622
IGxhdWdoYWJsZQ== 47623
IEVuZGVk 47624
dmlhdGlvbg== 47625
ZW1hcms= 47626
UGl0dA== 47627
IG1ldGljdWxvdXM= 47628
IExG 47629
IGNvbmdyYXR1bGF0ZWQ= 47630
IEJpcmNo 47631
IHN3YXllZA== 47632
IHNlbWlmaW5hbHM= 47633
IGh1bWFua2luZA== 47634
bWF0dGVy 47635
IEVxdWlw 47636
b3BhdXNhbA== 47637
U2FpZA== 47638
IExheW91dA== 47639
IHZvaWNpbmc= 47640
IHRodWc= 47641
IHBvcm5vZ3JhcGhpYw== 47642
SVBT 47643
IG1vYW5pbmc= 47644
IGdyaWV2YW5jZQ== 47645
IGNvbmZlc3Npb25z 47646
ZXNjYWw= 47647
VEVYVFVSRQ== 47648
QXV0aGVudA== 47649
b3NhdXJ1cw== 47650
UHVyY2hhc2U= 47651
IHJlbGVnYXRpb24= 47652
YWx0ZXI= 47653
IMKgwqA= 47654
IHJpZGRsZWQ= 47655
IG9ncmU= 47656
IExvd2VsbA== 47657
T2NjdXA= 47658
RWF0 47659
IEh5ZGVy 47660
IEFkdmlzZXI= 47661
Q29tbWVyY2U= 47662
SHVudA== 47663
IE9ydGg= 47664
IENvbXBldGl0aXZl 47665
IENMQQ== 47666
Q0RD 47667
IHNhbGFkcw== 47668
Rmxl 47669
IGluZHVzdHJpYWxpemVk 47670
YCw= 47671
IE9XTg== 47672
IGJlY2s= 47673
IFBhcnRpY3VsYXJseQ== 47674
b3VidA== 47675
IG1N 47676
IEh1c3NhaW4= 47677
IENoZW5uYWk= 47678
IDkyMA== 47679
IGFwcG9pbnRpbmc= 47680
IEN1bGxlbg== 47681
LCwsLCwsLCw= 47682
IHBvcmVz 47683
dmVyaWZpZWQ= 47684
IGJpb2NoZW1pY2Fs 47685
ZW1hdGU= 47686
IGNvd2FyZGx5 47687
IEhlbHNpbmtp 47688
IEV0aGlvcGlhbg== 47689
U09VUkNF 47690
RVJD 47691
ZXN0cm8= 47692
IGJpb3RlY2g= 47693
IFNvdXI= 47694
IGJyZXdlcg== 47695
Qmxvb21iZXJn 47696
IGludGVuc2lmeQ== 47697
R2xhc3M= 47698
YW5jbw== 47699
IEZEUg== 47700
Z3JlU1FM 47701
IEZpcmVz 47702
qbbmpbU= 47703
ZWNv 47704
MTAwMQ== 47705
IEhvbWVsZXNz 47706
IGluc3RhbnRhbmVvdXM= 47707
IEhhc3Rl 47708
aWdlbA== 47709
RGlhbW9uZA== 47710
IHBhdmluZw== 47711
IGxhbmRmaWxs 47712
IGRhZHM= 47713
aG91bg== 47714
Ol0= 47715
IGluY2VuZGlhcnk= 47716
IExpdmluZ3N0b24= 47717
IEhpbGJlcnQ= 47718
IENoZWNrcw== 47719
c3R5bGVz 47720
aW5hdG9ycw== 47721
IENsaXZl 47722
cGhyaW5l 47723
IGNoaW1wYW56ZWVz 47724
IHBhbGw= 47725
IEpN 47726
IEFhZGhhYXI= 47727
8J0= 47728
IGFjaGlldmFibGU= 47729
ZGlzYWJsZWQ= 47730
UEVU 47731
T09PT09PT08= 47732
TW90 47733
IGludGFuZ2libGU= 47734
IGJhbGxldA== 47735
IFdlYnM= 47736
IEVzdGltYXRlZA== 47737
RWZmZWN0cw== 47738
IGJhaWxlZA== 47739
Sm9zaHVh 47740
IHR1cmJ1bGVuY2U= 47741
IG9jY3VwYW50 47742
IERheWxpZ2h0 47743
IDM2MQ== 47744
bWVldA== 47745
IHN0YXRpY2FsbHk= 47746
IG9ubG9vaw== 47747
IGtp 47748
aWxsZWdhbA== 47749
IHZlbHZldA== 47750
IGRlaHlkcmF0aW9u 47751
IGFjcXVpZXM= 47752
IFJleg== 47753
YWt1cmE= 47754
IFVwdG9u 47755
YXRybw== 47756
IGluY29tcHJlaGVuc2libGU= 47757
IGJhY2tkb29y 47758
IFJoaW5v 47759
NzI3 47760
IG1hdGhz 47761
KSs= 47762
IGhlcmVzeQ== 47763
IGRm 47764
IFJvY2hl 47765
IEx5ZGlh 47766
IHBhbmNyZWF0 47767
cmVwbHk= 47768
YXJyZWxs 47769
IHNvbGljaXRhdGlvbg== 47770
IGNpcmNhZGlhbg== 47771
QklQ 47772
IGZvcmF5 47773
IGNyeXB0aWM= 47774
aXp1 47775
aW1lbw== 47776
IFRvbWF0bw== 47777
IEhvbXM= 47778
ZXhhbWluYXRpb24= 47779
IHF1YXJyeQ== 47780
IFZhbGlhbnQ= 47781
IEplcmljaG8= 47782
IElOQ0xVRA== 47783
IDE4NDA= 47784
NTE5 47785
IHJlc2lzdHM= 47786
IHNuYXBzaG90cw== 47787
IFNwdXI= 47788
IEFudGlxdQ== 47789
TG9naW4= 47790
IGJlc3RzZWxsaW5n 47791
IGFudGlj 47792
IFN1dGhlcmxhbmQ= 47793
44Ki44Or 47794
IH4v 47795
IFBhcm0= 47796
6IM= 47797
UGFnZXM= 47798
aW50ZW5zaXR5 47799
IGltbW9iaWw= 47800
IDE4NjU= 47801
enpv 47802
IG5pZnR5 47803
IGZlbnRhbnls 47804
IFByZXNlcnZhdGlvbg== 47805
b3BoZW4= 47806
IGRhcnRz 47807
IERpbm9zYXVy 47808
cG9pbnRlcnM= 47809
IFJpdGU= 47810
c3VnZ2VzdA== 47811
YXdhcmVuZXNz 47812
IFNoZXJpZGFu 47813
IHN0YW5jZXM= 47814
IHNvcmNlcnk= 47815
IHBlcmp1cnk= 47816
IE5pa29sYQ== 47817
aWV2ZXI= 47818
IGZpYW5jZQ== 47819
IEpvcmRhbmlhbg== 47820
IEJhbGxvb24= 47821
IG5hYg== 47822
IGti 47823
IGh1bWFuaXRpZXM= 47824
IFRhbmFrYQ== 47825
aGlsbGFyeQ== 47826
IGNvbnN1bHRhbmN5 47827
IFp1Yg== 47828
IHJlbWlzc2lvbg== 47829
IGNvbmZpZA== 47830
Q0hR 47831
IEZ1Zw== 47832
IGltcHJvdmlz 47833
WWVw 47834
L18= 47835
IHVud2lsbGluZ25lc3M= 47836
IHBvcnRmb2xpb3M= 47837
MDU1 47838
IEluc3RydWN0b3I= 47839
YWltYW4= 47840
IGNsYWltYW50cw== 47841
TWJwcw== 47842
IEJ5ZQ== 47843
cmVjZWl2ZWQ= 47844
VHdlZXQ= 47845
IGluZGVtbg== 47846
cml6 47847
YW1hcmE= 47848
TmF0 47849
IGV2YWx1YXRlcw== 47850
IEx1cg== 47851
ZXBhZA== 47852
Rk9Y 47853
IFRocm8= 47854
IHJ1c3R5 47855
IGJlZHJvY2s= 47856
IE9wcmFo 47857
SkI= 47858
IG1hbmlwdWxhdGl2ZQ== 47859
IHdpbGxmdWw= 47860
IHJlbGFwc2U= 47861
IGV4dGFudA== 47862
VGhlbWU= 47863
U2Vuc29y 47864
IFN0YWJpbGl0eQ== 47865
Z292ZXJu 47866
IHBvcHB5 47867
IGtuYWNr 47868
IGluc3VsYXRlZA== 47869
IFRpbGU= 47870
IEV4dHJlbQ== 47871
IHVudG9sZA== 47872
IGNvbnZlcmdl 47873
IHJlZnVlbA== 47874
aWdyb3Vw 47875
IGRpc3RvcnRpb25z 47876
IHJhdmFnZWQ= 47877
IG1lY2hhbmljYWxseQ== 47878
IFJlaWxseQ== 47879
IE5vc2U= 47880
IEluY2FybmF0aW9u 47881
IEJlY2t5 47882
YWJibGluZw== 47883
IHRhY28= 47884
IHJha2U= 47885
IG1lbGFuY2hvbHk= 47886
IGlsbHVzdHJpb3Vz 47887
IERhcnRtb3V0aA== 47888
R3VpZGU= 47889
IFJhemVy 47890
IEJlbno= 47891
VWx0aW1hdGU= 47892
IFN1cnByaXNl 47893
IHBhZ2VhbnQ= 47894
b2ZmZXI= 47895
V2hvZXZlcg== 47896
IHdpc2Vy 47897
IGNoZW1pc3Q= 47898
IEhFTEw= 47899
IEJ1bGs= 47900
IHBsdXRvbml1bQ== 47901
IENPVkVS 47902
1rw= 47903
ZmFpbGVk 47904
IHRpcmVsZXNzbHk= 47905
IGluZmVydGlsaXR5 47906
IFRyaWRlbnQ= 47907
IFNob3d0aW1l 47908
IENpdg== 47909
VmljZQ== 47910
cmVxdWlyZXM= 47911
aXR0YW5jZQ== 47912
IHVuY29udHJvbGxlZA== 47913
aW50ZXJlc3Rpbmc= 47914
NTYx 47915
IGlubm92YXRl 47916
YXRlZ2lj 47917
TGll 47918
IFNlbGxpbmc= 47919
VWw= 47920
IHNhdmlvcg== 47921
IFRvc2g= 47922
IHN3YXN0 47923
UEFTUw== 47924
IHJpbms= 47925
IGNhcmRpbw== 47926
IElybw== 47927
dWRp 47928
IHZhbnRhZ2U= 47929
IHZhbnM= 47930
IE5pw7Fv 47931
Kz0= 47932
IHByb3BhZ2F0ZQ== 47933
PD8= 47934
IG1ldGhvZG9sb2dpY2Fs 47935
MjA0Mzk= 47936
IHRyaWdseWNlcg== 47937
IGluZ3JhaW5lZA== 47938
IEFubm90YXRpb25z 47939
YXJyYW50ZWQ= 47940
NjE3 47941
IFNvZGl1bQ== 47942
IEFBQw== 47943
dGVjaG5pY2Fs 47944
bXVsdGlwbA== 47945
IDM3Mw== 47946
5Ys= 47947
IGRlY2lzaXZlbHk= 47948
IGJvb3N0ZXJz 47949
IGRlc3NlcnRz 47950
IEdyZW5hZGU= 47951
IHRlc3RpZnlpbmc= 47952
IFNjdWxseQ== 47953
SURz 47954
IGxvY2tkb3du 47955
IFNjaGVy 47956
IFLDqQ== 47957
IFdoaXRtYW4= 47958
IFJhbXNheQ== 47959
cmVtb3Rl 47960
IGhpa2Vycw== 47961
IEh5dW5kYWk= 47962
IGNvbnNjaWVudGlvdXM= 47963
IGNsZXJpY3M= 47964
IFNpYmVyaWFu 47965
dXRp 47966
aXNidXJ5 47967
IHJlbGF5ZWQ= 47968
IHF1YXJ0eg== 47969
IENCSQ== 47970
c2Vla2Vycw== 47971
dWxsYQ== 47972
IHdlbGRpbmc= 47973
IFNoYWw= 47974
YmxlYWNoZXI= 47975
VGFp 47976
IFNhbXNvbg== 47977
IHR1bWJsZQ== 47978
IEludmVzdG9y 47979
IHN1YmNvbnRyYWN0 47980
IFNoaW5yYQ== 47981
b3dpY3o= 47982
amFuZHJv 47983
ZGFk 47984
IHRlcm1pbmF0aW5n 47985
IE5ldXJhbA== 47986
5Luj 47987
IGxlYWthZ2U= 47988
IE1pZGxhbmRz 47989
IENhdWNhc3Vz 47990
7ZU= 47991
Y2l0 47992
bGxhbg== 47993
aXZhYmx5 47994
IEFsYmlvbg== 47995
IDQ1Nw== 47996
IHJlZ2lzdHJhdGlvbnM= 47997
IGNvbXJhZGU= 47998
IGNsaXBib2FyZA== 47999
MDQ3 48000
IGRpc2NvdXJhZ2luZw== 48001
IE9vcHM= 48002
QWRhcHQ= 48003
IGVtcGF0aA== 48004
bnY= 48005
IFBST1Q= 48006
IERvbm4= 48007
IFBheA== 48008
IEJheWVy 48009
dGlz 48010
U3F1YXJl 48011
IGZvb3RwcmludHM= 48012
cGFydGljaXA= 48013
IENoaWxlYW4= 48014
QnJlbmQ= 48015
aW5kdWNpbmc= 48016
TWFnbg== 48017
IGNsdWJob3VzZQ== 48018
IE1hZ251bQ== 48019
IGVuY2FtcA== 48020
IEV0aG5pYw== 48021
dWNoYQ== 48022
ZXJleQ== 48023
IHdhdGVyZWQ= 48024
IENhbGFpcw== 48025
IGNvbXBsZXhpb24= 48026
IHNlY3Rz 48027
IHJlbnRlcnM= 48028
IGJyYXM= 48029
b8SfYW4= 48030
VGltZW91dA== 48031
TWFuYWdlbWVudA== 48032
IGluZm9ncmFwaGlj 48033
UG9rZW1vbg== 48034
Q2xhcg== 48035
IGxvY2FsaXR5 48036
IGZsb3Jh 48037
YXNlbA== 48038
UG9udA== 48039
IHBvcHVsYXRl 48040
IE9uZw== 48041
IHN1YnNpc3RlbmNl 48042
IGF1Y3Rpb25z 48043
IE1jQXVsaWZmZQ== 48044
IExPT0s= 48045
YnJpbmdlcg== 48046
IHRpdGFu 48047
IG1hbmlmb2xk 48048
IOKXjw== 48049
IGNhbGlicmF0ZWQ= 48050
IGNhbGlwaGF0ZQ== 48051
IFNIRQ== 48052
IENvbW1pc3Npb25lcnM= 48053
Y2VpdmFibGU= 48054
amM= 48055
V2lubmVy 48056
NTI0 48057
IGNvbmRvbmU= 48058
T3RoZXJ3aXNl 48059
IHBpbGluZw== 48060
IGVtYm9keQ== 48061
IENyaW1lYW4= 48062
dXRpY3M= 48063
IEV4aGliaXRpb24= 48064
IDQyNg== 48065
ZWVyaW5n 48066
IHZ5aW5n 48067
IEhVR0U= 48068
Kj0t 48069
IHByaW5jaXBsZWQ= 48070
4KY= 48071
IHF1aXJrcw== 48072
IEVkaXRvcnM= 48073
cHV0aW5n 48074
R0VT 48075
IEZUQQ== 48076
4KS+ 48077
YWRkb24= 48078
IEhBTQ== 48079
IEZyaWV6YQ== 48080
V29tYW4= 48081
LiQ= 48082
IGNyaWI= 48083
IEhlcm9k 48084
IHRpbWVycw== 48085
IFNwYWNlcw== 48086
IE1hY2ludG9zaA== 48087
YXRha2E= 48088
IGdsaWRl 48089
IHNtZWxsaW5n 48090
IEJBTA== 48091
IHVuc3U= 48092
IGNvbmRvcw== 48093
IGJpY3ljbA== 48094
IFJldml2YWw= 48095
NTUz 48096
IGp1Z2dsaW5n 48097
SHVn 48098
IEthcmRhc2hpYW4= 48099
IEJhbGthbnM= 48100
bXVsdGlwbGU= 48101
IG51dHJpdGlvdXM= 48102
b2NyeQ== 48103
MTkwMA== 48104
IGludGVncmF0ZXM= 48105
IGFkam9pbmluZw== 48106
IEZvbGRlcg== 48107
cm9sbG1lbnQ= 48108
dmVuaWVudA== 48109
IHViZXI= 48110
eWk= 48111
IHdoaWZm 48112
IEp1dmVu 48113
IEJvcm91Z2g= 48114
bmV0dGU= 48115
IGJpbGluZ3VhbA== 48116
IFNwYXJrcw== 48117
cGh0aGFs 48118
bWFudWZhY3Q= 48119
IHRvdXRpbmc= 48120
IFBISQ== 48121
S2VlZmU= 48122
UmV3YXJk 48123
IGluZmFsbA== 48124
IFRlbXBlcg== 48125
dHlwaWNhbGx5 48126
IE5pa29s 48127
IHJlZ3VsYXJz 48128
IHBzZXVkb255bQ== 48129
IGV4aGliaXRpb25z 48130
IGJsYXN0ZXI= 48131
IDQwOQ== 48132
d2FybWluZw== 48133
IHJldmVyYmVy 48134
IHJlY2lwcm9jYWw= 48135
IDY3MA== 48136
aXBpZW50 48137
YmV0dA== 48138
IEJlZ2lucw== 48139
IGl0Y2hpbmc= 48140
IFBoYXI= 48141
QXNzdW1pbmc= 48142
IGVtaXR0aW5n 48143
IE1MRw== 48144
IGJpcnRocGxhY2U= 48145
IHRhdW50 48146
IEx1ZmZ5 48147
IEFtaXQ= 48148
IGNpcmNsZWQ= 48149
IE5vc3Q= 48150
ZW5uZXR0 48151
IGRlZm9yZXN0YXRpb24= 48152
IEhpc3RvcmljYWxseQ== 48153
IEV2ZXJ5ZGF5 48154
IG92ZXJ0YWtl 48155
Nzky 48156
IG51bg== 48157
IEx1Y2lh 48158
IGFjY29tcGFuaWVz 48159
IFNlZWtpbmc= 48160
IFRyYXNo 48161
YW5pc20= 48162
Um9ndWU= 48163
IG5vcnRod2VzdGVybg== 48164
IFN1cHBsZW1lbnRhbA== 48165
IE5ZVQ== 48166
IEZSSQ== 48167
IFNhdGlzZg== 48168
eGVz 48169
NTE3 48170
IHJlYXNzdXJlZA== 48171
IHNwb3JhZGlj 48172
IDcwMQ== 48173
IG1lZGlhbA== 48174
IGNhbm5hYmlub2lk 48175
IGJhcmJhcmlj 48176
IGVwaXM= 48177
IEV4cGxvc2l2ZQ== 48178
IERvdWdo 48179
IHVuc29sdmVk 48180
U3VwcG9ydGVk 48181
IGFja25vd2xlZGdtZW50 48182
c3Bhd24= 48183
IGtpdGNoZW5z 48184
IC09 48185
dGFsa2luZw== 48186
aWNpc3Q= 48187
IFBlZ2FzdXM= 48188
IFBTVQ== 48189
IHBob3Rvbg== 48190
IEF1dGhlbnRpY2F0aW9u 48191
Ukc= 48192
QCMm 48193
NzYy 48194
IENsYWly 48195
IGRpYXBlcg== 48196
IGJyaXN0 48197
IFByb3NlY3V0b3Jz 48198
IEplbQ== 48199
NjI4 48200
IEV2ZXJ5d2hlcmU= 48201
IEplYW5uZQ== 48202
ZXF1YWxpdHk= 48203
44Op44Oz 48204
b2JqZWN0cw== 48205
IFBlbGljYW5z 48206
IDM5Mg== 48207
IGJsdQ== 48208
Ynlz 48209
IEFnbw== 48210
IGluc3RydWN0aW9uYWw= 48211
IGRpc2NyaW1pbmF0aW5n 48212
IFRSQU4= 48213
IENvcm5lbA== 48214
YWdvcw== 48215
IHR5cmU= 48216
IGFzcGlyYXRpb24= 48217
IEJyaWRnZXdhdGVy 48218
Ijot 48219
ISIu 48220
IEVucw== 48221
IENvY28= 48222
UGll 48223
IGRldGFjaA== 48224
IENvdWNo 48225
IHBoeXNpcXVl 48226
IE9jY3VwYXRpb25z 48227
b3Njb3BpYw== 48228
ZW5vdWdo 48229
QnV6eg== 48230
QXBwZWFyYW5jZQ== 48231
WVA= 48232
IHJhY2Vy 48233
IGNvbXBsaWNpdHk= 48234
cnBt 48235
VG95 48236
IGludGVycnVwdHM= 48237
IENhdGFseXN0 48238
IHV0aWxpdGFyaWFu 48239
aW1wYWN0 48240
IHNwYWdoZXR0aQ== 48241
IHBvcm91cw== 48242
IGVzdGVlbWVk 48243
IGluY2luZXI= 48244
IElPQw== 48245
NzQ4 48246
IGVzcHJlc3Nv 48247
IFNtaWxl 48248
YWJpbGlh 48249
NjM1 48250
IG1hdGhlbWF0aWNpYW4= 48251
IDQyNA== 48252
IEtM 48253
IEhJUA== 48254
IG92ZXJoZWFyZA== 48255
IFR1ZA== 48256
IFRlYw== 48257
IHF1aXp6 48258
IGZsYXR0ZXJpbmc= 48259
IGNvbm4= 48260
4oCO 48261
IGF0dGFjaGVz 48262
IFJPUw== 48263
IEFDUw== 48264
IHRjcA== 48265
IFNoYW1l 48266
c2tpcA== 48267
cmVzcGVjdGVk 48268
IFRyaW5pZGFk 48269
Z3JhaW4= 48270
IGZvb3Rob2xk 48271
IFVuY2hhcnRlZA== 48272
IEp1bGlv 48273
emw= 48274
YXZvcmVk 48275
IEFueGlldHk= 48276
ZXJyb3Jz 48277
IENlbnRhdXJp 48278
aXRzY2g= 48279
RGFkZHk= 48280
IGNsdXRjaGluZw== 48281
IEltcGxlbWVudA== 48282
IEd1dGllcnJleg== 48283
IDc2MA== 48284
IHRlbGVwb3J0YXRpb24= 48285
ZW5kcmE= 48286
IHJldmVyc2libGU= 48287
c3Ryb3M= 48288
QWR2ZW50dXJl 48289
MDgz 48290
IGxpYmVyYXRpbmc= 48291
IGFzcGhhbHQ= 48292
IFNwZW5k 48293
QVJEUw== 48294
aW1zeQ== 48295
UFJFUw== 48296
IEVtZXJnaW5n 48297
IHdpbGRmaXJlcw== 48298
IHRlY2hub2xvZ2ljYWxseQ== 48299
IGVtaXRz 48300
IEFSVElDTEU= 48301
IGlycmVndWxhcml0aWVz 48302
IGNoZXJpc2g= 48303
54mI 48304
IHN0aW5r 48305
IFJvc3Q= 48306
RWNvbm9taWM= 48307
IGNvdWdoaW5n 48308
IE1jQ2Fubg== 48309
cHJvcGVydGllcw== 48310
aWxhbnRybw== 48311
IHJlbmVnb3Rp 48312
VHJhbnNsYXRpb24= 48313
IGlucXVlc3Q= 48314
IEdyYXBl 48315
b290ZXJz 48316
Z3Vp 48317
IFN3b3Jkc21hbg== 48318
YWNlYWU= 48319
aGl0dGluZw== 48320
IHJj 48321
IGV4ZXJ0ZWQ= 48322
IFNBUA== 48323
aXRlbnQ= 48324
IHBlcmlsb3Vz 48325
IG9ic2N1cml0eQ== 48326
IGFzc2Fzc2luYXRl 48327
IGFib3JpZ2luYWw= 48328
IHJlc2N1aW5n 48329
IFNoYXR0ZXJlZA== 48330
bG9ja2luZw== 48331
YWxsaW9u 48332
Q2hhbmdpbmc= 48333
IEhhcnJpbmd0b24= 48334
IEJvcmQ= 48335
IEFmZ2hhbnM= 48336
SmFtaWU= 48337
YXJldHo= 48338
IEF1Z3VzdHVz 48339
IDM4Ng== 48340
ODMw 48341
IGpvZw== 48342
b2tpbmdseQ== 48343
VHJpZ2dlcg== 48344
IEhPUg== 48345
U3RhdGlzdGljcw== 48346
IHZpZXdlcnNoaXA= 48347
IGFkZGl0aXZlcw== 48348
aHVy 48349
IG1heGltaXppbmc= 48350
IFJvdmU= 48351
IExvdWll 48352
IEJ1Y2tldA== 48353
IENIUklTVA== 48354
b3VzZWw= 48355
IHN0cmVha3M= 48356
aXJ0ZWQ= 48357
IHRlcnQ= 48358
IGNvbG9uaWFsaXNt 48359
IGJ1cnlpbmc= 48360
eWs= 48361
Q29uZGl0aW9u 48362
IERQUks= 48363
QnlJZA== 48364
NzUx 48365
4pe8 48366
IHdvcnJpc29tZQ== 48367
IHZvY2F0aW9uYWw= 48368
c2xpY2U= 48369
IHNhaWxz 48370
IENvcnJlY3Rpb25hbA== 48371
OTU0 48372
IHR1bA== 48373
S2lk 48374
bHVzdGVy 48375
IGZhbWlsaWFs 48376
IFNwaXQ= 48377
IEVwaXNjb3BhbA== 48378
U3BlY2lmaWNhbGx5 48379
IFZvbGNhbm8= 48380
cnVucw== 48381
cXM= 48382
IHZldHRlZA== 48383
IGNyYW1tZWQ= 48384
dHJvcA== 48385
aGVyZXI= 48386
VGhhbmtmdWxseQ== 48387
IHBlcmN1c3Npb24= 48388
IG9yYW5nZXM= 48389
IHJvdW5kdXA= 48390
IDQ5OQ== 48391
eGlvdXM= 48392
Q2hhcmFjdGVycw== 48393
IFppb25pc20= 48394
IFJhbw== 48395
w5vDmw== 48396
V0Y= 48397
IHVuaW50ZW50aW9uYWw= 48398
T05FWQ== 48399
R3JhYg== 48400
Q29tbWVyY2lhbA== 48401
IGdsdXRhbWF0ZQ== 48402
IE1jS2VubmE= 48403
cnVjaWF0aW5n 48404
bmluZ3Rvbg== 48405
aWh1 48406
Q2hhbg== 48407
IFN3YXA= 48408
IGxlYWZsZXRz 48409
IGZ1bmN0aW9uYWxseQ== 48410
ZXJvdXM= 48411
RmFybQ== 48412
IGNhbG9yaWM= 48413
IExpdGVyYWxseQ== 48414
Y29uY2VydA== 48415
IHNoZW5hbg== 48416
IHJlcGFpZA== 48417
ZXllcw== 48418
IGJhc2hpbmc= 48419
IEdvcmdl 48420
IGNvbGxhYm9yYXRpb25z 48421
IHVuYWNjb3VudA== 48422
aXRjaGll 48423
IHRlYW13b3Jr 48424
cHBlbGlu 48425
IHBpcGluZw== 48426
IG1pbmNlZA== 48427
IGRpYW0= 48428
cmllZw== 48429
IG1hc2NhcmE= 48430
IHN1Y2tlcg== 48431
IE1vb25z 48432
QXBwcw== 48433
IFBlY2s= 48434
IHBlcnY= 48435
IEZsb2F0 48436
b2xleQ== 48437
IE5pc2g= 48438
aW1pemU= 48439
IGFyb21hdGlj 48440
dWlu 48441
ZW5kaXNo 48442
IS8= 48443
IEJpY3ljbGU= 48444
IEFTSUM= 48445
aWxlZ2Vk 48446
IFF1YWRybw== 48447
aW9zeW4= 48448
IGxvY2tvdXQ= 48449
IFdpbms= 48450
U1BFQw== 48451
QXR0ZW1wdHM= 48452
IHNlZWRlZA== 48453
cmVkbw== 48454
aWFzaXM= 48455
IHNuYWc= 48456
44OV44Kp 48457
44K2 48458
IGdyb3VuZGluZw== 48459
IHJlbGlldmVy 48460
IGZyaXZvbG91cw== 48461
IEdpZnRz 48462
IEZhY2Vz 48463
RXNwZWNpYWxseQ== 48464
IG1pY3JvYmlvbWU= 48465
aW1hZw== 48466
IFNjaGw= 48467
IFBsZXM= 48468
IEJsZWFjaA== 48469
IElyd2lu 48470
IEVhdG9u 48471
IERpc2NpcGxl 48472
IG11bHRpcGxpY2F0aW9u 48473
IGNvZXJjZWQ= 48474
IDQxOQ== 48475
c3Ro 48476
RXZpbA== 48477
Qm9tYg== 48478
IGV4b3Jj 48479
IHN0YWdnZXJlZA== 48480
TEVTUw== 48481
IGluZXJ0aWE= 48482
IEVESVQ= 48483
IGdvYg== 48484
VHJhZGl0aW9uYWw= 48485
IGNsYXNzeQ== 48486
TGVhcnk= 48487
IFBBR0U= 48488
eXJz 48489
IHRyYW5zcG9ydGVy 48490
IG1hdHVyZWQ= 48491
IGhpamFi 48492
IGJpb21l 48493
V2hlcmVhcw== 48494
IGV4dGVybWluYXRpb24= 48495
IFR1ZXM= 48496
IFRha2VydQ== 48497
IEF1ZHJleQ== 48498
ZXJpYWw= 48499
IEFkZW4= 48500
YWZmbGVz 48501
IG5hcmNpc3Npc3RpYw== 48502
IEJhaXJk 48503
VVRG 48504
SXJl 48505
IENvbm5pZQ== 48506
Q2hhbXA= 48507
IHdoaXNwZXJpbmc= 48508
IEhhdHQ= 48509
REs= 48510
IGRpc2luZmVjdA== 48511
IGRlZHVjdGVk 48512
IHBhcnRha2U= 48513
IGRvd25ncmFkZQ== 48514
IEVzcG9ydHM= 48515
IENvbnRpbnVpbmc= 48516
IGRlbW9jcmF0aWNhbGx5 48517
aWNyb2JpYWw= 48518
aXR0YQ== 48519
IGxpbWVzdG9uZQ== 48520
IGV4ZW1wdGVk 48521
IEZyZW56eQ== 48522
SGVybQ== 48523
NzI4 48524
IGZsZWRnbGluZw== 48525
TWV0YQ== 48526
NzY1NjE= 48527
Njkz 48528
JTo= 48529
d2FrZQ== 48530
NTI2 48531
IERpc2NpcGxpbmU= 48532
IHZpcmdpbml0eQ== 48533
IExlZ2lvbnM= 48534
IEZyYW5raWU= 48535
aW50ZW50 48536
IHJlc3Ryb29tcw== 48537
IFJvdXRlcg== 48538
ZGFx 48539
IG9iamVjdGlvbmFibGU= 48540
4oaR 48541
d2Fyaw== 48542
IFJhaHVs 48543
Z2Fpbg== 48544
YWN0aXZhdGlvbg== 48545
YWJzb2x1dGU= 48546
IEFjY2Vzc2Vk 48547
IDI0MDA= 48548
b2dnbGVz 48549
IHNlY29uZGx5 48550
IERFRkVOU0U= 48551
IHBvc3RhZ2U= 48552
d3JhcHBlcg== 48553
c2hhcnA= 48554
NzI5 48555
IGNvbW11bmljYXRlcw== 48556
IGFkZG9u 48557
IE1pbGl0aWE= 48558
SG9uZw== 48559
IHNsdW1wZWQ= 48560
IEpQRUc= 48561
IEljYXI= 48562
YWRpc2g= 48563
Njgx 48564
IG1hamVzdHk= 48565
IFdvbGZnYW5n 48566
IEVsYXN0aWM= 48567
dXBlcg== 48568
IHZpeg== 48569
IHVuY29uc2Npb3VzbHk= 48570
IFNURA== 48571
IFNhc3M= 48572
IGZsb3dlcmluZw== 48573
IEhlbGlj 48574
IERyYXBlcg== 48575
IEFtYXRldXI= 48576
IG1hbnVyZQ== 48577
IGRpc2luZ2Vu 48578
IExlaQ== 48579
YnJpbmc= 48580
OTQ5 48581
IGluaGliaXRlZA== 48582
IGhlYWRxdWFydGVyZWQ= 48583
IGVuaWdtYXRpYw== 48584
77+977+977+9 48585
IHJlZHJlc3M= 48586
Ukg= 48587
IHJhdHRsZWQ= 48588
IGRpY3Rpb24= 48589
bGlv 48590
IFRCQQ== 48591
IFNOQVA= 48592
Q2FsbGluZw== 48593
IGZhc2Npc3Rz 48594
IERvdmU= 48595
aWV3aWN6 48596
MDM2 48597
IGNvYXN0cw== 48598
IFJlY3Q= 48599
ICld 48600
TG90 48601
NjI5 48602
IFNFTQ== 48603
IFBldGVyc2Vu 48604
IEV4cGxhaW4= 48605
IEJvYXJkcw== 48606
IEJlem9z 48607
IEpvdXJuYWxz 48608
IDIwMjQ= 48609
cGFyc2Vy 48610
IG1pc3RydXN0 48611
IGdyYXRl 48612
IExvY2tlZA== 48613
Ym9h 48614
U2FpbnQ= 48615
Z2FtaW5n 48616
IHZvd2Vs 48617
aW5hdGVseQ== 48618
Ymxvdw== 48619
QWxsYWg= 48620
IHVubWF0Y2hlZA== 48621
IGJvcmRlcmluZw== 48622
IEV4cGVuZA== 48623
bnI= 48624
T3JhY2xl 48625
cm91Y2g= 48626
IGNvbnRpZ3VvdXM= 48627
YWN1cw== 48628
IGRpc3RyYXVnaHQ= 48629
NTgx 48630
IGFuYXRvbWljYWw= 48631
T1g= 48632
YXBpeGVs 48633
ODMz 48634
IFBMVVM= 48635
IHJlc3VzYw== 48636
IGFiaWRpbmc= 48637
NTcz 48638
IHZhY2FuY2llcw== 48639
RW1pbHk= 48640
IGh5cG90aGFs 48641
IFdlcm5lcg== 48642
IFdlZQ== 48643
IERKcw== 48644
NTEz 48645
IHdpdGNoY3JhZnQ= 48646
IGFjdXB1bmN0dXJl 48647
ZW50YXJ5 48648
YmVuZWZpdA== 48649
UHJvZHVjdHM= 48650
IFBTUA== 48651
IE1QRw== 48652
IEppbm4= 48653
IEphcnJldHQ= 48654
IDQ0NQ== 48655
IEltYWdpbmc= 48656
IFB5dGg= 48657
RmluaXNo 48658
IHRleA== 48659
IGp1dmVuaWxlcw== 48660
IGhlcm9pc20= 48661
IGRvdWJ0bGVzcw== 48662
IEFraQ== 48663
IFRlbmQ= 48664
IFBhdHJpYXJjaA== 48665
IGJpdHRlcnM= 48666
IFRlbGVjb21tdW5pY2F0aW9ucw== 48667
aXRhdGl2ZWx5 48668
YWduYQ== 48669
IHJn 48670
IFNPTEQ= 48671
IGNvbXB1bHNpb24= 48672
IE5hc2E= 48673
IEthdGhyeW4= 48674
IG1pbGxpb25haXJlcw== 48675
IGludHJpbnNpY2FsbHk= 48676
IGJvbHN0ZXJlZA== 48677
dGltZW91dA== 48678
Zmxv 48679
IHR1dG9y 48680
cG91cg== 48681
U3RhdGVtZW50 48682
IHsq 48683
IFJ1ZG9scGg= 48684
IEtpbWJlcmx5 48685
cm9nZW5z 48686
YWRpcQ== 48687
XSs= 48688
IGluZGlnbmF0aW9u 48689
IGZyYWN0dXJpbmc= 48690
IFJlbGVhc2Vz 48691
IEdyYWlu 48692
cHJvdGVpbg== 48693
TGFnbw== 48694
IHZhY2F0aW9ucw== 48695
IGJvb3RlZA== 48696
IFRIUkVF 48697
IEhH 48698
b3Jlc2NlbmNl 48699
IHRm 48700
IHNvYXI= 48701
aW9zeW5jcg== 48702
IGdsYW5jZXM= 48703
IFNwb29u 48704
IEp1cnk= 48705
IENvd2JveQ== 48706
IGNyZWF0aXZlbHk= 48707
SGlnaGVy 48708
IHNvbGljaXRvcg== 48709
IGhhd2s= 48710
YWNpbw== 48711
ODk2 48712
IHN1cGVyZmx1 48713
IGJvbWJzaGVsbA== 48714
Y3R1cmU= 48715
IGJyb2tlcmFnZQ== 48716
IHJhaWRpbmc= 48717
IGZyZW5jaA== 48718
IGFuZ2xlZA== 48719
VHJhbnNhY3Rpb24= 48720
IEdlbm9jaWRl 48721
dXBl 48722
IEhhaXRpYW4= 48723
NTcy 48724
ITo= 48725
IHVud2l0dGluZ2x5 48726
aXRlcmF0b3I= 48727
c2Nyb2xs 48728
IHRhbGxpZWQ= 48729
IGJpb21lZGljYWw= 48730
IENBUkQ= 48731
IGV1cGhlbQ== 48732
IGJyYWluc3Rvcm0= 48733
YXF1aW4= 48734
S28= 48735
TWljaGVsbGU= 48736
IFJ1bmVz 48737
IEJhbGxpc3RpYw== 48738
dWRlcnM= 48739
IG1vZGVzdHk= 48740
IGlQYWRz 48741
IEV6ZWtpZWw= 48742
WUU= 48743
IHN0YXJzaGlw 48744
IHBvd2VyZnVsbHk= 48745
IHBlcmw= 48746
IFNoYWRl 48747
IFF1YXJ0 48748
IEVFRw== 48749
IGZpc2hlcm1hbg== 48750
T1NFRA== 48751
IFR5cGljYWw= 48752
ZGZ4 48753
IG1lc2hlcw== 48754
IGV0Y2hlZA== 48755
d29ydGhpbmVzcw== 48756
IHRvcHBsZWQ= 48757
IDM5Ng== 48758
b3JpdXM= 48759
V2Vpc3M= 48760
IG15c3Fs 48761
IFZhbGhhbGxh 48762
2ZI= 48763
bGVhc2luZw== 48764
IHJlY29tcA== 48765
cmFwbmVs 48766
U2Vs 48767
MDQz 48768
IGRlcmFpbGVk 48769
IEd1aWRlcw== 48770
SVJU 48771
IGRlaHVtYW4= 48772
IEJyaXR0YW55 48773
Iikp 48774
IGV4Y2xhaW0= 48775
IGJhbGs= 48776
IDg0MA== 48777
Q0xBSU0= 48778
aW50ZWw= 48779
TEFC 48780
IHBlZ2dlZA== 48781
IGFzdHJvcGg= 48782
c21va2luZw== 48783
IHJpZ2dpbmc= 48784
IGZpeGF0aW9u 48785
IGNhdGFwdWx0 48786
aW5zaWRl 48787
IENhc2NhZGU= 48788
IEJvbHNoZXZpaw== 48789
R2F6YQ== 48790
RGVwdGg= 48791
IGxvdWRzcGU= 48792
IGFsbW9uZHM= 48793
bWV5ZXI= 48794
bGVuZXNz 48795
amVu 48796
ZnJlc2g= 48797
IHVuYmVhdGVu 48798
IFNxdWlk 48799
IFByZXN1bWFibHk= 48800
VGltZXI= 48801
Qlc= 48802
IHJvc3RlcnM= 48803
IGVsbGlwdA== 48804
IEhhcnJpZXQ= 48805
ZGF0YWJhc2U= 48806
IE11dHVhbA== 48807
IENvbW1vZG9yZQ== 48808
dWtlZA== 48809
a25pZmU= 48810
IENPTU1VTg== 48811
aHlh 48812
IG1lbHRz 48813
YXJjaGl2ZXM= 48814
IHJhdGlmaWNhdGlvbg== 48815
IG11bHRpcGx5aW5n 48816
IGludGVyb3Blcg== 48817
IGFzY2VydA== 48818
d2luZ3M= 48819
dmVydGluZw== 48820
IFNjb3JwaW9u 48821
YXll 48822
IFBvcnRzbW91dGg= 48823
IE1UQQ== 48824
bml0 48825
aWF6ZXA= 48826
IHF1YXJhbnRpbmU= 48827
IHNsaWRlc2hvdw== 48828
IGNlbnRpbWV0ZXJz 48829
IHN5bm9wc2lz 48830
IHNwYXRl 48831
dGhpcnN0 48832
IG5vbWluYXRpbmc= 48833
IE1lbHZpbg== 48834
UHJldmlldw== 48835
IHRocm9i 48836
IGdlbmVyYXRpb25hbA== 48837
IFJhZGl1cw== 48838
cmVzdGxpbmc= 48839
cHV0YWJsZQ== 48840
YXdhcg== 48841
TkVDVA== 48842
IHVubGF3ZnVsbHk= 48843
IFJldmVsYXRpb25z 48844
V2lraXBlZGlh 48845
c3Vydg== 48846
IGV5ZWluZw== 48847
aWpu 48848
IEZX 48849
IGJydW50 48850
IGludGVyc3RlbGxhcg== 48851
IGNsaXRvcg== 48852
IENyb2F0aWFu 48853
IENoaWM= 48854
ZXZh 48855
IERpc2FwcA== 48856
IEFraW4= 48857
aW5lcmllcw== 48858
ZHVzdA== 48859
SW50ZXJlc3RlZA== 48860
IGdlbmVzaXM= 48861
IEV1Y2w= 48862
w7Zu 48863
cGlja2luZw== 48864
IG11dGF0ZWQ= 48865
IGRpc2FwcHJvdmU= 48866
IEhETA== 48867
IDYyNQ== 48868
zLY= 48869
Y2FuY2Vy 48870
IHNxdWF0cw== 48871
IGxldmVycw== 48872
RGlzY3Vzcw== 48873
PV0= 48874
RGV4 48875
IFZJREVPUw== 48876
QVVE 48877
IHRyYW5zYWN0 48878
IEtpbmVjdA== 48879
IEt1YWxh 48880
IEN5cA== 48881
NzQ3 48882
IHNoYXR0ZXJpbmc= 48883
IGFyc2VuaWM= 48884
IEludGFrZQ== 48885
IEFuZ2Vsbw== 48886
IFF1aXQ= 48887
IEtoZQ== 48888
IDE4OTM= 48889
TWFrZXI= 48890
MDI5 48891
IFBhaW50aW5n 48892
RGlzYWJsZQ== 48893
OTE2 48894
IGFuYWxnZXM= 48895
IHRhY3RpbGU= 48896
IHByb3BoZXM= 48897
IGRpY2Vk 48898
IFRyYXZlbHM= 48899
IEhlYWRlcg== 48900
IENsdWJz 48901
QXNzaXN0YW50 48902
IGluY3JpbQ== 48903
IGRpcHM= 48904
IGNydWNpZml4 48905
IFNoYW5haGFu 48906
IEludGVycHJldA== 48907
IDQwOTA= 48908
YWxvZ3k= 48909
YWJiYQ== 48910
IHNpbXVsYWM= 48911
aHVzYmFuZA== 48912
U0lN 48913
IHJlY3ljbGU= 48914
dWNlcg== 48915
ZWRnZWQ= 48916
IHJlbmFpc3NhbmNl 48917
IEJvbWJheQ== 48918
Q2F0aG9saWM= 48919
IExJTkU= 48920
IENsb3RoaW5n 48921
cmVwb3J0cw== 48922
IHBsYXVz 48923
IGRhZw== 48924
IE1hY2U= 48925
Wkk= 48926
IGludHJ1ZGVy 48927
IFZldGVyaW5hcnk= 48928
Z3J1 48929
IHNuZWFreQ== 48930
IFNpZQ== 48931
IENpbm5hbW9u 48932
UE9TRQ== 48933
IGNvdXJpZXI= 48934
IENOUw== 48935
IGVtYW5jaXBhdGlvbg== 48936
c2l0 48937
IHBsYXl0aHJvdWdo 48938
IEZhY2lsaXRpZXM= 48939
dmlydA== 48940
IEdhdW50bGV0 48941
VGhvbXBzb24= 48942
IHVuYmVsaWV2YWJseQ== 48943
UGFyYW1ldGVycw== 48944
IHN0aXRjaGluZw== 48945
aWduZQ== 48946
IFRIRVNF 48947
UHJpdmFjeQ== 48948
IHNoZW5hbmlnYW5z 48949
IHZpdHJp 48950
IFZhbGlk 48951
NTkx 48952
rbc= 48953
IFByb3RvdHlwZQ== 48954
aW5rYQ== 48955
U0NQ 48956
IFRpZA== 48957
6Ig= 48958
b2xkZWQ= 48959
IGluZGl2aWR1YWxpdHk= 48960
IGJhcmtpbmc= 48961
IG1hcnM= 48962
IFdE 48963
IDgyMA== 48964
IHRpcg== 48965
IHNsYXBwaW5n 48966
IGRpc2dydW50bGVk 48967
IEFuZ29sYQ== 48968
cml1cw== 48969
IFRvcm5hZG8= 48970
IFRodXJz 48971
IGNhcHRjaGE= 48972
IGFuZ3N0 48973
IFBvZw== 48974
IEFzc2Fzc2lucw== 48975
IEFkaWRhcw== 48976
IGpveWZ1bA== 48977
IHdoaW5pbmc= 48978
RW1lcmdlbmN5 48979
IHBob3NwaG9ydXM= 48980
IGF0dHJpdGlvbg== 48981
b3Bob24= 48982
IFRpbWJlcndvbHZlcw== 48983
IEphaA== 48984
IEJyaW5naW5n 48985
IFdhZA== 48986
IEVuc3VyZQ== 48987
b2hs 48988
IFhpZQ== 48989
b21tZWw= 48990
Y21w 48991
IHppcHBlcg== 48992
IHJlbGF0 48993
IENvcnJpZG9y 48994
bWlsbw== 48995
VElORw== 48996
QXZn 48997
IGNyb3BwZWQ= 48998
XX0= 48999
IHJhZ2Vk 49000
IEx1bXB1cg== 49001
IEd1ZXJyZXJv 49002
b3Vya2U= 49003
TnV0 49004
IG9mZnNldHM= 49005
b2dsdQ== 49006
ZHJt 49007
IG1vcnRhbHM= 49008
bGF0YWJsZQ== 49009
IGRpc21pc3NpdmU= 49010
5LiJ 49011
IHRocm9hdHM= 49012
IGNoaXBzZXQ= 49013
IFNwb3RsaWdodA== 49014
Q2F0YWxvZw== 49015
YXJ0aXN0 49016
R2I= 49017
IGNoaWxseQ== 49018
IHN0b2tlZA== 49019
IDM3NA== 49020
V2FyZA== 49021
TGF0aW4= 49022
IGZpYXNjbw== 49023
IGJsZWFjaA== 49024
IGJyYXY= 49025
RW5oYW5jZWQ= 49026
IGlub2M= 49027
IEZpb3JpbmE= 49028
Xz4= 49029
IGxldWtlbWlh 49030
IGVsdWM= 49031
IGFubm91bmNlcg== 49032
IExpdGh1YW4= 49033
IEFybWFnZWRkb24= 49034
5Yc= 49035
TGVuaW4= 49036
IFJ1aw== 49037
IHBlcHA= 49038
IFJvbWFudGlj 49039
IFBJVA== 49040
IEludGVyc3RlbGxhcg== 49041
IEF0a2luc29u 49042
UmFpZA== 49043
SnM= 49044
R29hbA== 49045
Q291cnNl 49046
IHZhbmlzaGluZw== 49047
ZXNsZXk= 49048
IFJvdW5kcw== 49049
RWxzYQ== 49050
NTkz 49051
IHJlZHVuZGFuY3k= 49052
IFNUQU5E 49053
IHByb3BoZXRpYw== 49054
IGhhYml0YWJsZQ== 49055
cnl1 49056
IGZhaW50bHk= 49057
TU9ERQ== 49058
IGZsYW5rZWQ= 49059
SVJD 49060
QXdlc29tZQ== 49061
IHNwdXJpb3Vz 49062
IFphaA== 49063
IE1TRw== 49064
IHNoYWRpbmc= 49065
IG1vdGl2YXRpb25hbA== 49066
IFNhbnRhbmE= 49067
IFNQUg== 49068
IGV4Y3J1Y2lhdGluZw== 49069
b21pYWw= 49070
IE1pa28= 49071
IExlb3BhcmQ= 49072
QWJ5c3M= 49073
IFt8 49074
ZGlydHk= 49075
IGJhdGhz 49076
IGRlbW9yYWw= 49077
YW5kcmU= 49078
UEI= 49079
IHVuaWZpY2F0aW9u 49080
IHNhY3JhbWVudA== 49081
IFsm 49082
IHByaWNlbGVzcw== 49083
IGdlbGF0aW4= 49084
IGVtYW5hdGluZw== 49085
IEFsbGFhaA== 49086
OTg2 49087
IG91dGJ1cnN0 49088
IGVyYXM= 49089
IFhWSQ== 49090
IFNQSQ== 49091
T3R0 49092
IExhemFydXM= 49093
UExJRUQ= 49094
Rmx5aW5n 49095
YmxvZ3M= 49096
V2lzY29uc2lu 49097
UmF2ZW4= 49098
IHJlYmF0ZQ== 49099
IGNyZWVwcw== 49100
IFNwYW4= 49101
IFBhaW50ZXI= 49102
IEtpcmE= 49103
IEFtb3M= 49104
IENvcnZldHRl 49105
Q29uc3VtZXI= 49106
IFJlY292ZXI= 49107
Y2tp 49108
IHBlc2t5 49109
IEludmVudGlvbg== 49110
Q29tcGFuaWVz 49111
IGNoYWxsZW5nZXJz 49112
YWRlbWlj 49113
IFVrcmFpbmlhbnM= 49114
IE5ldXJvbG9n 49115
IEZvcnNha2Vu 49116
IGVudHJhbnRz 49117
IGVtYmF0dGxlZA== 49118
IGRlZnVuY3Q= 49119
IEdsYWNpZXI= 49120
IHBvaXNvbnM= 49121
IEhvcnNlcw== 49122
bWFrZXM= 49123
IERpcnQ= 49124
IDQyMw== 49125
aGho 49126
IFRyYW5zZm9ybWF0aW9u 49127
UVVJUkU= 49128
Li4uLi4uLi4uLi4uLi4uLi4u 49129
IHRyYXZlbGxlcg== 49130
IFNleHk= 49131
IEtlcm4= 49132
aXBvbGFy 49133
IHJhbnNvbXdhcmU= 49134
b29vb29vb29vb29vb29vbw== 49135
RWM= 49136
cnVieQ== 49137
UHJvZmVzc2lvbmFs 49138
IE91dGJyZWFr 49139
YXJndW1lbnQ= 49140
R3JleQ== 49141
IEZpZmE= 49142
IENITw== 49143
IEZPUk0= 49144
IEFtdHJhaw== 49145
LVs= 49146
IGNyYWRsZQ== 49147
IGFudGlveGlkYW50cw== 49148
44Gu5a4= 49149
NzM2 49150
IE5BU0w= 49151
IENvbnRyaWJ1dGlvbnM= 49152
SW5kaWFuYQ== 49153
IFNURVA= 49154
Q1NT 49155
IHNhbGllbnQ= 49156
IGFsbG9jYXRpb25z 49157
eXJpZ2h0cw== 49158
IG1hc2hlZA== 49159
IEN1dHRlcg== 49160
U2V4dWFs 49161
IHBvdW5kZWQ= 49162
IGZhbmJhc2U= 49163
IGNhc2M= 49164
IFRyYW5zcGFyZW5jeQ== 49165
IGFuYWx5dGlj 49166
IFN1bW1vbmVy 49167
154= 49168
IEFEQw== 49169
ZGV0YWls 49170
IHZhbnF1aXNoZWQ= 49171
IGNyYWJz 49172
YXJpZQ== 49173
RGVzdHJveQ== 49174
IFNhY2s= 49175
IHRyYW5zaXN0b3I= 49176
QWxhYmFtYQ== 49177
IEtvZW4= 49178
IEZpc2hlcmllcw== 49179
Y29uZQ== 49180
IGFubmV4ZWQ= 49181
IE1HTQ== 49182
ZXNh 49183
IGZha2Vk 49184
IENvbmdyYXR1bGF0aW9ucw== 49185
IGhpbmRlcmVk 49186
IGNvcnJlY3Rpb25hbA== 49187
IElUVg== 49188
bGVldmU= 49189
IGluYXBwcm9wcmlhdGVseQ== 49190
bGlja3M= 49191
IHRyZXNwYXNz 49192
IHBhd3M= 49193
IG5lZ290aWF0b3I= 49194
IENocmlzdGVuc2Vu 49195
bGltaXRz 49196
IERpYW5uZQ== 49197
IGVsZWdhbmNl 49198
IENvbnRyYWN0cw== 49199
YW5rZQ== 49200
T2Jq 49201
IHZpZ2lsYW5jZQ== 49202
IGNhc3RsZXM= 49203
IE5BRA== 49204
IEhvbG8= 49205
IGVtcGhhdGljYWxseQ== 49206
IFRpdHVz 49207
IFNlcnZpbmc= 49208
IFJpY2hpZQ== 49209
IFBpZ3M= 49210
NTY4 49211
IGFuaW1vc2l0eQ== 49212
IEF0dHJpYnV0ZXM= 49213
IFVyaWVs 49214
TVE= 49215
bXlyYQ== 49216
IEFwcGxpY2FudA== 49217
IHBzeWNoaWF0cmlzdHM= 49218
IFZpag== 49219
IEFiYnk= 49220
YWdyZWU= 49221
UHVzaA== 49222
IGtXaA== 49223
aGliYQ== 49224
IGluY2l0ZQ== 49225
IFdlYXNsZXk= 49226
IFRheGk= 49227
bWluaXN0aWM= 49228
aHlwZXI= 49229
IEZhcm4= 49230
IDYwMQ== 49231
IE5hdGlvbndpZGU= 49232
RmFrZQ== 49233
OTUy 49234
IG1haXpl 49235
IGludGVyYWN0ZWQ= 49236
IHRyYW5zaXRpb25lZA== 49237
IHBhcmFzaXRpYw== 49238
IGhhcm1vbmlj 49239
IGRlY2F5aW5n 49240
IGJhc2VsZXNz 49241
bnNpY3M= 49242
IHRyYW5zcGlyZWQ= 49243
IGFidW5kYW50bHk= 49244
IEZvcmVuc2lj 49245
IHRyZWFkbWlsbA== 49246
IEphdg== 49247
YWJhbmQ= 49248
IHNzaGQ= 49249
IGZyb250bWFu 49250
IEpha2FydGE= 49251
b2xsZXI= 49252
ZHJvcHM= 49253
IFNFUlZJQ0VT 49254
cm9tcHR1 49255
b3BoaWNhbA== 49256
aG9zcGl0YWw= 49257
YmxlZG9u 49258
NjQ1 49259
IG1pZHJhbmdl 49260
IEVWRU5U 49261
Y3VsYXRlZA== 49262
cmF3bGVk 49263
IHBlcmNoZWQ= 49264
IG92ZXJib2FyZA== 49265
IFBlZWw= 49266
IFB3cg== 49267
IENhcnRo 49268
IENPTVBMRQ== 49269
Y29l 49270
c2hhbGw= 49271
IGRldGVycmVuY2U= 49272
TUVUSE9E 49273
IEFic2VudA== 49274
TUVO 49275
IHNpbGw= 49276
IExFVkVM 49277
WW9yaw== 49278
IHNpbm5lcnM= 49279
IE9QRUM= 49280
IE51cg== 49281
IERlc2lnbnM= 49282
c2VsZWN0aW9u 49283
IHVud29ydGh5 49284
Q0hB 49285
IHN0cmVuZ3RoZW5z 49286
ODgz 49287
ZWRseQ== 49288
IHNsaWNpbmc= 49289
IG1hbG51dHJpdGlvbg== 49290
IGZpbG1tYWtpbmc= 49291
IFBvbGs= 49292
dXJhdGVk 49293
IDQyMQ== 49294
YnJlYWtlcnM= 49295
ISci 49296
IHdldGxhbmRz 49297
IERpc2NyaW1pbmF0aW9u 49298
IGFsbG93YWJsZQ== 49299
IHN0ZWVyZWQ= 49300
IFNpY2lseQ== 49301
U0FN 49302
IG11c3RhY2hl 49303
IG1pZHM= 49304
IGNsaXBwZWQ= 49305
IGNpcmN1bGF0ZQ== 49306
IGJyaXR0bGU= 49307
IEJ1aWxkaW5ncw== 49308
cmFpc2Vk 49309
IFJvdW5kdXA= 49310
IHdlYWx0aGllcg== 49311
IG92ZXJ3cml0ZQ== 49312
IG92ZXJwb3dlcmVk 49313
IEdlcnJhcmQ= 49314
c2l0ZXM= 49315
UERBVEVE 49316
IGFjdXRlbHk= 49317
IEdhbWJsZQ== 49318
IHBpbQ== 49319
IEt1cw== 49320
VHlwaWNhbGx5 49321
RGVwbG95 49322
IE1vcm9jY2Fu 49323
cG90aW9u 49324
Y29tYmU= 49325
IHZpZ2lsYW50ZQ== 49326
IDM2Mw== 49327
U3Rldw== 49328
IEJhZ2c= 49329
IHJlc2lkZWQ= 49330
IFNwbw== 49331
IHJlbW5hbnQ= 49332
IGVtcHRpbmVzcw== 49333
YnJhaW5lcg== 49334
IG91dHBhdGllbnQ= 49335
cHJpb3JpdHk= 49336
IGxlcHRpbg== 49337
IFBheXRvbg== 49338
IEdsZWFtaW5n 49339
IFNoZWQ= 49340
IFBvbG8= 49341
IE1vcm1vbmlzbQ== 49342
cmVzdHJpY3RlZA== 49343
YXJsYW5l 49344
d3g= 49345
IGNyZWF0aW5l 49346
IEFub24= 49347
IFNUVUQ= 49348
IEpVTA== 49349
IFRlZQ== 49350
NTI4 49351
MDg5 49352
IGhhdGNoZWQ= 49353
RGlzcGF0Y2g= 49354
IENvbXBvc2l0ZQ== 49355
IDQ1MQ== 49356
cHVmZg== 49357
IFhDT00= 49358
IE9ybg== 49359
IFRIQU5L 49360
RU5ERUQ= 49361
IEFzaGV2aWxsZQ== 49362
IMOc 49363
IG1hbmdv 49364
IFNsaWdodGx5 49365
d29ybGRseQ== 49366
IFdhbmRlcg== 49367
IEV4cGFuZA== 49368
IENocg== 49369
TWlzdA== 49370
IG9ydGhvZG94eQ== 49371
IFVORVNDTw== 49372
cmVnYXRl 49373
RWxzZXdoZXJl 49374
a2ll 49375
aXJsZWQ= 49376
IHRvcHBsZQ== 49377
IGFkb3B0aXZl 49378
IExlZ3M= 49379
ZHJlc3M= 49380
IFNhZ2Fu 49381
YmFyZQ== 49382
IEdsb3U= 49383
Q3J1bmNo 49384
IGhlbHBlcnM= 49385
IGNocm9uaWNhbGx5 49386
IEh1bWE= 49387
MTAwMDA= 49388
IGFjY29tbW9kYXRpbmc= 49389
5LqU 49390
IHdyaW5rbGVz 49391
IGRvZGdlZA== 49392
Zm91cnRo 49393
IHByZWNvbg== 49394
IGNvbXByZXNzb3I= 49395
IEthcmU= 49396
IGV2aWN0 49397
IFdhcndpY2s= 49398
aW1hcg== 49399
IG1vZGVybml6YXRpb24= 49400
IGJhbmR3YWdvbg== 49401
IHJlZnV0ZWQ= 49402
IG5ldHRlZA== 49403
IE5hcGxlcw== 49404
IEdlbmll 49405
cGVyb3Jz 49406
IGZpZWxkZWQ= 49407
IGRlcmU= 49408
IFBhcmFibGVz 49409
bGVlcw== 49410
IHRyb3V0 49411
YXNwZXJz 49412
IG5paGls 49413
IGhhcHBpZXN0 49414
IGZsb3BweQ== 49415
IExvZnQ= 49416
IEhlYXJk 49417
IHVuaXNvbg== 49418
IGx1Zw== 49419
IFJlZG1vbmQ= 49420
Y2xhc3NpYw== 49421
U3VwcG9ydGVycw== 49422
U0hJUA== 49423
R01U 49424
IGZ1ZWxsZWQ= 49425
55A= 49426
IGRk 49427
IEVtaW5lbQ== 49428
IDE4OTc= 49429
TllTRQ== 49430
IHNlY3JldGFyaWVz 49431
IEZJQQ== 49432
IENhbmF2ZXJhbA== 49433
RmF2b3JpdGU= 49434
IHBvbXA= 49435
IGRldGFpbmVl 49436
ZXJzaGlw 49437
YWltb24= 49438
aW91cg== 49439
IEFwZXg= 49440
IHBsYW50YXRpb25z 49441
YW1pYQ== 49442
YWNpb24= 49443
UnVzdA== 49444
IHRvd2Vk 49445
IFRydWx5 49446
NTc3 49447
IHNoZWx0ZXJlZA== 49448
cmlkZXI= 49449
V28= 49450
IGxhaXI= 49451
IEludGVsbGlnZW50 49452
aW1wcm92ZQ== 49453
bWF0aWNhbGx5 49454
IGV0aXF1ZXR0ZQ== 49455
YWRyYQ== 49456
YWxsbw== 49457
IEp1bm8= 49458
YW55dGhpbmc= 49459
IFN0cnVnZ2xl 49460
IFByZWRpY3Q= 49461
IEdyaW1lcw== 49462
IEFNRVJJQ0E= 49463
Y3R4 49464
IFNpdHVhdGlvbg== 49465
V09PRA== 49466
IHNvbHVibGU= 49467
bWVpZXI= 49468
IGludG9sZXJhYmxl 49469
YW5nZXJpbmc= 49470
IHVuaW50ZXJydXB0ZWQ= 49471
IHRvb2x0aXA= 49472
IGludGVycm9nYXRlZA== 49473
IGd1bm5lZA== 49474
IFNuZWFr 49475
5q2m 49476
IHRldGhlcg== 49477
IGNydW1ibGU= 49478
TGVucw== 49479
IGNsdXN0ZXJlZA== 49480
IFN5bA== 49481
IEhhc2Fu 49482
IGR5c3RvcGlhbg== 49483
d2FuYQ== 49484
IGpveXN0aWNr 49485
IFRoaWI= 49486
YW1tdQ== 49487
VG9tb3Jyb3c= 49488
NTQ2 49489
IG92ZXJjYW1l 49490
IG1pbmltaXplZA== 49491
Y2VwdG9y 49492
UnVubmVy 49493
RU5HVEg= 49494
IEJyZW5kYQ== 49495
IEFjaGlldmVtZW50cw== 49496
IHRvcmNoZXM= 49497
IHJhcHBvcnQ= 49498
IEludmVzdGlnYXRvcg== 49499
IEhhbmRsaW5n 49500
cmVsYXRpb24= 49501
Z3JleQ== 49502
ODE1 49503
IGtjYWw= 49504
IENvbW1hbmRz 49505
ZHE= 49506
IGN1cmxz 49507
IGJlYXJlcg== 49508
IGN5bmljaXNt 49509
aXRyaQ== 49510
IFVzZWZ1bA== 49511
QmVl 49512
RENT 49513
IGFicmFz 49514
UHJhY3Q= 49515
QklMSVRJRVM= 49516
NzEy 49517
IGRlYnVnZ2Vy 49518
IGRlYnRvcg== 49519
IExpYQ== 49520
IEtlcnM= 49521
IGV4YWNlcmJhdGU= 49522
IFN0YWN5 49523
IEJsYW5k 49524
IFNjZW5lcw== 49525
IGJyYW5jaGluZw== 49526
4paI4paI4paI4paI4paI4paI4paI4paI 49527
YXBlYWtl 49528
IHNhbHNh 49529
IG1pc2hhbmQ= 49530
IEtvbmFtaQ== 49531
IE5pYg== 49532
IGFuZWNkb3Rl 49533
IGFncmVlYWJsZQ== 49534
z4k= 49535
IE5hdGhhbmllbA== 49536
IEhlaXNtYW4= 49537
IEJld2FyZQ== 49538
IDE4ODY= 49539
c3BlY3RpdmU= 49540
Njkx 49541
NTIy 49542
IGluaGliaXRz 49543
IGhhc2hpbmc= 49544
IDE4ODk= 49545
5bCG 49546
dmljaA== 49547
UHVyZQ== 49548
IHNvbGlkbHk= 49549
IGFzcGlyaW4= 49550
aW1hcnU= 49551
IHN0cmVldGNhcg== 49552
IFVDUw== 49553
IEp1ZGQ= 49554
IGZsYXNoYmFja3M= 49555
cGlucw== 49556
IDE0NDA= 49557
IFVOSENS 49558
IFN5bXB0b21z 49559
VElU 49560
NTM4 49561
RnJh 49562
JSk7 49563
IG9veg== 49564
IGN1cmZldw== 49565
IGNhbG1lZA== 49566
IHBhcnRpY2lwYXRlcw== 49567
VGVY 49568
IG5vbnNlbnNpY2Fs 49569
IGZ1bGxiYWNr 49570
IERlTA== 49571
bW9ua2V5 49572
aGFyaQ== 49573
IG1ldGFib2xpdGVz 49574
IGxvb3RlZA== 49575
IEFMV0FZUw== 49576
IEJDQw== 49577
THQ= 49578
b2NoZXQ= 49579
Qm9uZQ== 49580
IHZldG9lZA== 49581
IGdjYw== 49582
IENMSUNL 49583
IDE4ODg= 49584
c2Fm 49585
IHN0aWZmbmVzcw== 49586
IGxvd2x5 49587
IEdlaA== 49588
dmVyc29u 49589
b3JzZXQ= 49590
IHVuZm9yZXNlZW4= 49591
IGFuZXN0aGVzaWE= 49592
IE9wdGljYWw= 49593
IHJlY29uc3RydWN0ZWQ= 49594
IFR1cA== 49595
c2hvd3M= 49596
TkVXUw== 49597
IE5ld3NwYXBlcg== 49598
IEFTQQ== 49599
dGVyYQ== 49600
TnVtYmVycw== 49601
IGluZXhwbGljYWJsZQ== 49602
15E= 49603
IGhhcmRuZXNz 49604
dW50YXJpbHk= 49605
IEFjZXI= 49606
Z3JhZGllbnQ= 49607
QVJESVM= 49608
IHdvb2RsYW5k 49609
IG1ldGFwaG9ycw== 49610
IFdlbWJsZXk= 49611
IFBhdmVs 49612
cGhpbGlz 49613
IHJld3JpdGluZw== 49614
IHBlcmNlcHR1YWw= 49615
IDEwNzA= 49616
d29ybXM= 49617
IERvd25z 49618
IHVuc3VycHJpc2luZ2x5 49619
IHRhZ2dpbmc= 49620
ZmxhbWU= 49621
IGxpdHJlcw== 49622
IGJvdW5jZXM= 49623
IEJhYmU= 49624
c2h1dA== 49625
IG92ZXJkb3Nlcw== 49626
IFNoZWlsYQ== 49627
IENoYXU= 49628
IEJsZXNz 49629
Q2FwdHVyZQ== 49630
IFNpZ25pZmljYW50 49631
IFNjaW9u 49632
IDM4OQ== 49633
IE1jSA== 49634
IFRpdGFuaXVt 49635
IE1lYWw= 49636
YW1lZGE= 49637
YWdlbnRz 49638
YWdncmVzc2l2ZQ== 49639
QmlsbHk= 49640
NzYz 49641
IFNheWluZw== 49642
REVSUg== 49643
aXRvbmU= 49644
Q29sbGlucw== 49645
Qm91bmQ= 49646
IGJvbHRlZA== 49647
IERNQ0E= 49648
OTUz 49649
IHVuaXF1ZW5lc3M= 49650
IGVwaWdlbg== 49651
dW5jaQ== 49652
YW50YW0= 49653
IHJlY2tvbmluZw== 49654
Y2hhaXJz 49655
T0dS 49656
IFNlbmVnYWw= 49657
IDE4NjI= 49658
cmVsZXZhbnQ= 49659
IMKv 49660
IHBoYXJtYWNpZXM= 49661
IEdlcmFs 49662
dmllcg== 49663
WWFu 49664
T1JQRw== 49665
IHJhYmlk 49666
YmVuZGluZw== 49667
IFVOSVRFRA== 49668
IDQ2NQ== 49669
QXNzZW1ibHk= 49670
IHdlZXA= 49671
IGJlaGVzdA== 49672
IE1vdGhlcnM= 49673
IEphY2U= 49674
aGlk 49675
IHdoaXJsd2luZA== 49676
IFVOSVZFUlM= 49677
IHV0b3BpYW4= 49678
IGtpZG5hcA== 49679
UGhpbGlwcA== 49680
S2lu 49681
ODkz 49682
IGxpdmVzdHJlYW0= 49683
IE1JU1M= 49684
IHN1YnZlcnNpdmU= 49685
IFRlY2huaXF1ZXM= 49686
IEpVU1RJQ0U= 49687
IEJBU0U= 49688
IDM4Nw== 49689
IGFzc2FpbGFudHM= 49690
IEhhcmRjb3Jl 49691
IHNwcmlua2xlZA== 49692
IFBzZQ== 49693
6Zo= 49694
cHJpbnRlZA== 49695
IEhhdQ== 49696
T1JHRQ== 49697
IFRPVVI= 49698
IGxhY2Vk 49699
IGl0Y2g= 49700
R2l2aW5n 49701
IHBvcnRlZA== 49702
Nzgx 49703
Ly8vLy8vLy8vLy8vLy8vLy8vLy8vLy8vLy8vLy8vLy8= 49704
YnJlZWRpbmc= 49705
IGxvZ2dlcg== 49706
IEhPTA== 49707
aW5uaWU= 49708
Rmlyc3RseQ== 49709
IGVtYnJ5b25pYw== 49710
IGRlbGVnYXRlZA== 49711
cGFp 49712
T0lM 49713
IGNlbnRyYWxseQ== 49714
IFJ4 49715
IFNjb3V0aW5n 49716
RHV0Y2g= 49717
IGhlcmVkaXRhcnk= 49718
IENydWlzZXI= 49719
c2F0 49720
NTI5 49721
IE1hcnJpb3R0 49722
b3RoZXJtYWw= 49723
IHByb2hpYml0aW9ucw== 49724
RWFybg== 49725
IFN0YWI= 49726
IENvbGxlZ2Vz 49727
IEJlbGllZg== 49728
c3RyZXRjaGVk 49729
IExI 49730
IEVudGl0eUl0ZW0= 49731
Q0lB 49732
IHVucmVt 49733
IGxhdXJlYXRl 49734
IGRlbm9taW5hdGlvbnM= 49735
c3VtbWFyeQ== 49736
aGxlcg== 49737
U3BlY3Q= 49738
IEtsYXVz 49739
IEJlYW5z 49740
IGluc3Vy 49741
IFBBWA== 49742
IGZpZWxkZXI= 49743
IFZldA== 49744
IFNwYXJyb3c= 49745
emll 49746
IFNR 49747
IE1vbmRheXM= 49748
IE9mZmxpbmU= 49749
IExlcm5lcg== 49750
IEV4dGVuc2lvbnM= 49751
SXJlbGFuZA== 49752
IHBhdHJvbmFnZQ== 49753
IGNvbnRyYXN0ZWQ= 49754
IE1hbmlh 49755
aGlydA== 49756
TW9zY293 49757
IGNvbmRlbW5z 49758
IEFuZ2U= 49759
IGNvbXBvc2luZw== 49760
IFBlcGU= 49761
IFBhZGRvY2s= 49762
IGhldGVyb2dlbmVpdHk= 49763
IGlkZW9sb2dpY2FsbHk= 49764
IGZpc2hlcw== 49765
IGN1cnNpbmc= 49766
IFJ1dGhlcmZvcmQ= 49767
IEZsb2F0aW5n 49768
IEFtZWxpYQ== 49769
VGVh 49770
U3lub3BzaXM= 49771
IHN0dW50cw== 49772
IGJlYWQ= 49773
IHN0b2NraW5n 49774
IE1JTEw= 49775
b2Jvb2s= 49776
bWFzc2l2ZQ== 49777
XDw= 49778
IGh1bXA= 49779
IFByZWZlcmVuY2Vz 49780
RW5naW5lRGVidWc= 49781
Z2Vpc3Q= 49782
IE5pZXRv 49783
b21ldmVy 49784
aXNoeQ== 49785
ZXZhbHVhdGU= 49786
Y29sb25pYWw= 49787
QWx0ZXJuYXRpdmU= 49788
IEdvUHJv 49789
IFZvcnRleA== 49790
IE5FVFdPUks= 49791
YW5za3k= 49792
U2VjdXJl 49793
IFRocnVzdA== 49794
U25ha2U= 49795
IHBhcmNlbHM= 49796
IHNhbXVyYWk= 49797
IGFjdHJlc3Nlcw== 49798
TmFw 49799
TUY= 49800
aWZlcmF0aW9u 49801
QmVlcg== 49802
NTIz 49803
IElseQ== 49804
b2ludG1lbnQ= 49805
UGluZw== 49806
IHN0cmlwZWQ= 49807
IE1lbGxvbg== 49808
b3NzZXNzaW9u 49809
IG5ldXRyb24= 49810
ZW5kaXVt 49811
IGFwaA== 49812
IEZsYXZvcmluZw== 49813
IDM4Mw== 49814
IHJlc3BvbnNpdmVuZXNz 49815
IEppbmRhbA== 49816
IEhpdGNoY29jaw== 49817
RGVudmVy 49818
IERSQUdPTg== 49819
c21hbnNoaXA= 49820
IER1cGw= 49821
IHNseQ== 49822
IHdlYmNhbQ== 49823
IFR3YWlu 49824
IERhcmxpbmc= 49825
aWxpYXRl 49826
Y29uc3VtZXI= 49827
RElU 49828
IG5hbWVzYWtl 49829
IHVub3J0aG9kb3g= 49830
IGZ1bmVy 49831
IFBMb1M= 49832
IENPTlRST0w= 49833
b3p5Zw== 49834
b2dsb2Jpbg== 49835
RkFDRQ== 49836
RVJH 49837
IERpYQ== 49838
IEZpZXN0YQ== 49839
Y2VsZQ== 49840
MDM0 49841
IGVuY2xhdmU= 49842
4pas4pas 49843
b25lbWVudA== 49844
YWxpc3Q= 49845
TWFuZA== 49846
IGhvbWVncm93bg== 49847
IEZhbmN5 49848
IGNvbmNlcHRpb25z 49849
IENvbnRhaW5z 49850
dXJlZW4= 49851
IHJlaXRlcmF0ZQ== 49852
IG1lYWdlcg== 49853
IGluc3RhbGxtZW50cw== 49854
U3Bhd24= 49855
NjI3 49856
IHBob3RvYw== 49857
IENhYnJlcmE= 49858
IFJvc2VudGhhbA== 49859
IExhbnNpbmc= 49860
aXNuZXI= 49861
IGludmVzdHM= 49862
IFVGT3M= 49863
RVhQ 49864
SGFyZHdhcmU= 49865
IHRyYWdpY2FsbHk= 49866
IGNvbmNlZGVz 49867
aWVmdA== 49868
Y2hhbQ== 49869
Ym9yZ2g= 49870
IFNjaHI= 49871
IE1lbGFuaWU= 49872
IEhveQ== 49873
IHZpc2l0YXRpb24= 49874
IGlkaW9zeW5jcg== 49875
IGZyYWN0aW9ucw== 49876
IGZvcmVza2lu 49877
b2Jvcw== 49878
IHBvYWNoaW5n 49879
IFZJRVc= 49880
IHN0aW11bGF0ZXM= 49881
IEdvcms= 49882
Y2Fub24= 49883
TUlD 49884
IE5lbWVzaXM= 49885
IEluZHJh 49886
IERNVg== 49887
IDUyOQ== 49888
IGluc3BlY3Rpbmc= 49889
IGdyYW5kbWE= 49890
IFdoZWRvbg== 49891
IFNoYW50 49892
IFB1cmc= 49893
aWthbg== 49894
IFRlZw== 49895
IENMUg== 49896
emFj 49897
VmljdG9yaWE= 49898
IFZlcmlmeQ== 49899
aW9uaWNz 49900
IHBhcnR5aW5n 49901
IE1vdQ== 49902
Y29sb3Vy 49903
IHRlc3RpbW9uaWVz 49904
bGF0aW9ucw== 49905
IHByZXNzdXJpbmc= 49906
aGlybw== 49907
YWNlcnM= 49908
IGZpZA== 49909
YW5nbGVy 49910
IENTSQ== 49911
IGhlcmVhZnRlcg== 49912
IGRpc3NpZGVudHM= 49913
cmVwb3J0aW5n 49914
aXBoYW55 49915
Y2hldg== 49916
IHNvbGl0dWRl 49917
IGxvYmU= 49918
IGluZGlz 49919
IGNyZWRlbnRpYWw= 49920
cmVjZW50 49921
YWR1bHQ= 49922
IE5pcnZhbmE= 49923
IEZyYW5jaGlzZQ== 49924
TGF5ZXI= 49925
SHlw 49926
IEJlcmtzaGlyZQ== 49927
IHdpbGxz 49928
dGlm 49929
IHRvdGVt 49930
IEp1ZGFo 49931
cmVwYWly 49932
SW5zdGFudA== 49933
NTQ4 49934
IGVtYmFzc2llcw== 49935
IGJvdHRsZW5lY2s= 49936
IGJvdW50 49937
IHR5cGV3 49938
IEFsdmlu 49939
amluZw== 49940
aW1pbGFy 49941
UnVzaA== 49942
IGJyaW0= 49943
IEhFTFA= 49944
QWlt 49945
XSc= 49946
IHBhc3NpdmVseQ== 49947
IGJvdW5kZWQ= 49948
IFJhdGVk 49949
IGNyaW1pbmFsaXR5 49950
IGJpb21hcms= 49951
IGRpc3BhdGNoZXI= 49952
IFRvd2FyZHM= 49953
ICsrKw== 49954
cmlnaHRlb3Vz 49955
ZnJvZw== 49956
IFBhbmM= 49957
Q2FydGVy 49958
MDMy 49959
5qmf 49960
IHVsdHJhdmlvbGV0 49961
IExpY2Vuc2Vk 49962
IFRhdGE= 49963
IEJsZXNzaW5n 49964
IEdBTQ== 49965
IGNoZW1pY2FsbHk= 49966
IFNlYWY= 49967
IFJFTEU= 49968
IE1lcmNlbmFyeQ== 49969
Y2FwaXRhbGlzdA== 49970
IGZvcm11bGF0aW9ucw== 49971
IGFubmloaWxhdGlvbg== 49972
IFZlcmI= 49973
IEFyZ29u 49974
IHVubG9hZGVk 49975
IG1vcnBoZWQ= 49976
IGNvbnF1ZXJpbmc= 49977
YmFja2Vy 49978
SUVMRA== 49979
IHRoZWZ0cw== 49980
IGZyb250cnVubmVy 49981
IFJveWFsZQ== 49982
IEZ1bmRhbWVudGFs 49983
ZWxpZ2h0 49984
Q2hpcA== 49985
bmVjZXNzYXJ5 49986
YXlu 49987
IFNsaXA= 49988
IDQ0OA== 49989
Y2VybmVk 49990
UGF1c2U= 49991
IHNob2NraW5nbHk= 49992
IEFCVg== 49993
IGNvbXBvc3VyZQ== 49994
NzMz 49995
IE1vdG9yc3BvcnQ= 49996
YWhpbWU= 49997
TXVycmF5 49998
TWFjaA== 49999
IGdyaWRz 50000
IGRlYmlhbg== 50001
IGZ1cnRoZXJtb3Jl 50002
IGRleHRlcml0eQ== 50003
IENvbGxlY3Rpb25z 50004
b3Nsb3Y= 50005
aWxhZ2U= 50006
Ymo= 50007
IE1vbnRlbmVn 50008
IHN0cnV0Q29ubmVjdG9y 50009
IG1hc3NhY3Jlcw== 50010
IGJyaWVmcw== 50011
ZmV0Y2hlZA== 50012
dXZpYW4= 50013
b2xpdGlvbg== 50014
RmFpbHVyZQ== 50015
ZW1vbmlj 50016
IGZsYXJlZA== 50017
IGNsYWltYW50 50018
IGN1cmVz 50019
IGdpdmVhd2F5cw== 50020
IFN1YnN0YW5jZQ== 50021
YWxpb25z 50022
IGNyaW5nZQ== 50023
IEt1bA== 50024
IGFyaXN0b2NyYWN5 50025
IFVsc3Rlcg== 50026
b2xhdGVk 50027
aG91c2luZw== 50028
IE1JUw== 50029
IGdsYXJlZA== 50030
IFdpbGhlbG0= 50031
bmVlZHM= 50032
bGFtYmRh 50033
YnVpbGRlcnM= 50034
IFZJUw== 50035
IHJhZGlhdG9y 50036
IEdob3N0YnVzdGVycw== 50037
IDQzNg== 50038
YWN0dWFs 50039
IGhlcmRz 50040
w6dh 50041
d2F0Y2hpbmc= 50042
IGNvdW50ZXJpbmc= 50043
Q2hhcmdl 50044
IGNoYXJyZWQ= 50045
IHdhcmhlYWRz 50046
IGlvZGluZQ== 50047
IE1hY3k= 50048
MDQx 50049
IGRlcGFydHVyZXM= 50050
IFNpbnM= 50051
IGR5ZWQ= 50052
IENvbmNlcHRz 50053
Z2Fkbw== 50054
NzEz 50055
IHF1b3RhdGlvbnM= 50056
IGdpc3Q= 50057
IENocmlzdHk= 50058
IGFudGlnZW4= 50059
IEhlbXA= 50060
IERyYXdu 50061
IEJhcmc= 50062
ZXp2b3Vz 50063
IHBhdGVybml0eQ== 50064
IGFyZHU= 50065
IEFuY2hvcmFnZQ== 50066
IFJpaw== 50067
IG92ZXJsb2FkZWQ= 50068
IFVzZXJuYW1l 50069
IFRhbW15 50070
IE5hdQ== 50071
IENlbGx1bGFy 50072
IHdhbmluZw== 50073
IHJvZGVudA== 50074
IFdvcmNlc3Rlcg== 50075
aWx0cw== 50076
IFRhZA== 50077
IGR3ZWxsaW5ncw== 50078
IGJ1bGxpc2g= 50079
NDMx 50080
IHJldGFsaWF0ZQ== 50081
IG1pZ3JhaW5l 50082
IENoZXZyb24= 50083
Q0hFQ0s= 50084
IGRvbmtleQ== 50085
Y3JpbQ== 50086
U1BB 50087
IEFuYWxvZw== 50088
IG1hcnF1ZWU= 50089
IEhhYXM= 50090
Qmly 50091
IEdERFI= 50092
IERvd25sb2Fkcw== 50093
IHdpbGxwb3dlcg== 50094
IEZvcnRo 50095
IFJlY29yZGVk 50096
IGltcG9zc2liaWxpdHk= 50097
IExvZ2dlZA== 50098
IEZyYW5rcw== 50099
IFJhdHQ= 50100
aW5pdGlvbnM= 50101
IGNsZWFuZXJz 50102
IHNvcmVseQ== 50103
IGZsaWNrZXJpbmc= 50104
IEV4YW1pbmF0aW9u 50105
Y2F0Y2hpbmc= 50106
YWxsb3dlZW4= 50107
TXNn 50108
IGR1bm5v 50109
RmE= 50110
IGR5c3Bo 50111
Y3Jhenk= 50112
LicnLg== 50113
IG1haW5saW5l 50114
IGNz 50115
IHB0cg== 50116
IFdhbGx5 50117
aWd1bg== 50118
OTUx 50119
IEJpZ2Zvb3Q= 50120
ZmlnaHRz 50121
IHJldHJpZXZpbmc= 50122
SnI= 50123
IGR1cGxpY2F0aW9u 50124
IEV4cGxhbg== 50125
IHJlbGF0aW9uYWw= 50126
IHF1YWludA== 50127
IGJpc2N1aXRz 50128
IGFkbw== 50129
IHNodWRkZXI= 50130
IGFudGlkb3Rl 50131
Ymxvb2RlZA== 50132
a3No 50133
IHNhdWNlcw== 50134
IHJlaW52ZXN0 50135
IGRpc3BlbnNhcnk= 50136
IERpdmVy 50137
IDkwMDA= 50138
c3R1ZGVudA== 50139
IGluc2VwYXI= 50140
ZXNjYXA= 50141
IHRvZGRsZXJz 50142
IEdQSU8= 50143
IEFzc2lnbm1lbnQ= 50144
aGVhZGVycw== 50145
IGxhY2tsdXN0ZXI= 50146
IGFiYWNr 50147
OTU2 50148
IHRvb2xiYXI= 50149
NzQ1 50150
IG91c3Q= 50151
IGNvbnRlbXBsYXRpb24= 50152
IFBSRVNJREVOVA== 50153
IDQ1OA== 50154
PT09PT09 50155
IGd1YXJhbnRlZWluZw== 50156
IEhlaXN0 50157
IENhbm5lcw== 50158
mb0= 50159
IGNvbGxhYm9yYXRvcg== 50160
IEFtcA== 50161
IGdvdQ== 50162
IFNIQUxM 50163
c3Rvcmllcw== 50164
Nzgz 50165
IG1vYmlsaXplZA== 50166
IGJyb29k 50167
IExV 50168
IPCfkQ== 50169
IHJlZmlu 50170
IEFudGhyb3BvbG9neQ== 50171
dmluZA== 50172
aWxsaQ== 50173
IHdhcnJhbnRpZXM= 50174
IEJhYmVs 50175
IHN3YXRo 50176
IGNhY2hlcw== 50177
IGFudGFnb25pc3Rz 50178
YXJ0aWZhY3Rz 50179
IGhvdGx5 50180
IFN0YXJ0cw== 50181
IEfDtg== 50182
emFn 50183
ISEhISE= 50184
IHNjb3VyZ2U= 50185
IGNvbnNwaXJpbmc= 50186
cnVpdHM= 50187
cmV2ZXJzZQ== 50188
IFNoZWVu 50189
IEplc3VpdA== 50190
IEdpb3Zhbm5p 50191
YWRpZXM= 50192
IGJ1dHRvY2tz 50193
ZWFyY2hlcg== 50194
YWNhbg== 50195
IHZvbGxleWJhbGw= 50196
IHNocm91ZGVk 50197
IHNjb3JlYm9hcmQ= 50198
YmF0cw== 50199
IElQTQ== 50200
IGFzc2Vz 50201
IGRlcmVndWxhdGlvbg== 50202
IFRlbGVncmFt 50203
IFJlYm9vdA== 50204
IDcwMDA= 50205
IENhbmFyeQ== 50206
IGtlcm5lbHM= 50207
IEZyYW7Dp29pcw== 50208
IER1ZmY= 50209
IFBvbg== 50210
IExlaWNh 50211
IEdhcm1pbg== 50212
IG9ycGhhbnM= 50213
IENsYXVkaWE= 50214
IGNhbGVuZGFycw== 50215
IExlaWxhbg== 50216
ZW50bw== 50217
Um9ja2V0 50218
IGJydW5jaA== 50219
IEhhd2tpbmc= 50220
YWluZXJz 50221
IHNlbnNpYmlsaXRpZXM= 50222
IGtX 50223
IEthbmQ= 50224
IHJlY2xhaW1lZA== 50225
IGludGVyZXN0aW5nbHk= 50226
16k= 50227
cm9teQ== 50228
Sk0= 50229
IEVuaGFuY2VtZW50 50230
YnVzaA== 50231
U2tpcA== 50232
IHJhcHBlcnM= 50233
IGdhemluZw== 50234
cGVkaWE= 50235
YXRobG9u 50236
UmV2b2x1dGlvbg== 50237
IHNuaXBlcnM= 50238
IHJldmVydGVk 50239
IGNvbmdsb21lcmF0ZQ== 50240
VGVycnk= 50241
Nzk0 50242
IGhhcnNoZXI= 50243
IGRlc29sYXRl 50244
IEhpdG1hbg== 50245
Q29tbWlzc2lvbg== 50246
ICgv 50247
4oCmLiI= 50248
Q29tcGFy 50249
IGFtcGxpZmljYXRpb24= 50250
b21pbmF0ZWQ= 50251
IHJlZ3Jlc3M= 50252
IENvbGxpZGVy 50253
IGluZm9ybWFudHM= 50254
IGdhemVk 50255

>>>> whisper/mlx_whisper/audio.py
# Copyright © 2023 Apple Inc.

import os
from functools import lru_cache
from subprocess import CalledProcessError, run
from typing import Optional, Union

import mlx.core as mx
import numpy as np

# hard-coded audio hyperparameters
SAMPLE_RATE = 16000
N_FFT = 400
HOP_LENGTH = 160
CHUNK_LENGTH = 30
N_SAMPLES = CHUNK_LENGTH * SAMPLE_RATE  # 480000 samples in a 30-second chunk
N_FRAMES = N_SAMPLES // HOP_LENGTH  # 3000 frames in a mel spectrogram input

N_SAMPLES_PER_TOKEN = HOP_LENGTH * 2  # the initial convolutions has stride 2
FRAMES_PER_SECOND = SAMPLE_RATE // HOP_LENGTH  # 10ms per audio frame
TOKENS_PER_SECOND = SAMPLE_RATE // N_SAMPLES_PER_TOKEN  # 20ms per audio token


def load_audio(file: str = Optional[str], sr: int = SAMPLE_RATE, from_stdin=False):
    """
    Open an audio file and read as mono waveform, resampling as necessary

    Parameters
    ----------
    file: str
        The audio file to open

    sr: int
        The sample rate to resample the audio if necessary

    Returns
    -------
    A NumPy array containing the audio waveform, in float32 dtype.
    """

    # This launches a subprocess to decode audio while down-mixing
    # and resampling as necessary. Requires the ffmpeg CLI in PATH.
    if from_stdin:
        cmd = ["ffmpeg", "-i", "pipe:0"]
    else:
        cmd = ["ffmpeg", "-nostdin", "-i", file]

    # fmt: off
    cmd.extend([
        "-threads", "0",
        "-f", "s16le",
        "-ac", "1",
        "-acodec", "pcm_s16le",
        "-ar", str(sr),
        "-"
    ])
    # fmt: on
    try:
        out = run(cmd, capture_output=True, check=True).stdout
    except CalledProcessError as e:
        raise RuntimeError(f"Failed to load audio: {e.stderr.decode()}") from e

    return mx.array(np.frombuffer(out, np.int16)).flatten().astype(mx.float32) / 32768.0


def pad_or_trim(array, length: int = N_SAMPLES, *, axis: int = -1):
    """
    Pad or trim the audio array to N_SAMPLES, as expected by the encoder.
    """
    if array.shape[axis] > length:
        sl = [slice(None)] * array.ndim
        sl[axis] = slice(0, length)
        array = array[tuple(sl)]

    if array.shape[axis] < length:
        pad_widths = [(0, 0)] * array.ndim
        pad_widths[axis] = (0, length - array.shape[axis])
        array = mx.pad(array, pad_widths)

    return array


@lru_cache(maxsize=None)
def mel_filters(n_mels: int) -> mx.array:
    """
    load the mel filterbank matrix for projecting STFT into a Mel spectrogram.
    Allows decoupling librosa dependency; saved using:

        np.savez_compressed(
            "mel_filters.npz",
            mel_80=librosa.filters.mel(sr=16000, n_fft=400, n_mels=80),
            mel_128=librosa.filters.mel(sr=16000, n_fft=400, n_mels=128),
        )
    """
    assert n_mels in {80, 128}, f"Unsupported n_mels: {n_mels}"

    filename = os.path.join(os.path.dirname(__file__), "assets", "mel_filters.npz")
    return mx.load(filename)[f"mel_{n_mels}"]


@lru_cache(maxsize=None)
def hanning(size):
    return mx.array(np.hanning(size + 1)[:-1])


def stft(x, window, nperseg=256, noverlap=None, nfft=None, axis=-1, pad_mode="reflect"):
    if nfft is None:
        nfft = nperseg
    if noverlap is None:
        noverlap = nfft // 4

    def _pad(x, padding, pad_mode="constant"):
        if pad_mode == "constant":
            return mx.pad(x, [(padding, padding)])
        elif pad_mode == "reflect":
            prefix = x[1 : padding + 1][::-1]
            suffix = x[-(padding + 1) : -1][::-1]
            return mx.concatenate([prefix, x, suffix])
        else:
            raise ValueError(f"Invalid pad_mode {pad_mode}")

    padding = nperseg // 2
    x = _pad(x, padding, pad_mode)

    strides = [noverlap, 1]
    t = (x.size - nperseg + noverlap) // noverlap
    shape = [t, nfft]
    x = mx.as_strided(x, shape=shape, strides=strides)
    return mx.fft.rfft(x * window)


def log_mel_spectrogram(
    audio: Union[str, np.ndarray],
    n_mels: int = 80,
    padding: int = 0,
):
    """
    Compute the log-Mel spectrogram of

    Parameters
    ----------
    audio: Union[str, np.ndarray, mx.array], shape = (*)
        The path to audio or either a NumPy or mlx array containing the audio waveform in 16 kHz

    n_mels: int
        The number of Mel-frequency filters, only 80 is supported

    padding: int
        Number of zero samples to pad to the right

    Returns
    -------
    mx.array, shape = (80, n_frames)
        An  array that contains the Mel spectrogram
    """
    if isinstance(audio, str):
        audio = load_audio(audio)
    elif not isinstance(audio, mx.array):
        audio = mx.array(audio)

    if padding > 0:
        audio = mx.pad(audio, (0, padding))
    window = hanning(N_FFT)
    freqs = stft(audio, window, nperseg=N_FFT, noverlap=HOP_LENGTH)
    magnitudes = freqs[:-1, :].abs().square()

    filters = mel_filters(n_mels)
    mel_spec = magnitudes @ filters.T

    log_spec = mx.maximum(mel_spec, 1e-10).log10()
    log_spec = mx.maximum(log_spec, log_spec.max() - 8.0)
    log_spec = (log_spec + 4.0) / 4.0
    return log_spec

>>>> whisper/mlx_whisper/transcribe.py
# Copyright © 2023 Apple Inc.

import sys
import warnings
from typing import List, Optional, Tuple, Union

import mlx.core as mx
import numpy as np
import tqdm

from .audio import (
    FRAMES_PER_SECOND,
    HOP_LENGTH,
    N_FRAMES,
    N_SAMPLES,
    SAMPLE_RATE,
    log_mel_spectrogram,
    pad_or_trim,
)
from .decoding import DecodingOptions, DecodingResult
from .load_models import load_model
from .timing import add_word_timestamps
from .tokenizer import LANGUAGES, get_tokenizer


def _format_timestamp(seconds: float):
    assert seconds >= 0, "non-negative timestamp expected"
    milliseconds = round(seconds * 1000.0)

    hours = milliseconds // 3_600_000
    milliseconds -= hours * 3_600_000

    minutes = milliseconds // 60_000
    milliseconds -= minutes * 60_000

    seconds = milliseconds // 1_000
    milliseconds -= seconds * 1_000

    hours_marker = f"{hours:02d}:" if hours > 0 else ""
    return f"{hours_marker}{minutes:02d}:{seconds:02d}.{milliseconds:03d}"


def _get_end(segments: List[dict]) -> Optional[float]:
    return next(
        (w["end"] for s in reversed(segments) for w in reversed(s["words"])),
        segments[-1]["end"] if segments else None,
    )


class ModelHolder:
    model = None
    model_path = None

    @classmethod
    def get_model(cls, model_path: str, dtype: mx.Dtype):
        if cls.model is None or model_path != cls.model_path:
            cls.model = load_model(model_path, dtype=dtype)
            cls.model_path = model_path
        return cls.model


def transcribe(
    audio: Union[str, np.ndarray, mx.array],
    *,
    path_or_hf_repo: str = "mlx-community/whisper-tiny",
    verbose: Optional[bool] = None,
    temperature: Union[float, Tuple[float, ...]] = (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),
    compression_ratio_threshold: Optional[float] = 2.4,
    logprob_threshold: Optional[float] = -1.0,
    no_speech_threshold: Optional[float] = 0.6,
    condition_on_previous_text: bool = True,
    initial_prompt: Optional[str] = None,
    word_timestamps: bool = False,
    prepend_punctuations: str = "\"'“¿([{-",
    append_punctuations: str = "\"'.。,，!！?？:：”)]}、",
    clip_timestamps: Union[str, List[float]] = "0",
    hallucination_silence_threshold: Optional[float] = None,
    **decode_options,
):
    """
    Transcribe an audio file using Whisper

    Parameters
    ----------
    audio: Union[str, np.ndarray, mx.array]
        The path to the audio file to open, or the audio waveform

    path_or_hf_repo: str
        The localpath to the Whisper model or HF Hub repo with the MLX converted weights.

    verbose: bool
        Whether to display the text being decoded to the console. If True, displays all the details,
        If False, displays minimal details. If None, does not display anything

    temperature: Union[float, Tuple[float, ...]]
        Temperature for sampling. It can be a tuple of temperatures, which will be successively used
        upon failures according to either `compression_ratio_threshold` or `logprob_threshold`.

    compression_ratio_threshold: float
        If the gzip compression ratio is above this value, treat as failed

    logprob_threshold: float
        If the average log probability over sampled tokens is below this value, treat as failed

    no_speech_threshold: float
        If the no_speech probability is higher than this value AND the average log probability
        over sampled tokens is below `logprob_threshold`, consider the segment as silent

    condition_on_previous_text: bool
        if True, the previous output of the model is provided as a prompt for the next window;
        disabling may make the text inconsistent across windows, but the model becomes less prone to
        getting stuck in a failure loop, such as repetition looping or timestamps going out of sync.

    word_timestamps: bool
        Extract word-level timestamps using the cross-attention pattern and dynamic time warping,
        and include the timestamps for each word in each segment.

    prepend_punctuations: str
        If word_timestamps is True, merge these punctuation symbols with the next word

    append_punctuations: str
        If word_timestamps is True, merge these punctuation symbols with the previous word

    initial_prompt: Optional[str]
        Optional text to provide as a prompt for the first window. This can be used to provide, or
        "prompt-engineer" a context for transcription, e.g. custom vocabularies or proper nouns
        to make it more likely to predict those word correctly.

    decode_options: dict
        Keyword arguments to construct `DecodingOptions` instances

    clip_timestamps: Union[str, List[float]]
        Comma-separated list start,end,start,end,... timestamps (in seconds) of clips to process.
        The last end timestamp defaults to the end of the file.

    hallucination_silence_threshold: Optional[float]
        When word_timestamps is True, skip silent periods longer than this threshold (in seconds)
        when a possible hallucination is detected

    Returns
    -------
    A dictionary containing the resulting text ("text") and segment-level details ("segments"), and
    the spoken language ("language"), which is detected when `decode_options["language"]` is None.
    """

    dtype = mx.float16 if decode_options.get("fp16", True) else mx.float32
    model = ModelHolder.get_model(path_or_hf_repo, dtype)

    # Pad 30-seconds of silence to the input audio, for slicing
    mel = log_mel_spectrogram(audio, n_mels=model.dims.n_mels, padding=N_SAMPLES)
    content_frames = mel.shape[-2] - N_FRAMES
    content_duration = float(content_frames * HOP_LENGTH / SAMPLE_RATE)

    if verbose:
        system_encoding = sys.getdefaultencoding()
        if system_encoding != "utf-8":
            make_safe = lambda x: x.encode(system_encoding, errors="replace").decode(
                system_encoding
            )
        else:
            make_safe = lambda x: x

    if decode_options.get("language", None) is None:
        if not model.is_multilingual:
            decode_options["language"] = "en"
        else:
            if verbose:
                print(
                    "Detecting language using up to the first 30 seconds. "
                    "Use the `language` decoding option to specify the language"
                )
            mel_segment = pad_or_trim(mel, N_FRAMES, axis=-2).astype(dtype)
            _, probs = model.detect_language(mel_segment)
            decode_options["language"] = max(probs, key=probs.get)
            if verbose is not None:
                print(
                    f"Detected language: {LANGUAGES[decode_options['language']].title()}"
                )

    language: str = decode_options["language"]
    task: str = decode_options.get("task", "transcribe")
    tokenizer = get_tokenizer(
        model.is_multilingual,
        num_languages=model.num_languages,
        language=language,
        task=task,
    )

    if isinstance(clip_timestamps, str):
        clip_timestamps = [
            float(ts) for ts in (clip_timestamps.split(",") if clip_timestamps else [])
        ]
    seek_points: List[int] = [round(ts * FRAMES_PER_SECOND) for ts in clip_timestamps]
    if len(seek_points) == 0:
        seek_points.append(0)
    if len(seek_points) % 2 == 1:
        seek_points.append(content_frames)
    else:
        seek_points[-1] = min(content_frames, seek_points[-1])
    seek_clips: List[Tuple[int, int]] = list(zip(seek_points[::2], seek_points[1::2]))

    punctuation = "\"'“¿([{-\"'.。,，!！?？:：”)]}、"

    if word_timestamps and task == "translate":
        warnings.warn("Word-level timestamps on translations may not be reliable.")

    def decode_with_fallback(segment: mx.array) -> DecodingResult:
        temperatures = (
            [temperature] if isinstance(temperature, (int, float)) else temperature
        )
        decode_result = None

        for t in temperatures:
            kwargs = {**decode_options}
            if t > 0:
                # disable beam_size and patience when t > 0
                kwargs.pop("beam_size", None)
                kwargs.pop("patience", None)
            else:
                # disable best_of when t == 0
                kwargs.pop("best_of", None)

            options = DecodingOptions(**kwargs, temperature=t)
            decode_result = model.decode(segment, options)

            needs_fallback = False
            if (
                compression_ratio_threshold is not None
                and decode_result.compression_ratio > compression_ratio_threshold
            ):
                needs_fallback = True  # too repetitive
            if (
                logprob_threshold is not None
                and decode_result.avg_logprob < logprob_threshold
            ):
                needs_fallback = True  # average log probability is too low
            if (
                no_speech_threshold is not None
                and decode_result.no_speech_prob > no_speech_threshold
            ):
                needs_fallback = False  # silence
            if not needs_fallback:
                break

        return decode_result

    clip_idx = 0
    seek = seek_clips[clip_idx][0]
    input_stride = N_FRAMES // model.dims.n_audio_ctx  # mel frames per output token: 2
    time_precision = (
        input_stride * HOP_LENGTH / SAMPLE_RATE
    )  # time per output token: 0.02 (seconds)
    all_tokens = []
    all_segments = []
    prompt_reset_since = 0

    if initial_prompt is not None:
        initial_prompt_tokens = tokenizer.encode(" " + initial_prompt.strip())
        all_tokens.extend(initial_prompt_tokens)
    else:
        initial_prompt_tokens = []

    def new_segment(
        *, start: float, end: float, tokens: mx.array, result: DecodingResult
    ):
        tokens = tokens.tolist()
        text_tokens = [token for token in tokens if token < tokenizer.eot]
        return {
            "seek": seek,
            "start": start,
            "end": end,
            "text": tokenizer.decode(text_tokens),
            "tokens": tokens,
            "temperature": result.temperature,
            "avg_logprob": result.avg_logprob,
            "compression_ratio": result.compression_ratio,
            "no_speech_prob": result.no_speech_prob,
        }

    # show the progress bar when verbose is False (if True, transcribed text will be printed)
    with tqdm.tqdm(
        total=content_frames, unit="frames", disable=verbose is not False
    ) as pbar:
        last_speech_timestamp = 0.0
        for seek_clip_start, seek_clip_end in seek_clips:
            while seek < seek_clip_end:
                time_offset = float(seek * HOP_LENGTH / SAMPLE_RATE)
                window_end_time = float((seek + N_FRAMES) * HOP_LENGTH / SAMPLE_RATE)
                segment_size = min(
                    N_FRAMES, content_frames - seek, seek_clip_end - seek
                )
                mel_segment = mel[seek : seek + segment_size]
                segment_duration = segment_size * HOP_LENGTH / SAMPLE_RATE
                mel_segment = pad_or_trim(mel_segment, N_FRAMES, axis=-2).astype(dtype)

                decode_options["prompt"] = all_tokens[prompt_reset_since:]
                result: DecodingResult = decode_with_fallback(mel_segment)

                tokens = np.array(result.tokens)

                if no_speech_threshold is not None:
                    # no voice activity check
                    should_skip = result.no_speech_prob > no_speech_threshold
                    if (
                        logprob_threshold is not None
                        and result.avg_logprob > logprob_threshold
                    ):
                        # don't skip if the logprob is high enough, despite the no_speech_prob
                        should_skip = False

                    if should_skip:
                        seek += (
                            segment_size  # fast-forward to the next segment boundary
                        )
                        continue

                previous_seek = seek
                current_segments = []

                # anomalous words are very long/short/improbable
                def word_anomaly_score(word: dict) -> float:
                    probability = word.get("probability", 0.0)
                    duration = word["end"] - word["start"]
                    score = 0.0
                    if probability < 0.15:
                        score += 1.0
                    if duration < 0.133:
                        score += (0.133 - duration) * 15
                    if duration > 2.0:
                        score += duration - 2.0
                    return score

                def is_segment_anomaly(segment: Optional[dict]) -> bool:
                    if segment is None or not segment["words"]:
                        return False
                    words = [
                        w for w in segment["words"] if w["word"] not in punctuation
                    ]
                    words = words[:8]
                    score = sum(word_anomaly_score(w) for w in words)
                    return score >= 3 or score + 0.01 >= len(words)

                def next_words_segment(segments: List[dict]) -> Optional[dict]:
                    return next((s for s in segments if s["words"]), None)

                timestamp_tokens = tokens >= tokenizer.timestamp_begin
                single_timestamp_ending = timestamp_tokens[-2:].tolist() == [
                    False,
                    True,
                ]

                consecutive = np.where(
                    np.logical_and(timestamp_tokens[:-1], timestamp_tokens[1:])
                )[0]
                consecutive += 1
                if len(consecutive) > 0:
                    # if the output contains two consecutive timestamp tokens
                    slices = consecutive.tolist()
                    if single_timestamp_ending:
                        slices.append(len(tokens))

                    last_slice = 0
                    for current_slice in slices:
                        sliced_tokens = tokens[last_slice:current_slice]
                        start_timestamp_pos = (
                            sliced_tokens[0].item() - tokenizer.timestamp_begin
                        )
                        end_timestamp_pos = (
                            sliced_tokens[-1].item() - tokenizer.timestamp_begin
                        )
                        current_segments.append(
                            new_segment(
                                start=time_offset
                                + start_timestamp_pos * time_precision,
                                end=time_offset + end_timestamp_pos * time_precision,
                                tokens=sliced_tokens,
                                result=result,
                            )
                        )
                        last_slice = current_slice

                    if single_timestamp_ending:
                        # single timestamp at the end means no speech after the last timestamp.
                        seek += segment_size
                    else:
                        # otherwise, ignore the unfinished segment and seek to the last timestamp
                        last_timestamp_pos = (
                            tokens[last_slice - 1].item() - tokenizer.timestamp_begin
                        )
                        seek += last_timestamp_pos * input_stride
                else:
                    duration = segment_duration
                    timestamps = tokens[timestamp_tokens.nonzero()[0]]
                    if (
                        len(timestamps) > 0
                        and timestamps[-1].item() != tokenizer.timestamp_begin
                    ):
                        # no consecutive timestamps but it has a timestamp; use the last one.
                        last_timestamp_pos = (
                            timestamps[-1].item() - tokenizer.timestamp_begin
                        )
                        duration = last_timestamp_pos * time_precision

                    current_segments.append(
                        new_segment(
                            start=time_offset,
                            end=time_offset + duration,
                            tokens=tokens,
                            result=result,
                        )
                    )
                    seek += segment_size

                if word_timestamps:
                    add_word_timestamps(
                        segments=current_segments,
                        model=model,
                        tokenizer=tokenizer,
                        mel=mel_segment,
                        num_frames=segment_size,
                        prepend_punctuations=prepend_punctuations,
                        append_punctuations=append_punctuations,
                        last_speech_timestamp=last_speech_timestamp,
                    )

                    if not single_timestamp_ending:
                        last_word_end = _get_end(current_segments)
                        if last_word_end is not None and last_word_end > time_offset:
                            seek = round(last_word_end * FRAMES_PER_SECOND)

                    # skip silence before possible hallucinations
                    if hallucination_silence_threshold is not None:
                        threshold = hallucination_silence_threshold
                        if not single_timestamp_ending:
                            last_word_end = _get_end(current_segments)
                            if (
                                last_word_end is not None
                                and last_word_end > time_offset
                            ):
                                remaining_duration = window_end_time - last_word_end
                                if remaining_duration > threshold:
                                    seek = round(last_word_end * FRAMES_PER_SECOND)
                                else:
                                    seek = previous_seek + segment_size

                        # if first segment might be a hallucination, skip leading silence
                        first_segment = next_words_segment(current_segments)
                        if first_segment is not None and is_segment_anomaly(
                            first_segment
                        ):
                            gap = first_segment["start"] - time_offset
                            if gap > threshold:
                                seek = previous_seek + round(gap * FRAMES_PER_SECOND)
                                continue

                        # skip silence before any possible hallucination that is surrounded
                        # by silence or more hallucinations
                        hal_last_end = last_speech_timestamp
                        for si in range(len(current_segments)):
                            segment = current_segments[si]
                            if not segment["words"]:
                                continue
                            if is_segment_anomaly(segment):
                                next_segment = next_words_segment(
                                    current_segments[si + 1 :]
                                )
                                if next_segment is not None:
                                    hal_next_start = next_segment["words"][0]["start"]
                                else:
                                    hal_next_start = time_offset + segment_duration
                                silence_before = (
                                    segment["start"] - hal_last_end > threshold
                                    or segment["start"] < threshold
                                    or segment["start"] - time_offset < 2.0
                                )
                                silence_after = (
                                    hal_next_start - segment["end"] > threshold
                                    or is_segment_anomaly(next_segment)
                                    or window_end_time - segment["end"] < 2.0
                                )
                                if silence_before and silence_after:
                                    seek = round(
                                        max(time_offset + 1, segment["start"])
                                        * FRAMES_PER_SECOND
                                    )
                                    if content_duration - segment["end"] < threshold:
                                        seek = content_frames
                                    current_segments[si:] = []
                                    break
                            hal_last_end = segment["end"]

                    last_word_end = _get_end(current_segments)
                    if last_word_end is not None:
                        last_speech_timestamp = last_word_end

                if verbose:
                    for segment in current_segments:
                        start, end, text = (
                            segment["start"],
                            segment["end"],
                            segment["text"],
                        )
                        line = f"[{_format_timestamp(start)} --> {_format_timestamp(end)}] {text}"
                        print(make_safe(line))

                # if a segment is instantaneous or does not contain text, clear it
                for i, segment in enumerate(current_segments):
                    if (
                        segment["start"] == segment["end"]
                        or segment["text"].strip() == ""
                    ):
                        segment["text"] = ""
                        segment["tokens"] = []
                        segment["words"] = []

                all_segments.extend(
                    [
                        {"id": i, **segment}
                        for i, segment in enumerate(
                            current_segments, start=len(all_segments)
                        )
                    ]
                )
                all_tokens.extend(
                    [
                        token
                        for segment in current_segments
                        for token in segment["tokens"]
                    ]
                )

                if not condition_on_previous_text or result.temperature > 0.5:
                    # do not feed the prompt tokens if a high temperature was used
                    prompt_reset_since = len(all_tokens)

                # update progress bar
                pbar.update(min(content_frames, seek) - previous_seek)

    return dict(
        text=tokenizer.decode(all_tokens[len(initial_prompt_tokens) :]),
        segments=all_segments,
        language=language,
    )

>>>> whisper/mlx_whisper/load_models.py
# Copyright © 2023 Apple Inc.

import json
from pathlib import Path

import mlx.core as mx
import mlx.nn as nn
from huggingface_hub import snapshot_download
from mlx.utils import tree_unflatten

from . import whisper


def load_model(
    path_or_hf_repo: str,
    dtype: mx.Dtype = mx.float32,
) -> whisper.Whisper:
    model_path = Path(path_or_hf_repo)
    if not model_path.exists():
        model_path = Path(snapshot_download(repo_id=path_or_hf_repo))

    with open(str(model_path / "config.json"), "r") as f:
        config = json.loads(f.read())
        config.pop("model_type", None)
        quantization = config.pop("quantization", None)

    model_args = whisper.ModelDimensions(**config)

    wf = model_path / "weights.safetensors"
    if not wf.exists():
        wf = model_path / "weights.npz"
    weights = mx.load(str(wf))

    model = whisper.Whisper(model_args, dtype)

    if quantization is not None:
        class_predicate = (
            lambda p, m: isinstance(m, (nn.Linear, nn.Embedding))
            and f"{p}.scales" in weights
        )
        nn.quantize(model, **quantization, class_predicate=class_predicate)

    weights = tree_unflatten(list(weights.items()))
    model.update(weights)
    mx.eval(model.parameters())
    return model

>>>> whisper/mlx_whisper/cli.py
# Copyright © 2024 Apple Inc.

import argparse
import os
import pathlib
import traceback
import warnings

from . import audio
from .tokenizer import LANGUAGES, TO_LANGUAGE_CODE
from .transcribe import transcribe
from .writers import get_writer


def build_parser():
    def optional_int(string):
        return None if string == "None" else int(string)

    def optional_float(string):
        return None if string == "None" else float(string)

    def str2bool(string):
        str2val = {"True": True, "False": False}
        if string in str2val:
            return str2val[string]
        else:
            raise ValueError(f"Expected one of {set(str2val.keys())}, got {string}")

    parser = argparse.ArgumentParser(
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    parser.add_argument("audio", nargs="+", help="Audio file(s) to transcribe")

    parser.add_argument(
        "--model",
        default="mlx-community/whisper-tiny",
        type=str,
        help="The model directory or hugging face repo",
    )
    parser.add_argument(
        "--output-name",
        type=str,
        default=None,
        help=(
            "The name of transcription/translation output files before "
            "--output-format extensions"
        ),
    )
    parser.add_argument(
        "--output-dir",
        "-o",
        type=str,
        default=".",
        help="Directory to save the outputs",
    )
    parser.add_argument(
        "--output-format",
        "-f",
        type=str,
        default="txt",
        choices=["txt", "vtt", "srt", "tsv", "json", "all"],
        help="Format of the output file",
    )
    parser.add_argument(
        "--verbose",
        type=str2bool,
        default=True,
        help="Whether to print out progress and debug messages",
    )
    parser.add_argument(
        "--task",
        type=str,
        default="transcribe",
        choices=["transcribe", "translate"],
        help="Perform speech recognition ('transcribe') or speech translation ('translate')",
    )
    parser.add_argument(
        "--language",
        type=str,
        default=None,
        choices=sorted(LANGUAGES.keys())
        + sorted([k.title() for k in TO_LANGUAGE_CODE.keys()]),
        help="Language spoken in the audio, specify None to auto-detect",
    )
    parser.add_argument(
        "--temperature", type=float, default=0, help="Temperature for sampling"
    )
    parser.add_argument(
        "--best-of",
        type=optional_int,
        default=5,
        help="Number of candidates when sampling with non-zero temperature",
    )
    parser.add_argument(
        "--patience",
        type=float,
        default=None,
        help="Optional patience value to use in beam decoding, as in https://arxiv.org/abs/2204.05424, the default (1.0) is equivalent to conventional beam search",
    )
    parser.add_argument(
        "--length-penalty",
        type=float,
        default=None,
        help="Optional token length penalty coefficient (alpha) as in https://arxiv.org/abs/1609.08144, uses simple length normalization by default.",
    )
    parser.add_argument(
        "--suppress-tokens",
        type=str,
        default="-1",
        help="Comma-separated list of token ids to suppress during sampling; '-1' will suppress most special characters except common punctuations",
    )
    parser.add_argument(
        "--initial-prompt",
        type=str,
        default=None,
        help="Optional text to provide as a prompt for the first window.",
    )
    parser.add_argument(
        "--condition-on-previous-text",
        type=str2bool,
        default=True,
        help="If True, provide the previous output of the model as a prompt for the next window; disabling may make the text inconsistent across windows, but the model becomes less prone to getting stuck in a failure loop",
    )
    parser.add_argument(
        "--fp16",
        type=str2bool,
        default=True,
        help="Whether to perform inference in fp16",
    )
    parser.add_argument(
        "--compression-ratio-threshold",
        type=optional_float,
        default=2.4,
        help="if the gzip compression ratio is higher than this value, treat the decoding as failed",
    )
    parser.add_argument(
        "--logprob-threshold",
        type=optional_float,
        default=-1.0,
        help="If the average log probability is lower than this value, treat the decoding as failed",
    )
    parser.add_argument(
        "--no-speech-threshold",
        type=optional_float,
        default=0.6,
        help="If the probability of the token is higher than this value the decoding has failed due to `logprob_threshold`, consider the segment as silence",
    )
    parser.add_argument(
        "--word-timestamps",
        type=str2bool,
        default=False,
        help="Extract word-level timestamps and refine the results based on them",
    )
    parser.add_argument(
        "--prepend-punctuations",
        type=str,
        default="\"'“¿([{-",
        help="If word-timestamps is True, merge these punctuation symbols with the next word",
    )
    parser.add_argument(
        "--append-punctuations",
        type=str,
        default="\"'.。,，!！?？:：”)]}、",
        help="If word_timestamps is True, merge these punctuation symbols with the previous word",
    )
    parser.add_argument(
        "--highlight-words",
        type=str2bool,
        default=False,
        help="(requires --word_timestamps True) underline each word as it is spoken in srt and vtt",
    )
    parser.add_argument(
        "--max-line-width",
        type=int,
        default=None,
        help="(requires --word_timestamps True) the maximum number of characters in a line before breaking the line",
    )
    parser.add_argument(
        "--max-line-count",
        type=int,
        default=None,
        help="(requires --word_timestamps True) the maximum number of lines in a segment",
    )
    parser.add_argument(
        "--max-words-per-line",
        type=int,
        default=None,
        help="(requires --word_timestamps True, no effect with --max_line_width) the maximum number of words in a segment",
    )
    parser.add_argument(
        "--hallucination-silence-threshold",
        type=optional_float,
        help="(requires --word_timestamps True) skip silent periods longer than this threshold (in seconds) when a possible hallucination is detected",
    )
    parser.add_argument(
        "--clip-timestamps",
        type=str,
        default="0",
        help="Comma-separated list start,end,start,end,... timestamps (in seconds) of clips to process, where the last end timestamp defaults to the end of the file",
    )
    return parser


def main():
    parser = build_parser()
    args = vars(parser.parse_args())
    if args["verbose"] is True:
        print(f"Args: {args}")

    path_or_hf_repo: str = args.pop("model")
    output_dir: str = args.pop("output_dir")
    output_format: str = args.pop("output_format")
    output_name: str = args.pop("output_name")
    os.makedirs(output_dir, exist_ok=True)

    writer = get_writer(output_format, output_dir)
    word_options = [
        "highlight_words",
        "max_line_count",
        "max_line_width",
        "max_words_per_line",
    ]
    writer_args = {arg: args.pop(arg) for arg in word_options}
    if not args["word_timestamps"]:
        for k, v in writer_args.items():
            if v:
                argop = k.replace("_", "-")
                parser.error(f"--{argop} requires --word-timestamps True")
    if writer_args["max_line_count"] and not writer_args["max_line_width"]:
        warnings.warn("--max-line-count has no effect without --max-line-width")
    if writer_args["max_words_per_line"] and writer_args["max_line_width"]:
        warnings.warn("--max-words-per-line has no effect with --max-line-width")

    for audio_obj in args.pop("audio"):
        if audio_obj == "-":
            # receive the contents from stdin rather than read a file
            audio_obj = audio.load_audio(from_stdin=True)

            output_name = output_name or "content"
        else:
            output_name = output_name or pathlib.Path(audio_obj).stem
        try:
            result = transcribe(
                audio_obj,
                path_or_hf_repo=path_or_hf_repo,
                **args,
            )
            writer(result, output_name, **writer_args)
        except Exception as e:
            traceback.print_exc()
            print(f"Skipping {audio_obj} due to {type(e).__name__}: {str(e)}")


if __name__ == "__main__":
    main()

>>>> whisper/mlx_whisper/timing.py
# Copyright © 2023 Apple Inc.

import itertools
from dataclasses import dataclass
from typing import TYPE_CHECKING, List

import mlx.core as mx
import numba
import numpy as np
from scipy import signal

from .audio import HOP_LENGTH, SAMPLE_RATE, TOKENS_PER_SECOND
from .tokenizer import Tokenizer

if TYPE_CHECKING:
    from .model import Whisper


def median_filter(x: np.ndarray, filter_width: int):
    """Apply a median filter of width `filter_width` along the last dimension of `x`"""
    pad_width = filter_width // 2
    if x.shape[-1] <= pad_width:
        # F.pad requires the padding width to be smaller than the input dimension
        return x

    if (ndim := x.ndim) <= 2:
        # `F.pad` does not support 1D or 2D inputs for reflect padding but supports 3D and 4D
        x = x[None, None, :]

    assert (
        filter_width > 0 and filter_width % 2 == 1
    ), "`filter_width` should be an odd number"

    x = np.pad(x, ((0, 0), (0, 0), (pad_width, pad_width)), mode="reflect")

    # todo: more efficient version in mlx
    result = signal.medfilt(x.astype(np.float32), kernel_size=(1, 1, filter_width))[
        ..., pad_width:-pad_width
    ]

    if ndim <= 2:
        result = result[0, 0]

    return result


@numba.jit(nopython=True)
def backtrace(trace: np.ndarray):
    i = trace.shape[0] - 1
    j = trace.shape[1] - 1
    trace[0, :] = 2
    trace[:, 0] = 1

    result = []
    while i > 0 or j > 0:
        result.append((i - 1, j - 1))

        if trace[i, j] == 0:
            i -= 1
            j -= 1
        elif trace[i, j] == 1:
            i -= 1
        elif trace[i, j] == 2:
            j -= 1
        else:
            raise ValueError("Unexpected trace[i, j]")

    result = np.array(result)
    return result[::-1, :].T


@numba.jit(nopython=True, parallel=True)
def dtw_cpu(x: np.ndarray):
    N, M = x.shape
    cost = np.ones((N + 1, M + 1), dtype=np.float32) * np.inf
    trace = -np.ones((N + 1, M + 1), dtype=np.float32)

    cost[0, 0] = 0
    for j in range(1, M + 1):
        for i in range(1, N + 1):
            c0 = cost[i - 1, j - 1]
            c1 = cost[i - 1, j]
            c2 = cost[i, j - 1]

            if c0 < c1 and c0 < c2:
                c, t = c0, 0
            elif c1 < c0 and c1 < c2:
                c, t = c1, 1
            else:
                c, t = c2, 2

            cost[i, j] = x[i - 1, j - 1] + c
            trace[i, j] = t

    return backtrace(trace)


def dtw(x: np.ndarray) -> np.ndarray:
    # todo: more efficient version in mlx
    return dtw_cpu(x)


@dataclass
class WordTiming:
    word: str
    tokens: List[int]
    start: float
    end: float
    probability: float


def find_alignment(
    model: "Whisper",
    tokenizer: Tokenizer,
    text_tokens: List[int],
    mel: mx.array,
    num_frames: int,
    *,
    medfilt_width: int = 7,
    qk_scale: float = 1.0,
) -> List[WordTiming]:
    if len(text_tokens) == 0:
        return []

    tokens = mx.array(
        [
            *tokenizer.sot_sequence,
            tokenizer.no_timestamps,
            *text_tokens,
            tokenizer.eot,
        ]
    )

    logits, cross_qk = model.forward_with_cross_qk(mel[None, :], tokens[None, :])
    # consider only the logits associated with predicting text
    sampled_logits = logits[0][len(tokenizer.sot_sequence) : -2, : tokenizer.eot]
    token_probs = mx.softmax(sampled_logits, precise=True, axis=-1)
    text_token_probs = mx.take_along_axis(
        token_probs, mx.array(text_tokens)[:, None], axis=1
    ).squeeze(1)
    text_token_probs = np.array(text_token_probs)

    # heads * tokens * frames
    weights = mx.stack(
        [cross_qk[_l][0, _h] for _l, _h in model.alignment_heads.tolist()]
    )
    weights = weights[:, :, : num_frames // 2]
    weights = mx.softmax(weights * qk_scale, axis=-1, precise=True)
    weights = weights.astype(mx.float32)
    mean = mx.mean(weights, axis=-2, keepdims=True)
    std = mx.var(weights, axis=-2, keepdims=True, ddof=0).sqrt()
    weights = (weights - mean) / std
    weights = median_filter(np.array(weights), medfilt_width)

    matrix = weights.mean(axis=0)
    matrix = matrix[len(tokenizer.sot_sequence) : -1]
    text_indices, time_indices = dtw(-matrix)

    words, word_tokens = tokenizer.split_to_word_tokens(text_tokens + [tokenizer.eot])
    if len(word_tokens) <= 1:
        # return on eot only
        # >>> np.pad([], (1, 0))
        # array([0.])
        # This results in crashes when we lookup jump_times with float, like
        # IndexError: arrays used as indices must be of integer (or boolean) type
        return []
    word_boundaries = np.pad(np.cumsum([len(t) for t in word_tokens[:-1]]), (1, 0))

    jumps = np.pad(np.diff(text_indices), (1, 0), constant_values=1).astype(bool)
    jump_times = time_indices[jumps] / TOKENS_PER_SECOND
    start_times = jump_times[word_boundaries[:-1]]
    end_times = jump_times[word_boundaries[1:]]
    word_probabilities = [
        np.mean(text_token_probs[i:j])
        for i, j in zip(word_boundaries[:-1], word_boundaries[1:])
    ]

    return [
        WordTiming(word, tokens, start, end, probability)
        for word, tokens, start, end, probability in zip(
            words, word_tokens, start_times, end_times, word_probabilities
        )
    ]


def merge_punctuations(alignment: List[WordTiming], prepended: str, appended: str):
    # merge prepended punctuations
    i = len(alignment) - 2
    j = len(alignment) - 1
    while i >= 0:
        previous = alignment[i]
        following = alignment[j]
        if previous.word.startswith(" ") and previous.word.strip() in prepended:
            # prepend it to the following word
            following.word = previous.word + following.word
            following.tokens = previous.tokens + following.tokens
            previous.word = ""
            previous.tokens = []
        else:
            j = i
        i -= 1

    # merge appended punctuations
    i = 0
    j = 1
    while j < len(alignment):
        previous = alignment[i]
        following = alignment[j]
        if not previous.word.endswith(" ") and following.word in appended:
            # append it to the previous word
            previous.word = previous.word + following.word
            previous.tokens = previous.tokens + following.tokens
            following.word = ""
            following.tokens = []
        else:
            i = j
        j += 1


def add_word_timestamps(
    *,
    segments: List[dict],
    model: "Whisper",
    tokenizer: Tokenizer,
    mel: mx.array,
    num_frames: int,
    prepend_punctuations: str = "\"'“¿([{-",
    append_punctuations: str = "\"'.。,，!！?？:：”)]}、",
    last_speech_timestamp: float,
    **kwargs,
):
    if len(segments) == 0:
        return

    text_tokens_per_segment = [
        [token for token in segment["tokens"] if token < tokenizer.eot]
        for segment in segments
    ]

    text_tokens = list(itertools.chain.from_iterable(text_tokens_per_segment))
    alignment = find_alignment(model, tokenizer, text_tokens, mel, num_frames, **kwargs)
    word_durations = np.array([t.end - t.start for t in alignment])
    word_durations = word_durations[word_durations.nonzero()]
    median_duration = np.median(word_durations) if len(word_durations) > 0 else 0.0
    median_duration = min(0.7, float(median_duration))
    max_duration = median_duration * 2

    # hack: truncate long words at sentence boundaries.
    # a better segmentation algorithm based on VAD should be able to replace this.
    if len(word_durations) > 0:
        sentence_end_marks = ".。!！?？"
        # ensure words at sentence boundaries are not longer than twice the median word duration.
        for i in range(1, len(alignment)):
            if alignment[i].end - alignment[i].start > max_duration:
                if alignment[i].word in sentence_end_marks:
                    alignment[i].end = alignment[i].start + max_duration
                elif alignment[i - 1].word in sentence_end_marks:
                    alignment[i].start = alignment[i].end - max_duration

    merge_punctuations(alignment, prepend_punctuations, append_punctuations)

    time_offset = segments[0]["seek"] * HOP_LENGTH / SAMPLE_RATE
    word_index = 0

    for segment, text_tokens in zip(segments, text_tokens_per_segment):
        saved_tokens = 0
        words = []

        while word_index < len(alignment) and saved_tokens < len(text_tokens):
            timing = alignment[word_index]

            if timing.word:
                words.append(
                    dict(
                        word=timing.word,
                        start=round(time_offset + timing.start, 2),
                        end=round(time_offset + timing.end, 2),
                        probability=float(timing.probability),
                    )
                )

            saved_tokens += len(timing.tokens)
            word_index += 1

        # hack: truncate long words at segment boundaries.
        # a better segmentation algorithm based on VAD should be able to replace this.
        if len(words) > 0:
            # ensure the first and second word after a pause is not longer than
            # twice the median word duration.
            if words[0]["end"] - last_speech_timestamp > median_duration * 4 and (
                words[0]["end"] - words[0]["start"] > max_duration
                or (
                    len(words) > 1
                    and words[1]["end"] - words[0]["start"] > max_duration * 2
                )
            ):
                if (
                    len(words) > 1
                    and words[1]["end"] - words[1]["start"] > max_duration
                ):
                    boundary = max(words[1]["end"] / 2, words[1]["end"] - max_duration)
                    words[0]["end"] = words[1]["start"] = boundary
                words[0]["start"] = max(0, words[0]["end"] - max_duration)

            # prefer the segment-level start timestamp if the first word is too long.
            if (
                segment["start"] < words[0]["end"]
                and segment["start"] - 0.5 > words[0]["start"]
            ):
                words[0]["start"] = max(
                    0, min(words[0]["end"] - median_duration, segment["start"])
                )
            else:
                segment["start"] = words[0]["start"]

            # prefer the segment-level end timestamp if the last word is too long.
            if (
                segment["end"] > words[-1]["start"]
                and segment["end"] + 0.5 < words[-1]["end"]
            ):
                words[-1]["end"] = max(
                    words[-1]["start"] + median_duration, segment["end"]
                )
            else:
                segment["end"] = words[-1]["end"]

            last_speech_timestamp = segment["end"]

        segment["words"] = words

>>>> whisper/mlx_whisper/tokenizer.py
# Copyright © 2023 Apple Inc.

import base64
import os
import string
from dataclasses import dataclass, field
from functools import cached_property, lru_cache
from typing import Dict, List, Optional, Tuple

import tiktoken

LANGUAGES = {
    "en": "english",
    "zh": "chinese",
    "de": "german",
    "es": "spanish",
    "ru": "russian",
    "ko": "korean",
    "fr": "french",
    "ja": "japanese",
    "pt": "portuguese",
    "tr": "turkish",
    "pl": "polish",
    "ca": "catalan",
    "nl": "dutch",
    "ar": "arabic",
    "sv": "swedish",
    "it": "italian",
    "id": "indonesian",
    "hi": "hindi",
    "fi": "finnish",
    "vi": "vietnamese",
    "he": "hebrew",
    "uk": "ukrainian",
    "el": "greek",
    "ms": "malay",
    "cs": "czech",
    "ro": "romanian",
    "da": "danish",
    "hu": "hungarian",
    "ta": "tamil",
    "no": "norwegian",
    "th": "thai",
    "ur": "urdu",
    "hr": "croatian",
    "bg": "bulgarian",
    "lt": "lithuanian",
    "la": "latin",
    "mi": "maori",
    "ml": "malayalam",
    "cy": "welsh",
    "sk": "slovak",
    "te": "telugu",
    "fa": "persian",
    "lv": "latvian",
    "bn": "bengali",
    "sr": "serbian",
    "az": "azerbaijani",
    "sl": "slovenian",
    "kn": "kannada",
    "et": "estonian",
    "mk": "macedonian",
    "br": "breton",
    "eu": "basque",
    "is": "icelandic",
    "hy": "armenian",
    "ne": "nepali",
    "mn": "mongolian",
    "bs": "bosnian",
    "kk": "kazakh",
    "sq": "albanian",
    "sw": "swahili",
    "gl": "galician",
    "mr": "marathi",
    "pa": "punjabi",
    "si": "sinhala",
    "km": "khmer",
    "sn": "shona",
    "yo": "yoruba",
    "so": "somali",
    "af": "afrikaans",
    "oc": "occitan",
    "ka": "georgian",
    "be": "belarusian",
    "tg": "tajik",
    "sd": "sindhi",
    "gu": "gujarati",
    "am": "amharic",
    "yi": "yiddish",
    "lo": "lao",
    "uz": "uzbek",
    "fo": "faroese",
    "ht": "haitian creole",
    "ps": "pashto",
    "tk": "turkmen",
    "nn": "nynorsk",
    "mt": "maltese",
    "sa": "sanskrit",
    "lb": "luxembourgish",
    "my": "myanmar",
    "bo": "tibetan",
    "tl": "tagalog",
    "mg": "malagasy",
    "as": "assamese",
    "tt": "tatar",
    "haw": "hawaiian",
    "ln": "lingala",
    "ha": "hausa",
    "ba": "bashkir",
    "jw": "javanese",
    "su": "sundanese",
    "yue": "cantonese",
}

# language code lookup by name, with a few language aliases
TO_LANGUAGE_CODE = {
    **{language: code for code, language in LANGUAGES.items()},
    "burmese": "my",
    "valencian": "ca",
    "flemish": "nl",
    "haitian": "ht",
    "letzeburgesch": "lb",
    "pushto": "ps",
    "panjabi": "pa",
    "moldavian": "ro",
    "moldovan": "ro",
    "sinhalese": "si",
    "castilian": "es",
    "mandarin": "zh",
}


@dataclass
class Tokenizer:
    """A thin wrapper around `tiktoken` providing quick access to special tokens"""

    encoding: tiktoken.Encoding
    num_languages: int
    language: Optional[str] = None
    task: Optional[str] = None
    sot_sequence: Tuple[int] = ()
    special_tokens: Dict[str, int] = field(default_factory=dict)

    def __post_init__(self):
        for special in self.encoding.special_tokens_set:
            special_token = self.encoding.encode_single_token(special)
            self.special_tokens[special] = special_token

        sot: int = self.special_tokens["<|startoftranscript|>"]
        translate: int = self.special_tokens["<|translate|>"]
        transcribe: int = self.special_tokens["<|transcribe|>"]

        langs = tuple(LANGUAGES.keys())[: self.num_languages]
        sot_sequence = [sot]
        if self.language is not None:
            sot_sequence.append(sot + 1 + langs.index(self.language))
        if self.task is not None:
            task_token: int = transcribe if self.task == "transcribe" else translate
            sot_sequence.append(task_token)

        self.sot_sequence = tuple(sot_sequence)

    def encode(self, text, **kwargs):
        return self.encoding.encode(text, **kwargs)

    def decode(self, token_ids: List[int], **kwargs) -> str:
        token_ids = [t for t in token_ids if t < self.timestamp_begin]
        return self.encoding.decode(token_ids, **kwargs)

    def decode_with_timestamps(self, token_ids: List[int], **kwargs) -> str:
        """
        Timestamp tokens are above other special tokens' id range and are ignored by `decode()`.
        This method decodes given tokens with timestamps tokens annotated, e.g. "<|1.08|>".
        """
        return self.encoding.decode(token_ids, **kwargs)

    @cached_property
    def eot(self) -> int:
        return self.encoding.eot_token

    @cached_property
    def transcribe(self) -> int:
        return self.special_tokens["<|transcribe|>"]

    @cached_property
    def translate(self) -> int:
        return self.special_tokens["<|translate|>"]

    @cached_property
    def sot(self) -> int:
        return self.special_tokens["<|startoftranscript|>"]

    @cached_property
    def sot_lm(self) -> int:
        return self.special_tokens["<|startoflm|>"]

    @cached_property
    def sot_prev(self) -> int:
        return self.special_tokens["<|startofprev|>"]

    @cached_property
    def no_speech(self) -> int:
        return self.special_tokens["<|nospeech|>"]

    @cached_property
    def no_timestamps(self) -> int:
        return self.special_tokens["<|notimestamps|>"]

    @cached_property
    def timestamp_begin(self) -> int:
        return self.special_tokens["<|0.00|>"]

    @cached_property
    def language_token(self) -> int:
        """Returns the token id corresponding to the value of the `language` field"""
        if self.language is None:
            raise ValueError("This tokenizer does not have language token configured")

        return self.to_language_token(self.language)

    def to_language_token(self, language):
        if token := self.special_tokens.get(f"<|{language}|>", None):
            return token

        raise KeyError(f"Language {language} not found in tokenizer.")

    @cached_property
    def all_language_tokens(self) -> Tuple[int]:
        result = []
        for token, token_id in self.special_tokens.items():
            if token.strip("<|>") in LANGUAGES:
                result.append(token_id)
        return tuple(result)[: self.num_languages]

    @cached_property
    def all_language_codes(self) -> Tuple[str]:
        return tuple(self.decode([_l]).strip("<|>") for _l in self.all_language_tokens)

    @cached_property
    def sot_sequence_including_notimestamps(self) -> Tuple[int]:
        return tuple(list(self.sot_sequence) + [self.no_timestamps])

    @cached_property
    def non_speech_tokens(self) -> Tuple[int]:
        """
        Returns the list of tokens to suppress in order to avoid any speaker tags or non-speech
        annotations, to prevent sampling texts that are not actually spoken in the audio, e.g.

        - ♪♪♪
        - ( SPEAKING FOREIGN LANGUAGE )
        - [DAVID] Hey there,

        keeping basic punctuations like commas, periods, question marks, exclamation points, etc.
        """
        symbols = list('"#()*+/:;<=>@[\\]^_`{|}~「」『』')
        symbols += (
            "<< >> <<< >>> -- --- -( -[ (' (\" (( )) ((( ))) [[ ]] {{ }} ♪♪ ♪♪♪".split()
        )

        # symbols that may be a single token or multiple tokens depending on the tokenizer.
        # In case they're multiple tokens, suppress the first token, which is safe because:
        # These are between U+2640 and U+267F miscellaneous symbols that are okay to suppress
        # in generations, and in the 3-byte UTF-8 representation they share the first two bytes.
        miscellaneous = set("♩♪♫♬♭♮♯")
        assert all(0x2640 <= ord(c) <= 0x267F for c in miscellaneous)

        # allow hyphens "-" and single quotes "'" between words, but not at the beginning of a word
        result = {self.encoding.encode(" -")[0], self.encoding.encode(" '")[0]}
        for symbol in symbols + list(miscellaneous):
            for tokens in [
                self.encoding.encode(symbol),
                self.encoding.encode(" " + symbol),
            ]:
                if len(tokens) == 1 or symbol in miscellaneous:
                    result.add(tokens[0])

        return tuple(sorted(result))

    def split_to_word_tokens(self, tokens: List[int]):
        if self.language in {"zh", "ja", "th", "lo", "my", "yue"}:
            # These languages don't typically use spaces, so it is difficult to split words
            # without morpheme analysis. Here, we instead split words at any
            # position where the tokens are decoded as valid unicode points
            return self.split_tokens_on_unicode(tokens)

        return self.split_tokens_on_spaces(tokens)

    def split_tokens_on_unicode(self, tokens: List[int]):
        decoded_full = self.decode_with_timestamps(tokens)
        replacement_char = "\ufffd"

        words = []
        word_tokens = []
        current_tokens = []
        unicode_offset = 0

        for token in tokens:
            current_tokens.append(token)
            decoded = self.decode_with_timestamps(current_tokens)

            if (
                replacement_char not in decoded
                or decoded_full[unicode_offset + decoded.index(replacement_char)]
                == replacement_char
            ):
                words.append(decoded)
                word_tokens.append(current_tokens)
                current_tokens = []
                unicode_offset += len(decoded)

        return words, word_tokens

    def split_tokens_on_spaces(self, tokens: List[int]):
        subwords, subword_tokens_list = self.split_tokens_on_unicode(tokens)
        words = []
        word_tokens = []

        for subword, subword_tokens in zip(subwords, subword_tokens_list):
            special = subword_tokens[0] >= self.eot
            with_space = subword.startswith(" ")
            punctuation = subword.strip() in string.punctuation
            if special or with_space or punctuation or len(words) == 0:
                words.append(subword)
                word_tokens.append(subword_tokens)
            else:
                words[-1] = words[-1] + subword
                word_tokens[-1].extend(subword_tokens)

        return words, word_tokens


@lru_cache(maxsize=None)
def get_encoding(name: str = "gpt2", num_languages: int = 99):
    vocab_path = os.path.join(os.path.dirname(__file__), "assets", f"{name}.tiktoken")
    with open(vocab_path) as fid:
        ranks = {
            base64.b64decode(token): int(rank)
            for token, rank in (line.split() for line in fid if line)
        }
    n_vocab = len(ranks)
    special_tokens = {}

    specials = [
        "<|endoftext|>",
        "<|startoftranscript|>",
        *[f"<|{lang}|>" for lang in list(LANGUAGES.keys())[:num_languages]],
        "<|translate|>",
        "<|transcribe|>",
        "<|startoflm|>",
        "<|startofprev|>",
        "<|nospeech|>",
        "<|notimestamps|>",
        *[f"<|{i * 0.02:.2f}|>" for i in range(1501)],
    ]

    for token in specials:
        special_tokens[token] = n_vocab
        n_vocab += 1

    return tiktoken.Encoding(
        name=os.path.basename(vocab_path),
        explicit_n_vocab=n_vocab,
        pat_str=r"""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+""",
        mergeable_ranks=ranks,
        special_tokens=special_tokens,
    )


@lru_cache(maxsize=None)
def get_tokenizer(
    multilingual: bool,
    *,
    num_languages: int = 99,
    language: Optional[str] = None,
    task: Optional[str] = None,  # Literal["transcribe", "translate", None]
) -> Tokenizer:
    if language is not None:
        language = language.lower()
        if language not in LANGUAGES:
            if language in TO_LANGUAGE_CODE:
                language = TO_LANGUAGE_CODE[language]
            else:
                raise ValueError(f"Unsupported language: {language}")

    if multilingual:
        encoding_name = "multilingual"
        language = language or "en"
        task = task or "transcribe"
    else:
        encoding_name = "gpt2"
        language = None
        task = None

    encoding = get_encoding(name=encoding_name, num_languages=num_languages)

    return Tokenizer(
        encoding=encoding, num_languages=num_languages, language=language, task=task
    )

>>>> whisper/mlx_whisper/whisper.py
# Copyright © 2023 Apple Inc.

import base64
import gzip
import math
from dataclasses import dataclass
from typing import Union

import mlx.core as mx
import mlx.nn as nn
import numpy as np

from .decoding import decode as decode_function
from .decoding import detect_language as detect_language_function


@dataclass
class ModelDimensions:
    n_mels: int
    n_audio_ctx: int
    n_audio_state: int
    n_audio_head: int
    n_audio_layer: int
    n_vocab: int
    n_text_ctx: int
    n_text_state: int
    n_text_head: int
    n_text_layer: int


def sinusoids(length, channels, max_timescale=10000):
    """Returns sinusoids for positional embedding"""
    assert channels % 2 == 0
    log_timescale_increment = math.log(max_timescale) / (channels // 2 - 1)
    inv_timescales = mx.exp(-log_timescale_increment * mx.arange(channels // 2))
    scaled_time = mx.arange(length)[:, None] * inv_timescales[None, :]
    return mx.concatenate([mx.sin(scaled_time), mx.cos(scaled_time)], axis=1)


class MultiHeadAttention(nn.Module):
    def __init__(self, n_state: int, n_head: int):
        super().__init__()
        self.n_head = n_head
        self.query = nn.Linear(n_state, n_state)
        self.key = nn.Linear(n_state, n_state, bias=False)
        self.value = nn.Linear(n_state, n_state)
        self.out = nn.Linear(n_state, n_state)

    def __call__(
        self,
        x,
        xa=None,
        mask=None,
        kv_cache=None,
    ):
        q = self.query(x)

        if xa is None:
            k = self.key(x)
            v = self.value(x)
            if kv_cache is not None:
                k = mx.concatenate([kv_cache[0], k], axis=1)
                v = mx.concatenate([kv_cache[1], v], axis=1)
        elif kv_cache is None:
            k = self.key(xa)
            v = self.value(xa)
        else:
            k, v = kv_cache

        wv, qk = self.qkv_attention(q, k, v, mask)
        return self.out(wv), (k, v), qk

    def qkv_attention(self, q, k, v, mask=None):
        n_batch, n_ctx, n_state = q.shape
        scale = (n_state // self.n_head) ** -0.25
        q = q.reshape(*q.shape[:2], self.n_head, -1).transpose(0, 2, 1, 3) * scale
        k = k.reshape(*k.shape[:2], self.n_head, -1).transpose(0, 2, 3, 1) * scale
        v = v.reshape(*v.shape[:2], self.n_head, -1).transpose(0, 2, 1, 3)

        qk = q @ k
        if mask is not None:
            qk = qk + mask[:n_ctx, :n_ctx]

        w = mx.softmax(qk, axis=-1, precise=True)
        out = (w @ v).transpose(0, 2, 1, 3)
        out = out.reshape(n_batch, n_ctx, n_state)
        return out, qk


class ResidualAttentionBlock(nn.Module):
    def __init__(self, n_state: int, n_head: int, cross_attention: bool = False):
        super().__init__()

        self.attn = MultiHeadAttention(n_state, n_head)
        self.attn_ln = nn.LayerNorm(n_state)

        self.cross_attn = (
            MultiHeadAttention(n_state, n_head) if cross_attention else None
        )
        self.cross_attn_ln = nn.LayerNorm(n_state) if cross_attention else None

        n_mlp = n_state * 4
        self.mlp1 = nn.Linear(n_state, n_mlp)
        self.mlp2 = nn.Linear(n_mlp, n_state)
        self.mlp_ln = nn.LayerNorm(n_state)

    def __call__(self, x, xa=None, mask=None, kv_cache=None):
        kv, cross_kv = kv_cache if kv_cache else (None, None)
        y, kv, _ = self.attn(self.attn_ln(x), mask=mask, kv_cache=kv)
        x += y
        cross_qk = None
        if self.cross_attn:
            y, cross_kv, cross_qk = self.cross_attn(
                self.cross_attn_ln(x), xa, kv_cache=cross_kv
            )
            x += y
        x = x + self.mlp2(nn.gelu(self.mlp1(self.mlp_ln(x))))
        return x, (kv, cross_kv), cross_qk


class AudioEncoder(nn.Module):
    def __init__(
        self,
        n_mels: int,
        n_ctx: int,
        n_state: int,
        n_head: int,
        n_layer: int,
        dtype: mx.Dtype = mx.float16,
    ):
        super().__init__()
        self.conv1 = nn.Conv1d(n_mels, n_state, kernel_size=3, padding=1)
        self.conv2 = nn.Conv1d(n_state, n_state, kernel_size=3, stride=2, padding=1)
        self._positional_embedding = sinusoids(n_ctx, n_state).astype(dtype)

        self.blocks = [ResidualAttentionBlock(n_state, n_head) for _ in range(n_layer)]
        self.ln_post = nn.LayerNorm(n_state)

    def __call__(self, x):
        x = nn.gelu(self.conv1(x))
        x = nn.gelu(self.conv2(x))
        assert x.shape[1:] == self._positional_embedding.shape, "incorrect audio shape"
        x = x + self._positional_embedding

        for block in self.blocks:
            x, _, _ = block(x)

        x = self.ln_post(x)
        return x


class TextDecoder(nn.Module):
    def __init__(
        self,
        n_vocab: int,
        n_ctx: int,
        n_state: int,
        n_head: int,
        n_layer: int,
        dtype: mx.Dtype = mx.float16,
    ):
        super().__init__()

        self.token_embedding = nn.Embedding(n_vocab, n_state)
        self.positional_embedding = mx.zeros((n_ctx, n_state))

        self.blocks = [
            ResidualAttentionBlock(n_state, n_head, cross_attention=True)
            for _ in range(n_layer)
        ]
        self.ln = nn.LayerNorm(n_state)
        self._mask = nn.MultiHeadAttention.create_additive_causal_mask(n_ctx).astype(
            dtype
        )

    def __call__(self, x, xa, kv_cache=None):
        """
        x : mx.array, shape = (batch_size, <= n_ctx)
            the text tokens
        xa : mx.array, shape = (batch_size, n_audio_ctx, n_audio_state)
            the encoded audio features to be attended on
        """
        offset = kv_cache[0][0][0].shape[1] if kv_cache else 0
        x = (
            self.token_embedding(x)
            + self.positional_embedding[offset : offset + x.shape[-1]]
        )

        if kv_cache is None:
            kv_cache = [None] * len(self.blocks)
        cross_qk = [None] * len(self.blocks)
        for e, block in enumerate(self.blocks):
            x, kv_cache[e], cross_qk[e] = block(
                x, xa, mask=self._mask, kv_cache=kv_cache[e]
            )

        x = self.ln(x)
        return self.token_embedding.as_linear(x), kv_cache, cross_qk


class Whisper(nn.Module):
    def __init__(self, dims: ModelDimensions, dtype: mx.Dtype = mx.float16):
        super().__init__()
        self.dims = dims
        self.encoder = AudioEncoder(
            self.dims.n_mels,
            self.dims.n_audio_ctx,
            self.dims.n_audio_state,
            self.dims.n_audio_head,
            self.dims.n_audio_layer,
            dtype,
        )
        self.decoder = TextDecoder(
            self.dims.n_vocab,
            self.dims.n_text_ctx,
            self.dims.n_text_state,
            self.dims.n_text_head,
            self.dims.n_text_layer,
            dtype,
        )
        # use the last half among the decoder layers for time alignment by default;
        # to use a specific set of heads, see `set_alignment_heads()` below.
        all_heads = np.zeros(
            (self.dims.n_text_layer, self.dims.n_text_head), dtype=bool
        )
        all_heads[self.dims.n_text_layer // 2 :] = True
        self.alignment_heads = mx.array(np.asarray(all_heads.nonzero()).T)

    def set_alignment_heads(self, dump: Union[bytes, np.ndarray]):
        if isinstance(dump, np.ndarray):
            self.alignment_heads = mx.array(dump)
        elif isinstance(dump, bytes):
            array = np.frombuffer(
                gzip.decompress(base64.b85decode(dump)), dtype=bool
            ).copy()
            mask = array.reshape(self.dims.n_text_layer, self.dims.n_text_head)
            self.alignment_heads = mx.array(np.asarray(mask.nonzero()).T)
        else:
            raise ValueError(
                f"Invalid type for `dump`: {type(dump)}. Expected a np.ndarray or base85-encoded bytes containing"
                " alignment_head information"
            )

    def embed_audio(self, mel):
        return self.encoder(mel)

    def logits(self, tokens, audio_features):
        return self.decoder(tokens, audio_features)[0]

    def forward_with_cross_qk(self, mel, tokens):
        logits, _, cross_qk = self.decoder(tokens, self.encoder(mel))
        return logits, cross_qk

    def __call__(self, mel, tokens):
        return self.decoder(tokens, self.encoder(mel))[0]

    @property
    def is_multilingual(self):
        return self.dims.n_vocab >= 51865

    @property
    def num_languages(self):
        return self.dims.n_vocab - 51765 - int(self.is_multilingual)

    detect_language = detect_language_function
    decode = decode_function

>>>> whisper/mlx_whisper/decoding.py
# Copyright © 2023 Apple Inc.

import zlib
from dataclasses import dataclass, field, replace
from typing import Dict, Iterable, List, Optional, Sequence, Tuple, Union

import mlx.core as mx
import numpy as np
from mlx.utils import tree_map

from .audio import CHUNK_LENGTH
from .tokenizer import Tokenizer, get_tokenizer


def compression_ratio(text) -> float:
    text_bytes = text.encode("utf-8")
    return len(text_bytes) / len(zlib.compress(text_bytes))


def detect_language(
    model: "Whisper", mel: mx.array, tokenizer: Tokenizer = None
) -> Tuple[mx.array, List[dict]]:
    """
    Detect the spoken language in the audio, and return them as list of strings, along with the ids
    of the most probable language tokens and the probability distribution over all language tokens.
    This is performed outside the main decode loop in order to not interfere with kv-caching.

    Returns
    -------
    language_tokens : mx.array, shape = (n_audio,)
        ids of the most probable language tokens, which appears after the startoftranscript token.
    language_probs : List[Dict[str, float]], length = n_audio
        list of dictionaries containing the probability distribution over all languages.
    """
    if tokenizer is None:
        tokenizer = get_tokenizer(
            model.is_multilingual, num_languages=model.num_languages
        )
    if (
        tokenizer.language is None
        or tokenizer.language_token not in tokenizer.sot_sequence
    ):
        raise ValueError(
            "This model doesn't have language tokens so it can't perform lang id"
        )

    single = mel.ndim == 2
    if single:
        mel = mel[None]

    # skip encoder forward pass if already-encoded audio features were given
    if mel.shape[-2:] != (model.dims.n_audio_ctx, model.dims.n_audio_state):
        mel = model.encoder(mel)

    # forward pass using a single token, startoftranscript
    n_audio = mel.shape[0]
    x = mx.array([[tokenizer.sot]] * n_audio)  # [n_audio, 1]
    logits = model.logits(x, mel)[:, 0]

    # collect detected languages; suppress all non-language tokens
    mask = mx.full(logits.shape[-1], -mx.inf, dtype=mx.float32)
    mask[list(tokenizer.all_language_tokens)] = 0.0
    logits += mask
    language_tokens = mx.argmax(logits, axis=-1)
    language_token_probs = mx.softmax(logits, axis=-1)
    language_token_probs = np.array(language_token_probs)
    language_probs = [
        {
            c: language_token_probs[i, j].item()
            for j, c in zip(tokenizer.all_language_tokens, tokenizer.all_language_codes)
        }
        for i in range(n_audio)
    ]

    if single:
        language_tokens = language_tokens[0]
        language_probs = language_probs[0]

    return language_tokens, language_probs


@dataclass(frozen=True)
class DecodingOptions:
    # whether to perform X->X "transcribe" or X->English "translate"
    task: str = "transcribe"

    # language that the audio is in; uses detected language if None
    language: Optional[str] = None

    # sampling-related options
    temperature: float = 0.0
    sample_len: Optional[int] = None  # maximum number of tokens to sample
    best_of: Optional[int] = None  # number of independent sample trajectories, if t > 0
    beam_size: Optional[int] = None  # number of beams in beam search, if t == 0
    patience: Optional[float] = None  # patience in beam search (arxiv:2204.05424)

    # "alpha" in Google NMT, or None for length norm, when ranking generations
    # to select which to return among the beams or best-of-N samples
    length_penalty: Optional[float] = None

    # text or tokens to feed as the prompt or the prefix; for more info:
    # https://github.com/openai/whisper/discussions/117#discussioncomment-3727051
    prompt: Optional[Union[str, List[int]]] = None  # for the previous context
    prefix: Optional[Union[str, List[int]]] = None  # to prefix the current context

    # list of tokens ids (or comma-separated token ids) to suppress
    # "-1" will suppress a set of symbols as defined in `tokenizer.non_speech_tokens()`
    suppress_tokens: Optional[Union[str, Iterable[int]]] = "-1"
    suppress_blank: bool = True  # this will suppress blank outputs

    # timestamp sampling options
    without_timestamps: bool = False  # use <|notimestamps|> to sample text tokens only
    max_initial_timestamp: Optional[float] = 1.0

    # implementation details
    fp16: bool = True  # use fp16 for most of the calculation


@dataclass(frozen=True)
class DecodingResult:
    audio_features: mx.array
    language: str
    language_probs: Optional[Dict[str, float]] = None
    tokens: List[int] = field(default_factory=list)
    text: str = ""
    avg_logprob: float = np.nan
    no_speech_prob: float = np.nan
    temperature: float = np.nan
    compression_ratio: float = np.nan


class Inference:
    def __init__(self, model: "Whisper"):
        self.model: "Whisper" = model
        self.kv_cache = None

    def logits(self, tokens: mx.array, audio_features: mx.array) -> mx.array:
        """Perform a forward pass on the decoder and return per-token logits"""
        logits, self.kv_cache, _ = self.model.decoder(
            tokens, audio_features, kv_cache=self.kv_cache
        )
        return logits.astype(mx.float32)

    def rearrange_kv_cache(self, source_indices):
        """Update the key-value cache according to the updated beams"""
        # update the key/value cache to contain the selected sequences
        if source_indices != list(range(len(source_indices))):
            self.kv_cache = tree_map(lambda x: x[source_indices], self.kv_cache)

    def reset(self):
        self.kv_cache = None


class SequenceRanker:
    def rank(
        self, tokens: List[List[mx.array]], sum_logprobs: List[List[float]]
    ) -> List[int]:
        """
        Given a list of groups of samples and their cumulative log probabilities,
        return the indices of the samples in each group to select as the final result
        """
        raise NotImplementedError


class MaximumLikelihoodRanker(SequenceRanker):
    """
    Select the sample with the highest log probabilities, penalized using either
    a simple length normalization or Google NMT paper's length penalty
    """

    def __init__(self, length_penalty: Optional[float]):
        self.length_penalty = length_penalty

    def rank(self, tokens: List[List[List[int]]], sum_logprobs: List[List[float]]):
        def scores(logprobs, lengths):
            result = []
            for logprob, length in zip(logprobs, lengths):
                if self.length_penalty is None:
                    penalty = length
                else:
                    # from the Google NMT paper
                    penalty = ((5 + length) / 6) ** self.length_penalty
                result.append(logprob / penalty)
            return result

        # get the sequence with the highest score
        lengths = [[len(t) for t in s] for s in tokens]
        return [np.argmax(scores(p, l)) for p, l in zip(sum_logprobs, lengths)]


class TokenDecoder:
    def reset(self):
        """Initialize any stateful variables for decoding a new sequence"""

    def update(
        self, tokens: mx.array, logits: mx.array, sum_logprobs: mx.array
    ) -> Tuple[mx.array, bool, mx.array]:
        """Specify how to select the next token, based on the current trace and logits

        Parameters
        ----------
        tokens : mx.array, shape = (n_batch, current_sequence_length)
            all tokens in the context so far, including the prefix and sot_sequence tokens

        logits : mx.array, shape = (n_batch, vocab_size)
            per-token logits of the probability distribution at the current step

        sum_logprobs : mx.array, shape = (n_batch)
            cumulative log probabilities for each sequence

        Returns
        -------
        tokens : mx.array, shape = (n_batch, current_sequence_length + 1)
            the tokens, appended with the selected next token

        completed : bool
            True if all sequences has reached the end of text

        sum_logprobs: mx.array, shape = (n_batch)
            updated cumulative log probabilities for each sequence

        """
        raise NotImplementedError

    def finalize(
        self, tokens: mx.array, sum_logprobs: mx.array
    ) -> Tuple[Sequence[Sequence[mx.array]], List[List[float]]]:
        """Finalize search and return the final candidate sequences

        Parameters
        ----------
        tokens : mx.array, shape = (n_audio, n_group, current_sequence_length)
            all tokens in the context so far, including the prefix and sot_sequence

        sum_logprobs : mx.array, shape = (n_audio, n_group)
            cumulative log probabilities for each sequence

        Returns
        -------
        tokens : Sequence[Sequence[mx.array]], length = n_audio
            sequence of mx.arrays containing candidate token sequences, for each audio input

        sum_logprobs : List[List[float]], length = n_audio
            sequence of cumulative log probabilities corresponding to the above

        """
        raise NotImplementedError


@mx.compile
def categorical(logits, temp):
    return mx.random.categorical(logits / temp)


class GreedyDecoder(TokenDecoder):
    def __init__(self, temperature: float, eot: int):
        self.temperature = temperature
        self.eot = eot

    def update(
        self, tokens: mx.array, logits: mx.array, sum_logprobs: mx.array
    ) -> Tuple[mx.array, bool, mx.array]:
        if self.temperature == 0:
            next_tokens = logits.argmax(axis=-1)
        else:
            next_tokens = categorical(logits, self.temperature)

        logprobs = logits - mx.logsumexp(logits, axis=-1)

        current_logprobs = logprobs[mx.arange(logprobs.shape[0]), next_tokens]
        sum_logprobs += current_logprobs * (tokens[:, -1] != self.eot)

        eot_mask = tokens[:, -1] == self.eot
        next_tokens = next_tokens * (1 - eot_mask) + self.eot * eot_mask
        tokens = mx.concatenate([tokens, next_tokens[:, None]], axis=-1)

        completed = mx.all(tokens[:, -1] == self.eot)
        return tokens, completed, sum_logprobs

    def finalize(self, tokens: mx.array, sum_logprobs: mx.array):
        # make sure each sequence has at least one EOT token at the end
        tokens = mx.pad(tokens, [(0, 0), (0, 0), (0, 1)], constant_values=self.eot)
        return tokens, sum_logprobs


class LogitFilter:
    def apply(self, logits: mx.array, tokens: mx.array) -> mx.array:
        """Apply any filtering or masking to logits

        Parameters
        ----------
        logits : mx.array, shape = (n_batch, vocab_size)
            per-token logits of the probability distribution at the current step

        tokens : mx.array, shape = (n_batch, current_sequence_length)
            all tokens in the context so far, including the prefix and sot_sequence tokens

        """
        raise NotImplementedError


class SuppressBlank(LogitFilter):
    def __init__(self, tokenizer: Tokenizer, sample_begin: int, n_vocab: int):
        self.sample_begin = sample_begin
        mask = np.zeros(n_vocab, np.float32)
        mask[tokenizer.encode(" ") + [tokenizer.eot]] = -np.inf
        self.mask = mx.array(mask)

    def apply(self, logits: mx.array, tokens: mx.array) -> mx.array:
        if tokens.shape[1] == self.sample_begin:
            return logits + self.mask
        return logits


class SuppressTokens(LogitFilter):
    def __init__(self, suppress_tokens: Sequence[int], n_vocab: int):
        mask = np.zeros(n_vocab, np.float32)
        mask[list(suppress_tokens)] = -np.inf
        self.mask = mx.array(mask)

    def apply(self, logits: mx.array, tokens: mx.array) -> mx.array:
        return logits + self.mask


class ApplyTimestampRules(LogitFilter):
    def __init__(
        self,
        tokenizer: Tokenizer,
        sample_begin: int,
        max_initial_timestamp_index: Optional[int],
    ):
        self.tokenizer = tokenizer
        self.sample_begin = sample_begin
        self.max_initial_timestamp_index = max_initial_timestamp_index

    def apply(self, logits: mx.array, tokens: mx.array) -> mx.array:
        mask = np.zeros(logits.shape, np.float32)
        # suppress <|notimestamps|> which is handled by without_timestamps
        if self.tokenizer.no_timestamps is not None:
            mask[:, self.tokenizer.no_timestamps] = -np.inf

        ## timestamps have to appear in pairs, except directly before EOT; mask logits accordingly
        tokens = tokens.tolist()
        for k in range(len(tokens)):
            seq = tokens[k][self.sample_begin :]
            last_was_timestamp = (
                len(seq) >= 1 and seq[-1] >= self.tokenizer.timestamp_begin
            )
            penultimate_was_timestamp = (
                len(seq) < 2 or seq[-2] >= self.tokenizer.timestamp_begin
            )

            if last_was_timestamp:
                if penultimate_was_timestamp:  # has to be non-timestamp
                    mask[k, self.tokenizer.timestamp_begin :] = -np.inf
                else:  # cannot be normal text tokens
                    mask[k, : self.tokenizer.eot] = -np.inf

            timestamps = [
                i for i, v in enumerate(seq) if v > self.tokenizer.timestamp_begin
            ]
            if len(timestamps) > 0:
                # timestamps shouldn't decrease; forbid timestamp tokens smaller than the last
                # also force each segment to have a nonzero length, to prevent infinite looping
                last_timestamp = timestamps[-1]
                if not last_timestamp or penultimate_was_timestamp:
                    last_timestamp += 1
                mask[k, self.tokenizer.timestamp_begin : last_timestamp] = -np.inf

        if len(tokens[0]) == self.sample_begin:
            # suppress generating non-timestamp tokens at the beginning
            mask[:, : self.tokenizer.timestamp_begin] = -np.inf

            # apply the `max_initial_timestamp` option
            if self.max_initial_timestamp_index is not None:
                last_allowed = (
                    self.tokenizer.timestamp_begin + self.max_initial_timestamp_index
                )
                mask[:, last_allowed + 1 :] = -np.inf

        # if sum of probability over timestamps is above any other token, sample timestamp
        mask = mx.array(mask)
        logprobs = logits - mx.logsumexp(logits, axis=-1)
        timestamp_logprob = logprobs[:, self.tokenizer.timestamp_begin :].logsumexp(
            axis=-1, keepdims=True
        )
        max_text_token_logprob = logprobs[:, : self.tokenizer.timestamp_begin].max(
            axis=-1, keepdims=True
        )
        mask[:, : self.tokenizer.timestamp_begin] = mx.where(
            timestamp_logprob > max_text_token_logprob,
            -mx.inf,
            mask[:, : self.tokenizer.timestamp_begin],
        )
        return logits + mask


class DecodingTask:
    inference: Inference
    sequence_ranker: SequenceRanker
    decoder: TokenDecoder
    logit_filters: List[LogitFilter]

    def __init__(self, model: "Whisper", options: DecodingOptions):
        self.model = model

        language = options.language or "en"
        tokenizer = get_tokenizer(
            model.is_multilingual,
            num_languages=model.num_languages,
            language=language,
            task=options.task,
        )
        self.tokenizer: Tokenizer = tokenizer
        self.options: DecodingOptions = self._verify_options(options)

        self.n_group: int = options.beam_size or options.best_of or 1
        self.n_ctx: int = model.dims.n_text_ctx
        self.sample_len: int = options.sample_len or model.dims.n_text_ctx // 2

        self.sot_sequence: Tuple[int] = tokenizer.sot_sequence
        if self.options.without_timestamps:
            self.sot_sequence = tokenizer.sot_sequence_including_notimestamps

        self.initial_tokens: Tuple[int] = self._get_initial_tokens()
        self.sample_begin: int = len(self.initial_tokens)
        self.sot_index: int = self.initial_tokens.index(tokenizer.sot)

        # inference: implements the forward pass through the decoder, including kv caching
        self.inference = Inference(model)

        # sequence ranker: implements how to rank a group of sampled sequences
        self.sequence_ranker = MaximumLikelihoodRanker(options.length_penalty)

        # decoder: implements how to select the next tokens, given the autoregressive distribution
        if options.beam_size is not None:
            raise NotImplementedError("Beam search decoder is not yet implemented")
        else:
            self.decoder = GreedyDecoder(options.temperature, tokenizer.eot)

        # logit filters: applies various rules to suppress or penalize certain tokens
        self.logit_filters = []
        if self.options.suppress_blank:
            self.logit_filters.append(
                SuppressBlank(self.tokenizer, self.sample_begin, model.dims.n_vocab)
            )
        if self.options.suppress_tokens:
            self.logit_filters.append(
                SuppressTokens(self._get_suppress_tokens(), model.dims.n_vocab)
            )

        if not options.without_timestamps:
            precision = CHUNK_LENGTH / model.dims.n_audio_ctx  # usually 0.02 seconds
            max_initial_timestamp_index = None
            if options.max_initial_timestamp:
                max_initial_timestamp_index = round(
                    self.options.max_initial_timestamp / precision
                )
            self.logit_filters.append(
                ApplyTimestampRules(
                    tokenizer, self.sample_begin, max_initial_timestamp_index
                )
            )

    def _verify_options(self, options: DecodingOptions) -> DecodingOptions:
        if options.beam_size is not None and options.best_of is not None:
            raise ValueError("beam_size and best_of can't be given together")
        if options.temperature == 0:
            if options.best_of is not None:
                raise ValueError("best_of with greedy sampling (T=0) is not compatible")
        if options.patience is not None and options.beam_size is None:
            raise ValueError("patience requires beam_size to be given")
        if options.length_penalty is not None and not (
            0 <= options.length_penalty <= 1
        ):
            raise ValueError("length_penalty (alpha) should be a value between 0 and 1")

        return options

    def _get_initial_tokens(self) -> Tuple[int]:
        tokens = list(self.sot_sequence)

        if prefix := self.options.prefix:
            prefix_tokens = (
                self.tokenizer.encode(" " + prefix.strip())
                if isinstance(prefix, str)
                else prefix
            )
            if self.sample_len is not None:
                max_prefix_len = self.n_ctx // 2 - self.sample_len
                prefix_tokens = prefix_tokens[-max_prefix_len:]
            tokens = tokens + prefix_tokens

        if prompt := self.options.prompt:
            prompt_tokens = (
                self.tokenizer.encode(" " + prompt.strip())
                if isinstance(prompt, str)
                else prompt
            )
            tokens = (
                [self.tokenizer.sot_prev]
                + prompt_tokens[-(self.n_ctx // 2 - 1) :]
                + tokens
            )

        return tuple(tokens)

    def _get_suppress_tokens(self) -> Tuple[int]:
        suppress_tokens = self.options.suppress_tokens

        if isinstance(suppress_tokens, str):
            suppress_tokens = [int(t) for t in suppress_tokens.split(",")]

        if -1 in suppress_tokens:
            suppress_tokens = [t for t in suppress_tokens if t >= 0]
            suppress_tokens.extend(self.tokenizer.non_speech_tokens)
        elif suppress_tokens is None or len(suppress_tokens) == 0:
            suppress_tokens = []  # interpret empty string as an empty list
        else:
            assert isinstance(suppress_tokens, list), "suppress_tokens must be a list"

        suppress_tokens.extend(
            [
                self.tokenizer.transcribe,
                self.tokenizer.translate,
                self.tokenizer.sot,
                self.tokenizer.sot_prev,
                self.tokenizer.sot_lm,
            ]
        )
        if self.tokenizer.no_speech is not None:
            # no-speech probability is collected separately
            suppress_tokens.append(self.tokenizer.no_speech)

        return tuple(sorted(set(suppress_tokens)))

    def _get_audio_features(self, mel: mx.array):
        if self.options.fp16:
            mel = mel.astype(mx.float16)

        if mel.shape[-2:] == (
            self.model.dims.n_audio_ctx,
            self.model.dims.n_audio_state,
        ):
            # encoded audio features are given; skip audio encoding
            audio_features = mel
        else:
            audio_features = self.model.encoder(mel)

        if audio_features.dtype != (mx.float16 if self.options.fp16 else mx.float32):
            raise TypeError(
                f"audio_features has an incorrect dtype: {audio_features.dtype}"
            )

        return audio_features

    def _detect_language(self, audio_features: mx.array, tokens: np.array):
        languages = [self.options.language] * audio_features.shape[0]
        lang_probs = None

        if self.options.language is None or self.options.task == "lang_id":
            lang_tokens, lang_probs = self.model.detect_language(
                audio_features, self.tokenizer
            )
            languages = [max(probs, key=probs.get) for probs in lang_probs]
            if self.options.language is None:
                # write language tokens
                tokens[:, self.sot_index + 1] = np.array(lang_tokens)

        return languages, lang_probs

    def _main_loop(self, audio_features: mx.array, tokens: mx.array):
        n_batch = tokens.shape[0]
        sum_logprobs = mx.zeros(n_batch)

        def _step(inputs, audio_features, tokens, sum_logprobs):
            pre_logits = self.inference.logits(inputs, audio_features)

            # consider the logits at the last token only
            logits = pre_logits[:, -1]

            # apply the logit filters, e.g. for suppressing or applying penalty to
            for logit_filter in self.logit_filters:
                logits = logit_filter.apply(logits, tokens)

            # expand the tokens tensor with the selected next tokens
            tokens, completed, sum_logprobs = self.decoder.update(
                tokens, logits, sum_logprobs
            )
            return tokens, completed, sum_logprobs, pre_logits

        tokens, completed, sum_logprobs, pre_logits = _step(
            tokens, audio_features, tokens, sum_logprobs
        )
        if self.tokenizer.no_speech is not None:  # compute no_speech_probs
            probs_at_sot = mx.softmax(pre_logits[:, self.sot_index], axis=-1)
            no_speech_probs = probs_at_sot[:, self.tokenizer.no_speech]
        else:
            no_speech_probs = mx.full(n_batch, mx.nan)
        mx.async_eval(completed, tokens, sum_logprobs, no_speech_probs)

        for i in range(1, self.sample_len):
            inputs = tokens[:, -1:]
            if tokens.shape[-1] > self.n_ctx:
                break
            next_tokens, next_completed, next_sum_logprobs, _ = _step(
                inputs, audio_features, tokens, sum_logprobs
            )
            mx.async_eval(next_completed, next_tokens, next_sum_logprobs)
            if completed:
                break
            tokens = next_tokens
            completed = next_completed
            sum_logprobs = next_sum_logprobs

        return tokens, sum_logprobs, no_speech_probs

    def run(self, mel: mx.array) -> List[DecodingResult]:
        self.inference.reset()
        self.decoder.reset()
        tokenizer: Tokenizer = self.tokenizer
        n_audio: int = mel.shape[0]

        audio_features: mx.array = self._get_audio_features(mel)  # encoder forward pass
        tokens: mx.array = mx.array(self.initial_tokens)
        tokens = mx.broadcast_to(tokens, (n_audio, len(self.initial_tokens)))

        # detect language if requested, overwriting the language token
        languages, language_probs = self._detect_language(audio_features, tokens)
        if self.options.task == "lang_id":
            return [
                DecodingResult(
                    audio_features=features, language=language, language_probs=probs
                )
                for features, language, probs in zip(
                    audio_features, languages, language_probs
                )
            ]

        # repeat tokens by the group size, for beam search or best-of-n sampling
        if self.n_group > 1:
            tokens = tokens[:, None, :]
            tokens = mx.broadcast_to(
                tokens, [n_audio, self.n_group, len(self.initial_tokens)]
            )
            tokens = tokens.reshape(
                tokens, (n_audio * self.n_group, len(self.initial_tokens))
            )

        # call the main sampling loop
        tokens, sum_logprobs, no_speech_probs = self._main_loop(audio_features, tokens)

        # reshape the tensors to have (n_audio, n_group) as the first two dimensions
        audio_features = audio_features[:: self.n_group]
        no_speech_probs = no_speech_probs[:: self.n_group]
        assert audio_features.shape[0] == len(no_speech_probs) == n_audio

        tokens = tokens.reshape(n_audio, self.n_group, -1)
        sum_logprobs = sum_logprobs.reshape(n_audio, self.n_group)

        # get the final candidates for each group, and slice between the first sampled token and EOT
        tokens, sum_logprobs = self.decoder.finalize(tokens, sum_logprobs)
        tokens = tokens[..., self.sample_begin :]

        # eval and convert to list
        mx.eval(tokens, sum_logprobs, no_speech_probs)
        tokens = tokens.tolist()
        sum_logprobs = sum_logprobs.tolist()
        no_speech_probs = no_speech_probs.tolist()
        tokens = [[t[: t.index(tokenizer.eot)] for t in s] for s in tokens]

        # select the top-ranked sample in each group
        selected = self.sequence_ranker.rank(tokens, sum_logprobs)
        tokens: List[List[int]] = [t[i] for i, t in zip(selected, tokens)]
        texts: List[str] = [tokenizer.decode(t).strip() for t in tokens]

        sum_logprobs: List[float] = [lp[i] for i, lp in zip(selected, sum_logprobs)]
        avg_logprobs: List[float] = [
            lp / (len(t) + 1) for t, lp in zip(tokens, sum_logprobs)
        ]

        fields = (
            texts,
            languages,
            tokens,
            audio_features,
            avg_logprobs,
            no_speech_probs,
        )
        if len(set(map(len, fields))) != 1:
            raise RuntimeError(f"inconsistent result lengths: {list(map(len, fields))}")

        return [
            DecodingResult(
                audio_features=features,
                language=language,
                tokens=tokens,
                text=text,
                avg_logprob=avg_logprob,
                no_speech_prob=no_speech_prob,
                temperature=self.options.temperature,
                compression_ratio=compression_ratio(text),
            )
            for text, language, tokens, features, avg_logprob, no_speech_prob in zip(
                *fields
            )
        ]


def decode(
    model: "Whisper",
    mel: mx.array,
    options: DecodingOptions = DecodingOptions(),
    **kwargs,
) -> Union[DecodingResult, List[DecodingResult]]:
    """
    Performs decoding of 30-second audio segment(s), provided as Mel spectrogram(s).

    Parameters
    ----------
    model: Whisper
        the Whisper model instance

    mel: mx.array, shape = (80, 3000) or (*, 80, 3000)
        An array containing the Mel spectrogram(s)

    options: DecodingOptions
        A dataclass that contains all necessary options for decoding 30-second segments

    Returns
    -------
    result: Union[DecodingResult, List[DecodingResult]]
        The result(s) of decoding contained in `DecodingResult` dataclass instance(s)
    """
    if single := mel.ndim == 2:
        mel = mel[None]

    if kwargs:
        options = replace(options, **kwargs)

    result = DecodingTask(model, options).run(mel)
    return result[0] if single else result

>>>> whisper/mlx_whisper/writers.py
# Copyright © 2024 Apple Inc.

import json
import pathlib
import re
from typing import Callable, List, Optional, TextIO


def format_timestamp(
    seconds: float, always_include_hours: bool = False, decimal_marker: str = "."
):
    assert seconds >= 0, "non-negative timestamp expected"
    milliseconds = round(seconds * 1000.0)

    hours = milliseconds // 3_600_000
    milliseconds -= hours * 3_600_000

    minutes = milliseconds // 60_000
    milliseconds -= minutes * 60_000

    seconds = milliseconds // 1_000
    milliseconds -= seconds * 1_000

    hours_marker = f"{hours:02d}:" if always_include_hours or hours > 0 else ""
    return (
        f"{hours_marker}{minutes:02d}:{seconds:02d}{decimal_marker}{milliseconds:03d}"
    )


def get_start(segments: List[dict]) -> Optional[float]:
    return next(
        (w["start"] for s in segments for w in s["words"]),
        segments[0]["start"] if segments else None,
    )


class ResultWriter:
    extension: str

    def __init__(self, output_dir: str):
        self.output_dir = output_dir

    def __call__(
        self, result: dict, output_name: str, options: Optional[dict] = None, **kwargs
    ):
        output_path = (pathlib.Path(self.output_dir) / output_name).with_suffix(
            f".{self.extension}"
        )

        with output_path.open("wt", encoding="utf-8") as f:
            self.write_result(result, file=f, options=options, **kwargs)

    def write_result(
        self, result: dict, file: TextIO, options: Optional[dict] = None, **kwargs
    ):
        raise NotImplementedError


class WriteTXT(ResultWriter):
    extension: str = "txt"

    def write_result(
        self, result: dict, file: TextIO, options: Optional[dict] = None, **kwargs
    ):
        for segment in result["segments"]:
            print(segment["text"].strip(), file=file, flush=True)


class SubtitlesWriter(ResultWriter):
    always_include_hours: bool
    decimal_marker: str

    def iterate_result(
        self,
        result: dict,
        options: Optional[dict] = None,
        *,
        max_line_width: Optional[int] = None,
        max_line_count: Optional[int] = None,
        highlight_words: bool = False,
        max_words_per_line: Optional[int] = None,
    ):
        options = options or {}
        max_line_width = max_line_width or options.get("max_line_width")
        max_line_count = max_line_count or options.get("max_line_count")
        highlight_words = highlight_words or options.get("highlight_words", False)
        max_words_per_line = max_words_per_line or options.get("max_words_per_line")
        preserve_segments = max_line_count is None or max_line_width is None
        max_line_width = max_line_width or 1000
        max_words_per_line = max_words_per_line or 1000

        def iterate_subtitles():
            line_len = 0
            line_count = 1
            # the next subtitle to yield (a list of word timings with whitespace)
            subtitle: List[dict] = []
            last: float = get_start(result["segments"]) or 0.0
            for segment in result["segments"]:
                chunk_index = 0
                words_count = max_words_per_line
                while chunk_index < len(segment["words"]):
                    remaining_words = len(segment["words"]) - chunk_index
                    if max_words_per_line > len(segment["words"]) - chunk_index:
                        words_count = remaining_words
                    for i, original_timing in enumerate(
                        segment["words"][chunk_index : chunk_index + words_count]
                    ):
                        timing = original_timing.copy()
                        long_pause = (
                            not preserve_segments and timing["start"] - last > 3.0
                        )
                        has_room = line_len + len(timing["word"]) <= max_line_width
                        seg_break = i == 0 and len(subtitle) > 0 and preserve_segments
                        if (
                            line_len > 0
                            and has_room
                            and not long_pause
                            and not seg_break
                        ):
                            # line continuation
                            line_len += len(timing["word"])
                        else:
                            # new line
                            timing["word"] = timing["word"].strip()
                            if (
                                len(subtitle) > 0
                                and max_line_count is not None
                                and (long_pause or line_count >= max_line_count)
                                or seg_break
                            ):
                                # subtitle break
                                yield subtitle
                                subtitle = []
                                line_count = 1
                            elif line_len > 0:
                                # line break
                                line_count += 1
                                timing["word"] = "\n" + timing["word"]
                            line_len = len(timing["word"].strip())
                        subtitle.append(timing)
                        last = timing["start"]
                    chunk_index += max_words_per_line
            if len(subtitle) > 0:
                yield subtitle

        if len(result["segments"]) > 0 and "words" in result["segments"][0]:
            for subtitle in iterate_subtitles():
                subtitle_start = self.format_timestamp(subtitle[0]["start"])
                subtitle_end = self.format_timestamp(subtitle[-1]["end"])
                subtitle_text = "".join([word["word"] for word in subtitle])
                if highlight_words:
                    last = subtitle_start
                    all_words = [timing["word"] for timing in subtitle]
                    for i, this_word in enumerate(subtitle):
                        start = self.format_timestamp(this_word["start"])
                        end = self.format_timestamp(this_word["end"])
                        if last != start:
                            yield last, start, subtitle_text

                        yield start, end, "".join(
                            [
                                (
                                    re.sub(r"^(\s*)(.*)$", r"\1<u>\2</u>", word)
                                    if j == i
                                    else word
                                )
                                for j, word in enumerate(all_words)
                            ]
                        )
                        last = end
                else:
                    yield subtitle_start, subtitle_end, subtitle_text
        else:
            for segment in result["segments"]:
                segment_start = self.format_timestamp(segment["start"])
                segment_end = self.format_timestamp(segment["end"])
                segment_text = segment["text"].strip().replace("-->", "->")
                yield segment_start, segment_end, segment_text

    def format_timestamp(self, seconds: float):
        return format_timestamp(
            seconds=seconds,
            always_include_hours=self.always_include_hours,
            decimal_marker=self.decimal_marker,
        )


class WriteVTT(SubtitlesWriter):
    extension: str = "vtt"
    always_include_hours: bool = False
    decimal_marker: str = "."

    def write_result(
        self, result: dict, file: TextIO, options: Optional[dict] = None, **kwargs
    ):
        print("WEBVTT\n", file=file)
        for start, end, text in self.iterate_result(result, options, **kwargs):
            print(f"{start} --> {end}\n{text}\n", file=file, flush=True)


class WriteSRT(SubtitlesWriter):
    extension: str = "srt"
    always_include_hours: bool = True
    decimal_marker: str = ","

    def write_result(
        self, result: dict, file: TextIO, options: Optional[dict] = None, **kwargs
    ):
        for i, (start, end, text) in enumerate(
            self.iterate_result(result, options, **kwargs), start=1
        ):
            print(f"{i}\n{start} --> {end}\n{text}\n", file=file, flush=True)


class WriteTSV(ResultWriter):
    """
    Write a transcript to a file in TSV (tab-separated values) format containing lines like:
    <start time in integer milliseconds>\t<end time in integer milliseconds>\t<transcript text>

    Using integer milliseconds as start and end times means there's no chance of interference from
    an environment setting a language encoding that causes the decimal in a floating point number
    to appear as a comma; also is faster and more efficient to parse & store, e.g., in C++.
    """

    extension: str = "tsv"

    def write_result(
        self, result: dict, file: TextIO, options: Optional[dict] = None, **kwargs
    ):
        print("start", "end", "text", sep="\t", file=file)
        for segment in result["segments"]:
            print(round(1000 * segment["start"]), file=file, end="\t")
            print(round(1000 * segment["end"]), file=file, end="\t")
            print(segment["text"].strip().replace("\t", " "), file=file, flush=True)


class WriteJSON(ResultWriter):
    extension: str = "json"

    def write_result(
        self, result: dict, file: TextIO, options: Optional[dict] = None, **kwargs
    ):
        json.dump(result, file, ensure_ascii=False)


def get_writer(
    output_format: str, output_dir: str
) -> Callable[[dict, TextIO, dict], None]:
    writers = {
        "txt": WriteTXT,
        "vtt": WriteVTT,
        "srt": WriteSRT,
        "tsv": WriteTSV,
        "json": WriteJSON,
    }

    if output_format == "all":
        all_writers = [writer(output_dir) for writer in writers.values()]

        def write_all(
            result: dict, file: TextIO, options: Optional[dict] = None, **kwargs
        ):
            for writer in all_writers:
                writer(result, file, options, **kwargs)

        return write_all

    return writers[output_format](output_dir)

>>>> whisper/mlx_whisper/_version.py
# Copyright © 2023-2024 Apple Inc.

__version__ = "0.4.1"

>>>> whisper/benchmark.py
# Copyright © 2023-2024 Apple Inc.
import argparse
import os
import time

import mlx.core as mx
from mlx_whisper import audio, decoding, load_models, transcribe

audio_file = "mlx_whisper/assets/ls_test.flac"


def parse_arguments():
    parser = argparse.ArgumentParser(description="Benchmark script.")
    parser.add_argument(
        "--mlx-dir",
        type=str,
        default="mlx_models",
        help="The folder of MLX models",
    )
    parser.add_argument(
        "--all",
        action="store_true",
        help="Use all available models, i.e. tiny,small,medium,large-v3",
    )
    parser.add_argument(
        "-m",
        "--models",
        type=str,
        help="Specify models as a comma-separated list (e.g., tiny,small,medium)",
    )
    return parser.parse_args()


def timer(fn, *args):
    for _ in range(5):
        fn(*args)

    num_its = 10

    tic = time.perf_counter()
    for _ in range(num_its):
        fn(*args)
    toc = time.perf_counter()
    return (toc - tic) / num_its


def feats(n_mels: int = 80):
    data = audio.load_audio(audio_file)
    data = audio.pad_or_trim(data)
    mels = audio.log_mel_spectrogram(data, n_mels)
    mx.eval(mels)
    return mels


def model_forward(model, mels, tokens):
    logits = model(mels, tokens)
    mx.eval(logits)
    return logits


def decode(model, mels):
    return decoding.decode(model, mels)


def everything(model_path):
    return transcribe(audio_file, path_or_hf_repo=model_path)


if __name__ == "__main__":
    args = parse_arguments()
    if args.all:
        models = ["tiny", "small", "medium", "large-v3"]
    elif args.models:
        models = args.models.split(",")
    else:
        models = ["tiny"]

    print("Selected models:", models)

    feat_time = timer(feats)
    print(f"\nFeature time {feat_time:.3f}")

    for model_name in models:
        model_path = f"mlx-community/whisper-{model_name}-mlx"
        print(f"\nModel: {model_name.upper()}")
        tokens = mx.array(
            [
                50364,
                1396,
                264,
                665,
                5133,
                23109,
                25462,
                264,
                6582,
                293,
                750,
                632,
                42841,
                292,
                370,
                938,
                294,
                4054,
                293,
                12653,
                356,
                50620,
                50620,
                23563,
                322,
                3312,
                13,
                50680,
            ],
            mx.int32,
        )[None]
        model = load_models.load_model(path_or_hf_repo=model_path, dtype=mx.float16)
        mels = feats(model.dims.n_mels)[None].astype(mx.float16)
        model_forward_time = timer(model_forward, model, mels, tokens)
        print(f"Model forward time {model_forward_time:.3f}")
        decode_time = timer(decode, model, mels)
        print(f"Decode time {decode_time:.3f}")
        everything_time = timer(everything, model_path)
        print(f"Everything time {everything_time:.3f}")
        print(f"\n{'-----' * 10}\n")

>>>> cifar/main.py
import argparse
import time
from functools import partial

import mlx.core as mx
import mlx.nn as nn
import mlx.optimizers as optim
import resnet
from dataset import get_cifar10

parser = argparse.ArgumentParser(add_help=True)
parser.add_argument(
    "--arch",
    type=str,
    default="resnet20",
    choices=[f"resnet{d}" for d in [20, 32, 44, 56, 110, 1202]],
    help="model architecture",
)
parser.add_argument("--batch_size", type=int, default=256, help="batch size")
parser.add_argument("--epochs", type=int, default=30, help="number of epochs")
parser.add_argument("--lr", type=float, default=1e-3, help="learning rate")
parser.add_argument("--seed", type=int, default=0, help="random seed")
parser.add_argument("--cpu", action="store_true", help="use cpu only")


def print_zero(group, *args, **kwargs):
    if group.rank() != 0:
        return
    flush = kwargs.pop("flush", True)
    print(*args, **kwargs, flush=flush)


def eval_fn(model, inp, tgt):
    return mx.mean(mx.argmax(model(inp), axis=1) == tgt)


def train_epoch(model, train_iter, optimizer, epoch):
    def train_step(model, inp, tgt):
        output = model(inp)
        loss = mx.mean(nn.losses.cross_entropy(output, tgt))
        acc = mx.mean(mx.argmax(output, axis=1) == tgt)
        return loss, acc

    world = mx.distributed.init()
    losses = 0
    accuracies = 0
    samples_per_sec = 0
    count = 0

    def average_stats(stats, count):
        if world.size() == 1:
            return [s / count for s in stats]

        with mx.stream(mx.cpu):
            stats = mx.distributed.all_sum(mx.array(stats))
            count = mx.distributed.all_sum(count)
            return (stats / count).tolist()

    state = [model.state, optimizer.state]

    @partial(mx.compile, inputs=state, outputs=state)
    def step(inp, tgt):
        train_step_fn = nn.value_and_grad(model, train_step)
        (loss, acc), grads = train_step_fn(model, inp, tgt)
        grads = nn.utils.average_gradients(grads)
        optimizer.update(model, grads)
        return loss, acc

    for batch_counter, batch in enumerate(train_iter):
        x = mx.array(batch["image"])
        y = mx.array(batch["label"])
        tic = time.perf_counter()
        loss, acc = step(x, y)
        mx.eval(loss, acc, state)
        toc = time.perf_counter()
        losses += loss.item()
        accuracies += acc.item()
        samples_per_sec += x.shape[0] / (toc - tic)
        count += 1
        if batch_counter % 10 == 0:
            l, a, s = average_stats(
                [losses, accuracies, world.size() * samples_per_sec],
                count,
            )
            print_zero(
                world,
                " | ".join(
                    (
                        f"Epoch {epoch:02d} [{batch_counter:03d}]",
                        f"Train loss {l:.3f}",
                        f"Train acc {a:.3f}",
                        f"Throughput: {s:.2f} images/second",
                    )
                ),
            )

    return average_stats([losses, accuracies, world.size() * samples_per_sec], count)


def test_epoch(model, test_iter, epoch):
    accuracies = 0
    count = 0
    for batch_counter, batch in enumerate(test_iter):
        x = mx.array(batch["image"])
        y = mx.array(batch["label"])
        acc = eval_fn(model, x, y)
        accuracies += acc.item()
        count += 1

    with mx.stream(mx.cpu):
        accuracies = mx.distributed.all_sum(accuracies)
        count = mx.distributed.all_sum(count)
        return (accuracies / count).item()


def main(args):
    mx.random.seed(args.seed)

    # Initialize the distributed group and report the nodes that showed up
    world = mx.distributed.init()
    if world.size() > 1:
        print(f"Starting rank {world.rank()} of {world.size()}", flush=True)

    model = getattr(resnet, args.arch)()

    print_zero(world, f"Number of params: {model.num_params() / 1e6:0.04f} M")

    optimizer = optim.Adam(learning_rate=args.lr)

    train_data, test_data = get_cifar10(args.batch_size)
    for epoch in range(args.epochs):
        tr_loss, tr_acc, throughput = train_epoch(model, train_data, optimizer, epoch)
        print_zero(
            world,
            " | ".join(
                (
                    f"Epoch: {epoch}",
                    f"avg. Train loss {tr_loss:.3f}",
                    f"avg. Train acc {tr_acc:.3f}",
                    f"Throughput: {throughput:.2f} images/sec",
                )
            ),
        )

        test_acc = test_epoch(model, test_data, epoch)
        print_zero(world, f"Epoch: {epoch} | Test acc {test_acc:.3f}")

        train_data.reset()
        test_data.reset()


if __name__ == "__main__":
    args = parser.parse_args()
    if args.cpu:
        mx.set_default_device(mx.cpu)
    main(args)

>>>> cifar/resnet.py
"""
Implementation of ResNets for CIFAR-10 as per the original paper [https://arxiv.org/abs/1512.03385].
Configurations include ResNet-20, ResNet-32, ResNet-44, ResNet-56, ResNet-110, ResNet-1202.
"""

import mlx.core as mx
import mlx.nn as nn
from mlx.utils import tree_flatten

__all__ = [
    "ResNet",
    "resnet20",
    "resnet32",
    "resnet44",
    "resnet56",
    "resnet110",
    "resnet1202",
]


class ShortcutA(nn.Module):
    def __init__(self, dims):
        super().__init__()
        self.dims = dims

    def __call__(self, x):
        return mx.pad(
            x[:, ::2, ::2, :],
            pad_width=[(0, 0), (0, 0), (0, 0), (self.dims // 4, self.dims // 4)],
        )


class Block(nn.Module):
    """
    Implements a ResNet block with two convolutional layers and a skip connection.
    As per the paper, CIFAR-10 uses Shortcut type-A skip connections. (See paper for details)
    """

    def __init__(self, in_dims, dims, stride=1):
        super().__init__()

        self.conv1 = nn.Conv2d(
            in_dims, dims, kernel_size=3, stride=stride, padding=1, bias=False
        )
        self.bn1 = nn.BatchNorm(dims)

        self.conv2 = nn.Conv2d(
            dims, dims, kernel_size=3, stride=1, padding=1, bias=False
        )
        self.bn2 = nn.BatchNorm(dims)

        if stride != 1:
            self.shortcut = ShortcutA(dims)
        else:
            self.shortcut = None

    def __call__(self, x):
        out = nn.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        if self.shortcut is None:
            out += x
        else:
            out += self.shortcut(x)
        out = nn.relu(out)
        return out


class ResNet(nn.Module):
    """
    Creates a ResNet model for CIFAR-10, as specified in the original paper.
    """

    def __init__(self, block, num_blocks, num_classes=10):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm(16)

        self.layer1 = self._make_layer(block, 16, 16, num_blocks[0], stride=1)
        self.layer2 = self._make_layer(block, 16, 32, num_blocks[1], stride=2)
        self.layer3 = self._make_layer(block, 32, 64, num_blocks[2], stride=2)

        self.linear = nn.Linear(64, num_classes)

    def _make_layer(self, block, in_dims, dims, num_blocks, stride):
        strides = [stride] + [1] * (num_blocks - 1)
        layers = []
        for stride in strides:
            layers.append(block(in_dims, dims, stride))
            in_dims = dims
        return nn.Sequential(*layers)

    def num_params(self):
        nparams = sum(x.size for k, x in tree_flatten(self.parameters()))
        return nparams

    def __call__(self, x):
        x = nn.relu(self.bn1(self.conv1(x)))
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = mx.mean(x, axis=[1, 2]).reshape(x.shape[0], -1)
        x = self.linear(x)
        return x


def resnet20(**kwargs):
    return ResNet(Block, [3, 3, 3], **kwargs)


def resnet32(**kwargs):
    return ResNet(Block, [5, 5, 5], **kwargs)


def resnet44(**kwargs):
    return ResNet(Block, [7, 7, 7], **kwargs)


def resnet56(**kwargs):
    return ResNet(Block, [9, 9, 9], **kwargs)


def resnet110(**kwargs):
    return ResNet(Block, [18, 18, 18], **kwargs)


def resnet1202(**kwargs):
    return ResNet(Block, [200, 200, 200], **kwargs)

>>>> cifar/dataset.py
import mlx.core as mx
import numpy as np
from mlx.data.datasets import load_cifar10


def get_cifar10(batch_size, root=None):
    tr = load_cifar10(root=root)

    mean = np.array([0.485, 0.456, 0.406]).reshape((1, 1, 3))
    std = np.array([0.229, 0.224, 0.225]).reshape((1, 1, 3))

    def normalize(x):
        x = x.astype("float32") / 255.0
        return (x - mean) / std

    group = mx.distributed.init()

    tr_iter = (
        tr.shuffle()
        .partition_if(group.size() > 1, group.size(), group.rank())
        .to_stream()
        .image_random_h_flip("image", prob=0.5)
        .pad("image", 0, 4, 4, 0.0)
        .pad("image", 1, 4, 4, 0.0)
        .image_random_crop("image", 32, 32)
        .key_transform("image", normalize)
        .batch(batch_size)
        .prefetch(4, 4)
    )

    test = load_cifar10(root=root, train=False)
    test_iter = (
        test.to_stream()
        .partition_if(group.size() > 1, group.size(), group.rank())
        .key_transform("image", normalize)
        .batch(batch_size)
    )

    return tr_iter, test_iter

>>>> cifar/README.md
# CIFAR and ResNets

An example of training a ResNet on CIFAR-10 with MLX. Several ResNet
configurations in accordance with the original
[paper](https://arxiv.org/abs/1512.03385) are available. The example also
illustrates how to use [MLX Data](https://github.com/ml-explore/mlx-data) to
load the dataset.

## Pre-requisites

Install the dependencies:

```
pip install -r requirements.txt
```

## Running the example

Run the example with:

```
python main.py
```

By default the example runs on the GPU. To run on the CPU, use: 

```
python main.py --cpu
```

For all available options, run:

```
python main.py --help
```

## Results

After training with the default `resnet20` architecture for 30 epochs, you
should see the following results:

```
Epoch: 29 | avg. Train loss 0.294 | avg. Train acc 0.897 | Throughput: 270.81 images/sec
Epoch: 29 | Test acc 0.841
```

Note this was run on an M1 Macbook Pro with 16GB RAM.

At the time of writing, `mlx` doesn't have built-in learning rate schedules.
We intend to update this example once these features are added.

## Distributed training

The example also supports distributed data parallel training. You can launch a
distributed training as follows:

```shell
$ cat >hostfile.json
[
    {"ssh": "host-to-ssh-to", "ips": ["ip-to-bind-to"]},
    {"ssh": "host-to-ssh-to", "ips": ["ip-to-bind-to"]}
]
$ mlx.launch --verbose --hostfile hostfile.json main.py --batch 256 --epochs 5 --arch resnet20
```

>>>> gcn/main.py
import time
from argparse import ArgumentParser
from functools import partial

import mlx.core as mx
import mlx.nn as nn
import mlx.optimizers as optim
from datasets import load_data, train_val_test_mask
from mlx.utils import tree_flatten

from gcn import GCN


def loss_fn(y_hat, y, weight_decay=0.0, parameters=None):
    l = mx.mean(nn.losses.cross_entropy(y_hat, y))

    if weight_decay != 0.0:
        assert parameters != None, "Model parameters missing for L2 reg."

        l2_reg = sum(mx.sum(p[1] ** 2) for p in tree_flatten(parameters)).sqrt()
        return l + weight_decay * l2_reg
    return l


def eval_fn(x, y):
    return mx.mean(mx.argmax(x, axis=1) == y)


def forward_fn(gcn, x, adj, y, train_mask, weight_decay):
    y_hat = gcn(x, adj)
    loss = loss_fn(y_hat[train_mask], y[train_mask], weight_decay, gcn.parameters())
    return loss, y_hat


def main(args):
    # Data loading
    x, y, adj = load_data(args)
    train_mask, val_mask, test_mask = train_val_test_mask()

    gcn = GCN(
        x_dim=x.shape[-1],
        h_dim=args.hidden_dim,
        out_dim=args.nb_classes,
        nb_layers=args.nb_layers,
        dropout=args.dropout,
        bias=args.bias,
    )
    mx.eval(gcn.parameters())

    optimizer = optim.Adam(learning_rate=args.lr)

    state = [gcn.state, optimizer.state, mx.random.state]

    @partial(mx.compile, inputs=state, outputs=state)
    def step():
        loss_and_grad_fn = nn.value_and_grad(gcn, forward_fn)
        (loss, y_hat), grads = loss_and_grad_fn(
            gcn, x, adj, y, train_mask, args.weight_decay
        )
        optimizer.update(gcn, grads)
        return loss, y_hat

    best_val_loss = float("inf")
    cnt = 0

    # Training loop
    for epoch in range(args.epochs):
        tic = time.time()
        loss, y_hat = step()
        mx.eval(state)

        # Validation
        val_loss = loss_fn(y_hat[val_mask], y[val_mask])
        val_acc = eval_fn(y_hat[val_mask], y[val_mask])
        toc = time.time()

        # Early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            cnt = 0
        else:
            cnt += 1
            if cnt == args.patience:
                break

        print(
            " | ".join(
                [
                    f"Epoch: {epoch:3d}",
                    f"Train loss: {loss.item():.3f}",
                    f"Val loss: {val_loss.item():.3f}",
                    f"Val acc: {val_acc.item():.2f}",
                    f"Time: {1e3*(toc - tic):.3f} (ms)",
                ]
            )
        )

    # Test
    test_y_hat = gcn(x, adj)
    test_loss = loss_fn(y_hat[test_mask], y[test_mask])
    test_acc = eval_fn(y_hat[test_mask], y[test_mask])

    print(f"Test loss: {test_loss.item():.3f}  |  Test acc: {test_acc.item():.2f}")


if __name__ == "__main__":
    parser = ArgumentParser()
    parser.add_argument("--nodes_path", type=str, default="cora/cora.content")
    parser.add_argument("--edges_path", type=str, default="cora/cora.cites")
    parser.add_argument("--hidden_dim", type=int, default=20)
    parser.add_argument("--dropout", type=float, default=0.5)
    parser.add_argument("--nb_layers", type=int, default=2)
    parser.add_argument("--nb_classes", type=int, default=7)
    parser.add_argument("--bias", type=bool, default=True)
    parser.add_argument("--lr", type=float, default=0.001)
    parser.add_argument("--weight_decay", type=float, default=0.0)
    parser.add_argument("--patience", type=int, default=20)
    parser.add_argument("--epochs", type=int, default=100)
    args = parser.parse_args()

    main(args)

>>>> gcn/README.md
# Graph Convolutional Network

An example of [GCN](https://arxiv.org/abs/1609.02907) implementation with MLX.

### Install requirements
First, install the few dependencies with `pip`.

```
pip install -r requirements.txt
```

### Run
To try the model, just run the `main.py` file. This will download the Cora dataset, run the training and testing.

```
python main.py
```

>>>> gcn/datasets.py
import os
import tarfile

import mlx.core as mx
import numpy as np
import requests
import scipy.sparse as sparse

"""
Preprocessing follows the same implementation as in:
https://github.com/tkipf/gcn
https://github.com/senadkurtisi/pytorch-GCN/tree/main
"""


def download_cora():
    """Downloads the cora dataset into a local cora folder."""

    url = "https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz"
    extract_to = "."

    if os.path.exists(os.path.join(extract_to, "cora")):
        return

    response = requests.get(url, stream=True)
    if response.status_code == 200:
        file_path = os.path.join(extract_to, url.split("/")[-1])

        # Write the file to local disk
        with open(file_path, "wb") as file:
            file.write(response.raw.read())

        # Extract the .tgz file
        with tarfile.open(file_path, "r:gz") as tar:
            tar.extractall(path=extract_to)
        print(f"Cora dataset extracted to {extract_to}")

        os.remove(file_path)


def train_val_test_mask():
    """Splits the loaded dataset into train/validation/test sets."""

    train_set = mx.arange(140)
    validation_set = mx.arange(200, 500)
    test_set = mx.arange(500, 1500)

    return train_set, validation_set, test_set


def enumerate_labels(labels):
    """Converts the labels from the original
    string form to the integer [0:MaxLabels-1]
    """
    label_map = {v: e for e, v in enumerate(set(labels))}
    labels = np.array([label_map[label] for label in labels])
    return labels


def normalize_adjacency(adj):
    """Normalizes the adjacency matrix according to the
    paper by Kipf et al.
    https://arxiv.org/abs/1609.02907
    """
    adj = adj + sparse.eye(adj.shape[0])

    node_degrees = np.array(adj.sum(1))
    node_degrees = np.power(node_degrees, -0.5).flatten()
    node_degrees[np.isinf(node_degrees)] = 0.0
    node_degrees[np.isnan(node_degrees)] = 0.0
    degree_matrix = sparse.diags(node_degrees, dtype=np.float32)

    adj = degree_matrix @ adj @ degree_matrix
    return adj


def load_data(config):
    """Loads the Cora graph data into MLX array format."""
    print("Loading Cora dataset...")

    # Download dataset files
    download_cora()

    # Graph nodes
    raw_nodes_data = np.genfromtxt(config.nodes_path, dtype="str")
    raw_node_ids = raw_nodes_data[:, 0].astype(
        "int32"
    )  # unique identifier of each node
    raw_node_labels = raw_nodes_data[:, -1]
    labels_enumerated = enumerate_labels(raw_node_labels)  # target labels as integers
    node_features = sparse.csr_matrix(raw_nodes_data[:, 1:-1], dtype="float32")

    # Edges
    ids_ordered = {raw_id: order for order, raw_id in enumerate(raw_node_ids)}
    raw_edges_data = np.genfromtxt(config.edges_path, dtype="int32")
    edges_ordered = np.array(
        list(map(ids_ordered.get, raw_edges_data.flatten())), dtype="int32"
    ).reshape(raw_edges_data.shape)

    # Adjacency matrix
    adj = sparse.coo_matrix(
        (np.ones(edges_ordered.shape[0]), (edges_ordered[:, 0], edges_ordered[:, 1])),
        shape=(labels_enumerated.shape[0], labels_enumerated.shape[0]),
        dtype=np.float32,
    )

    # Make the adjacency matrix symmetric
    adj = adj + adj.T.multiply(adj.T > adj)
    adj = normalize_adjacency(adj)

    # Convert to mlx array
    features = mx.array(node_features.toarray(), mx.float32)
    labels = mx.array(labels_enumerated, mx.int32)
    adj = mx.array(adj.toarray())

    print("Dataset loaded.")
    return features, labels, adj

>>>> gcn/gcn.py
import mlx.nn as nn


class GCNLayer(nn.Module):
    def __init__(self, in_features, out_features, bias=True):
        super(GCNLayer, self).__init__()
        self.linear = nn.Linear(in_features, out_features, bias)

    def __call__(self, x, adj):
        x = self.linear(x)
        return adj @ x


class GCN(nn.Module):
    def __init__(self, x_dim, h_dim, out_dim, nb_layers=2, dropout=0.5, bias=True):
        super(GCN, self).__init__()

        layer_sizes = [x_dim] + [h_dim] * nb_layers + [out_dim]
        self.gcn_layers = [
            GCNLayer(in_dim, out_dim, bias)
            for in_dim, out_dim in zip(layer_sizes[:-1], layer_sizes[1:])
        ]
        self.dropout = nn.Dropout(p=dropout)

    def __call__(self, x, adj):
        for layer in self.gcn_layers[:-1]:
            x = nn.relu(layer(x, adj))
            x = self.dropout(x)

        x = self.gcn_layers[-1](x, adj)
        return x

>>>> llava/language.py
# Copyright © 2024 Apple Inc.

import inspect
from dataclasses import dataclass
from typing import Dict, Optional, Tuple, Union

import mlx.core as mx
import mlx.nn as nn


@dataclass
class TextConfig:
    model_type: str
    hidden_size: int = 4096
    num_hidden_layers: int = 32
    intermediate_size: int = 11008
    num_attention_heads: int = 32
    rms_norm_eps: float = 1e-6
    vocab_size: int = 32000
    num_key_value_heads: int = None
    rope_theta: float = 10000
    rope_traditional: bool = False
    rope_scaling: Optional[Dict[str, Union[float, str]]] = None

    @classmethod
    def from_dict(cls, params):
        return cls(
            **{
                k: v
                for k, v in params.items()
                if k in inspect.signature(cls).parameters
            }
        )

    def __post_init__(self):
        if self.num_key_value_heads is None:
            self.num_key_value_heads = self.num_attention_heads

        if self.rope_scaling:
            required_keys = {"factor", "type"}
            if not all(key in self.rope_scaling for key in required_keys):
                raise ValueError(f"rope_scaling must contain keys {required_keys}")

            if self.rope_scaling["type"] != "linear":
                raise ValueError("rope_scaling 'type' currently only supports 'linear'")


class Attention(nn.Module):
    def __init__(self, config: TextConfig):
        super().__init__()

        dim = config.hidden_size
        self.n_heads = n_heads = config.num_attention_heads
        self.n_kv_heads = n_kv_heads = config.num_key_value_heads

        self.repeats = n_heads // n_kv_heads

        head_dim = config.hidden_size // n_heads
        self.scale = head_dim**-0.5

        self.q_proj = nn.Linear(dim, n_heads * head_dim, bias=False)
        self.k_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=False)
        self.v_proj = nn.Linear(dim, n_kv_heads * head_dim, bias=False)
        self.o_proj = nn.Linear(n_heads * head_dim, dim, bias=False)

        rope_scale = (
            1 / config.rope_scaling["factor"]
            if config.rope_scaling is not None
            and config.rope_scaling["type"] == "linear"
            else 1
        )
        self.rope = nn.RoPE(
            head_dim,
            traditional=config.rope_traditional,
            base=config.rope_theta,
            scale=rope_scale,
        )

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Tuple[mx.array, mx.array]] = None,
    ) -> mx.array:
        B, L, D = x.shape

        queries, keys, values = self.q_proj(x), self.k_proj(x), self.v_proj(x)

        # Prepare the queries, keys and values for the attention computation
        queries = queries.reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)
        keys = keys.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)
        values = values.reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)

        if cache is not None:
            key_cache, value_cache = cache
            queries = self.rope(queries, offset=key_cache.shape[2])
            keys = self.rope(keys, offset=key_cache.shape[2])
            keys = mx.concatenate([key_cache, keys], axis=2)
            values = mx.concatenate([value_cache, values], axis=2)
        else:
            queries = self.rope(queries)
            keys = self.rope(keys)

        output = mx.fast.scaled_dot_product_attention(
            queries, keys, values, scale=self.scale, mask=mask
        )
        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)
        return self.o_proj(output), (keys, values)


class MLP(nn.Module):
    def __init__(self, dim, hidden_dim):
        super().__init__()
        self.gate_proj = nn.Linear(dim, hidden_dim, bias=False)
        self.down_proj = nn.Linear(hidden_dim, dim, bias=False)
        self.up_proj = nn.Linear(dim, hidden_dim, bias=False)

    def __call__(self, x) -> mx.array:
        return self.down_proj(nn.silu(self.gate_proj(x)) * self.up_proj(x))


class TransformerBlock(nn.Module):
    def __init__(self, config: TextConfig):
        super().__init__()
        self.num_attention_heads = config.num_attention_heads
        self.hidden_size = config.hidden_size
        self.self_attn = Attention(config)
        self.mlp = MLP(config.hidden_size, config.intermediate_size)
        self.input_layernorm = nn.RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = nn.RMSNorm(
            config.hidden_size, eps=config.rms_norm_eps
        )
        self.config = config

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Tuple[mx.array, mx.array]] = None,
    ) -> mx.array:
        r, cache = self.self_attn(self.input_layernorm(x), mask, cache)
        h = x + r
        r = self.mlp(self.post_attention_layernorm(h))
        out = h + r
        return out, cache


class Llama(nn.Module):
    def __init__(self, config: TextConfig):
        super().__init__()
        self.config = config
        self.vocab_size = config.vocab_size
        self.num_hidden_layers = config.num_hidden_layers
        assert self.vocab_size > 0
        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)
        self.layers = [
            TransformerBlock(config=config) for _ in range(config.num_hidden_layers)
        ]
        self.norm = nn.RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

    def __call__(
        self,
        inputs: mx.array,
        cache=None,
        inputs_embeds=None,
    ):
        # for passing merged input embeddings
        if inputs_embeds is None:
            h = self.embed_tokens(inputs)
        else:
            h = inputs_embeds

        mask = None
        if h.shape[1] > 1:
            mask = nn.MultiHeadAttention.create_additive_causal_mask(h.shape[1])
            mask = mask.astype(h.dtype)

        if cache is None:
            cache = [None] * len(self.layers)

        for e, layer in enumerate(self.layers):
            h, cache[e] = layer(h, mask, cache[e])

        return self.norm(h), cache


class LanguageModel(nn.Module):
    def __init__(self, config: TextConfig):
        super().__init__()
        self.model_type = config.model_type
        if self.model_type != "llama":
            raise ValueError(
                f"Model type {self.model_type} not supported. Currently only 'llama' is supported"
            )
        self.model = Llama(config)
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)

    def __call__(
        self,
        inputs: mx.array,
        cache=None,
        inputs_embeds=None,
    ):
        out, cache = self.model(inputs, cache, inputs_embeds)
        return self.lm_head(out), cache

    @staticmethod
    def sanitize(weights):
        # Remove unused precomputed rotary freqs
        return {
            k: v for k, v in weights.items() if "self_attn.rotary_emb.inv_freq" not in k
        }

>>>> llava/README.md
# LLaVA

An example of LLaVA: Large Language and Vision Assistant in MLX.[^1] LLlava is
a multimodal model that can generate text given combined image and text inputs.

## Setup

Install the dependencies:

```bash
pip install -r requirements.txt
```

## Run

You can use LLaVA to ask questions about images.

For example, using the command line:

```bash
python generate.py 
  --model llava-hf/llava-1.5-7b-hf 
  --image "http://images.cocodataset.org/val2017/000000039769.jpg" 
  --prompt "USER: <image>\nWhat are these?\nASSISTANT:" 
  --max-tokens 128 
  --temp 0
```

This uses the following image:

![alt text](http://images.cocodataset.org/val2017/000000039769.jpg)
 
And generates the output:

```
These are two cats lying on a pink couch.
```

You can also use LLaVA in Python:

```python
from generate import load_model, prepare_inputs, generate_text

processor, model = load_model("llava-hf/llava-1.5-7b-hf")

max_tokens, temperature = 128, 0.0

prompt = "USER: <image>\nWhat are these?\nASSISTANT:"
image = "http://images.cocodataset.org/val2017/000000039769.jpg"
input_ids, pixel_values = prepare_inputs(processor, image, prompt)

reply = generate_text(
    input_ids, pixel_values, model, processor, max_tokens, temperature
)

print(reply)
```

[^1]:
    Refer to [LLaVA project webpage](https://llava-vl.github.io/) for more
    information.

>>>> llava/llava.py
# Copyright © 2024 Apple Inc.

import glob
import inspect
import json
from dataclasses import dataclass
from pathlib import Path
from typing import Optional

import mlx.core as mx
import mlx.nn as nn
import numpy as np
from huggingface_hub import snapshot_download
from language import LanguageModel, TextConfig
from vision import VisionConfig, VisionModel


@dataclass
class LlaVAConfig:
    text_config: TextConfig
    vision_config: VisionConfig
    ignore_index: int = -100
    image_token_index: int = 32000
    vision_feature_select_strategy: str = "default"
    vision_feature_layer: int = -2
    vocab_size: int = 32000

    @classmethod
    def from_dict(cls, params):
        return cls(
            **{
                k: v
                for k, v in params.items()
                if k in inspect.signature(cls).parameters
            }
        )


class LlavaMultiModalProjector(nn.Module):
    def __init__(self, config: LlaVAConfig):
        super().__init__()
        self.linear_1 = nn.Linear(
            config.vision_config.hidden_size, config.text_config.hidden_size, bias=True
        )
        self.gelu = nn.GELU()
        self.linear_2 = nn.Linear(
            config.text_config.hidden_size, config.text_config.hidden_size, bias=True
        )

    def __call__(self, x: mx.array) -> mx.array:
        x = self.linear_1(x)
        x = self.gelu(x)
        x = self.linear_2(x)
        return x


class LlavaModel(nn.Module):
    def __init__(self, config: LlaVAConfig):
        self.config = config
        self.vision_tower = VisionModel(config.vision_config)
        self.language_model = LanguageModel(config.text_config)
        self.multi_modal_projector = LlavaMultiModalProjector(config)
        self.vision_feature_layer = config.vision_feature_layer
        self.vision_feature_select_strategy = config.vision_feature_select_strategy

    def get_input_embeddings(
        self,
        input_ids: Optional[mx.array] = None,
        pixel_values: Optional[mx.array] = None,
    ):
        # Get the input embeddings from the language model
        inputs_embeds = self.language_model.model.embed_tokens(input_ids)
        if pixel_values is None:
            return inputs_embeds

        # Get the ouptut hidden states from the vision model
        *_, hidden_states = self.vision_tower(
            pixel_values.transpose(0, 2, 3, 1), output_hidden_states=True
        )

        # Select the hidden states from the desired layer
        selected_image_feature = hidden_states[self.vision_feature_layer]

        if self.vision_feature_select_strategy == "default":
            selected_image_feature = selected_image_feature[:, 1:]
        elif self.vision_feature_select_strategy == "full":
            selected_image_feature = selected_image_feature
        else:
            raise ValueError(
                "Unexpected feature selection strategy: "
                f"{self.vision_feature_select_strategy}"
            )

        # Pass image features through the multi-modal projector
        image_features = self.multi_modal_projector(selected_image_feature)

        # Insert special image tokens in the input_ids
        final_inputs_embeds = self._merge_input_ids_with_image_features(
            image_features, inputs_embeds, input_ids
        )
        return final_inputs_embeds

    def _merge_input_ids_with_image_features(
        self, image_features, inputs_embeds, input_ids
    ):
        image_token_index = self.config.image_token_index
        batch_size, num_image_patches, embed_dim = image_features.shape

        # Positions of <image> tokens in input_ids, assuming batch size is 1
        image_positions = mx.array(
            np.where(input_ids[0] == image_token_index)[0], mx.uint32
        )

        if len(image_positions) != num_image_patches:
            raise ValueError(
                f"The number of image tokens ({len(image_positions)}) does not "
                f" match the number of image patches ({num_image_patches})."
            )

        inputs_embeds[0, image_positions] = image_features
        return inputs_embeds

    def __call__(self, input_ids: mx.array, pixel_values: mx.array, cache=None):
        input_embddings = self.get_input_embeddings(input_ids, pixel_values)
        logits, cache = self.language_model(
            input_ids, cache=cache, inputs_embeds=input_embddings
        )
        return logits, cache

    @staticmethod
    def from_pretrained(path_or_hf_repo: str):
        path = Path(path_or_hf_repo)
        if not path.exists():
            path = Path(
                snapshot_download(
                    repo_id=path_or_hf_repo,
                    allow_patterns=[
                        "*.json",
                        "*.safetensors",
                        "*.py",
                        "tokenizer.model",
                        "*.tiktoken",
                    ],
                )
            )

        with open(path / "config.json", "r") as f:
            model_config = json.load(f)

        model_config = LlaVAConfig.from_dict(model_config)

        model_config.vision_config = VisionConfig.from_dict(model_config.vision_config)
        model_config.text_config = TextConfig.from_dict(model_config.text_config)

        model = LlavaModel(model_config)
        weight_files = glob.glob(str(path / "*.safetensors"))
        if not weight_files:
            raise FileNotFoundError(f"No safetensors found in {path}")

        weights = {}
        for wf in weight_files:
            weights.update(mx.load(wf))

        weights = VisionModel.sanitize(weights)
        weights = LanguageModel.sanitize(weights)

        model.load_weights(list(weights.items()))
        return model

>>>> llava/test.py
# Copyright © 2024 Apple Inc.

import unittest

import mlx.core as mx
import requests
import torch
from PIL import Image
from transformers import AutoProcessor, LlavaForConditionalGeneration

from llava import LlavaModel

MODEL_PATH = "llava-hf/llava-1.5-7b-hf"
PROMPT = "USER: <image>\nWhat are these?\nASSISTANT:"
IMAGE_FILE = "http://images.cocodataset.org/val2017/000000039769.jpg"


def load_mlx_models(path):
    model = LlavaModel.from_pretrained(path)
    model.eval()
    return model


def load_hf_models(path):
    model = LlavaForConditionalGeneration.from_pretrained(path)
    model.eval()
    return model


class TestVisionTower(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        cls.mx_llava = load_mlx_models(MODEL_PATH)
        cls.hf_llava = load_hf_models(MODEL_PATH)
        cls.proc = AutoProcessor.from_pretrained(MODEL_PATH)

    def test_image_features(self):
        raw_image = Image.open(requests.get(IMAGE_FILE, stream=True).raw)
        vision_feature_layer = -2
        with torch.no_grad():
            pixel_values = self.proc(PROMPT, raw_image, return_tensors="pt")[
                "pixel_values"
            ]

            hf_pixel_values = pixel_values
            mx_pixel_values = mx.array(pixel_values.numpy()).transpose(0, 2, 3, 1)

            _, _, hidden_states = self.mx_llava.vision_tower(
                mx_pixel_values,
                output_hidden_states=True,
            )

            mx_elected_image_feature = hidden_states[vision_feature_layer]
            mx_image_features = self.mx_llava.multi_modal_projector(
                mx_elected_image_feature
            )

            hf_image_outputs = self.hf_llava.vision_tower(
                hf_pixel_values, output_hidden_states=True
            )
            hf_elected_image_feature = hf_image_outputs.hidden_states[
                vision_feature_layer
            ]
            hf_image_features = self.hf_llava.multi_modal_projector(
                hf_elected_image_feature
            )

            self.assertTrue(
                mx.allclose(
                    mx_image_features,
                    mx.array(hf_image_features.numpy()),
                    atol=1e-2,
                )
            )


class TestLlava(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        cls.mx_llava = load_mlx_models(MODEL_PATH)
        cls.hf_llava = load_hf_models(MODEL_PATH)
        cls.proc = AutoProcessor.from_pretrained(MODEL_PATH)

    def test_merge_input_ids_with_image_features(self):
        raw_image = Image.open(requests.get(IMAGE_FILE, stream=True).raw)
        vision_feature_layer = -2
        with torch.no_grad():
            values = self.proc(PROMPT, raw_image, return_tensors="pt")
            pixel_values = values["pixel_values"]
            input_ids = values["input_ids"]

            hf_pixel_values = pixel_values
            mx_pixel_values = mx.array(pixel_values.numpy()).transpose(0, 2, 3, 1)

            _, _, hidden_states = self.mx_llava.vision_tower(
                mx_pixel_values,
                output_hidden_states=True,
            )
            mx_input_ids = mx.array(input_ids.numpy())
            mx_elected_image_feature = hidden_states[vision_feature_layer]
            mx_image_features = self.mx_llava.multi_modal_projector(
                mx_elected_image_feature
            )
            mx_inputs_embeds = self.mx_llava.language_model.model.embed_tokens(
                mx_input_ids
            )
            mx_final_embedding = self.mx_llava._merge_input_ids_with_image_features(
                mx_image_features, mx_inputs_embeds, mx_input_ids
            )

            hf_image_outputs = self.hf_llava.vision_tower(
                hf_pixel_values, output_hidden_states=True
            )
            hf_elected_image_feature = hf_image_outputs.hidden_states[
                vision_feature_layer
            ]
            hf_image_features = self.hf_llava.multi_modal_projector(
                hf_elected_image_feature
            )
            hf_inputs_embeds = self.hf_llava.get_input_embeddings()(input_ids)
            hf_final_embedding, _, _, _ = (
                self.hf_llava._merge_input_ids_with_image_features(
                    hf_image_features,
                    hf_inputs_embeds,
                    input_ids,
                    attention_mask=input_ids,
                    labels=torch.ones_like(input_ids),
                )
            )

            self.assertTrue(
                mx.allclose(
                    mx_final_embedding,
                    mx.array(hf_final_embedding.numpy()),
                    atol=1e-1,
                )
            )

    def test_generated_tokens(self):
        raw_image = Image.open(requests.get(IMAGE_FILE, stream=True).raw)
        with torch.no_grad():
            hf_inputs = self.proc(PROMPT, raw_image, return_tensors="pt")
            hf_outputs = self.hf_llava(**hf_inputs)
            hf_logits = hf_outputs.logits

            mx_inputs = self.proc(PROMPT, raw_image, return_tensors="np")
            pixel_values = mx.array(mx_inputs["pixel_values"])
            input_ids = mx.array(mx_inputs["input_ids"])

            mx_logits, _ = self.mx_llava(input_ids, pixel_values)

            self.assertTrue(
                mx.allclose(
                    mx_logits[:, -1, :].argmax(axis=-1),
                    mx.array(hf_logits.numpy())[:, -1, :].argmax(axis=-1),
                    atol=1e-2,
                )
            )


if __name__ == "__main__":
    unittest.main()

>>>> llava/generate.py
# Copyright © 2024 Apple Inc.

import argparse
import codecs
from pathlib import Path

import mlx.core as mx
import requests
from PIL import Image
from transformers import AutoProcessor

from llava import LlavaModel


def parse_arguments():
    parser = argparse.ArgumentParser(
        description="Generate text from an image using a model."
    )
    parser.add_argument(
        "--model",
        type=str,
        default="llava-hf/llava-1.5-7b-hf",
        help="The path to the local model directory or Hugging Face repo.",
    )
    parser.add_argument(
        "--image",
        type=str,
        default="http://images.cocodataset.org/val2017/000000039769.jpg",
        help="URL or path of the image to process.",
    )
    parser.add_argument(
        "--prompt",
        type=str,
        default="USER: <image>\nWhat are these?\nASSISTANT:",
        help="Message to be processed by the model.",
    )
    parser.add_argument(
        "--max-tokens",
        type=int,
        default=100,
        help="Maximum number of tokens to generate.",
    )
    parser.add_argument(
        "--temp", type=float, default=0.3, help="Temperature for sampling."
    )
    parser.add_argument(
        "--eos-token",
        type=str,
        default=None,
        help="End of sequence token for tokenizer",
    )
    return parser.parse_args()


def load_image(image_source):
    """
    Helper function to load an image from either a URL or file.
    """
    if image_source.startswith(("http://", "https://")):
        try:
            response = requests.get(image_source, stream=True)
            response.raise_for_status()
            return Image.open(response.raw)
        except Exception as e:
            raise ValueError(
                f"Failed to load image from URL: {image_source} with error {e}"
            )
    elif Path(image_source).is_file():
        try:
            return Image.open(image_source)
        except IOError as e:
            raise ValueError(f"Failed to load image {image_source} with error: {e}")
    else:
        raise ValueError(
            f"The image {image_source} must be a valid URL or existing file."
        )


def prepare_inputs(processor, image, prompt):
    if isinstance(image, str):
        image = load_image(image)
    inputs = processor(image, prompt, return_tensors="np")
    pixel_values = mx.array(inputs["pixel_values"])
    input_ids = mx.array(inputs["input_ids"])
    return pixel_values, input_ids


def load_model(model_path, tokenizer_config={}):
    processor = AutoProcessor.from_pretrained(model_path, **tokenizer_config)
    model = LlavaModel.from_pretrained(model_path)
    return processor, model


def sample(logits, temperature=0.0):
    if temperature == 0:
        return mx.argmax(logits, axis=-1)
    else:
        return mx.random.categorical(logits * (1 / temperature))


def generate_text(input_ids, pixel_values, model, processor, max_tokens, temperature):
    logits, cache = model(input_ids, pixel_values)
    logits = logits[:, -1, :]
    y = sample(logits, temperature=temperature)
    tokens = [y.item()]

    for n in range(max_tokens - 1):
        logits, cache = model.language_model(y[None], cache=cache)
        logits = logits[:, -1, :]
        y = sample(logits, temperature)
        token = y.item()
        if token == processor.tokenizer.eos_token_id:
            break
        tokens.append(token)

    return processor.tokenizer.decode(tokens)


def main():
    args = parse_arguments()

    tokenizer_config = {}
    if args.eos_token is not None:
        tokenizer_config["eos_token"] = args.eos_token

    processor, model = load_model(args.model, tokenizer_config)

    prompt = codecs.decode(args.prompt, "unicode_escape")
    pixel_values, input_ids = prepare_inputs(processor, args.image, prompt)

    print(prompt)
    generated_text = generate_text(
        input_ids, pixel_values, model, processor, args.max_tokens, args.temp
    )
    print(generated_text)


if __name__ == "__main__":
    main()

>>>> llava/vision.py
# Copyright © 2024 Apple Inc.

import inspect
import math
from dataclasses import dataclass
from typing import Optional

import mlx.core as mx
import mlx.nn as nn


@dataclass
class VisionConfig:
    model_type: str
    num_hidden_layers: int = 24
    hidden_size: int = 1024
    intermediate_size: int = 4096
    num_attention_heads: int = 16
    image_size: int = 336
    patch_size: int = 14
    projection_dim: int = 768
    vocab_size: int = 32000
    num_channels: int = 3
    layer_norm_eps: float = 1e-5

    @classmethod
    def from_dict(cls, params):
        return cls(
            **{
                k: v
                for k, v in params.items()
                if k in inspect.signature(cls).parameters
            }
        )


class Attention(nn.Module):
    def __init__(
        self,
        dims: int,
        num_heads: int,
        query_input_dims: Optional[int] = None,
        key_input_dims: Optional[int] = None,
        value_input_dims: Optional[int] = None,
        value_dims: Optional[int] = None,
        value_output_dims: Optional[int] = None,
        bias: bool = False,
    ):
        super().__init__()

        if (dims % num_heads) != 0:
            raise ValueError(
                "The input feature dimensions should be divisible by the "
                f"number of heads ({dims} % {num_heads}) != 0"
            )

        query_input_dims = query_input_dims or dims
        key_input_dims = key_input_dims or dims
        value_input_dims = value_input_dims or key_input_dims
        value_dims = value_dims or dims
        value_output_dims = value_output_dims or dims

        self.num_heads = num_heads
        self.q_proj = nn.Linear(query_input_dims, dims, bias=bias)
        self.k_proj = nn.Linear(key_input_dims, dims, bias=bias)
        self.v_proj = nn.Linear(value_input_dims, value_dims, bias=bias)
        self.out_proj = nn.Linear(value_dims, value_output_dims, bias=bias)

    def __call__(self, queries, keys, values, mask=None):
        queries = self.q_proj(queries)
        keys = self.k_proj(keys)
        values = self.v_proj(values)

        num_heads = self.num_heads
        B, L, D = queries.shape
        _, S, _ = keys.shape
        queries = queries.reshape(B, L, num_heads, -1).transpose(0, 2, 1, 3)
        keys = keys.reshape(B, S, num_heads, -1).transpose(0, 2, 3, 1)
        values = values.reshape(B, S, num_heads, -1).transpose(0, 2, 1, 3)

        scale = math.sqrt(1 / queries.shape[-1])
        scores = (queries * scale) @ keys
        if mask is not None:
            scores = scores + mask.astype(scores.dtype)
        scores = mx.softmax(scores, axis=-1)
        values_hat = (scores @ values).transpose(0, 2, 1, 3).reshape(B, L, -1)

        return self.out_proj(values_hat)


class MLP(nn.Module):
    def __init__(self, config: VisionConfig):
        super().__init__()
        self.activation_fn = nn.GELU(approx="fast")
        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)
        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)

    def __call__(self, x: mx.array) -> mx.array:
        x = self.activation_fn(self.fc1(x))
        x = self.fc2(x)
        return x


class EncoderLayer(nn.Module):
    def __init__(self, config: VisionConfig):
        super().__init__()
        self.embed_dim = config.hidden_size
        self.self_attn = Attention(
            config.hidden_size, config.num_attention_heads, bias=True
        )
        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)
        self.mlp = MLP(config)
        self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)

    def __call__(self, x: mx.array, mask: Optional[mx.array] = None) -> mx.array:
        y = self.layer_norm1(x)
        y = self.self_attn(y, y, y, mask)
        x = x + y
        y = self.layer_norm2(x)
        y = self.mlp(y)
        return x + y


class Encoder(nn.Module):
    def __init__(self, config: VisionConfig):
        super().__init__()
        self.layers = [EncoderLayer(config) for _ in range(config.num_hidden_layers)]


class VisionEmbeddings(nn.Module):
    def __init__(self, config: VisionConfig):
        super().__init__()
        self.config = config
        self.embed_dim = config.hidden_size
        self.image_size = config.image_size
        self.patch_size = config.patch_size

        self.class_embedding = mx.zeros((config.hidden_size,))

        self.patch_embedding = nn.Conv2d(
            in_channels=config.num_channels,
            out_channels=self.embed_dim,
            kernel_size=self.patch_size,
            stride=self.patch_size,
            bias=False,
        )

        self.num_patches = (self.image_size // self.patch_size) ** 2
        self.num_positions = self.num_patches + 1
        self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim)

    def __call__(self, x: mx.array) -> mx.array:
        batch_size = x.shape[0]
        patch_embeddings = self.patch_embedding(x)
        patch_embeddings = mx.flatten(patch_embeddings, start_axis=1, end_axis=2)
        embed_dim = patch_embeddings.shape[-1]
        cls_embeddings = mx.broadcast_to(
            self.class_embedding, (batch_size, 1, embed_dim)
        )
        embeddings = mx.concatenate((cls_embeddings, patch_embeddings), axis=1)
        embeddings += self.position_embedding.weight
        return embeddings


class ClipVisionModel(nn.Module):
    def __init__(self, config: VisionConfig):
        super().__init__()
        self.embeddings = VisionEmbeddings(config)
        self.pre_layrnorm = nn.LayerNorm(config.hidden_size)
        self.encoder = Encoder(config)
        self.post_layernorm = nn.LayerNorm(config.hidden_size)

    def __call__(
        self,
        x: mx.array,
        output_hidden_states: Optional[bool] = None,
    ) -> mx.array:
        x = self.embeddings(x)
        x = self.pre_layrnorm(x)

        encoder_states = (x,) if output_hidden_states else None

        for l in self.encoder.layers:
            x = l(x, mask=None)
            if output_hidden_states:
                encoder_states = encoder_states + (x,)

        pooler_output = self.post_layernorm(x[:, 0, :])
        return pooler_output, x, encoder_states


class VisionModel(nn.Module):
    def __init__(self, config: VisionConfig):
        super().__init__()

        self.model_type = config.model_type
        if self.model_type != "clip_vision_model":
            raise ValueError(f"Unsupported model type: {self.model_type}")

        self.vision_model = ClipVisionModel(config)

    def __call__(
        self, x: mx.array, output_hidden_states: Optional[bool] = None
    ) -> mx.array:
        return self.vision_model(x, output_hidden_states)

    @staticmethod
    def sanitize(weights):
        sanitized_weights = {}
        for k, v in weights.items():
            if "position_ids" in k:
                # Remove unused position_ids
                continue
            elif "patch_embedding.weight" in k:
                # PyTorch conv2d weight tensors have shape:
                #   [out_channels, in_channels, kH, KW]
                # MLX conv2d expects the weight be of shape:
                #   [out_channels, kH, KW, in_channels]
                sanitized_weights[k] = v.transpose(0, 2, 3, 1)
            else:
                sanitized_weights[k] = v

        return sanitized_weights

>>>> README.md
# MLX Examples

This repo contains a variety of standalone examples using the [MLX
framework](https://github.com/ml-explore/mlx).

The [MNIST](mnist) example is a good starting point to learn how to use MLX.

Some more useful examples are listed below.

### Text Models 

- [MLX LM](llms/README.md) a package for LLM text generation, fine-tuning, and more.
- [Transformer language model](transformer_lm) training.
- Minimal examples of large scale text generation with [LLaMA](llms/llama),
  [Mistral](llms/mistral), and more in the [LLMs](llms) directory.
- A mixture-of-experts (MoE) language model with [Mixtral 8x7B](llms/mixtral).
- Parameter efficient fine-tuning with [LoRA or QLoRA](lora).
- Text-to-text multi-task Transformers with [T5](t5).
- Bidirectional language understanding with [BERT](bert).

### Image Models 

- Generating images
  - [FLUX](flux)
  - [Stable Diffusion or SDXL](stable_diffusion)
- Image classification using [ResNets on CIFAR-10](cifar).
- Convolutional variational autoencoder [(CVAE) on MNIST](cvae).

### Audio Models

- Speech recognition with [OpenAI's Whisper](whisper).
- Audio compression and generation with [Meta's EnCodec](encodec).

### Multimodal models

- Joint text and image embeddings with [CLIP](clip).
- Text generation from image and text inputs with [LLaVA](llava).
- Image segmentation with [Segment Anything (SAM)](segment_anything).

### Other Models 

- Semi-supervised learning on graph-structured data with [GCN](gcn).
- Real NVP [normalizing flow](normalizing_flow) for density estimation and
  sampling.

### Hugging Face

You can directly use or download converted checkpoints from the [MLX
Community](https://huggingface.co/mlx-community) organization on Hugging Face.
We encourage you to join the community and [contribute new
models](https://github.com/ml-explore/mlx-examples/issues/155).

## Contributing 

We are grateful for all of [our
contributors](ACKNOWLEDGMENTS.md#Individual-Contributors). If you contribute
to MLX Examples and wish to be acknowledged, please add your name to the list in your
pull request.

## Citing MLX Examples

The MLX software suite was initially developed with equal contribution by Awni
Hannun, Jagrit Digani, Angelos Katharopoulos, and Ronan Collobert. If you find
MLX Examples useful in your research and wish to cite it, please use the following
BibTex entry:

```
@software{mlx2023,
  author = {Awni Hannun and Jagrit Digani and Angelos Katharopoulos and Ronan Collobert},
  title = {{MLX}: Efficient and flexible machine learning on Apple silicon},
  url = {https://github.com/ml-explore},
  version = {0.0},
  year = {2023},
}
```

>>>> encodec/encodec.py
# Copyright © 2024 Apple Inc.

import functools
import json
import math
from pathlib import Path
from types import SimpleNamespace
from typing import List, Optional, Tuple, Union

import mlx.core as mx
import mlx.nn as nn
import numpy as np

_lstm_kernel = mx.fast.metal_kernel(
    name="lstm",
    input_names=["x", "h_in", "cell", "hidden_size", "time_step", "num_time_steps"],
    output_names=["hidden_state", "cell_state"],
    header="""
    template <typename T>
    T sigmoid(T x) {
        auto y = 1 / (1 + metal::exp(-metal::abs(x)));
        return (x < 0) ? 1 - y : y;
    }
    """,
    source="""
        uint b = thread_position_in_grid.x;
        uint d = hidden_size * 4;

        uint elem = b * d + thread_position_in_grid.y;
        uint index = elem;
        uint x_index = b * num_time_steps * d + time_step * d + index;

        auto i = sigmoid(h_in[index] + x[x_index]);
        index += hidden_size;
        x_index += hidden_size;
        auto f = sigmoid(h_in[index] + x[x_index]);
        index += hidden_size;
        x_index += hidden_size;
        auto g = metal::precise::tanh(h_in[index] + x[x_index]);
        index += hidden_size;
        x_index += hidden_size;
        auto o = sigmoid(h_in[index] + x[x_index]);

        cell_state[elem] = f * cell[elem] + i * g;
        hidden_state[elem] = o * metal::precise::tanh(cell_state[elem]);
    """,
)


def lstm_custom(x, h_in, cell, time_step):
    assert x.ndim == 3, "Input to LSTM must have 3 dimensions."
    out_shape = cell.shape
    return _lstm_kernel(
        inputs=[x, h_in, cell, out_shape[-1], time_step, x.shape[-2]],
        output_shapes=[out_shape, out_shape],
        output_dtypes=[h_in.dtype, h_in.dtype],
        grid=(x.shape[0], h_in.size // 4, 1),
        threadgroup=(256, 1, 1),
    )


class LSTM(nn.Module):
    def __init__(
        self,
        input_size: int,
        hidden_size: int,
        bias: bool = True,
    ):
        super().__init__()

        self.hidden_size = hidden_size
        self.Wx = mx.zeros((4 * hidden_size, input_size))
        self.Wh = mx.zeros((4 * hidden_size, hidden_size))
        self.bias = mx.zeros((4 * hidden_size,)) if bias else None

    def __call__(self, x, hidden=None, cell=None):
        if self.bias is not None:
            x = mx.addmm(self.bias, x, self.Wx.T)
        else:
            x = x @ self.Wx.T

        all_hidden = []

        B = x.shape[0]
        cell = cell or mx.zeros((B, self.hidden_size), x.dtype)
        for t in range(x.shape[-2]):
            if hidden is None:
                hidden = mx.zeros((B, self.hidden_size * 4), x.dtype)
            else:
                hidden = hidden @ self.Wh.T
            hidden, cell = lstm_custom(x, hidden, cell, t)
            all_hidden.append(hidden)

        return mx.stack(all_hidden, axis=-2)


class EncodecConv1d(nn.Module):
    """Conv1d with asymmetric or causal padding and normalization."""

    def __init__(
        self,
        config,
        in_channels: int,
        out_channels: int,
        kernel_size: int,
        stride: int = 1,
        dilation: int = 1,
    ):
        super().__init__()
        self.causal = config.use_causal_conv
        self.pad_mode = config.pad_mode
        self.norm_type = config.norm_type

        self.conv = nn.Conv1d(
            in_channels, out_channels, kernel_size, stride, dilation=dilation
        )
        if self.norm_type == "time_group_norm":
            self.norm = nn.GroupNorm(1, out_channels, pytorch_compatible=True)

        self.stride = stride

        # Effective kernel size with dilations.
        self.kernel_size = (kernel_size - 1) * dilation + 1

        self.padding_total = kernel_size - stride

    def _get_extra_padding_for_conv1d(
        self,
        hidden_states: mx.array,
    ) -> mx.array:
        length = hidden_states.shape[1]
        n_frames = (length - self.kernel_size + self.padding_total) / self.stride + 1
        n_frames = int(math.ceil(n_frames)) - 1
        ideal_length = n_frames * self.stride + self.kernel_size - self.padding_total
        return ideal_length - length

    def _pad1d(
        self,
        hidden_states: mx.array,
        paddings: Tuple[int, int],
        mode: str = "zero",
        value: float = 0.0,
    ):
        if mode != "reflect":
            return mx.pad(
                hidden_states, paddings, mode="constant", constant_values=value
            )

        length = hidden_states.shape[1]
        prefix = hidden_states[:, 1 : paddings[0] + 1][:, ::-1]
        suffix = hidden_states[:, max(length - (paddings[1] + 1), 0) : -1][:, ::-1]
        return mx.concatenate([prefix, hidden_states, suffix], axis=1)

    def __call__(self, hidden_states):
        extra_padding = self._get_extra_padding_for_conv1d(hidden_states)

        if self.causal:
            # Left padding for causal
            hidden_states = self._pad1d(
                hidden_states, (self.padding_total, extra_padding), mode=self.pad_mode
            )
        else:
            # Asymmetric padding required for odd strides
            padding_right = self.padding_total // 2
            padding_left = self.padding_total - padding_right
            hidden_states = self._pad1d(
                hidden_states,
                (padding_left, padding_right + extra_padding),
                mode=self.pad_mode,
            )

        hidden_states = self.conv(hidden_states)

        if self.norm_type == "time_group_norm":
            hidden_states = self.norm(hidden_states)

        return hidden_states


class EncodecConvTranspose1d(nn.Module):
    """ConvTranspose1d with asymmetric or causal padding and normalization."""

    def __init__(
        self,
        config,
        in_channels: int,
        out_channels: int,
        kernel_size: int,
        stride: int = 1,
    ):
        super().__init__()
        self.causal = config.use_causal_conv
        self.trim_right_ratio = config.trim_right_ratio
        self.norm_type = config.norm_type
        self.conv = nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride)
        if config.norm_type == "time_group_norm":
            self.norm = nn.GroupNorm(1, out_channels, pytorch_compatible=True)
        self.padding_total = kernel_size - stride

    def __call__(self, hidden_states):
        hidden_states = self.conv(hidden_states)

        if self.norm_type == "time_group_norm":
            hidden_states = self.norm(hidden_states)

        if self.causal:
            padding_right = math.ceil(self.padding_total * self.trim_right_ratio)
        else:
            padding_right = self.padding_total // 2

        padding_left = self.padding_total - padding_right

        end = hidden_states.shape[1] - padding_right
        hidden_states = hidden_states[:, padding_left:end, :]
        return hidden_states


class EncodecLSTM(nn.Module):
    def __init__(self, config, dimension):
        super().__init__()
        self.lstm = [LSTM(dimension, dimension) for _ in range(config.num_lstm_layers)]

    def __call__(self, hidden_states):
        h = hidden_states
        for lstm in self.lstm:
            h = lstm(h)
        return h + hidden_states


class EncodecResnetBlock(nn.Module):
    """
    Residual block from SEANet model as used by EnCodec.
    """

    def __init__(self, config, dim: int, dilations: List[int]):
        super().__init__()
        kernel_sizes = (config.residual_kernel_size, 1)
        if len(kernel_sizes) != len(dilations):
            raise ValueError("Number of kernel sizes should match number of dilations")

        hidden = dim // config.compress
        block = []
        for i, (kernel_size, dilation) in enumerate(zip(kernel_sizes, dilations)):
            in_chs = dim if i == 0 else hidden
            out_chs = dim if i == len(kernel_sizes) - 1 else hidden
            block += [nn.ELU()]
            block += [
                EncodecConv1d(config, in_chs, out_chs, kernel_size, dilation=dilation)
            ]
        self.block = block

        if getattr(config, "use_conv_shortcut", True):
            self.shortcut = EncodecConv1d(config, dim, dim, kernel_size=1)
        else:
            self.shortcut = nn.Identity()

    def __call__(self, hidden_states):
        residual = hidden_states
        for layer in self.block:
            hidden_states = layer(hidden_states)

        return self.shortcut(residual) + hidden_states


class EncodecEncoder(nn.Module):
    """SEANet encoder as used by EnCodec."""

    def __init__(self, config):
        super().__init__()
        model = [
            EncodecConv1d(
                config, config.audio_channels, config.num_filters, config.kernel_size
            )
        ]
        scaling = 1

        for ratio in reversed(config.upsampling_ratios):
            current_scale = scaling * config.num_filters
            for j in range(config.num_residual_layers):
                model += [
                    EncodecResnetBlock(
                        config, current_scale, [config.dilation_growth_rate**j, 1]
                    )
                ]
            model += [nn.ELU()]
            model += [
                EncodecConv1d(
                    config,
                    current_scale,
                    current_scale * 2,
                    kernel_size=ratio * 2,
                    stride=ratio,
                )
            ]
            scaling *= 2

        model += [EncodecLSTM(config, scaling * config.num_filters)]
        model += [nn.ELU()]
        model += [
            EncodecConv1d(
                config,
                scaling * config.num_filters,
                config.hidden_size,
                config.last_kernel_size,
            )
        ]

        self.layers = model

    def __call__(self, hidden_states):
        for layer in self.layers:
            hidden_states = layer(hidden_states)
        return hidden_states


class EncodecDecoder(nn.Module):
    """SEANet decoder as used by EnCodec."""

    def __init__(self, config):
        super().__init__()
        scaling = int(2 ** len(config.upsampling_ratios))
        model = [
            EncodecConv1d(
                config,
                config.hidden_size,
                scaling * config.num_filters,
                config.kernel_size,
            )
        ]

        model += [EncodecLSTM(config, scaling * config.num_filters)]

        for ratio in config.upsampling_ratios:
            current_scale = scaling * config.num_filters
            model += [nn.ELU()]
            model += [
                EncodecConvTranspose1d(
                    config,
                    current_scale,
                    current_scale // 2,
                    kernel_size=ratio * 2,
                    stride=ratio,
                )
            ]
            for j in range(config.num_residual_layers):
                model += [
                    EncodecResnetBlock(
                        config, current_scale // 2, (config.dilation_growth_rate**j, 1)
                    )
                ]
            scaling //= 2

        model += [nn.ELU()]
        model += [
            EncodecConv1d(
                config,
                config.num_filters,
                config.audio_channels,
                config.last_kernel_size,
            )
        ]
        self.layers = model

    def __call__(self, hidden_states):
        for layer in self.layers:
            hidden_states = layer(hidden_states)
        return hidden_states


class EncodecEuclideanCodebook(nn.Module):
    """Codebook with Euclidean distance."""

    def __init__(self, config):
        super().__init__()
        self.embed = mx.zeros((config.codebook_size, config.codebook_dim))

    def quantize(self, hidden_states):
        embed = self.embed.T
        scaled_states = hidden_states.square().sum(axis=1, keepdims=True)
        dist = -(
            scaled_states
            - 2 * hidden_states @ embed
            + embed.square().sum(axis=0, keepdims=True)
        )
        embed_ind = dist.argmax(axis=-1)
        return embed_ind

    def encode(self, hidden_states):
        shape = hidden_states.shape
        hidden_states = hidden_states.reshape((-1, shape[-1]))
        embed_ind = self.quantize(hidden_states)
        embed_ind = embed_ind.reshape(*shape[:-1])
        return embed_ind

    def decode(self, embed_ind):
        return self.embed[embed_ind]


class EncodecVectorQuantization(nn.Module):
    """
    Vector quantization implementation. Currently supports only euclidean distance.
    """

    def __init__(self, config):
        super().__init__()
        self.codebook = EncodecEuclideanCodebook(config)

    def encode(self, hidden_states):
        return self.codebook.encode(hidden_states)

    def decode(self, embed_ind):
        return self.codebook.decode(embed_ind)


class EncodecResidualVectorQuantizer(nn.Module):
    """Residual Vector Quantizer."""

    def __init__(self, config):
        super().__init__()
        self.codebook_size = config.codebook_size

        hop_length = np.prod(config.upsampling_ratios)
        self.frame_rate = math.ceil(config.sampling_rate / hop_length)
        self.num_quantizers = int(
            1000 * config.target_bandwidths[-1] // (self.frame_rate * 10)
        )
        self.layers = [
            EncodecVectorQuantization(config) for _ in range(self.num_quantizers)
        ]

    def get_num_quantizers_for_bandwidth(
        self, bandwidth: Optional[float] = None
    ) -> int:
        """Return num_quantizers based on specified target bandwidth."""
        bw_per_q = math.log2(self.codebook_size) * self.frame_rate
        num_quantizers = self.num_quantizers
        if bandwidth is not None and bandwidth > 0.0:
            num_quantizers = int(max(1, math.floor(bandwidth * 1000 / bw_per_q)))
        return num_quantizers

    def encode(
        self, embeddings: mx.array, bandwidth: Optional[float] = None
    ) -> mx.array:
        """
        Encode a given input array with the specified frame rate at the given
        bandwidth. The RVQ encode method sets the appropriate number of
        quantizers to use and returns indices for each quantizer.
        """
        num_quantizers = self.get_num_quantizers_for_bandwidth(bandwidth)
        residual = embeddings
        all_indices = []
        for layer in self.layers[:num_quantizers]:
            indices = layer.encode(residual)
            quantized = layer.decode(indices)
            residual = residual - quantized
            all_indices.append(indices)
        out_indices = mx.stack(all_indices, axis=1)
        return out_indices

    def decode(self, codes: mx.array) -> mx.array:
        """Decode the given codes to the quantized representation."""
        quantized_out = None
        for i, indices in enumerate(codes.split(codes.shape[1], axis=1)):
            layer = self.layers[i]
            quantized = layer.decode(indices.squeeze(1))
            if quantized_out is None:
                quantized_out = quantized
            else:
                quantized_out = quantized + quantized_out
        return quantized_out


class EncodecModel(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.encoder = EncodecEncoder(config)
        self.decoder = EncodecDecoder(config)
        self.quantizer = EncodecResidualVectorQuantizer(config)

    def _encode_frame(
        self, input_values: mx.array, bandwidth: float, padding_mask: mx.array
    ) -> Tuple[mx.array, Optional[mx.array]]:
        """
        Encodes the given input using the underlying VQVAE.
        """
        length = input_values.shape[1]
        duration = length / self.config.sampling_rate

        if (
            self.config.chunk_length_s is not None
            and duration > 1e-5 + self.config.chunk_length_s
        ):
            raise RuntimeError(
                f"Duration of frame ({duration}) is longer than chunk {self.config.chunk_length_s}"
            )

        scale = None
        if self.config.normalize:
            # if the padding is non zero
            input_values = input_values * padding_mask[..., None]
            mono = mx.sum(input_values, axis=2, keepdims=True) / input_values.shape[2]
            scale = mono.square().mean(axis=1, keepdims=True).sqrt() + 1e-8
            input_values = input_values / scale

        embeddings = self.encoder(input_values)
        codes = self.quantizer.encode(embeddings, bandwidth)
        return codes, scale

    def encode(
        self,
        input_values: mx.array,
        padding_mask: mx.array = None,
        bandwidth: Optional[float] = None,
    ) -> Tuple[mx.array, Optional[mx.array]]:
        """
        Encodes the input audio waveform into discrete codes.

        Args:
            input_values (mx.array): The input audio waveform with shape
                ``(batch_size, channels, sequence_length)``.
            padding_mask (mx.array): Padding mask used to pad the ``input_values``.
            bandwidth (float, optional): The target bandwidth. Must be one of
                ``config.target_bandwidths``. If ``None``, uses the smallest
                possible bandwidth. bandwidth is represented as a thousandth of
                what it is, e.g. 6kbps bandwidth is represented as bandwidth == 6.0

        Returns:
            A list of frames containing the discrete encoded codes for the
            input audio waveform, along with rescaling factors for each chunk
            when ``config.normalize==True``. Each frame is a tuple ``(codebook,
            scale)``, with ``codebook`` of shape ``(batch_size, num_codebooks,
            frames)``.
        """

        if bandwidth is None:
            bandwidth = self.config.target_bandwidths[0]
        if bandwidth not in self.config.target_bandwidths:
            raise ValueError(
                f"This model doesn't support the bandwidth {bandwidth}. "
                f"Select one of {self.config.target_bandwidths}."
            )

        _, input_length, channels = input_values.shape

        if channels < 1 or channels > 2:
            raise ValueError(
                f"Number of audio channels must be 1 or 2, but got {channels}"
            )

        chunk_length = self.chunk_length
        if chunk_length is None:
            chunk_length = input_length
            stride = input_length
        else:
            stride = self.chunk_stride

        if padding_mask is None:
            padding_mask = mx.ones(input_values.shape[:2], dtype=mx.bool_)
        encoded_frames = []
        scales = []

        step = chunk_length - stride
        if (input_length % stride) != step:
            raise ValueError(
                "The input length is not properly padded for batched chunked "
                "encoding. Make sure to pad the input correctly."
            )

        for offset in range(0, input_length - step, stride):
            mask = padding_mask[:, offset : offset + chunk_length].astype(mx.bool_)
            frame = input_values[:, offset : offset + chunk_length]
            encoded_frame, scale = self._encode_frame(frame, bandwidth, mask)
            encoded_frames.append(encoded_frame)
            scales.append(scale)

        encoded_frames = mx.stack(encoded_frames)

        return (encoded_frames, scales)

    @staticmethod
    def _linear_overlap_add(frames: List[mx.array], stride: int):
        if len(frames) == 0:
            raise ValueError("`frames` cannot be an empty list.")

        dtype = frames[0].dtype
        N, frame_length, C = frames[0].shape
        total_size = stride * (len(frames) - 1) + frames[-1].shape[1]

        time_vec = mx.linspace(0, 1, frame_length + 2, dtype=dtype)[1:-1]
        weight = 0.5 - (time_vec - 0.5).abs()

        weight = weight[:, None]
        sum_weight = mx.zeros((total_size, 1), dtype=dtype)
        out = mx.zeros((N, total_size, C), dtype=dtype)
        offset = 0

        for frame in frames:
            frame_length = frame.shape[1]
            out[:, offset : offset + frame_length] += weight[:frame_length] * frame
            sum_weight[offset : offset + frame_length] += weight[:frame_length]
            offset += stride

        return out / sum_weight

    def _decode_frame(
        self, codes: mx.array, scale: Optional[mx.array] = None
    ) -> mx.array:
        embeddings = self.quantizer.decode(codes)
        outputs = self.decoder(embeddings)
        if scale is not None:
            outputs = outputs * scale
        return outputs

    @property
    def channels(self):
        return self.config.audio_channels

    @property
    def sampling_rate(self):
        return self.config.sampling_rate

    @property
    def chunk_length(self):
        if self.config.chunk_length_s is None:
            return None
        else:
            return int(self.config.chunk_length_s * self.config.sampling_rate)

    @property
    def chunk_stride(self):
        if self.config.chunk_length_s is None or self.config.overlap is None:
            return None
        else:
            return max(1, int((1.0 - self.config.overlap) * self.chunk_length))

    def decode(
        self,
        audio_codes: mx.array,
        audio_scales: Union[mx.array, List[mx.array]],
        padding_mask: Optional[mx.array] = None,
    ) -> Tuple[mx.array, mx.array]:
        """
        Decodes the given frames into an output audio waveform.

        Note that the output might be a bit bigger than the input. In that
        case, any extra steps at the end should be trimmed.

        Args:
            audio_codes (mx.array): Discret code embeddings of shape
                ``(batch_size, nb_chunks, chunk_length)``.
            audio_scales (mx.array): Scaling factor for each input.
            padding_mask (mx.array): Padding mask.
        """
        chunk_length = self.chunk_length
        if chunk_length is None:
            if audio_codes.shape[1] != 1:
                raise ValueError(f"Expected one frame, got {len(audio_codes)}")
            audio_values = self._decode_frame(audio_codes[:, 0], audio_scales[0])
        else:
            decoded_frames = []

            for frame, scale in zip(audio_codes, audio_scales):
                frames = self._decode_frame(frame, scale)
                decoded_frames.append(frames)

            audio_values = self._linear_overlap_add(
                decoded_frames, self.chunk_stride or 1
            )

        # truncate based on padding mask
        if padding_mask is not None and padding_mask.shape[1] < audio_values.shape[1]:
            audio_values = audio_values[:, : padding_mask.shape[1]]
        return audio_values

    @classmethod
    def from_pretrained(cls, path_or_repo: str):
        from huggingface_hub import snapshot_download

        path = Path(path_or_repo)
        if not path.exists():
            path = Path(
                snapshot_download(
                    repo_id=path_or_repo,
                    allow_patterns=["*.json", "*.safetensors", "*.model"],
                )
            )

        with open(path / "config.json", "r") as f:
            config = SimpleNamespace(**json.load(f))

        model = EncodecModel(config)
        model.load_weights(str(path / "model.safetensors"))
        processor = functools.partial(
            preprocess_audio,
            sampling_rate=config.sampling_rate,
            chunk_length=model.chunk_length,
            chunk_stride=model.chunk_stride,
        )
        mx.eval(model)
        return model, processor


def preprocess_audio(
    raw_audio: Union[mx.array, List[mx.array]],
    sampling_rate: int = 24000,
    chunk_length: Optional[int] = None,
    chunk_stride: Optional[int] = None,
):
    r"""
    Prepare inputs for the EnCodec model.

    Args:
        raw_audio (mx.array or List[mx.array]): The sequence or batch of
            sequences to be processed.
        sampling_rate (int): The sampling rate at which the audio waveform
            should be digitalized.
        chunk_length (int, optional): The model's chunk length.
        chunk_stride (int, optional): The model's chunk stride.
    """
    if not isinstance(raw_audio, list):
        raw_audio = [raw_audio]

    raw_audio = [x[..., None] if x.ndim == 1 else x for x in raw_audio]

    max_length = max(array.shape[0] for array in raw_audio)
    if chunk_length is not None:
        max_length += chunk_length - (max_length % chunk_stride)

    inputs = []
    masks = []
    for x in raw_audio:
        length = x.shape[0]
        mask = mx.ones((length,), dtype=mx.bool_)
        difference = max_length - length
        if difference > 0:
            mask = mx.pad(mask, (0, difference))
            x = mx.pad(x, ((0, difference), (0, 0)))
        inputs.append(x)
        masks.append(mask)
    return mx.stack(inputs), mx.stack(masks)

>>>> encodec/benchmarks/bench_pt.py
# Copyright © 2024 Apple Inc.

import time

import numpy as np
import torch
from transformers import AutoProcessor, EncodecModel

processor = AutoProcessor.from_pretrained("facebook/encodec_48khz")
audio = np.random.uniform(size=(2, 288000)).astype(np.float32)

pt_model = EncodecModel.from_pretrained("facebook/encodec_48khz").to("mps")
pt_inputs = processor(
    raw_audio=audio, sampling_rate=processor.sampling_rate, return_tensors="pt"
).to("mps")


def fun():
    pt_encoded = pt_model.encode(pt_inputs["input_values"], pt_inputs["padding_mask"])
    pt_audio = pt_model.decode(
        pt_encoded.audio_codes, pt_encoded.audio_scales, pt_inputs["padding_mask"]
    )
    torch.mps.synchronize()


for _ in range(5):
    fun()

tic = time.time()
for _ in range(10):
    fun()
toc = time.time()
ms = 1000 * (toc - tic) / 10
print(f"Time per it: {ms:.3f}")

>>>> encodec/benchmarks/bench_mx.py
# Copyright © 2024 Apple Inc.

import time

import mlx.core as mx

from encodec import EncodecModel

model, processor = EncodecModel.from_pretrained("mlx-community/encodec-48khz-float32")

audio = mx.random.uniform(shape=(288000, 2))
feats, mask = processor(audio)
mx.eval(model, feats, mask)


@mx.compile
def fun():
    codes, scales = model.encode(feats, mask, bandwidth=3)
    reconstructed = model.decode(codes, scales, mask)
    return reconstructed


for _ in range(5):
    mx.eval(fun())

tic = time.time()
for _ in range(10):
    mx.eval(fun())
toc = time.time()
ms = 1000 * (toc - tic) / 10
print(f"Time per it: {ms:.3f}")

>>>> encodec/utils.py
# Copyright © 2024 Apple Inc.

import mlx.core as mx
import numpy as np


def save_audio(file: str, audio: mx.array, sampling_rate: int):
    """
    Save audio to a wave (.wav) file.
    """
    from scipy.io.wavfile import write

    audio = (audio * 32767).astype(mx.int16)
    write(file, sampling_rate, np.array(audio))


def load_audio(file: str, sampling_rate: int, channels: int):
    """
    Read audio into an mx.array, resampling if necessary.

    Args:
        file (str): The audio file to open.
        sampling_rate (int): The sample rate to resample the audio at if needed.
        channels (int): The number of audio channels.

    Returns:
        An mx.array containing the audio waveform in float32.
    """
    from subprocess import CalledProcessError, run

    # This launches a subprocess to decode audio while down-mixing
    # and resampling as necessary.  Requires the ffmpeg CLI in PATH.
    # fmt: off
    cmd = [
        "ffmpeg",
        "-nostdin",
        "-threads", "0",
        "-i", file,
        "-f", "s16le",
        "-ac", str(channels),
        "-acodec", "pcm_s16le",
        "-ar", str(sampling_rate),
        "-"
    ]
    # fmt: on
    try:
        out = run(cmd, capture_output=True, check=True).stdout
    except CalledProcessError as e:
        raise RuntimeError(f"Failed to load audio: {e.stderr.decode()}") from e

    out = mx.array(np.frombuffer(out, np.int16))
    return out.reshape(-1, channels).astype(mx.float32) / 32767.0

>>>> encodec/example.py
# Copyright © 2024 Apple Inc.

import mlx.core as mx
from utils import load_audio, save_audio

from encodec import EncodecModel

# Load the 48 KHz model and preprocessor.
model, processor = EncodecModel.from_pretrained("mlx-community/encodec-48khz-float32")

# Load an audio file
audio = load_audio("/path/to/audio", model.sampling_rate, model.channels)

# Preprocess the audio (this can also be a list of arrays for batched
# processing).
feats, mask = processor(audio)


# Encode at the given bandwidth. A lower bandwidth results in more
# compression but lower reconstruction quality.
@mx.compile
def encode(feats, mask):
    return model.encode(feats, mask, bandwidth=3)


# Decode to reconstruct the audio
@mx.compile
def decode(codes, scales, mask):
    return model.decode(codes, scales, mask)


codes, scales = encode(feats, mask)
reconstructed = decode(codes, scales, mask)

# Trim any padding:
reconstructed = reconstructed[0, : len(audio)]

# Save the audio as a wave file
save_audio("reconstructed.wav", reconstructed, model.sampling_rate)

>>>> encodec/README.md
# EnCodec

An example of Meta's EnCodec model in MLX.[^1] EnCodec is used to compress and
generate audio.

### Setup

Install the requirements:

```
pip install -r requirements.txt
```

Optionally install FFmpeg and SciPy for loading and saving audio files,
respectively.

Install [FFmpeg](https://ffmpeg.org/):

```
# on macOS using Homebrew (https://brew.sh/)
brew install ffmpeg
```

Install SciPy:

```
pip install scipy
```

### Example

An example using the model:

```python
import mlx.core as mx
from encodec import EncodecModel
from utils import load_audio, save_audio

# Load the 48 KHz model and preprocessor.
model, processor = EncodecModel.from_pretrained("mlx-community/encodec-48khz-float32")

# Load an audio file
audio = load_audio("path/to/audio", model.sampling_rate, model.channels)

# Preprocess the audio (this can also be a list of arrays for batched
# processing).
feats, mask = processor(audio)

# Encode at the given bandwidth. A lower bandwidth results in more
# compression but lower reconstruction quality.
@mx.compile
def encode(feats, mask):
    return model.encode(feats, mask, bandwidth=3)

# Decode to reconstruct the audio
@mx.compile
def decode(codes, scales, mask):
    return model.decode(codes, scales, mask)


codes, scales = encode(feats, mask)
reconstructed = decode(codes, scales, mask)

# Trim any padding:
reconstructed = reconstructed[0, : len(audio)]

# Save the audio as a wave file
save_audio("reconstructed.wav", reconstructed, model.sampling_rate)
```

The 24 KHz, 32 KHz, and 48 KHz MLX formatted models are available in the
[Hugging Face MLX Community](https://huggingface.co/collections/mlx-community/encodec-66e62334038300b07a43b164)
in several data types.

### Optional

To convert models, use the `convert.py` script. To see the options, run:

```bash
python convert.py -h
```

[^1]: Refer to the [arXiv paper](https://arxiv.org/abs/2210.13438) and
  [code](https://github.com/facebookresearch/encodec) for more details.

>>>> encodec/test.py
# Copyright © 2024 Apple Inc.

import mlx.core as mx
import numpy as np
import torch
from transformers import AutoProcessor
from transformers import EncodecModel as PTEncodecModel

from encodec import EncodecModel, preprocess_audio


def compare_processors():
    np.random.seed(0)
    audio_length = 95500
    audio = np.random.uniform(size=(2, audio_length)).astype(np.float32)

    processor = AutoProcessor.from_pretrained("facebook/encodec_48khz")

    pt_inputs = processor(
        raw_audio=audio, sampling_rate=processor.sampling_rate, return_tensors="pt"
    )
    mx_inputs = preprocess_audio(
        mx.array(audio).T,
        processor.sampling_rate,
        processor.chunk_length,
        processor.chunk_stride,
    )

    assert np.array_equal(pt_inputs["input_values"], mx_inputs[0].moveaxis(2, 1))
    assert np.array_equal(pt_inputs["padding_mask"], mx_inputs[1])


def compare_models():
    pt_model = PTEncodecModel.from_pretrained("facebook/encodec_48khz")
    mx_model, _ = EncodecModel.from_pretrained("mlx-community/encodec-48khz-float32")

    np.random.seed(0)
    audio_length = 190560
    audio = np.random.uniform(size=(1, audio_length, 2)).astype(np.float32)
    mask = np.ones((1, audio_length), dtype=np.int32)
    pt_encoded = pt_model.encode(
        torch.tensor(audio).moveaxis(2, 1), torch.tensor(mask)[None]
    )
    mx_encoded = mx_model.encode(mx.array(audio), mx.array(mask))
    pt_codes = pt_encoded.audio_codes.numpy()
    mx_codes = mx_encoded[0]
    assert np.array_equal(pt_codes, mx_codes), "Encoding codes mismatch"

    for mx_scale, pt_scale in zip(mx_encoded[1], pt_encoded.audio_scales):
        if mx_scale is not None:
            pt_scale = pt_scale.numpy()
            assert np.allclose(pt_scale, mx_scale, atol=1e-3, rtol=1e-4)

    pt_audio = pt_model.decode(
        pt_encoded.audio_codes, pt_encoded.audio_scales, torch.tensor(mask)[None]
    )
    pt_audio = pt_audio[0].squeeze().T.detach().numpy()
    mx_audio = mx_model.decode(*mx_encoded, mx.array(mask))
    mx_audio = mx_audio.squeeze()
    assert np.allclose(
        pt_audio, mx_audio, atol=1e-4, rtol=1e-4
    ), "Decoding audio mismatch"


if __name__ == "__main__":
    compare_processors()
    compare_models()

>>>> encodec/convert.py
# Copyright © 2024 Apple Inc.

import argparse
import json
from pathlib import Path
from textwrap import dedent
from types import SimpleNamespace
from typing import Any, Dict, Union

import mlx.core as mx
import mlx.nn as nn
from huggingface_hub import snapshot_download

import encodec


def fetch_from_hub(hf_repo: str) -> Path:
    model_path = Path(
        snapshot_download(
            repo_id=hf_repo,
            allow_patterns=["*.json", "*.safetensors"],
        )
    )
    return model_path


def upload_to_hub(path: str, upload_repo: str, hf_path: str):
    """
    Uploads the model to Hugging Face hub.

    Args:
        path (str): Local path to the model.
        upload_repo (str): Name of the HF repo to upload to.
        hf_path (str): Path to the original Hugging Face model.
    """
    import os

    from huggingface_hub import HfApi, ModelCard, logging

    content = dedent(
        f"""
        ---
        language: en
        license: other
        library: mlx
        tags:
        - mlx
        ---

        The Model [{upload_repo}](https://huggingface.co/{upload_repo}) was
        converted to MLX format from
        [{hf_path}](https://huggingface.co/{hf_path}).

        This model is intended to be used with the [EnCodec MLX
        example](https://github.com/ml-explore/mlx-examples/tree/main/encodec).
        """
    )

    card = ModelCard(content)
    card.save(os.path.join(path, "README.md"))

    logging.set_verbosity_info()

    api = HfApi()
    api.create_repo(repo_id=upload_repo, exist_ok=True)
    api.upload_folder(
        folder_path=path,
        repo_id=upload_repo,
        repo_type="model",
        multi_commits=True,
        multi_commits_verbose=True,
    )
    print(f"Upload successful, go to https://huggingface.co/{upload_repo} for details.")


def save_weights(save_path: Union[str, Path], weights: Dict[str, Any]) -> None:
    if isinstance(save_path, str):
        save_path = Path(save_path)
    save_path.mkdir(parents=True, exist_ok=True)

    total_size = sum(v.nbytes for v in weights.values())
    index_data = {"metadata": {"total_size": total_size}, "weight_map": {}}
    mx.save_safetensors(
        str(save_path / "model.safetensors"), weights, metadata={"format": "mlx"}
    )

    for weight_name in weights.keys():
        index_data["weight_map"][weight_name] = "model.safetensors"

    index_data["weight_map"] = {
        k: index_data["weight_map"][k] for k in sorted(index_data["weight_map"])
    }

    with open(save_path / "model.safetensors.index.json", "w") as f:
        json.dump(index_data, f, indent=4)


def save_config(
    config: dict,
    config_path: Union[str, Path],
) -> None:
    """Save the model configuration to the ``config_path``.

    The final configuration will be sorted before saving for better readability.

    Args:
        config (dict): The model configuration.
        config_path (Union[str, Path]): Model configuration file path.
    """
    # Clean unused keys
    config.pop("_name_or_path", None)

    # sort the config for better readability
    config = dict(sorted(config.items()))

    # write the updated config to the config_path (if provided)
    with open(config_path, "w") as fid:
        json.dump(config, fid, indent=4)


def convert(
    upload: bool,
    model: str,
    dtype: str = None,
):
    hf_repo = f"facebook/encodec_{model}"
    mlx_repo = f"mlx-community/encodec-{model}-{dtype}"
    path = fetch_from_hub(hf_repo)
    save_path = Path("mlx_models")

    weights = mx.load(str(Path(path) / "model.safetensors"))

    with open(path / "config.json", "r") as fid:
        config = SimpleNamespace(**json.load(fid))

    model = encodec.EncodecModel(config)

    new_weights = {}
    for k, v in weights.items():
        basename, pname = k.rsplit(".", 1)
        if pname == "weight_v":
            g = weights[basename + ".weight_g"]
            v = g * (v / mx.linalg.norm(v, axis=(1, 2), keepdims=True))
            k = basename + ".weight"
        elif pname in ["weight_g", "embed_avg", "cluster_size", "inited"]:
            continue
        elif "lstm" in basename:
            w_or_b, ih_or_hh, ln = pname.split("_")
            if w_or_b == "weight":
                new_pname = "Wx" if ih_or_hh == "ih" else "Wh"
            elif w_or_b == "bias" and ih_or_hh == "ih":
                continue
            else:
                v = v + weights[k.replace("_hh_", "_ih_")]
                new_pname = "bias"
            k = basename + "." + ln[1:] + "." + new_pname
        if "conv.weight" in k:
            # Possibly a transposed conv which has a different order
            if "decoder" in k:
                ln = int(k.split(".")[2])
                if "conv" in model.decoder.layers[ln] and isinstance(
                    model.decoder.layers[ln].conv, nn.ConvTranspose1d
                ):
                    v = mx.moveaxis(v, 0, 2)
                else:
                    v = mx.moveaxis(v, 1, 2)
            else:
                v = mx.moveaxis(v, 1, 2)

        new_weights[k] = v
    weights = new_weights

    model.load_weights(list(weights.items()))

    if dtype is not None:
        t = getattr(mx, dtype)
        weights = {k: v.astype(t) for k, v in weights.items()}

    if isinstance(save_path, str):
        save_path = Path(save_path)

    save_weights(save_path, weights)

    save_config(vars(config), config_path=save_path / "config.json")

    if upload:
        upload_to_hub(save_path, mlx_repo, hf_repo)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Convert EnCodec weights to MLX.")
    parser.add_argument(
        "--model",
        type=str,
        default="48khz",
        help="",
        choices=["24khz", "32khz", "48khz"],
    )
    parser.add_argument(
        "--upload",
        action="store_true",
        help="Upload the weights to Hugging Face.",
    )
    parser.add_argument(
        "--dtype",
        type=str,
        help="Data type to convert the model to.",
        default="float32",
        choices=["float32", "bfloat16", "float16"],
    )
    args = parser.parse_args()
    convert(upload=args.upload, model=args.model, dtype=args.dtype)

>>>> ACKNOWLEDGMENTS.md
# Individual Contributors

If you wish to be acknowledged for your contributions, please list your name
with a short description of your contribution(s) below. For example:

- Jane Smith: Added the `foo` example.

MLX Examples was developed with contributions from the following individuals:

- Juarez Bochi: Added support for T5 models.
- Sarthak Yadav: Added the `cifar` and `speechcommands` examples.
- Shunta Saito: Added support for PLaMo models.
- Gabrijel Boduljak: Implemented `CLIP`.
- Markus Enzweiler: Added the `cvae` examples.
- Prince Canuma: Helped add support for `Starcoder2` models.
- Shiyu Li: Added the `Segment Anything Model`.
- Gökdeniz Gülmez: Added support for `MiniCPM`, `Helium`, `Mamba version 1`, `OLMoE` archtectures and support for `full-fine-tuning`.
>>>> clip/hf_preproc.py
import mlx.core as mx
import transformers
from PIL import Image

import clip

hf_model = "openai/clip-vit-base-patch32"
mlx_model = "mlx_model"

model, *_ = clip.load(mlx_model)
processor = transformers.CLIPProcessor.from_pretrained(hf_model)

inputs = processor(
    text=["a photo of a cat", "a photo of a dog"],
    images=[Image.open("assets/cat.jpeg"), Image.open("assets/dog.jpeg")],
    return_tensors="np",
)

out = model(
    input_ids=mx.array(inputs.input_ids),
    pixel_values=mx.array(inputs.pixel_values).transpose((0, 2, 3, 1)),
    return_loss=True,
)

print("text embeddings:")
print(out.text_embeds)
print("image embeddings:")
print(out.image_embeds)
print(f"CLIP loss: {out.loss.item():.3f}")

>>>> clip/clip.py
from typing import Tuple

from image_processor import CLIPImageProcessor
from model import CLIPModel
from tokenizer import CLIPTokenizer


def load(model_dir: str) -> Tuple[CLIPModel, CLIPTokenizer, CLIPImageProcessor]:
    model = CLIPModel.from_pretrained(model_dir)
    tokenizer = CLIPTokenizer.from_pretrained(model_dir)
    img_processor = CLIPImageProcessor.from_pretrained(model_dir)
    return model, tokenizer, img_processor


if __name__ == "__main__":
    from PIL import Image

    model, tokenizer, img_processor = load("mlx_model")
    inputs = {
        "input_ids": tokenizer(["a photo of a cat", "a photo of a dog"]),
        "pixel_values": img_processor(
            [Image.open("assets/cat.jpeg"), Image.open("assets/dog.jpeg")]
        ),
    }
    output = model(**inputs)

    # Get text and image embeddings:
    text_embeds = output.text_embeds
    image_embeds = output.image_embeds
    print("Text embeddings shape:", text_embeds.shape)
    print("Image embeddings shape:", image_embeds.shape)

>>>> clip/tokenizer.py
# Copyright © 2023-2024 Apple Inc.

import json
from pathlib import Path
from typing import Any

import mlx.core as mx
import regex


class CLIPTokenizer:
    """A simple port of CLIPTokenizer from https://github.com/huggingface/transformers/ ."""

    def __init__(self, bpe_ranks, vocab):
        self.bpe_ranks = bpe_ranks
        self.vocab = vocab
        self.pat = regex.compile(
            r"""<\|startoftext\|>|<\|endoftext\|>|'s|'t|'re|'ve|'m|'ll|'d|[\p{L}]+|[\p{N}]|[^\s\p{L}\p{N}]+""",
            regex.IGNORECASE,
        )
        self._cache = {self.bos: self.bos, self.eos: self.eos}

    @property
    def bos(self):
        return "<|startoftext|>"

    @property
    def bos_token(self):
        return self.vocab[self.bos]

    @property
    def eos(self):
        return "<|endoftext|>"

    @property
    def eos_token(self):
        return self.vocab[self.eos]

    def bpe(self, text):
        if text in self._cache:
            return self._cache[text]

        unigrams = list(text[:-1]) + [text[-1] + "</w>"]
        unique_bigrams = set(zip(unigrams, unigrams[1:]))

        if not unique_bigrams:
            return unigrams

        # In every iteration try to merge the two most likely bigrams. If none
        # was merged we are done.
        #
        # Ported from https://github.com/huggingface/transformers/blob/main/src/transformers/models/clip/tokenization_py
        while unique_bigrams:
            bigram = min(
                unique_bigrams, key=lambda pair: self.bpe_ranks.get(pair, float("inf"))
            )
            if bigram not in self.bpe_ranks:
                break

            new_unigrams = []
            skip = False
            for a, b in zip(unigrams, unigrams[1:]):
                if skip:
                    skip = False
                    continue

                if (a, b) == bigram:
                    new_unigrams.append(a + b)
                    skip = True

                else:
                    new_unigrams.append(a)

            if not skip:
                new_unigrams.append(b)

            unigrams = new_unigrams
            unique_bigrams = set(zip(unigrams, unigrams[1:]))

        self._cache[text] = unigrams

        return unigrams

    def __call__(self, *args: Any, **kwargs: Any) -> Any:
        return self.tokenize(*args, **kwargs)

    def tokenize(self, text, prepend_bos=True, append_eos=True) -> mx.array:
        if isinstance(text, list):
            return mx.array([self.tokenize(t, prepend_bos, append_eos) for t in text])

        # Lower case, cleanup, and split. Hugging Face does a much,
        # more thorough job here but this should suffice for 95% of
        # cases.
        clean_text = regex.sub(r"\s+", " ", text.lower())
        tokens = regex.findall(self.pat, clean_text)

        # Split the tokens according to the byte-pair merge file
        bpe_tokens = [ti for t in tokens for ti in self.bpe(t)]

        # Map to token ids and return
        tokens = []
        if prepend_bos:
            tokens.append(self.bos_token)
        tokens.extend(self.vocab[t] for t in bpe_tokens)
        if append_eos:
            tokens.append(self.eos_token)
        return mx.array(tokens)

    @staticmethod
    def from_pretrained(path: str):
        path = Path(path)

        with open(path / "vocab.json", encoding="utf-8") as f:
            vocab = json.load(f)
        with open(path / "merges.txt", encoding="utf-8") as f:
            bpe_merges = f.read().strip().split("\n")[1 : 49152 - 256 - 2 + 1]

        bpe_merges = [tuple(m.split()) for m in bpe_merges]
        bpe_ranks = dict(map(reversed, enumerate(bpe_merges)))

        return CLIPTokenizer(bpe_ranks, vocab)

>>>> clip/README.md
# CLIP

An example of OpenAI's CLIP in MLX. The CLIP (contrastive language-image
pre-training) model embeds images and text in the same space.[^1]

### Setup

Install the dependencies:

```shell
pip install -r requirements.txt
```

Next, download a CLIP model from Hugging Face and convert it to MLX. The
default model is
[openai/clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32).

```
python convert.py
```

The script will by default download the model and configuration files to the
directory ``mlx_model/``.

### Run

You can use the CLIP model to embed images and text. 

```python
from PIL import Image
import clip

model, tokenizer, img_processor = clip.load("mlx_model")
inputs = {
    "input_ids": tokenizer(["a photo of a cat", "a photo of a dog"]),
    "pixel_values": img_processor(
        [Image.open("assets/cat.jpeg"), Image.open("assets/dog.jpeg")]
    ),
}
output = model(**inputs)

# Get text and image embeddings:
text_embeds = output.text_embeds
image_embeds = output.image_embeds
```

Run the above example with `python clip.py`.

To embed only images or only the text, pass only the ``input_ids`` or
``pixel_values``, respectively.

This example re-implements minimal image preprocessing and tokenization to reduce
dependencies. For additional preprocessing functionality, you can use
``transformers``. The file `hf_preproc.py` has an example.

MLX CLIP has been tested and works with the following Hugging Face repos:

- [openai/clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32)
- [openai/clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)

You can run the tests with:

```shell
python test.py
```

To test new models, update the `MLX_PATH` and `HF_PATH` in `test.py`.

### Attribution

- `assets/cat.jpeg` is a "Cat" by London's, licensed under CC BY-SA 2.0.
- `assets/dog.jpeg` is a "Happy Dog" by tedmurphy, licensed under CC BY 2.0.

[^1]: Refer to the original paper [Learning Transferable Visual Models From
  Natural Language Supervision ](https://arxiv.org/abs/2103.00020) or [blog
  post](https://openai.com/research/clip)

>>>> clip/model.py
# Copyright © 2023-2024 Apple Inc.

import glob
import json
import logging
import math
from dataclasses import dataclass
from pathlib import Path
from typing import Optional

import mlx.core as mx
import mlx.nn as nn
from mlx.core import linalg as LA
from mlx.nn.losses import cross_entropy


@dataclass
class CLIPVisionOutput:
    pooler_output: mx.array
    last_hidden_state: mx.array
    hidden_states: Optional[mx.array]


@dataclass
class CLIPTextOutput:
    pooler_output: mx.array
    last_hidden_state: mx.array


@dataclass
class CLIPModelOutput:
    loss: Optional[mx.array]
    text_embeds: Optional[mx.array]
    image_embeds: Optional[mx.array]
    text_model_output: CLIPTextOutput
    vision_model_output: CLIPVisionOutput


@dataclass
class CLIPTextConfig:
    num_hidden_layers: int
    hidden_size: int
    intermediate_size: int
    num_attention_heads: int
    max_position_embeddings: int
    vocab_size: int
    layer_norm_eps: float


@dataclass
class CLIPVisionConfig:
    num_hidden_layers: int
    hidden_size: int
    intermediate_size: int
    num_attention_heads: int
    num_channels: int
    image_size: int
    patch_size: int
    layer_norm_eps: float


@dataclass
class CLIPConfig:
    text_config: CLIPTextConfig
    vision_config: CLIPVisionConfig
    projection_dim: int


def quick_gelu(x: mx.array) -> mx.array:
    """
    A fast GELU approximation https://github.com/hendrycks/GELUs
    """
    return x * mx.sigmoid(1.702 * x)


def clip_loss(logits: mx.array) -> mx.array:
    N, M = logits.shape
    caption_loss = cross_entropy(logits, mx.arange(N), reduction="mean")
    image_loss = cross_entropy(logits.T, mx.arange(M), reduction="mean")
    return (caption_loss + image_loss) / 2.0


class Attention(nn.Module):
    def __init__(
        self,
        dims: int,
        num_heads: int,
        query_input_dims: Optional[int] = None,
        key_input_dims: Optional[int] = None,
        value_input_dims: Optional[int] = None,
        value_dims: Optional[int] = None,
        value_output_dims: Optional[int] = None,
        bias: bool = False,
    ):
        super().__init__()

        if (dims % num_heads) != 0:
            raise ValueError(
                "The input feature dimensions should be divisible by the "
                f"number of heads ({dims} % {num_heads}) != 0"
            )

        query_input_dims = query_input_dims or dims
        key_input_dims = key_input_dims or dims
        value_input_dims = value_input_dims or key_input_dims
        value_dims = value_dims or dims
        value_output_dims = value_output_dims or dims

        self.num_heads = num_heads
        self.q_proj = nn.Linear(query_input_dims, dims, bias=bias)
        self.k_proj = nn.Linear(key_input_dims, dims, bias=bias)
        self.v_proj = nn.Linear(value_input_dims, value_dims, bias=bias)
        self.out_proj = nn.Linear(value_dims, value_output_dims, bias=bias)

    def __call__(self, queries, keys, values, mask=None):
        queries = self.q_proj(queries)
        keys = self.k_proj(keys)
        values = self.v_proj(values)

        num_heads = self.num_heads
        B, L, D = queries.shape
        _, S, _ = keys.shape
        queries = queries.reshape(B, L, num_heads, -1).transpose(0, 2, 1, 3)
        keys = keys.reshape(B, S, num_heads, -1).transpose(0, 2, 3, 1)
        values = values.reshape(B, S, num_heads, -1).transpose(0, 2, 1, 3)

        scale = math.sqrt(1 / queries.shape[-1])
        scores = (queries * scale) @ keys
        if mask is not None:
            scores = scores + mask.astype(scores.dtype)
        scores = mx.softmax(scores, axis=-1)
        values_hat = (scores @ values).transpose(0, 2, 1, 3).reshape(B, L, -1)

        return self.out_proj(values_hat)


class MLP(nn.Module):
    def __init__(self, config: CLIPTextConfig):
        super().__init__()
        self.config = config
        self.activation_fn = quick_gelu
        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)
        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)

    def __call__(self, x: mx.array) -> mx.array:
        x = self.activation_fn(self.fc1(x))
        x = self.fc2(x)
        return x


class EncoderLayer(nn.Module):
    """The transformer encoder layer from CLIP."""

    def __init__(self, config: CLIPTextConfig):
        super().__init__()
        self.embed_dim = config.hidden_size
        # Add biases to the attention projections
        self.self_attn = Attention(
            config.hidden_size, config.num_attention_heads, bias=True
        )
        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)
        self.mlp = MLP(config)
        self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)

    def __call__(self, x: mx.array, mask: Optional[mx.array] = None) -> mx.array:
        y = self.layer_norm1(x)
        y = self.self_attn(y, y, y, mask)
        x = x + y
        y = self.layer_norm2(x)
        y = self.mlp(y)
        return x + y


class TextEmbeddings(nn.Module):
    def __init__(self, config: CLIPTextConfig):
        super().__init__()
        embed_dim = config.hidden_size

        self.token_embedding = nn.Embedding(config.vocab_size, embed_dim)
        self.position_embedding = nn.Embedding(
            config.max_position_embeddings, embed_dim
        )

    def __call__(self, x: mx.array) -> mx.array:
        embeddings = self.token_embedding(x)
        embeddings += self.position_embedding.weight[: x.shape[1]]
        return embeddings


class Encoder(nn.Module):
    def __init__(self, config: CLIPTextConfig):
        self.layers = [EncoderLayer(config) for _ in range(config.num_hidden_layers)]


class ClipTextModel(nn.Module):
    """Implements the text encoder transformer from CLIP."""

    def __init__(self, config: CLIPTextConfig):
        super().__init__()
        self.embeddings = TextEmbeddings(config)
        self.encoder = Encoder(config)
        self.final_layer_norm = nn.LayerNorm(config.hidden_size)

    def __call__(self, x: mx.array) -> CLIPTextOutput:
        B, N = x.shape
        eot_tokens = mx.argmax(x, axis=-1)
        x = self.embeddings(x)
        mask = nn.MultiHeadAttention.create_additive_causal_mask(N, x.dtype)
        for l in self.encoder.layers:
            x = l(x, mask)
        last_hidden_state = self.final_layer_norm(x)
        pooler_output = last_hidden_state[mx.arange(B), eot_tokens]

        return CLIPTextOutput(
            pooler_output=pooler_output, last_hidden_state=last_hidden_state
        )


class VisionEmbeddings(nn.Module):
    def __init__(self, config: CLIPVisionConfig):
        super().__init__()
        self.config = config
        self.embed_dim = config.hidden_size
        self.image_size = config.image_size
        self.patch_size = config.patch_size

        self.class_embedding = mx.zeros((config.hidden_size,))

        self.patch_embedding = nn.Conv2d(
            in_channels=config.num_channels,
            out_channels=self.embed_dim,
            kernel_size=self.patch_size,
            stride=self.patch_size,
            bias=False,
        )

        self.num_patches = (self.image_size // self.patch_size) ** 2
        self.num_positions = self.num_patches + 1
        self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim)

    def __call__(self, x: mx.array) -> mx.array:
        batch_size = x.shape[0]
        # Patchify using conv:
        # [batch_size, sqrt(num_patches), sqrt(num_patches), embed_dim]
        patch_embeddings = self.patch_embedding(x)
        # [batch_size, num_patches, embed_dim]
        patch_embeddings = mx.flatten(patch_embeddings, start_axis=1, end_axis=2)
        embed_dim = patch_embeddings.shape[-1]
        # Prepend <CLS> embeddings
        # [batch_size, 1, embed_dim]
        cls_embeddings = mx.broadcast_to(
            self.class_embedding, (batch_size, 1, embed_dim)
        )
        # [batch_size, num_patches + 1, embed_dim]
        embeddings = mx.concatenate((cls_embeddings, patch_embeddings), axis=1)
        # Add positional encoding
        embeddings += self.position_embedding.weight
        return embeddings


class ClipVisionModel(nn.Module):
    """Implements the vision encoder transformer from CLIP."""

    def __init__(self, config: CLIPVisionConfig):
        super().__init__()
        self.embeddings = VisionEmbeddings(config)
        self.pre_layrnorm = nn.LayerNorm(config.hidden_size)
        self.encoder = Encoder(config)
        self.post_layernorm = nn.LayerNorm(config.hidden_size)

    def __call__(
        self,
        x: mx.array,
        output_hidden_states: Optional[bool] = None,
    ) -> CLIPVisionOutput:
        x = self.embeddings(x)
        x = self.pre_layrnorm(x)

        encoder_states = (x,) if output_hidden_states else None

        for l in self.encoder.layers:
            x = l(x, mask=None)
            if output_hidden_states:
                encoder_states = encoder_states + (x,)

        # Extract <CLS> token embedding
        pooler_output = self.post_layernorm(x[:, 0, :])
        return CLIPVisionOutput(
            pooler_output=pooler_output,
            last_hidden_state=x,
            hidden_states=encoder_states,
        )


class CLIPModel(nn.Module):
    def __init__(self, config: CLIPConfig):
        self.text_model = ClipTextModel(config.text_config)
        self.vision_model = ClipVisionModel(config.vision_config)

        text_embed_dim = config.text_config.hidden_size
        vision_embed_dim = config.vision_config.hidden_size
        projection_dim = config.projection_dim

        self.visual_projection = nn.Linear(vision_embed_dim, projection_dim, bias=False)
        self.text_projection = nn.Linear(text_embed_dim, projection_dim, bias=False)
        self.logit_scale = mx.array(0.0)

    def get_text_features(self, x: mx.array) -> mx.array:
        return self.text_projection(self.text_model(x).pooler_output)

    def get_image_features(self, x: mx.array) -> mx.array:
        return self.visual_projection(self.vision_model(x).pooler_output)

    def __call__(
        self,
        input_ids: Optional[mx.array] = None,
        pixel_values: Optional[mx.array] = None,
        return_loss=False,
    ) -> CLIPModelOutput:
        if input_ids is not None:
            text_model_output = self.text_model(input_ids)
            text_embeds = self.text_projection(text_model_output.pooler_output)
            text_embeds = text_embeds / LA.norm(text_embeds, axis=-1, keepdims=True)
        else:
            text_embeds = None
            text_model_output = None

        if pixel_values is not None:
            vision_model_output = self.vision_model(pixel_values)
            image_embeds = self.visual_projection(vision_model_output.pooler_output)
            image_embeds = image_embeds / LA.norm(image_embeds, axis=-1, keepdims=True)
        else:
            image_embeds = None
            vision_model_output = None

        if return_loss and (input_ids is None or pixel_values is None):
            raise ValueError("Must provide text and image inputs to compute loss.")

        if return_loss:
            logit_scale = mx.exp(self.logit_scale)
            logits = (text_embeds @ image_embeds.T) * logit_scale
            loss = clip_loss(logits)
        else:
            loss = None

        return CLIPModelOutput(
            loss=loss,
            text_embeds=text_embeds,
            image_embeds=image_embeds,
            vision_model_output=vision_model_output,
            text_model_output=text_model_output,
        )

    @staticmethod
    def from_pretrained(path: str):
        path = Path(path)

        with open(path / "config.json", "r") as fid:
            config = json.load(fid)

        text_config = config["text_config"]
        text_config = CLIPTextConfig(
            num_hidden_layers=text_config["num_hidden_layers"],
            hidden_size=text_config["hidden_size"],
            intermediate_size=text_config["intermediate_size"],
            num_attention_heads=text_config["num_attention_heads"],
            max_position_embeddings=text_config["max_position_embeddings"],
            vocab_size=text_config["vocab_size"],
            layer_norm_eps=text_config["layer_norm_eps"],
        )

        vision_config = config["vision_config"]

        vision_config = CLIPVisionConfig(
            num_hidden_layers=vision_config["num_hidden_layers"],
            hidden_size=vision_config["hidden_size"],
            intermediate_size=vision_config["intermediate_size"],
            num_attention_heads=vision_config["num_attention_heads"],
            num_channels=3,
            image_size=vision_config["image_size"],
            patch_size=vision_config["patch_size"],
            layer_norm_eps=vision_config["layer_norm_eps"],
        )

        config = CLIPConfig(
            text_config=text_config,
            vision_config=vision_config,
            projection_dim=config["projection_dim"],
        )
        model = CLIPModel(config)
        weight_files = glob.glob(str(path / "*.safetensors"))
        if not weight_files:
            logging.error(f"No safetensors found in {path}")
            raise FileNotFoundError(f"No safetensors found in {path}")

        weights = {}
        for wf in weight_files:
            weights.update(mx.load(wf))

        weights = model.sanitize(weights)
        model.load_weights(list(weights.items()))
        return model

    @staticmethod
    def sanitize(weights):
        sanitized_weights = {}
        for k, v in weights.items():
            if "position_ids" in k:
                # Remove unused position_ids
                continue
            elif "patch_embedding.weight" in k:
                # pytorch conv2d expects the weight tensor to be of shape [out_channels, in_channels, kH, KW]
                # mlx conv2d expects the weight tensor to be of shape [out_channels, kH, KW, in_channels]
                sanitized_weights[k] = v.transpose(0, 2, 3, 1)
            else:
                sanitized_weights[k] = v

        return sanitized_weights

>>>> clip/test.py
import unittest

import mlx.core as mx
import model
import numpy as np
import torch
import transformers
from image_processor import CLIPImageProcessor
from PIL import Image
from tokenizer import CLIPTokenizer
from transformers import AutoTokenizer
from transformers.image_processing_utils import ChannelDimension

MLX_PATH = "mlx_model"
HF_PATH = "openai/clip-vit-base-patch32"


def load_mlx_models(path):
    image_proc = CLIPImageProcessor.from_pretrained(path)
    tokenizer = CLIPTokenizer.from_pretrained(path)
    clip = model.CLIPModel.from_pretrained(path)
    return image_proc, tokenizer, clip


def load_hf_models(path):
    image_proc = transformers.CLIPImageProcessor.from_pretrained(path)
    tokenizer = AutoTokenizer.from_pretrained(path)
    clip = transformers.CLIPModel.from_pretrained(path)
    return image_proc, tokenizer, clip


class TestCLIP(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        cls.mx_image_proc, cls.mx_tokenizer, cls.mx_clip = load_mlx_models(MLX_PATH)
        cls.hf_image_proc, cls.hf_tokenizer, cls.hf_clip = load_hf_models(HF_PATH)

    def test_image_processor(self):
        image = Image.open("assets/cat.jpeg")

        mx_data = self.mx_image_proc([image])
        hf_data = mx.array(
            np.array(
                self.hf_image_proc([image], data_format=ChannelDimension.LAST)[
                    "pixel_values"
                ]
            )
        )
        self.assertTrue(mx.allclose(mx_data, hf_data, atol=1e-5))

    def test_text_tokenizer(self):
        texts = ["a photo of a cat", "a photo of a dog"]
        for txt in texts:
            self.assertTrue(
                np.array_equal(
                    self.mx_tokenizer.tokenize(txt)[None, :],
                    self.hf_tokenizer(txt, return_tensors="np")["input_ids"],
                ),
            )

    def test_text_encoder(self):
        texts = ["a photo of a cat", "a photo of a dog"]
        # Tokenize
        hf_tokens = self.hf_tokenizer(texts, return_tensors="pt")
        mx_tokens = self.mx_tokenizer(texts)
        # Get expected
        with torch.inference_mode():
            expected_out = self.hf_clip.text_model(**hf_tokens)
            expected_last_hidden = expected_out.last_hidden_state.numpy()
            expected_pooler_output = expected_out.pooler_output.numpy()
        out = self.mx_clip.text_model(mx_tokens)
        self.assertTrue(
            np.allclose(out.last_hidden_state, expected_last_hidden, atol=1e-5)
        )
        self.assertTrue(
            np.allclose(out.pooler_output, expected_pooler_output, atol=1e-5)
        )

    def test_vision_encoder(self):
        # Load and process test image
        x = self.hf_image_proc(
            images=[Image.open("assets/dog.jpeg")], return_tensors="np"
        ).pixel_values

        # Infer with HuggingFace model
        with torch.inference_mode():
            # Get expected
            x_tc = torch.tensor(x)
            expected_out = self.hf_clip.vision_model(x_tc, output_hidden_states=True)
            expected_last_hidden = expected_out.last_hidden_state.numpy()
            expected_pooler_output = expected_out.pooler_output.numpy()
            expected_hidden_states = [hs.numpy() for hs in expected_out.hidden_states]
        # Test MLX vision encoder
        out = self.mx_clip.vision_model(
            mx.array(x.transpose(0, 2, 3, 1)), output_hidden_states=True
        )
        self.assertTrue(
            np.allclose(
                out.last_hidden_state, expected_last_hidden, rtol=1e-4, atol=1e-3
            ),
        )
        self.assertTrue(
            np.allclose(
                out.pooler_output, expected_pooler_output, rtol=1e-4, atol=1e-3
            ),
        )
        for expected_hs, out_hs in zip(expected_hidden_states, out.hidden_states):
            self.assertTrue(
                np.allclose(expected_hs, out_hs, rtol=1e-4, atol=1e-3),
            )

    def test_clip_model(self):
        image_input = self.hf_image_proc(
            images=[Image.open("assets/cat.jpeg"), Image.open("assets/dog.jpeg")],
            return_tensors="np",
        )["pixel_values"]
        text = ["a photo of a cat", "a photo of a dog"]
        tokens = self.hf_tokenizer(text, return_tensors="np")["input_ids"]
        with torch.inference_mode():
            expected_out = self.hf_clip(
                input_ids=torch.tensor(tokens),
                pixel_values=torch.tensor(image_input),
                return_loss=True,
            )

        out = self.mx_clip(
            input_ids=mx.array(tokens),
            pixel_values=mx.array(image_input.transpose((0, 2, 3, 1))),
            return_loss=True,
        )

        self.assertTrue(
            np.allclose(out.text_embeds, expected_out.text_embeds, atol=1e-5)
        )
        self.assertTrue(
            np.allclose(out.image_embeds, expected_out.image_embeds, atol=1e-5)
        )
        self.assertTrue(np.allclose(out.loss, expected_out.loss, atol=1e-5))


if __name__ == "__main__":
    unittest.main()

>>>> clip/linear_probe.py
# Mirror of the Linear Probe Evaluation Script
# from the official CLIP Repository.

import mlx.core as mx
import numpy as np
from image_processor import CLIPImageProcessor
from mlx.data.datasets import load_cifar10
from model import CLIPModel
from PIL import Image
from sklearn.linear_model import LogisticRegression
from tqdm import tqdm


def get_cifar10(batch_size, root=None):
    tr = load_cifar10(root=root).batch(batch_size)
    test = load_cifar10(root=root, train=False).batch(batch_size)

    return tr, test


def get_features(model, image_proc, iter):
    all_features = []
    all_labels = []

    for batch in tqdm(iter):
        image, label = batch["image"], batch["label"]
        x = image_proc([Image.fromarray(im) for im in image])
        y = mx.array(label)

        image_embeds = model.get_image_features(x)
        mx.eval(image_embeds)

        all_features.append(image_embeds)
        all_labels.append(y)

    return mx.concatenate(all_features), mx.concatenate(all_labels)


if __name__ == "__main__":
    model = CLIPModel.from_pretrained("mlx_model")
    image_proc = CLIPImageProcessor.from_pretrained("mlx_model")

    train_iter, test_iter = get_cifar10(batch_size=256)
    train_features, train_labels = get_features(model, image_proc, train_iter)
    test_features, test_labels = get_features(model, image_proc, test_iter)

    # Perform logistic regression
    # NOTE: The value of C should be determined via a hyperparameter sweep
    # using a validation split
    classifier = LogisticRegression(random_state=0, C=0.316, max_iter=1000, verbose=1)
    classifier.fit(train_features, train_labels)

    # Evaluate using the logistic regression classifier
    predictions = classifier.predict(test_features)
    accuracy = (test_labels.squeeze() == predictions).mean().item() * 100
    print(f"Accuracy = {accuracy:.3f}")

>>>> clip/convert.py
# Copyright © 2023-2024 Apple Inc.

import argparse
import json
import shutil
from pathlib import Path
from typing import Any, Dict, Union

import mlx.core as mx
import torch
from huggingface_hub import snapshot_download


def make_shards(weights: dict, max_file_size_gb: int = 5) -> list:
    max_file_size_bytes = max_file_size_gb << 30
    shards = []
    shard, shard_size = {}, 0
    for k, v in weights.items():
        if shard_size + v.nbytes > max_file_size_bytes:
            shards.append(shard)
            shard, shard_size = {}, 0
        shard[k] = v
        shard_size += v.nbytes
    shards.append(shard)
    return shards


def save_weights(save_path: Union[str, Path], weights: Dict[str, Any]) -> None:
    """Save model weights into specified directory."""
    if isinstance(save_path, str):
        save_path = Path(save_path)
    save_path.mkdir(parents=True, exist_ok=True)

    shards = make_shards(weights)
    shards_count = len(shards)
    shard_file_format = (
        "model-{:05d}-of-{:05d}.safetensors"
        if shards_count > 1
        else "model.safetensors"
    )

    total_size = sum(v.nbytes for v in weights.values())
    index_data = {"metadata": {"total_size": total_size}, "weight_map": {}}

    for i, shard in enumerate(shards):
        shard_name = shard_file_format.format(i + 1, shards_count)
        shard_path = save_path / shard_name

        mx.save_safetensors(str(shard_path), shard)

        for weight_name in shard.keys():
            index_data["weight_map"][weight_name] = shard_name

    index_data["weight_map"] = {
        k: index_data["weight_map"][k] for k in sorted(index_data["weight_map"])
    }

    with open(save_path / "model.safetensors.index.json", "w") as f:
        json.dump(
            index_data,
            f,
            indent=4,
        )


def get_model_path(path_or_hf_repo: str, force_download: bool = False) -> Path:
    model_path = Path(path_or_hf_repo)
    if not model_path.exists():
        model_path = Path(
            snapshot_download(
                repo_id=path_or_hf_repo,
                allow_patterns=[
                    "*.bin",
                    "*.json",
                    "*.txt",
                ],
                force_download=force_download,
            )
        )
    return model_path


def torch_to_mx(a: torch.Tensor, *, dtype: str) -> mx.array:
    # bfloat16 is not numpy convertible. Upcast to float32 to avoid precision loss
    a = a.to(torch.float32) if dtype == "bfloat16" else a.to(getattr(torch, dtype))
    return mx.array(a.numpy(), getattr(mx, dtype))


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Download and Convert (OpenAI) CLIP weights to MLX"
    )
    parser.add_argument(
        "--hf-repo",
        type=str,
        default="openai/clip-vit-base-patch32",
        help="Hugging Face repository name.",
    )
    parser.add_argument(
        "--mlx-path",
        type=str,
        default="mlx_model",
        help="Path to save the MLX model.",
    )
    parser.add_argument(
        "--dtype",
        help="The data type to save the converted model.",
        type=str,
        default="float32",
    )
    parser.add_argument(
        "-f",
        "--force-download",
        help="Force download the model from Hugging Face.",
        action="store_true",
    )
    args = parser.parse_args()

    torch_path = get_model_path(args.hf_repo, args.force_download)
    mlx_path = Path(args.mlx_path)
    mlx_path.mkdir(parents=True, exist_ok=True)

    print("[INFO] Loading")
    torch_weights = torch.load(torch_path / "pytorch_model.bin", weights_only=True)
    print("[INFO] Converting")
    mlx_weights = {
        k: torch_to_mx(v, dtype=args.dtype) for k, v in torch_weights.items()
    }
    print("[INFO] Saving")
    save_weights(mlx_path, mlx_weights)
    for fn in ["config.json", "merges.txt", "vocab.json", "preprocessor_config.json"]:
        shutil.copyfile(
            str(torch_path / f"{fn}"),
            str(mlx_path / f"{fn}"),
        )

>>>> clip/image_processor.py
# Copyright © 2023-2024 Apple Inc.

import json
from pathlib import Path
from typing import List, Tuple

import mlx.core as mx
import numpy as np
from PIL.Image import Image


class CLIPImageProcessor:
    """
    A simple port of
    https://github.com/huggingface/transformers/blob/main/src/transformers/models/clip/image_processing_clip.py.
    """

    def __init__(
        self,
        crop_size: int = 224,
        do_center_crop: bool = True,
        do_normalize: bool = True,
        do_resize: bool = True,
        image_mean: List[float] = [0.48145466, 0.4578275, 0.40821073],
        image_std: List[float] = [0.26862954, 0.26130258, 0.27577711],
        size: int = 224,
        **kwargs
    ) -> None:
        self.crop_size = crop_size
        self.do_center_crop = do_center_crop
        self.do_normalize = do_normalize
        self.do_resize = do_resize
        self.image_mean = mx.array(image_mean)
        self.image_std = mx.array(image_std)
        self.size = size

    def __call__(self, images: List[Image]) -> mx.array:
        return mx.concatenate(
            [self._preprocess(image)[None] for image in images], axis=0
        )

    def _preprocess(self, image: Image) -> mx.array:
        if self.do_resize:
            image = resize(image, self.size)
        if self.do_center_crop:
            image = center_crop(image, (self.crop_size, self.crop_size))
        image = mx.array(np.array(image))
        image = rescale(image)
        if self.do_normalize:
            image = normalize(image, self.image_mean, self.image_std)
        return image

    @staticmethod
    def from_pretrained(path: str):
        path = Path(path)
        with open(path / "preprocessor_config.json", encoding="utf-8") as f:
            config = json.load(f)
        return CLIPImageProcessor(**config)


def resize(image: Image, short_size: int) -> Image:
    """
    Resize so small size to short_size
    """
    width, height = image.size
    short = min(width, height)
    long = max(width, height)
    if short == short_size:
        return image
    new_short = short_size
    new_long = int(short_size * long / short)
    new_size = (new_short, new_long) if width <= height else (new_long, new_short)
    return image.resize(new_size)


def center_crop(image: Image, size: Tuple[int, int]) -> Image:
    if size[0] % 2 != 0 or size[1] % 2 != 0:
        raise ValueError("Only even crop sizes supported.")
    original_width, original_height = image.size
    crop_height, crop_width = size
    top = (original_height - crop_height) // 2
    bottom = top + crop_height
    left = (original_width - crop_width) // 2
    right = left + crop_width
    return image.crop((left, top, right, bottom))


def rescale(image: mx.array) -> mx.array:
    return image.astype(mx.float32) * (1 / 255.0)


def normalize(image: mx.array, mean: mx.array, std: mx.array) -> mx.array:
    return (image - mean) / std

>>>> transformer_lm/main.py
# Copyright © 2023-2024 Apple Inc.

import math
import time
from functools import partial

import datasets
import mlx.core as mx
import mlx.nn as nn
import mlx.optimizers as optim
from mlx.utils import tree_flatten


class TransformerLM(nn.Module):
    def __init__(
        self,
        vocab_size: int,
        num_layers: int,
        dims: int,
        num_heads: int,
        checkpoint: bool,
    ):
        super().__init__()

        self.embedding = nn.Embedding(vocab_size, dims)
        self.pe = nn.SinusoidalPositionalEncoding(dims)
        self.transformer = nn.TransformerEncoder(
            num_layers, dims, num_heads, norm_first=True, checkpoint=checkpoint
        )
        self.out_proj = nn.Linear(dims, vocab_size)

    def __call__(self, x):
        L = x.shape[1]
        mask = nn.MultiHeadAttention.create_additive_causal_mask(L)
        x = self.embedding(x)
        x = x + self.pe(mx.arange(L))
        x = self.transformer(x, mask)
        return self.out_proj(x)


def to_samples(context_size, dataset):
    window_size = context_size + 1  # include target
    samples = dataset.size // window_size
    dataset = dataset[: samples * window_size]
    return mx.array(dataset.reshape(samples, -1))


def iterate_batches(batch_size, context_size, dataset):
    inputs = to_samples(context_size, dataset)
    s = 0
    while True:
        if s == 0:
            # Reset permutation:
            perm = mx.random.permutation(inputs.shape[0])
        ids = perm[s : s + batch_size]
        yield inputs[ids]
        s += batch_size
        if s >= inputs.shape[0]:
            s = 0


def main(args):
    batch_size = args.batch_size
    context_size = args.context_size
    steps_per_eval = args.steps_per_eval
    steps_per_report = args.steps_per_report

    # Load vocab and dataset:
    vocab, train, valid, test = datasets.load_dataset(args.dataset)

    # Initialize model:
    model = TransformerLM(
        len(vocab), args.num_blocks, args.dim, args.num_heads, args.checkpoint
    )
    mx.eval(model.parameters())
    nparams = sum(
        x.size for k, x in tree_flatten(model.parameters()) if "embedding" not in k
    )
    print(f"Training a transformer with {nparams / 1024**2:.3f} M parameters")

    def loss_fn(model, inputs, reduction="mean"):
        x, y = inputs[..., :-1], inputs[..., 1:]
        logits = model(x)
        return nn.losses.cross_entropy(logits, y, reduction=reduction)

    optimizer = optim.AdamW(
        learning_rate=args.learning_rate, weight_decay=args.weight_decay
    )

    def eval_fn(dataset):
        inputs = to_samples(context_size, dataset)
        loss = 0
        for s in range(0, inputs.shape[0], batch_size):
            losses = loss_fn(model, inputs[s : s + batch_size], reduction="sum")
            loss += losses.item()
        return loss / (inputs.size - inputs.shape[0])

    state = [model.state, optimizer.state]

    @partial(mx.compile, inputs=state, outputs=state)
    def step(inputs):
        loss_and_grad_fn = nn.value_and_grad(model, loss_fn)
        loss, grads = loss_and_grad_fn(model, inputs)
        optimizer.update(model, grads)
        return loss

    train_iterator = iterate_batches(batch_size, context_size, train)
    losses = []
    tic = time.perf_counter()
    for it, inputs in zip(range(args.num_iters), train_iterator):
        optimizer.learning_rate = min(1, it / args.lr_warmup) * args.learning_rate
        loss = step(inputs)
        mx.eval(state)
        losses.append(loss.item())
        if (it + 1) % steps_per_report == 0:
            train_loss = sum(losses) / len(losses)
            toc = time.perf_counter()
            print(
                f"Iter {it + 1}: Train loss {train_loss:.3f}, "
                f"It/sec {steps_per_report / (toc - tic):.3f}"
            )
            losses = []
            tic = time.perf_counter()
        if (it + 1) % steps_per_eval == 0:
            val_loss = eval_fn(valid)
            toc = time.perf_counter()
            print(
                f"Iter {it + 1}: "
                f"Val loss {val_loss:.3f}, "
                f"Val ppl {math.exp(val_loss):.3f}, "
                f"Val took {(toc - tic):.3f}s, "
            )
            tic = time.perf_counter()

    if args.eval_test:
        test_loss = eval_fn(test)
        test_ppl = math.exp(test_loss)
        print(f"Test loss {test_loss:.3f}, Test ppl {test_ppl:.3f}.")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser("Train a decoder-only Transformer LM with MLX.")
    parser.add_argument("--gpu", action="store_true", help="Use the Metal back-end.")
    parser.add_argument("--seed", type=int, default=42, help="Seed for the RNGs.")
    parser.add_argument(
        "--dataset",
        type=str,
        default="ptb",
        choices=["enwik8", "ptb", "wikitext2", "wikitext103"],
        help="Dataset to train and evaluate on.",
    )
    parser.add_argument(
        "--context_size",
        type=int,
        default=1024,
        help="Context size in tokens of the model.",
    )
    parser.add_argument(
        "--num_blocks", type=int, default=12, help="Number of Transformer blocks."
    )
    parser.add_argument(
        "--dim",
        type=int,
        default=1024,
        help="Dimensionality of embeddings and hidden layers.",
    )
    parser.add_argument(
        "--num_heads",
        type=int,
        default=16,
        help="Number of heads used for multi-head attention",
    )
    parser.add_argument(
        "--checkpoint", action="store_true", help="Perform gradient checkpointing"
    )
    parser.add_argument("--batch_size", type=int, default=2, help="Minibatch size.")
    parser.add_argument(
        "--num_iters", type=int, default=100000, help="Iterations to train for."
    )
    parser.add_argument(
        "--learning_rate", type=float, default=3e-4, help="AdamW learning rate."
    )
    parser.add_argument(
        "--weight_decay", type=float, default=1e-5, help="Set the weight decay"
    )
    parser.add_argument(
        "--lr_warmup", type=int, default=200, help="LR linear warmup iterations"
    )
    parser.add_argument(
        "--steps_per_report",
        type=int,
        default=10,
        help="Number of training steps between loss reporting.",
    )
    parser.add_argument(
        "--steps_per_eval",
        type=int,
        default=1000,
        help="Number of training steps between validations.",
    )
    parser.add_argument(
        "--eval_test",
        action="store_true",
        help="Evaluate on the test set after training",
    )
    args = parser.parse_args()
    if not args.gpu:
        mx.set_default_device(mx.cpu)
    main(args)

>>>> transformer_lm/README.md
# Transformer LM 

This is an example of a decoder-only Transformer LM. The only dependency is
MLX. 

Run the example on the GPU with:

```
python main.py --gpu
```

By default the dataset is the [PTB corpus](https://paperswithcode.com/dataset/penn-treebank). Choose a different dataset with the `--dataset` option.

>>>> transformer_lm/datasets.py
# Copyright © 2023 Apple Inc.

import io
import itertools
import os
import zipfile
from urllib import request

import numpy as np


def load_dataset(dataname):
    if dataname == "enwik8":
        return enwik8()
    elif dataname == "ptb":
        return ptb()
    elif dataname == "wikitext2":
        return wikitext(dataset="2")
    else:
        return wikitext(dataset="103")


def _load(save_dir, filenames):
    # *NB* First file is expected to be the training set
    with open(os.path.join(save_dir, filenames[0]), "r") as fid:
        vocab = set(t for l in fid.readlines() for t in l.strip().split(" "))
    eos = "<eos>"
    vocab.add(eos)
    vocab = {v: i for i, v in enumerate(vocab)}

    def to_array(dataset):
        with open(os.path.join(save_dir, dataset), "r") as fid:
            lines = (l.strip().split(" ") for l in fid.readlines())
        return np.array(
            [vocab[w] for line in lines for w in itertools.chain(line, [eos])],
            dtype=np.uint32,
        )

    datasets = [to_array(fn) for fn in filenames]
    return vocab, *datasets


def wikitext(dataset="2", save_dir="/tmp"):
    """
    Load the WikiText-* language modeling dataset:
        https://paperswithcode.com/dataset/wikitext-2
        https://paperswithcode.com/dataset/wikitext-103

    """
    if dataset not in ("2", "103"):
        raise ValueError(f'Dataset must be either "2" or "103", got {dataset}')

    filenames = ["wiki.train.tokens", "wiki.valid.tokens", "wiki.test.tokens"]
    dataname = f"wikitext-{dataset}"
    data_dir = os.path.join(save_dir, dataname)
    if not os.path.exists(data_dir):
        base_url = "https://s3.amazonaws.com/research.metamind.io/wikitext/"
        zip_file_url = base_url + dataname + "-v1.zip"
        r = request.urlopen(zip_file_url)
        with zipfile.ZipFile(io.BytesIO(r.read())) as zf:
            zf.extractall(save_dir)

    return _load(data_dir, filenames)


def ptb(save_dir="/tmp"):
    """
    Load the PTB language modeling dataset:
        https://paperswithcode.com/dataset/penn-treebank
    """
    filenames = [
        "ptb.train.txt",
        "ptb.valid.txt",
        "ptb.test.txt",
    ]

    def download_and_save(save_dir):
        base_url = "https://raw.githubusercontent.com/wojzaremba/lstm/master/data/"
        for name in filenames:
            out_file = os.path.join(save_dir, name)
            if not os.path.exists(out_file):
                request.urlretrieve(base_url + name, out_file)

    save_dir = os.path.join(save_dir, "ptb")
    if not os.path.exists(save_dir):
        os.mkdir(save_dir)
    download_and_save(save_dir)

    return _load(save_dir, filenames)


def enwik8(save_dir="/tmp"):
    """
    Load the enwik8 language modeling dataset:
        https://mattmahoney.net/dc/textdata.html
    """
    out_file = os.path.join(save_dir, "enwik8.zip")
    if not os.path.exists(out_file):
        request.urlretrieve("http://mattmahoney.net/dc/enwik8.zip", out_file)

    with zipfile.ZipFile(out_file) as zf:
        data = zf.read("enwik8")

    num_test_bytes = 5000000  # 90 + 5 + 5 split

    train_data = data[: -2 * num_test_bytes]
    valid_data = data[-2 * num_test_bytes : -num_test_bytes]
    test_data = data[-num_test_bytes:]

    vocab = set(c for c in train_data)
    vocab = {c: i for i, c in enumerate(vocab)}

    def to_array(dataset):
        return np.array([vocab[c] for c in dataset], dtype=np.uint32)

    return vocab, to_array(train_data), to_array(valid_data), to_array(test_data)


if __name__ == "__main__":
    vocab, train, val, test = enwik8()
    assert len(vocab) == 205, "enwik8: Wrong vocab size"

    vocab, train, val, test = ptb()
    assert len(vocab) == 10000, "PTB: Wrong vocab size"

    vocab, train, val, test = wikitext()
    assert len(vocab) == 33279, "WikiText: Wrong vocab size"

>>>> flux/txt2image.py
# Copyright © 2024 Apple Inc.

import argparse

import mlx.core as mx
import mlx.nn as nn
import numpy as np
from PIL import Image
from tqdm import tqdm

from flux import FluxPipeline


def to_latent_size(image_size):
    h, w = image_size
    h = ((h + 15) // 16) * 16
    w = ((w + 15) // 16) * 16

    if (h, w) != image_size:
        print(
            "Warning: The image dimensions need to be divisible by 16px. "
            f"Changing size to {h}x{w}."
        )

    return (h // 8, w // 8)


def quantization_predicate(name, m):
    return hasattr(m, "to_quantized") and m.weight.shape[1] % 512 == 0


def load_adapter(flux, adapter_file, fuse=False):
    weights, lora_config = mx.load(adapter_file, return_metadata=True)
    rank = int(lora_config["lora_rank"])
    num_blocks = int(lora_config["lora_blocks"])
    flux.linear_to_lora_layers(rank, num_blocks)
    flux.flow.load_weights(list(weights.items()), strict=False)
    if fuse:
        flux.fuse_lora_layers()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Generate images from a textual prompt using stable diffusion"
    )
    parser.add_argument("prompt")
    parser.add_argument("--model", choices=["schnell", "dev"], default="schnell")
    parser.add_argument("--n-images", type=int, default=4)
    parser.add_argument(
        "--image-size", type=lambda x: tuple(map(int, x.split("x"))), default=(512, 512)
    )
    parser.add_argument("--steps", type=int)
    parser.add_argument("--guidance", type=float, default=4.0)
    parser.add_argument("--n-rows", type=int, default=1)
    parser.add_argument("--decoding-batch-size", type=int, default=1)
    parser.add_argument("--quantize", "-q", action="store_true")
    parser.add_argument("--preload-models", action="store_true")
    parser.add_argument("--output", default="out.png")
    parser.add_argument("--save-raw", action="store_true")
    parser.add_argument("--seed", type=int)
    parser.add_argument("--verbose", "-v", action="store_true")
    parser.add_argument("--adapter")
    parser.add_argument("--fuse-adapter", action="store_true")
    parser.add_argument("--no-t5-padding", dest="t5_padding", action="store_false")
    args = parser.parse_args()

    # Load the models
    flux = FluxPipeline("flux-" + args.model, t5_padding=args.t5_padding)
    args.steps = args.steps or (50 if args.model == "dev" else 2)

    if args.adapter:
        load_adapter(flux, args.adapter, fuse=args.fuse_adapter)

    if args.quantize:
        nn.quantize(flux.flow, class_predicate=quantization_predicate)
        nn.quantize(flux.t5, class_predicate=quantization_predicate)
        nn.quantize(flux.clip, class_predicate=quantization_predicate)

    if args.preload_models:
        flux.ensure_models_are_loaded()

    # Make the generator
    latent_size = to_latent_size(args.image_size)
    latents = flux.generate_latents(
        args.prompt,
        n_images=args.n_images,
        num_steps=args.steps,
        latent_size=latent_size,
        guidance=args.guidance,
        seed=args.seed,
    )

    # First we get and eval the conditioning
    conditioning = next(latents)
    mx.eval(conditioning)
    peak_mem_conditioning = mx.metal.get_peak_memory() / 1024**3
    mx.metal.reset_peak_memory()

    # The following is not necessary but it may help in memory constrained
    # systems by reusing the memory kept by the text encoders.
    del flux.t5
    del flux.clip

    # Actual denoising loop
    for x_t in tqdm(latents, total=args.steps):
        mx.eval(x_t)

    # The following is not necessary but it may help in memory constrained
    # systems by reusing the memory kept by the flow transformer.
    del flux.flow
    peak_mem_generation = mx.metal.get_peak_memory() / 1024**3
    mx.metal.reset_peak_memory()

    # Decode them into images
    decoded = []
    for i in tqdm(range(0, args.n_images, args.decoding_batch_size)):
        decoded.append(flux.decode(x_t[i : i + args.decoding_batch_size], latent_size))
        mx.eval(decoded[-1])
    peak_mem_decoding = mx.metal.get_peak_memory() / 1024**3
    peak_mem_overall = max(
        peak_mem_conditioning, peak_mem_generation, peak_mem_decoding
    )

    if args.save_raw:
        *name, suffix = args.output.split(".")
        name = ".".join(name)
        x = mx.concatenate(decoded, axis=0)
        x = (x * 255).astype(mx.uint8)
        for i in range(len(x)):
            im = Image.fromarray(np.array(x[i]))
            im.save(".".join([name, str(i), suffix]))
    else:
        # Arrange them on a grid
        x = mx.concatenate(decoded, axis=0)
        x = mx.pad(x, [(0, 0), (4, 4), (4, 4), (0, 0)])
        B, H, W, C = x.shape
        x = x.reshape(args.n_rows, B // args.n_rows, H, W, C).transpose(0, 2, 1, 3, 4)
        x = x.reshape(args.n_rows * H, B // args.n_rows * W, C)
        x = (x * 255).astype(mx.uint8)

        # Save them to disc
        im = Image.fromarray(np.array(x))
        im.save(args.output)

    # Report the peak memory used during generation
    if args.verbose:
        print(f"Peak memory used for the text:       {peak_mem_conditioning:.3f}GB")
        print(f"Peak memory used for the generation: {peak_mem_generation:.3f}GB")
        print(f"Peak memory used for the decoding:   {peak_mem_decoding:.3f}GB")
        print(f"Peak memory used overall:            {peak_mem_overall:.3f}GB")

>>>> flux/dreambooth.py
# Copyright © 2024 Apple Inc.

import argparse
import time
from functools import partial
from pathlib import Path

import mlx.core as mx
import mlx.nn as nn
import mlx.optimizers as optim
import numpy as np
from mlx.nn.utils import average_gradients
from mlx.utils import tree_flatten, tree_map, tree_reduce
from PIL import Image

from flux import FluxPipeline, Trainer, load_dataset, save_config


def generate_progress_images(iteration, flux, args):
    """Generate images to monitor the progress of the finetuning."""
    out_dir = Path(args.output_dir)
    out_dir.mkdir(parents=True, exist_ok=True)
    out_file = out_dir / f"{iteration:07d}_progress.png"
    print(f"Generating {str(out_file)}", flush=True)

    # Generate some images and arrange them in a grid
    n_rows = 2
    n_images = 4
    x = flux.generate_images(
        args.progress_prompt,
        n_images,
        args.progress_steps,
    )
    x = mx.pad(x, [(0, 0), (4, 4), (4, 4), (0, 0)])
    B, H, W, C = x.shape
    x = x.reshape(n_rows, B // n_rows, H, W, C).transpose(0, 2, 1, 3, 4)
    x = x.reshape(n_rows * H, B // n_rows * W, C)
    x = mx.pad(x, [(4, 4), (4, 4), (0, 0)])
    x = (x * 255).astype(mx.uint8)

    # Save them to disc
    im = Image.fromarray(np.array(x))
    im.save(out_file)


def save_adapters(adapter_name, flux, args):
    out_dir = Path(args.output_dir)
    out_dir.mkdir(parents=True, exist_ok=True)
    out_file = out_dir / adapter_name
    print(f"Saving {str(out_file)}")

    mx.save_safetensors(
        str(out_file),
        dict(tree_flatten(flux.flow.trainable_parameters())),
        metadata={
            "lora_rank": str(args.lora_rank),
            "lora_blocks": str(args.lora_blocks),
        },
    )


def setup_arg_parser():
    """Set up and return the argument parser."""
    parser = argparse.ArgumentParser(
        description="Finetune Flux to generate images with a specific subject"
    )

    parser.add_argument(
        "--model",
        default="dev",
        choices=[
            "dev",
            "schnell",
        ],
        help="Which flux model to train",
    )
    parser.add_argument(
        "--guidance", type=float, default=4.0, help="The guidance factor to use."
    )
    parser.add_argument(
        "--iterations",
        type=int,
        default=600,
        help="How many iterations to train for",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=1,
        help="The batch size to use when training the stable diffusion model",
    )
    parser.add_argument(
        "--resolution",
        type=lambda x: tuple(map(int, x.split("x"))),
        default=(512, 512),
        help="The resolution of the training images",
    )
    parser.add_argument(
        "--num-augmentations",
        type=int,
        default=5,
        help="Augment the images by random cropping and panning",
    )
    parser.add_argument(
        "--progress-prompt",
        required=True,
        help="Use this prompt when generating images for evaluation",
    )
    parser.add_argument(
        "--progress-steps",
        type=int,
        default=50,
        help="Use this many steps when generating images for evaluation",
    )
    parser.add_argument(
        "--progress-every",
        type=int,
        default=50,
        help="Generate images every PROGRESS_EVERY steps",
    )
    parser.add_argument(
        "--checkpoint-every",
        type=int,
        default=50,
        help="Save the model every CHECKPOINT_EVERY steps",
    )
    parser.add_argument(
        "--lora-blocks",
        type=int,
        default=-1,
        help="Train the last LORA_BLOCKS transformer blocks",
    )
    parser.add_argument(
        "--lora-rank", type=int, default=8, help="LoRA rank for finetuning"
    )
    parser.add_argument(
        "--warmup-steps", type=int, default=100, help="Learning rate warmup"
    )
    parser.add_argument(
        "--learning-rate", type=float, default="1e-4", help="Learning rate for training"
    )
    parser.add_argument(
        "--grad-accumulate",
        type=int,
        default=4,
        help="Accumulate gradients for that many iterations before applying them",
    )
    parser.add_argument(
        "--output-dir", default="mlx_output", help="Folder to save the checkpoints in"
    )

    parser.add_argument("dataset")
    return parser


if __name__ == "__main__":
    parser = setup_arg_parser()
    args = parser.parse_args()

    output_path = Path(args.output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    save_config(vars(args), output_path / "adapter_config.json")

    # Load the model and set it up for LoRA training. We use the same random
    # state when creating the LoRA layers so all workers will have the same
    # initial weights.
    mx.random.seed(0x0F0F0F0F)
    flux = FluxPipeline("flux-" + args.model)
    flux.flow.freeze()
    flux.linear_to_lora_layers(args.lora_rank, args.lora_blocks)

    # Reset the seed to a different seed per worker if we are in distributed
    # mode so that each worker is working on different data, diffusion step and
    # random noise.
    mx.random.seed(0xF0F0F0F0 + mx.distributed.init().rank())

    # Report how many parameters we are training
    trainable_params = tree_reduce(
        lambda acc, x: acc + x.size, flux.flow.trainable_parameters(), 0
    )
    print(f"Training {trainable_params / 1024 ** 2:.3f}M parameters", flush=True)

    # Set up the optimizer and training steps. The steps are a bit verbose to
    # support gradient accumulation together with compilation.
    warmup = optim.linear_schedule(0, args.learning_rate, args.warmup_steps)
    cosine = optim.cosine_decay(
        args.learning_rate, args.iterations // args.grad_accumulate
    )
    lr_schedule = optim.join_schedules([warmup, cosine], [args.warmup_steps])
    optimizer = optim.Adam(learning_rate=lr_schedule)
    state = [flux.flow.state, optimizer.state, mx.random.state]

    @partial(mx.compile, inputs=state, outputs=state)
    def single_step(x, t5_feat, clip_feat, guidance):
        loss, grads = nn.value_and_grad(flux.flow, flux.training_loss)(
            x, t5_feat, clip_feat, guidance
        )
        grads = average_gradients(grads)
        optimizer.update(flux.flow, grads)

        return loss

    @partial(mx.compile, inputs=state, outputs=state)
    def compute_loss_and_grads(x, t5_feat, clip_feat, guidance):
        return nn.value_and_grad(flux.flow, flux.training_loss)(
            x, t5_feat, clip_feat, guidance
        )

    @partial(mx.compile, inputs=state, outputs=state)
    def compute_loss_and_accumulate_grads(x, t5_feat, clip_feat, guidance, prev_grads):
        loss, grads = nn.value_and_grad(flux.flow, flux.training_loss)(
            x, t5_feat, clip_feat, guidance
        )
        grads = tree_map(lambda a, b: a + b, prev_grads, grads)
        return loss, grads

    @partial(mx.compile, inputs=state, outputs=state)
    def grad_accumulate_and_step(x, t5_feat, clip_feat, guidance, prev_grads):
        loss, grads = nn.value_and_grad(flux.flow, flux.training_loss)(
            x, t5_feat, clip_feat, guidance
        )
        grads = tree_map(
            lambda a, b: (a + b) / args.grad_accumulate,
            prev_grads,
            grads,
        )
        grads = average_gradients(grads)
        optimizer.update(flux.flow, grads)

        return loss

    # We simply route to the appropriate step based on whether we have
    # gradients from a previous step and whether we should be performing an
    # update or simply computing and accumulating gradients in this step.
    def step(x, t5_feat, clip_feat, guidance, prev_grads, perform_step):
        if prev_grads is None:
            if perform_step:
                return single_step(x, t5_feat, clip_feat, guidance), None
            else:
                return compute_loss_and_grads(x, t5_feat, clip_feat, guidance)
        else:
            if perform_step:
                return (
                    grad_accumulate_and_step(
                        x, t5_feat, clip_feat, guidance, prev_grads
                    ),
                    None,
                )
            else:
                return compute_loss_and_accumulate_grads(
                    x, t5_feat, clip_feat, guidance, prev_grads
                )

    dataset = load_dataset(args.dataset)
    trainer = Trainer(flux, dataset, args)
    trainer.encode_dataset()

    guidance = mx.full((args.batch_size,), args.guidance, dtype=flux.dtype)

    # An initial generation to compare
    generate_progress_images(0, flux, args)

    grads = None
    losses = []
    tic = time.time()
    for i, batch in zip(range(args.iterations), trainer.iterate(args.batch_size)):
        loss, grads = step(*batch, guidance, grads, (i + 1) % args.grad_accumulate == 0)
        mx.eval(loss, grads, state)
        losses.append(loss.item())

        if (i + 1) % 10 == 0:
            toc = time.time()
            peak_mem = mx.metal.get_peak_memory() / 1024**3
            print(
                f"Iter: {i + 1} Loss: {sum(losses) / 10:.3f} "
                f"It/s: {10 / (toc - tic):.3f} "
                f"Peak mem: {peak_mem:.3f} GB",
                flush=True,
            )

        if (i + 1) % args.progress_every == 0:
            generate_progress_images(i + 1, flux, args)

        if (i + 1) % args.checkpoint_every == 0:
            save_adapters(f"{i + 1:07d}_adapters.safetensors", flux, args)

        if (i + 1) % 10 == 0:
            losses = []
            tic = time.time()

    save_adapters("final_adapters.safetensors", flux, args)
    print("Training successful.")

>>>> segment_anything/notebooks/predictor_example.ipynb
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmenting from Prompts\n",
    "\n",
    "This notebook walks through predicting object segmentations from a provided prompt. It uses the `Predictor` class. It is modified from [original SAM GitHub repo](https://github.com/facebookresearch/segment-anything/).\n",
    "\n",
    "### Setup\n",
    "Necessary imports and helper functions for displaying points, boxes, and masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import mlx.core as mx\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[:2]\n",
    "    mask_image = np.array(mask).reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "    \n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = np.array(coords)[labels==1]\n",
    "    neg_points = np.array(coords)[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n",
    "    \n",
    "def show_box(box, ax):\n",
    "    box = box.tolist()\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('images/truck.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(image)\n",
    "plt.axis('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting objects with SAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load the SAM model and predictor. Change the path below to point to the SAM checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from segment_anything.sam import load\n",
    "from segment_anything.predictor import SamPredictor\n",
    "\n",
    "sam_checkpoint = \"../sam-vit-base\"\n",
    "sam = load(sam_checkpoint)\n",
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the image to produce an image embedding by calling `SamPredictor.set_image`. `SamPredictor` remembers this embedding and will use it for subsequent mask prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.set_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select the truck, choose a point on it. Points are input to the model in (x,y) format and come with labels 1 (foreground point) or 0 (background point). Multiple points can be input; here we use only one. The chosen point will be shown as a star on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_point = mx.array([[500, 375]])\n",
    "input_label = mx.array([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(image)\n",
    "show_points(input_point, input_label, plt.gca())\n",
    "plt.axis('on')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict with `SamPredictor.predict`. The model returns masks, quality predictions for those masks, and low resolution mask logits that can be passed to the next iteration of prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks, scores, logits = predictor.predict(\n",
    "    point_coords=input_point[None],\n",
    "    point_labels=input_label[None],\n",
    "    multimask_output=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `multimask_output=True` (the default setting), SAM outputs 3 masks, where `scores` gives the model's own estimation of the quality of these masks. This setting is intended for ambiguous input prompts, and helps the model disambiguate different objects consistent with the prompt. When `False`, it will return a single mask. For ambiguous prompts such as a single point, it is recommended to use `multimask_output=True` even if only a single mask is desired; the best single mask can be chosen by picking the one with the highest score returned in `scores`. This will often result in a better mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(masks.shape[-1]):\n",
    "    mask = masks[..., i]\n",
    "    score = scores[..., i].item()\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(image)\n",
    "    show_mask(mask[0], plt.gca())\n",
    "    show_points(input_point, input_label, plt.gca())\n",
    "    plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n",
    "    plt.axis('off')\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying a specific object with additional points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The single input point is ambiguous, and the model has returned multiple objects consistent with it. To obtain a single object, multiple points can be provided. If available, a mask from a previous iteration can also be supplied to the model to aid in prediction. When specifying a single object with multiple prompts, a single mask can be requested by setting `multimask_output=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_point = mx.array([[500, 375], [1125, 625]])\n",
    "input_label = mx.array([1, 1])\n",
    "mask_input = logits[..., mx.argmax(scores)]  # Choose the model's best mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks, _, _ = predictor.predict(\n",
    "    point_coords=input_point[None],\n",
    "    point_labels=input_label[None],\n",
    "    mask_input=mask_input[..., None],\n",
    "    multimask_output=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(image)\n",
    "show_mask(masks[0], plt.gca())\n",
    "show_points(input_point, input_label, plt.gca())\n",
    "plt.axis('off')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To exclude the car and specify just the window, a background point (with label 0, here shown in red) can be supplied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_point = mx.array([[500, 375], [1125, 625]])\n",
    "input_label = mx.array([1, 0])\n",
    "mask_input = logits[..., mx.argmax(scores)]  # Choose the model's best mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks, _, _ = predictor.predict(\n",
    "    point_coords=input_point[None],\n",
    "    point_labels=input_label[None],\n",
    "    mask_input=mask_input[..., None],\n",
    "    multimask_output=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "show_mask(masks[0], plt.gca())\n",
    "show_points(input_point, input_label, plt.gca())\n",
    "plt.axis('off')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying a specific object with a box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can also take a box as input, provided in xyxy format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_box = mx.array([425, 600, 700, 875])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks, _, _ = predictor.predict(\n",
    "    point_coords=None,\n",
    "    point_labels=None,\n",
    "    box=input_box[None, :],\n",
    "    multimask_output=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "show_mask(masks[0, ..., 0], plt.gca())\n",
    "show_box(input_box, plt.gca())\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining points and boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points and boxes may be combined, just by including both types of prompts to the predictor. Here this can be used to select just the trucks's tire, instead of the entire wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_box = mx.array([425, 600, 700, 875])\n",
    "input_point = mx.array([[575, 750]])\n",
    "input_label = mx.array([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks, _, _ = predictor.predict(\n",
    "    point_coords=input_point[None],\n",
    "    point_labels=input_label[None],\n",
    "    box=input_box,\n",
    "    multimask_output=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "show_mask(masks[0, ..., 0], plt.gca())\n",
    "show_box(input_box, plt.gca())\n",
    "show_points(input_point, input_label, plt.gca())\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batched prompt inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SamPredictor` can take multiple input prompts for the same image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_boxes = mx.array([\n",
    "    [75, 275, 1725, 850],\n",
    "    [425, 600, 700, 875],\n",
    "    [1375, 550, 1650, 800],\n",
    "    [1240, 675, 1400, 750],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks, _, _ = predictor.predict(\n",
    "    point_coords=None,\n",
    "    point_labels=None,\n",
    "    box=input_boxes,\n",
    "    multimask_output=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "for mask in masks:\n",
    "    show_mask(mask, plt.gca(), random_color=True)\n",
    "for box in input_boxes:\n",
    "    show_box(box, plt.gca())\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-end batched inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all prompts are available in advance, it is possible to run SAM directly in an end-to-end fashion. This also allows batching over images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = image  # truck.jpg from above\n",
    "image1_boxes = mx.array([\n",
    "    [75, 275, 1725, 850],\n",
    "    [425, 600, 700, 875],\n",
    "    [1375, 550, 1650, 800],\n",
    "    [1240, 675, 1400, 750],\n",
    "])\n",
    "\n",
    "image2 = cv2.imread('images/groceries.jpg')\n",
    "image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "image2_boxes = mx.array([\n",
    "    [450, 170, 520, 350],\n",
    "    [350, 190, 450, 350],\n",
    "    [500, 170, 580, 350],\n",
    "    [580, 170, 640, 350],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both images and prompts are input as mlx array that are already transformed to the correct frame. Inputs are packaged as a list over images, which each element is a dict that takes the following keys:\n",
    "* `image`: The input image as a mlx array in HWC format.\n",
    "* `original_size`: The size of the image before transforming for input to SAM, in (H, W) format.\n",
    "* `point_coords`: Batched coordinates of point prompts.\n",
    "* `point_labels`: Batched labels of point prompts.\n",
    "* `boxes`: Batched input boxes.\n",
    "* `mask_inputs`: Batched input masks.\n",
    "\n",
    "If a prompt is not present, the key can be excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "resize_transform = ResizeLongestSide(sam.vision_encoder.img_size)\n",
    "\n",
    "def prepare_image(image, transform, device):\n",
    "    image = transform.apply_image(image)\n",
    "    image = mx.array(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_input = [\n",
    "    {\n",
    "        'image': prepare_image(image1, resize_transform, sam),\n",
    "        'boxes': resize_transform.apply_boxes(image1_boxes, image1.shape[:2]),\n",
    "        'original_size': image1.shape[:2]\n",
    "    },\n",
    "    {\n",
    "        'image': prepare_image(image2, resize_transform, sam),\n",
    "        'boxes': resize_transform.apply_boxes(image2_boxes, image2.shape[:2]),\n",
    "        'original_size': image2.shape[:2]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_output = sam(batched_input, multimask_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a list over results for each input image, where list elements are dictionaries with the following keys:\n",
    "* `masks`: A batched mlx array of predicted binary masks, the size of the original image.\n",
    "* `iou_predictions`: The model's prediction of the quality for each mask.\n",
    "* `low_res_logits`: Low res logits for each mask, which can be passed back to the model as mask input on a later iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20, 20))\n",
    "\n",
    "ax[0].imshow(image1)\n",
    "for mask in batched_output[0]['masks']:\n",
    "    show_mask(np.array(mask), ax[0], random_color=True)\n",
    "for box in image1_boxes:\n",
    "    show_box(np.array(box), ax[0])\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].imshow(image2)\n",
    "for mask in batched_output[1]['masks']:\n",
    "    show_mask(np.array(mask), ax[1], random_color=True)\n",
    "for box in image2_boxes:\n",
    "    show_box(np.array(box), ax[1])\n",
    "ax[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

>>>> flux/README.md
FLUX
====

FLUX implementation in MLX. The implementation is ported directly from
[https://github.com/black-forest-labs/flux](https://github.com/black-forest-labs/flux)
and the model weights are downloaded directly from the Hugging Face Hub.

The goal of this example is to be clean, educational and to allow for
experimentation with finetuning FLUX models as well as adding extra
functionality such as in-/outpainting, guidance with custom losses etc.

![MLX image](static/generated-mlx.png)    
*Image generated using FLUX-dev in MLX and the prompt 'An image in the style of
tron emanating futuristic technology with the word "MLX" in the center with
capital red letters.'*

Installation
------------

The dependencies are minimal, namely:

- `huggingface-hub` to download the checkpoints.
- `regex` for the tokenization
- `tqdm`, `PIL`, and `numpy` for the scripts
- `sentencepiece` for the T5 tokenizer
- `datasets` for using an HF dataset directly

You can install all of the above with the `requirements.txt` as follows:

    pip install -r requirements.txt


Usage
---------

You can use the following command to generate an image, using `--output` to specify the storage location of the image, defaulting to `out.png`.

```shell
python txt2image.py --model schnell 
    --n-images 1 
    --image-size 256x512 
    --verbose 
    'A photo of an astronaut riding a horse on Mars.'
```

For more parameters, please use the `--help` command to view.

```shell
python txt2image.py --help
```

Inference
---------

Inference in this example is similar to the stable diffusion example. The
classes to get you started are `FluxPipeline` from the `flux` module.

```python
import mlx.core as mx
from flux import FluxPipeline

# This will download all the weights from HF hub
flux = FluxPipeline("flux-schnell")

# Make a generator that returns the latent variables from the reverse diffusion
# process
latent_generator = flux.generate_latents(
    "A photo of an astronaut riding a horse on Mars",
    num_steps=4,
    latent_size=(32, 64),  # 256x512 image
)

# The first return value of the generator contains the conditioning and the
# random noise at the beginning of the diffusion process.
conditioning = next(latent_generator)
(
    x_T,                # The initial noise
    x_positions,        # The integer positions used for image positional encoding
    t5_conditioning,    # The T5 features from the text prompt
    t5_positions,       # Integer positions for text (normally all 0s)
    clip_conditioning,  # The clip text features from the text prompt
) = conditioning

# Returning the conditioning as the first output from the generator allows us
# to unload T5 and clip before running the diffusion transformer.
mx.eval(conditioning)

# Evaluate each diffusion step
for x_t in latent_generator:
    mx.eval(x_t)

# Note that we need to pass the latent size because it is collapsed and
# patchified in x_t and we need to unwrap it.
img = flux.decode(x_t, latent_size=(32, 64))
```

The above are essentially the implementation of the `txt2image.py` script
except for some additional logic to quantize and/or load trained adapters. One
can use the script as follows:

```shell
python txt2image.py 
    --n-images 4 
    --n-rows 2 
    --image-size 256x512 
    'A photo of an astronaut riding a horse on Mars.'
```

### Experimental Options

FLUX pads the prompt to a specific size of 512 tokens for the dev model and
256 for the schnell model. Not applying padding results in faster generation
but it is not clear how it may affect the generated images. To enable that
option in this example pass `--no-t5-padding` to the `txt2image.py` script or
instantiate the pipeline with `FluxPipeline("flux-schnell", t5_padding=False)`.

Finetuning
----------

The `dreambooth.py` script supports LoRA finetuning of FLUX-dev (and schnell
but ymmv) on a provided image dataset. The dataset folder must have an
`train.jsonl` file with the following format:

```jsonl
{"image": "path-to-image-relative-to-dataset", "prompt": "Prompt to use with this image"}
{"image": "path-to-image-relative-to-dataset", "prompt": "Prompt to use with this image"}
...
```

The training script by default trains for 600 iterations with a batch size of
1, gradient accumulation of 4 and LoRA rank of 8. Run `python dreambooth.py
--help` for the list of hyperparameters you can tune.

> [!Note]
> FLUX finetuning requires approximately 50GB of RAM. QLoRA is coming soon and
> should reduce this number significantly.

### Training Example

This is a step-by-step finetuning example. We will be using the data from
[https://github.com/google/dreambooth](https://github.com/google/dreambooth).
In particular, we will use `dog6` which is a popular example for showcasing
dreambooth [^1].

The training images are the following 5 images [^2]:

![dog6](static/dog6.png)

We start by making the following `train.jsonl` file and placing it in the same
folder as the images.

```jsonl
{"image": "00.jpg", "prompt": "A photo of sks dog"}
{"image": "01.jpg", "prompt": "A photo of sks dog"}
{"image": "02.jpg", "prompt": "A photo of sks dog"}
{"image": "03.jpg", "prompt": "A photo of sks dog"}
{"image": "04.jpg", "prompt": "A photo of sks dog"}
```

Subsequently we finetune FLUX using the following command:

```shell
python dreambooth.py 
    --progress-prompt 'A photo of an sks dog lying on the sand at a beach in Greece' 
    --progress-every 600 --iterations 1200 --learning-rate 0.0001 
    --lora-rank 4 --grad-accumulate 8 
    path/to/dreambooth/dataset/dog6
```


Or you can directly use the pre-processed Hugging Face dataset [mlx-community/dreambooth-dog6](https://huggingface.co/datasets/mlx-community/dreambooth-dog6) for fine-tuning.

```shell
python dreambooth.py 
    --progress-prompt 'A photo of an sks dog lying on the sand at a beach in Greece' 
    --progress-every 600 --iterations 1200 --learning-rate 0.0001 
    --lora-rank 4 --grad-accumulate 8 
    mlx-community/dreambooth-dog6
```

The training requires approximately 50GB of RAM and on an M2 Ultra it takes a
bit more than 1 hour.

### Using the Adapter

The adapters are saved in `mlx_output` and can be used directly by the
`txt2image.py` script. For instance,

```shell
python txt2image.py --model dev --save-raw --image-size 512x512 --n-images 1 
    --adapter mlx_output/final_adapters.safetensors 
    --fuse-adapter 
    --no-t5-padding 
    'A photo of an sks dog lying on the sand at a beach in Greece'
```

generates an image that looks like the following,

![dog image](static/dog-r4-g8-1200.png)

and of course we can pass `--image-size 512x1024` to get larger images with
different aspect ratios,

![wide dog image](static/dog-r4-g8-1200-512x1024.png)

The arguments that are relevant to the adapters are of course `--adapter` and
`--fuse-adapter`. The first defines the path to an adapter to apply to the
model and the second fuses the adapter back into the model to get a bit more
speed during generation.

[^1]: Refer to the [arXiv paper](https://arxiv.org/abs/2208.12242) for more details.
[^2]: The images are from unsplash by https://unsplash.com/@alvannee .

>>>> segment_anything/notebooks/automatic_mask_generator_example.ipynb
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatically generating object masks with SAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks through how to automatically segment objects in an image. It is modified from [original SAM GitHub repo](https://github.com/facebookresearch/segment-anything/).\n",
    "\n",
    "Since SAM can efficiently process prompts, masks for the entire image can be generated by sampling a large number of prompts over an image. This method was used to generate the dataset SA-1B. \n",
    "\n",
    "The class `SamAutomaticMaskGenerator` implements this. It samples single-point input prompts in a grid over the image, from each of which SAM then predicts multiple masks. The masks are filtered for quality and deduplicated using non-max suppression. Additional options allow for further improvement of mask quality and quantity, such as running prediction on multiple crops of the image or postprocessing masks to remove small disconnected regions and holes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_anns(anns):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "\n",
    "    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
    "    img[:,:,3] = 0\n",
    "    for ann in sorted_anns:\n",
    "        m = ann['segmentation']\n",
    "        color_mask = np.concatenate([np.random.random(3), [0.35]])\n",
    "        img[m] = color_mask\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('images/dog.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic mask generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run automatic mask generation, provide a SAM model to the `SamAutomaticMaskGenerator` class. Set the path below to the SAM checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from segment_anything import SamAutomaticMaskGenerator\n",
    "from segment_anything.sam import load\n",
    "\n",
    "sam_checkpoint = \"../sam-vit-base\"\n",
    "sam = load(sam_checkpoint)\n",
    "\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate masks, run `generate` on an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = mask_generator.generate(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mask generation returns a list over masks. Each item is a dictionary with keys:\n",
    "* `segmentation` : the mask\n",
    "* `area` : the area of the mask in pixels\n",
    "* `bbox` : the boundary box of the mask in XYWH format\n",
    "* `predicted_iou` : the model's own prediction for the quality of the mask\n",
    "* `point_coords` : the sampled input point that generated this mask\n",
    "* `stability_score` : an additional measure of mask quality\n",
    "* `crop_box` : the crop of the image used to generate this mask in XYWH format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(masks))\n",
    "print(masks[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all the masks overlayed on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(image)\n",
    "show_anns(masks)\n",
    "plt.axis('off')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic mask generation options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Generation can be automatically run on crops of the image to get better results for smaller objects. Post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_generator_2 = SamAutomaticMaskGenerator(\n",
    "    model=sam,\n",
    "    points_per_side=32,\n",
    "    pred_iou_thresh=0.86,\n",
    "    stability_score_thresh=0.92,\n",
    "    crop_n_layers=1,\n",
    "    crop_n_points_downscale_factor=2,\n",
    "    min_mask_region_area=100,  # Requires open-cv to run post-processing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks2 = mask_generator_2.generate(image)\n",
    "len(masks2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(image)\n",
    "show_anns(masks2)\n",
    "plt.axis('off')\n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

>>>> segment_anything/main.py
import argparse
import json
import os
import sys
from typing import Any, Dict, List

import cv2

from segment_anything import SamAutomaticMaskGenerator, sam

parser = argparse.ArgumentParser(
    description=(
        "Runs automatic mask generation on an input image or directory of images, "
        "and outputs masks as either PNGs or COCO-style RLEs. Requires open-cv, "
        "as well as pycocotools if saving in RLE format."
    )
)

parser.add_argument(
    "--input",
    type=str,
    required=True,
    help="Path to either a single input image or folder of images.",
)

parser.add_argument(
    "--output",
    type=str,
    required=True,
    help=(
        "Path to the directory where masks will be output. Output will be either a folder "
        "of PNGs per image or a single json with COCO-style masks."
    ),
)

parser.add_argument(
    "--model",
    type=str,
    required=True,
    help="The path to the SAM model to use for mask generation.",
)

parser.add_argument(
    "--convert-to-rle",
    action="store_true",
    help=(
        "Save masks as COCO RLEs in a single json instead of as a folder of PNGs. "
        "Requires pycocotools."
    ),
)

amg_settings = parser.add_argument_group("AMG Settings")

amg_settings.add_argument(
    "--points-per-side",
    type=int,
    default=None,
    help="Generate masks by sampling a grid over the image with this many points to a side.",
)

amg_settings.add_argument(
    "--points-per-batch",
    type=int,
    default=None,
    help="How many input points to process simultaneously in one batch.",
)

amg_settings.add_argument(
    "--pred-iou-thresh",
    type=float,
    default=None,
    help="Exclude masks with a predicted score from the model that is lower than this threshold.",
)

amg_settings.add_argument(
    "--stability-score-thresh",
    type=float,
    default=None,
    help="Exclude masks with a stability score lower than this threshold.",
)

amg_settings.add_argument(
    "--stability-score-offset",
    type=float,
    default=None,
    help="Larger values perturb the mask more when measuring stability score.",
)

amg_settings.add_argument(
    "--box-nms-thresh",
    type=float,
    default=None,
    help="The overlap threshold for excluding a duplicate mask.",
)

amg_settings.add_argument(
    "--crop-n-layers",
    type=int,
    default=None,
    help=(
        "If >0, mask generation is run on smaller crops of the image to generate more masks. "
        "The value sets how many different scales to crop at."
    ),
)

amg_settings.add_argument(
    "--crop-nms-thresh",
    type=float,
    default=None,
    help="The overlap threshold for excluding duplicate masks across different crops.",
)

amg_settings.add_argument(
    "--crop-overlap-ratio",
    type=int,
    default=None,
    help="Larger numbers mean image crops will overlap more.",
)

amg_settings.add_argument(
    "--crop-n-points-downscale-factor",
    type=int,
    default=None,
    help="The number of points-per-side in each layer of crop is reduced by this factor.",
)

amg_settings.add_argument(
    "--min-mask-region-area",
    type=int,
    default=None,
    help=(
        "Disconnected mask regions or holes with area smaller than this value "
        "in pixels are removed by postprocessing."
    ),
)


def write_masks_to_folder(masks: List[Dict[str, Any]], path: str) -> None:
    header = "id,area,bbox_x0,bbox_y0,bbox_w,bbox_h,point_input_x,point_input_y,predicted_iou,stability_score,crop_box_x0,crop_box_y0,crop_box_w,crop_box_h"  # noqa
    metadata = [header]
    for i, mask_data in enumerate(masks):
        mask = mask_data["segmentation"]
        filename = f"{i}.png"
        cv2.imwrite(os.path.join(path, filename), mask * 255)
        mask_metadata = [
            str(i),
            str(mask_data["area"]),
            *[str(x) for x in mask_data["bbox"]],
            *[str(x) for x in mask_data["point_coords"][0]],
            str(mask_data["predicted_iou"]),
            str(mask_data["stability_score"]),
            *[str(x) for x in mask_data["crop_box"]],
        ]
        row = ",".join(mask_metadata)
        metadata.append(row)
    metadata_path = os.path.join(path, "metadata.csv")
    with open(metadata_path, "w") as f:
        f.write("\n".join(metadata))

    return


def get_amg_kwargs(args):
    amg_kwargs = {
        "points_per_side": args.points_per_side,
        "points_per_batch": args.points_per_batch,
        "pred_iou_thresh": args.pred_iou_thresh,
        "stability_score_thresh": args.stability_score_thresh,
        "stability_score_offset": args.stability_score_offset,
        "box_nms_thresh": args.box_nms_thresh,
        "crop_n_layers": args.crop_n_layers,
        "crop_nms_thresh": args.crop_nms_thresh,
        "crop_overlap_ratio": args.crop_overlap_ratio,
        "crop_n_points_downscale_factor": args.crop_n_points_downscale_factor,
        "min_mask_region_area": args.min_mask_region_area,
    }
    amg_kwargs = {k: v for k, v in amg_kwargs.items() if v is not None}
    return amg_kwargs


def main(args: argparse.Namespace) -> None:
    print("Loading model...")
    model = sam.load(args.model)
    output_mode = "coco_rle" if args.convert_to_rle else "binary_mask"
    amg_kwargs = get_amg_kwargs(args)
    generator = SamAutomaticMaskGenerator(model, output_mode=output_mode, **amg_kwargs)

    if not os.path.isdir(args.input):
        targets = [args.input]
    else:
        targets = [
            f
            for f in os.listdir(args.input)
            if not os.path.isdir(os.path.join(args.input, f))
        ]
        targets = [os.path.join(args.input, f) for f in targets]

    os.makedirs(args.output, exist_ok=True)

    for t in targets:
        print(f"Processing '{t}'...")
        image = cv2.imread(t)
        if image is None:
            print(f"Could not load '{t}' as an image, skipping...")
            continue
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        masks = generator.generate(image)

        base = os.path.basename(t)
        base = os.path.splitext(base)[0]
        save_base = os.path.join(args.output, base)
        if output_mode == "binary_mask":
            os.makedirs(save_base, exist_ok=False)
            write_masks_to_folder(masks, save_base)
        else:
            save_file = save_base + ".json"
            with open(save_file, "w") as f:
                json.dump(masks, f)
    print("Done!")


if __name__ == "__main__":
    args = parser.parse_args()
    main(args)

>>>> flux/flux/tokenizers.py
# Copyright © 2024 Apple Inc.

import mlx.core as mx
import regex
from sentencepiece import SentencePieceProcessor


class CLIPTokenizer:
    """A simple port of CLIPTokenizer from https://github.com/huggingface/transformers/ ."""

    def __init__(self, bpe_ranks, vocab, max_length=77):
        self.max_length = max_length
        self.bpe_ranks = bpe_ranks
        self.vocab = vocab
        self.pat = regex.compile(
            r"""<\|startoftext\|>|<\|endoftext\|>|'s|'t|'re|'ve|'m|'ll|'d|[\p{L}]+|[\p{N}]|[^\s\p{L}\p{N}]+""",
            regex.IGNORECASE,
        )

        self._cache = {self.bos: self.bos, self.eos: self.eos}

    @property
    def bos(self):
        return "<|startoftext|>"

    @property
    def bos_token(self):
        return self.vocab[self.bos]

    @property
    def eos(self):
        return "<|endoftext|>"

    @property
    def eos_token(self):
        return self.vocab[self.eos]

    def bpe(self, text):
        if text in self._cache:
            return self._cache[text]

        unigrams = list(text[:-1]) + [text[-1] + "</w>"]
        unique_bigrams = set(zip(unigrams, unigrams[1:]))

        if not unique_bigrams:
            return unigrams

        # In every iteration try to merge the two most likely bigrams. If none
        # was merged we are done.
        #
        # Ported from https://github.com/huggingface/transformers/blob/main/src/transformers/models/clip/tokenization_clip.py
        while unique_bigrams:
            bigram = min(
                unique_bigrams, key=lambda pair: self.bpe_ranks.get(pair, float("inf"))
            )
            if bigram not in self.bpe_ranks:
                break

            new_unigrams = []
            skip = False
            for a, b in zip(unigrams, unigrams[1:]):
                if skip:
                    skip = False
                    continue

                if (a, b) == bigram:
                    new_unigrams.append(a + b)
                    skip = True

                else:
                    new_unigrams.append(a)

            if not skip:
                new_unigrams.append(b)

            unigrams = new_unigrams
            unique_bigrams = set(zip(unigrams, unigrams[1:]))

        self._cache[text] = unigrams

        return unigrams

    def tokenize(self, text, prepend_bos=True, append_eos=True):
        if isinstance(text, list):
            return [self.tokenize(t, prepend_bos, append_eos) for t in text]

        # Lower case cleanup and split according to self.pat. Hugging Face does
        # a much more thorough job here but this should suffice for 95% of
        # cases.
        clean_text = regex.sub(r"\s+", " ", text.lower())
        tokens = regex.findall(self.pat, clean_text)

        # Split the tokens according to the byte-pair merge file
        bpe_tokens = [ti for t in tokens for ti in self.bpe(t)]

        # Map to token ids and return
        tokens = [self.vocab[t] for t in bpe_tokens]
        if prepend_bos:
            tokens = [self.bos_token] + tokens
        if append_eos:
            tokens.append(self.eos_token)

        if len(tokens) > self.max_length:
            tokens = tokens[: self.max_length]
            if append_eos:
                tokens[-1] = self.eos_token

        return tokens

    def encode(self, text):
        if not isinstance(text, list):
            return self.encode([text])

        tokens = self.tokenize(text)
        length = max(len(t) for t in tokens)
        for t in tokens:
            t.extend([self.eos_token] * (length - len(t)))

        return mx.array(tokens)


class T5Tokenizer:
    def __init__(self, model_file, max_length=512):
        self._tokenizer = SentencePieceProcessor(model_file)
        self.max_length = max_length

    @property
    def pad(self):
        try:
            return self._tokenizer.id_to_piece(self.pad_token)
        except IndexError:
            return None

    @property
    def pad_token(self):
        return self._tokenizer.pad_id()

    @property
    def bos(self):
        try:
            return self._tokenizer.id_to_piece(self.bos_token)
        except IndexError:
            return None

    @property
    def bos_token(self):
        return self._tokenizer.bos_id()

    @property
    def eos(self):
        try:
            return self._tokenizer.id_to_piece(self.eos_token)
        except IndexError:
            return None

    @property
    def eos_token(self):
        return self._tokenizer.eos_id()

    def tokenize(self, text, prepend_bos=True, append_eos=True, pad=True):
        if isinstance(text, list):
            return [self.tokenize(t, prepend_bos, append_eos, pad) for t in text]

        tokens = self._tokenizer.encode(text)

        if prepend_bos and self.bos_token >= 0:
            tokens = [self.bos_token] + tokens
        if append_eos and self.eos_token >= 0:
            tokens.append(self.eos_token)
        if pad and len(tokens) < self.max_length and self.pad_token >= 0:
            tokens += [self.pad_token] * (self.max_length - len(tokens))

        return tokens

    def encode(self, text, pad=True):
        if not isinstance(text, list):
            return self.encode([text], pad=pad)

        pad_token = self.pad_token if self.pad_token >= 0 else 0
        tokens = self.tokenize(text, pad=pad)
        length = max(len(t) for t in tokens)
        for t in tokens:
            t.extend([pad_token] * (length - len(t)))

        return mx.array(tokens)

>>>> flux/flux/flux.py
# Copyright © 2024 Apple Inc.

from typing import Tuple

import mlx.core as mx
import mlx.nn as nn
from mlx.utils import tree_unflatten
from tqdm import tqdm

from .lora import LoRALinear
from .sampler import FluxSampler
from .utils import (
    load_ae,
    load_clip,
    load_clip_tokenizer,
    load_flow_model,
    load_t5,
    load_t5_tokenizer,
)


class FluxPipeline:
    def __init__(self, name: str, t5_padding: bool = True):
        self.dtype = mx.bfloat16
        self.name = name
        self.t5_padding = t5_padding

        self.ae = load_ae(name)
        self.flow = load_flow_model(name)
        self.clip = load_clip(name)
        self.clip_tokenizer = load_clip_tokenizer(name)
        self.t5 = load_t5(name)
        self.t5_tokenizer = load_t5_tokenizer(name)
        self.sampler = FluxSampler(name)

    def ensure_models_are_loaded(self):
        mx.eval(
            self.ae.parameters(),
            self.flow.parameters(),
            self.clip.parameters(),
            self.t5.parameters(),
        )

    def reload_text_encoders(self):
        self.t5 = load_t5(self.name)
        self.clip = load_clip(self.name)

    def tokenize(self, text):
        t5_tokens = self.t5_tokenizer.encode(text, pad=self.t5_padding)
        clip_tokens = self.clip_tokenizer.encode(text)
        return t5_tokens, clip_tokens

    def _prepare_latent_images(self, x):
        b, h, w, c = x.shape

        # Pack the latent image to 2x2 patches
        x = x.reshape(b, h // 2, 2, w // 2, 2, c)
        x = x.transpose(0, 1, 3, 5, 2, 4).reshape(b, h * w // 4, c * 4)

        # Create positions ids used to positionally encode each patch. Due to
        # the way RoPE works, this results in an interesting positional
        # encoding where parts of the feature are holding different positional
        # information. Namely, the first part holds information independent of
        # the spatial position (hence 0s), the 2nd part holds vertical spatial
        # information and the last one horizontal.
        i = mx.zeros((h // 2, w // 2), dtype=mx.int32)
        j, k = mx.meshgrid(mx.arange(h // 2), mx.arange(w // 2), indexing="ij")
        x_ids = mx.stack([i, j, k], axis=-1)
        x_ids = mx.repeat(x_ids.reshape(1, h * w // 4, 3), b, 0)

        return x, x_ids

    def _prepare_conditioning(self, n_images, t5_tokens, clip_tokens):
        # Prepare the text features
        txt = self.t5(t5_tokens)
        if len(txt) == 1 and n_images > 1:
            txt = mx.broadcast_to(txt, (n_images, *txt.shape[1:]))
        txt_ids = mx.zeros((n_images, txt.shape[1], 3), dtype=mx.int32)

        # Prepare the clip text features
        vec = self.clip(clip_tokens).pooled_output
        if len(vec) == 1 and n_images > 1:
            vec = mx.broadcast_to(vec, (n_images, *vec.shape[1:]))

        return txt, txt_ids, vec

    def _denoising_loop(
        self,
        x_t,
        x_ids,
        txt,
        txt_ids,
        vec,
        num_steps: int = 35,
        guidance: float = 4.0,
        start: float = 1,
        stop: float = 0,
    ):
        B = len(x_t)

        def scalar(x):
            return mx.full((B,), x, dtype=self.dtype)

        guidance = scalar(guidance)
        timesteps = self.sampler.timesteps(
            num_steps,
            x_t.shape[1],
            start=start,
            stop=stop,
        )
        for i in range(num_steps):
            t = timesteps[i]
            t_prev = timesteps[i + 1]

            pred = self.flow(
                img=x_t,
                img_ids=x_ids,
                txt=txt,
                txt_ids=txt_ids,
                y=vec,
                timesteps=scalar(t),
                guidance=guidance,
            )
            x_t = self.sampler.step(pred, x_t, t, t_prev)

            yield x_t

    def generate_latents(
        self,
        text: str,
        n_images: int = 1,
        num_steps: int = 35,
        guidance: float = 4.0,
        latent_size: Tuple[int, int] = (64, 64),
        seed=None,
    ):
        # Set the PRNG state
        if seed is not None:
            mx.random.seed(seed)

        # Create the latent variables
        x_T = self.sampler.sample_prior((n_images, *latent_size, 16), dtype=self.dtype)
        x_T, x_ids = self._prepare_latent_images(x_T)

        # Get the conditioning
        t5_tokens, clip_tokens = self.tokenize(text)
        txt, txt_ids, vec = self._prepare_conditioning(n_images, t5_tokens, clip_tokens)

        # Yield the conditioning for controlled evaluation by the caller
        yield (x_T, x_ids, txt, txt_ids, vec)

        # Yield the latent sequences from the denoising loop
        yield from self._denoising_loop(
            x_T, x_ids, txt, txt_ids, vec, num_steps=num_steps, guidance=guidance
        )

    def decode(self, x, latent_size: Tuple[int, int] = (64, 64)):
        h, w = latent_size
        x = x.reshape(len(x), h // 2, w // 2, -1, 2, 2)
        x = x.transpose(0, 1, 4, 2, 5, 3).reshape(len(x), h, w, -1)
        x = self.ae.decode(x)
        return mx.clip(x + 1, 0, 2) * 0.5

    def generate_images(
        self,
        text: str,
        n_images: int = 1,
        num_steps: int = 35,
        guidance: float = 4.0,
        latent_size: Tuple[int, int] = (64, 64),
        seed=None,
        reload_text_encoders: bool = True,
        progress: bool = True,
    ):
        latents = self.generate_latents(
            text, n_images, num_steps, guidance, latent_size, seed
        )
        mx.eval(next(latents))

        if reload_text_encoders:
            self.reload_text_encoders()

        for x_t in tqdm(latents, total=num_steps, disable=not progress, leave=True):
            mx.eval(x_t)

        images = []
        for i in tqdm(range(len(x_t)), disable=not progress, desc="generate images"):
            images.append(self.decode(x_t[i : i + 1]))
            mx.eval(images[-1])
        images = mx.concatenate(images, axis=0)
        mx.eval(images)

        return images

    def training_loss(
        self,
        x_0: mx.array,
        t5_features: mx.array,
        clip_features: mx.array,
        guidance: mx.array,
    ):
        # Get the text conditioning
        txt = t5_features
        txt_ids = mx.zeros(txt.shape[:-1] + (3,), dtype=mx.int32)
        vec = clip_features

        # Prepare the latent input
        x_0, x_ids = self._prepare_latent_images(x_0)

        # Forward process
        t = self.sampler.random_timesteps(*x_0.shape[:2], dtype=self.dtype)
        eps = mx.random.normal(x_0.shape, dtype=self.dtype)
        x_t = self.sampler.add_noise(x_0, t, noise=eps)
        x_t = mx.stop_gradient(x_t)

        # Do the denoising
        pred = self.flow(
            img=x_t,
            img_ids=x_ids,
            txt=txt,
            txt_ids=txt_ids,
            y=vec,
            timesteps=t,
            guidance=guidance,
        )

        return (pred + x_0 - eps).square().mean()

    def linear_to_lora_layers(self, rank: int = 8, num_blocks: int = -1):
        """Swap the linear layers in the transformer blocks with LoRA layers."""
        all_blocks = self.flow.double_blocks + self.flow.single_blocks
        all_blocks.reverse()
        num_blocks = num_blocks if num_blocks > 0 else len(all_blocks)
        for i, block in zip(range(num_blocks), all_blocks):
            loras = []
            for name, module in block.named_modules():
                if isinstance(module, nn.Linear):
                    loras.append((name, LoRALinear.from_base(module, r=rank)))
            block.update_modules(tree_unflatten(loras))

    def fuse_lora_layers(self):
        fused_layers = []
        for name, module in self.flow.named_modules():
            if isinstance(module, LoRALinear):
                fused_layers.append((name, module.fuse()))
        self.flow.update_modules(tree_unflatten(fused_layers))

>>>> flux/flux/sampler.py
# Copyright © 2024 Apple Inc.

import math
from functools import lru_cache

import mlx.core as mx


class FluxSampler:
    def __init__(self, name: str, base_shift: float = 0.5, max_shift: float = 1.15):
        self._base_shift = base_shift
        self._max_shift = max_shift
        self._schnell = "schnell" in name

    def _time_shift(self, x, t):
        x1, x2 = 256, 4096
        t1, t2 = self._base_shift, self._max_shift
        exp_mu = math.exp((x - x1) * (t2 - t1) / (x2 - x1) + t1)
        t = exp_mu / (exp_mu + (1 / t - 1))
        return t

    @lru_cache
    def timesteps(
        self, num_steps, image_sequence_length, start: float = 1, stop: float = 0
    ):
        t = mx.linspace(start, stop, num_steps + 1)

        if not self._schnell:
            t = self._time_shift(image_sequence_length, t)

        return t.tolist()

    def random_timesteps(self, B, L, dtype=mx.float32, key=None):
        if self._schnell:
            # TODO: Should we upweigh 1 and 0.75?
            t = mx.random.randint(1, 5, shape=(B,), key=key)
            t = t.astype(dtype) / 4
        else:
            t = mx.random.uniform(shape=(B,), dtype=dtype, key=key)
            t = self._time_shift(L, t)

        return t

    def sample_prior(self, shape, dtype=mx.float32, key=None):
        return mx.random.normal(shape, dtype=dtype, key=key)

    def add_noise(self, x, t, noise=None, key=None):
        noise = (
            noise
            if noise is not None
            else mx.random.normal(x.shape, dtype=x.dtype, key=key)
        )
        t = t.reshape([-1] + [1] * (x.ndim - 1))
        return x * (1 - t) + t * noise

    def step(self, pred, x_t, t, t_prev):
        return x_t + (t_prev - t) * pred

>>>> flux/flux/layers.py
# Copyright © 2024 Apple Inc.

import math
from dataclasses import dataclass
from functools import partial
from typing import List, Optional, Tuple

import mlx.core as mx
import mlx.nn as nn


def _rope(pos: mx.array, dim: int, theta: float):
    scale = mx.arange(0, dim, 2, dtype=mx.float32) / dim
    omega = 1.0 / (theta**scale)
    x = pos[..., None] * omega
    cosx = mx.cos(x)
    sinx = mx.sin(x)
    pe = mx.stack([cosx, -sinx, sinx, cosx], axis=-1)
    pe = pe.reshape(*pe.shape[:-1], 2, 2)

    return pe


@partial(mx.compile, shapeless=True)
def _ab_plus_cd(a, b, c, d):
    return a * b + c * d


def _apply_rope(x, pe):
    s = x.shape
    x = x.reshape(*s[:-1], -1, 1, 2)
    x = _ab_plus_cd(x[..., 0], pe[..., 0], x[..., 1], pe[..., 1])
    return x.reshape(s)


def _attention(q: mx.array, k: mx.array, v: mx.array, pe: mx.array):
    B, H, L, D = q.shape

    q = _apply_rope(q, pe)
    k = _apply_rope(k, pe)
    x = mx.fast.scaled_dot_product_attention(q, k, v, scale=D ** (-0.5))

    return x.transpose(0, 2, 1, 3).reshape(B, L, -1)


def timestep_embedding(
    t: mx.array, dim: int, max_period: int = 10000, time_factor: float = 1000.0
):
    half = dim // 2
    freqs = mx.arange(0, half, dtype=mx.float32) / half
    freqs = freqs * (-math.log(max_period))
    freqs = mx.exp(freqs)

    x = (time_factor * t)[:, None] * freqs[None]
    x = mx.concatenate([mx.cos(x), mx.sin(x)], axis=-1)

    return x.astype(t.dtype)


class EmbedND(nn.Module):
    def __init__(self, dim: int, theta: int, axes_dim: List[int]):
        super().__init__()

        self.dim = dim
        self.theta = theta
        self.axes_dim = axes_dim

    def __call__(self, ids: mx.array):
        n_axes = ids.shape[-1]
        pe = mx.concatenate(
            [_rope(ids[..., i], self.axes_dim[i], self.theta) for i in range(n_axes)],
            axis=-3,
        )

        return pe[:, None]


class MLPEmbedder(nn.Module):
    def __init__(self, in_dim: int, hidden_dim: int):
        super().__init__()
        self.in_layer = nn.Linear(in_dim, hidden_dim, bias=True)
        self.out_layer = nn.Linear(hidden_dim, hidden_dim, bias=True)

    def __call__(self, x: mx.array) -> mx.array:
        return self.out_layer(nn.silu(self.in_layer(x)))


class QKNorm(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.query_norm = nn.RMSNorm(dim)
        self.key_norm = nn.RMSNorm(dim)

    def __call__(self, q: mx.array, k: mx.array) -> tuple[mx.array, mx.array]:
        return self.query_norm(q), self.key_norm(k)


class SelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int = 8, qkv_bias: bool = False):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.norm = QKNorm(head_dim)
        self.proj = nn.Linear(dim, dim)

    def __call__(self, x: mx.array, pe: mx.array) -> mx.array:
        H = self.num_heads
        B, L, _ = x.shape
        qkv = self.qkv(x)
        q, k, v = mx.split(qkv, 3, axis=-1)
        q = q.reshape(B, L, H, -1).transpose(0, 2, 1, 3)
        k = k.reshape(B, L, H, -1).transpose(0, 2, 1, 3)
        v = v.reshape(B, L, H, -1).transpose(0, 2, 1, 3)
        q, k = self.norm(q, k)
        x = _attention(q, k, v, pe)
        x = self.proj(x)
        return x


@dataclass
class ModulationOut:
    shift: mx.array
    scale: mx.array
    gate: mx.array


class Modulation(nn.Module):
    def __init__(self, dim: int, double: bool):
        super().__init__()
        self.is_double = double
        self.multiplier = 6 if double else 3
        self.lin = nn.Linear(dim, self.multiplier * dim, bias=True)

    def __call__(self, x: mx.array) -> Tuple[ModulationOut, Optional[ModulationOut]]:
        x = self.lin(nn.silu(x))
        xs = mx.split(x[:, None, :], self.multiplier, axis=-1)

        mod1 = ModulationOut(*xs[:3])
        mod2 = ModulationOut(*xs[3:]) if self.is_double else None

        return mod1, mod2


class DoubleStreamBlock(nn.Module):
    def __init__(
        self, hidden_size: int, num_heads: int, mlp_ratio: float, qkv_bias: bool = False
    ):
        super().__init__()

        mlp_hidden_dim = int(hidden_size * mlp_ratio)
        self.num_heads = num_heads
        self.hidden_size = hidden_size
        self.img_mod = Modulation(hidden_size, double=True)
        self.img_norm1 = nn.LayerNorm(hidden_size, affine=False, eps=1e-6)
        self.img_attn = SelfAttention(
            dim=hidden_size, num_heads=num_heads, qkv_bias=qkv_bias
        )

        self.img_norm2 = nn.LayerNorm(hidden_size, affine=False, eps=1e-6)
        self.img_mlp = nn.Sequential(
            nn.Linear(hidden_size, mlp_hidden_dim, bias=True),
            nn.GELU(approx="tanh"),
            nn.Linear(mlp_hidden_dim, hidden_size, bias=True),
        )

        self.txt_mod = Modulation(hidden_size, double=True)
        self.txt_norm1 = nn.LayerNorm(hidden_size, affine=False, eps=1e-6)
        self.txt_attn = SelfAttention(
            dim=hidden_size, num_heads=num_heads, qkv_bias=qkv_bias
        )

        self.txt_norm2 = nn.LayerNorm(hidden_size, affine=False, eps=1e-6)
        self.txt_mlp = nn.Sequential(
            nn.Linear(hidden_size, mlp_hidden_dim, bias=True),
            nn.GELU(approx="tanh"),
            nn.Linear(mlp_hidden_dim, hidden_size, bias=True),
        )

    def __call__(
        self, img: mx.array, txt: mx.array, vec: mx.array, pe: mx.array
    ) -> Tuple[mx.array, mx.array]:
        B, L, _ = img.shape
        _, S, _ = txt.shape
        H = self.num_heads

        img_mod1, img_mod2 = self.img_mod(vec)
        txt_mod1, txt_mod2 = self.txt_mod(vec)

        # prepare image for attention
        img_modulated = self.img_norm1(img)
        img_modulated = (1 + img_mod1.scale) * img_modulated + img_mod1.shift
        img_qkv = self.img_attn.qkv(img_modulated)
        img_q, img_k, img_v = mx.split(img_qkv, 3, axis=-1)
        img_q = img_q.reshape(B, L, H, -1).transpose(0, 2, 1, 3)
        img_k = img_k.reshape(B, L, H, -1).transpose(0, 2, 1, 3)
        img_v = img_v.reshape(B, L, H, -1).transpose(0, 2, 1, 3)
        img_q, img_k = self.img_attn.norm(img_q, img_k)

        # prepare txt for attention
        txt_modulated = self.txt_norm1(txt)
        txt_modulated = (1 + txt_mod1.scale) * txt_modulated + txt_mod1.shift
        txt_qkv = self.txt_attn.qkv(txt_modulated)
        txt_q, txt_k, txt_v = mx.split(txt_qkv, 3, axis=-1)
        txt_q = txt_q.reshape(B, S, H, -1).transpose(0, 2, 1, 3)
        txt_k = txt_k.reshape(B, S, H, -1).transpose(0, 2, 1, 3)
        txt_v = txt_v.reshape(B, S, H, -1).transpose(0, 2, 1, 3)
        txt_q, txt_k = self.txt_attn.norm(txt_q, txt_k)

        # run actual attention
        q = mx.concatenate([txt_q, img_q], axis=2)
        k = mx.concatenate([txt_k, img_k], axis=2)
        v = mx.concatenate([txt_v, img_v], axis=2)

        attn = _attention(q, k, v, pe)
        txt_attn, img_attn = mx.split(attn, [S], axis=1)

        # calculate the img bloks
        img = img + img_mod1.gate * self.img_attn.proj(img_attn)
        img = img + img_mod2.gate * self.img_mlp(
            (1 + img_mod2.scale) * self.img_norm2(img) + img_mod2.shift
        )

        # calculate the txt bloks
        txt = txt + txt_mod1.gate * self.txt_attn.proj(txt_attn)
        txt = txt + txt_mod2.gate * self.txt_mlp(
            (1 + txt_mod2.scale) * self.txt_norm2(txt) + txt_mod2.shift
        )

        return img, txt


class SingleStreamBlock(nn.Module):
    def __init__(
        self,
        hidden_size: int,
        num_heads: int,
        mlp_ratio: float = 4.0,
        qk_scale: Optional[float] = None,
    ):
        super().__init__()
        self.hidden_dim = hidden_size
        self.num_heads = num_heads
        head_dim = hidden_size // num_heads
        self.scale = qk_scale or head_dim**-0.5

        self.mlp_hidden_dim = int(hidden_size * mlp_ratio)
        # qkv and mlp_in
        self.linear1 = nn.Linear(hidden_size, hidden_size * 3 + self.mlp_hidden_dim)
        # proj and mlp_out
        self.linear2 = nn.Linear(hidden_size + self.mlp_hidden_dim, hidden_size)

        self.norm = QKNorm(head_dim)

        self.hidden_size = hidden_size
        self.pre_norm = nn.LayerNorm(hidden_size, affine=False, eps=1e-6)

        self.mlp_act = nn.GELU(approx="tanh")
        self.modulation = Modulation(hidden_size, double=False)

    def __call__(self, x: mx.array, vec: mx.array, pe: mx.array):
        B, L, _ = x.shape
        H = self.num_heads

        mod, _ = self.modulation(vec)
        x_mod = (1 + mod.scale) * self.pre_norm(x) + mod.shift

        q, k, v, mlp = mx.split(
            self.linear1(x_mod),
            [self.hidden_size, 2 * self.hidden_size, 3 * self.hidden_size],
            axis=-1,
        )
        q = q.reshape(B, L, H, -1).transpose(0, 2, 1, 3)
        k = k.reshape(B, L, H, -1).transpose(0, 2, 1, 3)
        v = v.reshape(B, L, H, -1).transpose(0, 2, 1, 3)
        q, k = self.norm(q, k)

        # compute attention
        y = _attention(q, k, v, pe)

        # compute activation in mlp stream, cat again and run second linear layer
        y = self.linear2(mx.concatenate([y, self.mlp_act(mlp)], axis=2))
        return x + mod.gate * y


class LastLayer(nn.Module):
    def __init__(self, hidden_size: int, patch_size: int, out_channels: int):
        super().__init__()
        self.norm_final = nn.LayerNorm(hidden_size, affine=False, eps=1e-6)
        self.linear = nn.Linear(
            hidden_size, patch_size * patch_size * out_channels, bias=True
        )
        self.adaLN_modulation = nn.Sequential(
            nn.SiLU(), nn.Linear(hidden_size, 2 * hidden_size, bias=True)
        )

    def __call__(self, x: mx.array, vec: mx.array):
        shift, scale = mx.split(self.adaLN_modulation(vec), 2, axis=1)
        x = (1 + scale[:, None, :]) * self.norm_final(x) + shift[:, None, :]
        x = self.linear(x)
        return x

>>>> flux/flux/clip.py
# Copyright © 2024 Apple Inc.

from dataclasses import dataclass
from typing import List, Optional

import mlx.core as mx
import mlx.nn as nn

_ACTIVATIONS = {"quick_gelu": nn.gelu_fast_approx, "gelu": nn.gelu}


@dataclass
class CLIPTextModelConfig:
    num_layers: int = 23
    model_dims: int = 1024
    num_heads: int = 16
    max_length: int = 77
    vocab_size: int = 49408
    hidden_act: str = "quick_gelu"

    @classmethod
    def from_dict(cls, config):
        return cls(
            num_layers=config["num_hidden_layers"],
            model_dims=config["hidden_size"],
            num_heads=config["num_attention_heads"],
            max_length=config["max_position_embeddings"],
            vocab_size=config["vocab_size"],
            hidden_act=config["hidden_act"],
        )


@dataclass
class CLIPOutput:
    # The last_hidden_state indexed at the EOS token and possibly projected if
    # the model has a projection layer
    pooled_output: Optional[mx.array] = None

    # The full sequence output of the transformer after the final layernorm
    last_hidden_state: Optional[mx.array] = None

    # A list of hidden states corresponding to the outputs of the transformer layers
    hidden_states: Optional[List[mx.array]] = None


class CLIPEncoderLayer(nn.Module):
    """The transformer encoder layer from CLIP."""

    def __init__(self, model_dims: int, num_heads: int, activation: str):
        super().__init__()

        self.layer_norm1 = nn.LayerNorm(model_dims)
        self.layer_norm2 = nn.LayerNorm(model_dims)

        self.attention = nn.MultiHeadAttention(model_dims, num_heads, bias=True)

        self.linear1 = nn.Linear(model_dims, 4 * model_dims)
        self.linear2 = nn.Linear(4 * model_dims, model_dims)

        self.act = _ACTIVATIONS[activation]

    def __call__(self, x, attn_mask=None):
        y = self.layer_norm1(x)
        y = self.attention(y, y, y, attn_mask)
        x = y + x

        y = self.layer_norm2(x)
        y = self.linear1(y)
        y = self.act(y)
        y = self.linear2(y)
        x = y + x

        return x


class CLIPTextModel(nn.Module):
    """Implements the text encoder transformer from CLIP."""

    def __init__(self, config: CLIPTextModelConfig):
        super().__init__()

        self.token_embedding = nn.Embedding(config.vocab_size, config.model_dims)
        self.position_embedding = nn.Embedding(config.max_length, config.model_dims)
        self.layers = [
            CLIPEncoderLayer(config.model_dims, config.num_heads, config.hidden_act)
            for i in range(config.num_layers)
        ]
        self.final_layer_norm = nn.LayerNorm(config.model_dims)

    def _get_mask(self, N, dtype):
        indices = mx.arange(N)
        mask = indices[:, None] < indices[None]
        mask = mask.astype(dtype) * (-6e4 if dtype == mx.float16 else -1e9)
        return mask

    def sanitize(self, weights):
        new_weights = {}
        for key, w in weights.items():
            # Remove prefixes
            if key.startswith("text_model."):
                key = key[11:]
            if key.startswith("embeddings."):
                key = key[11:]
            if key.startswith("encoder."):
                key = key[8:]

            # Map attention layers
            if "self_attn." in key:
                key = key.replace("self_attn.", "attention.")
            if "q_proj." in key:
                key = key.replace("q_proj.", "query_proj.")
            if "k_proj." in key:
                key = key.replace("k_proj.", "key_proj.")
            if "v_proj." in key:
                key = key.replace("v_proj.", "value_proj.")

            # Map ffn layers
            if "mlp.fc1" in key:
                key = key.replace("mlp.fc1", "linear1")
            if "mlp.fc2" in key:
                key = key.replace("mlp.fc2", "linear2")

            new_weights[key] = w

        return new_weights

    def __call__(self, x):
        # Extract some shapes
        B, N = x.shape
        eos_tokens = x.argmax(-1)

        # Compute the embeddings
        x = self.token_embedding(x)
        x = x + self.position_embedding.weight[:N]

        # Compute the features from the transformer
        mask = self._get_mask(N, x.dtype)
        hidden_states = []
        for l in self.layers:
            x = l(x, mask)
            hidden_states.append(x)

        # Apply the final layernorm and return
        x = self.final_layer_norm(x)
        last_hidden_state = x

        # Select the EOS token
        pooled_output = x[mx.arange(len(x)), eos_tokens]

        return CLIPOutput(
            pooled_output=pooled_output,
            last_hidden_state=last_hidden_state,
            hidden_states=hidden_states,
        )

>>>> flux/flux/lora.py
# Copyright © 2024 Apple Inc.

import math

import mlx.core as mx
import mlx.nn as nn


class LoRALinear(nn.Module):
    @staticmethod
    def from_base(
        linear: nn.Linear,
        r: int = 8,
        dropout: float = 0.0,
        scale: float = 1.0,
    ):
        output_dims, input_dims = linear.weight.shape
        lora_lin = LoRALinear(
            input_dims=input_dims,
            output_dims=output_dims,
            r=r,
            dropout=dropout,
            scale=scale,
        )
        lora_lin.linear = linear
        return lora_lin

    def fuse(self):
        linear = self.linear
        bias = "bias" in linear
        weight = linear.weight
        dtype = weight.dtype

        output_dims, input_dims = weight.shape
        fused_linear = nn.Linear(input_dims, output_dims, bias=bias)

        lora_b = self.scale * self.lora_b.T
        lora_a = self.lora_a.T
        fused_linear.weight = weight + (lora_b @ lora_a).astype(dtype)
        if bias:
            fused_linear.bias = linear.bias

        return fused_linear

    def __init__(
        self,
        input_dims: int,
        output_dims: int,
        r: int = 8,
        dropout: float = 0.0,
        scale: float = 1.0,
        bias: bool = False,
    ):
        super().__init__()

        # Regular linear layer weights
        self.linear = nn.Linear(input_dims, output_dims, bias=bias)

        self.dropout = nn.Dropout(p=dropout)

        # Scale for low-rank update
        self.scale = scale

        # Low rank lora weights
        scale = 1 / math.sqrt(input_dims)
        self.lora_a = mx.random.uniform(
            low=-scale,
            high=scale,
            shape=(input_dims, r),
        )
        self.lora_b = mx.zeros(shape=(r, output_dims))

    def __call__(self, x):
        y = self.linear(x)
        z = (self.dropout(x) @ self.lora_a) @ self.lora_b
        return y + (self.scale * z).astype(x.dtype)

>>>> flux/flux/trainer.py
import mlx.core as mx
import numpy as np
from PIL import Image, ImageFile
from tqdm import tqdm

from .datasets import Dataset
from .flux import FluxPipeline


class Trainer:

    def __init__(self, flux: FluxPipeline, dataset: Dataset, args):
        self.flux = flux
        self.dataset = dataset
        self.args = args
        self.latents = []
        self.t5_features = []
        self.clip_features = []

    def _random_crop_resize(self, img):
        resolution = self.args.resolution
        width, height = img.size

        a, b, c, d = mx.random.uniform(shape=(4,), stream=mx.cpu).tolist()

        # Random crop the input image between 0.8 to 1.0 of its original dimensions
        crop_size = (
            max((0.8 + 0.2 * a) * width, resolution[0]),
            max((0.8 + 0.2 * b) * height, resolution[1]),
        )
        pan = (width - crop_size[0], height - crop_size[1])
        img = img.crop(
            (
                pan[0] * c,
                pan[1] * d,
                crop_size[0] + pan[0] * c,
                crop_size[1] + pan[1] * d,
            )
        )

        # Fit the largest rectangle with the ratio of resolution in the image
        # rectangle.
        width, height = crop_size
        ratio = resolution[0] / resolution[1]
        r1 = (height * ratio, height)
        r2 = (width, width / ratio)
        r = r1 if r1[0] <= width else r2
        img = img.crop(
            (
                (width - r[0]) / 2,
                (height - r[1]) / 2,
                (width + r[0]) / 2,
                (height + r[1]) / 2,
            )
        )

        # Finally resize the image to resolution
        img = img.resize(resolution, Image.LANCZOS)

        return mx.array(np.array(img))

    def _encode_image(self, input_img: ImageFile.ImageFile, num_augmentations: int):
        for i in range(num_augmentations):
            img = self._random_crop_resize(input_img)
            img = (img[:, :, :3].astype(self.flux.dtype) / 255) * 2 - 1
            x_0 = self.flux.ae.encode(img[None])
            x_0 = x_0.astype(self.flux.dtype)
            mx.eval(x_0)
            self.latents.append(x_0)

    def _encode_prompt(self, prompt):
        t5_tok, clip_tok = self.flux.tokenize([prompt])
        t5_feat = self.flux.t5(t5_tok)
        clip_feat = self.flux.clip(clip_tok).pooled_output
        mx.eval(t5_feat, clip_feat)
        self.t5_features.append(t5_feat)
        self.clip_features.append(clip_feat)

    def encode_dataset(self):
        """Encode the images & prompt in the latent space to prepare for training."""
        self.flux.ae.eval()
        for image, prompt in tqdm(self.dataset, desc="encode dataset"):
            self._encode_image(image, self.args.num_augmentations)
            self._encode_prompt(prompt)

    def iterate(self, batch_size):
        xs = mx.concatenate(self.latents)
        t5 = mx.concatenate(self.t5_features)
        clip = mx.concatenate(self.clip_features)
        mx.eval(xs, t5, clip)
        n_aug = self.args.num_augmentations
        while True:
            x_indices = mx.random.permutation(len(self.latents))
            c_indices = x_indices // n_aug
            for i in range(0, len(self.latents), batch_size):
                x_i = x_indices[i : i + batch_size]
                c_i = c_indices[i : i + batch_size]
                yield xs[x_i], t5[c_i], clip[c_i]

>>>> flux/flux/utils.py
# Copyright © 2024 Apple Inc.

import json
import os
from dataclasses import dataclass
from pathlib import Path
from typing import Optional, Union

import mlx.core as mx
from huggingface_hub import hf_hub_download

from .autoencoder import AutoEncoder, AutoEncoderParams
from .clip import CLIPTextModel, CLIPTextModelConfig
from .model import Flux, FluxParams
from .t5 import T5Config, T5Encoder
from .tokenizers import CLIPTokenizer, T5Tokenizer


@dataclass
class ModelSpec:
    params: FluxParams
    ae_params: AutoEncoderParams
    ckpt_path: Optional[str]
    ae_path: Optional[str]
    repo_id: Optional[str]
    repo_flow: Optional[str]
    repo_ae: Optional[str]


configs = {
    "flux-dev": ModelSpec(
        repo_id="black-forest-labs/FLUX.1-dev",
        repo_flow="flux1-dev.safetensors",
        repo_ae="ae.safetensors",
        ckpt_path=os.getenv("FLUX_DEV"),
        params=FluxParams(
            in_channels=64,
            vec_in_dim=768,
            context_in_dim=4096,
            hidden_size=3072,
            mlp_ratio=4.0,
            num_heads=24,
            depth=19,
            depth_single_blocks=38,
            axes_dim=[16, 56, 56],
            theta=10_000,
            qkv_bias=True,
            guidance_embed=True,
        ),
        ae_path=os.getenv("AE"),
        ae_params=AutoEncoderParams(
            resolution=256,
            in_channels=3,
            ch=128,
            out_ch=3,
            ch_mult=[1, 2, 4, 4],
            num_res_blocks=2,
            z_channels=16,
            scale_factor=0.3611,
            shift_factor=0.1159,
        ),
    ),
    "flux-schnell": ModelSpec(
        repo_id="black-forest-labs/FLUX.1-schnell",
        repo_flow="flux1-schnell.safetensors",
        repo_ae="ae.safetensors",
        ckpt_path=os.getenv("FLUX_SCHNELL"),
        params=FluxParams(
            in_channels=64,
            vec_in_dim=768,
            context_in_dim=4096,
            hidden_size=3072,
            mlp_ratio=4.0,
            num_heads=24,
            depth=19,
            depth_single_blocks=38,
            axes_dim=[16, 56, 56],
            theta=10_000,
            qkv_bias=True,
            guidance_embed=False,
        ),
        ae_path=os.getenv("AE"),
        ae_params=AutoEncoderParams(
            resolution=256,
            in_channels=3,
            ch=128,
            out_ch=3,
            ch_mult=[1, 2, 4, 4],
            num_res_blocks=2,
            z_channels=16,
            scale_factor=0.3611,
            shift_factor=0.1159,
        ),
    ),
}


def load_flow_model(name: str, hf_download: bool = True):
    # Get the safetensors file to load
    ckpt_path = configs[name].ckpt_path

    # Download if needed
    if (
        ckpt_path is None
        and configs[name].repo_id is not None
        and configs[name].repo_flow is not None
        and hf_download
    ):
        ckpt_path = hf_hub_download(configs[name].repo_id, configs[name].repo_flow)

    # Make the model
    model = Flux(configs[name].params)

    # Load the checkpoint if needed
    if ckpt_path is not None:
        weights = mx.load(ckpt_path)
        weights = model.sanitize(weights)
        model.load_weights(list(weights.items()))

    return model


def load_ae(name: str, hf_download: bool = True):
    # Get the safetensors file to load
    ckpt_path = configs[name].ae_path

    # Download if needed
    if (
        ckpt_path is None
        and configs[name].repo_id is not None
        and configs[name].repo_ae is not None
        and hf_download
    ):
        ckpt_path = hf_hub_download(configs[name].repo_id, configs[name].repo_ae)

    # Make the autoencoder
    ae = AutoEncoder(configs[name].ae_params)

    # Load the checkpoint if needed
    if ckpt_path is not None:
        weights = mx.load(ckpt_path)
        weights = ae.sanitize(weights)
        ae.load_weights(list(weights.items()))

    return ae


def load_clip(name: str):
    # Load the config
    config_path = hf_hub_download(configs[name].repo_id, "text_encoder/config.json")
    with open(config_path) as f:
        config = CLIPTextModelConfig.from_dict(json.load(f))

    # Make the clip text encoder
    clip = CLIPTextModel(config)

    # Load the weights
    ckpt_path = hf_hub_download(configs[name].repo_id, "text_encoder/model.safetensors")
    weights = mx.load(ckpt_path)
    weights = clip.sanitize(weights)
    clip.load_weights(list(weights.items()))

    return clip


def load_t5(name: str):
    # Load the config
    config_path = hf_hub_download(configs[name].repo_id, "text_encoder_2/config.json")
    with open(config_path) as f:
        config = T5Config.from_dict(json.load(f))

    # Make the T5 model
    t5 = T5Encoder(config)

    # Load the weights
    model_index = hf_hub_download(
        configs[name].repo_id, "text_encoder_2/model.safetensors.index.json"
    )
    weight_files = set()
    with open(model_index) as f:
        for _, w in json.load(f)["weight_map"].items():
            weight_files.add(w)
    weights = {}
    for w in weight_files:
        w = f"text_encoder_2/{w}"
        w = hf_hub_download(configs[name].repo_id, w)
        weights.update(mx.load(w))
    weights = t5.sanitize(weights)
    t5.load_weights(list(weights.items()))

    return t5


def load_clip_tokenizer(name: str):
    vocab_file = hf_hub_download(configs[name].repo_id, "tokenizer/vocab.json")
    with open(vocab_file, encoding="utf-8") as f:
        vocab = json.load(f)

    merges_file = hf_hub_download(configs[name].repo_id, "tokenizer/merges.txt")
    with open(merges_file, encoding="utf-8") as f:
        bpe_merges = f.read().strip().split("\n")[1 : 49152 - 256 - 2 + 1]
    bpe_merges = [tuple(m.split()) for m in bpe_merges]
    bpe_ranks = dict(map(reversed, enumerate(bpe_merges)))

    return CLIPTokenizer(bpe_ranks, vocab, max_length=77)


def load_t5_tokenizer(name: str, pad: bool = True):
    model_file = hf_hub_download(configs[name].repo_id, "tokenizer_2/spiece.model")
    return T5Tokenizer(model_file, 256 if "schnell" in name else 512)


def save_config(
    config: dict,
    config_path: Union[str, Path],
) -> None:
    """Save the model configuration to the ``config_path``.

    The final configuration will be sorted before saving for better readability.

    Args:
        config (dict): The model configuration.
        config_path (Union[str, Path]): Model configuration file path.
    """
    # Sort the config for better readability
    config = dict(sorted(config.items()))

    # Write the config to the provided file
    with open(config_path, "w") as fid:
        json.dump(config, fid, indent=4)

>>>> flux/flux/autoencoder.py
# Copyright © 2024 Apple Inc.

from dataclasses import dataclass
from typing import List

import mlx.core as mx
import mlx.nn as nn
from mlx.nn.layers.upsample import upsample_nearest


@dataclass
class AutoEncoderParams:
    resolution: int
    in_channels: int
    ch: int
    out_ch: int
    ch_mult: List[int]
    num_res_blocks: int
    z_channels: int
    scale_factor: float
    shift_factor: float


class AttnBlock(nn.Module):
    def __init__(self, in_channels: int):
        super().__init__()
        self.in_channels = in_channels

        self.norm = nn.GroupNorm(
            num_groups=32,
            dims=in_channels,
            eps=1e-6,
            affine=True,
            pytorch_compatible=True,
        )
        self.q = nn.Linear(in_channels, in_channels)
        self.k = nn.Linear(in_channels, in_channels)
        self.v = nn.Linear(in_channels, in_channels)
        self.proj_out = nn.Linear(in_channels, in_channels)

    def __call__(self, x: mx.array) -> mx.array:
        B, H, W, C = x.shape

        y = x.reshape(B, 1, -1, C)
        y = self.norm(y)
        q = self.q(y)
        k = self.k(y)
        v = self.v(y)
        y = mx.fast.scaled_dot_product_attention(q, k, v, scale=C ** (-0.5))
        y = self.proj_out(y)

        return x + y.reshape(B, H, W, C)


class ResnetBlock(nn.Module):
    def __init__(self, in_channels: int, out_channels: int):
        super().__init__()
        self.in_channels = in_channels
        out_channels = in_channels if out_channels is None else out_channels
        self.out_channels = out_channels

        self.norm1 = nn.GroupNorm(
            num_groups=32,
            dims=in_channels,
            eps=1e-6,
            affine=True,
            pytorch_compatible=True,
        )
        self.conv1 = nn.Conv2d(
            in_channels, out_channels, kernel_size=3, stride=1, padding=1
        )
        self.norm2 = nn.GroupNorm(
            num_groups=32,
            dims=out_channels,
            eps=1e-6,
            affine=True,
            pytorch_compatible=True,
        )
        self.conv2 = nn.Conv2d(
            out_channels, out_channels, kernel_size=3, stride=1, padding=1
        )
        if self.in_channels != self.out_channels:
            self.nin_shortcut = nn.Linear(in_channels, out_channels)

    def __call__(self, x):
        h = x
        h = self.norm1(h)
        h = nn.silu(h)
        h = self.conv1(h)

        h = self.norm2(h)
        h = nn.silu(h)
        h = self.conv2(h)

        if self.in_channels != self.out_channels:
            x = self.nin_shortcut(x)

        return x + h


class Downsample(nn.Module):
    def __init__(self, in_channels: int):
        super().__init__()
        self.conv = nn.Conv2d(
            in_channels, in_channels, kernel_size=3, stride=2, padding=0
        )

    def __call__(self, x: mx.array):
        x = mx.pad(x, [(0, 0), (0, 1), (0, 1), (0, 0)])
        x = self.conv(x)
        return x


class Upsample(nn.Module):
    def __init__(self, in_channels: int):
        super().__init__()
        self.conv = nn.Conv2d(
            in_channels, in_channels, kernel_size=3, stride=1, padding=1
        )

    def __call__(self, x: mx.array):
        x = upsample_nearest(x, (2, 2))
        x = self.conv(x)
        return x


class Encoder(nn.Module):
    def __init__(
        self,
        resolution: int,
        in_channels: int,
        ch: int,
        ch_mult: list[int],
        num_res_blocks: int,
        z_channels: int,
    ):
        super().__init__()
        self.ch = ch
        self.num_resolutions = len(ch_mult)
        self.num_res_blocks = num_res_blocks
        self.resolution = resolution
        self.in_channels = in_channels
        # downsampling
        self.conv_in = nn.Conv2d(
            in_channels, self.ch, kernel_size=3, stride=1, padding=1
        )

        curr_res = resolution
        in_ch_mult = (1,) + tuple(ch_mult)
        self.in_ch_mult = in_ch_mult
        self.down = []
        block_in = self.ch
        for i_level in range(self.num_resolutions):
            block = []
            attn = []  # TODO: Remove the attn, nobody appends anything to it
            block_in = ch * in_ch_mult[i_level]
            block_out = ch * ch_mult[i_level]
            for _ in range(self.num_res_blocks):
                block.append(ResnetBlock(in_channels=block_in, out_channels=block_out))
                block_in = block_out
            down = {}
            down["block"] = block
            down["attn"] = attn
            if i_level != self.num_resolutions - 1:
                down["downsample"] = Downsample(block_in)
                curr_res = curr_res // 2
            self.down.append(down)

        # middle
        self.mid = {}
        self.mid["block_1"] = ResnetBlock(in_channels=block_in, out_channels=block_in)
        self.mid["attn_1"] = AttnBlock(block_in)
        self.mid["block_2"] = ResnetBlock(in_channels=block_in, out_channels=block_in)

        # end
        self.norm_out = nn.GroupNorm(
            num_groups=32, dims=block_in, eps=1e-6, affine=True, pytorch_compatible=True
        )
        self.conv_out = nn.Conv2d(
            block_in, 2 * z_channels, kernel_size=3, stride=1, padding=1
        )

    def __call__(self, x: mx.array):
        hs = [self.conv_in(x)]
        for i_level in range(self.num_resolutions):
            for i_block in range(self.num_res_blocks):
                h = self.down[i_level]["block"][i_block](hs[-1])

                # TODO: Remove the attn
                if len(self.down[i_level]["attn"]) > 0:
                    h = self.down[i_level]["attn"][i_block](h)

                hs.append(h)

            if i_level != self.num_resolutions - 1:
                hs.append(self.down[i_level]["downsample"](hs[-1]))

        # middle
        h = hs[-1]
        h = self.mid["block_1"](h)
        h = self.mid["attn_1"](h)
        h = self.mid["block_2"](h)

        # end
        h = self.norm_out(h)
        h = nn.silu(h)
        h = self.conv_out(h)

        return h


class Decoder(nn.Module):
    def __init__(
        self,
        ch: int,
        out_ch: int,
        ch_mult: list[int],
        num_res_blocks: int,
        in_channels: int,
        resolution: int,
        z_channels: int,
    ):
        super().__init__()
        self.ch = ch
        self.num_resolutions = len(ch_mult)
        self.num_res_blocks = num_res_blocks
        self.resolution = resolution
        self.in_channels = in_channels
        self.ffactor = 2 ** (self.num_resolutions - 1)

        # compute in_ch_mult, block_in and curr_res at lowest res
        block_in = ch * ch_mult[self.num_resolutions - 1]
        curr_res = resolution // 2 ** (self.num_resolutions - 1)
        self.z_shape = (1, z_channels, curr_res, curr_res)

        # z to block_in
        self.conv_in = nn.Conv2d(
            z_channels, block_in, kernel_size=3, stride=1, padding=1
        )

        # middle
        self.mid = {}
        self.mid["block_1"] = ResnetBlock(in_channels=block_in, out_channels=block_in)
        self.mid["attn_1"] = AttnBlock(block_in)
        self.mid["block_2"] = ResnetBlock(in_channels=block_in, out_channels=block_in)

        # upsampling
        self.up = []
        for i_level in reversed(range(self.num_resolutions)):
            block = []
            attn = []  # TODO: Remove the attn, nobody appends anything to it

            block_out = ch * ch_mult[i_level]
            for _ in range(self.num_res_blocks + 1):
                block.append(ResnetBlock(in_channels=block_in, out_channels=block_out))
                block_in = block_out
            up = {}
            up["block"] = block
            up["attn"] = attn
            if i_level != 0:
                up["upsample"] = Upsample(block_in)
                curr_res = curr_res * 2
            self.up.insert(0, up)  # prepend to get consistent order

        # end
        self.norm_out = nn.GroupNorm(
            num_groups=32, dims=block_in, eps=1e-6, affine=True, pytorch_compatible=True
        )
        self.conv_out = nn.Conv2d(block_in, out_ch, kernel_size=3, stride=1, padding=1)

    def __call__(self, z: mx.array):
        # z to block_in
        h = self.conv_in(z)

        # middle
        h = self.mid["block_1"](h)
        h = self.mid["attn_1"](h)
        h = self.mid["block_2"](h)

        # upsampling
        for i_level in reversed(range(self.num_resolutions)):
            for i_block in range(self.num_res_blocks + 1):
                h = self.up[i_level]["block"][i_block](h)

                # TODO: Remove the attn
                if len(self.up[i_level]["attn"]) > 0:
                    h = self.up[i_level]["attn"][i_block](h)

            if i_level != 0:
                h = self.up[i_level]["upsample"](h)

        # end
        h = self.norm_out(h)
        h = nn.silu(h)
        h = self.conv_out(h)

        return h


class DiagonalGaussian(nn.Module):
    def __call__(self, z: mx.array):
        mean, logvar = mx.split(z, 2, axis=-1)
        if self.training:
            std = mx.exp(0.5 * logvar)
            eps = mx.random.normal(shape=z.shape, dtype=z.dtype)
            return mean + std * eps
        else:
            return mean


class AutoEncoder(nn.Module):
    def __init__(self, params: AutoEncoderParams):
        super().__init__()
        self.encoder = Encoder(
            resolution=params.resolution,
            in_channels=params.in_channels,
            ch=params.ch,
            ch_mult=params.ch_mult,
            num_res_blocks=params.num_res_blocks,
            z_channels=params.z_channels,
        )
        self.decoder = Decoder(
            resolution=params.resolution,
            in_channels=params.in_channels,
            ch=params.ch,
            out_ch=params.out_ch,
            ch_mult=params.ch_mult,
            num_res_blocks=params.num_res_blocks,
            z_channels=params.z_channels,
        )
        self.reg = DiagonalGaussian()

        self.scale_factor = params.scale_factor
        self.shift_factor = params.shift_factor

    def sanitize(self, weights):
        new_weights = {}
        for k, w in weights.items():
            if w.ndim == 4:
                w = w.transpose(0, 2, 3, 1)
                w = w.reshape(-1).reshape(w.shape)
                if w.shape[1:3] == (1, 1):
                    w = w.squeeze((1, 2))
            new_weights[k] = w
        return new_weights

    def encode(self, x: mx.array):
        z = self.reg(self.encoder(x))
        z = self.scale_factor * (z - self.shift_factor)
        return z

    def decode(self, z: mx.array):
        z = z / self.scale_factor + self.shift_factor
        return self.decoder(z)

    def __call__(self, x: mx.array):
        return self.decode(self.encode(x))

>>>> flux/flux/model.py
# Copyright © 2024 Apple Inc.

from dataclasses import dataclass
from typing import Optional

import mlx.core as mx
import mlx.nn as nn

from .layers import (
    DoubleStreamBlock,
    EmbedND,
    LastLayer,
    MLPEmbedder,
    SingleStreamBlock,
    timestep_embedding,
)


@dataclass
class FluxParams:
    in_channels: int
    vec_in_dim: int
    context_in_dim: int
    hidden_size: int
    mlp_ratio: float
    num_heads: int
    depth: int
    depth_single_blocks: int
    axes_dim: list[int]
    theta: int
    qkv_bias: bool
    guidance_embed: bool


class Flux(nn.Module):
    def __init__(self, params: FluxParams):
        super().__init__()

        self.params = params
        self.in_channels = params.in_channels
        self.out_channels = self.in_channels
        if params.hidden_size % params.num_heads != 0:
            raise ValueError(
                f"Hidden size {params.hidden_size} must be divisible by num_heads {params.num_heads}"
            )
        pe_dim = params.hidden_size // params.num_heads
        if sum(params.axes_dim) != pe_dim:
            raise ValueError(
                f"Got {params.axes_dim} but expected positional dim {pe_dim}"
            )
        self.hidden_size = params.hidden_size
        self.num_heads = params.num_heads
        self.pe_embedder = EmbedND(
            dim=pe_dim, theta=params.theta, axes_dim=params.axes_dim
        )
        self.img_in = nn.Linear(self.in_channels, self.hidden_size, bias=True)
        self.time_in = MLPEmbedder(in_dim=256, hidden_dim=self.hidden_size)
        self.vector_in = MLPEmbedder(params.vec_in_dim, self.hidden_size)
        self.guidance_in = (
            MLPEmbedder(in_dim=256, hidden_dim=self.hidden_size)
            if params.guidance_embed
            else nn.Identity()
        )
        self.txt_in = nn.Linear(params.context_in_dim, self.hidden_size)

        self.double_blocks = [
            DoubleStreamBlock(
                self.hidden_size,
                self.num_heads,
                mlp_ratio=params.mlp_ratio,
                qkv_bias=params.qkv_bias,
            )
            for _ in range(params.depth)
        ]

        self.single_blocks = [
            SingleStreamBlock(
                self.hidden_size, self.num_heads, mlp_ratio=params.mlp_ratio
            )
            for _ in range(params.depth_single_blocks)
        ]

        self.final_layer = LastLayer(self.hidden_size, 1, self.out_channels)

    def sanitize(self, weights):
        new_weights = {}
        for k, w in weights.items():
            if k.startswith("model.diffusion_model."):
                k = k[22:]
            if k.endswith(".scale"):
                k = k[:-6] + ".weight"
            for seq in ["img_mlp", "txt_mlp", "adaLN_modulation"]:
                if f".{seq}." in k:
                    k = k.replace(f".{seq}.", f".{seq}.layers.")
                    break
            new_weights[k] = w
        return new_weights

    def __call__(
        self,
        img: mx.array,
        img_ids: mx.array,
        txt: mx.array,
        txt_ids: mx.array,
        timesteps: mx.array,
        y: mx.array,
        guidance: Optional[mx.array] = None,
    ) -> mx.array:
        if img.ndim != 3 or txt.ndim != 3:
            raise ValueError("Input img and txt tensors must have 3 dimensions.")

        img = self.img_in(img)
        vec = self.time_in(timestep_embedding(timesteps, 256))
        if self.params.guidance_embed:
            if guidance is None:
                raise ValueError(
                    "Didn't get guidance strength for guidance distilled model."
                )
            vec = vec + self.guidance_in(timestep_embedding(guidance, 256))
        vec = vec + self.vector_in(y)
        txt = self.txt_in(txt)

        ids = mx.concatenate([txt_ids, img_ids], axis=1)
        pe = self.pe_embedder(ids).astype(img.dtype)

        for block in self.double_blocks:
            img, txt = block(img=img, txt=txt, vec=vec, pe=pe)

        img = mx.concatenate([txt, img], axis=1)
        for block in self.single_blocks:
            img = block(img, vec=vec, pe=pe)
        img = img[:, txt.shape[1] :, ...]

        img = self.final_layer(img, vec)

        return img

>>>> segment_anything/segment_anything/mask_decoder.py
import math
from typing import List, Tuple, Type, Union

import mlx.core as mx
import mlx.nn as nn

from .common import LayerNorm2d


class MaskDecoder(nn.Module):
    def __init__(
        self,
        *,
        transformer_dim: int,
        transformer: nn.Module,
        num_multimask_outputs: int = 3,
        activation: Type[nn.Module] = nn.GELU,
        iou_head_depth: int = 3,
        iou_head_hidden_dim: int = 256,
    ) -> None:
        """
        Predicts masks given an image and prompt embeddings, using a
        transformer architecture.

        Args:
            transformer_dim (int): the channel dimension of the transformer
            transformer (nn.Module): the transformer used to predict masks
            num_multimask_outputs (int): the number of masks to predict
                when disambiguating masks
            activation (nn.Module): the type of activation to use when
                upscaling masks
            iou_head_depth (int): the depth of the MLP used to predict
                mask quality
            iou_head_hidden_dim (int): the hidden dimension of the MLP
                used to predict mask quality
        """
        super().__init__()
        self.transformer_dim = transformer_dim
        self.transformer = transformer

        self.num_multimask_outputs = num_multimask_outputs

        self.iou_token = nn.Embedding(1, transformer_dim)
        self.num_mask_tokens = num_multimask_outputs + 1
        self.mask_tokens = nn.Embedding(self.num_mask_tokens, transformer_dim)

        self.upscale_conv1 = ConvTranspose2d(
            transformer_dim,
            transformer_dim // 4,
            kernel_size=2,
            stride=2,
            padding=1,
        )
        self.upscale_layer_norm = LayerNorm2d(transformer_dim // 4)
        self.activation = activation()
        self.upscale_conv2 = ConvTranspose2d(
            transformer_dim // 4,
            transformer_dim // 8,
            kernel_size=2,
            stride=2,
            padding=1,
        )
        self.output_hypernetworks_mlps = [
            MLP(transformer_dim, transformer_dim, transformer_dim // 8, 1)
            for i in range(self.num_mask_tokens)
        ]

        self.iou_prediction_head = MLP(
            transformer_dim,
            iou_head_hidden_dim,
            self.num_mask_tokens,
            iou_head_depth - 2,
        )

    def __call__(
        self,
        image_embeddings: mx.array,
        image_pe: mx.array,
        sparse_prompt_embeddings: mx.array,
        dense_prompt_embeddings: mx.array,
        multimask_output: bool,
    ) -> Tuple[mx.array, mx.array]:
        """
        Predict masks given image and prompt embeddings.

        Args:
            image_embeddings (mx.array): the embeddings from the image encoder
            image_pe (mx.array): positional encoding
            sparse_prompt_embeddings (mx.array): the embeddings of the points and boxes
            dense_prompt_embeddings (mx.array): the embeddings of the mask inputs
            multimask_output (bool): Whether to return multiple masks or a single
                mask.

        Returns:
            mx.array: batched predicted masks
            mx.array: batched predictions of mask quality
        """
        masks, iou_pred = self.predict_masks(
            image_embeddings=image_embeddings,
            image_pe=image_pe,
            sparse_prompt_embeddings=sparse_prompt_embeddings,
            dense_prompt_embeddings=dense_prompt_embeddings,
        )

        # Select the correct mask or masks for output
        if multimask_output:
            mask_slice = slice(1, None)
        else:
            mask_slice = slice(0, 1)
        masks = masks[:, :, :, mask_slice]
        iou_pred = iou_pred[:, mask_slice]

        # Prepare output
        return masks, iou_pred

    def predict_masks(
        self,
        image_embeddings: mx.array,
        image_pe: mx.array,
        sparse_prompt_embeddings: mx.array,
        dense_prompt_embeddings: mx.array,
    ) -> Tuple[mx.array, mx.array]:
        """Predicts masks. See '__call__' for more details."""
        # Concatenate output tokens
        output_tokens = mx.concatenate(
            [self.iou_token.weight, self.mask_tokens.weight], axis=0
        )
        output_tokens = mx.broadcast_to(
            output_tokens[None],
            [
                sparse_prompt_embeddings.shape[0],
                output_tokens.shape[0],
                output_tokens.shape[1],
            ],
        )
        tokens = mx.concatenate((output_tokens, sparse_prompt_embeddings), axis=1)

        # Expand per-image data in batch direction to be per-mask
        src = mx.repeat(image_embeddings, repeats=tokens.shape[0], axis=0)
        src = src + dense_prompt_embeddings
        b, h, w, c = src.shape

        # Run the transformer
        hs, src = self.transformer(src, image_pe, tokens)
        iou_token_out = hs[:, 0, :]
        mask_tokens_out = hs[:, 1 : (1 + self.num_mask_tokens), :]

        # Upscale mask embeddings and predict masks using the mask tokens
        src = src.reshape(b, h, w, c)
        src = self.upscale_conv1(src)
        src = self.upscale_layer_norm(src)
        src = self.activation(src)
        src = self.upscale_conv2(src)
        upscaled_embedding = self.activation(src)
        hyper_in_list: List[mx.array] = []
        for i in range(self.num_mask_tokens):
            hyper_in_list.append(
                self.output_hypernetworks_mlps[i](mask_tokens_out[:, i, :])
            )
        hyper_in = mx.stack(hyper_in_list, axis=1)
        b, h, w, c = upscaled_embedding.shape

        masks = (
            (hyper_in @ upscaled_embedding.reshape(b, h * w, c).transpose(0, 2, 1))
            .transpose(0, 2, 1)
            .reshape(b, h, w, -1)
        )

        # Generate mask quality predictions
        iou_pred = self.iou_prediction_head(iou_token_out)

        return masks, iou_pred


class MLP(nn.Module):
    def __init__(
        self,
        input_dim: int,
        hidden_dim: int,
        output_dim: int,
        num_layers: int,
        sigmoid_output: bool = False,
    ) -> None:
        super().__init__()
        self.num_layers = num_layers
        self.proj_in = nn.Linear(input_dim, hidden_dim)
        self.layers = [nn.Linear(hidden_dim, hidden_dim) for _ in range(num_layers)]
        self.proj_out = nn.Linear(hidden_dim, output_dim)
        self.sigmoid_output = sigmoid_output

    def __call__(self, x):
        x = nn.relu(self.proj_in(x))
        for i, layer in enumerate(self.layers):
            x = nn.relu(layer(x))
        x = self.proj_out(x)
        if self.sigmoid_output:
            x = mx.sigmoid(x)
        return x


# TODO: Naive implem. Replace when mlx.nn support conv_transpose
class ConvTranspose2d(nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: Union[int, tuple],
        stride: Union[int, tuple] = 1,
        padding: Union[int, tuple] = 0,
        dilation: Union[int, tuple] = 1,
        bias: bool = True,
    ):
        super().__init__()

        kernel_size, stride, padding = map(
            lambda x: (x, x) if isinstance(x, int) else x,
            (kernel_size, stride, padding),
        )
        scale = math.sqrt(1 / (in_channels * kernel_size[0] * kernel_size[1]))
        self.weight = mx.random.uniform(
            low=-scale,
            high=scale,
            shape=(out_channels, *kernel_size, in_channels),
        )
        if bias:
            self.bias = mx.zeros((out_channels,))

        self.padding = padding
        self.stride = stride
        self.dilation = dilation

    def _extra_repr(self):
        return (
            f"{self.weight.shape[-1]}, {self.weight.shape[0]}, "
            f"kernel_size={self.weight.shape[1:2]}, stride={self.stride}, "
            f"padding={self.padding}, dilation={self.dilation}, "
            f"bias={'bias' in self}"
        )

    def __call__(self, x):
        y = mx.conv_general(
            x,
            self.weight,
            stride=1,
            padding=self.padding,
            kernel_dilation=self.dilation,
            input_dilation=self.stride,
            flip=True,
        )
        if "bias" in self:
            y = y + self.bias
        return y

>>>> flux/flux/datasets.py
import json
from pathlib import Path

from PIL import Image


class Dataset:
    def __getitem__(self, index: int):
        raise NotImplementedError()

    def __len__(self):
        raise NotImplementedError()


class LocalDataset(Dataset):
    prompt_key = "prompt"

    def __init__(self, dataset: str, data_file):
        self.dataset_base = Path(dataset)
        with open(data_file, "r") as fid:
            self._data = [json.loads(l) for l in fid]

    def __len__(self):
        return len(self._data)

    def __getitem__(self, index: int):
        item = self._data[index]
        image = Image.open(self.dataset_base / item["image"])
        return image, item[self.prompt_key]


class LegacyDataset(LocalDataset):
    prompt_key = "text"

    def __init__(self, dataset: str):
        self.dataset_base = Path(dataset)
        with open(self.dataset_base / "index.json") as f:
            self._data = json.load(f)["data"]


class HuggingFaceDataset(Dataset):

    def __init__(self, dataset: str):
        from datasets import load_dataset as hf_load_dataset

        self._df = hf_load_dataset(dataset)["train"]

    def __len__(self):
        return len(self._df)

    def __getitem__(self, index: int):
        item = self._df[index]
        return item["image"], item["prompt"]


def load_dataset(dataset: str):
    dataset_base = Path(dataset)
    data_file = dataset_base / "train.jsonl"
    legacy_file = dataset_base / "index.json"

    if data_file.exists():
        print(f"Load the local dataset {data_file} .", flush=True)
        dataset = LocalDataset(dataset, data_file)
    elif legacy_file.exists():
        print(f"Load the local dataset {legacy_file} .")
        print()
        print("     WARNING: 'index.json' is deprecated in favor of 'train.jsonl'.")
        print("              See the README for details.")
        print(flush=True)
        dataset = LegacyDataset(dataset)
    else:
        print(f"Load the Hugging Face dataset {dataset} .", flush=True)
        dataset = HuggingFaceDataset(dataset)

    return dataset

>>>> segment_anything/segment_anything/sam.py
import json
from functools import partial
from pathlib import Path
from typing import Any, Dict, List, Tuple

import mlx.core as mx
import mlx.nn as nn

from .image_encoder import ImageEncoderViT
from .mask_decoder import MaskDecoder
from .prompt_encoder import PositionEmbeddingRandom, PromptEncoder
from .transformer import TwoWayTransformer


class Sam(nn.Module):
    mask_threshold: float = 0.0
    image_format: str = "RGB"

    def __init__(
        self,
        vision_encoder: ImageEncoderViT,
        prompt_encoder: PromptEncoder,
        mask_decoder: MaskDecoder,
        pixel_mean: List[float] = [123.675, 116.28, 103.53],
        pixel_std: List[float] = [58.395, 57.12, 57.375],
    ) -> None:
        """
        SAM predicts object masks from an image and input prompts.

        Args:
            vision_encoder (ImageEncoderViT): The backbone used to encode the
                image into image embeddings that allow for efficient mask prediction.
            prompt_encoder (PromptEncoder): Encodes various types of input prompts.
            mask_decoder (MaskDecoder): Predicts masks from the image embeddings
                and encoded prompts.
            pixel_mean (list(float)): Mean values for normalizing pixels in the input image.
            pixel_std (list(float)): Std values for normalizing pixels in the input image.
        """
        super().__init__()
        self.vision_encoder = vision_encoder
        self.prompt_encoder = prompt_encoder
        self.mask_decoder = mask_decoder
        self._pixel_mean = mx.array(pixel_mean).reshape(1, 1, -1)
        self._pixel_std = mx.array(pixel_std).reshape(1, 1, -1)
        self.shared_image_embedding = PositionEmbeddingRandom(
            prompt_encoder.embed_dim // 2
        )

    def __call__(
        self,
        batched_input: List[Dict[str, Any]],
        multimask_output: bool,
    ) -> List[Dict[str, mx.array]]:
        """
        Predicts masks end-to-end from provided images and prompts.
        If prompts are not known in advance, using SamPredictor is
        recommended over calling the model directly.

        Args:
            batched_input (list(dict)): A list over input images, each a
                dictionary with the following keys. A prompt key can be
                excluded if it is not present.
                'image': The image as a mlx tensor in HxWx3 format,
                    already transformed for input to the model.
                'original_size': (tuple(int, int)) The original size of
                    the image before transformation, as (H, W).
                'point_coords': (mx.array) Batched point prompts for
                    this image, with shape BxNx2. Already transformed to the
                    input frame of the model.
                'point_labels': (mx.array) Batched labels for point prompts,
                    with shape BxN.
                'boxes': (mx.array) Batched box inputs, with shape Bx4.
                    Already transformed to the input frame of the model.
                'mask_inputs': (mx.array) Batched mask inputs to the model,
                    in the form BxHxWx1.
            multimask_output (bool): Whether the model should predict multiple
                disambiguating masks, or return a single mask.

        Returns:
            (list(dict)): A list over input images, where each element is
                as dictionary with the following keys.
                'masks': (mx.array) Batched binary mask predictions,
                    with shape BxCxHxW, where B is the number of input prompts,
                    C is determined by multimask_output, and (H, W) is the
                    original size of the image.
                'iou_predictions': (mx.array) The model's predictions
                    of mask quality, in shape BxC.
                'low_res_logits': (mx.array) Low resolution logits with
                    shape BxCxHxW, where H=W=256. Can be passed as mask input
                    to subsequent iterations of prediction.
        """
        input_images = mx.stack(
            [self.preprocess(x["image"]) for x in batched_input], axis=0
        )
        image_embeddings = self.vision_encoder(input_images)

        outputs = []
        for image_record, curr_embedding in zip(batched_input, image_embeddings):
            if "point_coords" in image_record:
                points = (image_record["point_coords"], image_record["point_labels"])
            else:
                points = None
            sparse_embeddings, dense_embeddings = self.prompt_encoder(
                points=points,
                boxes=image_record.get("boxes", None),
                masks=image_record.get("mask_inputs", None),
                pe_layer=self.shared_image_embedding,
            )
            low_res_masks, iou_predictions = self.mask_decoder(
                image_embeddings=curr_embedding[None],
                image_pe=self.shared_image_embedding(
                    self.prompt_encoder.image_embedding_size
                ),
                sparse_prompt_embeddings=sparse_embeddings,
                dense_prompt_embeddings=dense_embeddings,
                multimask_output=multimask_output,
            )

            masks = self.postprocess_masks(
                low_res_masks,
                input_size=image_record["image"].shape[-3:-1],
                original_size=image_record["original_size"],
            )
            masks = masks > self.mask_threshold
            outputs.append(
                {
                    "masks": masks,
                    "iou_predictions": iou_predictions,
                    "low_res_logits": low_res_masks,
                }
            )
        return outputs

    def postprocess_masks(
        self,
        masks: mx.array,
        input_size: Tuple[int, ...],
        original_size: Tuple[int, ...],
    ) -> mx.array:
        """
        Remove padding and upscale masks to the original image size.

        Args:
            masks (mx.array): Batched masks from the mask_decoder,
                in BxHxWxC format.
            input_size (tuple(int, int)): The size of the image input to the
                model, in (H, W) format. Used to remove padding.
            original_size (tuple(int, int)): The original size of the image
                before resizing for input to the model, in (H, W) format.

        Returns:
            (mx.array): Batched masks in BxCxHxW format, where (H, W)
                is given by original_size.
        """
        scale_factor = (
            self.vision_encoder.img_size / masks.shape[1],
            self.vision_encoder.img_size / masks.shape[2],
        )
        masks = nn.Upsample(
            scale_factor=scale_factor, mode="linear", align_corners=False
        )(masks)
        masks = masks[:, : input_size[0], : input_size[1]]
        scale_factor = (
            original_size[0] / masks.shape[1],
            original_size[1] / masks.shape[2],
        )
        masks = nn.Upsample(
            scale_factor=scale_factor, mode="linear", align_corners=False
        )(masks)
        return masks

    def preprocess(self, x: mx.array) -> mx.array:
        """Normalize pixel values and pad to a square input."""
        # Normalize colors
        x = (x - self._pixel_mean) / self._pixel_std

        # Pad
        h, w = x.shape[-3:-1]
        padh = self.vision_encoder.img_size - h
        padw = self.vision_encoder.img_size - w

        if x.ndim == 3:
            pad_width = [(0, padh), (0, padw), (0, 0)]
        elif x.ndim == 4:
            pad_width = [(0, 0), (0, padh), (0, padw), (0, 0)]
        else:
            raise Exception("x.ndim can only be 3 or 4.")

        x = mx.pad(x, pad_width)
        return x


def load(model_path):
    model_path = Path(model_path)
    with open(model_path / "config.json", "r") as fid:
        config = json.load(fid)
    encoder_embed_dim = config["vision_config"]["hidden_size"]
    encoder_depth = config["vision_config"]["num_hidden_layers"]
    encoder_num_heads = config["vision_config"]["num_attention_heads"]
    encoder_global_attn_indexes = config["vision_config"]["global_attn_indexes"]
    prompt_embed_dim = 256
    image_size = 1024
    vit_patch_size = 16
    image_embedding_size = image_size // vit_patch_size
    sam = Sam(
        vision_encoder=ImageEncoderViT(
            depth=encoder_depth,
            embed_dim=encoder_embed_dim,
            img_size=image_size,
            mlp_ratio=4,
            norm_layer=partial(nn.LayerNorm, eps=1e-6),
            num_heads=encoder_num_heads,
            patch_size=vit_patch_size,
            qkv_bias=True,
            use_rel_pos=True,
            global_attn_indexes=encoder_global_attn_indexes,
            window_size=14,
            out_chans=prompt_embed_dim,
        ),
        prompt_encoder=PromptEncoder(
            embed_dim=prompt_embed_dim,
            image_embedding_size=(image_embedding_size, image_embedding_size),
            input_image_size=(image_size, image_size),
            mask_in_chans=16,
        ),
        mask_decoder=MaskDecoder(
            num_multimask_outputs=3,
            transformer=TwoWayTransformer(
                depth=2,
                embedding_dim=prompt_embed_dim,
                mlp_dim=2048,
                num_heads=8,
            ),
            transformer_dim=prompt_embed_dim,
            iou_head_depth=3,
            iou_head_hidden_dim=256,
        ),
    )
    sam.load_weights(str(model_path / "model.safetensors"), strict=True)
    return sam

>>>> flux/flux/t5.py
# Copyright © 2024 Apple Inc.

import math
from dataclasses import dataclass
from typing import List, Optional, Tuple

import mlx.core as mx
import mlx.nn as nn

_SHARED_REPLACEMENT_PATTERNS = [
    (".block.", ".layers."),
    (".k.", ".key_proj."),
    (".o.", ".out_proj."),
    (".q.", ".query_proj."),
    (".v.", ".value_proj."),
    ("shared.", "wte."),
    ("lm_head.", "lm_head.linear."),
    (".layer.0.layer_norm.", ".ln1."),
    (".layer.1.layer_norm.", ".ln2."),
    (".layer.2.layer_norm.", ".ln3."),
    (".final_layer_norm.", ".ln."),
    (
        "layers.0.layer.0.SelfAttention.relative_attention_bias.",
        "relative_attention_bias.embeddings.",
    ),
]

_ENCODER_REPLACEMENT_PATTERNS = [
    (".layer.0.SelfAttention.", ".attention."),
    (".layer.1.DenseReluDense.", ".dense."),
]


@dataclass
class T5Config:
    vocab_size: int
    num_layers: int
    num_heads: int
    relative_attention_num_buckets: int
    d_kv: int
    d_model: int
    feed_forward_proj: str
    tie_word_embeddings: bool

    d_ff: Optional[int] = None
    num_decoder_layers: Optional[int] = None
    relative_attention_max_distance: int = 128
    layer_norm_epsilon: float = 1e-6

    @classmethod
    def from_dict(cls, config):
        return cls(
            vocab_size=config["vocab_size"],
            num_layers=config["num_layers"],
            num_heads=config["num_heads"],
            relative_attention_num_buckets=config["relative_attention_num_buckets"],
            d_kv=config["d_kv"],
            d_model=config["d_model"],
            feed_forward_proj=config["feed_forward_proj"],
            tie_word_embeddings=config["tie_word_embeddings"],
            d_ff=config.get("d_ff", 4 * config["d_model"]),
            num_decoder_layers=config.get("num_decoder_layers", config["num_layers"]),
            relative_attention_max_distance=config.get(
                "relative_attention_max_distance", 128
            ),
            layer_norm_epsilon=config.get("layer_norm_epsilon", 1e-6),
        )


class RelativePositionBias(nn.Module):
    def __init__(self, config: T5Config, bidirectional: bool):
        self.bidirectional = bidirectional
        self.num_buckets = config.relative_attention_num_buckets
        self.max_distance = config.relative_attention_max_distance
        self.n_heads = config.num_heads
        self.embeddings = nn.Embedding(self.num_buckets, self.n_heads)

    @staticmethod
    def _relative_position_bucket(rpos, bidirectional, num_buckets, max_distance):
        num_buckets = num_buckets // 2 if bidirectional else num_buckets
        max_exact = num_buckets // 2

        abspos = rpos.abs()
        is_small = abspos < max_exact

        scale = (num_buckets - max_exact) / math.log(max_distance / max_exact)
        buckets_large = (mx.log(abspos / max_exact) * scale).astype(mx.int16)
        buckets_large = mx.minimum(max_exact + buckets_large, num_buckets - 1)

        buckets = mx.where(is_small, abspos, buckets_large)
        if bidirectional:
            buckets = buckets + (rpos > 0) * num_buckets
        else:
            buckets = buckets * (rpos < 0)

        return buckets

    def __call__(self, query_length: int, key_length: int, offset: int = 0):
        """Compute binned relative position bias"""
        context_position = mx.arange(offset, query_length)[:, None]
        memory_position = mx.arange(key_length)[None, :]

        # shape (query_length, key_length)
        relative_position = memory_position - context_position
        relative_position_bucket = self._relative_position_bucket(
            relative_position,
            bidirectional=self.bidirectional,
            num_buckets=self.num_buckets,
            max_distance=self.max_distance,
        )

        # shape (query_length, key_length, num_heads)
        values = self.embeddings(relative_position_bucket)

        # shape (num_heads, query_length, key_length)
        return values.transpose(2, 0, 1)


class MultiHeadAttention(nn.Module):
    def __init__(self, config: T5Config):
        super().__init__()
        inner_dim = config.d_kv * config.num_heads
        self.num_heads = config.num_heads
        self.query_proj = nn.Linear(config.d_model, inner_dim, bias=False)
        self.key_proj = nn.Linear(config.d_model, inner_dim, bias=False)
        self.value_proj = nn.Linear(config.d_model, inner_dim, bias=False)
        self.out_proj = nn.Linear(inner_dim, config.d_model, bias=False)

    def __call__(
        self,
        queries: mx.array,
        keys: mx.array,
        values: mx.array,
        mask: Optional[mx.array],
        cache: Optional[Tuple[mx.array, mx.array]] = None,
    ) -> [mx.array, Tuple[mx.array, mx.array]]:
        queries = self.query_proj(queries)
        keys = self.key_proj(keys)
        values = self.value_proj(values)

        num_heads = self.num_heads
        B, L, _ = queries.shape
        _, S, _ = keys.shape
        queries = queries.reshape(B, L, num_heads, -1).transpose(0, 2, 1, 3)
        keys = keys.reshape(B, S, num_heads, -1).transpose(0, 2, 1, 3)
        values = values.reshape(B, S, num_heads, -1).transpose(0, 2, 1, 3)

        if cache is not None:
            key_cache, value_cache = cache
            keys = mx.concatenate([key_cache, keys], axis=3)
            values = mx.concatenate([value_cache, values], axis=2)

        values_hat = mx.fast.scaled_dot_product_attention(
            queries, keys, values, scale=1.0, mask=mask.astype(queries.dtype)
        )
        values_hat = values_hat.transpose(0, 2, 1, 3).reshape(B, L, -1)

        return self.out_proj(values_hat), (keys, values)


class DenseActivation(nn.Module):
    def __init__(self, config: T5Config):
        super().__init__()
        mlp_dims = config.d_ff or config.d_model * 4
        self.gated = config.feed_forward_proj.startswith("gated")
        if self.gated:
            self.wi_0 = nn.Linear(config.d_model, mlp_dims, bias=False)
            self.wi_1 = nn.Linear(config.d_model, mlp_dims, bias=False)
        else:
            self.wi = nn.Linear(config.d_model, mlp_dims, bias=False)
        self.wo = nn.Linear(mlp_dims, config.d_model, bias=False)
        activation = config.feed_forward_proj.removeprefix("gated-")
        if activation == "relu":
            self.act = nn.relu
        elif activation == "gelu":
            self.act = nn.gelu
        elif activation == "silu":
            self.act = nn.silu
        else:
            raise ValueError(f"Unknown activation: {activation}")

    def __call__(self, x):
        if self.gated:
            hidden_act = self.act(self.wi_0(x))
            hidden_linear = self.wi_1(x)
            x = hidden_act * hidden_linear
        else:
            x = self.act(self.wi(x))
        return self.wo(x)


class TransformerEncoderLayer(nn.Module):
    def __init__(self, config: T5Config):
        super().__init__()
        self.attention = MultiHeadAttention(config)
        self.ln1 = nn.RMSNorm(config.d_model, eps=config.layer_norm_epsilon)
        self.ln2 = nn.RMSNorm(config.d_model, eps=config.layer_norm_epsilon)
        self.dense = DenseActivation(config)

    def __call__(self, x, mask):
        y = self.ln1(x)
        y, _ = self.attention(y, y, y, mask=mask)
        x = x + y

        y = self.ln2(x)
        y = self.dense(y)
        return x + y


class TransformerEncoder(nn.Module):
    def __init__(self, config: T5Config):
        super().__init__()
        self.layers = [
            TransformerEncoderLayer(config) for i in range(config.num_layers)
        ]
        self.ln = nn.RMSNorm(config.d_model, eps=config.layer_norm_epsilon)
        self.relative_attention_bias = RelativePositionBias(config, bidirectional=True)

    def __call__(self, x: mx.array):
        pos_bias = self.relative_attention_bias(x.shape[1], x.shape[1])
        pos_bias = pos_bias.astype(x.dtype)
        for layer in self.layers:
            x = layer(x, mask=pos_bias)
        return self.ln(x)


class T5Encoder(nn.Module):
    def __init__(self, config: T5Config):
        self.wte = nn.Embedding(config.vocab_size, config.d_model)
        self.encoder = TransformerEncoder(config)

    def sanitize(self, weights):
        new_weights = {}
        for k, w in weights.items():
            for old, new in _SHARED_REPLACEMENT_PATTERNS:
                k = k.replace(old, new)
            if k.startswith("encoder."):
                for old, new in _ENCODER_REPLACEMENT_PATTERNS:
                    k = k.replace(old, new)
            new_weights[k] = w
        return new_weights

    def __call__(self, inputs: mx.array):
        return self.encoder(self.wte(inputs))

>>>> segment_anything/segment_anything/prompt_encoder.py
from typing import Optional, Tuple, Type

import mlx.core as mx
import mlx.nn as nn

from .common import LayerNorm2d


class PromptEncoder(nn.Module):
    def __init__(
        self,
        embed_dim: int,
        image_embedding_size: Tuple[int, int],
        input_image_size: Tuple[int, int],
        mask_in_chans: int,
        activation: Type[nn.Module] = nn.GELU,
    ) -> None:
        """
        Encodes prompts for input to SAM's mask decoder.

        Args:
            embed_dim (int): The prompts' embedding dimension
            image_embedding_size (tuple(int, int)): The spatial size of the
                image embedding, as (H, W).
            input_image_size (int): The padded size of the image as input
                to the image encoder, as (H, W).
            mask_in_chans (int): The number of hidden channels used for
                encoding input masks.
            activation (nn.Module): The activation to use when encoding
                input masks.
        """
        super().__init__()
        self.embed_dim = embed_dim
        self.input_image_size = input_image_size
        self.image_embedding_size = image_embedding_size

        self.num_point_embeddings: int = 4  # pos/neg point + 2 box corners
        self.point_embed = [
            nn.Embedding(1, embed_dim) for i in range(self.num_point_embeddings)
        ]
        self.not_a_point_embed = nn.Embedding(1, embed_dim)

        self.mask_input_size = (
            4 * image_embedding_size[0],
            4 * image_embedding_size[1],
        )
        self.mask_embed = MaskEmbed(embed_dim, mask_in_chans, activation)
        self.no_mask_embed = nn.Embedding(1, embed_dim)

    def _embed_points(
        self,
        points: mx.array,
        labels: mx.array,
        pad: bool,
        pe_layer: nn.Module,
    ) -> mx.array:
        """Embeds point prompts."""
        points = points + 0.5  # Shift to center of pixel
        if pad:
            padding_point = mx.zeros((points.shape[0], 1, 2))
            padding_label = -mx.ones((labels.shape[0], 1))
            points = mx.concatenate([points, padding_point], axis=1)
            labels = mx.concatenate([labels, padding_label], axis=1)
        point_embedding = pe_layer.forward_with_coords(points, self.input_image_size)
        point_embedding = mx.where(
            labels[..., None] == -1,
            self.not_a_point_embed.weight[:, None],
            point_embedding,
        )
        point_embedding = mx.where(
            labels[..., None] == 0,
            point_embedding + self.point_embed[0].weight[:, None],
            point_embedding,
        )
        point_embedding = mx.where(
            labels[..., None] == 1,
            point_embedding + self.point_embed[1].weight[:, None],
            point_embedding,
        )
        return point_embedding

    def _embed_boxes(self, boxes: mx.array, pe_layer: nn.Module) -> mx.array:
        """Embeds box prompts."""
        boxes = boxes + 0.5  # Shift to center of pixel
        coords = boxes.reshape(-1, 2, 2)
        corner_embedding = pe_layer.forward_with_coords(coords, self.input_image_size)
        corner_embedding[:, 0, :] += self.point_embed[2].weight
        corner_embedding[:, 1, :] += self.point_embed[3].weight
        return corner_embedding

    def _embed_masks(self, masks: mx.array) -> mx.array:
        """Embeds mask inputs."""
        mask_embedding = self.mask_embed(masks)
        return mask_embedding

    def _get_batch_size(
        self,
        points: Optional[Tuple[mx.array, mx.array]],
        boxes: Optional[mx.array],
        masks: Optional[mx.array],
    ) -> int:
        """
        Gets the batch size of the output given the batch size of the input prompts.
        """
        if points is not None:
            return points[0].shape[0]
        elif boxes is not None:
            return boxes.shape[0]
        elif masks is not None:
            return masks.shape[0]
        else:
            return 1

    def __call__(
        self,
        points: Optional[Tuple[mx.array, mx.array]],
        boxes: Optional[mx.array],
        masks: Optional[mx.array],
        pe_layer: nn.Module,
    ) -> Tuple[mx.array, mx.array]:
        """
        Embeds different types of prompts, returning both sparse and dense
        embeddings.

        Args:
            points (tuple(mx.array, mx.array) or none): point coordinates
                and labels to embed
            boxes (mx.array or none): boxes to embed
            masks (mx.array or none): masks to embed
            pe_layer (PositionEmbeddingRandom): shared position embedding
                layer

        Returns:
            mx.array: sparse embeddings for the points and boxes, with shape
                BxNx(embed_dim), where N is determined by the number of input points
                and boxes.
            mx.array: dense embeddings for the masks, in the shape
                Bx(embed_H)x(embed_W)x(embed_dim)
        """
        bs = self._get_batch_size(points, boxes, masks)
        sparse_embeddings = mx.zeros((bs, 0, self.embed_dim))
        if points is not None:
            coords, labels = points
            point_embeddings = self._embed_points(
                coords, labels, pad=(boxes is None), pe_layer=pe_layer
            )
            sparse_embeddings = mx.concatenate(
                [sparse_embeddings, point_embeddings], axis=1
            )
        if boxes is not None:
            box_embeddings = self._embed_boxes(boxes, pe_layer=pe_layer)
            sparse_embeddings = mx.concatenate(
                [sparse_embeddings, box_embeddings], axis=1
            )

        if masks is not None:
            dense_embeddings = self._embed_masks(masks)
        else:
            dense_embeddings = mx.broadcast_to(
                self.no_mask_embed.weight,
                shape=(
                    bs,
                    self.image_embedding_size[0],
                    self.image_embedding_size[1],
                    self.embed_dim,
                ),
            )

        return sparse_embeddings, dense_embeddings


class MaskEmbed(nn.Module):
    def __init__(self, embed_dim, mask_in_chans, activation):
        super().__init__()
        self.conv1 = nn.Conv2d(1, mask_in_chans // 4, kernel_size=2, stride=2)
        self.layer_norm1 = LayerNorm2d(mask_in_chans // 4)
        self.conv2 = nn.Conv2d(
            mask_in_chans // 4, mask_in_chans, kernel_size=2, stride=2
        )
        self.layer_norm2 = LayerNorm2d(mask_in_chans)
        self.conv3 = nn.Conv2d(mask_in_chans, embed_dim, kernel_size=1)
        self.activation = activation()

    def __call__(self, x):
        x = self.activation(self.layer_norm1(self.conv1(x)))
        x = self.activation(self.layer_norm2(self.conv2(x)))
        return self.conv3(x)


class PositionEmbeddingRandom(nn.Module):
    """
    Positional encoding using random spatial frequencies.
    """

    def __init__(self, num_pos_feats: int = 64, scale: Optional[float] = None) -> None:
        super().__init__()
        if scale is None or scale <= 0.0:
            scale = 1.0
        self.positional_embedding = scale * mx.random.normal((2, num_pos_feats))

    def _pe_encoding(self, coords: mx.array) -> mx.array:
        """Positionally encode points that are normalized to [0,1]."""
        # assuming coords are in [0, 1]^2 square and have d_1 x ... x d_n x 2 shape
        coords = 2 * coords - 1
        coords = coords @ self.positional_embedding
        coords = 2 * mx.pi * coords
        # outputs d_1 x ... x d_n x C shape
        return mx.concatenate([mx.sin(coords), mx.cos(coords)], axis=-1)

    def __call__(self, size: Tuple[int, int]) -> mx.array:
        """Generate positional encoding for a grid of the specified size."""
        h, w = size
        grid = mx.ones((h, w), dtype=mx.float32)
        y_embed = grid.cumsum(axis=0) - 0.5
        x_embed = grid.cumsum(axis=1) - 0.5
        y_embed = y_embed / h
        x_embed = x_embed / w

        pe = self._pe_encoding(mx.stack([x_embed, y_embed], axis=-1))
        return pe  # HWC

    def forward_with_coords(
        self, coords_input: mx.array, image_size: Tuple[int, int]
    ) -> mx.array:
        """Positionally encode points that are not normalized to [0,1]."""
        coords = coords_input * 1
        coords[:, :, 0] = coords[:, :, 0] / image_size[1]
        coords[:, :, 1] = coords[:, :, 1] / image_size[0]
        return self._pe_encoding(coords.astype(mx.float32))  # B x N x C

>>>> segment_anything/segment_anything/image_encoder.py
from typing import Optional, Tuple, Type

import mlx.core as mx
import mlx.nn as nn

from .common import LayerNorm2d, MLPBlock


class ImageEncoderViT(nn.Module):
    def __init__(
        self,
        img_size: int = 1024,
        patch_size: int = 16,
        in_chans: int = 3,
        embed_dim: int = 768,
        depth: int = 12,
        num_heads: int = 12,
        mlp_ratio: float = 4.0,
        out_chans: int = 256,
        qkv_bias: bool = True,
        norm_layer: Type[nn.Module] = nn.LayerNorm,
        act_layer: Type[nn.Module] = nn.GELU,
        use_abs_pos: bool = True,
        use_rel_pos: bool = False,
        rel_pos_zero_init: bool = True,
        window_size: int = 0,
        global_attn_indexes: Tuple[int, ...] = (),
    ) -> None:
        """
        Args:
            img_size (int): Input image size.
            patch_size (int): Patch size.
            in_chans (int): Number of input image channels.
            embed_dim (int): Patch embedding dimension.
            depth (int): Depth of ViT.
            num_heads (int): Number of attention heads in each ViT block.
            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
            qkv_bias (bool): If True, add a learnable bias to query, key, value.
            norm_layer (nn.Module): Normalization layer.
            act_layer (nn.Module): Activation layer.
            use_abs_pos (bool): If True, use absolute positional embeddings.
            use_rel_pos (bool): If True, add relative positional embeddings to the attention map.
            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.
            window_size (int): Window size for window attention blocks.
            global_attn_indexes (list): Indexes for blocks using global attention.
        """
        super().__init__()
        self.img_size = img_size

        self.patch_embed = PatchEmbed(
            kernel_size=(patch_size, patch_size),
            stride=(patch_size, patch_size),
            in_chans=in_chans,
            embed_dim=embed_dim,
        )

        if use_abs_pos:
            # Initialize absolute positional embedding with pretrain image size.
            self.pos_embed = mx.zeros(
                [1, img_size // patch_size, img_size // patch_size, embed_dim]
            )
        else:
            self.pos_embed = None

        self.layers = []
        for i in range(depth):
            block = Block(
                dim=embed_dim,
                num_heads=num_heads,
                mlp_ratio=mlp_ratio,
                qkv_bias=qkv_bias,
                norm_layer=norm_layer,
                act_layer=act_layer,
                use_rel_pos=use_rel_pos,
                rel_pos_zero_init=rel_pos_zero_init,
                window_size=window_size if i not in global_attn_indexes else 0,
                input_size=(img_size // patch_size, img_size // patch_size),
            )
            self.layers.append(block)

        self.neck = Neck(embed_dim, out_chans)

    def __call__(self, x: mx.array) -> mx.array:
        x = self.patch_embed(x)
        if self.pos_embed is not None:
            x = x + self.pos_embed

        for blk in self.layers:
            x = blk(x)

        x = self.neck(x)
        return x


class Neck(nn.Module):
    def __init__(self, embed_dim, out_chans):
        super().__init__()
        self.conv1 = nn.Conv2d(
            embed_dim,
            out_chans,
            kernel_size=1,
            bias=False,
        )
        self.layer_norm1 = LayerNorm2d(out_chans)
        self.conv2 = nn.Conv2d(
            out_chans,
            out_chans,
            kernel_size=3,
            padding=1,
            bias=False,
        )
        self.layer_norm2 = LayerNorm2d(out_chans)

    def __call__(self, x):
        return self.layer_norm2(self.conv2(self.layer_norm1(self.conv1(x))))


class Block(nn.Module):
    """Transformer blocks with support of window attention and residual propagation blocks"""

    def __init__(
        self,
        dim: int,
        num_heads: int,
        mlp_ratio: float = 4.0,
        qkv_bias: bool = True,
        norm_layer: Type[nn.Module] = nn.LayerNorm,
        act_layer: Type[nn.Module] = nn.GELU,
        use_rel_pos: bool = False,
        rel_pos_zero_init: bool = True,
        window_size: int = 0,
        input_size: Optional[Tuple[int, int]] = None,
    ) -> None:
        """
        Args:
            dim (int): Number of input channels.
            num_heads (int): Number of attention heads in each ViT block.
            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
            qkv_bias (bool): If True, add a learnable bias to query, key, value.
            norm_layer (nn.Module): Normalization layer.
            act_layer (nn.Module): Activation layer.
            use_rel_pos (bool): If True, add relative positional embeddings to the attention map.
            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.
            window_size (int): Window size for window attention blocks. If it equals 0, then
                use global attention.
            input_size (tuple(int, int) or None): Input resolution for calculating the relative
                positional parameter size.
        """
        super().__init__()
        self.layer_norm1 = norm_layer(dim)
        self.attn = Attention(
            dim,
            num_heads=num_heads,
            qkv_bias=qkv_bias,
            use_rel_pos=use_rel_pos,
            rel_pos_zero_init=rel_pos_zero_init,
            input_size=input_size if window_size == 0 else (window_size, window_size),
        )

        self.layer_norm2 = norm_layer(dim)
        self.mlp = MLPBlock(
            embedding_dim=dim, mlp_dim=int(dim * mlp_ratio), act=act_layer
        )

        self.window_size = window_size

    def __call__(self, x: mx.array) -> mx.array:
        shortcut = x
        x = self.layer_norm1(x)
        # Window partition
        if self.window_size > 0:
            H, W = x.shape[1], x.shape[2]
            x, pad_hw = window_partition(x, self.window_size)

        x = self.attn(x)
        # Reverse window partition
        if self.window_size > 0:
            x = window_unpartition(x, self.window_size, pad_hw, (H, W))

        x = shortcut + x
        x = x + self.mlp(self.layer_norm2(x))

        return x


class Attention(nn.Module):
    """Multi-head Attention block with relative position embeddings."""

    def __init__(
        self,
        dim: int,
        num_heads: int = 8,
        qkv_bias: bool = True,
        use_rel_pos: bool = False,
        rel_pos_zero_init: bool = True,
        input_size: Optional[Tuple[int, int]] = None,
    ) -> None:
        """
        Args:
            dim (int): Number of input channels.
            num_heads (int): Number of attention heads.
            qkv_bias (bool):  If True, add a learnable bias to query, key, value.
            rel_pos (bool): If True, add relative positional embeddings to the attention map.
            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.
            input_size (tuple(int, int) or None): Input resolution for calculating the relative
                positional parameter size.
        """
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = head_dim**-0.5

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.proj = nn.Linear(dim, dim)

        self.use_rel_pos = use_rel_pos
        if self.use_rel_pos:
            assert (
                input_size is not None
            ), "Input size must be provided if using relative positional encoding."
            # initialize relative positional embeddings
            self.rel_pos_h = mx.zeros(shape=(2 * input_size[0] - 1, head_dim))
            self.rel_pos_w = mx.zeros(shape=(2 * input_size[1] - 1, head_dim))

    def __call__(self, x: mx.array) -> mx.array:
        B, H, W, _ = x.shape
        # qkv with shape (3, B, nHead, H * W, C)
        qkv = (
            self.qkv(x)
            .reshape(B, H * W, 3, self.num_heads, -1)
            .transpose(2, 0, 3, 1, 4)
        )

        # q, k, v with shape (B * nHead, H * W, C)
        qkv = qkv.reshape(3, B * self.num_heads, H * W, -1)
        q, k, v = qkv[0], qkv[1], qkv[2]

        attn = (q * self.scale) @ k.transpose(0, 2, 1)

        if self.use_rel_pos:
            attn = add_decomposed_rel_pos(
                attn, q, self.rel_pos_h, self.rel_pos_w, (H, W), (H, W)
            )

        attn = mx.softmax(attn, axis=-1)
        x = (
            (attn @ v)
            .reshape(B, self.num_heads, H, W, -1)
            .transpose(0, 2, 3, 1, 4)
            .reshape(B, H, W, -1)
        )
        x = self.proj(x)

        return x


def window_partition(x: mx.array, window_size: int) -> Tuple[mx.array, Tuple[int, int]]:
    """
    Partition into non-overlapping windows with padding if needed.
    Args:
        x (mx.array): input tokens with [B, H, W, C].
        window_size (int): window size.

    Returns:
        windows: windows after partition with [B * num_windows, window_size, window_size, C].
        (Hp, Wp): padded height and width before partition
    """
    B, H, W, C = x.shape

    pad_h = (window_size - H % window_size) % window_size
    pad_w = (window_size - W % window_size) % window_size
    if pad_h > 0 or pad_w > 0:
        x = mx.pad(x, ((0, 0), (0, pad_w), (0, pad_h), (0, 0)))
    Hp, Wp = H + pad_h, W + pad_w

    x = x.reshape(B, Hp // window_size, window_size, Wp // window_size, window_size, C)
    windows = x.transpose(0, 1, 3, 2, 4, 5).reshape(-1, window_size, window_size, C)
    return windows, (Hp, Wp)


def window_unpartition(
    windows: mx.array,
    window_size: int,
    pad_hw: Tuple[int, int],
    hw: Tuple[int, int],
) -> mx.array:
    """
    Window unpartition into original sequences and removing padding.
    Args:
        windows (mx.array): input tokens with [B * num_windows, window_size, window_size, C].
        window_size (int): window size.
        pad_hw (Tuple): padded height and width (Hp, Wp).
        hw (Tuple): original height and width (H, W) before padding.

    Returns:
        x: unpartitioned sequences with [B, H, W, C].
    """
    Hp, Wp = pad_hw
    H, W = hw
    B = windows.shape[0] // (Hp * Wp // window_size // window_size)
    x = windows.reshape(
        B, Hp // window_size, Wp // window_size, window_size, window_size, -1
    )
    x = x.transpose(0, 1, 3, 2, 4, 5).reshape(B, Hp, Wp, -1)

    if Hp > H or Wp > W:
        x = x[:, :H, :W, :]
    return x


def get_rel_pos(q_size: int, k_size: int, rel_pos: mx.array) -> mx.array:
    """
    Get relative positional embeddings according to the relative positions of
        query and key sizes.
    Args:
        q_size (int): size of query q.
        k_size (int): size of key k.
        rel_pos (mx.array): relative position embeddings (L, C).

    Returns:
        Extracted positional embeddings according to relative positions.
    """
    max_rel_dist = int(2 * max(q_size, k_size) - 1)
    # Interpolate rel pos if needed.
    if rel_pos.shape[0] != max_rel_dist:
        # Interpolate rel pos.
        rel_pos_resized = rel_pos.reshape(1, rel_pos.shape[0], -1).transpose(0, 2, 1)
        scale_factor = (
            max_rel_dist[0] / rel_pos_resized.shape[1],
            max_rel_dist[1] / rel_pos_resized.shape[2],
        )
        rel_pos_resized = nn.Upsample(scale_factor=scale_factor, mode="linear")(
            rel_pos_resized
        )
        rel_pos_resized = rel_pos_resized.reshape(-1, max_rel_dist).transpose(1, 0)
    else:
        rel_pos_resized = rel_pos

    # Scale the coords with short length if shapes for q and k are different.
    q_coords = mx.arange(q_size)[:, None] * max(k_size / q_size, 1.0)
    k_coords = mx.arange(k_size)[None, :] * max(q_size / k_size, 1.0)
    relative_coords = (q_coords - k_coords) + (k_size - 1) * max(q_size / k_size, 1.0)

    return rel_pos_resized[relative_coords.astype(mx.int64)]


def add_decomposed_rel_pos(
    attn,
    q,
    rel_pos_h: mx.array,
    rel_pos_w: mx.array,
    q_size: Tuple[int, int],
    k_size: Tuple[int, int],
) -> mx.array:
    """
    Calculate decomposed Relative Positional Embeddings from :paper:`mvitv2`.
    https://github.com/facebookresearch/mvit/blob/19786631e330df9f3622e5402b4a419a263a2c80/mvit/models/attention.py   # noqa B950
    Args:
        attn (mx.array): attention map.
        q (mx.array): query q in the attention layer with shape (B, q_h * q_w, C).
        rel_pos_h (mx.array): relative position embeddings (Lh, C) for height axis.
        rel_pos_w (mx.array): relative position embeddings (Lw, C) for width axis.
        q_size (Tuple): spatial sequence size of query q with (q_h, q_w).
        k_size (Tuple): spatial sequence size of key k with (k_h, k_w).

    Returns:
        attn (mx.array): attention map with added relative positional embeddings.
    """
    q_h, q_w = q_size
    k_h, k_w = k_size
    Rh = get_rel_pos(q_h, k_h, rel_pos_h)
    Rw = get_rel_pos(q_w, k_w, rel_pos_w)

    B, _, dim = q.shape
    r_q = q.reshape(B, q_h, q_w, dim)

    # TODO: replace mx.einsum when its ready
    # workaround for these einsum computations
    # rel_h = torch.einsum("bhwc,hkc->bhwk", r_q, Rh)
    # rel_w = torch.einsum("bhwc,wkc->bhwk", r_q, Rw)
    rel_h = r_q @ Rh.transpose(0, 2, 1)
    rel_w = (r_q.transpose(0, 2, 1, 3) @ Rw.transpose(0, 2, 1)).transpose(0, 2, 1, 3)

    attn = (
        attn.reshape(B, q_h, q_w, k_h, k_w)
        + rel_h[:, :, :, :, None]
        + rel_w[:, :, :, None, :]
    ).reshape(B, q_h * q_w, k_h * k_w)

    return attn


class PatchEmbed(nn.Module):
    """
    Image to Patch Embedding.
    """

    def __init__(
        self,
        kernel_size: Tuple[int, int] = (16, 16),
        stride: Tuple[int, int] = (16, 16),
        padding: Tuple[int, int] = (0, 0),
        in_chans: int = 3,
        embed_dim: int = 768,
    ) -> None:
        """
        Args:
            kernel_size (Tuple): kernel size of the projection layer.
            stride (Tuple): stride of the projection layer.
            padding (Tuple): padding size of the projection layer.
            in_chans (int): Number of input image channels.
            embed_dim (int): Patch embedding dimension.
        """
        super().__init__()

        self.projection = nn.Conv2d(
            in_chans, embed_dim, kernel_size=kernel_size, stride=stride, padding=padding
        )

    def __call__(self, x: mx.array) -> mx.array:
        x = self.projection(x)
        return x

>>>> segment_anything/segment_anything/transformer.py
import math
from typing import Tuple, Type

import mlx.core as mx
import mlx.nn as nn

from .common import MLPBlock


class TwoWayTransformer(nn.Module):
    def __init__(
        self,
        depth: int,
        embedding_dim: int,
        num_heads: int,
        mlp_dim: int,
        activation: Type[nn.Module] = nn.ReLU,
        attention_downsample_rate: int = 2,
    ) -> None:
        """
        A transformer decoder that attends to an input image using
        queries whose positional embedding is supplied.

        Args:
            depth (int): number of layers in the transformer
            embedding_dim (int): the channel dimension for the input embeddings
            num_heads (int): the number of heads for multihead attention. Must
                divide embedding_dim
            mlp_dim (int): the channel dimension internal to the MLP block
            activation (nn.Module): the activation to use in the MLP block
        """
        super().__init__()
        self.depth = depth
        self.embedding_dim = embedding_dim
        self.num_heads = num_heads
        self.mlp_dim = mlp_dim
        self.layers = []

        for i in range(depth):
            self.layers.append(
                TwoWayAttentionBlock(
                    embedding_dim=embedding_dim,
                    num_heads=num_heads,
                    mlp_dim=mlp_dim,
                    activation=activation,
                    attention_downsample_rate=attention_downsample_rate,
                    skip_first_layer_pe=(i == 0),
                )
            )

        self.final_attn_token_to_image = Attention(
            embedding_dim, num_heads, downsample_rate=attention_downsample_rate
        )
        self.layer_norm_final_attn = nn.LayerNorm(embedding_dim)

    def __call__(
        self,
        image_embedding: mx.array,
        image_pe: mx.array,
        point_embedding: mx.array,
    ) -> Tuple[mx.array, mx.array]:
        """
        Args:
            image_embedding (mx.array): image to attend to. Should be shape
                B x h x w x embedding_dim for any h and w.
            image_pe (mx.array): the positional encoding to add to the image. Must
                have the same shape as image_embedding.
            point_embedding (mx.array): the embedding to add to the query points.
                Must have shape B x N_points x embedding_dim for any N_points.

        Returns:
            mx.array: the processed point_embedding
            mx.array: the processed image_embedding
        """
        # BxHxWxC -> BxHWxC == B x N_image_tokens x C
        bs, h, w, c = image_embedding.shape
        image_embedding = image_embedding.reshape(bs, h * w, c)
        image_pe = image_pe.reshape(h * w, c)

        # Prepare queries
        queries = point_embedding
        keys = image_embedding
        # Apply transformer blocks and final layernorm
        for layer in self.layers:
            queries, keys = layer(
                queries=queries,
                keys=keys,
                query_pe=point_embedding,
                key_pe=image_pe,
            )

        # Apply the final attention layer from the points to the image
        q = queries + point_embedding
        k = keys + image_pe
        attn_out = self.final_attn_token_to_image(q=q, k=k, v=keys)
        queries = queries + attn_out
        queries = self.layer_norm_final_attn(queries)

        return queries, keys


class TwoWayAttentionBlock(nn.Module):
    def __init__(
        self,
        embedding_dim: int,
        num_heads: int,
        mlp_dim: int = 2048,
        activation: Type[nn.Module] = nn.ReLU,
        attention_downsample_rate: int = 2,
        skip_first_layer_pe: bool = False,
    ) -> None:
        """
        A transformer block with four layers: (1) self-attention of sparse
        inputs, (2) cross attention of sparse inputs to dense inputs, (3) mlp
        block on sparse inputs, and (4) cross attention of dense inputs to sparse
        inputs.

        Args:
            embedding_dim (int): the channel dimension of the embeddings
            num_heads (int): the number of heads in the attention layers
            mlp_dim (int): the hidden dimension of the mlp block
            activation (nn.Module): the activation of the mlp block
            skip_first_layer_pe (bool): skip the PE on the first layer
        """
        super().__init__()
        self.self_attn = Attention(embedding_dim, num_heads)
        self.layer_norm1 = nn.LayerNorm(embedding_dim)

        self.cross_attn_token_to_image = Attention(
            embedding_dim, num_heads, downsample_rate=attention_downsample_rate
        )
        self.layer_norm2 = nn.LayerNorm(embedding_dim)

        self.mlp = MLPBlock(embedding_dim, mlp_dim, activation)
        self.layer_norm3 = nn.LayerNorm(embedding_dim)

        self.layer_norm4 = nn.LayerNorm(embedding_dim)
        self.cross_attn_image_to_token = Attention(
            embedding_dim, num_heads, downsample_rate=attention_downsample_rate
        )

        self.skip_first_layer_pe = skip_first_layer_pe

    def __call__(
        self, queries: mx.array, keys: mx.array, query_pe: mx.array, key_pe: mx.array
    ) -> Tuple[mx.array, mx.array]:
        # Self attention block
        if self.skip_first_layer_pe:
            queries = self.self_attn(q=queries, k=queries, v=queries)
        else:
            q = queries + query_pe
            attn_out = self.self_attn(q=q, k=q, v=queries)
            queries = queries + attn_out
        queries = self.layer_norm1(queries)

        # Cross attention block, tokens attending to image embedding
        q = queries + query_pe
        k = keys + key_pe
        attn_out = self.cross_attn_token_to_image(q=q, k=k, v=keys)
        queries = queries + attn_out
        queries = self.layer_norm2(queries)

        # MLP block
        mlp_out = self.mlp(queries)
        queries = queries + mlp_out
        queries = self.layer_norm3(queries)

        # Cross attention block, image embedding attending to tokens
        q = queries + query_pe
        k = keys + key_pe
        attn_out = self.cross_attn_image_to_token(q=k, k=q, v=queries)
        keys = keys + attn_out
        keys = self.layer_norm4(keys)

        return queries, keys


class Attention(nn.Module):
    """
    An attention layer that allows for downscaling the size of the embedding
    after projection to queries, keys, and values.
    """

    def __init__(
        self,
        embedding_dim: int,
        num_heads: int,
        downsample_rate: int = 1,
    ) -> None:
        super().__init__()
        self.embedding_dim = embedding_dim
        self.internal_dim = embedding_dim // downsample_rate
        self.num_heads = num_heads
        assert (
            self.internal_dim % num_heads == 0
        ), "num_heads must divide embedding_dim."

        self.q_proj = nn.Linear(embedding_dim, self.internal_dim)
        self.k_proj = nn.Linear(embedding_dim, self.internal_dim)
        self.v_proj = nn.Linear(embedding_dim, self.internal_dim)
        self.out_proj = nn.Linear(self.internal_dim, embedding_dim)

    def _separate_heads(self, x: mx.array, num_heads: int) -> mx.array:
        b, n, c = x.shape
        x = x.reshape(b, n, num_heads, c // num_heads)
        return x.transpose(0, 2, 1, 3)  # B x N_heads x N_tokens x C_per_head

    def _recombine_heads(self, x: mx.array) -> mx.array:
        b, n_heads, n_tokens, c_per_head = x.shape
        x = x.transpose(0, 2, 1, 3)
        return x.reshape(b, n_tokens, n_heads * c_per_head)  # B x N_tokens x C

    def __call__(self, q: mx.array, k: mx.array, v: mx.array) -> mx.array:
        # Input projections
        q = self.q_proj(q)
        k = self.k_proj(k)
        v = self.v_proj(v)

        # Separate into heads
        q = self._separate_heads(q, self.num_heads)
        k = self._separate_heads(k, self.num_heads)
        v = self._separate_heads(v, self.num_heads)

        # Attention
        _, _, _, c_per_head = q.shape
        attn = q @ k.transpose(0, 1, 3, 2)  # B x N_heads x N_tokens x N_tokens
        attn = attn / math.sqrt(c_per_head)
        attn = mx.softmax(attn, axis=-1)

        # Get output
        out = attn @ v
        out = self._recombine_heads(out)
        out = self.out_proj(out)

        return out

>>>> segment_anything/segment_anything/common.py
from typing import Type

import mlx.core as mx
import mlx.nn as nn


class MLPBlock(nn.Module):
    def __init__(
        self,
        embedding_dim: int,
        mlp_dim: int,
        act: Type[nn.Module] = nn.GELU,
    ) -> None:
        super().__init__()
        self.lin1 = nn.Linear(embedding_dim, mlp_dim)
        self.lin2 = nn.Linear(mlp_dim, embedding_dim)
        self.act = act()

    def __call__(self, x: mx.array) -> mx.array:
        return self.lin2(self.act(self.lin1(x)))


class LayerNorm2d(nn.Module):
    def __init__(self, num_channels: int, eps: float = 1e-6) -> None:
        super().__init__()
        self.weight = mx.ones(num_channels)
        self.bias = mx.zeros(num_channels)
        self.eps = eps

    def __call__(self, x: mx.array) -> mx.array:
        u = x.mean(3, keepdims=True)
        s = ((x - u) ** 2).mean(3, keepdims=True)
        x = (x - u) / mx.sqrt(s + self.eps)
        x = self.weight * x + self.bias
        return x

>>>> CODE_OF_CONDUCT.md
# Contributor Covenant Code of Conduct

## Our Pledge

We as members, contributors, and leaders pledge to make participation in our
community a harassment-free experience for everyone, regardless of age, body
size, visible or invisible disability, ethnicity, sex characteristics, gender
identity and expression, level of experience, education, socio-economic status,
nationality, personal appearance, race, caste, color, religion, or sexual
identity and orientation.

We pledge to act and interact in ways that contribute to an open, welcoming,
diverse, inclusive, and healthy community.

## Our Standards

Examples of behavior that contributes to a positive environment for our
community include:

* Demonstrating empathy and kindness toward other people
* Being respectful of differing opinions, viewpoints, and experiences
* Giving and gracefully accepting constructive feedback
* Accepting responsibility and apologizing to those affected by our mistakes,
  and learning from the experience
* Focusing on what is best not just for us as individuals, but for the overall
  community

Examples of unacceptable behavior include:

* The use of sexualized language or imagery, and sexual attention or advances of
  any kind
* Trolling, insulting or derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or email address,
  without their explicit permission
* Other conduct which could reasonably be considered inappropriate in a
  professional setting

## Enforcement Responsibilities

Community leaders are responsible for clarifying and enforcing our standards of
acceptable behavior and will take appropriate and fair corrective action in
response to any behavior that they deem inappropriate, threatening, offensive,
or harmful.

Community leaders have the right and responsibility to remove, edit, or reject
comments, commits, code, wiki edits, issues, and other contributions that are
not aligned to this Code of Conduct, and will communicate reasons for moderation
decisions when appropriate.

## Scope

This Code of Conduct applies within all community spaces, and also applies when
an individual is officially representing the community in public spaces.
Examples of representing our community include using an official e-mail address,
posting via an official social media account, or acting as an appointed
representative at an online or offline event.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported to the community leaders responsible for enforcement at
[opensource-conduct@group.apple.com](mailto:opensource-conduct@group.apple.com).
All complaints will be reviewed and investigated promptly and fairly.

All community leaders are obligated to respect the privacy and security of the
reporter of any incident.

## Enforcement Guidelines

Community leaders will follow these Community Impact Guidelines in determining
the consequences for any action they deem in violation of this Code of Conduct:

### 1. Correction

**Community Impact**: Use of inappropriate language or other behavior deemed
unprofessional or unwelcome in the community.

**Consequence**: A private, written warning from community leaders, providing
clarity around the nature of the violation and an explanation of why the
behavior was inappropriate. A public apology may be requested.

### 2. Warning

**Community Impact**: A violation through a single incident or series of
actions.

**Consequence**: A warning with consequences for continued behavior. No
interaction with the people involved, including unsolicited interaction with
those enforcing the Code of Conduct, for a specified period of time. This
includes avoiding interactions in community spaces as well as external channels
like social media. Violating these terms may lead to a temporary or permanent
ban.

### 3. Temporary Ban

**Community Impact**: A serious violation of community standards, including
sustained inappropriate behavior.

**Consequence**: A temporary ban from any sort of interaction or public
communication with the community for a specified period of time. No public or
private interaction with the people involved, including unsolicited interaction
with those enforcing the Code of Conduct, is allowed during this period.
Violating these terms may lead to a permanent ban.

### 4. Permanent Ban

**Community Impact**: Demonstrating a pattern of violation of community
standards, including sustained inappropriate behavior, harassment of an
individual, or aggression toward or disparagement of classes of individuals.

**Consequence**: A permanent ban from any sort of public interaction within the
community.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage],
version 2.1, available at
[https://www.contributor-covenant.org/version/2/1/code_of_conduct.html][v2.1].

Community Impact Guidelines were inspired by
[Mozilla's code of conduct enforcement ladder][Mozilla CoC].

For answers to common questions about this code of conduct, see the FAQ at
[https://www.contributor-covenant.org/faq][FAQ]. Translations are available at
[https://www.contributor-covenant.org/translations][translations].

[homepage]: https://www.contributor-covenant.org
[v2.1]: https://www.contributor-covenant.org/version/2/1/code_of_conduct.html
[Mozilla CoC]: https://github.com/mozilla/diversity
[FAQ]: https://www.contributor-covenant.org/faq
[translations]: https://www.contributor-covenant.org/translations

>>>> segment_anything/segment_anything/utils/transforms.py
from copy import deepcopy
from typing import Tuple

import mlx.core as mx
import mlx.nn as nn
import numpy as np
from PIL import Image


class ResizeLongestSide:
    """
    Resizes images to the longest side 'target_length', as well as provides
    methods for resizing coordinates and boxes. Provides methods for
    transforming both numpy array and batched mlx tensors.
    """

    def __init__(self, target_length: int) -> None:
        self.target_length = target_length

    def apply_image(self, image: np.ndarray) -> np.ndarray:
        """
        Expects a numpy array with shape HxWxC in uint8 format.
        """
        target_size = self.get_preprocess_shape(
            image.shape[0], image.shape[1], self.target_length
        )
        return np.array(
            Image.fromarray(image).resize(
                target_size[::-1], resample=Image.Resampling.BILINEAR
            )
        )

    def apply_coords(
        self, coords: mx.array, original_size: Tuple[int, ...]
    ) -> mx.array:
        """
        Expects a mlx tensor with length 2 in the last dimension. Requires the
        original image size in (H, W) format.
        """
        old_h, old_w = original_size
        new_h, new_w = self.get_preprocess_shape(
            original_size[0], original_size[1], self.target_length
        )
        return coords * mx.array([new_w / old_w, new_h / old_h])

    def apply_boxes(self, boxes: mx.array, original_size: Tuple[int, ...]) -> mx.array:
        """
        Expects a mlx tensor with shape ...x4. Requires the original image
        size in (H, W) format.
        """
        boxes = self.apply_coords(boxes.reshape(-1, 2, 2), original_size)
        return boxes.reshape(-1, 4)

    @staticmethod
    def get_preprocess_shape(
        oldh: int, oldw: int, long_side_length: int
    ) -> Tuple[int, int]:
        """
        Compute the output size given input size and target long side length.
        """
        scale = long_side_length * 1.0 / max(oldh, oldw)
        newh, neww = oldh * scale, oldw * scale
        neww = int(neww + 0.5)
        newh = int(newh + 0.5)
        return (newh, neww)

>>>> segment_anything/segment_anything/utils/amg.py
import math
from copy import deepcopy
from itertools import product
from typing import Any, Dict, Generator, ItemsView, List, Tuple

import mlx.core as mx
import numpy as np


class MaskData:
    """
    A structure for storing masks and their related data in batched format.
    Implements basic filtering and concatenation.
    """

    def __init__(self, **kwargs) -> None:
        for v in kwargs.values():
            assert isinstance(
                v, (list, np.ndarray, mx.array)
            ), "MaskData only supports list, numpy arrays, and mlx arrays."
        self._stats = dict(**kwargs)

    def __setitem__(self, key: str, item: Any) -> None:
        assert isinstance(
            item, (list, np.ndarray, mx.array)
        ), "MaskData only supports list, numpy arrays, and mlx arrays."
        self._stats[key] = item

    def __delitem__(self, key: str) -> None:
        del self._stats[key]

    def __getitem__(self, key: str) -> Any:
        return self._stats[key]

    def items(self) -> ItemsView[str, Any]:
        return self._stats.items()

    def filter(self, keep: mx.array) -> None:
        if keep.dtype == mx.bool_:
            keep = mx.array(np.where(keep)[0])
        for k, v in self._stats.items():
            if v is None:
                self._stats[k] = None
            elif isinstance(v, mx.array):
                self._stats[k] = v[keep]
            elif isinstance(v, np.ndarray):
                self._stats[k] = v[keep]
            elif isinstance(v, list):
                self._stats[k] = [v[i] for i in keep.tolist()]
            else:
                raise TypeError(f"MaskData key {k} has an unsupported type {type(v)}.")

    def cat(self, new_stats: "MaskData") -> None:
        for k, v in new_stats.items():
            if k not in self._stats or self._stats[k] is None:
                self._stats[k] = deepcopy(v)
            elif isinstance(v, mx.array):
                self._stats[k] = mx.concatenate([self._stats[k], v], axis=0)
            elif isinstance(v, np.ndarray):
                self._stats[k] = np.concatenate([self._stats[k], v], axis=0)
            elif isinstance(v, list):
                self._stats[k] = self._stats[k] + deepcopy(v)
            else:
                raise TypeError(f"MaskData key {k} has an unsupported type {type(v)}.")

    def to_numpy(self) -> None:
        for k, v in self._stats.items():
            if isinstance(v, mx.array):
                self._stats[k] = np.array(v)


def is_box_near_crop_edge(
    boxes: mx.array, crop_box: List[int], orig_box: List[int], atol: float = 20.0
) -> mx.array:
    """Filter masks at the edge of a crop, but not at the edge of the original image."""
    crop_box_mlx = mx.array(crop_box, dtype=mx.float32)
    orig_box_mlx = mx.array(orig_box, dtype=mx.float32)
    boxes = uncrop_boxes_xyxy(boxes, crop_box).astype(mx.float32)
    near_crop_edge = mx.isclose(boxes, crop_box_mlx[None, :], atol=atol, rtol=0)
    near_image_edge = mx.isclose(boxes, orig_box_mlx[None, :], atol=atol, rtol=0)
    near_crop_edge = mx.logical_and(near_crop_edge, ~near_image_edge)
    return mx.any(near_crop_edge, axis=1)


def box_xyxy_to_xywh(box_xyxy: mx.array) -> mx.array:
    box_xywh = deepcopy(box_xyxy)
    box_xywh[2] = box_xywh[2] - box_xywh[0]
    box_xywh[3] = box_xywh[3] - box_xywh[1]
    return box_xywh


def batch_iterator(batch_size: int, *args) -> Generator[List[Any], None, None]:
    assert len(args) > 0 and all(
        len(a) == len(args[0]) for a in args
    ), "Batched iteration must have inputs of all the same size."
    n_batches = len(args[0]) // batch_size + int(len(args[0]) % batch_size != 0)
    for b in range(n_batches):
        yield [arg[b * batch_size : (b + 1) * batch_size] for arg in args]


def mask_to_rle_mlx(tensor: mx.array) -> List[Dict[str, Any]]:
    """
    Encodes masks to an uncompressed RLE, in the format expected by
    pycoco tools.
    """
    # Put in fortran order and flatten h,w
    b, h, w = tensor.shape
    tensor = mx.transpose(tensor, axes=(0, 2, 1)).flatten(1)

    # Compute change indices
    diff = mx.bitwise_xor(tensor[:, 1:], tensor[:, :-1])
    # TODO: fix this with mlx
    change_indices = np.stack(np.array(diff).nonzero(), axis=1)

    # Encode run length
    out = []
    for i in range(b):
        cur_idxs = change_indices[change_indices[:, 0] == i, 1]
        cur_idxs = mx.array(cur_idxs)
        cur_idxs = mx.concatenate(
            [
                mx.array([0], dtype=cur_idxs.dtype),
                cur_idxs + 1,
                mx.array([h * w], dtype=cur_idxs.dtype),
            ]
        )
        btw_idxs = cur_idxs[1:] - cur_idxs[:-1]
        counts = [] if tensor[i, 0] == 0 else [0]
        counts.extend(btw_idxs.tolist())
        out.append({"size": [h, w], "counts": counts})
    return out


def rle_to_mask(rle: Dict[str, Any]) -> np.ndarray:
    """Compute a binary mask from an uncompressed RLE."""
    h, w = rle["size"]
    mask = np.empty(h * w, dtype=bool)
    idx = 0
    parity = False
    for count in rle["counts"]:
        mask[idx : idx + count] = parity
        idx += count
        parity ^= True
    mask = mask.reshape(w, h)
    return mask.transpose()  # Put in C order


def area_from_rle(rle: Dict[str, Any]) -> int:
    return sum(rle["counts"][1::2])


def calculate_stability_score(
    masks: mx.array, mask_threshold: float, threshold_offset: float
) -> mx.array:
    """
    Computes the stability score for a batch of masks. The stability
    score is the IoU between the binary masks obtained by thresholding
    the predicted mask logits at high and low values.
    """
    # One mask is always contained inside the other.
    # Save memory by preventing unnecessary cast to mx.int64

    # COMMENT OUT DTYPE CASTING FOR COREML
    intersections = (
        (masks > (mask_threshold + threshold_offset))
        .astype(mx.int16)
        .sum(-1)
        .astype(mx.int32)
        .sum(-1)
    )
    unions = (
        (masks > (mask_threshold - threshold_offset))
        .astype(mx.int16)
        .sum(-1)
        .astype(mx.int32)
        .sum(-1)
    )
    return intersections / unions


def build_point_grid(n_per_side: int) -> np.ndarray:
    """Generates a 2D grid of points evenly spaced in [0,1]x[0,1]."""
    offset = 1 / (2 * n_per_side)
    points_one_side = np.linspace(offset, 1 - offset, n_per_side)
    points_x = np.tile(points_one_side[None, :], (n_per_side, 1))
    points_y = np.tile(points_one_side[:, None], (1, n_per_side))
    points = np.stack([points_x, points_y], axis=-1).reshape(-1, 2)
    return points


def build_all_layer_point_grids(
    n_per_side: int, n_layers: int, scale_per_layer: int
) -> List[mx.array]:
    """Generates point grids for all crop layers."""
    points_by_layer = []
    for i in range(n_layers + 1):
        n_points = int(n_per_side / (scale_per_layer**i))
        points_by_layer.append(mx.array(build_point_grid(n_points)))
    return points_by_layer


def generate_crop_boxes(
    im_size: Tuple[int, ...], n_layers: int, overlap_ratio: float
) -> Tuple[List[List[int]], List[int]]:
    """
    Generates a list of crop boxes of different sizes. Each layer
    has (2**i)**2 boxes for the ith layer.
    """
    crop_boxes, layer_idxs = [], []
    im_h, im_w = im_size
    short_side = min(im_h, im_w)

    # Original image
    crop_boxes.append([0, 0, im_w, im_h])
    layer_idxs.append(0)

    def crop_len(orig_len, n_crops, overlap):
        return int(math.ceil((overlap * (n_crops - 1) + orig_len) / n_crops))

    for i_layer in range(n_layers):
        n_crops_per_side = 2 ** (i_layer + 1)
        overlap = int(overlap_ratio * short_side * (2 / n_crops_per_side))

        crop_w = crop_len(im_w, n_crops_per_side, overlap)
        crop_h = crop_len(im_h, n_crops_per_side, overlap)

        crop_box_x0 = [int((crop_w - overlap) * i) for i in range(n_crops_per_side)]
        crop_box_y0 = [int((crop_h - overlap) * i) for i in range(n_crops_per_side)]

        # Crops in XYWH format
        for x0, y0 in product(crop_box_x0, crop_box_y0):
            box = [x0, y0, min(x0 + crop_w, im_w), min(y0 + crop_h, im_h)]
            crop_boxes.append(box)
            layer_idxs.append(i_layer + 1)

    return crop_boxes, layer_idxs


def uncrop_boxes_xyxy(boxes: mx.array, crop_box: List[int]) -> mx.array:
    x0, y0, _, _ = crop_box
    offset = mx.array([[x0, y0, x0, y0]])
    # Check if boxes has a channel dimension
    if len(boxes.shape) == 3:
        offset = offset.unsqueeze(1)
    return boxes + offset


def uncrop_points(points: mx.array, crop_box: List[int]) -> mx.array:
    x0, y0, _, _ = crop_box
    offset = mx.array([[x0, y0]])
    # Check if points has a channel dimension
    if len(points.shape) == 3:
        offset = offset.unsqueeze(1)
    return points + offset


def uncrop_masks(
    masks: mx.array, crop_box: List[int], orig_h: int, orig_w: int
) -> mx.array:
    x0, y0, x1, y1 = crop_box
    if x0 == 0 and y0 == 0 and x1 == orig_w and y1 == orig_h:
        return masks
    # Coordinate transform masks
    pad_x, pad_y = orig_w - (x1 - x0), orig_h - (y1 - y0)
    pad = [(0, 0), (y0, pad_y - y0), (x0, pad_x - x0)]
    return mx.pad(masks, pad, 0)


def remove_small_regions(
    mask: np.ndarray, area_thresh: float, mode: str
) -> Tuple[np.ndarray, bool]:
    """
    Removes small disconnected regions and holes in a mask. Returns the
    mask and an indicator of if the mask has been modified.
    """
    import cv2  # type: ignore

    assert mode in ["holes", "islands"]
    correct_holes = mode == "holes"
    working_mask = (correct_holes ^ mask).astype(np.uint8)
    n_labels, regions, stats, _ = cv2.connectedComponentsWithStats(working_mask, 8)
    sizes = stats[:, -1][1:]  # Row 0 is background label
    small_regions = [i + 1 for i, s in enumerate(sizes) if s < area_thresh]
    if len(small_regions) == 0:
        return mask, False
    fill_labels = [0] + small_regions
    if not correct_holes:
        fill_labels = [i for i in range(n_labels) if i not in fill_labels]
        # If every region is below threshold, keep largest
        if len(fill_labels) == 0:
            fill_labels = [int(np.argmax(sizes)) + 1]
    mask = np.isin(regions, fill_labels)
    return mask, True


def coco_encode_rle(uncompressed_rle: Dict[str, Any]) -> Dict[str, Any]:
    from pycocotools import mask as mask_utils  # type: ignore

    h, w = uncompressed_rle["size"]
    rle = mask_utils.frPyObjects(uncompressed_rle, h, w)
    rle["counts"] = rle["counts"].decode("utf-8")  # Necessary to serialize with json
    return rle


def batched_mask_to_box(masks: mx.array) -> mx.array:
    """
    Calculates boxes in XYXY format around masks. Return [0,0,0,0] for
    an empty mask. For input shape C1xC2x...xHxW, the output shape is C1xC2x...x4.
    """
    # mx.max below raises an error on empty inputs, just skip in this case
    if np.prod(masks.shape) == 0:
        return mx.zeros(*masks.shape[:-2], 4)

    # Normalize shape to CxHxW
    shape = masks.shape
    h, w = shape[-2:]
    if len(shape) > 2:
        masks = masks.flatten(0, -3)
    else:
        masks = masks.unsqueeze(0)

    # Get top and bottom edges
    in_height = mx.max(masks, axis=-1)
    in_height_coords = in_height * mx.arange(h)[None, :]
    bottom_edges = mx.max(in_height_coords, axis=-1)
    in_height_coords = in_height_coords + h * (~in_height)
    top_edges = mx.min(in_height_coords, axis=-1)

    # Get left and right edges
    in_width = mx.max(masks, axis=-2)
    in_width_coords = in_width * mx.arange(w)[None, :]
    right_edges = mx.max(in_width_coords, axis=-1)
    in_width_coords = in_width_coords + w * (~in_width)
    left_edges = mx.min(in_width_coords, axis=-1)

    # If the mask is empty the right edge will be to the left of the left edge.
    # Replace these boxes with [0, 0, 0, 0]
    empty_filter = (right_edges < left_edges) | (bottom_edges < top_edges)
    out = mx.stack([left_edges, top_edges, right_edges, bottom_edges], axis=-1)
    out = out * (~empty_filter)[..., None]

    # Return to original shape
    if len(shape) > 2:
        out = out.reshape(*shape[:-2], 4)
    else:
        out = out[0]

    return out

>>>> segment_anything/segment_anything/automatic_mask_generator.py
from typing import Any, Dict, List, Optional, Tuple

import mlx.core as mx
import numpy as np

from .predictor import SamPredictor
from .sam import Sam
from .utils.amg import (
    MaskData,
    area_from_rle,
    batch_iterator,
    batched_mask_to_box,
    box_xyxy_to_xywh,
    build_all_layer_point_grids,
    calculate_stability_score,
    coco_encode_rle,
    generate_crop_boxes,
    is_box_near_crop_edge,
    mask_to_rle_mlx,
    remove_small_regions,
    rle_to_mask,
    uncrop_boxes_xyxy,
    uncrop_masks,
    uncrop_points,
)


class SamAutomaticMaskGenerator:
    def __init__(
        self,
        model: Sam,
        points_per_side: Optional[int] = 32,
        points_per_batch: int = 64,
        pred_iou_thresh: float = 0.88,
        stability_score_thresh: float = 0.95,
        stability_score_offset: float = 1.0,
        box_nms_thresh: float = 0.7,
        crop_n_layers: int = 0,
        crop_nms_thresh: float = 0.7,
        crop_overlap_ratio: float = 512 / 1500,
        crop_n_points_downscale_factor: int = 1,
        point_grids: Optional[List[mx.array]] = None,
        min_mask_region_area: int = 0,
        output_mode: str = "binary_mask",
    ) -> None:
        """
        Using a SAM model, generates masks for the entire image.
        Generates a grid of point prompts over the image, then filters
        low quality and duplicate masks. The default settings are chosen
        for SAM with a ViT-H backbone.

        Arguments:
            model (Sam): The SAM model to use for mask prediction.
            points_per_side (int or None): The number of points to be sampled
                along one side of the image. The total number of points is
                points_per_side**2. If None, 'point_grids' must provide explicit
                point sampling.
            points_per_batch (int): Sets the number of points run simultaneously
                by the model. Higher numbers may be faster but use more GPU memory.
            pred_iou_thresh (float): A filtering threshold in [0,1], using the
                model's predicted mask quality.
            stability_score_thresh (float): A filtering threshold in [0,1], using
                the stability of the mask under changes to the cutoff used to binarize
                the model's mask predictions.
            stability_score_offset (float): The amount to shift the cutoff when
                calculated the stability score.
            box_nms_thresh (float): The box IoU cutoff used by non-maximal
                suppression to filter duplicate masks.
            crop_n_layers (int): If >0, mask prediction will be run again on
                crops of the image. Sets the number of layers to run, where each
                layer has 2**i_layer number of image crops.
            crop_nms_thresh (float): The box IoU cutoff used by non-maximal
                suppression to filter duplicate masks between different crops.
            crop_overlap_ratio (float): Sets the degree to which crops overlap.
                In the first crop layer, crops will overlap by this fraction of
                the image length. Later layers with more crops scale down this overlap.
            crop_n_points_downscale_factor (int): The number of points-per-side
                sampled in layer n is scaled down by crop_n_points_downscale_factor**n.
            point_grids (list(mx.array) or None): A list over explicit grids
                of points used for sampling, normalized to [0,1]. The nth grid in the
                list is used in the nth crop layer. Exclusive with points_per_side.
            min_mask_region_area (int): If >0, postprocessing will be applied
                to remove disconnected regions and holes in masks with area smaller
                than min_mask_region_area. Requires opencv.
            output_mode (str): The form masks are returned in. Can be 'binary_mask',
                'uncompressed_rle', or 'coco_rle'. 'coco_rle' requires pycocotools.
                For large resolutions, 'binary_mask' may consume large amounts of
                memory.
        """

        assert (points_per_side is None) != (
            point_grids is None
        ), "Exactly one of points_per_side or point_grid must be provided."
        if points_per_side is not None:
            self.point_grids = build_all_layer_point_grids(
                points_per_side,
                crop_n_layers,
                crop_n_points_downscale_factor,
            )
        elif point_grids is not None:
            self.point_grids = point_grids
        else:
            raise ValueError("Can't have both points_per_side and point_grid be None.")

        assert output_mode in [
            "binary_mask",
            "uncompressed_rle",
            "coco_rle",
        ], f"Unknown output_mode {output_mode}."
        if output_mode == "coco_rle":
            from pycocotools import mask as mask_utils  # type: ignore # noqa: F401

        if min_mask_region_area > 0:
            import cv2  # type: ignore # noqa: F401

        self.predictor = SamPredictor(model)
        self.points_per_batch = points_per_batch
        self.pred_iou_thresh = pred_iou_thresh
        self.stability_score_thresh = stability_score_thresh
        self.stability_score_offset = stability_score_offset
        self.box_nms_thresh = box_nms_thresh
        self.crop_n_layers = crop_n_layers
        self.crop_nms_thresh = crop_nms_thresh
        self.crop_overlap_ratio = crop_overlap_ratio
        self.crop_n_points_downscale_factor = crop_n_points_downscale_factor
        self.min_mask_region_area = min_mask_region_area
        self.output_mode = output_mode

    def generate(self, image: np.ndarray) -> List[Dict[str, Any]]:
        """
        Generates masks for the given image.

        Arguments:
            image (np.ndarray): The image to generate masks for, in HWC uint8 format.

        Returns:
            list(dict(str, any)): A list over records for masks. Each record is
                a dict containing the following keys:
                segmentation (dict(str, any) or np.ndarray): The mask. If
                    output_mode='binary_mask', is an array of shape HW. Otherwise,
                    is a dictionary containing the RLE.
                bbox (list(float)): The box around the mask, in XYWH format.
                area (int): The area in pixels of the mask.
                predicted_iou (float): The model's own prediction of the mask's
                    quality. This is filtered by the pred_iou_thresh parameter.
                point_coords (list(list(float))): The point coordinates input
                    to the model to generate this mask.
                stability_score (float): A measure of the mask's quality. This
                    is filtered on using the stability_score_thresh parameter.
                crop_box (list(float)): The crop of the image used to generate
                    the mask, given in XYWH format.
        """

        # Generate masks
        mask_data = self._generate_masks(image)

        # Filter small disconnected regions and holes in masks
        if self.min_mask_region_area > 0:
            mask_data = self.postprocess_small_regions(
                mask_data,
                self.min_mask_region_area,
                max(self.box_nms_thresh, self.crop_nms_thresh),
            )

        # Encode masks
        if self.output_mode == "coco_rle":
            mask_data["segmentations"] = [
                coco_encode_rle(rle) for rle in mask_data["rles"]
            ]
        elif self.output_mode == "binary_mask":
            mask_data["segmentations"] = [rle_to_mask(rle) for rle in mask_data["rles"]]
        else:
            mask_data["segmentations"] = mask_data["rles"]

        # Write mask records
        curr_anns = []
        for idx in range(len(mask_data["segmentations"])):
            ann = {
                "segmentation": mask_data["segmentations"][idx],
                "area": area_from_rle(mask_data["rles"][idx]),
                "bbox": box_xyxy_to_xywh(mask_data["boxes"][idx]).tolist(),
                "predicted_iou": mask_data["iou_preds"][idx].item(),
                "point_coords": [mask_data["points"][idx].tolist()],
                "stability_score": mask_data["stability_score"][idx].item(),
                "crop_box": box_xyxy_to_xywh(mask_data["crop_boxes"][idx]).tolist(),
            }
            curr_anns.append(ann)

        return curr_anns

    def _generate_masks(self, image: np.ndarray) -> MaskData:
        orig_size = image.shape[:2]
        crop_boxes, layer_idxs = generate_crop_boxes(
            orig_size, self.crop_n_layers, self.crop_overlap_ratio
        )

        # Iterate over image crops
        data = MaskData()
        for crop_box, layer_idx in zip(crop_boxes, layer_idxs):
            crop_data = self._process_crop(image, crop_box, layer_idx, orig_size)
            data.cat(crop_data)

        # Remove duplicate masks between crops
        if len(crop_boxes) > 1:
            # Prefer masks from smaller crops
            scores = 1 / box_area(data["crop_boxes"])
            keep_by_nms = non_max_supression(
                data["boxes"].astype(mx.float32),
                scores,
                iou_threshold=self.crop_nms_thresh,
            )
            data.filter(keep_by_nms)

        data.to_numpy()
        return data

    def _process_crop(
        self,
        image: np.ndarray,
        crop_box: List[int],
        crop_layer_idx: int,
        orig_size: Tuple[int, ...],
    ) -> MaskData:
        # Crop the image and calculate embeddings
        x0, y0, x1, y1 = crop_box
        cropped_im = image[y0:y1, x0:x1, :]
        cropped_im_size = cropped_im.shape[:2]
        self.predictor.set_image(cropped_im)

        # Get points for this crop
        points_scale = mx.array(cropped_im_size[::-1])[None]
        points_for_image = self.point_grids[crop_layer_idx] * points_scale

        # Generate masks for this crop in batches
        data = MaskData()
        for (points,) in batch_iterator(self.points_per_batch, points_for_image):
            batch_data = self._process_batch(
                points, cropped_im_size, crop_box, orig_size
            )
            data.cat(batch_data)
            del batch_data
        self.predictor.reset_image()

        # Remove duplicates within this crop.
        keep_by_nms = non_max_supression(
            data["boxes"].astype(mx.float32),
            data["iou_preds"],
            iou_threshold=self.box_nms_thresh,
        )
        data.filter(keep_by_nms)

        # Return to the original image frame
        data["boxes"] = uncrop_boxes_xyxy(data["boxes"], crop_box)
        data["points"] = uncrop_points(data["points"], crop_box)
        data["crop_boxes"] = mx.array([crop_box for _ in range(len(data["rles"]))])
        return data

    def _process_batch(
        self,
        points: np.ndarray,
        im_size: Tuple[int, ...],
        crop_box: List[int],
        orig_size: Tuple[int, ...],
    ) -> MaskData:
        orig_h, orig_w = orig_size

        masks, iou_preds, _ = self.predictor.predict(
            points[:, None, :],
            mx.ones((points.shape[0], 1), dtype=mx.int64),
            multimask_output=True,
            return_logits=True,
        )
        masks = masks.transpose(0, 3, 1, 2)
        # Serialize predictions and store in MaskData
        data = MaskData(
            masks=masks.flatten(0, 1),
            iou_preds=iou_preds.flatten(0, 1),
            points=mx.repeat(points, masks.shape[1], axis=0),
        )
        del masks

        # Filter by predicted IoU
        if self.pred_iou_thresh > 0.0:
            keep_mask = data["iou_preds"] > self.pred_iou_thresh
            data.filter(keep_mask)

        # Calculate stability score
        data["stability_score"] = calculate_stability_score(
            data["masks"],
            self.predictor.model.mask_threshold,
            self.stability_score_offset,
        )
        if self.stability_score_thresh > 0.0:
            keep_mask = data["stability_score"] >= self.stability_score_thresh
            data.filter(keep_mask)

        # Threshold masks and calculate boxes
        data["masks"] = data["masks"] > self.predictor.model.mask_threshold
        data["boxes"] = batched_mask_to_box(data["masks"])

        # Filter boxes that touch crop boundaries
        keep_mask = ~is_box_near_crop_edge(
            data["boxes"], crop_box, [0, 0, orig_w, orig_h]
        )
        if not mx.all(keep_mask):
            data.filter(keep_mask)

        # Compress to RLE
        data["masks"] = uncrop_masks(data["masks"], crop_box, orig_h, orig_w)
        data["rles"] = mask_to_rle_mlx(data["masks"])
        del data["masks"]

        return data

    @staticmethod
    def postprocess_small_regions(
        mask_data: MaskData, min_area: int, nms_thresh: float
    ) -> MaskData:
        """
        Removes small disconnected regions and holes in masks, then reruns
        box NMS to remove any new duplicates.

        Edits mask_data in place.

        Requires open-cv as a dependency.
        """
        if len(mask_data["rles"]) == 0:
            return mask_data

        # Filter small disconnected regions and holes
        new_masks = []
        scores = []
        for rle in mask_data["rles"]:
            mask = rle_to_mask(rle)

            mask, changed = remove_small_regions(mask, min_area, mode="holes")
            unchanged = not changed
            mask, changed = remove_small_regions(mask, min_area, mode="islands")
            unchanged = unchanged and not changed

            new_masks.append(mx.array(mask)[None])
            # Give score=0 to changed masks and score=1 to unchanged masks
            # so NMS will prefer ones that didn't need postprocessing
            scores.append(float(unchanged))
        scores = mx.array(scores)

        # Recalculate boxes and remove any new duplicates
        masks = mx.concatenate(new_masks, axis=0)
        boxes = batched_mask_to_box(masks)
        keep_by_nms = non_max_supression(
            boxes.astype(mx.float32),
            scores,
            iou_threshold=nms_thresh,
        )
        # Only recalculate RLEs for masks that have changed
        for i_mask, keep in enumerate(keep_by_nms):
            if not keep:
                continue
            if scores[i_mask] == 0.0:
                mask_mlx = masks[i_mask][None]
                mask_data["rles"][i_mask] = mask_to_rle_mlx(mask_mlx)[0]
                mask_data["boxes"][i_mask] = boxes[i_mask]  # update res directly
        mask_data.filter(keep_by_nms)

        return mask_data


def box_area(boxes: mx.array) -> mx.array:
    """
    Computes the area of a set of bounding boxes, which are specified by their
    (x1, y1, x2, y2) coordinates.

    Args:
        boxes (mx.array[N, 4]): boxes for which the area will be computed. They
            are expected to be in (x1, y1, x2, y2) format with
            ``0 <= x1 < x2`` and ``0 <= y1 < y2``.

    Returns:
        mx.array[N]: the area for each box
    """
    return (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])


def batched_iou(boxes_a: mx.array, boxes_b: mx.array) -> mx.array:
    """Compute IoU for batched boxes.

    Args:
        boxes_a (mx.array): [..., [x1, y1, x2, y2]] sized Mx4
        boxes_b (mx.array): [..., [x1, y1, x2, y2]] sized Nx4

    Returns:
        mx.array: MxN
    """

    area_a = box_area(boxes_a)  # M
    area_b = box_area(boxes_b)  # N

    top_left = mx.maximum(boxes_a[:, None, :2], boxes_b[:, :2])
    bottom_right = mx.minimum(boxes_a[:, None, 2:], boxes_b[:, 2:])

    area_inter = mx.prod(mx.clip(bottom_right - top_left, a_min=0, a_max=None), 2)

    return area_inter / (area_a[:, None] + area_b - area_inter)


def non_max_supression(
    boxes: mx.array, scores: mx.array, iou_threshold: float = 0.5
) -> mx.array:
    sort_index = mx.argsort(-scores)
    boxes = boxes[sort_index]

    n_boxes = boxes.shape[0]
    ious = batched_iou(boxes, boxes)
    ious -= mx.eye(n_boxes)

    ious = np.array(ious)
    keep = np.ones(n_boxes, dtype=np.bool_)
    for i, iou in enumerate(ious):
        if not keep[i]:
            continue

        condition = iou <= iou_threshold
        keep = keep & condition

    return sort_index[mx.array(np.where(keep)[0])]

>>>> segment_anything/segment_anything/predictor.py
from typing import Optional, Tuple

import mlx.core as mx
import numpy as np

from .sam import Sam
from .utils.transforms import ResizeLongestSide


class SamPredictor:
    def __init__(
        self,
        sam_model: Sam,
    ) -> None:
        """
        Uses SAM to calculate the image embedding for an image, and then
        allow repeated, efficient mask prediction given prompts.

        Args:
            sam_model (Sam): The model to use for mask prediction.
        """
        super().__init__()
        self.model = sam_model
        self.transform = ResizeLongestSide(sam_model.vision_encoder.img_size)
        self.reset_image()

    def set_image(
        self,
        image: np.ndarray,
        image_format: str = "RGB",
    ) -> None:
        """
        Calculates the image embeddings for the provided image, allowing
        masks to be predicted with the 'predict' method.

        Args:
            image (np.ndarray): The image for calculating masks. Expects an
                image in HWC uint8 format, with pixel values in [0, 255].
            image_format (str): The color format of the image, in ['RGB', 'BGR'].
        """
        self.reset_image()
        assert image_format in [
            "RGB",
            "BGR",
        ], f"image_format must be in ['RGB', 'BGR'], is {image_format}."
        if image_format != self.model.image_format:
            image = image[..., ::-1]

        # Transform the image to the form expected by the model
        input_image = self.transform.apply_image(image)
        input_image = mx.array(input_image)[None, :, :, :]

        self.original_size = image.shape[:2]
        self.input_size = input_image.shape[1:3]
        input_image = self.model.preprocess(input_image)
        self.features = self.model.vision_encoder(input_image)
        self.is_image_set = True

    def predict(
        self,
        point_coords: Optional[mx.array],
        point_labels: Optional[mx.array],
        box: Optional[mx.array] = None,
        mask_input: Optional[mx.array] = None,
        multimask_output: bool = True,
        return_logits: bool = False,
    ) -> Tuple[mx.array, mx.array, mx.array]:
        """
        Predict masks for the given input prompts, using the currently set image.
        Input prompts are batched mlx tensors and are expected to already be
        transformed to the input frame using ResizeLongestSide.

        Args:
            point_coords (mx.array or None): A BxNx2 array of point prompts to the
                model. Each point is in (X,Y) in pixels.
            point_labels (mx.array or None): A BxN array of labels for the
                point prompts. 1 indicates a foreground point and 0 indicates a
                background point.
            box (mx.array or None): A size 4 array giving a box prompt to the
                model, in XYXY format.
            mask_input (mx.array): A low resolution mask input to the model, typically
                coming from a previous prediction iteration. Has form BxHxWx1, where
                for SAM, H=W=256. Masks returned by a previous iteration of the
                predict method do not need further transformation.
            multimask_output (bool): If true, the model will return three masks.
                For ambiguous input prompts (such as a single click), this will often
                produce better masks than a single prediction. If only a single
                mask is needed, the model's predicted quality score can be used
                to select the best mask. For non-ambiguous prompts, such as multiple
                input prompts, multimask_output=False can give better results.
            return_logits (bool): If true, returns un-thresholded masks logits
                instead of a binary mask.

        Returns:
            (mx.array): The output masks in BxHxWxC format, where C is the
                number of masks, and (H, W) is the original image size.
            (mx.array): An array of shape BxC containing the model's
                predictions for the quality of each mask.
            (mx.array): An array of shape BxHxWxC, where C is the number
                of masks and H=W=256. These low res logits can be passed to
                a subsequent iteration as mask input.
        """
        if not self.is_image_set:
            raise RuntimeError(
                "An image must be set with .set_image(...) before mask prediction."
            )

        # Transform input prompts
        points = None
        if point_coords is not None:
            assert (
                point_labels is not None
            ), "point_labels must be supplied if point_coords is supplied."
            point_coords = self.transform.apply_coords(point_coords, self.original_size)
            points = (point_coords, point_labels)
        if box is not None:
            box = self.transform.apply_boxes(box, self.original_size)

        # Embed prompts
        sparse_embeddings, dense_embeddings = self.model.prompt_encoder(
            points=points,
            boxes=box,
            masks=mask_input,
            pe_layer=self.model.shared_image_embedding,
        )

        # Predict masks
        low_res_masks, iou_predictions = self.model.mask_decoder(
            image_embeddings=self.features,
            image_pe=self.model.shared_image_embedding(
                self.model.prompt_encoder.image_embedding_size
            ),
            sparse_prompt_embeddings=sparse_embeddings,
            dense_prompt_embeddings=dense_embeddings,
            multimask_output=multimask_output,
        )

        # Upscale the masks to the original image resolution
        masks = self.model.postprocess_masks(
            low_res_masks, self.input_size, self.original_size
        )

        if not return_logits:
            masks = masks > self.model.mask_threshold

        return masks, iou_predictions, low_res_masks

    def get_image_embedding(self) -> mx.array:
        """
        Returns the image embeddings for the currently set image, with
        shape 1xCxHxW, where C is the embedding dimension and (H,W) are
        the embedding spatial dimension of SAM (typically C=256, H=W=64).
        """
        if not self.is_image_set:
            raise RuntimeError(
                "An image must be set with .set_image(...) to generate an embedding."
            )
        assert (
            self.features is not None
        ), "Features must exist if an image has been set."
        return self.features

    def reset_image(self) -> None:
        """Resets the currently set image."""
        self.is_image_set = False
        self.features = None
        self.orig_h = None
        self.orig_w = None
        self.input_h = None
        self.input_w = None

>>>> segment_anything/README.md
# Segment Anything

An implementation of the Segment Anything Model (SAM) in MLX. See the original
repo by Meta AI for more details.[^1]

## Installation

```bash
pip install -r requirements.txt
```

## Convert

```bash
python convert.py --hf-path facebook/sam-vit-base --mlx-path sam-vit-base
```

The `safetensors` weight file and configs are downloaded from Hugging Face,
converted, and saved in the directory specified by `--mlx-path`.

The model sizes are:

- `facebook/sam-vit-base`
- `facebook/sam-vit-large`
- `facebook/sam-vit-huge`

## Run

See examples `notebooks/predictor_example.ipynb` and
`notebooks/automatic_mask_generator_example.ipynb` to try the Segment Anything
Model with MLX.

You can also generate masks from the command line:

```bash
python main.py --model <path/to/model> --input <image_or_folder> --output <path/to/output>
```

[^1]: The original Segment Anything [GitHub repo](https://github.com/facebookresearch/segment-anything/tree/main).

>>>> segment_anything/convert.py
import argparse
import json
import shutil
from pathlib import Path
from typing import Dict, Union

import mlx.core as mx
from huggingface_hub import snapshot_download


def save_weights(save_path: Union[str, Path], weights: Dict[str, mx.array]) -> None:
    """Save model weights into specified directory."""
    if isinstance(save_path, str):
        save_path = Path(save_path)
    save_path.mkdir(parents=True, exist_ok=True)

    total_size = sum(v.nbytes for v in weights.values())
    index_data = {"metadata": {"total_size": total_size}, "weight_map": {}}

    model_path = save_path / "model.safetensors"
    mx.save_safetensors(str(model_path), weights)

    for weight_name in weights.keys():
        index_data["weight_map"][weight_name] = "model.safetensors"

    index_data["weight_map"] = {
        k: index_data["weight_map"][k] for k in sorted(index_data["weight_map"])
    }

    with open(save_path / "model.safetensors.index.json", "w") as f:
        json.dump(index_data, f, indent=4)


def download(hf_repo):
    return Path(
        snapshot_download(
            repo_id=hf_repo,
            allow_patterns=["*.safetensors", "*.json"],
            resume_download=True,
        )
    )


def convert(model_path):
    weight_file = str(model_path / "model.safetensors")
    weights = mx.load(weight_file)

    mlx_weights = dict()
    for k, v in weights.items():
        if k in {
            "vision_encoder.patch_embed.projection.weight",
            "vision_encoder.neck.conv1.weight",
            "vision_encoder.neck.conv2.weight",
            "prompt_encoder.mask_embed.conv1.weight",
            "prompt_encoder.mask_embed.conv2.weight",
            "prompt_encoder.mask_embed.conv3.weight",
        }:
            v = v.transpose(0, 2, 3, 1)
        if k in {
            "mask_decoder.upscale_conv1.weight",
            "mask_decoder.upscale_conv2.weight",
        }:
            v = v.transpose(1, 2, 3, 0)
        mlx_weights[k] = v
    return mlx_weights


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Convert Meta SAM weights to MLX")
    parser.add_argument(
        "--hf-path",
        default="facebook/sam-vit-base",
        type=str,
        help="Path to the Hugging Face model repo.",
    )
    parser.add_argument(
        "--mlx-path",
        type=str,
        default="sam-vit-base",
        help="Path to save the MLX model.",
    )
    args = parser.parse_args()

    model_path = download(args.hf_path)

    mlx_path = Path(args.mlx_path)
    mlx_path.mkdir(parents=True, exist_ok=True)

    mlx_weights = convert(model_path)
    save_weights(mlx_path, mlx_weights)
    shutil.copy(model_path / "config.json", mlx_path / "config.json")

>>>> stable_diffusion/txt2image.py
# Copyright © 2023 Apple Inc.

import argparse

import mlx.core as mx
import mlx.nn as nn
import numpy as np
from PIL import Image
from tqdm import tqdm

from stable_diffusion import StableDiffusion, StableDiffusionXL

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Generate images from a textual prompt using stable diffusion"
    )
    parser.add_argument("prompt")
    parser.add_argument("--model", choices=["sd", "sdxl"], default="sdxl")
    parser.add_argument("--n_images", type=int, default=4)
    parser.add_argument("--steps", type=int)
    parser.add_argument("--cfg", type=float)
    parser.add_argument("--negative_prompt", default="")
    parser.add_argument("--n_rows", type=int, default=1)
    parser.add_argument("--decoding_batch_size", type=int, default=1)
    parser.add_argument("--no-float16", dest="float16", action="store_false")
    parser.add_argument("--quantize", "-q", action="store_true")
    parser.add_argument("--preload-models", action="store_true")
    parser.add_argument("--output", default="out.png")
    parser.add_argument("--seed", type=int)
    parser.add_argument("--verbose", "-v", action="store_true")
    args = parser.parse_args()

    # Load the models
    if args.model == "sdxl":
        sd = StableDiffusionXL("stabilityai/sdxl-turbo", float16=args.float16)
        if args.quantize:
            nn.quantize(
                sd.text_encoder_1, class_predicate=lambda _, m: isinstance(m, nn.Linear)
            )
            nn.quantize(
                sd.text_encoder_2, class_predicate=lambda _, m: isinstance(m, nn.Linear)
            )
            nn.quantize(sd.unet, group_size=32, bits=8)
        args.cfg = args.cfg or 0.0
        args.steps = args.steps or 2
    else:
        sd = StableDiffusion(
            "stabilityai/stable-diffusion-2-1-base", float16=args.float16
        )
        if args.quantize:
            nn.quantize(
                sd.text_encoder, class_predicate=lambda _, m: isinstance(m, nn.Linear)
            )
            nn.quantize(sd.unet, group_size=32, bits=8)
        args.cfg = args.cfg or 7.5
        args.steps = args.steps or 50

    # Ensure that models are read in memory if needed
    if args.preload_models:
        sd.ensure_models_are_loaded()

    # Generate the latent vectors using diffusion
    latents = sd.generate_latents(
        args.prompt,
        n_images=args.n_images,
        cfg_weight=args.cfg,
        num_steps=args.steps,
        seed=args.seed,
        negative_text=args.negative_prompt,
    )
    for x_t in tqdm(latents, total=args.steps):
        mx.eval(x_t)

    # The following is not necessary but it may help in memory
    # constrained systems by reusing the memory kept by the unet and the text
    # encoders.
    if args.model == "sdxl":
        del sd.text_encoder_1
        del sd.text_encoder_2
    else:
        del sd.text_encoder
    del sd.unet
    del sd.sampler
    peak_mem_unet = mx.metal.get_peak_memory() / 1024**3

    # Decode them into images
    decoded = []
    for i in tqdm(range(0, args.n_images, args.decoding_batch_size)):
        decoded.append(sd.decode(x_t[i : i + args.decoding_batch_size]))
        mx.eval(decoded[-1])
    peak_mem_overall = mx.metal.get_peak_memory() / 1024**3

    # Arrange them on a grid
    x = mx.concatenate(decoded, axis=0)
    x = mx.pad(x, [(0, 0), (8, 8), (8, 8), (0, 0)])
    B, H, W, C = x.shape
    x = x.reshape(args.n_rows, B // args.n_rows, H, W, C).transpose(0, 2, 1, 3, 4)
    x = x.reshape(args.n_rows * H, B // args.n_rows * W, C)
    x = (x * 255).astype(mx.uint8)

    # Save them to disc
    im = Image.fromarray(np.array(x))
    im.save(args.output)

    # Report the peak memory used during generation
    if args.verbose:
        print(f"Peak memory used for the unet: {peak_mem_unet:.3f}GB")
        print(f"Peak memory used overall:      {peak_mem_overall:.3f}GB")

>>>> stable_diffusion/image2image.py
# Copyright © 2023 Apple Inc.

import argparse
import math

import mlx.core as mx
import mlx.nn as nn
import numpy as np
from PIL import Image
from tqdm import tqdm

from stable_diffusion import StableDiffusion, StableDiffusionXL

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Generate images from an image and a textual prompt using stable diffusion"
    )
    parser.add_argument("image")
    parser.add_argument("prompt")
    parser.add_argument("--model", choices=["sd", "sdxl"], default="sdxl")
    parser.add_argument("--strength", type=float, default=0.9)
    parser.add_argument("--n_images", type=int, default=4)
    parser.add_argument("--steps", type=int)
    parser.add_argument("--cfg", type=float)
    parser.add_argument("--negative_prompt", default="")
    parser.add_argument("--n_rows", type=int, default=1)
    parser.add_argument("--decoding_batch_size", type=int, default=1)
    parser.add_argument("--quantize", "-q", action="store_true")
    parser.add_argument("--no-float16", dest="float16", action="store_false")
    parser.add_argument("--preload-models", action="store_true")
    parser.add_argument("--output", default="out.png")
    parser.add_argument("--verbose", "-v", action="store_true")
    parser.add_argument("--seed", type=int)
    args = parser.parse_args()

    # Load the models
    if args.model == "sdxl":
        sd = StableDiffusionXL("stabilityai/sdxl-turbo", float16=args.float16)

        if args.quantize:
            nn.quantize(
                sd.text_encoder_1, class_predicate=lambda _, m: isinstance(m, nn.Linear)
            )
            nn.quantize(
                sd.text_encoder_2, class_predicate=lambda _, m: isinstance(m, nn.Linear)
            )

            nn.quantize(sd.text_encoder_1)
            nn.quantize(sd.text_encoder_2)
            nn.quantize(sd.unet, group_size=32, bits=8)
        args.cfg = args.cfg or 0.0
        args.steps = args.steps or 2
    else:
        sd = StableDiffusion(
            "stabilityai/stable-diffusion-2-1-base", float16=args.float16
        )
        if args.quantize:
            nn.quantize(
                sd.text_encoder, class_predicate=lambda _, m: isinstance(m, nn.Linear)
            )
            nn.quantize(sd.unet, group_size=32, bits=8)
        args.cfg = args.cfg or 7.5
        args.steps = args.steps or 50

    # Fix the steps if they were set too low
    if int(args.steps * args.strength) < 1:
        args.steps = int(math.ceil(1 / args.strength))
        if args.verbose:
            print(
                f"Strength {args.strength} is too low so steps were set to {args.steps}"
            )

    # Ensure that models are read in memory if needed
    if args.preload_models:
        sd.ensure_models_are_loaded()

    # Read the image
    img = Image.open(args.image)

    # Make sure image shape is divisible by 64
    W, H = (dim - dim % 64 for dim in (img.width, img.height))
    if W != img.width or H != img.height:
        print(f"Warning: image shape is not divisible by 64, downsampling to {W}x{H}")
        img = img.resize((W, H), Image.NEAREST)  # use desired downsampling filter

    img = mx.array(np.array(img))
    img = (img[:, :, :3].astype(mx.float32) / 255) * 2 - 1

    # Noise and denoise the latents produced by encoding the img.
    latents = sd.generate_latents_from_image(
        img,
        args.prompt,
        strength=args.strength,
        n_images=args.n_images,
        cfg_weight=args.cfg,
        num_steps=args.steps,
        negative_text=args.negative_prompt,
        seed=args.seed,
    )
    for x_t in tqdm(latents, total=int(args.steps * args.strength)):
        mx.eval(x_t)

    # The following is not necessary but it may help in memory
    # constrained systems by reusing the memory kept by the unet and the text
    # encoders.
    if args.model == "sdxl":
        del sd.text_encoder_1
        del sd.text_encoder_2
    else:
        del sd.text_encoder
    del sd.unet
    del sd.sampler
    peak_mem_unet = mx.metal.get_peak_memory() / 1024**3

    # Decode them into images
    decoded = []
    for i in tqdm(range(0, args.n_images, args.decoding_batch_size)):
        decoded.append(sd.decode(x_t[i : i + args.decoding_batch_size]))
        mx.eval(decoded[-1])
    peak_mem_overall = mx.metal.get_peak_memory() / 1024**3

    # Arrange them on a grid
    x = mx.concatenate(decoded, axis=0)
    x = mx.pad(x, [(0, 0), (8, 8), (8, 8), (0, 0)])
    B, H, W, C = x.shape
    x = x.reshape(args.n_rows, B // args.n_rows, H, W, C).transpose(0, 2, 1, 3, 4)
    x = x.reshape(args.n_rows * H, B // args.n_rows * W, C)
    x = (x * 255).astype(mx.uint8)

    # Save them to disc
    im = Image.fromarray(np.array(x))
    im.save(args.output)

    # Report the peak memory used during generation
    if args.verbose:
        print(f"Peak memory used for the unet: {peak_mem_unet:.3f}GB")
        print(f"Peak memory used overall:      {peak_mem_overall:.3f}GB")

>>>> cvae/vae.py
# Copyright © 2023-2024 Apple Inc.

import math

import mlx.core as mx
import mlx.nn as nn


# from https://github.com/ml-explore/mlx-examples/blob/main/stable_diffusion/stable_diffusion/unet.py
def upsample_nearest(x, scale: int = 2):
    B, H, W, C = x.shape
    x = mx.broadcast_to(x[:, :, None, :, None, :], (B, H, scale, W, scale, C))
    x = x.reshape(B, H * scale, W * scale, C)
    return x


class UpsamplingConv2d(nn.Module):
    """
    A convolutional layer that upsamples the input by a factor of 2. MLX does
    not yet support transposed convolutions, so we approximate them with
    nearest neighbor upsampling followed by a convolution. This is similar to
    the approach used in the original U-Net.
    """

    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):
        super().__init__()
        self.conv = nn.Conv2d(
            in_channels, out_channels, kernel_size, stride=stride, padding=padding
        )

    def __call__(self, x):
        x = self.conv(upsample_nearest(x))
        return x


class Encoder(nn.Module):
    """
    A convolutional variational encoder.
    Maps the input to a normal distribution in latent space and sample a latent
    vector from that distribution.
    """

    def __init__(self, num_latent_dims, image_shape, max_num_filters):
        super().__init__()

        # number of filters in the convolutional layers
        num_filters_1 = max_num_filters // 4
        num_filters_2 = max_num_filters // 2
        num_filters_3 = max_num_filters

        # Output (BHWC):  B x 32 x 32 x num_filters_1
        self.conv1 = nn.Conv2d(image_shape[-1], num_filters_1, 3, stride=2, padding=1)
        # Output (BHWC):  B x 16 x 16 x num_filters_2
        self.conv2 = nn.Conv2d(num_filters_1, num_filters_2, 3, stride=2, padding=1)
        # Output (BHWC):  B x 8 x 8 x num_filters_3
        self.conv3 = nn.Conv2d(num_filters_2, num_filters_3, 3, stride=2, padding=1)

        # Batch Normalization
        self.bn1 = nn.BatchNorm(num_filters_1)
        self.bn2 = nn.BatchNorm(num_filters_2)
        self.bn3 = nn.BatchNorm(num_filters_3)

        # Divide the spatial dimensions by 8 because of the 3 strided convolutions
        output_shape = [num_filters_3] + [
            dimension // 8 for dimension in image_shape[:-1]
        ]

        flattened_dim = math.prod(output_shape)

        # Linear mappings to mean and standard deviation
        self.proj_mu = nn.Linear(flattened_dim, num_latent_dims)
        self.proj_log_var = nn.Linear(flattened_dim, num_latent_dims)

    def __call__(self, x):
        x = nn.leaky_relu(self.bn1(self.conv1(x)))
        x = nn.leaky_relu(self.bn2(self.conv2(x)))
        x = nn.leaky_relu(self.bn3(self.conv3(x)))
        x = mx.flatten(x, 1)  # flatten all dimensions except batch

        mu = self.proj_mu(x)
        logvar = self.proj_log_var(x)
        # Ensure this is the std deviation, not variance
        sigma = mx.exp(logvar * 0.5)

        # Generate a tensor of random values from a normal distribution
        eps = mx.random.normal(sigma.shape)

        # Reparametrization trick to brackpropagate through sampling.
        z = eps * sigma + mu

        return z, mu, logvar


class Decoder(nn.Module):
    """A convolutional decoder"""

    def __init__(self, num_latent_dims, image_shape, max_num_filters):
        super().__init__()
        self.num_latent_dims = num_latent_dims
        num_img_channels = image_shape[-1]
        self.max_num_filters = max_num_filters

        # decoder layers
        num_filters_1 = max_num_filters
        num_filters_2 = max_num_filters // 2
        num_filters_3 = max_num_filters // 4

        # divide the last two dimensions by 8 because of the 3 upsampling convolutions
        self.input_shape = [dimension // 8 for dimension in image_shape[:-1]] + [
            num_filters_1
        ]
        flattened_dim = math.prod(self.input_shape)

        # Output: flattened_dim
        self.lin1 = nn.Linear(num_latent_dims, flattened_dim)
        # Output (BHWC):  B x 16 x 16 x num_filters_2
        self.upconv1 = UpsamplingConv2d(
            num_filters_1, num_filters_2, 3, stride=1, padding=1
        )
        # Output (BHWC):  B x 32 x 32 x num_filters_1
        self.upconv2 = UpsamplingConv2d(
            num_filters_2, num_filters_3, 3, stride=1, padding=1
        )
        # Output (BHWC):  B x 64 x 64 x #img_channels
        self.upconv3 = UpsamplingConv2d(
            num_filters_3, num_img_channels, 3, stride=1, padding=1
        )

        # Batch Normalizations
        self.bn1 = nn.BatchNorm(num_filters_2)
        self.bn2 = nn.BatchNorm(num_filters_3)

    def __call__(self, z):
        x = self.lin1(z)

        # reshape to BHWC
        x = x.reshape(
            -1, self.input_shape[0], self.input_shape[1], self.max_num_filters
        )

        # approximate transposed convolutions with nearest neighbor upsampling
        x = nn.leaky_relu(self.bn1(self.upconv1(x)))
        x = nn.leaky_relu(self.bn2(self.upconv2(x)))
        # sigmoid to ensure pixel values are in [0,1]
        x = mx.sigmoid(self.upconv3(x))
        return x


class CVAE(nn.Module):
    """
    A convolutional variational autoencoder consisting of an encoder and a
    decoder.
    """

    def __init__(self, num_latent_dims, input_shape, max_num_filters):
        super().__init__()
        self.num_latent_dims = num_latent_dims
        self.encoder = Encoder(num_latent_dims, input_shape, max_num_filters)
        self.decoder = Decoder(num_latent_dims, input_shape, max_num_filters)

    def __call__(self, x):
        # image to latent vector
        z, mu, logvar = self.encoder(x)
        # latent vector to image
        x = self.decode(z)
        return x, mu, logvar

    def encode(self, x):
        return self.encoder(x)[0]

    def decode(self, z):
        return self.decoder(z)

>>>> stable_diffusion/stable_diffusion/vae.py
# Copyright © 2023 Apple Inc.

import math
from typing import List

import mlx.core as mx
import mlx.nn as nn

from .config import AutoencoderConfig
from .unet import ResnetBlock2D, upsample_nearest


class Attention(nn.Module):
    """A single head unmasked attention for use with the VAE."""

    def __init__(self, dims: int, norm_groups: int = 32):
        super().__init__()

        self.group_norm = nn.GroupNorm(norm_groups, dims, pytorch_compatible=True)
        self.query_proj = nn.Linear(dims, dims)
        self.key_proj = nn.Linear(dims, dims)
        self.value_proj = nn.Linear(dims, dims)
        self.out_proj = nn.Linear(dims, dims)

    def __call__(self, x):
        B, H, W, C = x.shape

        y = self.group_norm(x)

        queries = self.query_proj(y).reshape(B, H * W, C)
        keys = self.key_proj(y).reshape(B, H * W, C)
        values = self.value_proj(y).reshape(B, H * W, C)

        scale = 1 / math.sqrt(queries.shape[-1])
        scores = (queries * scale) @ keys.transpose(0, 2, 1)
        attn = mx.softmax(scores, axis=-1)
        y = (attn @ values).reshape(B, H, W, C)

        y = self.out_proj(y)
        x = x + y

        return x


class EncoderDecoderBlock2D(nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        num_layers: int = 1,
        resnet_groups: int = 32,
        add_downsample=True,
        add_upsample=True,
    ):
        super().__init__()

        # Add the resnet blocks
        self.resnets = [
            ResnetBlock2D(
                in_channels=in_channels if i == 0 else out_channels,
                out_channels=out_channels,
                groups=resnet_groups,
            )
            for i in range(num_layers)
        ]

        # Add an optional downsampling layer
        if add_downsample:
            self.downsample = nn.Conv2d(
                out_channels, out_channels, kernel_size=3, stride=2, padding=0
            )

        # or upsampling layer
        if add_upsample:
            self.upsample = nn.Conv2d(
                out_channels, out_channels, kernel_size=3, stride=1, padding=1
            )

    def __call__(self, x):
        for resnet in self.resnets:
            x = resnet(x)

        if "downsample" in self:
            x = mx.pad(x, [(0, 0), (0, 1), (0, 1), (0, 0)])
            x = self.downsample(x)

        if "upsample" in self:
            x = self.upsample(upsample_nearest(x))

        return x


class Encoder(nn.Module):
    """Implements the encoder side of the Autoencoder."""

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        block_out_channels: List[int] = [64],
        layers_per_block: int = 2,
        resnet_groups: int = 32,
    ):
        super().__init__()

        self.conv_in = nn.Conv2d(
            in_channels, block_out_channels[0], kernel_size=3, stride=1, padding=1
        )

        channels = [block_out_channels[0]] + list(block_out_channels)
        self.down_blocks = [
            EncoderDecoderBlock2D(
                in_channels,
                out_channels,
                num_layers=layers_per_block,
                resnet_groups=resnet_groups,
                add_downsample=i < len(block_out_channels) - 1,
                add_upsample=False,
            )
            for i, (in_channels, out_channels) in enumerate(zip(channels, channels[1:]))
        ]

        self.mid_blocks = [
            ResnetBlock2D(
                in_channels=block_out_channels[-1],
                out_channels=block_out_channels[-1],
                groups=resnet_groups,
            ),
            Attention(block_out_channels[-1], resnet_groups),
            ResnetBlock2D(
                in_channels=block_out_channels[-1],
                out_channels=block_out_channels[-1],
                groups=resnet_groups,
            ),
        ]

        self.conv_norm_out = nn.GroupNorm(
            resnet_groups, block_out_channels[-1], pytorch_compatible=True
        )
        self.conv_out = nn.Conv2d(block_out_channels[-1], out_channels, 3, padding=1)

    def __call__(self, x):
        x = self.conv_in(x)

        for l in self.down_blocks:
            x = l(x)

        x = self.mid_blocks[0](x)
        x = self.mid_blocks[1](x)
        x = self.mid_blocks[2](x)

        x = self.conv_norm_out(x)
        x = nn.silu(x)
        x = self.conv_out(x)

        return x


class Decoder(nn.Module):
    """Implements the decoder side of the Autoencoder."""

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        block_out_channels: List[int] = [64],
        layers_per_block: int = 2,
        resnet_groups: int = 32,
    ):
        super().__init__()

        self.conv_in = nn.Conv2d(
            in_channels, block_out_channels[-1], kernel_size=3, stride=1, padding=1
        )

        self.mid_blocks = [
            ResnetBlock2D(
                in_channels=block_out_channels[-1],
                out_channels=block_out_channels[-1],
                groups=resnet_groups,
            ),
            Attention(block_out_channels[-1], resnet_groups),
            ResnetBlock2D(
                in_channels=block_out_channels[-1],
                out_channels=block_out_channels[-1],
                groups=resnet_groups,
            ),
        ]

        channels = list(reversed(block_out_channels))
        channels = [channels[0]] + channels
        self.up_blocks = [
            EncoderDecoderBlock2D(
                in_channels,
                out_channels,
                num_layers=layers_per_block,
                resnet_groups=resnet_groups,
                add_downsample=False,
                add_upsample=i < len(block_out_channels) - 1,
            )
            for i, (in_channels, out_channels) in enumerate(zip(channels, channels[1:]))
        ]

        self.conv_norm_out = nn.GroupNorm(
            resnet_groups, block_out_channels[0], pytorch_compatible=True
        )
        self.conv_out = nn.Conv2d(block_out_channels[0], out_channels, 3, padding=1)

    def __call__(self, x):
        x = self.conv_in(x)

        x = self.mid_blocks[0](x)
        x = self.mid_blocks[1](x)
        x = self.mid_blocks[2](x)

        for l in self.up_blocks:
            x = l(x)

        x = self.conv_norm_out(x)
        x = nn.silu(x)
        x = self.conv_out(x)

        return x


class Autoencoder(nn.Module):
    """The autoencoder that allows us to perform diffusion in the latent space."""

    def __init__(self, config: AutoencoderConfig):
        super().__init__()

        self.latent_channels = config.latent_channels_in
        self.scaling_factor = config.scaling_factor
        self.encoder = Encoder(
            config.in_channels,
            config.latent_channels_out,
            config.block_out_channels,
            config.layers_per_block,
            resnet_groups=config.norm_num_groups,
        )
        self.decoder = Decoder(
            config.latent_channels_in,
            config.out_channels,
            config.block_out_channels,
            config.layers_per_block + 1,
            resnet_groups=config.norm_num_groups,
        )

        self.quant_proj = nn.Linear(
            config.latent_channels_out, config.latent_channels_out
        )
        self.post_quant_proj = nn.Linear(
            config.latent_channels_in, config.latent_channels_in
        )

    def decode(self, z):
        z = z / self.scaling_factor
        return self.decoder(self.post_quant_proj(z))

    def encode(self, x):
        x = self.encoder(x)
        x = self.quant_proj(x)
        mean, logvar = x.split(2, axis=-1)
        mean = mean * self.scaling_factor
        logvar = logvar + 2 * math.log(self.scaling_factor)

        return mean, logvar

    def __call__(self, x, key=None):
        mean, logvar = self.encode(x)
        z = mx.random.normal(mean.shape, key=key) * mx.exp(0.5 * logvar) + mean
        x_hat = self.decode(z)

        return dict(x_hat=x_hat, z=z, mean=mean, logvar=logvar)

>>>> stable_diffusion/stable_diffusion/sampler.py
# Copyright © 2023 Apple Inc.

import mlx.core as mx

from .config import DiffusionConfig


def _linspace(a, b, num):
    x = mx.arange(0, num) / (num - 1)
    return (b - a) * x + a


def _interp(y, x_new):
    """Interpolate the function defined by (arange(0, len(y)), y) at positions x_new."""
    x_low = x_new.astype(mx.int32)
    x_high = mx.minimum(x_low + 1, len(y) - 1)

    y_low = y[x_low]
    y_high = y[x_high]
    delta_x = x_new - x_low
    y_new = y_low * (1 - delta_x) + delta_x * y_high

    return y_new


class SimpleEulerSampler:
    """A simple Euler integrator that can be used to sample from our diffusion models.

    The method ``step()`` performs one Euler step from x_t to x_t_prev.
    """

    def __init__(self, config: DiffusionConfig):
        # Compute the noise schedule
        if config.beta_schedule == "linear":
            betas = _linspace(
                config.beta_start, config.beta_end, config.num_train_steps
            )
        elif config.beta_schedule == "scaled_linear":
            betas = _linspace(
                config.beta_start**0.5, config.beta_end**0.5, config.num_train_steps
            ).square()
        else:
            raise NotImplementedError(f"{config.beta_schedule} is not implemented.")

        alphas = 1 - betas
        alphas_cumprod = mx.cumprod(alphas)

        self._sigmas = mx.concatenate(
            [mx.zeros(1), ((1 - alphas_cumprod) / alphas_cumprod).sqrt()]
        )

    @property
    def max_time(self):
        return len(self._sigmas) - 1

    def sample_prior(self, shape, dtype=mx.float32, key=None):
        noise = mx.random.normal(shape, key=key)
        return (
            noise * self._sigmas[-1] * (self._sigmas[-1].square() + 1).rsqrt()
        ).astype(dtype)

    def add_noise(self, x, t, key=None):
        noise = mx.random.normal(x.shape, key=key)
        s = self.sigmas(t)
        return (x + noise * s) * (s.square() + 1).rsqrt()

    def sigmas(self, t):
        return _interp(self._sigmas, t)

    def timesteps(self, num_steps: int, start_time=None, dtype=mx.float32):
        start_time = start_time or (len(self._sigmas) - 1)
        assert 0 < start_time <= (len(self._sigmas) - 1)
        steps = _linspace(start_time, 0, num_steps + 1).astype(dtype)
        return list(zip(steps, steps[1:]))

    def step(self, eps_pred, x_t, t, t_prev):
        sigma = self.sigmas(t).astype(eps_pred.dtype)
        sigma_prev = self.sigmas(t_prev).astype(eps_pred.dtype)

        dt = sigma_prev - sigma
        x_t_prev = (sigma.square() + 1).sqrt() * x_t + eps_pred * dt

        x_t_prev = x_t_prev * (sigma_prev.square() + 1).rsqrt()

        return x_t_prev


class SimpleEulerAncestralSampler(SimpleEulerSampler):
    def step(self, eps_pred, x_t, t, t_prev):
        sigma = self.sigmas(t).astype(eps_pred.dtype)
        sigma_prev = self.sigmas(t_prev).astype(eps_pred.dtype)

        sigma2 = sigma.square()
        sigma_prev2 = sigma_prev.square()
        sigma_up = (sigma_prev2 * (sigma2 - sigma_prev2) / sigma2).sqrt()
        sigma_down = (sigma_prev2 - sigma_up**2).sqrt()

        dt = sigma_down - sigma
        x_t_prev = (sigma2 + 1).sqrt() * x_t + eps_pred * dt
        noise = mx.random.normal(x_t_prev.shape).astype(x_t_prev.dtype)
        x_t_prev = x_t_prev + noise * sigma_up

        x_t_prev = x_t_prev * (sigma_prev2 + 1).rsqrt()

        return x_t_prev

>>>> stable_diffusion/stable_diffusion/clip.py
# Copyright © 2023-2024 Apple Inc.

from dataclasses import dataclass
from typing import List, Optional

import mlx.core as mx
import mlx.nn as nn

from .config import CLIPTextModelConfig

_ACTIVATIONS = {"quick_gelu": nn.gelu_fast_approx, "gelu": nn.gelu}


@dataclass
class CLIPOutput:
    # The last_hidden_state indexed at the EOS token and possibly projected if
    # the model has a projection layer
    pooled_output: Optional[mx.array] = None

    # The full sequence output of the transformer after the final layernorm
    last_hidden_state: Optional[mx.array] = None

    # A list of hidden states corresponding to the outputs of the transformer layers
    hidden_states: Optional[List[mx.array]] = None


class CLIPEncoderLayer(nn.Module):
    """The transformer encoder layer from CLIP."""

    def __init__(self, model_dims: int, num_heads: int, activation: str):
        super().__init__()

        self.layer_norm1 = nn.LayerNorm(model_dims)
        self.layer_norm2 = nn.LayerNorm(model_dims)

        self.attention = nn.MultiHeadAttention(model_dims, num_heads)
        # Add biases to the attention projections to match CLIP
        self.attention.query_proj.bias = mx.zeros(model_dims)
        self.attention.key_proj.bias = mx.zeros(model_dims)
        self.attention.value_proj.bias = mx.zeros(model_dims)
        self.attention.out_proj.bias = mx.zeros(model_dims)

        self.linear1 = nn.Linear(model_dims, 4 * model_dims)
        self.linear2 = nn.Linear(4 * model_dims, model_dims)

        self.act = _ACTIVATIONS[activation]

    def __call__(self, x, attn_mask=None):
        y = self.layer_norm1(x)
        y = self.attention(y, y, y, attn_mask)
        x = y + x

        y = self.layer_norm2(x)
        y = self.linear1(y)
        y = self.act(y)
        y = self.linear2(y)
        x = y + x

        return x


class CLIPTextModel(nn.Module):
    """Implements the text encoder transformer from CLIP."""

    def __init__(self, config: CLIPTextModelConfig):
        super().__init__()

        self.token_embedding = nn.Embedding(config.vocab_size, config.model_dims)
        self.position_embedding = nn.Embedding(config.max_length, config.model_dims)
        self.layers = [
            CLIPEncoderLayer(config.model_dims, config.num_heads, config.hidden_act)
            for i in range(config.num_layers)
        ]
        self.final_layer_norm = nn.LayerNorm(config.model_dims)

        if config.projection_dim is not None:
            self.text_projection = nn.Linear(
                config.model_dims, config.projection_dim, bias=False
            )

    def _get_mask(self, N, dtype):
        indices = mx.arange(N)
        mask = indices[:, None] < indices[None]
        mask = mask.astype(dtype) * (-6e4 if dtype == mx.float16 else -1e9)
        return mask

    def __call__(self, x):
        # Extract some shapes
        B, N = x.shape
        eos_tokens = x.argmax(-1)

        # Compute the embeddings
        x = self.token_embedding(x)
        x = x + self.position_embedding.weight[:N]

        # Compute the features from the transformer
        mask = self._get_mask(N, x.dtype)
        hidden_states = []
        for l in self.layers:
            x = l(x, mask)
            hidden_states.append(x)

        # Apply the final layernorm and return
        x = self.final_layer_norm(x)
        last_hidden_state = x

        # Select the EOS token
        pooled_output = x[mx.arange(len(x)), eos_tokens]
        if "text_projection" in self:
            pooled_output = self.text_projection(pooled_output)

        return CLIPOutput(
            pooled_output=pooled_output,
            last_hidden_state=last_hidden_state,
            hidden_states=hidden_states,
        )

>>>> stable_diffusion/stable_diffusion/tokenizer.py
# Copyright © 2023 Apple Inc.

import regex


class Tokenizer:
    """A simple port of CLIPTokenizer from https://github.com/huggingface/transformers/ ."""

    def __init__(self, bpe_ranks, vocab):
        self.bpe_ranks = bpe_ranks
        self.vocab = vocab
        self.pat = regex.compile(
            r"""<\|startoftext\|>|<\|endoftext\|>|'s|'t|'re|'ve|'m|'ll|'d|[\p{L}]+|[\p{N}]|[^\s\p{L}\p{N}]+""",
            regex.IGNORECASE,
        )

        self._cache = {self.bos: self.bos, self.eos: self.eos}

    @property
    def bos(self):
        return "<|startoftext|>"

    @property
    def bos_token(self):
        return self.vocab[self.bos]

    @property
    def eos(self):
        return "<|endoftext|>"

    @property
    def eos_token(self):
        return self.vocab[self.eos]

    def bpe(self, text):
        if text in self._cache:
            return self._cache[text]

        unigrams = list(text[:-1]) + [text[-1] + "</w>"]
        unique_bigrams = set(zip(unigrams, unigrams[1:]))

        if not unique_bigrams:
            return unigrams

        # In every iteration try to merge the two most likely bigrams. If none
        # was merged we are done.
        #
        # Ported from https://github.com/huggingface/transformers/blob/main/src/transformers/models/clip/tokenization_clip.py
        while unique_bigrams:
            bigram = min(
                unique_bigrams, key=lambda pair: self.bpe_ranks.get(pair, float("inf"))
            )
            if bigram not in self.bpe_ranks:
                break

            new_unigrams = []
            skip = False
            for a, b in zip(unigrams, unigrams[1:]):
                if skip:
                    skip = False
                    continue

                if (a, b) == bigram:
                    new_unigrams.append(a + b)
                    skip = True

                else:
                    new_unigrams.append(a)

            if not skip:
                new_unigrams.append(b)

            unigrams = new_unigrams
            unique_bigrams = set(zip(unigrams, unigrams[1:]))

        self._cache[text] = unigrams

        return unigrams

    def tokenize(self, text, prepend_bos=True, append_eos=True):
        if isinstance(text, list):
            return [self.tokenize(t, prepend_bos, append_eos) for t in text]

        # Lower case cleanup and split according to self.pat. Hugging Face does
        # a much more thorough job here but this should suffice for 95% of
        # cases.
        clean_text = regex.sub(r"\s+", " ", text.lower())
        tokens = regex.findall(self.pat, clean_text)

        # Split the tokens according to the byte-pair merge file
        bpe_tokens = [ti for t in tokens for ti in self.bpe(t)]

        # Map to token ids and return
        tokens = [self.vocab[t] for t in bpe_tokens]
        if prepend_bos:
            tokens = [self.bos_token] + tokens
        if append_eos:
            tokens.append(self.eos_token)

        return tokens

>>>> stable_diffusion/stable_diffusion/model_io.py
# Copyright © 2023-2024 Apple Inc.

import json
from typing import Optional

import mlx.core as mx
from huggingface_hub import hf_hub_download
from mlx.utils import tree_unflatten

from .clip import CLIPTextModel
from .config import AutoencoderConfig, CLIPTextModelConfig, DiffusionConfig, UNetConfig
from .tokenizer import Tokenizer
from .unet import UNetModel
from .vae import Autoencoder

_DEFAULT_MODEL = "stabilityai/stable-diffusion-2-1-base"
_MODELS = {
    # See https://huggingface.co/stabilityai/sdxl-turbo for the model details and license
    "stabilityai/sdxl-turbo": {
        "unet_config": "unet/config.json",
        "unet": "unet/diffusion_pytorch_model.safetensors",
        "text_encoder_config": "text_encoder/config.json",
        "text_encoder": "text_encoder/model.safetensors",
        "text_encoder_2_config": "text_encoder_2/config.json",
        "text_encoder_2": "text_encoder_2/model.safetensors",
        "vae_config": "vae/config.json",
        "vae": "vae/diffusion_pytorch_model.safetensors",
        "diffusion_config": "scheduler/scheduler_config.json",
        "tokenizer_vocab": "tokenizer/vocab.json",
        "tokenizer_merges": "tokenizer/merges.txt",
        "tokenizer_2_vocab": "tokenizer_2/vocab.json",
        "tokenizer_2_merges": "tokenizer_2/merges.txt",
    },
    # See https://huggingface.co/stabilityai/stable-diffusion-2-1-base for the model details and license
    "stabilityai/stable-diffusion-2-1-base": {
        "unet_config": "unet/config.json",
        "unet": "unet/diffusion_pytorch_model.safetensors",
        "text_encoder_config": "text_encoder/config.json",
        "text_encoder": "text_encoder/model.safetensors",
        "vae_config": "vae/config.json",
        "vae": "vae/diffusion_pytorch_model.safetensors",
        "diffusion_config": "scheduler/scheduler_config.json",
        "tokenizer_vocab": "tokenizer/vocab.json",
        "tokenizer_merges": "tokenizer/merges.txt",
    },
}


def map_unet_weights(key, value):
    # Map up/downsampling
    if "downsamplers" in key:
        key = key.replace("downsamplers.0.conv", "downsample")
    if "upsamplers" in key:
        key = key.replace("upsamplers.0.conv", "upsample")

    # Map the mid block
    if "mid_block.resnets.0" in key:
        key = key.replace("mid_block.resnets.0", "mid_blocks.0")
    if "mid_block.attentions.0" in key:
        key = key.replace("mid_block.attentions.0", "mid_blocks.1")
    if "mid_block.resnets.1" in key:
        key = key.replace("mid_block.resnets.1", "mid_blocks.2")

    # Map attention layers
    if "to_k" in key:
        key = key.replace("to_k", "key_proj")
    if "to_out.0" in key:
        key = key.replace("to_out.0", "out_proj")
    if "to_q" in key:
        key = key.replace("to_q", "query_proj")
    if "to_v" in key:
        key = key.replace("to_v", "value_proj")

    # Map transformer ffn
    if "ff.net.2" in key:
        key = key.replace("ff.net.2", "linear3")
    if "ff.net.0" in key:
        k1 = key.replace("ff.net.0.proj", "linear1")
        k2 = key.replace("ff.net.0.proj", "linear2")
        v1, v2 = mx.split(value, 2)

        return [(k1, v1), (k2, v2)]

    if "conv_shortcut.weight" in key:
        value = value.squeeze()

    # Transform the weights from 1x1 convs to linear
    if len(value.shape) == 4 and ("proj_in" in key or "proj_out" in key):
        value = value.squeeze()

    if len(value.shape) == 4:
        value = value.transpose(0, 2, 3, 1)
        value = value.reshape(-1).reshape(value.shape)

    return [(key, value)]


def map_clip_text_encoder_weights(key, value):
    # Remove prefixes
    if key.startswith("text_model."):
        key = key[11:]
    if key.startswith("embeddings."):
        key = key[11:]
    if key.startswith("encoder."):
        key = key[8:]

    # Map attention layers
    if "self_attn." in key:
        key = key.replace("self_attn.", "attention.")
    if "q_proj." in key:
        key = key.replace("q_proj.", "query_proj.")
    if "k_proj." in key:
        key = key.replace("k_proj.", "key_proj.")
    if "v_proj." in key:
        key = key.replace("v_proj.", "value_proj.")

    # Map ffn layers
    if "mlp.fc1" in key:
        key = key.replace("mlp.fc1", "linear1")
    if "mlp.fc2" in key:
        key = key.replace("mlp.fc2", "linear2")

    return [(key, value)]


def map_vae_weights(key, value):
    # Map up/downsampling
    if "downsamplers" in key:
        key = key.replace("downsamplers.0.conv", "downsample")
    if "upsamplers" in key:
        key = key.replace("upsamplers.0.conv", "upsample")

    # Map attention layers
    if "to_k" in key:
        key = key.replace("to_k", "key_proj")
    if "to_out.0" in key:
        key = key.replace("to_out.0", "out_proj")
    if "to_q" in key:
        key = key.replace("to_q", "query_proj")
    if "to_v" in key:
        key = key.replace("to_v", "value_proj")

    # Map the mid block
    if "mid_block.resnets.0" in key:
        key = key.replace("mid_block.resnets.0", "mid_blocks.0")
    if "mid_block.attentions.0" in key:
        key = key.replace("mid_block.attentions.0", "mid_blocks.1")
    if "mid_block.resnets.1" in key:
        key = key.replace("mid_block.resnets.1", "mid_blocks.2")

    # Map the quant/post_quant layers
    if "quant_conv" in key:
        key = key.replace("quant_conv", "quant_proj")
        value = value.squeeze()

    # Map the conv_shortcut to linear
    if "conv_shortcut.weight" in key:
        value = value.squeeze()

    if len(value.shape) == 4:
        value = value.transpose(0, 2, 3, 1)
        value = value.reshape(-1).reshape(value.shape)

    return [(key, value)]


def _flatten(params):
    return [(k, v) for p in params for (k, v) in p]


def _load_safetensor_weights(mapper, model, weight_file, float16: bool = False):
    dtype = mx.float16 if float16 else mx.float32
    weights = mx.load(weight_file)
    weights = _flatten([mapper(k, v.astype(dtype)) for k, v in weights.items()])
    model.update(tree_unflatten(weights))


def _check_key(key: str, part: str):
    if key not in _MODELS:
        raise ValueError(
            f"[{part}] '{key}' model not found, choose one of {{{','.join(_MODELS.keys())}}}"
        )


def load_unet(key: str = _DEFAULT_MODEL, float16: bool = False):
    """Load the stable diffusion UNet from Hugging Face Hub."""
    _check_key(key, "load_unet")

    # Download the config and create the model
    unet_config = _MODELS[key]["unet_config"]
    with open(hf_hub_download(key, unet_config)) as f:
        config = json.load(f)

    n_blocks = len(config["block_out_channels"])
    model = UNetModel(
        UNetConfig(
            in_channels=config["in_channels"],
            out_channels=config["out_channels"],
            block_out_channels=config["block_out_channels"],
            layers_per_block=[config["layers_per_block"]] * n_blocks,
            transformer_layers_per_block=config.get(
                "transformer_layers_per_block", (1,) * 4
            ),
            num_attention_heads=(
                [config["attention_head_dim"]] * n_blocks
                if isinstance(config["attention_head_dim"], int)
                else config["attention_head_dim"]
            ),
            cross_attention_dim=[config["cross_attention_dim"]] * n_blocks,
            norm_num_groups=config["norm_num_groups"],
            down_block_types=config["down_block_types"],
            up_block_types=config["up_block_types"][::-1],
            addition_embed_type=config.get("addition_embed_type", None),
            addition_time_embed_dim=config.get("addition_time_embed_dim", None),
            projection_class_embeddings_input_dim=config.get(
                "projection_class_embeddings_input_dim", None
            ),
        )
    )

    # Download the weights and map them into the model
    unet_weights = _MODELS[key]["unet"]
    weight_file = hf_hub_download(key, unet_weights)
    _load_safetensor_weights(map_unet_weights, model, weight_file, float16)

    return model


def load_text_encoder(
    key: str = _DEFAULT_MODEL,
    float16: bool = False,
    model_key: str = "text_encoder",
    config_key: Optional[str] = None,
):
    """Load the stable diffusion text encoder from Hugging Face Hub."""
    _check_key(key, "load_text_encoder")

    config_key = config_key or (model_key + "_config")

    # Download the config and create the model
    text_encoder_config = _MODELS[key][config_key]
    with open(hf_hub_download(key, text_encoder_config)) as f:
        config = json.load(f)

    with_projection = "WithProjection" in config["architectures"][0]

    model = CLIPTextModel(
        CLIPTextModelConfig(
            num_layers=config["num_hidden_layers"],
            model_dims=config["hidden_size"],
            num_heads=config["num_attention_heads"],
            max_length=config["max_position_embeddings"],
            vocab_size=config["vocab_size"],
            projection_dim=config["projection_dim"] if with_projection else None,
            hidden_act=config.get("hidden_act", "quick_gelu"),
        )
    )

    # Download the weights and map them into the model
    text_encoder_weights = _MODELS[key][model_key]
    weight_file = hf_hub_download(key, text_encoder_weights)
    _load_safetensor_weights(map_clip_text_encoder_weights, model, weight_file, float16)

    return model


def load_autoencoder(key: str = _DEFAULT_MODEL, float16: bool = False):
    """Load the stable diffusion autoencoder from Hugging Face Hub."""
    _check_key(key, "load_autoencoder")

    # Download the config and create the model
    vae_config = _MODELS[key]["vae_config"]
    with open(hf_hub_download(key, vae_config)) as f:
        config = json.load(f)

    model = Autoencoder(
        AutoencoderConfig(
            in_channels=config["in_channels"],
            out_channels=config["out_channels"],
            latent_channels_out=2 * config["latent_channels"],
            latent_channels_in=config["latent_channels"],
            block_out_channels=config["block_out_channels"],
            layers_per_block=config["layers_per_block"],
            norm_num_groups=config["norm_num_groups"],
            scaling_factor=config.get("scaling_factor", 0.18215),
        )
    )

    # Download the weights and map them into the model
    vae_weights = _MODELS[key]["vae"]
    weight_file = hf_hub_download(key, vae_weights)
    _load_safetensor_weights(map_vae_weights, model, weight_file, float16)

    return model


def load_diffusion_config(key: str = _DEFAULT_MODEL):
    """Load the stable diffusion config from Hugging Face Hub."""
    _check_key(key, "load_diffusion_config")

    diffusion_config = _MODELS[key]["diffusion_config"]
    with open(hf_hub_download(key, diffusion_config)) as f:
        config = json.load(f)

    return DiffusionConfig(
        beta_start=config["beta_start"],
        beta_end=config["beta_end"],
        beta_schedule=config["beta_schedule"],
        num_train_steps=config["num_train_timesteps"],
    )


def load_tokenizer(
    key: str = _DEFAULT_MODEL,
    vocab_key: str = "tokenizer_vocab",
    merges_key: str = "tokenizer_merges",
):
    _check_key(key, "load_tokenizer")

    vocab_file = hf_hub_download(key, _MODELS[key][vocab_key])
    with open(vocab_file, encoding="utf-8") as f:
        vocab = json.load(f)

    merges_file = hf_hub_download(key, _MODELS[key][merges_key])
    with open(merges_file, encoding="utf-8") as f:
        bpe_merges = f.read().strip().split("\n")[1 : 49152 - 256 - 2 + 1]
    bpe_merges = [tuple(m.split()) for m in bpe_merges]
    bpe_ranks = dict(map(reversed, enumerate(bpe_merges)))

    return Tokenizer(bpe_ranks, vocab)

>>>> stable_diffusion/stable_diffusion/unet.py
# Copyright © 2023 Apple Inc.

import math
from typing import Optional

import mlx.core as mx
import mlx.nn as nn

from .config import UNetConfig


def upsample_nearest(x, scale: int = 2):
    B, H, W, C = x.shape
    x = mx.broadcast_to(x[:, :, None, :, None, :], (B, H, scale, W, scale, C))
    x = x.reshape(B, H * scale, W * scale, C)

    return x


class TimestepEmbedding(nn.Module):
    def __init__(self, in_channels: int, time_embed_dim: int):
        super().__init__()

        self.linear_1 = nn.Linear(in_channels, time_embed_dim)
        self.linear_2 = nn.Linear(time_embed_dim, time_embed_dim)

    def __call__(self, x):
        x = self.linear_1(x)
        x = nn.silu(x)
        x = self.linear_2(x)

        return x


class TransformerBlock(nn.Module):
    def __init__(
        self,
        model_dims: int,
        num_heads: int,
        hidden_dims: Optional[int] = None,
        memory_dims: Optional[int] = None,
    ):
        super().__init__()

        self.norm1 = nn.LayerNorm(model_dims)
        self.attn1 = nn.MultiHeadAttention(model_dims, num_heads)
        self.attn1.out_proj.bias = mx.zeros(model_dims)

        memory_dims = memory_dims or model_dims
        self.norm2 = nn.LayerNorm(model_dims)
        self.attn2 = nn.MultiHeadAttention(
            model_dims, num_heads, key_input_dims=memory_dims
        )
        self.attn2.out_proj.bias = mx.zeros(model_dims)

        hidden_dims = hidden_dims or 4 * model_dims
        self.norm3 = nn.LayerNorm(model_dims)
        self.linear1 = nn.Linear(model_dims, hidden_dims)
        self.linear2 = nn.Linear(model_dims, hidden_dims)
        self.linear3 = nn.Linear(hidden_dims, model_dims)

    def __call__(self, x, memory, attn_mask, memory_mask):
        # Self attention
        y = self.norm1(x)
        y = self.attn1(y, y, y, attn_mask)
        x = x + y

        # Cross attention
        y = self.norm2(x)
        y = self.attn2(y, memory, memory, memory_mask)
        x = x + y

        # FFN
        y = self.norm3(x)
        y_a = self.linear1(y)
        y_b = self.linear2(y)
        y = y_a * nn.gelu(y_b)
        y = self.linear3(y)
        x = x + y

        return x


class Transformer2D(nn.Module):
    """A transformer model for inputs with 2 spatial dimensions."""

    def __init__(
        self,
        in_channels: int,
        model_dims: int,
        encoder_dims: int,
        num_heads: int,
        num_layers: int = 1,
        norm_num_groups: int = 32,
    ):
        super().__init__()

        self.norm = nn.GroupNorm(norm_num_groups, in_channels, pytorch_compatible=True)
        self.proj_in = nn.Linear(in_channels, model_dims)
        self.transformer_blocks = [
            TransformerBlock(model_dims, num_heads, memory_dims=encoder_dims)
            for i in range(num_layers)
        ]
        self.proj_out = nn.Linear(model_dims, in_channels)

    def __call__(self, x, encoder_x, attn_mask, encoder_attn_mask):
        # Save the input to add to the output
        input_x = x
        dtype = x.dtype

        # Perform the input norm and projection
        B, H, W, C = x.shape
        x = self.norm(x).reshape(B, -1, C)
        x = self.proj_in(x)

        # Apply the transformer
        for block in self.transformer_blocks:
            x = block(x, encoder_x, attn_mask, encoder_attn_mask)

        # Apply the output projection and reshape
        x = self.proj_out(x)
        x = x.reshape(B, H, W, C)

        return x + input_x


class ResnetBlock2D(nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: Optional[int] = None,
        groups: int = 32,
        temb_channels: Optional[int] = None,
    ):
        super().__init__()

        out_channels = out_channels or in_channels

        self.norm1 = nn.GroupNorm(groups, in_channels, pytorch_compatible=True)
        self.conv1 = nn.Conv2d(
            in_channels, out_channels, kernel_size=3, stride=1, padding=1
        )
        if temb_channels is not None:
            self.time_emb_proj = nn.Linear(temb_channels, out_channels)
        self.norm2 = nn.GroupNorm(groups, out_channels, pytorch_compatible=True)
        self.conv2 = nn.Conv2d(
            out_channels, out_channels, kernel_size=3, stride=1, padding=1
        )

        if in_channels != out_channels:
            self.conv_shortcut = nn.Linear(in_channels, out_channels)

    def __call__(self, x, temb=None):
        dtype = x.dtype

        if temb is not None:
            temb = self.time_emb_proj(nn.silu(temb))

        y = self.norm1(x)
        y = nn.silu(y)
        y = self.conv1(y)
        if temb is not None:
            y = y + temb[:, None, None, :]
        y = self.norm2(y)
        y = nn.silu(y)
        y = self.conv2(y)

        x = y + (x if "conv_shortcut" not in self else self.conv_shortcut(x))

        return x


class UNetBlock2D(nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        temb_channels: int,
        prev_out_channels: Optional[int] = None,
        num_layers: int = 1,
        transformer_layers_per_block: int = 1,
        num_attention_heads: int = 8,
        cross_attention_dim=1280,
        resnet_groups: int = 32,
        add_downsample=True,
        add_upsample=True,
        add_cross_attention=True,
    ):
        super().__init__()

        # Prepare the in channels list for the resnets
        if prev_out_channels is None:
            in_channels_list = [in_channels] + [out_channels] * (num_layers - 1)
        else:
            in_channels_list = [prev_out_channels] + [out_channels] * (num_layers - 1)
            res_channels_list = [out_channels] * (num_layers - 1) + [in_channels]
            in_channels_list = [
                a + b for a, b in zip(in_channels_list, res_channels_list)
            ]

        # Add resnet blocks that also process the time embedding
        self.resnets = [
            ResnetBlock2D(
                in_channels=ic,
                out_channels=out_channels,
                temb_channels=temb_channels,
                groups=resnet_groups,
            )
            for ic in in_channels_list
        ]

        # Add optional cross attention layers
        if add_cross_attention:
            self.attentions = [
                Transformer2D(
                    in_channels=out_channels,
                    model_dims=out_channels,
                    num_heads=num_attention_heads,
                    num_layers=transformer_layers_per_block,
                    encoder_dims=cross_attention_dim,
                )
                for i in range(num_layers)
            ]

        # Add an optional downsampling layer
        if add_downsample:
            self.downsample = nn.Conv2d(
                out_channels, out_channels, kernel_size=3, stride=2, padding=1
            )

        # or upsampling layer
        if add_upsample:
            self.upsample = nn.Conv2d(
                out_channels, out_channels, kernel_size=3, stride=1, padding=1
            )

    def __call__(
        self,
        x,
        encoder_x=None,
        temb=None,
        attn_mask=None,
        encoder_attn_mask=None,
        residual_hidden_states=None,
    ):
        output_states = []

        for i in range(len(self.resnets)):
            if residual_hidden_states is not None:
                x = mx.concatenate([x, residual_hidden_states.pop()], axis=-1)

            x = self.resnets[i](x, temb)

            if "attentions" in self:
                x = self.attentions[i](x, encoder_x, attn_mask, encoder_attn_mask)

            output_states.append(x)

        if "downsample" in self:
            x = self.downsample(x)
            output_states.append(x)

        if "upsample" in self:
            x = self.upsample(upsample_nearest(x))
            output_states.append(x)

        return x, output_states


class UNetModel(nn.Module):
    """The conditional 2D UNet model that actually performs the denoising."""

    def __init__(self, config: UNetConfig):
        super().__init__()

        self.conv_in = nn.Conv2d(
            config.in_channels,
            config.block_out_channels[0],
            config.conv_in_kernel,
            padding=(config.conv_in_kernel - 1) // 2,
        )

        self.timesteps = nn.SinusoidalPositionalEncoding(
            config.block_out_channels[0],
            max_freq=1,
            min_freq=math.exp(
                -math.log(10000) + 2 * math.log(10000) / config.block_out_channels[0]
            ),
            scale=1.0,
            cos_first=True,
            full_turns=False,
        )
        self.time_embedding = TimestepEmbedding(
            config.block_out_channels[0],
            config.block_out_channels[0] * 4,
        )

        if config.addition_embed_type == "text_time":
            self.add_time_proj = nn.SinusoidalPositionalEncoding(
                config.addition_time_embed_dim,
                max_freq=1,
                min_freq=math.exp(
                    -math.log(10000)
                    + 2 * math.log(10000) / config.addition_time_embed_dim
                ),
                scale=1.0,
                cos_first=True,
                full_turns=False,
            )
            self.add_embedding = TimestepEmbedding(
                config.projection_class_embeddings_input_dim,
                config.block_out_channels[0] * 4,
            )

        # Make the downsampling blocks
        block_channels = [config.block_out_channels[0]] + list(
            config.block_out_channels
        )
        self.down_blocks = [
            UNetBlock2D(
                in_channels=in_channels,
                out_channels=out_channels,
                temb_channels=config.block_out_channels[0] * 4,
                num_layers=config.layers_per_block[i],
                transformer_layers_per_block=config.transformer_layers_per_block[i],
                num_attention_heads=config.num_attention_heads[i],
                cross_attention_dim=config.cross_attention_dim[i],
                resnet_groups=config.norm_num_groups,
                add_downsample=(i < len(config.block_out_channels) - 1),
                add_upsample=False,
                add_cross_attention="CrossAttn" in config.down_block_types[i],
            )
            for i, (in_channels, out_channels) in enumerate(
                zip(block_channels, block_channels[1:])
            )
        ]

        # Make the middle block
        self.mid_blocks = [
            ResnetBlock2D(
                in_channels=config.block_out_channels[-1],
                out_channels=config.block_out_channels[-1],
                temb_channels=config.block_out_channels[0] * 4,
                groups=config.norm_num_groups,
            ),
            Transformer2D(
                in_channels=config.block_out_channels[-1],
                model_dims=config.block_out_channels[-1],
                num_heads=config.num_attention_heads[-1],
                num_layers=config.transformer_layers_per_block[-1],
                encoder_dims=config.cross_attention_dim[-1],
            ),
            ResnetBlock2D(
                in_channels=config.block_out_channels[-1],
                out_channels=config.block_out_channels[-1],
                temb_channels=config.block_out_channels[0] * 4,
                groups=config.norm_num_groups,
            ),
        ]

        # Make the upsampling blocks
        block_channels = (
            [config.block_out_channels[0]]
            + list(config.block_out_channels)
            + [config.block_out_channels[-1]]
        )
        self.up_blocks = [
            UNetBlock2D(
                in_channels=in_channels,
                out_channels=out_channels,
                temb_channels=config.block_out_channels[0] * 4,
                prev_out_channels=prev_out_channels,
                num_layers=config.layers_per_block[i] + 1,
                transformer_layers_per_block=config.transformer_layers_per_block[i],
                num_attention_heads=config.num_attention_heads[i],
                cross_attention_dim=config.cross_attention_dim[i],
                resnet_groups=config.norm_num_groups,
                add_downsample=False,
                add_upsample=(i > 0),
                add_cross_attention="CrossAttn" in config.up_block_types[i],
            )
            for i, (in_channels, out_channels, prev_out_channels) in reversed(
                list(
                    enumerate(
                        zip(block_channels, block_channels[1:], block_channels[2:])
                    )
                )
            )
        ]

        self.conv_norm_out = nn.GroupNorm(
            config.norm_num_groups,
            config.block_out_channels[0],
            pytorch_compatible=True,
        )
        self.conv_out = nn.Conv2d(
            config.block_out_channels[0],
            config.out_channels,
            config.conv_out_kernel,
            padding=(config.conv_out_kernel - 1) // 2,
        )

    def __call__(
        self,
        x,
        timestep,
        encoder_x,
        attn_mask=None,
        encoder_attn_mask=None,
        text_time=None,
    ):
        # Compute the time embeddings
        temb = self.timesteps(timestep).astype(x.dtype)
        temb = self.time_embedding(temb)

        # Add the extra text_time conditioning
        if text_time is not None:
            text_emb, time_ids = text_time
            emb = self.add_time_proj(time_ids).flatten(1).astype(x.dtype)
            emb = mx.concatenate([text_emb, emb], axis=-1)
            emb = self.add_embedding(emb)
            temb = temb + emb

        # Preprocess the input
        x = self.conv_in(x)

        # Run the downsampling part of the unet
        residuals = [x]
        for block in self.down_blocks:
            x, res = block(
                x,
                encoder_x=encoder_x,
                temb=temb,
                attn_mask=attn_mask,
                encoder_attn_mask=encoder_attn_mask,
            )
            residuals.extend(res)

        # Run the middle part of the unet
        x = self.mid_blocks[0](x, temb)
        x = self.mid_blocks[1](x, encoder_x, attn_mask, encoder_attn_mask)
        x = self.mid_blocks[2](x, temb)

        # Run the upsampling part of the unet
        for block in self.up_blocks:
            x, _ = block(
                x,
                encoder_x=encoder_x,
                temb=temb,
                attn_mask=attn_mask,
                encoder_attn_mask=encoder_attn_mask,
                residual_hidden_states=residuals,
            )

        # Postprocess the output
        x = self.conv_norm_out(x)
        x = nn.silu(x)
        x = self.conv_out(x)

        return x

>>>> stable_diffusion/stable_diffusion/config.py
# Copyright © 2023-2024 Apple Inc.

from dataclasses import dataclass
from typing import Optional, Tuple


@dataclass
class AutoencoderConfig:
    in_channels: int = 3
    out_channels: int = 3
    latent_channels_out: int = 8
    latent_channels_in: int = 4
    block_out_channels: Tuple[int] = (128, 256, 512, 512)
    layers_per_block: int = 2
    norm_num_groups: int = 32
    scaling_factor: float = 0.18215


@dataclass
class CLIPTextModelConfig:
    num_layers: int = 23
    model_dims: int = 1024
    num_heads: int = 16
    max_length: int = 77
    vocab_size: int = 49408
    projection_dim: Optional[int] = None
    hidden_act: str = "quick_gelu"


@dataclass
class UNetConfig:
    in_channels: int = 4
    out_channels: int = 4
    conv_in_kernel: int = 3
    conv_out_kernel: int = 3
    block_out_channels: Tuple[int] = (320, 640, 1280, 1280)
    layers_per_block: Tuple[int] = (2, 2, 2, 2)
    mid_block_layers: int = 2
    transformer_layers_per_block: Tuple[int] = (1, 1, 1, 1)
    num_attention_heads: Tuple[int] = (5, 10, 20, 20)
    cross_attention_dim: Tuple[int] = (1024,) * 4
    norm_num_groups: int = 32
    down_block_types: Tuple[str] = (
        "CrossAttnDownBlock2D",
        "CrossAttnDownBlock2D",
        "CrossAttnDownBlock2D",
        "DownBlock2D",
    )
    up_block_types: Tuple[str] = (
        "UpBlock2D",
        "CrossAttnUpBlock2D",
        "CrossAttnUpBlock2D",
        "CrossAttnUpBlock2D",
    )
    addition_embed_type: Optional[str] = None
    addition_time_embed_dim: Optional[int] = None
    projection_class_embeddings_input_dim: Optional[int] = None


@dataclass
class DiffusionConfig:
    beta_schedule: str = "scaled_linear"
    beta_start: float = 0.00085
    beta_end: float = 0.012
    num_train_steps: int = 1000

>>>> stable_diffusion/README.md
Stable Diffusion
================

Stable Diffusion in MLX. The implementation was ported from Hugging Face's
[diffusers](https://huggingface.co/docs/diffusers/index) and model weights are
downloaded directly from the Hugging Face hub. The implementation currently
supports the following models:

- [stabilityai/sdxl-turbo](https://huggingface.co/stabilityai/sdxl-turbo)
- [stabilitiai/stable-diffusion-2-1](https://huggingface.co/stabilityai/stable-diffusion-2-1)

![out](generated-mlx.png)    
*Image generated using Stable Diffusion in MLX and the prompt 'A big red sign
saying MLX in capital letters.'*

Installation
------------

The dependencies are minimal, namely:

- `huggingface-hub` to download the checkpoints.
- `regex` for the tokenization
- `tqdm`, `PIL`, and `numpy` for the `txt2image.py` script

You can install all of the above with the `requirements.txt` as follows:

    pip install -r requirements.txt

Usage
------

Although each component in this repository can be used by itself, the fastest
way to get started is by using the `StableDiffusion` class from the `stable_diffusion`
module.

```python
import mlx.core as mx
from stable_diffusion import StableDiffusion

# This will download all the weights from HF hub and load the models in
# memory
sd = StableDiffusion()

# This creates a python generator that returns the latent produced by the
# reverse diffusion process.
#
# Because MLX is lazily evaluated iterating over this generator doesn't
# actually perform the computation until mx.eval() is called.
latent_generator = sd.generate_latents(
    "A photo of an astronaut riding a horse on Mars."
)

# Here we are evaluating each diffusion step but we could also evaluate
# once at the end.
for x_t in latent_generator:
    mx.eval(x_t)

# Now x_t is the last latent from the reverse process aka x_0. We can
# decode it into an image using the stable diffusion VAE.
im = sd.decode(x_t)
```

The above is essentially the implementation of the `txt2image.py` script in the
root of the repository. You can use the script as follows:


```shell
python txt2image.py "A photo of an astronaut riding a horse on Mars." --n_images 4 --n_rows 2
```

You can select the model using `--model` argument. Currently supported models
are `sdxl` (default) and `sd`.

Image 2 Image
-------------

There is also the option of generating images based on another image using the
example script `image2image.py`. To do that an image is first encoded using the
autoencoder to get its latent representation and then noise is added according
to the forward diffusion process and the `strength` parameter. A `strength` of
0.0 means no noise and a `strength` of 1.0 means starting from completely
random noise.

![image2image](im2im.png)    

*Generations with varying strength using the original image and the prompt 'A lit fireplace'.*

The command to generate the above images is:

```shell
python image2image.py --strength 0.5 original.png 'A lit fireplace'
```

> [!Note]
> `image2image.py` will automatically downsample your input image to guarantee
> that its dimensions are divisible by 64. If you want full control of this
> process, resize your image prior to using the script.

Memory constrained devices
--------------------------

The `txt2image.py` script by default loads the model in float16 which reduces
significantly the required memory for image generation. However, since the
Stable Diffusion XL UNet alone has 2.6B parameters in order to use it in
devices with 8GB of RAM, quantization is practically necessary.

The `txt2image.py` script supports quantization using the `-q` or `--quantize`
command line arguments. When quantization is used, the script quantizes the
text encoder models to 4 bits and the unet to 8 bits. This allows generating
images on an 8GB Mac Mini with no-swapping.

```
python txt2image.py --n_images 4 -q -v --output still-life.png "A painting of a vase on a wooden table, dark background, still life."
```

![painting](still-life.png)    
*Image generated using Stable Diffusion XL turbo in MLX with the above command on an 8GB M1 Mac mini*

>>>> CONTRIBUTING.md
# Contributing to mlx-examples

We want to make contributing to this project as easy and transparent as
possible.

## Pull Requests

1. Fork and submit pull requests to the repo. 
2. If you've added code that should be tested, add tests.
3. Every PR should have passing tests and at least one review. 
4. For code formatting install `pre-commit` using something like `pip install pre-commit` and run `pre-commit install`.
   This should install hooks for running `black` and `clang-format` to ensure
   consistent style for C++ and python code.
 
   You can also run the formatters manually as follows on individual files:
 
     ```bash
     clang-format -i file.cpp
     ```
 
     ```bash
     black file.py
     ```

     or,

     ```bash
     # single file
     pre-commit run --files file1.py 

     # specific files
     pre-commit run --files file1.py file2.py
     ```
 
   or run `pre-commit run --all-files` to check all files in the repo.

## Issues

We use GitHub issues to track public bugs. Please ensure your description is
clear and has sufficient instructions to be able to reproduce the issue.

## License

By contributing to mlx-examples, you agree that your contributions will be licensed
under the LICENSE file in the root directory of this source tree.

>>>> cvae/main.py
# Copyright © 2023-2024 Apple Inc.

import argparse
import time
from functools import partial
from pathlib import Path

import dataset
import mlx.core as mx
import mlx.nn as nn
import mlx.optimizers as optim
import numpy as np
import vae
from mlx.utils import tree_flatten
from PIL import Image


def grid_image_from_batch(image_batch, num_rows):
    """
    Generate a grid image from a batch of images.
    Assumes input has shape (B, H, W, C).
    """

    B, H, W, _ = image_batch.shape

    num_cols = B // num_rows

    # Calculate the size of the output grid image
    grid_height = num_rows * H
    grid_width = num_cols * W

    # Normalize and convert to the desired data type
    image_batch = np.array(image_batch * 255).astype(np.uint8)

    # Reshape the batch of images into a 2D grid
    grid_image = image_batch.reshape(num_rows, num_cols, H, W, -1)
    grid_image = grid_image.swapaxes(1, 2)
    grid_image = grid_image.reshape(grid_height, grid_width, -1)

    # Convert the grid to a PIL Image
    return Image.fromarray(grid_image.squeeze())


def loss_fn(model, X):
    X_recon, mu, logvar = model(X)

    # Reconstruction loss
    recon_loss = nn.losses.mse_loss(X_recon, X, reduction="sum")

    # KL divergence between encoder distribution and standard normal:
    kl_div = -0.5 * mx.sum(1 + logvar - mu.square() - logvar.exp())

    # Total loss
    return recon_loss + kl_div


def reconstruct(model, batch, out_file):
    # Reconstruct a single batch only
    images = mx.array(batch["image"])
    images_recon = model(images)[0]
    paired_images = mx.stack([images, images_recon]).swapaxes(0, 1).flatten(0, 1)
    grid_image = grid_image_from_batch(paired_images, num_rows=16)
    grid_image.save(out_file)


def generate(
    model,
    out_file,
    num_samples=128,
):
    # Sample from the latent distribution:
    z = mx.random.normal([num_samples, model.num_latent_dims])

    # Decode the latent vectors to images:
    images = model.decode(z)

    # Save all images in a single file
    grid_image = grid_image_from_batch(images, num_rows=8)
    grid_image.save(out_file)


def main(args):
    # Load the data
    img_size = (64, 64, 1)
    train_iter, test_iter = dataset.mnist(
        batch_size=args.batch_size, img_size=img_size[:2]
    )

    save_dir = Path(args.save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)

    # Load the model
    model = vae.CVAE(args.latent_dims, img_size, args.max_filters)
    mx.eval(model.parameters())

    num_params = sum(x.size for _, x in tree_flatten(model.trainable_parameters()))
    print("Number of trainable params: {:0.04f} M".format(num_params / 1e6))

    optimizer = optim.AdamW(learning_rate=args.lr)

    # Batches for reconstruction
    train_batch = next(train_iter)
    test_batch = next(test_iter)

    state = [model.state, optimizer.state]

    @partial(mx.compile, inputs=state, outputs=state)
    def step(X):
        loss_and_grad_fn = nn.value_and_grad(model, loss_fn)
        loss, grads = loss_and_grad_fn(model, X)
        optimizer.update(model, grads)
        return loss

    for e in range(1, args.epochs + 1):
        # Reset iterators and stats at the beginning of each epoch
        train_iter.reset()
        model.train()

        # Train one epoch
        tic = time.perf_counter()
        loss_acc = 0.0
        throughput_acc = 0.0

        # Iterate over training batches
        for batch_count, batch in enumerate(train_iter):
            X = mx.array(batch["image"])
            throughput_tic = time.perf_counter()

            # Forward pass + backward pass + update
            loss = step(X)

            # Evaluate updated model parameters
            mx.eval(state)

            throughput_toc = time.perf_counter()
            throughput_acc += X.shape[0] / (throughput_toc - throughput_tic)
            loss_acc += loss.item()

            if batch_count > 0 and (batch_count % 10 == 0):
                print(
                    " | ".join(
                        [
                            f"Epoch {e:4d}",
                            f"Loss {(loss_acc / batch_count):10.2f}",
                            f"Throughput {(throughput_acc / batch_count):8.2f} im/s",
                            f"Batch {batch_count:5d}",
                        ]
                    ),
                    end="\r",
                )
        toc = time.perf_counter()

        print(
            " | ".join(
                [
                    f"Epoch {e:4d}",
                    f"Loss {(loss_acc / batch_count):10.2f}",
                    f"Throughput {(throughput_acc / batch_count):8.2f} im/s",
                    f"Time {toc - tic:8.1f} (s)",
                ]
            )
        )

        model.eval()

        # Reconstruct a batch of training and test images
        reconstruct(model, train_batch, save_dir / f"train_{e:03d}.png")
        reconstruct(model, test_batch, save_dir / f"test_{e:03d}.png")

        # Generate images
        generate(model, save_dir / f"generated_{e:03d}.png")

        model.save_weights(str(save_dir / "weights.npz"))


if __name__ == "__main__":
    parser = argparse.ArgumentParser()

    parser.add_argument(
        "--cpu",
        action="store_true",
        help="Use CPU instead of GPU acceleration",
    )
    parser.add_argument("--seed", type=int, default=0, help="Random seed")
    parser.add_argument(
        "--batch-size", type=int, default=128, help="Batch size for training"
    )
    parser.add_argument(
        "--max-filters",
        type=int,
        default=64,
        help="Maximum number of filters in the convolutional layers",
    )
    parser.add_argument(
        "--epochs", type=int, default=50, help="Number of training epochs"
    )
    parser.add_argument("--lr", type=float, default=1e-3, help="Learning rate")

    parser.add_argument(
        "--latent-dims",
        type=int,
        default=8,
        help="Number of latent dimensions (positive integer)",
    )
    parser.add_argument(
        "--save-dir",
        type=str,
        default="models/",
        help="Path to save the model and reconstructed images.",
    )

    args = parser.parse_args()

    if args.cpu:
        mx.set_default_device(mx.cpu)

    np.random.seed(args.seed)
    mx.random.seed(args.seed)

    print("Options: ")
    print(f"  Device: {'GPU' if not args.cpu else 'CPU'}")
    print(f"  Seed: {args.seed}")
    print(f"  Batch size: {args.batch_size}")
    print(f"  Max number of filters: {args.max_filters}")
    print(f"  Number of epochs: {args.epochs}")
    print(f"  Learning rate: {args.lr}")
    print(f"  Number of latent dimensions: {args.latent_dims}")

    main(args)

>>>> cvae/dataset.py
# Copyright © 2023-2024 Apple Inc.

from mlx.data.datasets import load_mnist


def mnist(batch_size, img_size, root=None):
    # load train and test sets using mlx-data
    load_fn = load_mnist
    tr = load_fn(root=root, train=True)
    test = load_fn(root=root, train=False)

    # number of image channels is 1 for MNIST
    num_img_channels = 1

    # normalize to [0,1]
    def normalize(x):
        return x.astype("float32") / 255.0

    # iterator over training set
    tr_iter = (
        tr.shuffle()
        .to_stream()
        .image_resize("image", h=img_size[0], w=img_size[1])
        .key_transform("image", normalize)
        .batch(batch_size)
        .prefetch(4, 4)
    )

    # iterator over test set
    test_iter = (
        test.to_stream()
        .image_resize("image", h=img_size[0], w=img_size[1])
        .key_transform("image", normalize)
        .batch(batch_size)
    )
    return tr_iter, test_iter


if __name__ == "__main__":
    batch_size = 32
    img_size = (64, 64)  # (H, W)

    tr_iter, test_iter = mnist(batch_size=batch_size, img_size=img_size)

    B, H, W, C = batch_size, img_size[0], img_size[1], 1
    print(f"Batch size: {B}, Channels: {C}, Height: {H}, Width: {W}")

    batch_tr_iter = next(tr_iter)
    assert batch_tr_iter["image"].shape == (B, H, W, C), "Wrong training set size"
    assert batch_tr_iter["label"].shape == (batch_size,), "Wrong training set size"

    batch_test_iter = next(test_iter)
    assert batch_test_iter["image"].shape == (B, H, W, C), "Wrong training set size"
    assert batch_test_iter["label"].shape == (batch_size,), "Wrong training set size"

>>>> cvae/README.md
# Convolutional Variational Autoencoder (CVAE) on MNIST

Convolutional variational autoencoder (CVAE) implementation in MLX using
MNIST.[^1]

## Setup 

Install the requirements:

```
pip install -r requirements.txt
```

## Run


To train a VAE run:

```shell
python main.py
```

To see the supported options, do `python main.py -h`.

Training with the default options should give:

```shell
$ python train.py 
Options: 
  Device: GPU
  Seed: 0
  Batch size: 128
  Max number of filters: 64
  Number of epochs: 50
  Learning rate: 0.001
  Number of latent dimensions: 8
Number of trainable params: 0.1493 M
Epoch    1 | Loss   14626.96 | Throughput  1803.44 im/s | Time     34.3 (s)
Epoch    2 | Loss   10462.21 | Throughput  1802.20 im/s | Time     34.3 (s)
...
Epoch   50 | Loss    8293.13 | Throughput  1804.91 im/s | Time     34.2 (s)
```

The throughput was measured on a 32GB M1 Max. 

Reconstructed and generated images will be saved after each epoch in the
`models/` path. Below are examples of reconstructed training set images and
generated images.

#### Reconstruction

![MNIST Reconstructions](assets/rec_mnist.png)

#### Generation 

![MNIST Samples](assets/samples_mnist.png)


## Limitations

At the time of writing, MLX does not have transposed 2D convolutions. The
example approximates them with a combination of nearest neighbor upsampling and
regular convolutions, similar to the original U-Net. We intend to update this
example once transposed 2D convolutions are available.

[^1]: For a good overview of VAEs see the original paper [Auto-Encoding
  Variational Bayes](https://arxiv.org/abs/1312.6114) or [An Introduction to
  Variational Autoencoders](https://arxiv.org/abs/1906.02691).

>>>> normalizing_flow/main.py
# Copyright © 2023-2024 Apple Inc.

from functools import partial

import matplotlib.pyplot as plt
import mlx.core as mx
import mlx.nn as nn
import mlx.optimizers as optim
import numpy as np
from flows import RealNVP
from sklearn import datasets, preprocessing
from tqdm import trange


def get_moons_dataset(n_samples=100_000, noise=0.06):
    """Get two moons dataset with given noise level."""
    x, _ = datasets.make_moons(n_samples=n_samples, noise=noise)
    scaler = preprocessing.StandardScaler()
    x = scaler.fit_transform(x)
    return x


def main(args):
    x = get_moons_dataset(n_samples=100_000, noise=args.noise)

    model = RealNVP(args.n_transforms, args.d_params, args.d_hidden, args.n_layers)
    mx.eval(model.parameters())

    def loss_fn(model, x):
        return -mx.mean(model(x))

    optimizer = optim.Adam(learning_rate=args.learning_rate)

    state = [model.state, optimizer.state]

    @partial(mx.compile, inputs=state, outputs=state)
    def step(x):
        loss_and_grad_fn = nn.value_and_grad(model, loss_fn)
        loss, grads = loss_and_grad_fn(model, x)
        optimizer.update(model, grads)
        return loss

    with trange(args.n_steps) as steps:
        for it in steps:
            idx = np.random.choice(x.shape[0], replace=False, size=args.n_batch)
            loss = step(mx.array(x[idx]))
            mx.eval(state)
            steps.set_postfix(val=loss.item())

    # Plot samples from trained flow

    fig, axs = plt.subplots(1, args.n_transforms + 2, figsize=(26, 4))
    cmap = plt.get_cmap("Blues")
    bins = 100

    # Sample from intermediate flow-transformed distributions
    for n_transforms in range(args.n_transforms + 1):
        x_samples = model.sample((100_000, 2), n_transforms=n_transforms)

        axs[n_transforms].hist2d(x_samples[:, 0], x_samples[:, 1], bins=bins, cmap=cmap)
        axs[n_transforms].set_xlim(-2, 2)
        axs[n_transforms].set_ylim(-2, 2)
        axs[n_transforms].set_title(
            f"{n_transforms} transforms" if n_transforms > 0 else "Base distribution"
        )
        axs[n_transforms].set_xticklabels([])
        axs[n_transforms].set_yticklabels([])

    # Plot original data
    axs[-1].hist2d(x[:, 0], x[:, 1], bins=bins, cmap=cmap)
    axs[-1].set_xlim(-2, 2)
    axs[-1].set_ylim(-2, 2)
    axs[-1].set_title("Original data")
    axs[-1].set_xticklabels([])
    axs[-1].set_yticklabels([])

    plt.tight_layout()
    plt.savefig("samples.png")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--n_steps", type=int, default=5_000, help="Number of steps to train"
    )
    parser.add_argument("--n_batch", type=int, default=64, help="Batch size")
    parser.add_argument(
        "--n_transforms", type=int, default=6, help="Number of flow transforms"
    )
    parser.add_argument(
        "--d_params", type=int, default=2, help="Dimensionality of modeled distribution"
    )
    parser.add_argument(
        "--d_hidden",
        type=int,
        default=128,
        help="Hidden dimensionality of coupling conditioner",
    )
    parser.add_argument(
        "--n_layers",
        type=int,
        default=4,
        help="Number of layers in coupling conditioner",
    )
    parser.add_argument(
        "--learning_rate", type=float, default=3e-4, help="Learning rate"
    )
    parser.add_argument(
        "--noise", type=float, default=0.06, help="Noise level in two moons dataset"
    )
    parser.add_argument("--cpu", action="store_true")

    args = parser.parse_args()

    if args.cpu:
        mx.set_default_device(mx.cpu)

    main(args)

>>>> normalizing_flow/flows.py
# Copyright © 2023-2024 Apple Inc.

from typing import Optional, Tuple, Union

import mlx.core as mx
import mlx.nn as nn
from bijectors import AffineBijector, MaskedCoupling
from distributions import Normal


class MLP(nn.Module):
    def __init__(self, n_layers: int, d_in: int, d_hidden: int, d_out: int):
        super().__init__()
        layer_sizes = [d_in] + [d_hidden] * n_layers + [d_out]
        self.layers = [
            nn.Linear(idim, odim)
            for idim, odim in zip(layer_sizes[:-1], layer_sizes[1:])
        ]

    def __call__(self, x):
        for l in self.layers[:-1]:
            x = nn.gelu(l(x))
        return self.layers[-1](x)


class RealNVP(nn.Module):
    def __init__(self, n_transforms: int, d_params: int, d_hidden: int, n_layers: int):
        super().__init__()

        # Alternating masks
        self.mask_list = [mx.arange(d_params) % 2 == i % 2 for i in range(n_transforms)]
        self.mask_list = [mask.astype(mx.bool_) for mask in self.mask_list]

        self.freeze(keys=["mask_list"])

        # Conditioning MLP
        self.conditioner_list = [
            MLP(n_layers, d_params, d_hidden, 2 * d_params) for _ in range(n_transforms)
        ]

        self.base_dist = Normal(mx.zeros(d_params), mx.ones(d_params))

    def log_prob(self, x: mx.array):
        """
        Flow back to the primal Gaussian and compute log-density,
        adding the transformation log-determinant along the way.
        """
        log_prob = mx.zeros(x.shape[0])
        for mask, conditioner in zip(self.mask_list[::-1], self.conditioner_list[::-1]):
            x, ldj = MaskedCoupling(
                mask, conditioner, AffineBijector
            ).inverse_and_log_det(x)
            log_prob += ldj
        return log_prob + self.base_dist.log_prob(x).sum(-1)

    def sample(
        self,
        sample_shape: Union[int, Tuple[int, ...]],
        key: Optional[mx.array] = None,
        n_transforms: Optional[int] = None,
    ):
        """
        Sample from the primal Gaussian and flow towards the target distribution.
        """
        x = self.base_dist.sample(sample_shape, key=key)
        for mask, conditioner in zip(
            self.mask_list[:n_transforms], self.conditioner_list[:n_transforms]
        ):
            x, _ = MaskedCoupling(
                mask, conditioner, AffineBijector
            ).forward_and_log_det(x)
        return x

    def __call__(self, x: mx.array):
        return self.log_prob(x)

>>>> normalizing_flow/distributions.py
# Copyright © 2023-2024 Apple Inc.

import math
from typing import Optional, Tuple, Union

import mlx.core as mx


class Normal:
    def __init__(self, mu: mx.array, sigma: mx.array):
        super().__init__()
        self.mu = mu
        self.sigma = sigma

    def sample(
        self, sample_shape: Union[int, Tuple[int, ...]], key: Optional[mx.array] = None
    ):
        return mx.random.normal(sample_shape, key=key) * self.sigma + self.mu

    def log_prob(self, x: mx.array):
        return (
            -0.5 * math.log(2 * math.pi)
            - mx.log(self.sigma)
            - 0.5 * ((x - self.mu) / self.sigma) ** 2
        )

    def sample_and_log_prob(
        self, sample_shape: Union[int, Tuple[int, ...]], key: Optional[mx.array] = None
    ):
        x = self.sample(sample_shape, key=key)
        return x, self.log_prob(x)

>>>> normalizing_flow/README.md
# Normalizing Flow

An example of a normalizing flow for density estimation and sampling
implemented in MLX. This example implements the real NVP (non-volume
preserving) model.[^1] 

## Basic usage

```python
import mlx.core as mx
from flows import RealNVP

model = RealNVP(n_transforms=8, d_params=4, d_hidden=256, n_layers=4)

x = mx.random.normal(shape=(32, 4))

# Evaluate log-density
log_prob = model.log_prob(x=x)

# Draw samples
x_samples = model.sample(sample_shape=(32, 4))
```

## Running the example

Install the dependencies:

```
pip install -r requirements.txt
```

The example can be run with:
```
python main.py [--cpu]
```

This trains the normalizing flow on the two moons dataset and plots the result
in `samples.png`. The optional `--cpu` flag can be used to run the example on
the CPU, otherwise it will use the GPU by default.

For all available options, run:

```
python main.py --help
```

## Results

![Samples](./samples.png)

[^1]: This example is from [Density estimation using Real NVP](
  https://arxiv.org/abs/1605.08803), Dinh et al. (2016)

>>>> normalizing_flow/bijectors.py
# Copyright © 2023-2024 Apple Inc.

from typing import Tuple

import mlx.core as mx
import mlx.nn as nn


class Bijector:
    def forward_and_log_det(self, x: mx.array) -> Tuple[mx.array, mx.array]:
        raise NotImplementedError

    def inverse_and_log_det(self, y: mx.array) -> Tuple[mx.array, mx.array]:
        raise NotImplementedError


class AffineBijector(Bijector):
    def __init__(self, shift_and_log_scale: mx.array):
        self.shift_and_log_scale = shift_and_log_scale

    def forward_and_log_det(self, x: mx.array):
        shift, log_scale = mx.split(self.shift_and_log_scale, 2, axis=-1)
        y = x * mx.exp(log_scale) + shift
        log_det = log_scale
        return y, log_det

    def inverse_and_log_det(self, y: mx.array):
        shift, log_scale = mx.split(self.shift_and_log_scale, 2, axis=-1)
        x = (y - shift) * mx.exp(-log_scale)
        log_det = -log_scale
        return x, log_det


class MaskedCoupling(Bijector):
    def __init__(self, mask: mx.array, conditioner: nn.Module, bijector: Bijector):
        """Coupling layer with masking and conditioner."""
        self.mask = mask
        self.conditioner = conditioner
        self.bijector = bijector

    def apply_mask(self, x: mx.array, func: callable):
        """Transforms masked indices of `x` conditioned on unmasked indices using `func`."""
        x_masked = mx.where(self.mask, 0.0, x)
        bijector_params = self.conditioner(x_masked)
        y, log_det = func(bijector_params)
        log_det = mx.where(self.mask, log_det, 0.0)
        y = mx.where(self.mask, y, x)
        return y, mx.sum(log_det, axis=-1)

    def forward_and_log_det(self, x: mx.array):
        """Transforms masked indices of `x` conditioned on unmasked indices using bijector."""
        return self.apply_mask(
            x, lambda params: self.bijector(params).forward_and_log_det(x)
        )

    def inverse_and_log_det(self, y: mx.array):
        """Transforms masked indices of `y` conditioned on unmasked indices using bijector."""
        return self.apply_mask(
            y, lambda params: self.bijector(params).inverse_and_log_det(y)
        )

>>>> mnist/mnist.py
# Copyright © 2023 Apple Inc.

import gzip
import os
import pickle
from urllib import request

import numpy as np


def mnist(
    save_dir="/tmp",
    base_url="https://raw.githubusercontent.com/fgnt/mnist/master/",
    filename="mnist.pkl",
):
    """
    Load the MNIST dataset in 4 tensors: train images, train labels,
    test images, and test labels.

    Checks `save_dir` for already downloaded data otherwise downloads.

    Download code modified from:
      https://github.com/hsjeong5/MNIST-for-Numpy
    """

    def download_and_save(save_file):
        filename = [
            ["training_images", "train-images-idx3-ubyte.gz"],
            ["test_images", "t10k-images-idx3-ubyte.gz"],
            ["training_labels", "train-labels-idx1-ubyte.gz"],
            ["test_labels", "t10k-labels-idx1-ubyte.gz"],
        ]

        mnist = {}
        for name in filename:
            out_file = os.path.join("/tmp", name[1])
            request.urlretrieve(base_url + name[1], out_file)
        for name in filename[:2]:
            out_file = os.path.join("/tmp", name[1])
            with gzip.open(out_file, "rb") as f:
                mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=16).reshape(
                    -1, 28 * 28
                )
        for name in filename[-2:]:
            out_file = os.path.join("/tmp", name[1])
            with gzip.open(out_file, "rb") as f:
                mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=8)
        with open(save_file, "wb") as f:
            pickle.dump(mnist, f)

    save_file = os.path.join(save_dir, filename)
    if not os.path.exists(save_file):
        download_and_save(save_file)
    with open(save_file, "rb") as f:
        mnist = pickle.load(f)

    def preproc(x):
        return x.astype(np.float32) / 255.0

    mnist["training_images"] = preproc(mnist["training_images"])
    mnist["test_images"] = preproc(mnist["test_images"])
    return (
        mnist["training_images"],
        mnist["training_labels"].astype(np.uint32),
        mnist["test_images"],
        mnist["test_labels"].astype(np.uint32),
    )


def fashion_mnist(save_dir="/tmp"):
    return mnist(
        save_dir,
        base_url="http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/",
        filename="fashion_mnist.pkl",
    )


if __name__ == "__main__":
    train_x, train_y, test_x, test_y = mnist()
    assert train_x.shape == (60000, 28 * 28), "Wrong training set size"
    assert train_y.shape == (60000,), "Wrong training set size"
    assert test_x.shape == (10000, 28 * 28), "Wrong test set size"
    assert test_y.shape == (10000,), "Wrong test set size"

>>>> mnist/main.py
# Copyright © 2023 Apple Inc.

import argparse
import time
from functools import partial

import mlx.core as mx
import mlx.nn as nn
import mlx.optimizers as optim
import numpy as np

import mnist


class MLP(nn.Module):
    """A simple MLP."""

    def __init__(
        self, num_layers: int, input_dim: int, hidden_dim: int, output_dim: int
    ):
        super().__init__()
        layer_sizes = [input_dim] + [hidden_dim] * num_layers + [output_dim]
        self.layers = [
            nn.Linear(idim, odim)
            for idim, odim in zip(layer_sizes[:-1], layer_sizes[1:])
        ]

    def __call__(self, x):
        for l in self.layers[:-1]:
            x = nn.relu(l(x))
        return self.layers[-1](x)


def loss_fn(model, X, y):
    return nn.losses.cross_entropy(model(X), y, reduction="mean")


def batch_iterate(batch_size, X, y):
    perm = mx.array(np.random.permutation(y.size))
    for s in range(0, y.size, batch_size):
        ids = perm[s : s + batch_size]
        yield X[ids], y[ids]


def main(args):
    seed = 0
    num_layers = 2
    hidden_dim = 32
    num_classes = 10
    batch_size = 256
    num_epochs = 10
    learning_rate = 1e-1

    np.random.seed(seed)

    # Load the data
    train_images, train_labels, test_images, test_labels = map(
        mx.array, getattr(mnist, args.dataset)()
    )

    # Load the model
    model = MLP(num_layers, train_images.shape[-1], hidden_dim, num_classes)
    mx.eval(model.parameters())

    optimizer = optim.SGD(learning_rate=learning_rate)
    loss_and_grad_fn = nn.value_and_grad(model, loss_fn)

    @partial(mx.compile, inputs=model.state, outputs=model.state)
    def step(X, y):
        loss, grads = loss_and_grad_fn(model, X, y)
        optimizer.update(model, grads)
        return loss

    @partial(mx.compile, inputs=model.state)
    def eval_fn(X, y):
        return mx.mean(mx.argmax(model(X), axis=1) == y)

    for e in range(num_epochs):
        tic = time.perf_counter()
        for X, y in batch_iterate(batch_size, train_images, train_labels):
            step(X, y)
            mx.eval(model.state)
        accuracy = eval_fn(test_images, test_labels)
        toc = time.perf_counter()
        print(
            f"Epoch {e}: Test accuracy {accuracy.item():.3f},"
            f" Time {toc - tic:.3f} (s)"
        )


if __name__ == "__main__":
    parser = argparse.ArgumentParser("Train a simple MLP on MNIST with MLX.")
    parser.add_argument("--gpu", action="store_true", help="Use the Metal back-end.")
    parser.add_argument(
        "--dataset",
        type=str,
        default="mnist",
        choices=["mnist", "fashion_mnist"],
        help="The dataset to use.",
    )
    args = parser.parse_args()
    if not args.gpu:
        mx.set_default_device(mx.cpu)
    main(args)

>>>> mnist/README.md
# MNIST

This example shows how to run some simple models on MNIST. 

Install the dependencies:

```
pip install -r requirements.txt
```

Run the example with:

```
python main.py
```

By default, the example runs on the CPU. To run on the GPU, use: 

```
python main.py --gpu
```

For a full list of options run:

```
python main.py --help
```

To run the PyTorch or JAX examples install the respective framework.

>>>> speechcommands/main.py
import argparse
import time
from functools import partial

import kwt
import mlx.core as mx
import mlx.nn as nn
import mlx.optimizers as optim
from mlx.data.datasets import load_speechcommands
from mlx.data.features import mfsc

parser = argparse.ArgumentParser(add_help=True)
parser.add_argument(
    "--arch",
    type=str,
    default="kwt1",
    choices=[f"kwt{d}" for d in [1, 2, 3]],
    help="model architecture",
)
parser.add_argument("--batch_size", type=int, default=256, help="batch size")
parser.add_argument("--epochs", type=int, default=100, help="number of epochs")
parser.add_argument("--lr", type=float, default=1e-3, help="learning rate")
parser.add_argument("--seed", type=int, default=0, help="random seed")
parser.add_argument("--cpu", action="store_true", help="use cpu only")


def prepare_dataset(batch_size, split, root=None):
    def normalize(x):
        return (x - x.mean()) / x.std()

    data = load_speechcommands(split=split, root=root)

    data_iter = (
        data.squeeze("audio")
        .key_transform(
            "audio",
            mfsc(
                40,
                16000,
                frame_size_ms=30,
                frame_stride_ms=10,
                high_freq=7600,
                low_freq=20,
            ),
        )
        .key_transform("audio", normalize)
        .shuffle()
        .batch(batch_size)
        .to_stream()
        .prefetch(4, 4)
    )
    return data_iter


def eval_fn(model, x, y):
    return mx.mean(mx.argmax(model(x), axis=1) == y)


def train_epoch(model, train_iter, optimizer, epoch):
    def train_step(model, x, y):
        output = model(x)
        loss = mx.mean(nn.losses.cross_entropy(output, y))
        acc = mx.mean(mx.argmax(output, axis=1) == y)
        return loss, acc

    state = [model.state, optimizer.state]

    @partial(mx.compile, inputs=state, outputs=state)
    def step(x, y):
        (loss, acc), grads = nn.value_and_grad(model, train_step)(model, x, y)
        optimizer.update(model, grads)
        return loss, acc

    losses = []
    accs = []
    samples_per_sec = []

    model.train(True)
    train_iter.reset()
    for batch_counter, batch in enumerate(train_iter):
        x = mx.array(batch["audio"])
        y = mx.array(batch["label"])
        tic = time.perf_counter()
        loss, acc = step(x, y)
        mx.eval(state)
        toc = time.perf_counter()
        loss = loss.item()
        acc = acc.item()
        losses.append(loss)
        accs.append(acc)
        throughput = x.shape[0] / (toc - tic)
        samples_per_sec.append(throughput)
        if batch_counter % 25 == 0:
            print(
                " | ".join(
                    (
                        f"Epoch {epoch:02d} [{batch_counter:03d}]",
                        f"Train loss {loss:.3f}",
                        f"Train acc {acc:.3f}",
                        f"Throughput: {throughput:.2f} samples/second",
                    )
                )
            )

    mean_tr_loss = mx.mean(mx.array(losses))
    mean_tr_acc = mx.mean(mx.array(accs))
    samples_per_sec = mx.mean(mx.array(samples_per_sec))
    return mean_tr_loss, mean_tr_acc, samples_per_sec


def test_epoch(model, test_iter):
    model.train(False)
    accs = []
    throughput = []
    test_iter.reset()
    for batch_counter, batch in enumerate(test_iter):
        x = mx.array(batch["audio"])
        y = mx.array(batch["label"])
        tic = time.perf_counter()
        acc = eval_fn(model, x, y)
        accs.append(acc.item())
        toc = time.perf_counter()
        throughput.append(x.shape[0] / (toc - tic))
    mean_acc = mx.mean(mx.array(accs))
    mean_throughput = mx.mean(mx.array(throughput))
    return mean_acc, mean_throughput


def main(args):
    mx.random.seed(args.seed)

    model = getattr(kwt, args.arch)()

    print("Number of params: {:0.04f} M".format(model.num_params() / 1e6))

    optimizer = optim.SGD(learning_rate=args.lr, momentum=0.9, weight_decay=1e-4)

    train_data = prepare_dataset(args.batch_size, "train")
    val_data = prepare_dataset(args.batch_size, "validation")

    best_params = None
    best_acc = 0.0
    best_epoch = 0
    for epoch in range(args.epochs):
        tr_loss, tr_acc, throughput = train_epoch(model, train_data, optimizer, epoch)
        print(
            " | ".join(
                (
                    f"Epoch: {epoch}",
                    f"avg. Train loss {tr_loss.item():.3f}",
                    f"avg. Train acc {tr_acc.item():.3f}",
                    f"Throughput: {throughput.item():.2f} samples/sec",
                )
            )
        )

        val_acc, val_throughput = test_epoch(model, val_data)
        print(
            f"Epoch: {epoch} | Val acc {val_acc.item():.3f} | Throughput: {val_throughput.item():.2f} samples/sec"
        )

        if val_acc >= best_acc:
            best_acc = val_acc
            best_epoch = epoch
            best_params = model.parameters()
    print(f"Testing best model from epoch {best_epoch}")
    model.update(best_params)
    test_data = prepare_dataset(args.batch_size, "test")
    test_acc, _ = test_epoch(model, test_data)
    print(f"Test acc -> {test_acc.item():.3f}")


if __name__ == "__main__":
    args = parser.parse_args()
    if args.cpu:
        mx.set_default_device(mx.cpu)
    main(args)

>>>> speechcommands/kwt.py
import mlx.core as mx
import mlx.nn as nn
from mlx.utils import tree_flatten

__all__ = ["KWT", "kwt1", "kwt2", "kwt3"]


class FeedForward(nn.Sequential):
    def __init__(self, dim, hidden_dim, dropout=0.0):
        super().__init__(
            nn.Linear(dim, hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, dim),
            nn.Dropout(dropout),
        )


class Attention(nn.Module):
    def __init__(self, dim, heads, dropout=0.0):
        super().__init__()
        self.heads = heads
        self.scale = dim**-0.5
        self.qkv = nn.Linear(dim, dim * 3, bias=False)
        self.out = nn.Sequential(nn.Linear(dim, dim), nn.Dropout(dropout))

    def __call__(self, x):
        b, n, _, h = *x.shape, self.heads
        qkv = self.qkv(x)
        qkv = qkv.reshape(b, n, 3, h, -1).transpose(2, 0, 3, 1, 4)
        q, k, v = qkv
        attn = (q @ k.transpose(0, 1, 3, 2)) * self.scale
        attn = mx.softmax(attn, axis=-1)
        x = (attn @ v).transpose(0, 2, 1, 3).reshape(b, n, -1)
        x = self.out(x)
        return x


class Block(nn.Module):
    def __init__(self, dim, heads, mlp_dim, dropout=0.0):
        super().__init__()
        self.attn = Attention(dim, heads, dropout=dropout)
        self.norm1 = nn.LayerNorm(dim)
        self.ff = FeedForward(dim, mlp_dim, dropout=dropout)
        self.norm2 = nn.LayerNorm(dim)

    def __call__(self, x):
        x = self.norm1(self.attn(x)) + x
        x = self.norm2(self.ff(x)) + x
        return x


class Transformer(nn.Module):
    def __init__(self, dim, depth, heads, mlp_dim, dropout=0.0):
        super().__init__()

        self.layers = []
        for _ in range(depth):
            self.layers.append(Block(dim, heads, mlp_dim, dropout=dropout))

    def __call__(self, x):
        for layer in self.layers:
            x = layer(x)
        return x


class KWT(nn.Module):
    """
    Implements the Keyword Transformer (KWT) [1] model.

    KWT is essentially a vision transformer [2] with minor modifications:
    - Instead of square patches, KWT uses rectangular patches -> a patch
      across frequency for every timestep
    - KWT modules apply layer normalization after attention/feedforward layers

    [1] https://arxiv.org/abs/2104.11178
    [2] https://arxiv.org/abs/2010.11929

    Parameters
    ----------
    input_res: tuple of ints
        Input resolution (time, frequency)
    patch_res: tuple of ints
        Patch resolution (time, frequency)
    num_classes: int
        Number of classes
    dim: int
        Model Embedding dimension
    depth: int
        Number of transformer layers
    heads: int
        Number of attention heads
    mlp_dim: int
        Feedforward hidden dimension
    pool: str
        Pooling type, either "cls" or "mean"
    in_channels: int, optional
        Number of input channels
    dropout: float, optional
        Dropout rate
    emb_dropout: float, optional
        Embedding dropout rate
    """

    def __init__(
        self,
        input_res,
        patch_res,
        num_classes,
        dim,
        depth,
        heads,
        mlp_dim,
        pool="mean",
        in_channels=1,
        dropout=0.0,
        emb_dropout=0.0,
    ):
        super().__init__()
        self.num_patches = int(
            (input_res[0] / patch_res[0]) * (input_res[1] / patch_res[1])
        )
        self.dim = dim

        self.patch_embedding = nn.Conv2d(
            in_channels, dim, kernel_size=patch_res, stride=patch_res
        )
        self.pos_embedding = mx.random.truncated_normal(
            -0.01,
            0.01,
            (self.num_patches + 1, dim),
        )
        self.cls_token = mx.random.truncated_normal(-0.01, 0.01, (dim,))
        self.dropout = nn.Dropout(emb_dropout)
        self.transformer = Transformer(dim, depth, heads, mlp_dim, dropout)
        self.pool = pool
        self.mlp_head = nn.Sequential(nn.LayerNorm(dim), nn.Linear(dim, num_classes))

    def num_params(self):
        nparams = sum(x.size for k, x in tree_flatten(self.parameters()))
        return nparams

    def __call__(self, x):
        if x.ndim != 4:
            x = mx.expand_dims(x, axis=-1)
        x = self.patch_embedding(x)
        x = x.reshape(x.shape[0], -1, self.dim)
        assert x.shape[1] == self.num_patches

        cls_tokens = mx.broadcast_to(self.cls_token, (x.shape[0], 1, self.dim))
        x = mx.concatenate((cls_tokens, x), axis=1)

        x = x + self.pos_embedding

        x = self.dropout(x)
        x = self.transformer(x)
        x = x.mean(axis=1) if self.pool == "mean" else x[:, 0]
        x = self.mlp_head(x)
        return x


def parse_kwt_args(**kwargs):
    input_res = kwargs.pop("input_res", [98, 40])
    patch_res = kwargs.pop("patch_res", [1, 40])
    num_classes = kwargs.pop("num_classes", 35)
    emb_dropout = kwargs.pop("emb_dropout", 0.1)
    return input_res, patch_res, num_classes, emb_dropout, kwargs


def kwt1(**kwargs):
    input_res, patch_res, num_classes, emb_dropout, kwargs = parse_kwt_args(**kwargs)
    return KWT(
        input_res,
        patch_res,
        num_classes,
        dim=64,
        depth=12,
        heads=1,
        mlp_dim=256,
        emb_dropout=emb_dropout,
        **kwargs
    )


def kwt2(**kwargs):
    input_res, patch_res, num_classes, emb_dropout, kwargs = parse_kwt_args(**kwargs)
    return KWT(
        input_res,
        patch_res,
        num_classes,
        dim=128,
        depth=12,
        heads=2,
        mlp_dim=512,
        emb_dropout=emb_dropout,
        **kwargs
    )


def kwt3(**kwargs):
    input_res, patch_res, num_classes, emb_dropout, kwargs = parse_kwt_args(**kwargs)
    return KWT(
        input_res,
        patch_res,
        num_classes,
        dim=192,
        depth=12,
        heads=3,
        mlp_dim=768,
        emb_dropout=emb_dropout,
        **kwargs
    )

>>>> speechcommands/README.md
# Train a Keyword Spotting Transformer on Speech Commands

An example of training a Keyword Spotting Transformer[^1] on the Speech
Commands dataset[^2] with MLX. All supervised only configurations from the
paper are available. The example also illustrates how to use [MLX
Data](https://github.com/ml-explore/mlx-data) to load and process an audio
dataset.

## Pre-requisites

Install the remaining python requirements:

```
pip install -r requirements.txt
```

## Running the example

Run the example with:

```
python main.py
```

By default the example runs on the GPU. To run it on the CPU, use:

```
python main.py --cpu
```

For all available options, run:

```
python main.py --help
```

## Results

After training with the `kwt1` architecture for 100 epochs, you
should see the following results:

```
Epoch: 99 | avg. Train loss 0.018 | avg. Train acc 0.996 | Throughput: 662.51 samples/sec
Epoch: 99 | Val acc 0.893 | Throughput: 3091.26 samples/sec
Testing best model from epoch 97
Test acc -> 0.882
```

For the `kwt2` model, you should see:
```
Epoch: 99 | avg. Train loss 0.003 | avg. Train acc 1.000 | Throughput: 396.53 samples/sec
Epoch: 99 | Val acc 0.901 | Throughput: 1543.48 samples/sec
Testing best model from epoch 94
Test acc -> 0.893
```

Note that this was run on an M1 Macbook Pro with 16GB RAM.

At the time of writing, `mlx` doesn't have built-in `cosine` learning rate
schedules, which is used along with the AdamW optimizer in the official
implementation. We intend to update this example once these features are added,
as well as with appropriate data augmentations.

[^1]: Based on the paper [Keyword Transformer: A Self-Attention Model for Keyword Spotting](https://www.isca-speech.org/archive/interspeech_2021/berg21_interspeech.html)  
[^2]: We use version 0.02. See the [paper](https://arxiv.org/abs/1804.03209) for more details.

>>>> musicgen/generate.py
# Copyright © 2024 Apple Inc.

import argparse

from utils import save_audio

from musicgen import MusicGen


def main(text: str, output_path: str, model_name: str, max_steps: int):
    model = MusicGen.from_pretrained(model_name)
    audio = model.generate(text, max_steps=max_steps)
    save_audio(output_path, audio, model.sampling_rate)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", required=False, default="facebook/musicgen-medium")
    parser.add_argument("--text", required=False, default="happy rock")
    parser.add_argument("--output-path", required=False, default="0.wav")
    parser.add_argument("--max-steps", required=False, default=500, type=int)
    args = parser.parse_args()
    main(args.text, args.output_path, args.model, args.max_steps)

>>>> musicgen/benchmarks/bench_pt.py
# Copyright © 2024 Apple Inc.

import time

import torch
from transformers import AutoProcessor, MusicgenForConditionalGeneration

model_name = "facebook/musicgen-medium"
processor = AutoProcessor.from_pretrained(model_name)
model = MusicgenForConditionalGeneration.from_pretrained(model_name).to("mps")

inputs = processor(
    text=["folk ballad"],
    padding=True,
    return_tensors="pt",
)
inputs["input_ids"] = inputs["input_ids"].to("mps")
inputs["attention_mask"] = inputs["attention_mask"].to("mps")

# warmup
audio_values = model.generate(**inputs, max_new_tokens=10)
torch.mps.synchronize()

max_steps = 100
tic = time.time()
audio_values = model.generate(**inputs, max_new_tokens=max_steps)
torch.mps.synchronize()
toc = time.time()

ms = 1000 * (toc - tic) / max_steps
print(f"Time (ms) per step: {ms:.3f}")

>>>> musicgen/benchmarks/bench_mx.py
# Copyright © 2024 Apple Inc.

import sys
import time
from pathlib import Path

import mlx.core as mx

cur_path = Path(__file__).parents[1].resolve()
sys.path.append(str(cur_path))

from musicgen import MusicGen

text = "folk ballad"
model = MusicGen.from_pretrained("facebook/musicgen-medium")

max_steps = 100

audio = model.generate(text, max_steps=10)
mx.eval(audio)

tic = time.time()
audio = model.generate(text, max_steps=max_steps)
mx.eval(audio)
toc = time.time()

ms = 1000 * (toc - tic) / max_steps
print(f"Time (ms) per step: {ms:.3f}")

>>>> musicgen/utils.py
# Copyright © 2024 Apple Inc.

import mlx.core as mx
import numpy as np


def save_audio(file: str, audio: mx.array, sampling_rate: int):
    """
    Save audio to a wave (.wav) file.
    """
    from scipy.io.wavfile import write

    audio = mx.clip(audio, -1, 1)
    audio = (audio * 32767).astype(mx.int16)
    write(file, sampling_rate, np.array(audio))

>>>> musicgen/README.md
# MusicGen

An example of Meta's MusicGen model in MLX.[^1] MusicGen is used to generate
music from text descriptions.

### Setup

Install the requirements:

```
pip install -r requirements.txt
```

### Example

An example using the model:

```python
from musicgen import MusicGen
from utils import save_audio

model = MusicGen.from_pretrained("facebook/musicgen-medium")

audio = model.generate("happy rock")

save_audio("out.wav", audio, model.sampling_rate)
```

[^1]: Refer to the [arXiv paper](https://arxiv.org/abs/2306.05284) and
  [code](https://github.com/facebookresearch/audiocraft/blob/main/docs/MUSICGEN.md) for more details.

>>>> musicgen/musicgen.py
# Copyright © 2024 Apple Inc.

import json
from functools import partial
from pathlib import Path
from types import SimpleNamespace
from typing import Optional

import mlx.core as mx
import mlx.nn as nn
from tqdm import tqdm

from encodec import EncodecModel
from t5 import T5


class TextConditioner(nn.Module):
    def __init__(self, t5_name, input_dim, output_dim):
        super().__init__()
        self._t5, self.tokenizer = T5.from_pretrained(t5_name)
        self.output_proj = nn.Linear(input_dim, output_dim)

    def __call__(self, text):
        x = self.tokenizer.encode(text)
        x = self._t5.encode(x)
        return self.output_proj(x)


class KVCache:
    def __init__(self, head_dim, n_kv_heads):
        self.n_kv_heads = n_kv_heads
        if isinstance(head_dim, int):
            self.k_head_dim = self.v_head_dim = head_dim
        elif isinstance(head_dim, tuple) and len(head_dim) == 2:
            self.k_head_dim, self.v_head_dim = head_dim
        else:
            raise ValueError("head_dim must be an int or a tuple of two ints")
        self.keys = None
        self.values = None
        self.offset = 0
        self.step = 256

    def update_and_fetch(self, keys, values):
        prev = self.offset
        if self.keys is None or (prev + keys.shape[2]) > self.keys.shape[2]:
            B = keys.shape[0]
            n_steps = (self.step + keys.shape[2] - 1) // self.step
            k_shape = (B, self.n_kv_heads, n_steps * self.step, self.k_head_dim)
            v_shape = (B, self.n_kv_heads, n_steps * self.step, self.v_head_dim)
            new_k = mx.zeros(k_shape, keys.dtype)
            new_v = mx.zeros(v_shape, values.dtype)
            if self.keys is not None:
                if prev % self.step != 0:
                    self.keys = self.keys[..., :prev, :]
                    self.values = self.values[..., :prev, :]
                self.keys = mx.concatenate([self.keys, new_k], axis=2)
                self.values = mx.concatenate([self.values, new_v], axis=2)
            else:
                self.keys, self.values = new_k, new_v

        self.offset += keys.shape[2]
        self.keys[..., prev : self.offset, :] = keys
        self.values[..., prev : self.offset, :] = values
        return self.keys[..., : self.offset, :], self.values[..., : self.offset, :]

    @property
    def state(self):
        return self.keys, self.values


class MultiHeadAttention(nn.Module):
    def __init__(self, dim, n_heads):
        super().__init__()

        self.n_heads = n_heads

        head_dim = dim // n_heads

        self.scale = head_dim**-0.5

        self.q_proj = nn.Linear(dim, dim, bias=False)
        self.k_proj = nn.Linear(dim, dim, bias=False)
        self.v_proj = nn.Linear(dim, dim, bias=False)
        self.out_proj = nn.Linear(dim, dim, bias=False)

    def __call__(
        self,
        queries: mx.array,
        keys: mx.array,
        values: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[KVCache] = None,
    ) -> mx.array:
        B, L_q, D = queries.shape
        L_k = keys.shape[1]

        queries, keys, values = (
            self.q_proj(queries),
            self.k_proj(keys),
            self.v_proj(values),
        )

        # Prepare the queries, keys and values for the attention computation
        queries = queries.reshape(B, L_q, self.n_heads, -1).transpose(0, 2, 1, 3)
        keys = keys.reshape(B, L_k, self.n_heads, -1).transpose(0, 2, 1, 3)
        values = values.reshape(B, L_k, self.n_heads, -1).transpose(0, 2, 1, 3)

        if cache is not None:
            keys, values = cache.update_and_fetch(keys, values)

        output = mx.fast.scaled_dot_product_attention(
            queries, keys, values, scale=self.scale, mask=mask
        )
        output = output.transpose(0, 2, 1, 3).reshape(B, L_q, -1)
        return self.out_proj(output)


class TransformerBlock(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.num_attention_heads = config.decoder.num_attention_heads
        self.hidden_size = config.decoder.hidden_size
        self.self_attn = MultiHeadAttention(self.hidden_size, self.num_attention_heads)
        self.cross_attn = MultiHeadAttention(self.hidden_size, self.num_attention_heads)
        self.linear1 = nn.Linear(self.hidden_size, config.decoder.ffn_dim, bias=False)
        self.linear2 = nn.Linear(config.decoder.ffn_dim, self.hidden_size, bias=False)

        self.norm1 = nn.LayerNorm(self.hidden_size, eps=1e-5)
        self.norm_cross = nn.LayerNorm(self.hidden_size, eps=1e-5)
        self.norm2 = nn.LayerNorm(self.hidden_size, eps=1e-5)

    def __call__(
        self,
        x: mx.array,
        conditioning: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[KVCache] = None,
    ) -> mx.array:
        xn = self.norm1(x)
        x += self.self_attn(xn, xn, xn, mask, cache)
        xn = self.norm_cross(x)
        x += self.cross_attn(xn, conditioning, conditioning, mask)
        xn = self.norm2(x)
        x += self.linear2(nn.gelu(self.linear1(xn)))
        return x


@partial(mx.compile, inputs=mx.random.state, outputs=mx.random.state)
def top_k_sampling(
    logits: mx.array, top_k: float, temperature: float, axis: int = -1
) -> mx.array:
    """
    Apply top-k sampling to logits.

    Args:
        logits: The logits from the model's output.
        top_k: Sample from the top k logits.
        temperature: Temperature parameter for softmax distribution reshaping.
        axis: Axis along which to sample.
    Returns:
        token selected based on the top-k criterion.
    """
    # referenced implementation from https://github.com/huggingface/transformers/blob/main/src/transformers/generation/logits_process.py#L449-L460
    probs = mx.softmax(logits * (1 / temperature), axis=axis)

    # sort probs in ascending order
    sorted_indices = mx.argsort(probs, axis=axis)
    sorted_probs = mx.take_along_axis(probs, sorted_indices, axis=axis)
    prob_threshold = mx.take(sorted_probs, mx.array(-top_k), axis=axis)

    # select the top K tokens in probability
    top_probs = mx.where(
        sorted_probs > prob_threshold,
        sorted_probs,
        0,
    )

    sorted_token = mx.random.categorical(mx.log(top_probs), axis=axis)
    token = mx.take_along_axis(
        sorted_indices, mx.expand_dims(sorted_token, axis), axis=axis
    )

    return token


def create_sin_embedding(positions: mx.array, dim: int, max_period: float = 10000):
    assert dim % 2 == 0
    half_dim = dim // 2
    adim = mx.arange(half_dim).reshape(1, 1, -1)
    phase = positions / (max_period ** (adim / (half_dim - 1)))
    return mx.concatenate([mx.cos(phase), mx.sin(phase)], axis=-1)


class MusicGen(nn.Module):
    def __init__(self, config):
        self.num_codebooks = config.decoder.num_codebooks
        self.codebook_size = config.audio_encoder.codebook_size
        self.bos_token_id = config.decoder.bos_token_id
        self.hidden_size = config.decoder.hidden_size
        self.num_attention_heads = config.decoder.num_attention_heads
        self.sampling_rate = config.audio_encoder.sampling_rate

        self.text_conditioner = TextConditioner(
            config.text_encoder._name_or_path,
            config.text_encoder.d_model,
            self.hidden_size,
        )
        self.emb = [
            nn.Embedding(self.codebook_size + 1, self.hidden_size)
            for _ in range(self.num_codebooks)
        ]
        self.layers = [
            TransformerBlock(config) for _ in range(config.decoder.num_hidden_layers)
        ]
        self.out_norm = nn.LayerNorm(self.hidden_size, eps=1e-5)
        self.linears = [
            nn.Linear(self.hidden_size, self.codebook_size, bias=False)
            for _ in range(self.num_codebooks)
        ]
        encodec_name = config.audio_encoder._name_or_path.split("/")[-1]
        encodec_name = encodec_name.replace("_", "-")
        self._audio_decoder, _ = EncodecModel.from_pretrained(
            f"mlx-community/{encodec_name}-float32"
        )

    def __call__(
        self,
        audio_tokens: mx.array,
        conditioning: mx.array,
        cache: list[KVCache] = None,
    ):

        if cache is None:
            cache = [None] * len(self.layers)

        x = sum([self.emb[k](audio_tokens[..., k]) for k in range(self.num_codebooks)])

        offset = cache[0].offset if cache[0] is not None else 0
        pos_emb = create_sin_embedding(offset, self.hidden_size)
        x += pos_emb.astype(x.dtype)

        for layer, c in zip(self.layers, cache):
            x = layer(x, conditioning, cache=c)

        x = self.out_norm(x)
        x = mx.stack([self.linears[k](x) for k in range(self.num_codebooks)], axis=-1)
        return x

    def generate(
        self,
        text: str,
        max_steps: int = 200,
        top_k: int = 250,
        temp: float = 1.0,
        guidance_coef: float = 3.0,
    ) -> mx.array:
        """
        Generates a waveform conditioned on `text`.

        Args:
            text (str): The text to condition generation on.
            max_steps (int): Max steps to generate.
            top_k (int): Top k used in sampling.
            temp (float): Sampling softmax temperature.
            guidance_coef (float): Classifier free guidance coefficent.
                Used to combine conditional and unconditional logits.

        Returns:
            An mx.array of audio samples of shape ``(num_samples,)``.
        """
        # Assuming no audio prompt we start with all bos token for the codebooks
        audio_shape = (1, max_steps + 1, self.num_codebooks)
        audio_seq = mx.full(audio_shape, self.bos_token_id)

        text_tokens = self.text_conditioner(text)
        # Compute conditional and unconditional logits in one batch
        text_tokens = mx.concatenate([text_tokens, mx.zeros_like(text_tokens)], axis=0)

        head_dim = self.hidden_size // self.num_attention_heads
        cache = [
            KVCache(head_dim, self.num_attention_heads) for _ in range(len(self.layers))
        ]
        for offset in tqdm(range(max_steps)):
            audio_input = mx.tile(audio_seq[:, offset : offset + 1], [2, 1, 1])
            audio_logits = self(audio_input, text_tokens, cache)
            cond_logits, uncond_logits = audio_logits[:1], audio_logits[1:2]
            audio_logits = uncond_logits + (cond_logits - uncond_logits) * guidance_coef
            audio_tokens = top_k_sampling(audio_logits, top_k, temp, axis=-2)
            # "delay" pattern
            audio_tokens[..., offset + 1 :] = self.bos_token_id
            audio_tokens[..., : -max_steps + offset] = self.bos_token_id
            audio_seq[:, offset + 1 : offset + 2] = audio_tokens
            mx.eval(audio_seq)

        # Undo delay
        for i in range(self.num_codebooks):
            audio_seq[:, : -self.num_codebooks, i] = audio_seq[
                :, i : -self.num_codebooks + i, i
            ]
        audio_seq = audio_seq[:, 1 : -self.num_codebooks + 1]

        audio_seq = mx.swapaxes(audio_seq, -1, -2)[:, mx.newaxis]
        audio = self._audio_decoder.decode(audio_seq, audio_scales=[None])
        return audio[0]

    @classmethod
    def sanitize(cls, weights):
        out_weights = {}
        for k, arr in weights.items():
            if k.startswith("transformer."):
                k = k[len("transformer.") :]

            if "cross_attention" in k:
                k = k.replace("cross_attention", "cross_attn")

            if "condition_provider" in k:
                k = k.replace(
                    "condition_provider.conditioners.description", "text_conditioner"
                )

            if "in_proj_weight" in k:
                dim = arr.shape[0] // 3
                name = "in_proj_weight"
                out_weights[k.replace(name, "q_proj.weight")] = arr[:dim]
                out_weights[k.replace(name, "k_proj.weight")] = arr[dim : dim * 2]
                out_weights[k.replace(name, "v_proj.weight")] = arr[dim * 2 :]
                continue

            out_weights[k] = arr
        return out_weights

    @classmethod
    def from_pretrained(cls, path_or_repo: str):
        import torch
        from huggingface_hub import snapshot_download

        path = Path(path_or_repo)
        if not path.exists():
            path = Path(
                snapshot_download(
                    repo_id=path_or_repo,
                    allow_patterns=["*.json", "state_dict.bin"],
                )
            )

        with open(path / "config.json", "r") as f:
            config = SimpleNamespace(**json.load(f))
            config.text_encoder = SimpleNamespace(**config.text_encoder)
            config.audio_encoder = SimpleNamespace(**config.audio_encoder)
            config.decoder = SimpleNamespace(**config.decoder)

        weights = torch.load(path / "state_dict.bin", weights_only=True)["best_state"]
        weights = {k: mx.array(v) for k, v in weights.items()}
        weights = cls.sanitize(weights)

        model = MusicGen(config)
        model.load_weights(list(weights.items()))
        return model
