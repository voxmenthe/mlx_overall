lora_parameters:
  rank: 64       # LoRA rank (dimension of the adapter matrices)
  dropout: 0.06  # Dropout applied to the LoRA matrices
  scale: 16.0    # Scaling factor for the LoRA update (higher means more influence) - original 10.0
learning_rate: 8e-6 # Overrides LEARNING_RATE in the bash script
